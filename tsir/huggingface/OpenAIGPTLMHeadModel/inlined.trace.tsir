graph(%self.1 : __torch__.transformers.modeling_openai.OpenAIGPTLMHeadModel,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_29034.Linear = prim::GetAttr[name="lm_head"](%self.1)
  %4 : __torch__.transformers.modeling_openai.___torch_mangle_29033.OpenAIGPTModel = prim::GetAttr[name="transformer"](%self.1)
  %8 : int = prim::Constant[value=9223372036854775807](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %9 : int = prim::Constant[value=-2](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %10 : Double() = prim::Constant[value={8}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:180:0
  %11 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:213:0
  %12 : int = prim::Constant[value=12](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:209:0
  %13 : Long() = prim::Constant[value={12}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:424:0
  %14 : int = prim::Constant[value=768](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:371:0
  %15 : int = prim::Constant[value=2304](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1095:0
  %16 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_1 # torch/nn/functional.py:2048:0
  %17 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_1 # torch/nn/functional.py:2048:0
  %18 : Double() = prim::Constant[value={1}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %19 : Double() = prim::Constant[value={0.797885}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %20 : Double() = prim::Constant[value={0.044715}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %21 : float = prim::Constant[value=3.](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %22 : Double() = prim::Constant[value={0.5}](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %23 : int = prim::Constant[value=3072](), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %24 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.drop # torch/nn/functional.py:973:0
  %25 : Long() = prim::Constant[value={0}](), scope: __module.transformer # transformers/modeling_openai.py:497:0
  %26 : Double() = prim::Constant[value={-10000}](), scope: __module.transformer # transformers/modeling_openai.py:484:0
  %27 : float = prim::Constant[value=1.](), scope: __module.transformer # torch/tensor.py:396:0
  %28 : None = prim::Constant(), scope: __module.transformer
  %29 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_openai.py:483:0
  %30 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_openai.py:483:0
  %31 : int = prim::Constant[value=2](), scope: __module.transformer # transformers/modeling_openai.py:476:0
  %32 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_openai.py:459:0
  %33 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_openai.py:458:0
  %34 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_openai.py:458:0
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %36 : __torch__.transformers.modeling_openai.___torch_mangle_29031.Block = prim::GetAttr[name="11"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %38 : __torch__.transformers.modeling_openai.___torch_mangle_29019.Block = prim::GetAttr[name="10"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %40 : __torch__.transformers.modeling_openai.___torch_mangle_29007.Block = prim::GetAttr[name="9"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %42 : __torch__.transformers.modeling_openai.___torch_mangle_28995.Block = prim::GetAttr[name="8"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %44 : __torch__.transformers.modeling_openai.___torch_mangle_28983.Block = prim::GetAttr[name="7"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %46 : __torch__.transformers.modeling_openai.___torch_mangle_28971.Block = prim::GetAttr[name="6"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %48 : __torch__.transformers.modeling_openai.___torch_mangle_28959.Block = prim::GetAttr[name="5"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %50 : __torch__.transformers.modeling_openai.___torch_mangle_28947.Block = prim::GetAttr[name="4"](%49)
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %52 : __torch__.transformers.modeling_openai.___torch_mangle_28935.Block = prim::GetAttr[name="3"](%51)
  %53 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %54 : __torch__.transformers.modeling_openai.___torch_mangle_28923.Block = prim::GetAttr[name="2"](%53)
  %55 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %56 : __torch__.transformers.modeling_openai.___torch_mangle_28911.Block = prim::GetAttr[name="1"](%55)
  %57 : __torch__.torch.nn.modules.container.___torch_mangle_29032.ModuleList = prim::GetAttr[name="h"](%4)
  %58 : __torch__.transformers.modeling_openai.___torch_mangle_28899.Block = prim::GetAttr[name="0"](%57)
  %59 : __torch__.torch.nn.modules.sparse.___torch_mangle_28886.Embedding = prim::GetAttr[name="positions_embed"](%4)
  %60 : __torch__.torch.nn.modules.sparse.___torch_mangle_28885.Embedding = prim::GetAttr[name="tokens_embed"](%4)
  %61 : Tensor = prim::GetAttr[name="position_ids"](%4)
  %62 : int = aten::size(%input_ids, %34), scope: __module.transformer # transformers/modeling_openai.py:458:0
  %63 : int = aten::size(%input_ids, %33), scope: __module.transformer # transformers/modeling_openai.py:458:0
  %64 : int[] = prim::ListConstruct(%32, %63), scope: __module.transformer
  %input.1 : Long(17:13, 13:1) = aten::view(%input_ids, %64), scope: __module.transformer # transformers/modeling_openai.py:459:0
  %66 : Long(1:512, 512:1) = aten::unsqueeze(%61, %34), scope: __module.transformer # transformers/modeling_openai.py:467:0
  %input.2 : Long(1:512, 13:1) = aten::slice(%66, %33, %34, %63, %33), scope: __module.transformer # transformers/modeling_openai.py:467:0
  %68 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %33), scope: __module.transformer # transformers/modeling_openai.py:476:0
  %attention_mask.2 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%68, %31), scope: __module.transformer # transformers/modeling_openai.py:476:0
  %70 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%attention_mask.2, %30, %29, %29, %28), scope: __module.transformer # transformers/modeling_openai.py:483:0
  %71 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%70, %27, %33), scope: __module.transformer # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%71, %26), scope: __module.transformer # transformers/modeling_openai.py:484:0
  %73 : Tensor = prim::GetAttr[name="weight"](%60)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%73, %input.1, %32, %29, %29), scope: __module.transformer/__module.transformer.tokens_embed # torch/nn/functional.py:1814:0
  %75 : Tensor = prim::GetAttr[name="weight"](%59)
  %position_embeds : Float(1:9984, 13:768, 768:1) = aten::embedding(%75, %input.2, %32, %29, %29), scope: __module.transformer/__module.transformer.positions_embed # torch/nn/functional.py:1814:0
  %77 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeds, %33), scope: __module.transformer # transformers/modeling_openai.py:497:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%77, %25, %33), scope: __module.transformer # transformers/modeling_openai.py:497:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.3, %24, %29), scope: __module.transformer/__module.transformer.drop # torch/nn/functional.py:973:0
  %80 : int = aten::size(%hidden_states.1, %32), scope: __module.transformer # transformers/modeling_openai.py:500:0
  %81 : __torch__.torch.nn.modules.normalization.___torch_mangle_28898.LayerNorm = prim::GetAttr[name="ln_2"](%58)
  %82 : __torch__.transformers.modeling_openai.___torch_mangle_28897.MLP = prim::GetAttr[name="mlp"](%58)
  %83 : __torch__.torch.nn.modules.normalization.___torch_mangle_28893.LayerNorm = prim::GetAttr[name="ln_1"](%58)
  %84 : __torch__.transformers.modeling_openai.___torch_mangle_28892.Attention = prim::GetAttr[name="attn"](%58)
  %85 : __torch__.transformers.modeling_utils.___torch_mangle_28889.Conv1D = prim::GetAttr[name="c_proj"](%84)
  %86 : Tensor = prim::GetAttr[name="bias"](%84)
  %87 : __torch__.transformers.modeling_utils.___torch_mangle_28888.Conv1D = prim::GetAttr[name="c_attn"](%84)
  %88 : Tensor = prim::GetAttr[name="weight"](%87)
  %89 : Tensor = prim::GetAttr[name="bias"](%87)
  %90 : int = aten::size(%hidden_states.1, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1093:0
  %91 : int = aten::size(%hidden_states.1, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1093:0
  %92 : int = aten::size(%hidden_states.1, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1094:0
  %93 : int[] = prim::ListConstruct(%32, %92), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn
  %94 : Float(221:768, 768:1) = aten::view(%hidden_states.1, %93), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.1 : Float(221:2304, 2304:1) = aten::addmm(%89, %94, %88, %33, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1094:0
  %96 : int[] = prim::ListConstruct(%90, %91, %15), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn
  %97 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.1, %96), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_attn # transformers/modeling_utils.py:1095:0
  %98 : Tensor[] = aten::split(%97, %14, %31), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:371:0
  %x.2 : Float(17:29952, 13:2304, 768:1), %x.4 : Float(17:29952, 13:2304, 768:1), %x.6 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%98), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %102 : int = aten::size(%x.2, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %103 : int = aten::size(%x.2, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %104 : int = aten::size(%x.2, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %105 : Long() = prim::NumToTensor(%104), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %106 : Long() = aten::floor_divide(%105, %13), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:424:0
  %107 : int = aten::Int(%106), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %108 : int[] = prim::ListConstruct(%102, %103, %12, %107), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %x.3 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.2, %108), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:209:0
  %110 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %q.1 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.3, %110), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:213:0
  %112 : int = aten::size(%x.4, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %113 : int = aten::size(%x.4, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %114 : int = aten::size(%x.4, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %115 : Long() = prim::NumToTensor(%114), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %116 : Long() = aten::floor_divide(%115, %13), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:424:0
  %117 : int = aten::Int(%116), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %118 : int[] = prim::ListConstruct(%112, %113, %12, %117), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %x.5 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.4, %118), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:209:0
  %120 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %k.1 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.5, %120), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:211:0
  %122 : int = aten::size(%x.6, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %123 : int = aten::size(%x.6, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %124 : int = aten::size(%x.6, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:208:0
  %125 : Long() = prim::NumToTensor(%124), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %126 : Long() = aten::floor_divide(%125, %13), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:424:0
  %127 : int = aten::Int(%126), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %128 : int[] = prim::ListConstruct(%122, %123, %12, %127), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %x.7 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.6, %128), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:209:0
  %130 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %v.1 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.7, %130), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:213:0
  %w.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.1, %k.1), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:178:0
  %w.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.1, %10), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:180:0
  %134 : int = aten::size(%w.2, %9), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %135 : int = aten::size(%w.2, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %136 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%86, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %137 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%136, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %138 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%137, %31, %34, %134, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %b.1 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%138, %11, %34, %135, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:183:0
  %140 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.2, %b.1), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:184:0
  %141 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.1, %33, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/tensor.py:396:0
  %142 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%141, %26), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:184:0
  %w.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%140, %142, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:184:0
  %input.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.3, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:188:0
  %input.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.4, %32, %28), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # torch/nn/functional.py:1498:0
  %w.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.5, %24, %29), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.8 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.4, %v.1), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:197:0
  %148 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %149 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.8, %148), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:203:0
  %x.9 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%149, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:203:0
  %151 : int = aten::size(%x.9, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:204:0
  %152 : int = aten::size(%x.9, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:204:0
  %153 : int = aten::size(%x.9, %9), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:204:0
  %154 : Long() = prim::NumToTensor(%153), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %155 : int = aten::size(%x.9, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:204:0
  %156 : Long() = prim::NumToTensor(%155), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %157 : Long() = aten::mul(%154, %156), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:204:0
  %158 : int = aten::Int(%157), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %159 : int[] = prim::ListConstruct(%151, %152, %158), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn
  %x.10 : Float(17:9984, 13:768, 768:1) = aten::view(%x.9, %159), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn # transformers/modeling_openai.py:205:0
  %161 : Tensor = prim::GetAttr[name="weight"](%85)
  %162 : Tensor = prim::GetAttr[name="bias"](%85)
  %163 : int = aten::size(%x.10, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1093:0
  %164 : int = aten::size(%x.10, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1093:0
  %165 : int = aten::size(%x.10, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1094:0
  %166 : int[] = prim::ListConstruct(%32, %165), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj
  %167 : Float(221:768, 768:1) = aten::view(%x.10, %166), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.11 : Float(221:768, 768:1) = aten::addmm(%162, %167, %161, %33, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1094:0
  %169 : int[] = prim::ListConstruct(%163, %164, %14), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj
  %input.6 : Float(17:9984, 13:768, 768:1) = aten::view(%x.11, %169), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.6, %24, %29), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.attn/__module.transformer.h.0.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.7 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %a.1, %33), scope: __module.transformer/__module.transformer.h.0 # transformers/modeling_openai.py:266:0
  %173 : Tensor = prim::GetAttr[name="bias"](%83)
  %174 : Tensor = prim::GetAttr[name="weight"](%83)
  %175 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_1
  %x.12 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.7, %175, %174, %173, %17, %16), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_1 # torch/nn/functional.py:2048:0
  %177 : __torch__.transformers.modeling_utils.___torch_mangle_28895.Conv1D = prim::GetAttr[name="c_proj"](%82)
  %178 : __torch__.transformers.modeling_utils.___torch_mangle_28894.Conv1D = prim::GetAttr[name="c_fc"](%82)
  %179 : Tensor = prim::GetAttr[name="weight"](%178)
  %180 : Tensor = prim::GetAttr[name="bias"](%178)
  %181 : int = aten::size(%x.12, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %182 : int = aten::size(%x.12, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %183 : int = aten::size(%x.12, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %184 : int[] = prim::ListConstruct(%32, %183), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc
  %185 : Float(221:768, 768:1) = aten::view(%x.12, %184), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.13 : Float(221:3072, 3072:1) = aten::addmm(%180, %185, %179, %33, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %187 : int[] = prim::ListConstruct(%181, %182, %23), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc
  %x.14 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.13, %187), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %189 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.14, %22), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %190 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.14, %21), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %191 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%190, %20), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %192 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.14, %191, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %193 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%192, %19), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %194 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%193), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %195 : Float(17:39936, 13:3072, 3072:1) = aten::add(%194, %18, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %x.15 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%189, %195), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp # transformers/activations.py:30:0
  %197 : Tensor = prim::GetAttr[name="weight"](%177)
  %198 : Tensor = prim::GetAttr[name="bias"](%177)
  %199 : int = aten::size(%x.15, %34), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %200 : int = aten::size(%x.15, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %201 : int = aten::size(%x.15, %32), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %202 : int[] = prim::ListConstruct(%32, %201), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj
  %203 : Float(221:3072, 3072:1) = aten::view(%x.15, %202), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.16 : Float(221:768, 768:1) = aten::addmm(%198, %203, %197, %33, %33), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %205 : int[] = prim::ListConstruct(%199, %200, %14), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%x.16, %205), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.8, %24, %29), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.mlp/__module.transformer.h.0.mlp.dropout # torch/nn/functional.py:973:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add(%x.12, %m.1, %33), scope: __module.transformer/__module.transformer.h.0 # transformers/modeling_openai.py:268:0
  %209 : Tensor = prim::GetAttr[name="bias"](%81)
  %210 : Tensor = prim::GetAttr[name="weight"](%81)
  %211 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_2
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.9, %211, %210, %209, %17, %16), scope: __module.transformer/__module.transformer.h.0/__module.transformer.h.0.ln_2 # torch/nn/functional.py:2048:0
  %213 : __torch__.torch.nn.modules.normalization.___torch_mangle_28910.LayerNorm = prim::GetAttr[name="ln_2"](%56)
  %214 : __torch__.transformers.modeling_openai.___torch_mangle_28909.MLP = prim::GetAttr[name="mlp"](%56)
  %215 : __torch__.torch.nn.modules.normalization.___torch_mangle_28905.LayerNorm = prim::GetAttr[name="ln_1"](%56)
  %216 : __torch__.transformers.modeling_openai.___torch_mangle_28904.Attention = prim::GetAttr[name="attn"](%56)
  %217 : __torch__.transformers.modeling_utils.___torch_mangle_28901.Conv1D = prim::GetAttr[name="c_proj"](%216)
  %218 : Tensor = prim::GetAttr[name="bias"](%216)
  %219 : __torch__.transformers.modeling_utils.___torch_mangle_28900.Conv1D = prim::GetAttr[name="c_attn"](%216)
  %220 : Tensor = prim::GetAttr[name="weight"](%219)
  %221 : Tensor = prim::GetAttr[name="bias"](%219)
  %222 : int = aten::size(%x.17, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1093:0
  %223 : int = aten::size(%x.17, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1093:0
  %224 : int = aten::size(%x.17, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1094:0
  %225 : int[] = prim::ListConstruct(%32, %224), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn
  %226 : Float(221:768, 768:1) = aten::view(%x.17, %225), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.18 : Float(221:2304, 2304:1) = aten::addmm(%221, %226, %220, %33, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1094:0
  %228 : int[] = prim::ListConstruct(%222, %223, %15), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn
  %229 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.18, %228), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_attn # transformers/modeling_utils.py:1095:0
  %230 : Tensor[] = aten::split(%229, %14, %31), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/tensor.py:371:0
  %x.19 : Float(17:29952, 13:2304, 768:1), %x.21 : Float(17:29952, 13:2304, 768:1), %x.23 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%230), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %234 : int = aten::size(%x.19, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %235 : int = aten::size(%x.19, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %236 : int = aten::size(%x.19, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %237 : Long() = prim::NumToTensor(%236), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %238 : Long() = aten::floor_divide(%237, %13), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/tensor.py:424:0
  %239 : int = aten::Int(%238), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %240 : int[] = prim::ListConstruct(%234, %235, %12, %239), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %x.20 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.19, %240), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:209:0
  %242 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %q.2 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.20, %242), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:213:0
  %244 : int = aten::size(%x.21, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %245 : int = aten::size(%x.21, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %246 : int = aten::size(%x.21, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %247 : Long() = prim::NumToTensor(%246), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %248 : Long() = aten::floor_divide(%247, %13), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/tensor.py:424:0
  %249 : int = aten::Int(%248), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %250 : int[] = prim::ListConstruct(%244, %245, %12, %249), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %x.22 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.21, %250), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:209:0
  %252 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %k.2 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.22, %252), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:211:0
  %254 : int = aten::size(%x.23, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %255 : int = aten::size(%x.23, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %256 : int = aten::size(%x.23, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:208:0
  %257 : Long() = prim::NumToTensor(%256), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %258 : Long() = aten::floor_divide(%257, %13), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/tensor.py:424:0
  %259 : int = aten::Int(%258), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %260 : int[] = prim::ListConstruct(%254, %255, %12, %259), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %x.24 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.23, %260), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:209:0
  %262 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %v.2 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.24, %262), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:213:0
  %w.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.2, %k.2), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:178:0
  %w.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.5, %10), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:180:0
  %266 : int = aten::size(%w.6, %9), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %267 : int = aten::size(%w.6, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %268 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%218, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %269 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%268, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %270 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%269, %31, %34, %266, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %b.2 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%270, %11, %34, %267, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:183:0
  %272 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.6, %b.2), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:184:0
  %273 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.2, %33, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/tensor.py:396:0
  %274 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%273, %26), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:184:0
  %w.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%272, %274, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:184:0
  %input.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.7, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:188:0
  %input.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.10, %32, %28), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # torch/nn/functional.py:1498:0
  %w.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.11, %24, %29), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.25 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.8, %v.2), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:197:0
  %280 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %281 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.25, %280), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:203:0
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%281, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:203:0
  %283 : int = aten::size(%x.26, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:204:0
  %284 : int = aten::size(%x.26, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:204:0
  %285 : int = aten::size(%x.26, %9), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:204:0
  %286 : Long() = prim::NumToTensor(%285), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %287 : int = aten::size(%x.26, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:204:0
  %288 : Long() = prim::NumToTensor(%287), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %289 : Long() = aten::mul(%286, %288), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:204:0
  %290 : int = aten::Int(%289), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %291 : int[] = prim::ListConstruct(%283, %284, %290), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::view(%x.26, %291), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn # transformers/modeling_openai.py:205:0
  %293 : Tensor = prim::GetAttr[name="weight"](%217)
  %294 : Tensor = prim::GetAttr[name="bias"](%217)
  %295 : int = aten::size(%x.27, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1093:0
  %296 : int = aten::size(%x.27, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1093:0
  %297 : int = aten::size(%x.27, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1094:0
  %298 : int[] = prim::ListConstruct(%32, %297), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj
  %299 : Float(221:768, 768:1) = aten::view(%x.27, %298), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.28 : Float(221:768, 768:1) = aten::addmm(%294, %299, %293, %33, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1094:0
  %301 : int[] = prim::ListConstruct(%295, %296, %14), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj
  %input.12 : Float(17:9984, 13:768, 768:1) = aten::view(%x.28, %301), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.12, %24, %29), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.attn/__module.transformer.h.1.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add(%x.17, %a.2, %33), scope: __module.transformer/__module.transformer.h.1 # transformers/modeling_openai.py:266:0
  %305 : Tensor = prim::GetAttr[name="bias"](%215)
  %306 : Tensor = prim::GetAttr[name="weight"](%215)
  %307 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.ln_1
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.13, %307, %306, %305, %17, %16), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.ln_1 # torch/nn/functional.py:2048:0
  %309 : __torch__.transformers.modeling_utils.___torch_mangle_28907.Conv1D = prim::GetAttr[name="c_proj"](%214)
  %310 : __torch__.transformers.modeling_utils.___torch_mangle_28906.Conv1D = prim::GetAttr[name="c_fc"](%214)
  %311 : Tensor = prim::GetAttr[name="weight"](%310)
  %312 : Tensor = prim::GetAttr[name="bias"](%310)
  %313 : int = aten::size(%x.29, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %314 : int = aten::size(%x.29, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %315 : int = aten::size(%x.29, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %316 : int[] = prim::ListConstruct(%32, %315), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc
  %317 : Float(221:768, 768:1) = aten::view(%x.29, %316), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.30 : Float(221:3072, 3072:1) = aten::addmm(%312, %317, %311, %33, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %319 : int[] = prim::ListConstruct(%313, %314, %23), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc
  %x.31 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.30, %319), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %321 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.31, %22), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %322 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.31, %21), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %323 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%322, %20), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %324 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.31, %323, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %325 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%324, %19), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %326 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%325), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %327 : Float(17:39936, 13:3072, 3072:1) = aten::add(%326, %18, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %x.32 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%321, %327), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp # transformers/activations.py:30:0
  %329 : Tensor = prim::GetAttr[name="weight"](%309)
  %330 : Tensor = prim::GetAttr[name="bias"](%309)
  %331 : int = aten::size(%x.32, %34), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %332 : int = aten::size(%x.32, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %333 : int = aten::size(%x.32, %32), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %334 : int[] = prim::ListConstruct(%32, %333), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj
  %335 : Float(221:3072, 3072:1) = aten::view(%x.32, %334), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.33 : Float(221:768, 768:1) = aten::addmm(%330, %335, %329, %33, %33), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %337 : int[] = prim::ListConstruct(%331, %332, %14), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::view(%x.33, %337), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.14, %24, %29), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.mlp/__module.transformer.h.1.mlp.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::add(%x.29, %m.2, %33), scope: __module.transformer/__module.transformer.h.1 # transformers/modeling_openai.py:268:0
  %341 : Tensor = prim::GetAttr[name="bias"](%213)
  %342 : Tensor = prim::GetAttr[name="weight"](%213)
  %343 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.ln_2
  %x.34 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.15, %343, %342, %341, %17, %16), scope: __module.transformer/__module.transformer.h.1/__module.transformer.h.1.ln_2 # torch/nn/functional.py:2048:0
  %345 : __torch__.torch.nn.modules.normalization.___torch_mangle_28922.LayerNorm = prim::GetAttr[name="ln_2"](%54)
  %346 : __torch__.transformers.modeling_openai.___torch_mangle_28921.MLP = prim::GetAttr[name="mlp"](%54)
  %347 : __torch__.torch.nn.modules.normalization.___torch_mangle_28917.LayerNorm = prim::GetAttr[name="ln_1"](%54)
  %348 : __torch__.transformers.modeling_openai.___torch_mangle_28916.Attention = prim::GetAttr[name="attn"](%54)
  %349 : __torch__.transformers.modeling_utils.___torch_mangle_28913.Conv1D = prim::GetAttr[name="c_proj"](%348)
  %350 : Tensor = prim::GetAttr[name="bias"](%348)
  %351 : __torch__.transformers.modeling_utils.___torch_mangle_28912.Conv1D = prim::GetAttr[name="c_attn"](%348)
  %352 : Tensor = prim::GetAttr[name="weight"](%351)
  %353 : Tensor = prim::GetAttr[name="bias"](%351)
  %354 : int = aten::size(%x.34, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1093:0
  %355 : int = aten::size(%x.34, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1093:0
  %356 : int = aten::size(%x.34, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1094:0
  %357 : int[] = prim::ListConstruct(%32, %356), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn
  %358 : Float(221:768, 768:1) = aten::view(%x.34, %357), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.35 : Float(221:2304, 2304:1) = aten::addmm(%353, %358, %352, %33, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1094:0
  %360 : int[] = prim::ListConstruct(%354, %355, %15), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn
  %361 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.35, %360), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_attn # transformers/modeling_utils.py:1095:0
  %362 : Tensor[] = aten::split(%361, %14, %31), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/tensor.py:371:0
  %x.36 : Float(17:29952, 13:2304, 768:1), %x.38 : Float(17:29952, 13:2304, 768:1), %x.40 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%362), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %366 : int = aten::size(%x.36, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %367 : int = aten::size(%x.36, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %368 : int = aten::size(%x.36, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %369 : Long() = prim::NumToTensor(%368), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %370 : Long() = aten::floor_divide(%369, %13), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/tensor.py:424:0
  %371 : int = aten::Int(%370), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %372 : int[] = prim::ListConstruct(%366, %367, %12, %371), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %x.37 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.36, %372), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:209:0
  %374 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %q.3 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.37, %374), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:213:0
  %376 : int = aten::size(%x.38, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %377 : int = aten::size(%x.38, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %378 : int = aten::size(%x.38, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %379 : Long() = prim::NumToTensor(%378), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %380 : Long() = aten::floor_divide(%379, %13), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/tensor.py:424:0
  %381 : int = aten::Int(%380), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %382 : int[] = prim::ListConstruct(%376, %377, %12, %381), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %x.39 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.38, %382), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:209:0
  %384 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %k.3 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.39, %384), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:211:0
  %386 : int = aten::size(%x.40, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %387 : int = aten::size(%x.40, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %388 : int = aten::size(%x.40, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:208:0
  %389 : Long() = prim::NumToTensor(%388), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %390 : Long() = aten::floor_divide(%389, %13), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/tensor.py:424:0
  %391 : int = aten::Int(%390), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %392 : int[] = prim::ListConstruct(%386, %387, %12, %391), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %x.41 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.40, %392), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:209:0
  %394 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %v.3 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.41, %394), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:213:0
  %w.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.3, %k.3), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:178:0
  %w.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.9, %10), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:180:0
  %398 : int = aten::size(%w.10, %9), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %399 : int = aten::size(%w.10, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %400 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%350, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %401 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%400, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %402 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%401, %31, %34, %398, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %b.3 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%402, %11, %34, %399, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:183:0
  %404 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.10, %b.3), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:184:0
  %405 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.3, %33, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/tensor.py:396:0
  %406 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%405, %26), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:184:0
  %w.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%404, %406, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:184:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.11, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:188:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %32, %28), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # torch/nn/functional.py:1498:0
  %w.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %24, %29), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.42 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.12, %v.3), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:197:0
  %412 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %413 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.42, %412), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:203:0
  %x.43 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%413, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:203:0
  %415 : int = aten::size(%x.43, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:204:0
  %416 : int = aten::size(%x.43, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:204:0
  %417 : int = aten::size(%x.43, %9), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:204:0
  %418 : Long() = prim::NumToTensor(%417), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %419 : int = aten::size(%x.43, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:204:0
  %420 : Long() = prim::NumToTensor(%419), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %421 : Long() = aten::mul(%418, %420), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:204:0
  %422 : int = aten::Int(%421), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %423 : int[] = prim::ListConstruct(%415, %416, %422), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn
  %x.44 : Float(17:9984, 13:768, 768:1) = aten::view(%x.43, %423), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn # transformers/modeling_openai.py:205:0
  %425 : Tensor = prim::GetAttr[name="weight"](%349)
  %426 : Tensor = prim::GetAttr[name="bias"](%349)
  %427 : int = aten::size(%x.44, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1093:0
  %428 : int = aten::size(%x.44, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1093:0
  %429 : int = aten::size(%x.44, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1094:0
  %430 : int[] = prim::ListConstruct(%32, %429), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj
  %431 : Float(221:768, 768:1) = aten::view(%x.44, %430), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.45 : Float(221:768, 768:1) = aten::addmm(%426, %431, %425, %33, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1094:0
  %433 : int[] = prim::ListConstruct(%427, %428, %14), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%x.45, %433), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.18, %24, %29), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.attn/__module.transformer.h.2.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add(%x.34, %a.3, %33), scope: __module.transformer/__module.transformer.h.2 # transformers/modeling_openai.py:266:0
  %437 : Tensor = prim::GetAttr[name="bias"](%347)
  %438 : Tensor = prim::GetAttr[name="weight"](%347)
  %439 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.ln_1
  %x.46 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.19, %439, %438, %437, %17, %16), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.ln_1 # torch/nn/functional.py:2048:0
  %441 : __torch__.transformers.modeling_utils.___torch_mangle_28919.Conv1D = prim::GetAttr[name="c_proj"](%346)
  %442 : __torch__.transformers.modeling_utils.___torch_mangle_28918.Conv1D = prim::GetAttr[name="c_fc"](%346)
  %443 : Tensor = prim::GetAttr[name="weight"](%442)
  %444 : Tensor = prim::GetAttr[name="bias"](%442)
  %445 : int = aten::size(%x.46, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %446 : int = aten::size(%x.46, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %447 : int = aten::size(%x.46, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %448 : int[] = prim::ListConstruct(%32, %447), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc
  %449 : Float(221:768, 768:1) = aten::view(%x.46, %448), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.47 : Float(221:3072, 3072:1) = aten::addmm(%444, %449, %443, %33, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %451 : int[] = prim::ListConstruct(%445, %446, %23), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc
  %x.48 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.47, %451), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %453 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.48, %22), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %454 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.48, %21), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %455 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%454, %20), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %456 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.48, %455, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %457 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%456, %19), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %458 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%457), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %459 : Float(17:39936, 13:3072, 3072:1) = aten::add(%458, %18, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %x.49 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%453, %459), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp # transformers/activations.py:30:0
  %461 : Tensor = prim::GetAttr[name="weight"](%441)
  %462 : Tensor = prim::GetAttr[name="bias"](%441)
  %463 : int = aten::size(%x.49, %34), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %464 : int = aten::size(%x.49, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %465 : int = aten::size(%x.49, %32), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %466 : int[] = prim::ListConstruct(%32, %465), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj
  %467 : Float(221:3072, 3072:1) = aten::view(%x.49, %466), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.50 : Float(221:768, 768:1) = aten::addmm(%462, %467, %461, %33, %33), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %469 : int[] = prim::ListConstruct(%463, %464, %14), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::view(%x.50, %469), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.20, %24, %29), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.mlp/__module.transformer.h.2.mlp.dropout # torch/nn/functional.py:973:0
  %input.21 : Float(17:9984, 13:768, 768:1) = aten::add(%x.46, %m.3, %33), scope: __module.transformer/__module.transformer.h.2 # transformers/modeling_openai.py:268:0
  %473 : Tensor = prim::GetAttr[name="bias"](%345)
  %474 : Tensor = prim::GetAttr[name="weight"](%345)
  %475 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.ln_2
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.21, %475, %474, %473, %17, %16), scope: __module.transformer/__module.transformer.h.2/__module.transformer.h.2.ln_2 # torch/nn/functional.py:2048:0
  %477 : __torch__.torch.nn.modules.normalization.___torch_mangle_28934.LayerNorm = prim::GetAttr[name="ln_2"](%52)
  %478 : __torch__.transformers.modeling_openai.___torch_mangle_28933.MLP = prim::GetAttr[name="mlp"](%52)
  %479 : __torch__.torch.nn.modules.normalization.___torch_mangle_28929.LayerNorm = prim::GetAttr[name="ln_1"](%52)
  %480 : __torch__.transformers.modeling_openai.___torch_mangle_28928.Attention = prim::GetAttr[name="attn"](%52)
  %481 : __torch__.transformers.modeling_utils.___torch_mangle_28925.Conv1D = prim::GetAttr[name="c_proj"](%480)
  %482 : Tensor = prim::GetAttr[name="bias"](%480)
  %483 : __torch__.transformers.modeling_utils.___torch_mangle_28924.Conv1D = prim::GetAttr[name="c_attn"](%480)
  %484 : Tensor = prim::GetAttr[name="weight"](%483)
  %485 : Tensor = prim::GetAttr[name="bias"](%483)
  %486 : int = aten::size(%x.51, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1093:0
  %487 : int = aten::size(%x.51, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1093:0
  %488 : int = aten::size(%x.51, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1094:0
  %489 : int[] = prim::ListConstruct(%32, %488), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn
  %490 : Float(221:768, 768:1) = aten::view(%x.51, %489), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.52 : Float(221:2304, 2304:1) = aten::addmm(%485, %490, %484, %33, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1094:0
  %492 : int[] = prim::ListConstruct(%486, %487, %15), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn
  %493 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.52, %492), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_attn # transformers/modeling_utils.py:1095:0
  %494 : Tensor[] = aten::split(%493, %14, %31), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/tensor.py:371:0
  %x.53 : Float(17:29952, 13:2304, 768:1), %x.55 : Float(17:29952, 13:2304, 768:1), %x.57 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%494), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %498 : int = aten::size(%x.53, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %499 : int = aten::size(%x.53, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %500 : int = aten::size(%x.53, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %501 : Long() = prim::NumToTensor(%500), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %502 : Long() = aten::floor_divide(%501, %13), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/tensor.py:424:0
  %503 : int = aten::Int(%502), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %504 : int[] = prim::ListConstruct(%498, %499, %12, %503), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %x.54 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.53, %504), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:209:0
  %506 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %q.4 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.54, %506), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:213:0
  %508 : int = aten::size(%x.55, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %509 : int = aten::size(%x.55, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %510 : int = aten::size(%x.55, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %511 : Long() = prim::NumToTensor(%510), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %512 : Long() = aten::floor_divide(%511, %13), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/tensor.py:424:0
  %513 : int = aten::Int(%512), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %514 : int[] = prim::ListConstruct(%508, %509, %12, %513), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %x.56 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.55, %514), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:209:0
  %516 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %k.4 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.56, %516), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:211:0
  %518 : int = aten::size(%x.57, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %519 : int = aten::size(%x.57, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %520 : int = aten::size(%x.57, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:208:0
  %521 : Long() = prim::NumToTensor(%520), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %522 : Long() = aten::floor_divide(%521, %13), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/tensor.py:424:0
  %523 : int = aten::Int(%522), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %524 : int[] = prim::ListConstruct(%518, %519, %12, %523), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %x.58 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.57, %524), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:209:0
  %526 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %v.4 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.58, %526), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:213:0
  %w.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.4, %k.4), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:178:0
  %w.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.13, %10), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:180:0
  %530 : int = aten::size(%w.14, %9), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %531 : int = aten::size(%w.14, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %532 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%482, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %533 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%532, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %534 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%533, %31, %34, %530, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %b.4 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%534, %11, %34, %531, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:183:0
  %536 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.14, %b.4), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:184:0
  %537 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.4, %33, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/tensor.py:396:0
  %538 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%537, %26), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:184:0
  %w.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%536, %538, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:184:0
  %input.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.15, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:188:0
  %input.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.22, %32, %28), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # torch/nn/functional.py:1498:0
  %w.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.23, %24, %29), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.59 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.16, %v.4), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:197:0
  %544 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %545 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.59, %544), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:203:0
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%545, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:203:0
  %547 : int = aten::size(%x.60, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:204:0
  %548 : int = aten::size(%x.60, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:204:0
  %549 : int = aten::size(%x.60, %9), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:204:0
  %550 : Long() = prim::NumToTensor(%549), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %551 : int = aten::size(%x.60, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:204:0
  %552 : Long() = prim::NumToTensor(%551), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %553 : Long() = aten::mul(%550, %552), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:204:0
  %554 : int = aten::Int(%553), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %555 : int[] = prim::ListConstruct(%547, %548, %554), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::view(%x.60, %555), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn # transformers/modeling_openai.py:205:0
  %557 : Tensor = prim::GetAttr[name="weight"](%481)
  %558 : Tensor = prim::GetAttr[name="bias"](%481)
  %559 : int = aten::size(%x.61, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1093:0
  %560 : int = aten::size(%x.61, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1093:0
  %561 : int = aten::size(%x.61, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1094:0
  %562 : int[] = prim::ListConstruct(%32, %561), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj
  %563 : Float(221:768, 768:1) = aten::view(%x.61, %562), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.62 : Float(221:768, 768:1) = aten::addmm(%558, %563, %557, %33, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1094:0
  %565 : int[] = prim::ListConstruct(%559, %560, %14), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::view(%x.62, %565), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.24, %24, %29), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.attn/__module.transformer.h.3.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::add(%x.51, %a.4, %33), scope: __module.transformer/__module.transformer.h.3 # transformers/modeling_openai.py:266:0
  %569 : Tensor = prim::GetAttr[name="bias"](%479)
  %570 : Tensor = prim::GetAttr[name="weight"](%479)
  %571 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.ln_1
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.25, %571, %570, %569, %17, %16), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.ln_1 # torch/nn/functional.py:2048:0
  %573 : __torch__.transformers.modeling_utils.___torch_mangle_28931.Conv1D = prim::GetAttr[name="c_proj"](%478)
  %574 : __torch__.transformers.modeling_utils.___torch_mangle_28930.Conv1D = prim::GetAttr[name="c_fc"](%478)
  %575 : Tensor = prim::GetAttr[name="weight"](%574)
  %576 : Tensor = prim::GetAttr[name="bias"](%574)
  %577 : int = aten::size(%x.63, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %578 : int = aten::size(%x.63, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %579 : int = aten::size(%x.63, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %580 : int[] = prim::ListConstruct(%32, %579), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc
  %581 : Float(221:768, 768:1) = aten::view(%x.63, %580), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.64 : Float(221:3072, 3072:1) = aten::addmm(%576, %581, %575, %33, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %583 : int[] = prim::ListConstruct(%577, %578, %23), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc
  %x.65 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.64, %583), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %585 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.65, %22), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %586 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.65, %21), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %587 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%586, %20), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %588 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.65, %587, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %589 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%588, %19), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %590 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%589), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %591 : Float(17:39936, 13:3072, 3072:1) = aten::add(%590, %18, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %x.66 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%585, %591), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp # transformers/activations.py:30:0
  %593 : Tensor = prim::GetAttr[name="weight"](%573)
  %594 : Tensor = prim::GetAttr[name="bias"](%573)
  %595 : int = aten::size(%x.66, %34), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %596 : int = aten::size(%x.66, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %597 : int = aten::size(%x.66, %32), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %598 : int[] = prim::ListConstruct(%32, %597), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj
  %599 : Float(221:3072, 3072:1) = aten::view(%x.66, %598), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.67 : Float(221:768, 768:1) = aten::addmm(%594, %599, %593, %33, %33), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %601 : int[] = prim::ListConstruct(%595, %596, %14), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj
  %input.26 : Float(17:9984, 13:768, 768:1) = aten::view(%x.67, %601), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.26, %24, %29), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.mlp/__module.transformer.h.3.mlp.dropout # torch/nn/functional.py:973:0
  %input.27 : Float(17:9984, 13:768, 768:1) = aten::add(%x.63, %m.4, %33), scope: __module.transformer/__module.transformer.h.3 # transformers/modeling_openai.py:268:0
  %605 : Tensor = prim::GetAttr[name="bias"](%477)
  %606 : Tensor = prim::GetAttr[name="weight"](%477)
  %607 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.ln_2
  %x.68 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.27, %607, %606, %605, %17, %16), scope: __module.transformer/__module.transformer.h.3/__module.transformer.h.3.ln_2 # torch/nn/functional.py:2048:0
  %609 : __torch__.torch.nn.modules.normalization.___torch_mangle_28946.LayerNorm = prim::GetAttr[name="ln_2"](%50)
  %610 : __torch__.transformers.modeling_openai.___torch_mangle_28945.MLP = prim::GetAttr[name="mlp"](%50)
  %611 : __torch__.torch.nn.modules.normalization.___torch_mangle_28941.LayerNorm = prim::GetAttr[name="ln_1"](%50)
  %612 : __torch__.transformers.modeling_openai.___torch_mangle_28940.Attention = prim::GetAttr[name="attn"](%50)
  %613 : __torch__.transformers.modeling_utils.___torch_mangle_28937.Conv1D = prim::GetAttr[name="c_proj"](%612)
  %614 : Tensor = prim::GetAttr[name="bias"](%612)
  %615 : __torch__.transformers.modeling_utils.___torch_mangle_28936.Conv1D = prim::GetAttr[name="c_attn"](%612)
  %616 : Tensor = prim::GetAttr[name="weight"](%615)
  %617 : Tensor = prim::GetAttr[name="bias"](%615)
  %618 : int = aten::size(%x.68, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1093:0
  %619 : int = aten::size(%x.68, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1093:0
  %620 : int = aten::size(%x.68, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1094:0
  %621 : int[] = prim::ListConstruct(%32, %620), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn
  %622 : Float(221:768, 768:1) = aten::view(%x.68, %621), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.69 : Float(221:2304, 2304:1) = aten::addmm(%617, %622, %616, %33, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1094:0
  %624 : int[] = prim::ListConstruct(%618, %619, %15), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn
  %625 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.69, %624), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_attn # transformers/modeling_utils.py:1095:0
  %626 : Tensor[] = aten::split(%625, %14, %31), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/tensor.py:371:0
  %x.70 : Float(17:29952, 13:2304, 768:1), %x.72 : Float(17:29952, 13:2304, 768:1), %x.74 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%626), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %630 : int = aten::size(%x.70, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %631 : int = aten::size(%x.70, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %632 : int = aten::size(%x.70, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %633 : Long() = prim::NumToTensor(%632), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %634 : Long() = aten::floor_divide(%633, %13), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/tensor.py:424:0
  %635 : int = aten::Int(%634), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %636 : int[] = prim::ListConstruct(%630, %631, %12, %635), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %x.71 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.70, %636), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:209:0
  %638 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %q.5 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.71, %638), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:213:0
  %640 : int = aten::size(%x.72, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %641 : int = aten::size(%x.72, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %642 : int = aten::size(%x.72, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %643 : Long() = prim::NumToTensor(%642), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %644 : Long() = aten::floor_divide(%643, %13), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/tensor.py:424:0
  %645 : int = aten::Int(%644), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %646 : int[] = prim::ListConstruct(%640, %641, %12, %645), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %x.73 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.72, %646), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:209:0
  %648 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %k.5 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.73, %648), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:211:0
  %650 : int = aten::size(%x.74, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %651 : int = aten::size(%x.74, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %652 : int = aten::size(%x.74, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:208:0
  %653 : Long() = prim::NumToTensor(%652), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %654 : Long() = aten::floor_divide(%653, %13), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/tensor.py:424:0
  %655 : int = aten::Int(%654), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %656 : int[] = prim::ListConstruct(%650, %651, %12, %655), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %x.75 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.74, %656), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:209:0
  %658 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %v.5 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.75, %658), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:213:0
  %w.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.5, %k.5), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:178:0
  %w.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.17, %10), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:180:0
  %662 : int = aten::size(%w.18, %9), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %663 : int = aten::size(%w.18, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %664 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%614, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %665 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%664, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %666 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%665, %31, %34, %662, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %b.5 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%666, %11, %34, %663, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:183:0
  %668 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.18, %b.5), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:184:0
  %669 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.5, %33, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/tensor.py:396:0
  %670 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%669, %26), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:184:0
  %w.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%668, %670, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:184:0
  %input.28 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.19, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:188:0
  %input.29 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.28, %32, %28), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # torch/nn/functional.py:1498:0
  %w.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.29, %24, %29), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.76 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.20, %v.5), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:197:0
  %676 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %677 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.76, %676), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:203:0
  %x.77 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%677, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:203:0
  %679 : int = aten::size(%x.77, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:204:0
  %680 : int = aten::size(%x.77, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:204:0
  %681 : int = aten::size(%x.77, %9), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:204:0
  %682 : Long() = prim::NumToTensor(%681), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %683 : int = aten::size(%x.77, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:204:0
  %684 : Long() = prim::NumToTensor(%683), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %685 : Long() = aten::mul(%682, %684), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:204:0
  %686 : int = aten::Int(%685), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %687 : int[] = prim::ListConstruct(%679, %680, %686), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn
  %x.78 : Float(17:9984, 13:768, 768:1) = aten::view(%x.77, %687), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn # transformers/modeling_openai.py:205:0
  %689 : Tensor = prim::GetAttr[name="weight"](%613)
  %690 : Tensor = prim::GetAttr[name="bias"](%613)
  %691 : int = aten::size(%x.78, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1093:0
  %692 : int = aten::size(%x.78, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1093:0
  %693 : int = aten::size(%x.78, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1094:0
  %694 : int[] = prim::ListConstruct(%32, %693), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj
  %695 : Float(221:768, 768:1) = aten::view(%x.78, %694), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.79 : Float(221:768, 768:1) = aten::addmm(%690, %695, %689, %33, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1094:0
  %697 : int[] = prim::ListConstruct(%691, %692, %14), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::view(%x.79, %697), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.30, %24, %29), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.attn/__module.transformer.h.4.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:9984, 13:768, 768:1) = aten::add(%x.68, %a.5, %33), scope: __module.transformer/__module.transformer.h.4 # transformers/modeling_openai.py:266:0
  %701 : Tensor = prim::GetAttr[name="bias"](%611)
  %702 : Tensor = prim::GetAttr[name="weight"](%611)
  %703 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.ln_1
  %x.80 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.31, %703, %702, %701, %17, %16), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.ln_1 # torch/nn/functional.py:2048:0
  %705 : __torch__.transformers.modeling_utils.___torch_mangle_28943.Conv1D = prim::GetAttr[name="c_proj"](%610)
  %706 : __torch__.transformers.modeling_utils.___torch_mangle_28942.Conv1D = prim::GetAttr[name="c_fc"](%610)
  %707 : Tensor = prim::GetAttr[name="weight"](%706)
  %708 : Tensor = prim::GetAttr[name="bias"](%706)
  %709 : int = aten::size(%x.80, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %710 : int = aten::size(%x.80, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %711 : int = aten::size(%x.80, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %712 : int[] = prim::ListConstruct(%32, %711), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc
  %713 : Float(221:768, 768:1) = aten::view(%x.80, %712), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.81 : Float(221:3072, 3072:1) = aten::addmm(%708, %713, %707, %33, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %715 : int[] = prim::ListConstruct(%709, %710, %23), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc
  %x.82 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.81, %715), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %717 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.82, %22), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %718 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.82, %21), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %719 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%718, %20), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %720 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.82, %719, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %721 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%720, %19), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %722 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%721), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %723 : Float(17:39936, 13:3072, 3072:1) = aten::add(%722, %18, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %x.83 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%717, %723), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp # transformers/activations.py:30:0
  %725 : Tensor = prim::GetAttr[name="weight"](%705)
  %726 : Tensor = prim::GetAttr[name="bias"](%705)
  %727 : int = aten::size(%x.83, %34), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %728 : int = aten::size(%x.83, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %729 : int = aten::size(%x.83, %32), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %730 : int[] = prim::ListConstruct(%32, %729), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj
  %731 : Float(221:3072, 3072:1) = aten::view(%x.83, %730), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.84 : Float(221:768, 768:1) = aten::addmm(%726, %731, %725, %33, %33), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %733 : int[] = prim::ListConstruct(%727, %728, %14), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj
  %input.32 : Float(17:9984, 13:768, 768:1) = aten::view(%x.84, %733), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.32, %24, %29), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.mlp/__module.transformer.h.4.mlp.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add(%x.80, %m.5, %33), scope: __module.transformer/__module.transformer.h.4 # transformers/modeling_openai.py:268:0
  %737 : Tensor = prim::GetAttr[name="bias"](%609)
  %738 : Tensor = prim::GetAttr[name="weight"](%609)
  %739 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.ln_2
  %x.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.33, %739, %738, %737, %17, %16), scope: __module.transformer/__module.transformer.h.4/__module.transformer.h.4.ln_2 # torch/nn/functional.py:2048:0
  %741 : __torch__.torch.nn.modules.normalization.___torch_mangle_28958.LayerNorm = prim::GetAttr[name="ln_2"](%48)
  %742 : __torch__.transformers.modeling_openai.___torch_mangle_28957.MLP = prim::GetAttr[name="mlp"](%48)
  %743 : __torch__.torch.nn.modules.normalization.___torch_mangle_28953.LayerNorm = prim::GetAttr[name="ln_1"](%48)
  %744 : __torch__.transformers.modeling_openai.___torch_mangle_28952.Attention = prim::GetAttr[name="attn"](%48)
  %745 : __torch__.transformers.modeling_utils.___torch_mangle_28949.Conv1D = prim::GetAttr[name="c_proj"](%744)
  %746 : Tensor = prim::GetAttr[name="bias"](%744)
  %747 : __torch__.transformers.modeling_utils.___torch_mangle_28948.Conv1D = prim::GetAttr[name="c_attn"](%744)
  %748 : Tensor = prim::GetAttr[name="weight"](%747)
  %749 : Tensor = prim::GetAttr[name="bias"](%747)
  %750 : int = aten::size(%x.85, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1093:0
  %751 : int = aten::size(%x.85, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1093:0
  %752 : int = aten::size(%x.85, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1094:0
  %753 : int[] = prim::ListConstruct(%32, %752), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn
  %754 : Float(221:768, 768:1) = aten::view(%x.85, %753), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.86 : Float(221:2304, 2304:1) = aten::addmm(%749, %754, %748, %33, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1094:0
  %756 : int[] = prim::ListConstruct(%750, %751, %15), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn
  %757 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.86, %756), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_attn # transformers/modeling_utils.py:1095:0
  %758 : Tensor[] = aten::split(%757, %14, %31), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/tensor.py:371:0
  %x.87 : Float(17:29952, 13:2304, 768:1), %x.89 : Float(17:29952, 13:2304, 768:1), %x.91 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%758), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %762 : int = aten::size(%x.87, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %763 : int = aten::size(%x.87, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %764 : int = aten::size(%x.87, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %765 : Long() = prim::NumToTensor(%764), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %766 : Long() = aten::floor_divide(%765, %13), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/tensor.py:424:0
  %767 : int = aten::Int(%766), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %768 : int[] = prim::ListConstruct(%762, %763, %12, %767), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %x.88 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.87, %768), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:209:0
  %770 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %q.6 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.88, %770), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:213:0
  %772 : int = aten::size(%x.89, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %773 : int = aten::size(%x.89, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %774 : int = aten::size(%x.89, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %775 : Long() = prim::NumToTensor(%774), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %776 : Long() = aten::floor_divide(%775, %13), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/tensor.py:424:0
  %777 : int = aten::Int(%776), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %778 : int[] = prim::ListConstruct(%772, %773, %12, %777), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %x.90 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.89, %778), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:209:0
  %780 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %k.6 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.90, %780), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:211:0
  %782 : int = aten::size(%x.91, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %783 : int = aten::size(%x.91, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %784 : int = aten::size(%x.91, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:208:0
  %785 : Long() = prim::NumToTensor(%784), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %786 : Long() = aten::floor_divide(%785, %13), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/tensor.py:424:0
  %787 : int = aten::Int(%786), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %788 : int[] = prim::ListConstruct(%782, %783, %12, %787), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %x.92 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.91, %788), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:209:0
  %790 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %v.6 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.92, %790), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:213:0
  %w.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.6, %k.6), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:178:0
  %w.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.21, %10), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:180:0
  %794 : int = aten::size(%w.22, %9), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %795 : int = aten::size(%w.22, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %796 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%746, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %797 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%796, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %798 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%797, %31, %34, %794, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %b.6 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%798, %11, %34, %795, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:183:0
  %800 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.22, %b.6), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:184:0
  %801 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.6, %33, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/tensor.py:396:0
  %802 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%801, %26), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:184:0
  %w.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%800, %802, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:184:0
  %input.34 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.23, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:188:0
  %input.35 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.34, %32, %28), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # torch/nn/functional.py:1498:0
  %w.24 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.35, %24, %29), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.93 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.24, %v.6), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:197:0
  %808 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %809 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.93, %808), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:203:0
  %x.94 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%809, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:203:0
  %811 : int = aten::size(%x.94, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:204:0
  %812 : int = aten::size(%x.94, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:204:0
  %813 : int = aten::size(%x.94, %9), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:204:0
  %814 : Long() = prim::NumToTensor(%813), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %815 : int = aten::size(%x.94, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:204:0
  %816 : Long() = prim::NumToTensor(%815), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %817 : Long() = aten::mul(%814, %816), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:204:0
  %818 : int = aten::Int(%817), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %819 : int[] = prim::ListConstruct(%811, %812, %818), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn
  %x.95 : Float(17:9984, 13:768, 768:1) = aten::view(%x.94, %819), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn # transformers/modeling_openai.py:205:0
  %821 : Tensor = prim::GetAttr[name="weight"](%745)
  %822 : Tensor = prim::GetAttr[name="bias"](%745)
  %823 : int = aten::size(%x.95, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1093:0
  %824 : int = aten::size(%x.95, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1093:0
  %825 : int = aten::size(%x.95, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1094:0
  %826 : int[] = prim::ListConstruct(%32, %825), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj
  %827 : Float(221:768, 768:1) = aten::view(%x.95, %826), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.96 : Float(221:768, 768:1) = aten::addmm(%822, %827, %821, %33, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1094:0
  %829 : int[] = prim::ListConstruct(%823, %824, %14), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj
  %input.36 : Float(17:9984, 13:768, 768:1) = aten::view(%x.96, %829), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.36, %24, %29), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.attn/__module.transformer.h.5.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.37 : Float(17:9984, 13:768, 768:1) = aten::add(%x.85, %a.6, %33), scope: __module.transformer/__module.transformer.h.5 # transformers/modeling_openai.py:266:0
  %833 : Tensor = prim::GetAttr[name="bias"](%743)
  %834 : Tensor = prim::GetAttr[name="weight"](%743)
  %835 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.ln_1
  %x.97 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.37, %835, %834, %833, %17, %16), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.ln_1 # torch/nn/functional.py:2048:0
  %837 : __torch__.transformers.modeling_utils.___torch_mangle_28955.Conv1D = prim::GetAttr[name="c_proj"](%742)
  %838 : __torch__.transformers.modeling_utils.___torch_mangle_28954.Conv1D = prim::GetAttr[name="c_fc"](%742)
  %839 : Tensor = prim::GetAttr[name="weight"](%838)
  %840 : Tensor = prim::GetAttr[name="bias"](%838)
  %841 : int = aten::size(%x.97, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %842 : int = aten::size(%x.97, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %843 : int = aten::size(%x.97, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %844 : int[] = prim::ListConstruct(%32, %843), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc
  %845 : Float(221:768, 768:1) = aten::view(%x.97, %844), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.98 : Float(221:3072, 3072:1) = aten::addmm(%840, %845, %839, %33, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %847 : int[] = prim::ListConstruct(%841, %842, %23), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc
  %x.99 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.98, %847), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %849 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.99, %22), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %850 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.99, %21), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %851 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%850, %20), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %852 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.99, %851, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %853 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%852, %19), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %854 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%853), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %855 : Float(17:39936, 13:3072, 3072:1) = aten::add(%854, %18, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %x.100 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%849, %855), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp # transformers/activations.py:30:0
  %857 : Tensor = prim::GetAttr[name="weight"](%837)
  %858 : Tensor = prim::GetAttr[name="bias"](%837)
  %859 : int = aten::size(%x.100, %34), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %860 : int = aten::size(%x.100, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %861 : int = aten::size(%x.100, %32), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %862 : int[] = prim::ListConstruct(%32, %861), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj
  %863 : Float(221:3072, 3072:1) = aten::view(%x.100, %862), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.101 : Float(221:768, 768:1) = aten::addmm(%858, %863, %857, %33, %33), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %865 : int[] = prim::ListConstruct(%859, %860, %14), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%x.101, %865), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.38, %24, %29), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.mlp/__module.transformer.h.5.mlp.dropout # torch/nn/functional.py:973:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add(%x.97, %m.6, %33), scope: __module.transformer/__module.transformer.h.5 # transformers/modeling_openai.py:268:0
  %869 : Tensor = prim::GetAttr[name="bias"](%741)
  %870 : Tensor = prim::GetAttr[name="weight"](%741)
  %871 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.ln_2
  %x.102 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.39, %871, %870, %869, %17, %16), scope: __module.transformer/__module.transformer.h.5/__module.transformer.h.5.ln_2 # torch/nn/functional.py:2048:0
  %873 : __torch__.torch.nn.modules.normalization.___torch_mangle_28970.LayerNorm = prim::GetAttr[name="ln_2"](%46)
  %874 : __torch__.transformers.modeling_openai.___torch_mangle_28969.MLP = prim::GetAttr[name="mlp"](%46)
  %875 : __torch__.torch.nn.modules.normalization.___torch_mangle_28965.LayerNorm = prim::GetAttr[name="ln_1"](%46)
  %876 : __torch__.transformers.modeling_openai.___torch_mangle_28964.Attention = prim::GetAttr[name="attn"](%46)
  %877 : __torch__.transformers.modeling_utils.___torch_mangle_28961.Conv1D = prim::GetAttr[name="c_proj"](%876)
  %878 : Tensor = prim::GetAttr[name="bias"](%876)
  %879 : __torch__.transformers.modeling_utils.___torch_mangle_28960.Conv1D = prim::GetAttr[name="c_attn"](%876)
  %880 : Tensor = prim::GetAttr[name="weight"](%879)
  %881 : Tensor = prim::GetAttr[name="bias"](%879)
  %882 : int = aten::size(%x.102, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1093:0
  %883 : int = aten::size(%x.102, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1093:0
  %884 : int = aten::size(%x.102, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1094:0
  %885 : int[] = prim::ListConstruct(%32, %884), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn
  %886 : Float(221:768, 768:1) = aten::view(%x.102, %885), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.103 : Float(221:2304, 2304:1) = aten::addmm(%881, %886, %880, %33, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1094:0
  %888 : int[] = prim::ListConstruct(%882, %883, %15), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn
  %889 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.103, %888), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_attn # transformers/modeling_utils.py:1095:0
  %890 : Tensor[] = aten::split(%889, %14, %31), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/tensor.py:371:0
  %x.104 : Float(17:29952, 13:2304, 768:1), %x.106 : Float(17:29952, 13:2304, 768:1), %x.108 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%890), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %894 : int = aten::size(%x.104, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %895 : int = aten::size(%x.104, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %896 : int = aten::size(%x.104, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %897 : Long() = prim::NumToTensor(%896), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %898 : Long() = aten::floor_divide(%897, %13), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/tensor.py:424:0
  %899 : int = aten::Int(%898), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %900 : int[] = prim::ListConstruct(%894, %895, %12, %899), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %x.105 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.104, %900), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:209:0
  %902 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %q.7 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.105, %902), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:213:0
  %904 : int = aten::size(%x.106, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %905 : int = aten::size(%x.106, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %906 : int = aten::size(%x.106, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %907 : Long() = prim::NumToTensor(%906), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %908 : Long() = aten::floor_divide(%907, %13), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/tensor.py:424:0
  %909 : int = aten::Int(%908), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %910 : int[] = prim::ListConstruct(%904, %905, %12, %909), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %x.107 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.106, %910), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:209:0
  %912 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %k.7 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.107, %912), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:211:0
  %914 : int = aten::size(%x.108, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %915 : int = aten::size(%x.108, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %916 : int = aten::size(%x.108, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:208:0
  %917 : Long() = prim::NumToTensor(%916), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %918 : Long() = aten::floor_divide(%917, %13), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/tensor.py:424:0
  %919 : int = aten::Int(%918), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %920 : int[] = prim::ListConstruct(%914, %915, %12, %919), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %x.109 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.108, %920), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:209:0
  %922 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %v.7 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.109, %922), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:213:0
  %w.25 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.7, %k.7), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:178:0
  %w.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.25, %10), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:180:0
  %926 : int = aten::size(%w.26, %9), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %927 : int = aten::size(%w.26, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %928 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%878, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %929 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%928, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %930 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%929, %31, %34, %926, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %b.7 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%930, %11, %34, %927, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:183:0
  %932 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.26, %b.7), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:184:0
  %933 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.7, %33, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/tensor.py:396:0
  %934 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%933, %26), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:184:0
  %w.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%932, %934, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:184:0
  %input.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.27, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:188:0
  %input.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.40, %32, %28), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # torch/nn/functional.py:1498:0
  %w.28 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.41, %24, %29), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.110 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.28, %v.7), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:197:0
  %940 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %941 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.110, %940), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:203:0
  %x.111 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%941, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:203:0
  %943 : int = aten::size(%x.111, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:204:0
  %944 : int = aten::size(%x.111, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:204:0
  %945 : int = aten::size(%x.111, %9), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:204:0
  %946 : Long() = prim::NumToTensor(%945), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %947 : int = aten::size(%x.111, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:204:0
  %948 : Long() = prim::NumToTensor(%947), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %949 : Long() = aten::mul(%946, %948), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:204:0
  %950 : int = aten::Int(%949), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %951 : int[] = prim::ListConstruct(%943, %944, %950), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn
  %x.112 : Float(17:9984, 13:768, 768:1) = aten::view(%x.111, %951), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn # transformers/modeling_openai.py:205:0
  %953 : Tensor = prim::GetAttr[name="weight"](%877)
  %954 : Tensor = prim::GetAttr[name="bias"](%877)
  %955 : int = aten::size(%x.112, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1093:0
  %956 : int = aten::size(%x.112, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1093:0
  %957 : int = aten::size(%x.112, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1094:0
  %958 : int[] = prim::ListConstruct(%32, %957), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj
  %959 : Float(221:768, 768:1) = aten::view(%x.112, %958), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.113 : Float(221:768, 768:1) = aten::addmm(%954, %959, %953, %33, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1094:0
  %961 : int[] = prim::ListConstruct(%955, %956, %14), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj
  %input.42 : Float(17:9984, 13:768, 768:1) = aten::view(%x.113, %961), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.42, %24, %29), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.attn/__module.transformer.h.6.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add(%x.102, %a.7, %33), scope: __module.transformer/__module.transformer.h.6 # transformers/modeling_openai.py:266:0
  %965 : Tensor = prim::GetAttr[name="bias"](%875)
  %966 : Tensor = prim::GetAttr[name="weight"](%875)
  %967 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.ln_1
  %x.114 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.43, %967, %966, %965, %17, %16), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.ln_1 # torch/nn/functional.py:2048:0
  %969 : __torch__.transformers.modeling_utils.___torch_mangle_28967.Conv1D = prim::GetAttr[name="c_proj"](%874)
  %970 : __torch__.transformers.modeling_utils.___torch_mangle_28966.Conv1D = prim::GetAttr[name="c_fc"](%874)
  %971 : Tensor = prim::GetAttr[name="weight"](%970)
  %972 : Tensor = prim::GetAttr[name="bias"](%970)
  %973 : int = aten::size(%x.114, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %974 : int = aten::size(%x.114, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %975 : int = aten::size(%x.114, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %976 : int[] = prim::ListConstruct(%32, %975), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc
  %977 : Float(221:768, 768:1) = aten::view(%x.114, %976), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.115 : Float(221:3072, 3072:1) = aten::addmm(%972, %977, %971, %33, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %979 : int[] = prim::ListConstruct(%973, %974, %23), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc
  %x.116 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.115, %979), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %981 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.116, %22), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %982 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.116, %21), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %983 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%982, %20), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %984 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.116, %983, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %985 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%984, %19), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %986 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%985), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %987 : Float(17:39936, 13:3072, 3072:1) = aten::add(%986, %18, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %x.117 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%981, %987), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp # transformers/activations.py:30:0
  %989 : Tensor = prim::GetAttr[name="weight"](%969)
  %990 : Tensor = prim::GetAttr[name="bias"](%969)
  %991 : int = aten::size(%x.117, %34), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %992 : int = aten::size(%x.117, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %993 : int = aten::size(%x.117, %32), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %994 : int[] = prim::ListConstruct(%32, %993), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj
  %995 : Float(221:3072, 3072:1) = aten::view(%x.117, %994), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.118 : Float(221:768, 768:1) = aten::addmm(%990, %995, %989, %33, %33), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %997 : int[] = prim::ListConstruct(%991, %992, %14), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::view(%x.118, %997), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.44, %24, %29), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.mlp/__module.transformer.h.6.mlp.dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::add(%x.114, %m.7, %33), scope: __module.transformer/__module.transformer.h.6 # transformers/modeling_openai.py:268:0
  %1001 : Tensor = prim::GetAttr[name="bias"](%873)
  %1002 : Tensor = prim::GetAttr[name="weight"](%873)
  %1003 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.ln_2
  %x.119 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.45, %1003, %1002, %1001, %17, %16), scope: __module.transformer/__module.transformer.h.6/__module.transformer.h.6.ln_2 # torch/nn/functional.py:2048:0
  %1005 : __torch__.torch.nn.modules.normalization.___torch_mangle_28982.LayerNorm = prim::GetAttr[name="ln_2"](%44)
  %1006 : __torch__.transformers.modeling_openai.___torch_mangle_28981.MLP = prim::GetAttr[name="mlp"](%44)
  %1007 : __torch__.torch.nn.modules.normalization.___torch_mangle_28977.LayerNorm = prim::GetAttr[name="ln_1"](%44)
  %1008 : __torch__.transformers.modeling_openai.___torch_mangle_28976.Attention = prim::GetAttr[name="attn"](%44)
  %1009 : __torch__.transformers.modeling_utils.___torch_mangle_28973.Conv1D = prim::GetAttr[name="c_proj"](%1008)
  %1010 : Tensor = prim::GetAttr[name="bias"](%1008)
  %1011 : __torch__.transformers.modeling_utils.___torch_mangle_28972.Conv1D = prim::GetAttr[name="c_attn"](%1008)
  %1012 : Tensor = prim::GetAttr[name="weight"](%1011)
  %1013 : Tensor = prim::GetAttr[name="bias"](%1011)
  %1014 : int = aten::size(%x.119, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1015 : int = aten::size(%x.119, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1016 : int = aten::size(%x.119, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1017 : int[] = prim::ListConstruct(%32, %1016), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn
  %1018 : Float(221:768, 768:1) = aten::view(%x.119, %1017), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.120 : Float(221:2304, 2304:1) = aten::addmm(%1013, %1018, %1012, %33, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1020 : int[] = prim::ListConstruct(%1014, %1015, %15), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn
  %1021 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.120, %1020), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_attn # transformers/modeling_utils.py:1095:0
  %1022 : Tensor[] = aten::split(%1021, %14, %31), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/tensor.py:371:0
  %x.121 : Float(17:29952, 13:2304, 768:1), %x.123 : Float(17:29952, 13:2304, 768:1), %x.125 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%1022), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1026 : int = aten::size(%x.121, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1027 : int = aten::size(%x.121, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1028 : int = aten::size(%x.121, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1029 : Long() = prim::NumToTensor(%1028), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1030 : Long() = aten::floor_divide(%1029, %13), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/tensor.py:424:0
  %1031 : int = aten::Int(%1030), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1032 : int[] = prim::ListConstruct(%1026, %1027, %12, %1031), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %x.122 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.121, %1032), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:209:0
  %1034 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %q.8 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.122, %1034), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:213:0
  %1036 : int = aten::size(%x.123, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1037 : int = aten::size(%x.123, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1038 : int = aten::size(%x.123, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1039 : Long() = prim::NumToTensor(%1038), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1040 : Long() = aten::floor_divide(%1039, %13), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/tensor.py:424:0
  %1041 : int = aten::Int(%1040), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1042 : int[] = prim::ListConstruct(%1036, %1037, %12, %1041), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %x.124 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.123, %1042), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:209:0
  %1044 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %k.8 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.124, %1044), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:211:0
  %1046 : int = aten::size(%x.125, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1047 : int = aten::size(%x.125, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1048 : int = aten::size(%x.125, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:208:0
  %1049 : Long() = prim::NumToTensor(%1048), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1050 : Long() = aten::floor_divide(%1049, %13), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/tensor.py:424:0
  %1051 : int = aten::Int(%1050), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1052 : int[] = prim::ListConstruct(%1046, %1047, %12, %1051), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %x.126 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.125, %1052), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:209:0
  %1054 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %v.8 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.126, %1054), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:213:0
  %w.29 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.8, %k.8), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:178:0
  %w.30 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.29, %10), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:180:0
  %1058 : int = aten::size(%w.30, %9), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %1059 : int = aten::size(%w.30, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %1060 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1010, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %1061 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1060, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %1062 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%1061, %31, %34, %1058, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %b.8 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%1062, %11, %34, %1059, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:183:0
  %1064 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.30, %b.8), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:184:0
  %1065 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.8, %33, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/tensor.py:396:0
  %1066 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%1065, %26), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:184:0
  %w.31 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%1064, %1066, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:184:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.31, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:188:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %32, %28), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # torch/nn/functional.py:1498:0
  %w.32 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %24, %29), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.127 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.32, %v.8), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:197:0
  %1072 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1073 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.127, %1072), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:203:0
  %x.128 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1073, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:203:0
  %1075 : int = aten::size(%x.128, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:204:0
  %1076 : int = aten::size(%x.128, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:204:0
  %1077 : int = aten::size(%x.128, %9), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:204:0
  %1078 : Long() = prim::NumToTensor(%1077), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1079 : int = aten::size(%x.128, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:204:0
  %1080 : Long() = prim::NumToTensor(%1079), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1081 : Long() = aten::mul(%1078, %1080), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:204:0
  %1082 : int = aten::Int(%1081), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %1083 : int[] = prim::ListConstruct(%1075, %1076, %1082), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn
  %x.129 : Float(17:9984, 13:768, 768:1) = aten::view(%x.128, %1083), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn # transformers/modeling_openai.py:205:0
  %1085 : Tensor = prim::GetAttr[name="weight"](%1009)
  %1086 : Tensor = prim::GetAttr[name="bias"](%1009)
  %1087 : int = aten::size(%x.129, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1088 : int = aten::size(%x.129, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1089 : int = aten::size(%x.129, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1090 : int[] = prim::ListConstruct(%32, %1089), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj
  %1091 : Float(221:768, 768:1) = aten::view(%x.129, %1090), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.130 : Float(221:768, 768:1) = aten::addmm(%1086, %1091, %1085, %33, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1093 : int[] = prim::ListConstruct(%1087, %1088, %14), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%x.130, %1093), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.48, %24, %29), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.attn/__module.transformer.h.7.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add(%x.119, %a.8, %33), scope: __module.transformer/__module.transformer.h.7 # transformers/modeling_openai.py:266:0
  %1097 : Tensor = prim::GetAttr[name="bias"](%1007)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1007)
  %1099 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.ln_1
  %x.131 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.49, %1099, %1098, %1097, %17, %16), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.ln_1 # torch/nn/functional.py:2048:0
  %1101 : __torch__.transformers.modeling_utils.___torch_mangle_28979.Conv1D = prim::GetAttr[name="c_proj"](%1006)
  %1102 : __torch__.transformers.modeling_utils.___torch_mangle_28978.Conv1D = prim::GetAttr[name="c_fc"](%1006)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1102)
  %1104 : Tensor = prim::GetAttr[name="bias"](%1102)
  %1105 : int = aten::size(%x.131, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1106 : int = aten::size(%x.131, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1107 : int = aten::size(%x.131, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1108 : int[] = prim::ListConstruct(%32, %1107), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc
  %1109 : Float(221:768, 768:1) = aten::view(%x.131, %1108), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.132 : Float(221:3072, 3072:1) = aten::addmm(%1104, %1109, %1103, %33, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1111 : int[] = prim::ListConstruct(%1105, %1106, %23), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc
  %x.133 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.132, %1111), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %1113 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.133, %22), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1114 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.133, %21), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1115 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1114, %20), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1116 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.133, %1115, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1117 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1116, %19), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1118 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%1117), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1119 : Float(17:39936, 13:3072, 3072:1) = aten::add(%1118, %18, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %x.134 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1113, %1119), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp # transformers/activations.py:30:0
  %1121 : Tensor = prim::GetAttr[name="weight"](%1101)
  %1122 : Tensor = prim::GetAttr[name="bias"](%1101)
  %1123 : int = aten::size(%x.134, %34), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1124 : int = aten::size(%x.134, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1125 : int = aten::size(%x.134, %32), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1126 : int[] = prim::ListConstruct(%32, %1125), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj
  %1127 : Float(221:3072, 3072:1) = aten::view(%x.134, %1126), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.135 : Float(221:768, 768:1) = aten::addmm(%1122, %1127, %1121, %33, %33), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1129 : int[] = prim::ListConstruct(%1123, %1124, %14), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::view(%x.135, %1129), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.50, %24, %29), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.mlp/__module.transformer.h.7.mlp.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(17:9984, 13:768, 768:1) = aten::add(%x.131, %m.8, %33), scope: __module.transformer/__module.transformer.h.7 # transformers/modeling_openai.py:268:0
  %1133 : Tensor = prim::GetAttr[name="bias"](%1005)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1005)
  %1135 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.ln_2
  %x.136 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.51, %1135, %1134, %1133, %17, %16), scope: __module.transformer/__module.transformer.h.7/__module.transformer.h.7.ln_2 # torch/nn/functional.py:2048:0
  %1137 : __torch__.torch.nn.modules.normalization.___torch_mangle_28994.LayerNorm = prim::GetAttr[name="ln_2"](%42)
  %1138 : __torch__.transformers.modeling_openai.___torch_mangle_28993.MLP = prim::GetAttr[name="mlp"](%42)
  %1139 : __torch__.torch.nn.modules.normalization.___torch_mangle_28989.LayerNorm = prim::GetAttr[name="ln_1"](%42)
  %1140 : __torch__.transformers.modeling_openai.___torch_mangle_28988.Attention = prim::GetAttr[name="attn"](%42)
  %1141 : __torch__.transformers.modeling_utils.___torch_mangle_28985.Conv1D = prim::GetAttr[name="c_proj"](%1140)
  %1142 : Tensor = prim::GetAttr[name="bias"](%1140)
  %1143 : __torch__.transformers.modeling_utils.___torch_mangle_28984.Conv1D = prim::GetAttr[name="c_attn"](%1140)
  %1144 : Tensor = prim::GetAttr[name="weight"](%1143)
  %1145 : Tensor = prim::GetAttr[name="bias"](%1143)
  %1146 : int = aten::size(%x.136, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1147 : int = aten::size(%x.136, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1148 : int = aten::size(%x.136, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1149 : int[] = prim::ListConstruct(%32, %1148), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn
  %1150 : Float(221:768, 768:1) = aten::view(%x.136, %1149), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.137 : Float(221:2304, 2304:1) = aten::addmm(%1145, %1150, %1144, %33, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1152 : int[] = prim::ListConstruct(%1146, %1147, %15), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn
  %1153 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.137, %1152), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_attn # transformers/modeling_utils.py:1095:0
  %1154 : Tensor[] = aten::split(%1153, %14, %31), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/tensor.py:371:0
  %x.138 : Float(17:29952, 13:2304, 768:1), %x.140 : Float(17:29952, 13:2304, 768:1), %x.142 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%1154), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1158 : int = aten::size(%x.138, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1159 : int = aten::size(%x.138, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1160 : int = aten::size(%x.138, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1161 : Long() = prim::NumToTensor(%1160), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1162 : Long() = aten::floor_divide(%1161, %13), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/tensor.py:424:0
  %1163 : int = aten::Int(%1162), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1164 : int[] = prim::ListConstruct(%1158, %1159, %12, %1163), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %x.139 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.138, %1164), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:209:0
  %1166 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %q.9 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.139, %1166), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:213:0
  %1168 : int = aten::size(%x.140, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1169 : int = aten::size(%x.140, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1170 : int = aten::size(%x.140, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1171 : Long() = prim::NumToTensor(%1170), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1172 : Long() = aten::floor_divide(%1171, %13), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/tensor.py:424:0
  %1173 : int = aten::Int(%1172), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1174 : int[] = prim::ListConstruct(%1168, %1169, %12, %1173), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %x.141 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.140, %1174), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:209:0
  %1176 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %k.9 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.141, %1176), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:211:0
  %1178 : int = aten::size(%x.142, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1179 : int = aten::size(%x.142, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1180 : int = aten::size(%x.142, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:208:0
  %1181 : Long() = prim::NumToTensor(%1180), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1182 : Long() = aten::floor_divide(%1181, %13), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/tensor.py:424:0
  %1183 : int = aten::Int(%1182), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1184 : int[] = prim::ListConstruct(%1178, %1179, %12, %1183), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %x.143 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.142, %1184), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:209:0
  %1186 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %v.9 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.143, %1186), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:213:0
  %w.33 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.9, %k.9), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:178:0
  %w.34 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.33, %10), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:180:0
  %1190 : int = aten::size(%w.34, %9), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %1191 : int = aten::size(%w.34, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %1192 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1142, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %1193 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1192, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %1194 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%1193, %31, %34, %1190, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %b.9 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%1194, %11, %34, %1191, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:183:0
  %1196 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.34, %b.9), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:184:0
  %1197 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.9, %33, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/tensor.py:396:0
  %1198 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%1197, %26), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:184:0
  %w.35 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%1196, %1198, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:184:0
  %input.52 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.35, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:188:0
  %input.53 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.52, %32, %28), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # torch/nn/functional.py:1498:0
  %w.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.53, %24, %29), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.144 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.36, %v.9), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:197:0
  %1204 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1205 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.144, %1204), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:203:0
  %x.145 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1205, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:203:0
  %1207 : int = aten::size(%x.145, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:204:0
  %1208 : int = aten::size(%x.145, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:204:0
  %1209 : int = aten::size(%x.145, %9), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:204:0
  %1210 : Long() = prim::NumToTensor(%1209), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1211 : int = aten::size(%x.145, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:204:0
  %1212 : Long() = prim::NumToTensor(%1211), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1213 : Long() = aten::mul(%1210, %1212), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:204:0
  %1214 : int = aten::Int(%1213), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %1215 : int[] = prim::ListConstruct(%1207, %1208, %1214), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn
  %x.146 : Float(17:9984, 13:768, 768:1) = aten::view(%x.145, %1215), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn # transformers/modeling_openai.py:205:0
  %1217 : Tensor = prim::GetAttr[name="weight"](%1141)
  %1218 : Tensor = prim::GetAttr[name="bias"](%1141)
  %1219 : int = aten::size(%x.146, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1220 : int = aten::size(%x.146, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1221 : int = aten::size(%x.146, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1222 : int[] = prim::ListConstruct(%32, %1221), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj
  %1223 : Float(221:768, 768:1) = aten::view(%x.146, %1222), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.147 : Float(221:768, 768:1) = aten::addmm(%1218, %1223, %1217, %33, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1225 : int[] = prim::ListConstruct(%1219, %1220, %14), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::view(%x.147, %1225), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.54, %24, %29), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.attn/__module.transformer.h.8.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::add(%x.136, %a.9, %33), scope: __module.transformer/__module.transformer.h.8 # transformers/modeling_openai.py:266:0
  %1229 : Tensor = prim::GetAttr[name="bias"](%1139)
  %1230 : Tensor = prim::GetAttr[name="weight"](%1139)
  %1231 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.ln_1
  %x.148 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.55, %1231, %1230, %1229, %17, %16), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.ln_1 # torch/nn/functional.py:2048:0
  %1233 : __torch__.transformers.modeling_utils.___torch_mangle_28991.Conv1D = prim::GetAttr[name="c_proj"](%1138)
  %1234 : __torch__.transformers.modeling_utils.___torch_mangle_28990.Conv1D = prim::GetAttr[name="c_fc"](%1138)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1234)
  %1236 : Tensor = prim::GetAttr[name="bias"](%1234)
  %1237 : int = aten::size(%x.148, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1238 : int = aten::size(%x.148, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1239 : int = aten::size(%x.148, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1240 : int[] = prim::ListConstruct(%32, %1239), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc
  %1241 : Float(221:768, 768:1) = aten::view(%x.148, %1240), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.149 : Float(221:3072, 3072:1) = aten::addmm(%1236, %1241, %1235, %33, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1243 : int[] = prim::ListConstruct(%1237, %1238, %23), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc
  %x.150 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.149, %1243), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %1245 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.150, %22), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1246 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.150, %21), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1247 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1246, %20), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1248 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.150, %1247, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1249 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1248, %19), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1250 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%1249), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1251 : Float(17:39936, 13:3072, 3072:1) = aten::add(%1250, %18, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %x.151 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1245, %1251), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp # transformers/activations.py:30:0
  %1253 : Tensor = prim::GetAttr[name="weight"](%1233)
  %1254 : Tensor = prim::GetAttr[name="bias"](%1233)
  %1255 : int = aten::size(%x.151, %34), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1256 : int = aten::size(%x.151, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1257 : int = aten::size(%x.151, %32), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1258 : int[] = prim::ListConstruct(%32, %1257), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj
  %1259 : Float(221:3072, 3072:1) = aten::view(%x.151, %1258), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.152 : Float(221:768, 768:1) = aten::addmm(%1254, %1259, %1253, %33, %33), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1261 : int[] = prim::ListConstruct(%1255, %1256, %14), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj
  %input.56 : Float(17:9984, 13:768, 768:1) = aten::view(%x.152, %1261), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.56, %24, %29), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.mlp/__module.transformer.h.8.mlp.dropout # torch/nn/functional.py:973:0
  %input.57 : Float(17:9984, 13:768, 768:1) = aten::add(%x.148, %m.9, %33), scope: __module.transformer/__module.transformer.h.8 # transformers/modeling_openai.py:268:0
  %1265 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1266 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1267 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.ln_2
  %x.153 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.57, %1267, %1266, %1265, %17, %16), scope: __module.transformer/__module.transformer.h.8/__module.transformer.h.8.ln_2 # torch/nn/functional.py:2048:0
  %1269 : __torch__.torch.nn.modules.normalization.___torch_mangle_29006.LayerNorm = prim::GetAttr[name="ln_2"](%40)
  %1270 : __torch__.transformers.modeling_openai.___torch_mangle_29005.MLP = prim::GetAttr[name="mlp"](%40)
  %1271 : __torch__.torch.nn.modules.normalization.___torch_mangle_29001.LayerNorm = prim::GetAttr[name="ln_1"](%40)
  %1272 : __torch__.transformers.modeling_openai.___torch_mangle_29000.Attention = prim::GetAttr[name="attn"](%40)
  %1273 : __torch__.transformers.modeling_utils.___torch_mangle_28997.Conv1D = prim::GetAttr[name="c_proj"](%1272)
  %1274 : Tensor = prim::GetAttr[name="bias"](%1272)
  %1275 : __torch__.transformers.modeling_utils.___torch_mangle_28996.Conv1D = prim::GetAttr[name="c_attn"](%1272)
  %1276 : Tensor = prim::GetAttr[name="weight"](%1275)
  %1277 : Tensor = prim::GetAttr[name="bias"](%1275)
  %1278 : int = aten::size(%x.153, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1279 : int = aten::size(%x.153, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1280 : int = aten::size(%x.153, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1281 : int[] = prim::ListConstruct(%32, %1280), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn
  %1282 : Float(221:768, 768:1) = aten::view(%x.153, %1281), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.154 : Float(221:2304, 2304:1) = aten::addmm(%1277, %1282, %1276, %33, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1284 : int[] = prim::ListConstruct(%1278, %1279, %15), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn
  %1285 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.154, %1284), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_attn # transformers/modeling_utils.py:1095:0
  %1286 : Tensor[] = aten::split(%1285, %14, %31), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/tensor.py:371:0
  %x.155 : Float(17:29952, 13:2304, 768:1), %x.157 : Float(17:29952, 13:2304, 768:1), %x.159 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%1286), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1290 : int = aten::size(%x.155, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1291 : int = aten::size(%x.155, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1292 : int = aten::size(%x.155, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1293 : Long() = prim::NumToTensor(%1292), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1294 : Long() = aten::floor_divide(%1293, %13), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/tensor.py:424:0
  %1295 : int = aten::Int(%1294), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1296 : int[] = prim::ListConstruct(%1290, %1291, %12, %1295), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %x.156 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.155, %1296), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:209:0
  %1298 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %q.10 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.156, %1298), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:213:0
  %1300 : int = aten::size(%x.157, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1301 : int = aten::size(%x.157, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1302 : int = aten::size(%x.157, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1303 : Long() = prim::NumToTensor(%1302), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1304 : Long() = aten::floor_divide(%1303, %13), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/tensor.py:424:0
  %1305 : int = aten::Int(%1304), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1306 : int[] = prim::ListConstruct(%1300, %1301, %12, %1305), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %x.158 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.157, %1306), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:209:0
  %1308 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %k.10 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.158, %1308), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:211:0
  %1310 : int = aten::size(%x.159, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1311 : int = aten::size(%x.159, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1312 : int = aten::size(%x.159, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:208:0
  %1313 : Long() = prim::NumToTensor(%1312), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1314 : Long() = aten::floor_divide(%1313, %13), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/tensor.py:424:0
  %1315 : int = aten::Int(%1314), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1316 : int[] = prim::ListConstruct(%1310, %1311, %12, %1315), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %x.160 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.159, %1316), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:209:0
  %1318 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %v.10 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.160, %1318), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:213:0
  %w.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.10, %k.10), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:178:0
  %w.38 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.37, %10), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:180:0
  %1322 : int = aten::size(%w.38, %9), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %1323 : int = aten::size(%w.38, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %1324 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1274, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %1325 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1324, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %1326 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%1325, %31, %34, %1322, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %b.10 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%1326, %11, %34, %1323, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:183:0
  %1328 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.38, %b.10), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:184:0
  %1329 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.10, %33, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/tensor.py:396:0
  %1330 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%1329, %26), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:184:0
  %w.39 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%1328, %1330, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:184:0
  %input.58 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.39, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:188:0
  %input.59 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.58, %32, %28), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # torch/nn/functional.py:1498:0
  %w.40 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.59, %24, %29), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.161 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.40, %v.10), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:197:0
  %1336 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1337 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.161, %1336), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:203:0
  %x.162 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1337, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:203:0
  %1339 : int = aten::size(%x.162, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:204:0
  %1340 : int = aten::size(%x.162, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:204:0
  %1341 : int = aten::size(%x.162, %9), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:204:0
  %1342 : Long() = prim::NumToTensor(%1341), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1343 : int = aten::size(%x.162, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:204:0
  %1344 : Long() = prim::NumToTensor(%1343), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1345 : Long() = aten::mul(%1342, %1344), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:204:0
  %1346 : int = aten::Int(%1345), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %1347 : int[] = prim::ListConstruct(%1339, %1340, %1346), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn
  %x.163 : Float(17:9984, 13:768, 768:1) = aten::view(%x.162, %1347), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn # transformers/modeling_openai.py:205:0
  %1349 : Tensor = prim::GetAttr[name="weight"](%1273)
  %1350 : Tensor = prim::GetAttr[name="bias"](%1273)
  %1351 : int = aten::size(%x.163, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1352 : int = aten::size(%x.163, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1353 : int = aten::size(%x.163, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1354 : int[] = prim::ListConstruct(%32, %1353), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj
  %1355 : Float(221:768, 768:1) = aten::view(%x.163, %1354), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.164 : Float(221:768, 768:1) = aten::addmm(%1350, %1355, %1349, %33, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1357 : int[] = prim::ListConstruct(%1351, %1352, %14), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::view(%x.164, %1357), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.60, %24, %29), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.attn/__module.transformer.h.9.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.61 : Float(17:9984, 13:768, 768:1) = aten::add(%x.153, %a.10, %33), scope: __module.transformer/__module.transformer.h.9 # transformers/modeling_openai.py:266:0
  %1361 : Tensor = prim::GetAttr[name="bias"](%1271)
  %1362 : Tensor = prim::GetAttr[name="weight"](%1271)
  %1363 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.ln_1
  %x.165 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.61, %1363, %1362, %1361, %17, %16), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.ln_1 # torch/nn/functional.py:2048:0
  %1365 : __torch__.transformers.modeling_utils.___torch_mangle_29003.Conv1D = prim::GetAttr[name="c_proj"](%1270)
  %1366 : __torch__.transformers.modeling_utils.___torch_mangle_29002.Conv1D = prim::GetAttr[name="c_fc"](%1270)
  %1367 : Tensor = prim::GetAttr[name="weight"](%1366)
  %1368 : Tensor = prim::GetAttr[name="bias"](%1366)
  %1369 : int = aten::size(%x.165, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1370 : int = aten::size(%x.165, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1371 : int = aten::size(%x.165, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1372 : int[] = prim::ListConstruct(%32, %1371), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc
  %1373 : Float(221:768, 768:1) = aten::view(%x.165, %1372), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.166 : Float(221:3072, 3072:1) = aten::addmm(%1368, %1373, %1367, %33, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1375 : int[] = prim::ListConstruct(%1369, %1370, %23), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc
  %x.167 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.166, %1375), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %1377 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.167, %22), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1378 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.167, %21), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1379 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1378, %20), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1380 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.167, %1379, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1381 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1380, %19), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1382 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%1381), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1383 : Float(17:39936, 13:3072, 3072:1) = aten::add(%1382, %18, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %x.168 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1377, %1383), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp # transformers/activations.py:30:0
  %1385 : Tensor = prim::GetAttr[name="weight"](%1365)
  %1386 : Tensor = prim::GetAttr[name="bias"](%1365)
  %1387 : int = aten::size(%x.168, %34), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1388 : int = aten::size(%x.168, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1389 : int = aten::size(%x.168, %32), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1390 : int[] = prim::ListConstruct(%32, %1389), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj
  %1391 : Float(221:3072, 3072:1) = aten::view(%x.168, %1390), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.169 : Float(221:768, 768:1) = aten::addmm(%1386, %1391, %1385, %33, %33), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1393 : int[] = prim::ListConstruct(%1387, %1388, %14), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj
  %input.62 : Float(17:9984, 13:768, 768:1) = aten::view(%x.169, %1393), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.62, %24, %29), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.mlp/__module.transformer.h.9.mlp.dropout # torch/nn/functional.py:973:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add(%x.165, %m.10, %33), scope: __module.transformer/__module.transformer.h.9 # transformers/modeling_openai.py:268:0
  %1397 : Tensor = prim::GetAttr[name="bias"](%1269)
  %1398 : Tensor = prim::GetAttr[name="weight"](%1269)
  %1399 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.ln_2
  %x.170 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.63, %1399, %1398, %1397, %17, %16), scope: __module.transformer/__module.transformer.h.9/__module.transformer.h.9.ln_2 # torch/nn/functional.py:2048:0
  %1401 : __torch__.torch.nn.modules.normalization.___torch_mangle_29018.LayerNorm = prim::GetAttr[name="ln_2"](%38)
  %1402 : __torch__.transformers.modeling_openai.___torch_mangle_29017.MLP = prim::GetAttr[name="mlp"](%38)
  %1403 : __torch__.torch.nn.modules.normalization.___torch_mangle_29013.LayerNorm = prim::GetAttr[name="ln_1"](%38)
  %1404 : __torch__.transformers.modeling_openai.___torch_mangle_29012.Attention = prim::GetAttr[name="attn"](%38)
  %1405 : __torch__.transformers.modeling_utils.___torch_mangle_29009.Conv1D = prim::GetAttr[name="c_proj"](%1404)
  %1406 : Tensor = prim::GetAttr[name="bias"](%1404)
  %1407 : __torch__.transformers.modeling_utils.___torch_mangle_29008.Conv1D = prim::GetAttr[name="c_attn"](%1404)
  %1408 : Tensor = prim::GetAttr[name="weight"](%1407)
  %1409 : Tensor = prim::GetAttr[name="bias"](%1407)
  %1410 : int = aten::size(%x.170, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1411 : int = aten::size(%x.170, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1412 : int = aten::size(%x.170, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1413 : int[] = prim::ListConstruct(%32, %1412), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn
  %1414 : Float(221:768, 768:1) = aten::view(%x.170, %1413), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.171 : Float(221:2304, 2304:1) = aten::addmm(%1409, %1414, %1408, %33, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1416 : int[] = prim::ListConstruct(%1410, %1411, %15), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn
  %1417 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.171, %1416), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_attn # transformers/modeling_utils.py:1095:0
  %1418 : Tensor[] = aten::split(%1417, %14, %31), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/tensor.py:371:0
  %x.172 : Float(17:29952, 13:2304, 768:1), %x.174 : Float(17:29952, 13:2304, 768:1), %x.176 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%1418), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1422 : int = aten::size(%x.172, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1423 : int = aten::size(%x.172, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1424 : int = aten::size(%x.172, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1425 : Long() = prim::NumToTensor(%1424), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1426 : Long() = aten::floor_divide(%1425, %13), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/tensor.py:424:0
  %1427 : int = aten::Int(%1426), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1428 : int[] = prim::ListConstruct(%1422, %1423, %12, %1427), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %x.173 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.172, %1428), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:209:0
  %1430 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %q.11 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.173, %1430), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:213:0
  %1432 : int = aten::size(%x.174, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1433 : int = aten::size(%x.174, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1434 : int = aten::size(%x.174, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1435 : Long() = prim::NumToTensor(%1434), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1436 : Long() = aten::floor_divide(%1435, %13), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/tensor.py:424:0
  %1437 : int = aten::Int(%1436), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1438 : int[] = prim::ListConstruct(%1432, %1433, %12, %1437), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %x.175 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.174, %1438), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:209:0
  %1440 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %k.11 : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.175, %1440), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:211:0
  %1442 : int = aten::size(%x.176, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1443 : int = aten::size(%x.176, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1444 : int = aten::size(%x.176, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:208:0
  %1445 : Long() = prim::NumToTensor(%1444), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1446 : Long() = aten::floor_divide(%1445, %13), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/tensor.py:424:0
  %1447 : int = aten::Int(%1446), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1448 : int[] = prim::ListConstruct(%1442, %1443, %12, %1447), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %x.177 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.176, %1448), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:209:0
  %1450 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %v.11 : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.177, %1450), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:213:0
  %w.41 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q.11, %k.11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:178:0
  %w.42 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.41, %10), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:180:0
  %1454 : int = aten::size(%w.42, %9), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %1455 : int = aten::size(%w.42, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %1456 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1406, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %1457 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1456, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %1458 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%1457, %31, %34, %1454, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %b.11 : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%1458, %11, %34, %1455, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:183:0
  %1460 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.42, %b.11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:184:0
  %1461 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b.11, %33, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/tensor.py:396:0
  %1462 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%1461, %26), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:184:0
  %w.43 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%1460, %1462, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:184:0
  %input.64 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.43, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:188:0
  %input.65 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.64, %32, %28), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # torch/nn/functional.py:1498:0
  %w.44 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.65, %24, %29), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.178 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w.44, %v.11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:197:0
  %1468 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1469 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.178, %1468), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:203:0
  %x.179 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1469, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:203:0
  %1471 : int = aten::size(%x.179, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:204:0
  %1472 : int = aten::size(%x.179, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:204:0
  %1473 : int = aten::size(%x.179, %9), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:204:0
  %1474 : Long() = prim::NumToTensor(%1473), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1475 : int = aten::size(%x.179, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:204:0
  %1476 : Long() = prim::NumToTensor(%1475), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1477 : Long() = aten::mul(%1474, %1476), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:204:0
  %1478 : int = aten::Int(%1477), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %1479 : int[] = prim::ListConstruct(%1471, %1472, %1478), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn
  %x.180 : Float(17:9984, 13:768, 768:1) = aten::view(%x.179, %1479), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn # transformers/modeling_openai.py:205:0
  %1481 : Tensor = prim::GetAttr[name="weight"](%1405)
  %1482 : Tensor = prim::GetAttr[name="bias"](%1405)
  %1483 : int = aten::size(%x.180, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1484 : int = aten::size(%x.180, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1485 : int = aten::size(%x.180, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1486 : int[] = prim::ListConstruct(%32, %1485), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj
  %1487 : Float(221:768, 768:1) = aten::view(%x.180, %1486), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.181 : Float(221:768, 768:1) = aten::addmm(%1482, %1487, %1481, %33, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1489 : int[] = prim::ListConstruct(%1483, %1484, %14), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj
  %input.66 : Float(17:9984, 13:768, 768:1) = aten::view(%x.181, %1489), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.66, %24, %29), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.attn/__module.transformer.h.10.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.67 : Float(17:9984, 13:768, 768:1) = aten::add(%x.170, %a.11, %33), scope: __module.transformer/__module.transformer.h.10 # transformers/modeling_openai.py:266:0
  %1493 : Tensor = prim::GetAttr[name="bias"](%1403)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1403)
  %1495 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.ln_1
  %x.182 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.67, %1495, %1494, %1493, %17, %16), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.ln_1 # torch/nn/functional.py:2048:0
  %1497 : __torch__.transformers.modeling_utils.___torch_mangle_29015.Conv1D = prim::GetAttr[name="c_proj"](%1402)
  %1498 : __torch__.transformers.modeling_utils.___torch_mangle_29014.Conv1D = prim::GetAttr[name="c_fc"](%1402)
  %1499 : Tensor = prim::GetAttr[name="weight"](%1498)
  %1500 : Tensor = prim::GetAttr[name="bias"](%1498)
  %1501 : int = aten::size(%x.182, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1502 : int = aten::size(%x.182, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1503 : int = aten::size(%x.182, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1504 : int[] = prim::ListConstruct(%32, %1503), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc
  %1505 : Float(221:768, 768:1) = aten::view(%x.182, %1504), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.183 : Float(221:3072, 3072:1) = aten::addmm(%1500, %1505, %1499, %33, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1507 : int[] = prim::ListConstruct(%1501, %1502, %23), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc
  %x.184 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.183, %1507), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %1509 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.184, %22), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1510 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.184, %21), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1511 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1510, %20), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1512 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.184, %1511, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1513 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1512, %19), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1514 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%1513), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1515 : Float(17:39936, 13:3072, 3072:1) = aten::add(%1514, %18, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %x.185 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1509, %1515), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp # transformers/activations.py:30:0
  %1517 : Tensor = prim::GetAttr[name="weight"](%1497)
  %1518 : Tensor = prim::GetAttr[name="bias"](%1497)
  %1519 : int = aten::size(%x.185, %34), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1520 : int = aten::size(%x.185, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1521 : int = aten::size(%x.185, %32), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1522 : int[] = prim::ListConstruct(%32, %1521), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj
  %1523 : Float(221:3072, 3072:1) = aten::view(%x.185, %1522), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x.186 : Float(221:768, 768:1) = aten::addmm(%1518, %1523, %1517, %33, %33), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1525 : int[] = prim::ListConstruct(%1519, %1520, %14), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%x.186, %1525), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.68, %24, %29), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.mlp/__module.transformer.h.10.mlp.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add(%x.182, %m.11, %33), scope: __module.transformer/__module.transformer.h.10 # transformers/modeling_openai.py:268:0
  %1529 : Tensor = prim::GetAttr[name="bias"](%1401)
  %1530 : Tensor = prim::GetAttr[name="weight"](%1401)
  %1531 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.ln_2
  %x.187 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.69, %1531, %1530, %1529, %17, %16), scope: __module.transformer/__module.transformer.h.10/__module.transformer.h.10.ln_2 # torch/nn/functional.py:2048:0
  %1533 : __torch__.torch.nn.modules.normalization.___torch_mangle_29030.LayerNorm = prim::GetAttr[name="ln_2"](%36)
  %1534 : __torch__.transformers.modeling_openai.___torch_mangle_29029.MLP = prim::GetAttr[name="mlp"](%36)
  %1535 : __torch__.torch.nn.modules.normalization.___torch_mangle_29025.LayerNorm = prim::GetAttr[name="ln_1"](%36)
  %1536 : __torch__.transformers.modeling_openai.___torch_mangle_29024.Attention = prim::GetAttr[name="attn"](%36)
  %1537 : __torch__.transformers.modeling_utils.___torch_mangle_29021.Conv1D = prim::GetAttr[name="c_proj"](%1536)
  %1538 : Tensor = prim::GetAttr[name="bias"](%1536)
  %1539 : __torch__.transformers.modeling_utils.___torch_mangle_29020.Conv1D = prim::GetAttr[name="c_attn"](%1536)
  %1540 : Tensor = prim::GetAttr[name="weight"](%1539)
  %1541 : Tensor = prim::GetAttr[name="bias"](%1539)
  %1542 : int = aten::size(%x.187, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1543 : int = aten::size(%x.187, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1093:0
  %1544 : int = aten::size(%x.187, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1545 : int[] = prim::ListConstruct(%32, %1544), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn
  %1546 : Float(221:768, 768:1) = aten::view(%x.187, %1545), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1094:0
  %x.188 : Float(221:2304, 2304:1) = aten::addmm(%1541, %1546, %1540, %33, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1094:0
  %1548 : int[] = prim::ListConstruct(%1542, %1543, %15), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn
  %1549 : Float(17:29952, 13:2304, 2304:1) = aten::view(%x.188, %1548), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_attn # transformers/modeling_utils.py:1095:0
  %1550 : Tensor[] = aten::split(%1549, %14, %31), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/tensor.py:371:0
  %x.189 : Float(17:29952, 13:2304, 768:1), %x.191 : Float(17:29952, 13:2304, 768:1), %x.193 : Float(17:29952, 13:2304, 768:1) = prim::ListUnpack(%1550), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1554 : int = aten::size(%x.189, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1555 : int = aten::size(%x.189, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1556 : int = aten::size(%x.189, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1557 : Long() = prim::NumToTensor(%1556), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1558 : Long() = aten::floor_divide(%1557, %13), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/tensor.py:424:0
  %1559 : int = aten::Int(%1558), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1560 : int[] = prim::ListConstruct(%1554, %1555, %12, %1559), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %x.190 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.189, %1560), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:209:0
  %1562 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %q : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.190, %1562), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:213:0
  %1564 : int = aten::size(%x.191, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1565 : int = aten::size(%x.191, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1566 : int = aten::size(%x.191, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1567 : Long() = prim::NumToTensor(%1566), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1568 : Long() = aten::floor_divide(%1567, %13), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/tensor.py:424:0
  %1569 : int = aten::Int(%1568), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1570 : int[] = prim::ListConstruct(%1564, %1565, %12, %1569), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %x.192 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.191, %1570), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:209:0
  %1572 : int[] = prim::ListConstruct(%34, %31, %11, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %k : Float(17:29952, 12:64, 64:1, 13:2304) = aten::permute(%x.192, %1572), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:211:0
  %1574 : int = aten::size(%x.193, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1575 : int = aten::size(%x.193, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1576 : int = aten::size(%x.193, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:208:0
  %1577 : Long() = prim::NumToTensor(%1576), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1578 : Long() = aten::floor_divide(%1577, %13), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/tensor.py:424:0
  %1579 : int = aten::Int(%1578), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1580 : int[] = prim::ListConstruct(%1574, %1575, %12, %1579), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %x.194 : Float(17:29952, 13:2304, 12:64, 64:1) = aten::view(%x.193, %1580), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:209:0
  %1582 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %v : Float(17:29952, 12:64, 13:2304, 64:1) = aten::permute(%x.194, %1582), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:213:0
  %w.45 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%q, %k), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:178:0
  %w.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%w.45, %10), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:180:0
  %1586 : int = aten::size(%w.46, %9), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %1587 : int = aten::size(%w.46, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %1588 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1538, %34, %34, %8, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %1589 : Float(1:262144, 1:262144, 512:512, 512:1) = aten::slice(%1588, %33, %34, %8, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %1590 : Float(1:262144, 1:262144, 13:512, 512:1) = aten::slice(%1589, %31, %34, %1586, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %b : Float(1:262144, 1:262144, 13:512, 13:1) = aten::slice(%1590, %11, %34, %1587, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:183:0
  %1592 : Float(17:2028, 12:169, 13:13, 13:1) = aten::mul(%w.46, %b), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:184:0
  %1593 : Float(1:169, 1:169, 13:13, 13:1) = aten::rsub(%b, %33, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/tensor.py:396:0
  %1594 : Float(1:169, 1:169, 13:13, 13:1) = aten::mul(%1593, %26), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:184:0
  %w.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%1592, %1594, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:184:0
  %input.70 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%w.47, %attention_mask, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:188:0
  %input.71 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.70, %32, %28), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # torch/nn/functional.py:1498:0
  %w : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.71, %24, %29), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.attn_dropout # torch/nn/functional.py:973:0
  %x.195 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%w, %v), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:197:0
  %1600 : int[] = prim::ListConstruct(%34, %31, %33, %11), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1601 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%x.195, %1600), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:203:0
  %x.196 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1601, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:203:0
  %1603 : int = aten::size(%x.196, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:204:0
  %1604 : int = aten::size(%x.196, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:204:0
  %1605 : int = aten::size(%x.196, %9), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:204:0
  %1606 : Long() = prim::NumToTensor(%1605), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1607 : int = aten::size(%x.196, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:204:0
  %1608 : Long() = prim::NumToTensor(%1607), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1609 : Long() = aten::mul(%1606, %1608), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:204:0
  %1610 : int = aten::Int(%1609), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %1611 : int[] = prim::ListConstruct(%1603, %1604, %1610), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn
  %x.197 : Float(17:9984, 13:768, 768:1) = aten::view(%x.196, %1611), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn # transformers/modeling_openai.py:205:0
  %1613 : Tensor = prim::GetAttr[name="weight"](%1537)
  %1614 : Tensor = prim::GetAttr[name="bias"](%1537)
  %1615 : int = aten::size(%x.197, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1616 : int = aten::size(%x.197, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1093:0
  %1617 : int = aten::size(%x.197, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1618 : int[] = prim::ListConstruct(%32, %1617), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj
  %1619 : Float(221:768, 768:1) = aten::view(%x.197, %1618), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1094:0
  %x.198 : Float(221:768, 768:1) = aten::addmm(%1614, %1619, %1613, %33, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1094:0
  %1621 : int[] = prim::ListConstruct(%1615, %1616, %14), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj
  %input.72 : Float(17:9984, 13:768, 768:1) = aten::view(%x.198, %1621), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.c_proj # transformers/modeling_utils.py:1095:0
  %a : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.72, %24, %29), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.attn/__module.transformer.h.11.attn.resid_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add(%x.187, %a, %33), scope: __module.transformer/__module.transformer.h.11 # transformers/modeling_openai.py:266:0
  %1625 : Tensor = prim::GetAttr[name="bias"](%1535)
  %1626 : Tensor = prim::GetAttr[name="weight"](%1535)
  %1627 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.ln_1
  %x.199 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.73, %1627, %1626, %1625, %17, %16), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.ln_1 # torch/nn/functional.py:2048:0
  %1629 : __torch__.transformers.modeling_utils.___torch_mangle_29027.Conv1D = prim::GetAttr[name="c_proj"](%1534)
  %1630 : __torch__.transformers.modeling_utils.___torch_mangle_29026.Conv1D = prim::GetAttr[name="c_fc"](%1534)
  %1631 : Tensor = prim::GetAttr[name="weight"](%1630)
  %1632 : Tensor = prim::GetAttr[name="bias"](%1630)
  %1633 : int = aten::size(%x.199, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1634 : int = aten::size(%x.199, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1093:0
  %1635 : int = aten::size(%x.199, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1636 : int[] = prim::ListConstruct(%32, %1635), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc
  %1637 : Float(221:768, 768:1) = aten::view(%x.199, %1636), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %x.200 : Float(221:3072, 3072:1) = aten::addmm(%1632, %1637, %1631, %33, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1094:0
  %1639 : int[] = prim::ListConstruct(%1633, %1634, %23), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc
  %x.201 : Float(17:39936, 13:3072, 3072:1) = aten::view(%x.200, %1639), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_fc # transformers/modeling_utils.py:1095:0
  %1641 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%x.201, %22), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1642 : Float(17:39936, 13:3072, 3072:1) = aten::pow(%x.201, %21), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1643 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1642, %20), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1644 : Float(17:39936, 13:3072, 3072:1) = aten::add(%x.201, %1643, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1645 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1644, %19), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1646 : Float(17:39936, 13:3072, 3072:1) = aten::tanh(%1645), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1647 : Float(17:39936, 13:3072, 3072:1) = aten::add(%1646, %18, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %x.202 : Float(17:39936, 13:3072, 3072:1) = aten::mul(%1641, %1647), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp # transformers/activations.py:30:0
  %1649 : Tensor = prim::GetAttr[name="weight"](%1629)
  %1650 : Tensor = prim::GetAttr[name="bias"](%1629)
  %1651 : int = aten::size(%x.202, %34), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1652 : int = aten::size(%x.202, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1093:0
  %1653 : int = aten::size(%x.202, %32), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1654 : int[] = prim::ListConstruct(%32, %1653), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj
  %1655 : Float(221:3072, 3072:1) = aten::view(%x.202, %1654), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %x : Float(221:768, 768:1) = aten::addmm(%1650, %1655, %1649, %33, %33), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1094:0
  %1657 : int[] = prim::ListConstruct(%1651, %1652, %14), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::view(%x, %1657), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.c_proj # transformers/modeling_utils.py:1095:0
  %m : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.74, %24, %29), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.mlp/__module.transformer.h.11.mlp.dropout # torch/nn/functional.py:973:0
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::add(%x.199, %m, %33), scope: __module.transformer/__module.transformer.h.11 # transformers/modeling_openai.py:268:0
  %1661 : Tensor = prim::GetAttr[name="bias"](%1533)
  %1662 : Tensor = prim::GetAttr[name="weight"](%1533)
  %1663 : int[] = prim::ListConstruct(%14), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.ln_2
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.75, %1663, %1662, %1661, %17, %16), scope: __module.transformer/__module.transformer.h.11/__module.transformer.h.11.ln_2 # torch/nn/functional.py:2048:0
  %1665 : int[] = prim::ListConstruct(%62, %63, %80), scope: __module.transformer
  %input : Float(17:9984, 13:768, 768:1) = aten::view(%hidden_states, %1665), scope: __module.transformer # transformers/modeling_openai.py:513:0
  %1667 : Tensor = prim::GetAttr[name="weight"](%3)
  %1668 : Float(768:1, 40478:768) = aten::t(%1667), scope: __module.lm_head # torch/nn/functional.py:1676:0
  %1669 : Float(17:526214, 13:40478, 40478:1) = aten::matmul(%input, %1668), scope: __module.lm_head # torch/nn/functional.py:1676:0
  %7 : (Float(17:526214, 13:40478, 40478:1)) = prim::TupleConstruct(%1669)
  return (%7)
