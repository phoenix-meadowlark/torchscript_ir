FlaubertWithLMHeadModel(
  (transformer): FlaubertModel(
    (position_embeddings): Embedding(512, 2048)
    (embeddings): Embedding(30145, 2048, padding_idx=2)
    (layer_norm_emb): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    (attentions): ModuleList(
      (0): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (1): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (2): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (3): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (4): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (5): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (6): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (7): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (8): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (9): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (10): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
      (11): MultiHeadAttention(
        (q_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (k_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (v_lin): Linear(in_features=2048, out_features=2048, bias=True)
        (out_lin): Linear(in_features=2048, out_features=2048, bias=True)
      )
    )
    (layer_norm1): ModuleList(
      (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    )
    (ffns): ModuleList(
      (0): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (1): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (2): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (3): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (4): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (5): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (6): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (7): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (8): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (9): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (10): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
      (11): TransformerFFN(
        (lin1): Linear(in_features=2048, out_features=8192, bias=True)
        (lin2): Linear(in_features=8192, out_features=2048, bias=True)
      )
    )
    (layer_norm2): ModuleList(
      (0): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (1): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (2): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (3): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (4): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (5): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (6): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (7): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (8): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (9): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (10): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
      (11): LayerNorm((2048,), eps=1e-12, elementwise_affine=True)
    )
  )
  (pred_layer): XLMPredLayer(
    (proj): Linear(in_features=2048, out_features=30145, bias=True)
  )
)

FlaubertWithLMHeadModel._actual_script_module
FlaubertWithLMHeadModel.forward
  graph(%self.1 : __torch__.transformers.modeling_flaubert.FlaubertWithLMHeadModel,
        %input_ids : Long(17:13, 13:1),
        %padding_mask : Long(17:13, 13:1)):
    %2852 : __torch__.transformers.modeling_xlm.XLMPredLayer = prim::GetAttr[name="pred_layer"](%self.1)
    %2848 : __torch__.transformers.modeling_flaubert.FlaubertModel = prim::GetAttr[name="transformer"](%self.1)
    %2979 : Tensor = prim::CallMethod[name="forward"](%2848, %input_ids, %padding_mask)
    %2980 : Tensor = prim::CallMethod[name="forward"](%2852, %2979)
    %2354 : (Float(17:391885, 13:30145, 30145:1)) = prim::TupleConstruct(%2980)
    return (%2354)

FlaubertWithLMHeadModel.pred_layer
XLMPredLayer._actual_script_module
  graph(%self.126 : __torch__.transformers.modeling_xlm.XLMPredLayer,
        %3 : Float(17:26624, 13:2048, 2048:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="proj"](%self.126)
    %4 : Tensor = prim::CallMethod[name="forward"](%1, %3)
    return (%4)

FlaubertWithLMHeadModel.transformer
FlaubertModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_flaubert.FlaubertModel,
        %input_ids : Long(17:13, 13:1),
        %padding_mask : Long(17:13, 13:1)):
    %1 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%1)
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %4 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="11"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %6 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="11"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %8 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="11"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %10 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %12 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="10"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %14 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="10"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %16 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="10"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %18 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %20 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="9"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %22 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="9"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %24 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="9"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %26 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%25)
    %27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %28 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="8"](%27)
    %29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %30 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="8"](%29)
    %31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %32 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="8"](%31)
    %33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %34 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%33)
    %35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %36 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="7"](%35)
    %37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %38 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="7"](%37)
    %39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %40 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="7"](%39)
    %41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %42 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%41)
    %43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %44 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="6"](%43)
    %45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %46 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="6"](%45)
    %47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %48 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="6"](%47)
    %49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %50 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%49)
    %51 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %52 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="5"](%51)
    %53 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %54 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="5"](%53)
    %55 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %56 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="5"](%55)
    %57 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %58 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%57)
    %59 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %60 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="4"](%59)
    %61 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %62 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="4"](%61)
    %63 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %64 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="4"](%63)
    %65 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %66 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%65)
    %67 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %68 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="3"](%67)
    %69 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %70 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="3"](%69)
    %71 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %72 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="3"](%71)
    %73 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %74 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%73)
    %75 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %76 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="2"](%75)
    %77 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %78 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="2"](%77)
    %79 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %80 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="2"](%79)
    %81 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %82 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%81)
    %83 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %84 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="1"](%83)
    %85 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %86 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="1"](%85)
    %87 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %88 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="1"](%87)
    %89 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm2"](%self.2)
    %90 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%89)
    %91 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="ffns"](%self.2)
    %92 : __torch__.transformers.modeling_xlm.TransformerFFN = prim::GetAttr[name="0"](%91)
    %93 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer_norm1"](%self.2)
    %94 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="0"](%93)
    %95 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="attentions"](%self.2)
    %96 : __torch__.transformers.modeling_xlm.MultiHeadAttention = prim::GetAttr[name="0"](%95)
    %97 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm_emb"](%self.2)
    %98 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.2)
    %99 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="embeddings"](%self.2)
    %100 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
    %101 : int = aten::size(%input_ids, %100), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
    %bs.1 : Long() = prim::NumToTensor(%101), scope: __module.transformer
    %104 : int = aten::Int(%bs.1), scope: __module.transformer
    %105 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
    %106 : int = aten::size(%input_ids, %105), scope: __module.transformer # transformers/modeling_flaubert.py:174:0
    %slen : Long() = prim::NumToTensor(%106), scope: __module.transformer
    %108 : int = aten::Int(%slen), scope: __module.transformer
    %109 : Scalar = aten::ScalarImplicit(%slen), scope: __module.transformer
    %147 : int = prim::Constant[value=4](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
    %148 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
    %149 : Device = prim::Constant[value="cpu"](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
    %150 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
    %position_ids : Long(13:1) = aten::arange(%109, %147, %148, %149, %150), scope: __module.transformer # transformers/modeling_flaubert.py:203:0
    %152 : int = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
    %153 : Long(1:13, 13:1) = aten::unsqueeze(%position_ids, %152), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
    %154 : int[] = prim::ListConstruct(%104, %108), scope: __module.transformer
    %155 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
    %input.1 : Long(17:0, 13:1) = aten::expand(%153, %154, %155), scope: __module.transformer # transformers/modeling_flaubert.py:204:0
    %402 : Tensor = prim::CallMethod[name="forward"](%99, %input_ids)
    %403 : Tensor = prim::CallMethod[name="forward"](%98, %input.1)
    %159 : Float(17:26624, 13:2048, 2048:1) = aten::expand_as(%403, %402), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
    %160 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
    %input.2 : Float(17:26624, 13:2048, 2048:1) = aten::add(%402, %159, %160), scope: __module.transformer # transformers/modeling_flaubert.py:231:0
    %404 : Tensor = prim::CallMethod[name="forward"](%97, %input.2)
    %163 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %164 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%404, %163, %164), scope: __module.transformer # torch/nn/functional.py:973:0
    %166 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %167 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %166), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %168 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %169 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %170 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %171 : None = prim::Constant(), scope: __module.transformer
    %172 : Float(17:13, 13:1, 1:1) = aten::to(%167, %168, %169, %170, %171), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %input.4 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%tensor.1, %172), scope: __module.transformer # transformers/modeling_flaubert.py:238:0
    %405 : Tensor = prim::CallMethod[name="forward"](%96, %input.4, %padding_mask)
    %175 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %176 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.1 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%405, %175, %176), scope: __module.transformer # torch/nn/functional.py:973:0
    %178 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.9 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.4, %attn.1, %178), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %406 : Tensor = prim::CallMethod[name="forward"](%94, %input.9)
    %407 : Tensor = prim::CallMethod[name="forward"](%92, %406)
    %182 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.13 : Float(17:26624, 13:2048, 2048:1) = aten::add(%406, %407, %182), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %408 : Tensor = prim::CallMethod[name="forward"](%90, %input.13)
    %185 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %186 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %185), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %187 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %188 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %189 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %190 : None = prim::Constant(), scope: __module.transformer
    %191 : Float(17:13, 13:1, 1:1) = aten::to(%186, %187, %188, %189, %190), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.14 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%408, %191), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %409 : Tensor = prim::CallMethod[name="forward"](%88, %input.14, %padding_mask)
    %194 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %195 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.2 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%409, %194, %195), scope: __module.transformer # torch/nn/functional.py:973:0
    %197 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.19 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.14, %attn.2, %197), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %410 : Tensor = prim::CallMethod[name="forward"](%86, %input.19)
    %411 : Tensor = prim::CallMethod[name="forward"](%84, %410)
    %201 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.23 : Float(17:26624, 13:2048, 2048:1) = aten::add(%410, %411, %201), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %412 : Tensor = prim::CallMethod[name="forward"](%82, %input.23)
    %204 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %205 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %204), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %206 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %207 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %208 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %209 : None = prim::Constant(), scope: __module.transformer
    %210 : Float(17:13, 13:1, 1:1) = aten::to(%205, %206, %207, %208, %209), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.24 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%412, %210), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %413 : Tensor = prim::CallMethod[name="forward"](%80, %input.24, %padding_mask)
    %213 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %214 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.3 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%413, %213, %214), scope: __module.transformer # torch/nn/functional.py:973:0
    %216 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.29 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.24, %attn.3, %216), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %414 : Tensor = prim::CallMethod[name="forward"](%78, %input.29)
    %415 : Tensor = prim::CallMethod[name="forward"](%76, %414)
    %220 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.33 : Float(17:26624, 13:2048, 2048:1) = aten::add(%414, %415, %220), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %416 : Tensor = prim::CallMethod[name="forward"](%74, %input.33)
    %223 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %224 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %223), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %225 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %226 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %227 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %228 : None = prim::Constant(), scope: __module.transformer
    %229 : Float(17:13, 13:1, 1:1) = aten::to(%224, %225, %226, %227, %228), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.34 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%416, %229), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %417 : Tensor = prim::CallMethod[name="forward"](%72, %input.34, %padding_mask)
    %232 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %233 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.4 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%417, %232, %233), scope: __module.transformer # torch/nn/functional.py:973:0
    %235 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.39 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.34, %attn.4, %235), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %418 : Tensor = prim::CallMethod[name="forward"](%70, %input.39)
    %419 : Tensor = prim::CallMethod[name="forward"](%68, %418)
    %239 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.43 : Float(17:26624, 13:2048, 2048:1) = aten::add(%418, %419, %239), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %420 : Tensor = prim::CallMethod[name="forward"](%66, %input.43)
    %242 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %243 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %242), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %244 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %245 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %246 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %247 : None = prim::Constant(), scope: __module.transformer
    %248 : Float(17:13, 13:1, 1:1) = aten::to(%243, %244, %245, %246, %247), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.44 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%420, %248), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %421 : Tensor = prim::CallMethod[name="forward"](%64, %input.44, %padding_mask)
    %251 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %252 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.5 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%421, %251, %252), scope: __module.transformer # torch/nn/functional.py:973:0
    %254 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.49 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.44, %attn.5, %254), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %422 : Tensor = prim::CallMethod[name="forward"](%62, %input.49)
    %423 : Tensor = prim::CallMethod[name="forward"](%60, %422)
    %258 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.53 : Float(17:26624, 13:2048, 2048:1) = aten::add(%422, %423, %258), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %424 : Tensor = prim::CallMethod[name="forward"](%58, %input.53)
    %261 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %262 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %261), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %263 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %264 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %265 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %266 : None = prim::Constant(), scope: __module.transformer
    %267 : Float(17:13, 13:1, 1:1) = aten::to(%262, %263, %264, %265, %266), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.54 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%424, %267), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %425 : Tensor = prim::CallMethod[name="forward"](%56, %input.54, %padding_mask)
    %270 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %271 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.6 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%425, %270, %271), scope: __module.transformer # torch/nn/functional.py:973:0
    %273 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.59 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.54, %attn.6, %273), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %426 : Tensor = prim::CallMethod[name="forward"](%54, %input.59)
    %427 : Tensor = prim::CallMethod[name="forward"](%52, %426)
    %277 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.63 : Float(17:26624, 13:2048, 2048:1) = aten::add(%426, %427, %277), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %428 : Tensor = prim::CallMethod[name="forward"](%50, %input.63)
    %280 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %281 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %280), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %282 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %283 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %284 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %285 : None = prim::Constant(), scope: __module.transformer
    %286 : Float(17:13, 13:1, 1:1) = aten::to(%281, %282, %283, %284, %285), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.64 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%428, %286), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %429 : Tensor = prim::CallMethod[name="forward"](%48, %input.64, %padding_mask)
    %289 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %290 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.7 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%429, %289, %290), scope: __module.transformer # torch/nn/functional.py:973:0
    %292 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.69 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.64, %attn.7, %292), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %430 : Tensor = prim::CallMethod[name="forward"](%46, %input.69)
    %431 : Tensor = prim::CallMethod[name="forward"](%44, %430)
    %296 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.73 : Float(17:26624, 13:2048, 2048:1) = aten::add(%430, %431, %296), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %432 : Tensor = prim::CallMethod[name="forward"](%42, %input.73)
    %299 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %300 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %299), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %301 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %302 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %303 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %304 : None = prim::Constant(), scope: __module.transformer
    %305 : Float(17:13, 13:1, 1:1) = aten::to(%300, %301, %302, %303, %304), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.74 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%432, %305), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %433 : Tensor = prim::CallMethod[name="forward"](%40, %input.74, %padding_mask)
    %308 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %309 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.8 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%433, %308, %309), scope: __module.transformer # torch/nn/functional.py:973:0
    %311 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.79 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.74, %attn.8, %311), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %434 : Tensor = prim::CallMethod[name="forward"](%38, %input.79)
    %435 : Tensor = prim::CallMethod[name="forward"](%36, %434)
    %315 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.83 : Float(17:26624, 13:2048, 2048:1) = aten::add(%434, %435, %315), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %436 : Tensor = prim::CallMethod[name="forward"](%34, %input.83)
    %318 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %319 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %318), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %320 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %321 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %322 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %323 : None = prim::Constant(), scope: __module.transformer
    %324 : Float(17:13, 13:1, 1:1) = aten::to(%319, %320, %321, %322, %323), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.84 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%436, %324), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %437 : Tensor = prim::CallMethod[name="forward"](%32, %input.84, %padding_mask)
    %327 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %328 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.9 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%437, %327, %328), scope: __module.transformer # torch/nn/functional.py:973:0
    %330 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.89 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.84, %attn.9, %330), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %438 : Tensor = prim::CallMethod[name="forward"](%30, %input.89)
    %439 : Tensor = prim::CallMethod[name="forward"](%28, %438)
    %334 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.93 : Float(17:26624, 13:2048, 2048:1) = aten::add(%438, %439, %334), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %440 : Tensor = prim::CallMethod[name="forward"](%26, %input.93)
    %337 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %338 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %337), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %339 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %340 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %341 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %342 : None = prim::Constant(), scope: __module.transformer
    %343 : Float(17:13, 13:1, 1:1) = aten::to(%338, %339, %340, %341, %342), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.94 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%440, %343), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %441 : Tensor = prim::CallMethod[name="forward"](%24, %input.94, %padding_mask)
    %346 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %347 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.10 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%441, %346, %347), scope: __module.transformer # torch/nn/functional.py:973:0
    %349 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.99 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.94, %attn.10, %349), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %442 : Tensor = prim::CallMethod[name="forward"](%22, %input.99)
    %443 : Tensor = prim::CallMethod[name="forward"](%20, %442)
    %353 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.103 : Float(17:26624, 13:2048, 2048:1) = aten::add(%442, %443, %353), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %444 : Tensor = prim::CallMethod[name="forward"](%18, %input.103)
    %356 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %357 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %356), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %358 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %359 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %360 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %361 : None = prim::Constant(), scope: __module.transformer
    %362 : Float(17:13, 13:1, 1:1) = aten::to(%357, %358, %359, %360, %361), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.104 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%444, %362), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %445 : Tensor = prim::CallMethod[name="forward"](%16, %input.104, %padding_mask)
    %365 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %366 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn.11 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%445, %365, %366), scope: __module.transformer # torch/nn/functional.py:973:0
    %368 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.109 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.104, %attn.11, %368), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %446 : Tensor = prim::CallMethod[name="forward"](%14, %input.109)
    %447 : Tensor = prim::CallMethod[name="forward"](%12, %446)
    %372 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.113 : Float(17:26624, 13:2048, 2048:1) = aten::add(%446, %447, %372), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %448 : Tensor = prim::CallMethod[name="forward"](%10, %input.113)
    %375 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %376 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %375), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %377 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %378 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %379 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %380 : None = prim::Constant(), scope: __module.transformer
    %381 : Float(17:13, 13:1, 1:1) = aten::to(%376, %377, %378, %379, %380), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input.114 : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%448, %381), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %449 : Tensor = prim::CallMethod[name="forward"](%8, %input.114, %padding_mask)
    %384 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer # torch/nn/functional.py:973:0
    %385 : bool = prim::Constant[value=0](), scope: __module.transformer # torch/nn/functional.py:973:0
    %attn : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%449, %384, %385), scope: __module.transformer # torch/nn/functional.py:973:0
    %387 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %input.119 : Float(17:26624, 13:2048, 2048:1) = aten::add(%input.114, %attn, %387), scope: __module.transformer # transformers/modeling_flaubert.py:265:0
    %450 : Tensor = prim::CallMethod[name="forward"](%6, %input.119)
    %451 : Tensor = prim::CallMethod[name="forward"](%4, %450)
    %391 : int = prim::Constant[value=1](), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %input.123 : Float(17:26624, 13:2048, 2048:1) = aten::add(%450, %451, %391), scope: __module.transformer # transformers/modeling_flaubert.py:285:0
    %452 : Tensor = prim::CallMethod[name="forward"](%2, %input.123)
    %394 : int = prim::Constant[value=-1](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %395 : Long(17:13, 13:1, 1:1) = aten::unsqueeze(%padding_mask, %394), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %396 : int = prim::Constant[value=6](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %397 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %398 : bool = prim::Constant[value=0](), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %399 : None = prim::Constant(), scope: __module.transformer
    %400 : Float(17:13, 13:1, 1:1) = aten::to(%395, %396, %397, %398, %399), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    %input : Float(17:26624, 13:2048, 2048:1) = aten::mul_(%452, %400), scope: __module.transformer # transformers/modeling_flaubert.py:291:0
    return (%input)

FlaubertModel.embeddings
Embedding._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.3)
    %8 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
    %9 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
    %10 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%2, %input_ids, %8, %9, %10), scope: __module.transformer/__module.transformer.embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

FlaubertModel.layer_norm_emb
LayerNorm._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.2 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.5)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm_emb
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
    %input.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.2, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm_emb # torch/nn/functional.py:2048:0
    return (%input.3)

FlaubertModel.position_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(17:0, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
    %6 : Float(17:26624, 13:2048, 2048:1) = aten::embedding(%2, %input.1, %3, %4, %5), scope: __module.transformer/__module.transformer.position_embeddings # torch/nn/functional.py:1814:0
    return (%6)

ModuleList.*
  module had no methods with graph attrs.

MultiHeadAttention._actual_script_module
  graph(%self.6 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.4 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.6)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.6)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.6)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.6)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.4, %7), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
    %bs.2 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.0
    %10 : int = aten::Int(%bs.2), scope: __module.transformer/__module.transformer.attentions.0
    %11 : int = aten::Int(%bs.2), scope: __module.transformer/__module.transformer.attentions.0
    %12 : int = aten::Int(%bs.2), scope: __module.transformer/__module.transformer.attentions.0
    %13 : int = aten::Int(%bs.2), scope: __module.transformer/__module.transformer.attentions.0
    %14 : int = aten::Int(%bs.2), scope: __module.transformer/__module.transformer.attentions.0
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.4, %15), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:151:0
    %qlen.1 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.0
    %18 : int = aten::Int(%qlen.1), scope: __module.transformer/__module.transformer.attentions.0
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.4)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.0
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %q.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.4)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.0
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %k.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.4)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.0
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %v.1 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
    %q.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.1, %49), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.1, %51, %52), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
    %scores.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.2, %53), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.0 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.0
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
    %mask.1 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
    %scores.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.1, %mask.1, %62), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.0
    %input.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.2, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.0
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.5, %69, %70), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:1498:0
    %input.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.5), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
    %weights.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.6, %73, %74), scope: __module.transformer/__module.transformer.attentions.0 # torch/nn/functional.py:973:0
    %x.4 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.1, %v.1), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.4, %77, %78), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.0
    %input.7 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.0 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.7)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.8)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.8)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %output.2 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
    %x.2 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.2, %2, %6), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.k_lin # torch/nn/functional.py:1678:0
    return (%x.2)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.10 : __torch__.torch.nn.modules.linear.Linear,
        %input.7 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.10)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.10)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %output.4 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.7, %4), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
    %input.8 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.4, %2, %6), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.out_lin # torch/nn/functional.py:1678:0
    return (%input.8)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.7)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %output.1 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
    %x.1 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.1, %2, %6), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.q_lin # torch/nn/functional.py:1678:0
    return (%x.1)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %input.4 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %output.3 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.4, %4), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
    %x.3 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.3, %2, %6), scope: __module.transformer/__module.transformer.attentions.0/__module.transformer.attentions.0.v_lin # torch/nn/functional.py:1678:0
    return (%x.3)

MultiHeadAttention._actual_script_module
  graph(%self.16 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.14 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.16)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.16)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.16)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.16)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.14, %7), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
    %bs.3 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.1
    %10 : int = aten::Int(%bs.3), scope: __module.transformer/__module.transformer.attentions.1
    %11 : int = aten::Int(%bs.3), scope: __module.transformer/__module.transformer.attentions.1
    %12 : int = aten::Int(%bs.3), scope: __module.transformer/__module.transformer.attentions.1
    %13 : int = aten::Int(%bs.3), scope: __module.transformer/__module.transformer.attentions.1
    %14 : int = aten::Int(%bs.3), scope: __module.transformer/__module.transformer.attentions.1
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.14, %15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:151:0
    %qlen.2 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.1
    %18 : int = aten::Int(%qlen.2), scope: __module.transformer/__module.transformer.attentions.1
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.14)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.1
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %q.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.14)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.1
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %k.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.14)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.1
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %v.2 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
    %q.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.3, %49), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.2, %51, %52), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
    %scores.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.4, %53), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.1 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.1
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
    %mask.2 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.3), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
    %scores.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.3, %mask.2, %62), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.1
    %input.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.4, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.1
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.15, %69, %70), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:1498:0
    %input.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.15), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
    %weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.16, %73, %74), scope: __module.transformer/__module.transformer.attentions.1 # torch/nn/functional.py:973:0
    %x.8 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.2, %v.2), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.8, %77, %78), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.1
    %input.17 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.1 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.17)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.18)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.18)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %output.8 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
    %x.6 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.8, %2, %6), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.k_lin # torch/nn/functional.py:1678:0
    return (%x.6)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.linear.Linear,
        %input.17 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %output.10 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.17, %4), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
    %input.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.10, %2, %6), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.out_lin # torch/nn/functional.py:1678:0
    return (%input.18)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.17)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.17)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %output.7 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
    %x.5 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.7, %2, %6), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.q_lin # torch/nn/functional.py:1678:0
    return (%x.5)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %input.14 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %output.9 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.14, %4), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
    %x.7 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.9, %2, %6), scope: __module.transformer/__module.transformer.attentions.1/__module.transformer.attentions.1.v_lin # torch/nn/functional.py:1678:0
    return (%x.7)

MultiHeadAttention._actual_script_module
  graph(%self.26 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.24 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.26)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.26)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.26)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.26)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.24, %7), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
    %bs.4 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.2
    %10 : int = aten::Int(%bs.4), scope: __module.transformer/__module.transformer.attentions.2
    %11 : int = aten::Int(%bs.4), scope: __module.transformer/__module.transformer.attentions.2
    %12 : int = aten::Int(%bs.4), scope: __module.transformer/__module.transformer.attentions.2
    %13 : int = aten::Int(%bs.4), scope: __module.transformer/__module.transformer.attentions.2
    %14 : int = aten::Int(%bs.4), scope: __module.transformer/__module.transformer.attentions.2
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.24, %15), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:151:0
    %qlen.3 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.2
    %18 : int = aten::Int(%qlen.3), scope: __module.transformer/__module.transformer.attentions.2
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.24)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.2
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %q.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.24)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.2
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %k.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.2
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %v.3 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
    %q.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.5, %49), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.3, %51, %52), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
    %scores.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.6, %53), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.2 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.2
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
    %mask.3 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.5), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
    %scores.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.5, %mask.3, %62), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.2
    %input.25 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.6, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.2
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.25, %69, %70), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:1498:0
    %input.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.25), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
    %weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.26, %73, %74), scope: __module.transformer/__module.transformer.attentions.2 # torch/nn/functional.py:973:0
    %x.12 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.3, %v.3), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.12, %77, %78), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.2
    %input.27 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.2 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.27)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.28 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.28)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.28)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %output.14 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
    %x.10 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.14, %2, %6), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.k_lin # torch/nn/functional.py:1678:0
    return (%x.10)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.30 : __torch__.torch.nn.modules.linear.Linear,
        %input.27 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.30)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.30)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %output.16 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.27, %4), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
    %input.28 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.16, %2, %6), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.out_lin # torch/nn/functional.py:1678:0
    return (%input.28)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %output.13 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
    %x.9 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.13, %2, %6), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.q_lin # torch/nn/functional.py:1678:0
    return (%x.9)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.linear.Linear,
        %input.24 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.29)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.29)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %output.15 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.24, %4), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
    %x.11 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.15, %2, %6), scope: __module.transformer/__module.transformer.attentions.2/__module.transformer.attentions.2.v_lin # torch/nn/functional.py:1678:0
    return (%x.11)

MultiHeadAttention._actual_script_module
  graph(%self.36 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.34 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.36)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.36)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.36)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.36)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.34, %7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
    %bs.5 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.3
    %10 : int = aten::Int(%bs.5), scope: __module.transformer/__module.transformer.attentions.3
    %11 : int = aten::Int(%bs.5), scope: __module.transformer/__module.transformer.attentions.3
    %12 : int = aten::Int(%bs.5), scope: __module.transformer/__module.transformer.attentions.3
    %13 : int = aten::Int(%bs.5), scope: __module.transformer/__module.transformer.attentions.3
    %14 : int = aten::Int(%bs.5), scope: __module.transformer/__module.transformer.attentions.3
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.34, %15), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:151:0
    %qlen.4 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.3
    %18 : int = aten::Int(%qlen.4), scope: __module.transformer/__module.transformer.attentions.3
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.34)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.3
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %q.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.34)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.3
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %k.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.34)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.3
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %v.4 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
    %q.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.7, %49), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.4, %51, %52), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
    %scores.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.8, %53), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.3 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.3
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
    %mask.4 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.7), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
    %scores.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.7, %mask.4, %62), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.3
    %input.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.8, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.3
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.35, %69, %70), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:1498:0
    %input.36 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.35), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
    %weights.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.36, %73, %74), scope: __module.transformer/__module.transformer.attentions.3 # torch/nn/functional.py:973:0
    %x.16 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.4, %v.4), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.16, %77, %78), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.3
    %input.37 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.3 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.37)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %output.20 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
    %x.14 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.20, %2, %6), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.k_lin # torch/nn/functional.py:1678:0
    return (%x.14)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.linear.Linear,
        %input.37 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %output.22 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.37, %4), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
    %input.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.22, %2, %6), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.out_lin # torch/nn/functional.py:1678:0
    return (%input.38)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %output.19 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
    %x.13 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.19, %2, %6), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.q_lin # torch/nn/functional.py:1678:0
    return (%x.13)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.39 : __torch__.torch.nn.modules.linear.Linear,
        %input.34 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.39)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.39)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %output.21 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.34, %4), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
    %x.15 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.21, %2, %6), scope: __module.transformer/__module.transformer.attentions.3/__module.transformer.attentions.3.v_lin # torch/nn/functional.py:1678:0
    return (%x.15)

MultiHeadAttention._actual_script_module
  graph(%self.46 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.44 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.46)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.46)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.46)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.46)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.44, %7), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
    %bs.6 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.4
    %10 : int = aten::Int(%bs.6), scope: __module.transformer/__module.transformer.attentions.4
    %11 : int = aten::Int(%bs.6), scope: __module.transformer/__module.transformer.attentions.4
    %12 : int = aten::Int(%bs.6), scope: __module.transformer/__module.transformer.attentions.4
    %13 : int = aten::Int(%bs.6), scope: __module.transformer/__module.transformer.attentions.4
    %14 : int = aten::Int(%bs.6), scope: __module.transformer/__module.transformer.attentions.4
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.44, %15), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:151:0
    %qlen.5 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.4
    %18 : int = aten::Int(%qlen.5), scope: __module.transformer/__module.transformer.attentions.4
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.44)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.4
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %q.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.44)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.4
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %k.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.4
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %v.5 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
    %q.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.9, %49), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.5, %51, %52), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
    %scores.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.10, %53), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.4 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.4
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
    %mask.5 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.9), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
    %scores.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.9, %mask.5, %62), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.4
    %input.45 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.10, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.4
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.45, %69, %70), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:1498:0
    %input.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.45), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
    %weights.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.46, %73, %74), scope: __module.transformer/__module.transformer.attentions.4 # torch/nn/functional.py:973:0
    %x.20 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.5, %v.5), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.20, %77, %78), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.4
    %input.47 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.4 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.47)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %output.26 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
    %x.18 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.26, %2, %6), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.k_lin # torch/nn/functional.py:1678:0
    return (%x.18)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %input.47 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %output.28 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.47, %4), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
    %input.48 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.28, %2, %6), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.out_lin # torch/nn/functional.py:1678:0
    return (%input.48)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.47 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.47)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.47)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %output.25 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
    %x.17 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.25, %2, %6), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.q_lin # torch/nn/functional.py:1678:0
    return (%x.17)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.linear.Linear,
        %input.44 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %output.27 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.44, %4), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
    %x.19 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.27, %2, %6), scope: __module.transformer/__module.transformer.attentions.4/__module.transformer.attentions.4.v_lin # torch/nn/functional.py:1678:0
    return (%x.19)

MultiHeadAttention._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.54 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.56)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.56)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.56)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.56)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.54, %7), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
    %bs.7 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.5
    %10 : int = aten::Int(%bs.7), scope: __module.transformer/__module.transformer.attentions.5
    %11 : int = aten::Int(%bs.7), scope: __module.transformer/__module.transformer.attentions.5
    %12 : int = aten::Int(%bs.7), scope: __module.transformer/__module.transformer.attentions.5
    %13 : int = aten::Int(%bs.7), scope: __module.transformer/__module.transformer.attentions.5
    %14 : int = aten::Int(%bs.7), scope: __module.transformer/__module.transformer.attentions.5
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.54, %15), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:151:0
    %qlen.6 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.5
    %18 : int = aten::Int(%qlen.6), scope: __module.transformer/__module.transformer.attentions.5
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.54)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.5
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %q.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.54)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.5
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %k.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.54)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.5
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %v.6 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
    %q.12 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.11, %49), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.6, %51, %52), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
    %scores.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.12, %53), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.5 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.5
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
    %mask.6 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.11), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
    %scores.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.11, %mask.6, %62), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.5
    %input.55 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.12, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.5
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.55, %69, %70), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:1498:0
    %input.56 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.55), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
    %weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.56, %73, %74), scope: __module.transformer/__module.transformer.attentions.5 # torch/nn/functional.py:973:0
    %x.24 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.6, %v.6), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.24, %77, %78), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.5
    %input.57 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.5 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.57)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.58 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.58)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.58)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %output.32 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
    %x.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.32, %2, %6), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.k_lin # torch/nn/functional.py:1678:0
    return (%x.22)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.linear.Linear,
        %input.57 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.60)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.60)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %output.34 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.57, %4), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
    %input.58 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.34, %2, %6), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.out_lin # torch/nn/functional.py:1678:0
    return (%input.58)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %output.31 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
    %x.21 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.31, %2, %6), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.q_lin # torch/nn/functional.py:1678:0
    return (%x.21)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %input.54 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %output.33 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.54, %4), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
    %x.23 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.33, %2, %6), scope: __module.transformer/__module.transformer.attentions.5/__module.transformer.attentions.5.v_lin # torch/nn/functional.py:1678:0
    return (%x.23)

MultiHeadAttention._actual_script_module
  graph(%self.66 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.64 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.66)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.66)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.66)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.66)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.64, %7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
    %bs.8 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.6
    %10 : int = aten::Int(%bs.8), scope: __module.transformer/__module.transformer.attentions.6
    %11 : int = aten::Int(%bs.8), scope: __module.transformer/__module.transformer.attentions.6
    %12 : int = aten::Int(%bs.8), scope: __module.transformer/__module.transformer.attentions.6
    %13 : int = aten::Int(%bs.8), scope: __module.transformer/__module.transformer.attentions.6
    %14 : int = aten::Int(%bs.8), scope: __module.transformer/__module.transformer.attentions.6
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.64, %15), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:151:0
    %qlen.7 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.6
    %18 : int = aten::Int(%qlen.7), scope: __module.transformer/__module.transformer.attentions.6
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.64)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.6
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %q.13 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.64)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.6
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %k.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.64)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.6
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %v.7 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
    %q.14 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.13, %49), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.7, %51, %52), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
    %scores.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.14, %53), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.6 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.6
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
    %mask.7 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.13), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
    %scores.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.13, %mask.7, %62), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.6
    %input.65 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.14, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.6
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.65, %69, %70), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:1498:0
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.65), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
    %weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.66, %73, %74), scope: __module.transformer/__module.transformer.attentions.6 # torch/nn/functional.py:973:0
    %x.28 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.7, %v.7), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.28, %77, %78), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.6
    %input.67 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.6 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.67)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %output.38 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
    %x.26 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.38, %2, %6), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.k_lin # torch/nn/functional.py:1678:0
    return (%x.26)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.linear.Linear,
        %input.67 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %output.40 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.67, %4), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
    %input.68 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.40, %2, %6), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.out_lin # torch/nn/functional.py:1678:0
    return (%input.68)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %output.37 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
    %x.25 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.37, %2, %6), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.q_lin # torch/nn/functional.py:1678:0
    return (%x.25)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.69 : __torch__.torch.nn.modules.linear.Linear,
        %input.64 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.69)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.69)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %output.39 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.64, %4), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
    %x.27 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.39, %2, %6), scope: __module.transformer/__module.transformer.attentions.6/__module.transformer.attentions.6.v_lin # torch/nn/functional.py:1678:0
    return (%x.27)

MultiHeadAttention._actual_script_module
  graph(%self.76 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.74 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.76)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.76)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.76)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.76)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.74, %7), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
    %bs.9 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.7
    %10 : int = aten::Int(%bs.9), scope: __module.transformer/__module.transformer.attentions.7
    %11 : int = aten::Int(%bs.9), scope: __module.transformer/__module.transformer.attentions.7
    %12 : int = aten::Int(%bs.9), scope: __module.transformer/__module.transformer.attentions.7
    %13 : int = aten::Int(%bs.9), scope: __module.transformer/__module.transformer.attentions.7
    %14 : int = aten::Int(%bs.9), scope: __module.transformer/__module.transformer.attentions.7
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.74, %15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:151:0
    %qlen.8 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.7
    %18 : int = aten::Int(%qlen.8), scope: __module.transformer/__module.transformer.attentions.7
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.74)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.7
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %q.15 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.74)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.7
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %k.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.74)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.7
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %v.8 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
    %q.16 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.15, %49), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.8, %51, %52), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
    %scores.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.16, %53), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.7 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.7
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
    %mask.8 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.15), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
    %scores.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.15, %mask.8, %62), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.7
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.16, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.7
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %69, %70), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:1498:0
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.75), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
    %weights.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %73, %74), scope: __module.transformer/__module.transformer.attentions.7 # torch/nn/functional.py:973:0
    %x.32 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.8, %v.8), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.32, %77, %78), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.7
    %input.77 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.7 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.77)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %output.44 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
    %x.30 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.44, %2, %6), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.k_lin # torch/nn/functional.py:1678:0
    return (%x.30)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.80 : __torch__.torch.nn.modules.linear.Linear,
        %input.77 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.80)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.80)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %output.46 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.77, %4), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
    %input.78 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.46, %2, %6), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.out_lin # torch/nn/functional.py:1678:0
    return (%input.78)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %output.43 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
    %x.29 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.43, %2, %6), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.q_lin # torch/nn/functional.py:1678:0
    return (%x.29)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %input.74 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %output.45 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.74, %4), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
    %x.31 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.45, %2, %6), scope: __module.transformer/__module.transformer.attentions.7/__module.transformer.attentions.7.v_lin # torch/nn/functional.py:1678:0
    return (%x.31)

MultiHeadAttention._actual_script_module
  graph(%self.86 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.84 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.86)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.86)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.86)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.86)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.84, %7), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
    %bs.10 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.8
    %10 : int = aten::Int(%bs.10), scope: __module.transformer/__module.transformer.attentions.8
    %11 : int = aten::Int(%bs.10), scope: __module.transformer/__module.transformer.attentions.8
    %12 : int = aten::Int(%bs.10), scope: __module.transformer/__module.transformer.attentions.8
    %13 : int = aten::Int(%bs.10), scope: __module.transformer/__module.transformer.attentions.8
    %14 : int = aten::Int(%bs.10), scope: __module.transformer/__module.transformer.attentions.8
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.84, %15), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:151:0
    %qlen.9 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.8
    %18 : int = aten::Int(%qlen.9), scope: __module.transformer/__module.transformer.attentions.8
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.84)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.8
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %q.17 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.84)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.8
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %k.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.84)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.8
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %v.9 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
    %q.18 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.17, %49), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.9, %51, %52), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
    %scores.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.18, %53), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.8 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.8
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
    %mask.9 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.17), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
    %scores.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.17, %mask.9, %62), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.8
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.18, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.8
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.85, %69, %70), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:1498:0
    %input.86 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.85), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
    %weights.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.86, %73, %74), scope: __module.transformer/__module.transformer.attentions.8 # torch/nn/functional.py:973:0
    %x.36 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.9, %v.9), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.36, %77, %78), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.8
    %input.87 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.8 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.87)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %output.50 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
    %x.34 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.50, %2, %6), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.k_lin # torch/nn/functional.py:1678:0
    return (%x.34)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.linear.Linear,
        %input.87 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %output.52 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.87, %4), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
    %input.88 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.52, %2, %6), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.out_lin # torch/nn/functional.py:1678:0
    return (%input.88)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %output.49 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
    %x.33 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.49, %2, %6), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.q_lin # torch/nn/functional.py:1678:0
    return (%x.33)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.linear.Linear,
        %input.84 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %output.51 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.84, %4), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
    %x.35 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.51, %2, %6), scope: __module.transformer/__module.transformer.attentions.8/__module.transformer.attentions.8.v_lin # torch/nn/functional.py:1678:0
    return (%x.35)

MultiHeadAttention._actual_script_module
  graph(%self.96 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.94 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.96)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.96)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.96)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.96)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.94, %7), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
    %bs.11 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.9
    %10 : int = aten::Int(%bs.11), scope: __module.transformer/__module.transformer.attentions.9
    %11 : int = aten::Int(%bs.11), scope: __module.transformer/__module.transformer.attentions.9
    %12 : int = aten::Int(%bs.11), scope: __module.transformer/__module.transformer.attentions.9
    %13 : int = aten::Int(%bs.11), scope: __module.transformer/__module.transformer.attentions.9
    %14 : int = aten::Int(%bs.11), scope: __module.transformer/__module.transformer.attentions.9
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.94, %15), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:151:0
    %qlen.10 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.9
    %18 : int = aten::Int(%qlen.10), scope: __module.transformer/__module.transformer.attentions.9
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.94)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.9
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %q.19 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.94)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.9
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %k.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.94)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.9
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %v.10 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
    %q.20 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.19, %49), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.10, %51, %52), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
    %scores.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.20, %53), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.9 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.9
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
    %mask.10 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.19), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
    %scores.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.19, %mask.10, %62), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.9
    %input.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.20, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.9
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.95, %69, %70), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:1498:0
    %input.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.95), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
    %weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.96, %73, %74), scope: __module.transformer/__module.transformer.attentions.9 # torch/nn/functional.py:973:0
    %x.40 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.10, %v.10), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.40, %77, %78), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.9
    %input.97 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.9 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.97)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.98 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.98)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.98)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %output.56 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
    %x.38 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.56, %2, %6), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.k_lin # torch/nn/functional.py:1678:0
    return (%x.38)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %input.97 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %output.58 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.97, %4), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
    %input.98 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.58, %2, %6), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.out_lin # torch/nn/functional.py:1678:0
    return (%input.98)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %output.55 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
    %x.37 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.55, %2, %6), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.q_lin # torch/nn/functional.py:1678:0
    return (%x.37)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %input.94 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %output.57 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.94, %4), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
    %x.39 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.57, %2, %6), scope: __module.transformer/__module.transformer.attentions.9/__module.transformer.attentions.9.v_lin # torch/nn/functional.py:1678:0
    return (%x.39)

MultiHeadAttention._actual_script_module
  graph(%self.106 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.104 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.106)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.106)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.106)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.106)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.104, %7), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
    %bs.12 : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.10
    %10 : int = aten::Int(%bs.12), scope: __module.transformer/__module.transformer.attentions.10
    %11 : int = aten::Int(%bs.12), scope: __module.transformer/__module.transformer.attentions.10
    %12 : int = aten::Int(%bs.12), scope: __module.transformer/__module.transformer.attentions.10
    %13 : int = aten::Int(%bs.12), scope: __module.transformer/__module.transformer.attentions.10
    %14 : int = aten::Int(%bs.12), scope: __module.transformer/__module.transformer.attentions.10
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.104, %15), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:151:0
    %qlen.11 : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.10
    %18 : int = aten::Int(%qlen.11), scope: __module.transformer/__module.transformer.attentions.10
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.104)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.10
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %q.21 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.104)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.10
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %k.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.104)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.10
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %v.11 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
    %q.22 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.21, %49), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k.11, %51, %52), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
    %scores.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q.22, %53), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.10 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.10
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
    %mask.11 : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.21), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
    %scores.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.21, %mask.11, %62), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.10
    %input.105 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores.22, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.10
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.105, %69, %70), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:1498:0
    %input.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.105), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
    %weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.106, %73, %74), scope: __module.transformer/__module.transformer.attentions.10 # torch/nn/functional.py:973:0
    %x.44 : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights.11, %v.11), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x.44, %77, %78), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.10
    %input.107 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.10 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.107)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %output.62 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
    %x.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.62, %2, %6), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.k_lin # torch/nn/functional.py:1678:0
    return (%x.42)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %input.107 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %output.64 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.107, %4), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
    %input.108 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.64, %2, %6), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.out_lin # torch/nn/functional.py:1678:0
    return (%input.108)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.107 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.107)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.107)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %output.61 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
    %x.41 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.61, %2, %6), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.q_lin # torch/nn/functional.py:1678:0
    return (%x.41)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.109 : __torch__.torch.nn.modules.linear.Linear,
        %input.104 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.109)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.109)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %output.63 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.104, %4), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
    %x.43 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.63, %2, %6), scope: __module.transformer/__module.transformer.attentions.10/__module.transformer.attentions.10.v_lin # torch/nn/functional.py:1678:0
    return (%x.43)

MultiHeadAttention._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_xlm.MultiHeadAttention,
        %input.114 : Float(17:26624, 13:2048, 2048:1),
        %padding_mask : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_lin"](%self.116)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="v_lin"](%self.116)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="k_lin"](%self.116)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="q_lin"](%self.116)
    %7 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
    %8 : int = aten::size(%input.114, %7), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
    %bs : Long() = prim::NumToTensor(%8), scope: __module.transformer/__module.transformer.attentions.11
    %10 : int = aten::Int(%bs), scope: __module.transformer/__module.transformer.attentions.11
    %11 : int = aten::Int(%bs), scope: __module.transformer/__module.transformer.attentions.11
    %12 : int = aten::Int(%bs), scope: __module.transformer/__module.transformer.attentions.11
    %13 : int = aten::Int(%bs), scope: __module.transformer/__module.transformer.attentions.11
    %14 : int = aten::Int(%bs), scope: __module.transformer/__module.transformer.attentions.11
    %15 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
    %16 : int = aten::size(%input.114, %15), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:151:0
    %qlen : Long() = prim::NumToTensor(%16), scope: __module.transformer/__module.transformer.attentions.11
    %18 : int = aten::Int(%qlen), scope: __module.transformer/__module.transformer.attentions.11
    %87 : Tensor = prim::CallMethod[name="forward"](%6, %input.114)
    %23 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %24 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %25 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %26 : int[] = prim::ListConstruct(%14, %23, %24, %25), scope: __module.transformer/__module.transformer.attentions.11
    %27 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%87, %26), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %28 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %29 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %q.23 : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%27, %28, %29), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %88 : Tensor = prim::CallMethod[name="forward"](%5, %input.114)
    %32 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %33 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %34 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %35 : int[] = prim::ListConstruct(%13, %32, %33, %34), scope: __module.transformer/__module.transformer.attentions.11
    %36 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%88, %35), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %37 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %38 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %k : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%36, %37, %38), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %89 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %41 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %42 : int = prim::Constant[value=16](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %43 : int = prim::Constant[value=128](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %44 : int[] = prim::ListConstruct(%12, %41, %42, %43), scope: __module.transformer/__module.transformer.attentions.11
    %45 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::view(%89, %44), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %46 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %47 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %v : Float(17:26624, 16:128, 13:2048, 128:1) = aten::transpose(%45, %46, %47), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:163:0
    %49 : Double() = prim::Constant[value={11.3137}](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
    %q : Float(17:26624, 16:128, 13:2048, 128:1) = aten::div(%q.23, %49), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:188:0
    %51 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
    %52 : int = prim::Constant[value=3](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
    %53 : Float(17:26624, 16:128, 128:1, 13:2048) = aten::transpose(%k, %51, %52), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
    %scores.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::matmul(%q, %53), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:189:0
    %55 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
    %56 : Bool(17:13, 13:1) = aten::eq(%padding_mask, %55), scope: __module.transformer/__module.transformer.attentions.11 # torch/tensor.py:22:0
    %57 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
    %58 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
    %59 : int[] = prim::ListConstruct(%11, %57, %58, %18), scope: __module.transformer/__module.transformer.attentions.11
    %60 : Bool(17:13, 1:13, 1:13, 13:1) = aten::view(%56, %59), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
    %mask : Bool(17:13, 16:0, 13:0, 13:1) = aten::expand_as(%60, %scores.23), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:190:0
    %62 : float = prim::Constant[value=-inf](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
    %scores : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill_(%scores.23, %mask, %62), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:191:0
    %64 : int = prim::Constant[value=6](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
    %65 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
    %66 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
    %67 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.11
    %input.115 : Float(17:2704, 16:169, 13:13, 13:1) = aten::to(%scores, %64, %65, %66, %67), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
    %69 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
    %70 : None = prim::Constant(), scope: __module.transformer/__module.transformer.attentions.11
    %71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.115, %69, %70), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:1498:0
    %input.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::type_as(%71, %input.115), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:193:0
    %73 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
    %74 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
    %weights : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.116, %73, %74), scope: __module.transformer/__module.transformer.attentions.11 # torch/nn/functional.py:973:0
    %x : Float(17:26624, 16:1664, 13:128, 128:1) = aten::matmul(%weights, %v), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:200:0
    %77 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %78 : int = prim::Constant[value=2](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %79 : Float(17:26624, 13:128, 16:1664, 128:1) = aten::transpose(%x, %77, %78), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %80 : int = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %81 : Float(17:26624, 13:2048, 16:128, 128:1) = aten::contiguous(%79, %80), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %82 : int = prim::Constant[value=-1](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %83 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %84 : int[] = prim::ListConstruct(%10, %82, %83), scope: __module.transformer/__module.transformer.attentions.11
    %input.117 : Float(17:26624, 13:2048, 2048:1) = aten::view(%81, %84), scope: __module.transformer/__module.transformer.attentions.11 # transformers/modeling_xlm.py:167:0
    %90 : Tensor = prim::CallMethod[name="forward"](%3, %input.117)
    return (%90)

MultiHeadAttention.k_lin
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %output.68 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
    %x.46 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.68, %2, %6), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.k_lin # torch/nn/functional.py:1678:0
    return (%x.46)

MultiHeadAttention.out_lin
Linear._actual_script_module
  graph(%self.120 : __torch__.torch.nn.modules.linear.Linear,
        %input.117 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.120)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.120)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %output.70 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.117, %4), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
    %input.118 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.70, %2, %6), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.out_lin # torch/nn/functional.py:1678:0
    return (%input.118)

MultiHeadAttention.q_lin
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.117)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %output.67 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
    %x.45 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.67, %2, %6), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.q_lin # torch/nn/functional.py:1678:0
    return (%x.45)

MultiHeadAttention.v_lin
Linear._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.linear.Linear,
        %input.114 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : Float(2048:1, 2048:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %output.69 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.114, %4), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
    %x.47 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.69, %2, %6), scope: __module.transformer/__module.transformer.attentions.11/__module.transformer.attentions.11.v_lin # torch/nn/functional.py:1678:0
    return (%x.47)

LayerNorm._actual_script_module
  graph(%self.11 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.9 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.11)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.11)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.0
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.9, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.0 # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

LayerNorm._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.19 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.1
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.19, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.1 # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

LayerNorm._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.2
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.29, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.2 # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

LayerNorm._actual_script_module
  graph(%self.41 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.39 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.41)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.41)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.3
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.39, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.3 # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

LayerNorm._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.49 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.51)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.51)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.4
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.49, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.4 # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

LayerNorm._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.59 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.5
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.59, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.5 # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

LayerNorm._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.71)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.71)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.6
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.6 # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

LayerNorm._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.79 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.7
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.79, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.7 # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

LayerNorm._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.89 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.8
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.89, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.8 # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

LayerNorm._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.99 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.101)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.101)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.9
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.99, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.9 # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

LayerNorm._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.109 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.111)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.111)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.10
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.109, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.10 # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

LayerNorm._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm1.11
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.119, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm1.11 # torch/nn/functional.py:2048:0
    return (%input_tensor)

TransformerFFN._actual_script_module
  graph(%self.12 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.12)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.12)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.11 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.11)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.0 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %output.5 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
    %input.10 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.5, %2, %6), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin1 # torch/nn/functional.py:1678:0
    return (%input.10)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %input.11 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %output.6 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.11, %4), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
    %input.12 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.6, %2, %6), scope: __module.transformer/__module.transformer.ffns.0/__module.transformer.ffns.0.lin2 # torch/nn/functional.py:1678:0
    return (%input.12)

TransformerFFN._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.22)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.22)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.21 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.21)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.1 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %output.11 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
    %input.20 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.11, %2, %6), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin1 # torch/nn/functional.py:1678:0
    return (%input.20)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.linear.Linear,
        %input.21 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %output.12 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.21, %4), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
    %input.22 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.12, %2, %6), scope: __module.transformer/__module.transformer.ffns.1/__module.transformer.ffns.1.lin2 # torch/nn/functional.py:1678:0
    return (%input.22)

TransformerFFN._actual_script_module
  graph(%self.32 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.32)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.32)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.31 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.31)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.2 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %output.17 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
    %input.30 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.17, %2, %6), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin1 # torch/nn/functional.py:1678:0
    return (%input.30)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.linear.Linear,
        %input.31 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.34)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.34)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %output.18 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.31, %4), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
    %input.32 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.18, %2, %6), scope: __module.transformer/__module.transformer.ffns.2/__module.transformer.ffns.2.lin2 # torch/nn/functional.py:1678:0
    return (%input.32)

TransformerFFN._actual_script_module
  graph(%self.42 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.42)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.42)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.41 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.41)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.3 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.43)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.43)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %output.23 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
    %input.40 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.23, %2, %6), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin1 # torch/nn/functional.py:1678:0
    return (%input.40)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %input.41 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %output.24 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.41, %4), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
    %input.42 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.24, %2, %6), scope: __module.transformer/__module.transformer.ffns.3/__module.transformer.ffns.3.lin2 # torch/nn/functional.py:1678:0
    return (%input.42)

TransformerFFN._actual_script_module
  graph(%self.52 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.52)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.52)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.51 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.51)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.4 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %output.29 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
    %input.50 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.29, %2, %6), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin1 # torch/nn/functional.py:1678:0
    return (%input.50)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.linear.Linear,
        %input.51 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.54)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.54)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %output.30 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.51, %4), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
    %input.52 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.30, %2, %6), scope: __module.transformer/__module.transformer.ffns.4/__module.transformer.ffns.4.lin2 # torch/nn/functional.py:1678:0
    return (%input.52)

TransformerFFN._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.62)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.62)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.61 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.61)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.5 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.63 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.63)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.63)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %output.35 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
    %input.60 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.35, %2, %6), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin1 # torch/nn/functional.py:1678:0
    return (%input.60)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.linear.Linear,
        %input.61 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %output.36 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.61, %4), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
    %input.62 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.36, %2, %6), scope: __module.transformer/__module.transformer.ffns.5/__module.transformer.ffns.5.lin2 # torch/nn/functional.py:1678:0
    return (%input.62)

TransformerFFN._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.72)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.72)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.71 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.71)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.6 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.73)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.73)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %output.41 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
    %input.70 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.41, %2, %6), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin1 # torch/nn/functional.py:1678:0
    return (%input.70)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %input.71 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %output.42 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.71, %4), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
    %input.72 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.42, %2, %6), scope: __module.transformer/__module.transformer.ffns.6/__module.transformer.ffns.6.lin2 # torch/nn/functional.py:1678:0
    return (%input.72)

TransformerFFN._actual_script_module
  graph(%self.82 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.82)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.82)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.81 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.81)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.7 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %output.47 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
    %input.80 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.47, %2, %6), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin1 # torch/nn/functional.py:1678:0
    return (%input.80)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %input.81 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %output.48 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.81, %4), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
    %input.82 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.48, %2, %6), scope: __module.transformer/__module.transformer.ffns.7/__module.transformer.ffns.7.lin2 # torch/nn/functional.py:1678:0
    return (%input.82)

TransformerFFN._actual_script_module
  graph(%self.92 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.92)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.92)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.91 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.91)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.8 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %output.53 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
    %input.90 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.53, %2, %6), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin1 # torch/nn/functional.py:1678:0
    return (%input.90)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.linear.Linear,
        %input.91 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.94)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.94)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %output.54 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.91, %4), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
    %input.92 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.54, %2, %6), scope: __module.transformer/__module.transformer.ffns.8/__module.transformer.ffns.8.lin2 # torch/nn/functional.py:1678:0
    return (%input.92)

TransformerFFN._actual_script_module
  graph(%self.102 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.102)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.102)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.101 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.101)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.9 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %output.59 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
    %input.100 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.59, %2, %6), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin1 # torch/nn/functional.py:1678:0
    return (%input.100)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.linear.Linear,
        %input.101 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %output.60 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.101, %4), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
    %input.102 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.60, %2, %6), scope: __module.transformer/__module.transformer.ffns.9/__module.transformer.ffns.9.lin2 # torch/nn/functional.py:1678:0
    return (%input.102)

TransformerFFN._actual_script_module
  graph(%self.112 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.112)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.112)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.111 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.111)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.10 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.113 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.113)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.113)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %output.65 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
    %input.110 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.65, %2, %6), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin1 # torch/nn/functional.py:1678:0
    return (%input.110)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.linear.Linear,
        %input.111 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %output.66 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.111, %4), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
    %input.112 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.66, %2, %6), scope: __module.transformer/__module.transformer.ffns.10/__module.transformer.ffns.10.lin2 # torch/nn/functional.py:1678:0
    return (%input.112)

TransformerFFN._actual_script_module
  graph(%self.122 : __torch__.transformers.modeling_xlm.TransformerFFN,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin2"](%self.122)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="lin1"](%self.122)
    %31 : Tensor = prim::CallMethod[name="forward"](%3, %1)
    %input.121 : Float(17:106496, 13:8192, 8192:1) = aten::gelu(%31), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:1369:0
    %32 : Tensor = prim::CallMethod[name="forward"](%2, %input.121)
    %28 : float = prim::Constant[value=0.10000000000000001](), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
    %29 : bool = prim::Constant[value=0](), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
    %30 : Float(17:26624, 13:2048, 2048:1) = aten::dropout(%32, %28, %29), scope: __module.transformer/__module.transformer.ffns.11 # torch/nn/functional.py:973:0
    return (%30)

TransformerFFN.lin1
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(2048:1, 8192:2048) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %output.71 : Float(17:106496, 13:8192, 8192:1) = aten::matmul(%1, %4), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
    %input.120 : Float(17:106496, 13:8192, 8192:1) = aten::add_(%output.71, %2, %6), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin1 # torch/nn/functional.py:1678:0
    return (%input.120)

TransformerFFN.lin2
Linear._actual_script_module
  graph(%self.124 : __torch__.torch.nn.modules.linear.Linear,
        %input.121 : Float(17:106496, 13:8192, 8192:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.124)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.124)
    %4 : Float(8192:1, 2048:8192) = aten::t(%3), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %output.72 : Float(17:26624, 13:2048, 2048:1) = aten::matmul(%input.121, %4), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
    %input.122 : Float(17:26624, 13:2048, 2048:1) = aten::add_(%output.72, %2, %6), scope: __module.transformer/__module.transformer.ffns.11/__module.transformer.ffns.11.lin2 # torch/nn/functional.py:1678:0
    return (%input.122)

LayerNorm._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.13 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.15)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.15)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.0
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
    %tensor.2 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.13, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.0 # torch/nn/functional.py:2048:0
    return (%tensor.2)

LayerNorm._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.23 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.1
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
    %tensor.3 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.23, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.1 # torch/nn/functional.py:2048:0
    return (%tensor.3)

LayerNorm._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.35)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.35)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.2
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
    %tensor.4 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.33, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.2 # torch/nn/functional.py:2048:0
    return (%tensor.4)

LayerNorm._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.43 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.45)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.45)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.3
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
    %tensor.5 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.43, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.3 # torch/nn/functional.py:2048:0
    return (%tensor.5)

LayerNorm._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.53 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.4
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
    %tensor.6 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.53, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.4 # torch/nn/functional.py:2048:0
    return (%tensor.6)

LayerNorm._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.63 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.5
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
    %tensor.7 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.63, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.5 # torch/nn/functional.py:2048:0
    return (%tensor.7)

LayerNorm._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.73 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.75)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.75)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.6
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
    %tensor.8 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.73, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.6 # torch/nn/functional.py:2048:0
    return (%tensor.8)

LayerNorm._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.85)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.85)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.7
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
    %tensor.9 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.83, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.7 # torch/nn/functional.py:2048:0
    return (%tensor.9)

LayerNorm._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.93 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.95)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.95)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.8
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
    %tensor.10 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.93, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.8 # torch/nn/functional.py:2048:0
    return (%tensor.10)

LayerNorm._actual_script_module
  graph(%self.105 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.103 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.105)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.105)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.9
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
    %tensor.11 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.103, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.9 # torch/nn/functional.py:2048:0
    return (%tensor.11)

LayerNorm._actual_script_module
  graph(%self.115 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.113 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.115)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.115)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.10
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
    %tensor.12 : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.113, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.10 # torch/nn/functional.py:2048:0
    return (%tensor.12)

LayerNorm._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : int = prim::Constant[value=2048](), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.transformer/__module.transformer.layer_norm2.11
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
    %tensor : Float(17:26624, 13:2048, 2048:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.transformer/__module.transformer.layer_norm2.11 # torch/nn/functional.py:2048:0
    return (%tensor)

XLMPredLayer.proj
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:26624, 13:2048, 2048:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self)
    %3 : Tensor = prim::GetAttr[name="weight"](%self)
    %4 : Float(2048:1, 30145:2048) = aten::t(%3), scope: __module.pred_layer/__module.pred_layer.proj # torch/nn/functional.py:1676:0
    %output : Float(17:391885, 13:30145, 30145:1) = aten::matmul(%1, %4), scope: __module.pred_layer/__module.pred_layer.proj # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.pred_layer/__module.pred_layer.proj # torch/nn/functional.py:1678:0
    %7 : Float(17:391885, 13:30145, 30145:1) = aten::add_(%output, %2, %6), scope: __module.pred_layer/__module.pred_layer.proj # torch/nn/functional.py:1678:0
    return (%7)

