graph(%self.1 : __torch__.transformers.modeling_camembert.CamembertForQuestionAnswering,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_11100.Linear = prim::GetAttr[name="qa_outputs"](%self.1)
  %4 : __torch__.transformers.modeling_roberta.___torch_mangle_11099.RobertaModel = prim::GetAttr[name="roberta"](%self.1)
  %17 : Double() = prim::Constant[value={8}](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %18 : int = prim::Constant[value=-2](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %19 : int = prim::Constant[value=64](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %20 : int = prim::Constant[value=12](), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %21 : Long() = prim::Constant[value={1}](), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %22 : int = prim::Constant[value=-1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %23 : bool = prim::Constant[value=1](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %25 : int = prim::Constant[value=768](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %26 : float = prim::Constant[value=0.10000000000000001](), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %27 : Double() = prim::Constant[value={-10000}](), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %28 : float = prim::Constant[value=1.](), scope: __module.roberta # torch/tensor.py:396:0
  %29 : None = prim::Constant(), scope: __module.roberta
  %30 : int = prim::Constant[value=6](), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %31 : int = prim::Constant[value=3](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %32 : int = prim::Constant[value=2](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %33 : int = prim::Constant[value=9223372036854775807](), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %34 : bool = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %35 : Device = prim::Constant[value="cpu"](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %36 : int = prim::Constant[value=4](), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %37 : int = prim::Constant[value=1](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %38 : int = prim::Constant[value=0](), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %39 : __torch__.transformers.modeling_roberta.___torch_mangle_11098.RobertaEncoder = prim::GetAttr[name="encoder"](%4)
  %40 : __torch__.transformers.modeling_roberta.___torch_mangle_10892.RobertaEmbeddings = prim::GetAttr[name="embeddings"](%4)
  %41 : int = aten::size(%input_ids, %38), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %42 : int = aten::size(%input_ids, %37), scope: __module.roberta # transformers/modeling_roberta.py:639:0
  %43 : int[] = prim::ListConstruct(%41, %42), scope: __module.roberta
  %input.2 : Long(17:13, 13:1) = aten::zeros(%43, %36, %38, %35, %34), scope: __module.roberta # transformers/modeling_roberta.py:650:0
  %45 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %38, %38, %33, %37), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %46 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%45, %37), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %47 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%46, %32), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%47, %31, %38, %33, %37), scope: __module.roberta # transformers/modeling_utils.py:244:0
  %49 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %30, %34, %34, %29), scope: __module.roberta # transformers/modeling_utils.py:257:0
  %50 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%49, %28, %37), scope: __module.roberta # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%50, %27), scope: __module.roberta # transformers/modeling_utils.py:258:0
  %52 : __torch__.torch.nn.modules.normalization.___torch_mangle_10890.LayerNorm = prim::GetAttr[name="LayerNorm"](%40)
  %53 : __torch__.torch.nn.modules.sparse.___torch_mangle_10889.Embedding = prim::GetAttr[name="token_type_embeddings"](%40)
  %54 : __torch__.torch.nn.modules.sparse.___torch_mangle_10888.Embedding = prim::GetAttr[name="position_embeddings"](%40)
  %55 : __torch__.torch.nn.modules.sparse.___torch_mangle_10887.Embedding = prim::GetAttr[name="word_embeddings"](%40)
  %56 : Bool(17:13, 13:1) = aten::ne(%input_ids, %37), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %mask : Int(17:13, 13:1) = aten::to(%56, %31, %34, %34, %29), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1332:0
  %58 : Long(17:13, 13:1) = aten::cumsum(%mask, %37, %29), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %59 : Int(17:13, 13:1) = aten::type_as(%58, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %incremental_indices : Int(17:13, 13:1) = aten::mul(%59, %mask), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1333:0
  %61 : Long(17:13, 13:1) = aten::to(%incremental_indices, %36, %34, %34, %29), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %62 : Long(17:13, 13:1) = aten::add(%61, %21, %37), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:1334:0
  %input.1 : Long(17:13, 13:1) = aten::to(%62, %36, %38, %35, %34, %34, %34, %29), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:98:0
  %64 : Tensor = prim::GetAttr[name="weight"](%55)
  %inputs_embeds : Float(17:9984, 13:768, 768:1) = aten::embedding(%64, %input_ids, %37, %34, %34), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %66 : Tensor = prim::GetAttr[name="weight"](%54)
  %position_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%66, %input.1, %37, %34, %34), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %68 : Tensor = prim::GetAttr[name="weight"](%53)
  %token_type_embeddings : Float(17:9984, 13:768, 768:1) = aten::embedding(%68, %input.2, %22, %34, %34), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %70 : Float(17:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %37), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %input.3 : Float(17:9984, 13:768, 768:1) = aten::add(%70, %token_type_embeddings, %37), scope: __module.roberta/__module.roberta.embeddings # transformers/modeling_roberta.py:121:0
  %72 : Tensor = prim::GetAttr[name="bias"](%52)
  %73 : Tensor = prim::GetAttr[name="weight"](%52)
  %74 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm
  %input.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %74, %73, %72, %24, %23), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.4, %26, %34), scope: __module.roberta/__module.roberta.embeddings/__module.roberta.embeddings.dropout # torch/nn/functional.py:973:0
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %78 : __torch__.transformers.modeling_roberta.___torch_mangle_11096.RobertaLayer = prim::GetAttr[name="11"](%77)
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %80 : __torch__.transformers.modeling_roberta.___torch_mangle_11079.RobertaLayer = prim::GetAttr[name="10"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %82 : __torch__.transformers.modeling_roberta.___torch_mangle_11062.RobertaLayer = prim::GetAttr[name="9"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %84 : __torch__.transformers.modeling_roberta.___torch_mangle_11045.RobertaLayer = prim::GetAttr[name="8"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %86 : __torch__.transformers.modeling_roberta.___torch_mangle_11028.RobertaLayer = prim::GetAttr[name="7"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %88 : __torch__.transformers.modeling_roberta.___torch_mangle_11011.RobertaLayer = prim::GetAttr[name="6"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %90 : __torch__.transformers.modeling_roberta.___torch_mangle_10994.RobertaLayer = prim::GetAttr[name="5"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %92 : __torch__.transformers.modeling_roberta.___torch_mangle_10977.RobertaLayer = prim::GetAttr[name="4"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %94 : __torch__.transformers.modeling_roberta.___torch_mangle_10960.RobertaLayer = prim::GetAttr[name="3"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %96 : __torch__.transformers.modeling_roberta.___torch_mangle_10943.RobertaLayer = prim::GetAttr[name="2"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %98 : __torch__.transformers.modeling_roberta.___torch_mangle_10926.RobertaLayer = prim::GetAttr[name="1"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_11097.ModuleList = prim::GetAttr[name="layer"](%39)
  %100 : __torch__.transformers.modeling_roberta.___torch_mangle_10909.RobertaLayer = prim::GetAttr[name="0"](%99)
  %101 : __torch__.transformers.modeling_roberta.___torch_mangle_10908.RobertaOutput = prim::GetAttr[name="output"](%100)
  %102 : __torch__.transformers.modeling_roberta.___torch_mangle_10904.RobertaIntermediate = prim::GetAttr[name="intermediate"](%100)
  %103 : __torch__.transformers.modeling_roberta.___torch_mangle_10902.RobertaAttention = prim::GetAttr[name="attention"](%100)
  %104 : __torch__.transformers.modeling_roberta.___torch_mangle_10901.RobertaSelfOutput = prim::GetAttr[name="output"](%103)
  %105 : __torch__.transformers.modeling_roberta.___torch_mangle_10897.RobertaSelfAttention = prim::GetAttr[name="self"](%103)
  %106 : __torch__.torch.nn.modules.linear.___torch_mangle_10895.Linear = prim::GetAttr[name="value"](%105)
  %107 : __torch__.torch.nn.modules.linear.___torch_mangle_10894.Linear = prim::GetAttr[name="key"](%105)
  %108 : __torch__.torch.nn.modules.linear.___torch_mangle_10893.Linear = prim::GetAttr[name="query"](%105)
  %109 : Tensor = prim::GetAttr[name="bias"](%108)
  %110 : Tensor = prim::GetAttr[name="weight"](%108)
  %111 : Float(768:1, 768:768) = aten::t(%110), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.1, %109, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %114 : Tensor = prim::GetAttr[name="bias"](%107)
  %115 : Tensor = prim::GetAttr[name="weight"](%107)
  %116 : Float(768:1, 768:768) = aten::t(%115), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %116), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.2, %114, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %119 : Tensor = prim::GetAttr[name="bias"](%106)
  %120 : Tensor = prim::GetAttr[name="weight"](%106)
  %121 : Float(768:1, 768:768) = aten::t(%120), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.5, %121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.3, %119, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %124 : int = aten::size(%x.1, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %125 : int = aten::size(%x.1, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %126 : int[] = prim::ListConstruct(%124, %125, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %126), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %128 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %query_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %128), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %130 : int = aten::size(%x.3, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %131 : int = aten::size(%x.3, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %132 : int[] = prim::ListConstruct(%130, %131, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %132), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %134 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %key_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %134), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %136 : int = aten::size(%x.5, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %137 : int = aten::size(%x.5, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:163:0
  %138 : int[] = prim::ListConstruct(%136, %137, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %x.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %138), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:164:0
  %140 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %value_layer.1 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %140), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:165:0
  %142 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %142), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:195:0
  %input.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:198:0
  %input.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self/__module.roberta.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:211:0
  %149 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %150 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %149), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.2 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%150, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:213:0
  %152 : int = aten::size(%context_layer.2, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %153 : int = aten::size(%context_layer.2, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:214:0
  %154 : int[] = prim::ListConstruct(%152, %153, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self
  %input.8 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.2, %154), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.self # transformers/modeling_roberta.py:215:0
  %156 : __torch__.torch.nn.modules.normalization.___torch_mangle_10899.LayerNorm = prim::GetAttr[name="LayerNorm"](%104)
  %157 : __torch__.torch.nn.modules.linear.___torch_mangle_10898.Linear = prim::GetAttr[name="dense"](%104)
  %158 : Tensor = prim::GetAttr[name="bias"](%157)
  %159 : Tensor = prim::GetAttr[name="weight"](%157)
  %160 : Float(768:1, 768:768) = aten::t(%159), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.8, %160), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.4, %158, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.9, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output # transformers/modeling_roberta.py:232:0
  %165 : Tensor = prim::GetAttr[name="bias"](%156)
  %166 : Tensor = prim::GetAttr[name="weight"](%156)
  %167 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %167, %166, %165, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.attention/__module.roberta.encoder.layer.0.attention.output/__module.roberta.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %169 : __torch__.torch.nn.modules.linear.___torch_mangle_10903.Linear = prim::GetAttr[name="dense"](%102)
  %170 : Tensor = prim::GetAttr[name="bias"](%169)
  %171 : Tensor = prim::GetAttr[name="weight"](%169)
  %172 : Float(768:1, 3072:768) = aten::t(%171), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %172), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.5, %170, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate/__module.roberta.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %176 : __torch__.torch.nn.modules.normalization.___torch_mangle_10906.LayerNorm = prim::GetAttr[name="LayerNorm"](%101)
  %177 : __torch__.torch.nn.modules.linear.___torch_mangle_10905.Linear = prim::GetAttr[name="dense"](%101)
  %178 : Tensor = prim::GetAttr[name="bias"](%177)
  %179 : Tensor = prim::GetAttr[name="weight"](%177)
  %180 : Float(3072:1, 768:3072) = aten::t(%179), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.12, %180), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.6, %178, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.13, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output # transformers/modeling_roberta.py:311:0
  %185 : Tensor = prim::GetAttr[name="bias"](%176)
  %186 : Tensor = prim::GetAttr[name="weight"](%176)
  %187 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm
  %input.15 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %187, %186, %185, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.0/__module.roberta.encoder.layer.0.output/__module.roberta.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %189 : __torch__.transformers.modeling_roberta.___torch_mangle_10925.RobertaOutput = prim::GetAttr[name="output"](%98)
  %190 : __torch__.transformers.modeling_roberta.___torch_mangle_10921.RobertaIntermediate = prim::GetAttr[name="intermediate"](%98)
  %191 : __torch__.transformers.modeling_roberta.___torch_mangle_10919.RobertaAttention = prim::GetAttr[name="attention"](%98)
  %192 : __torch__.transformers.modeling_roberta.___torch_mangle_10918.RobertaSelfOutput = prim::GetAttr[name="output"](%191)
  %193 : __torch__.transformers.modeling_roberta.___torch_mangle_10914.RobertaSelfAttention = prim::GetAttr[name="self"](%191)
  %194 : __torch__.torch.nn.modules.linear.___torch_mangle_10912.Linear = prim::GetAttr[name="value"](%193)
  %195 : __torch__.torch.nn.modules.linear.___torch_mangle_10911.Linear = prim::GetAttr[name="key"](%193)
  %196 : __torch__.torch.nn.modules.linear.___torch_mangle_10910.Linear = prim::GetAttr[name="query"](%193)
  %197 : Tensor = prim::GetAttr[name="bias"](%196)
  %198 : Tensor = prim::GetAttr[name="weight"](%196)
  %199 : Float(768:1, 768:768) = aten::t(%198), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %199), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.7, %197, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %202 : Tensor = prim::GetAttr[name="bias"](%195)
  %203 : Tensor = prim::GetAttr[name="weight"](%195)
  %204 : Float(768:1, 768:768) = aten::t(%203), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %204), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.8, %202, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %207 : Tensor = prim::GetAttr[name="bias"](%194)
  %208 : Tensor = prim::GetAttr[name="weight"](%194)
  %209 : Float(768:1, 768:768) = aten::t(%208), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.15, %209), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.9, %207, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %212 : int = aten::size(%x.7, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %213 : int = aten::size(%x.7, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %214 : int[] = prim::ListConstruct(%212, %213, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %214), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %216 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %query_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %216), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %218 : int = aten::size(%x.9, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %219 : int = aten::size(%x.9, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %220 : int[] = prim::ListConstruct(%218, %219, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %220), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %222 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %key_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %222), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %224 : int = aten::size(%x.11, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %225 : int = aten::size(%x.11, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:163:0
  %226 : int[] = prim::ListConstruct(%224, %225, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %x.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %226), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:164:0
  %228 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %value_layer.2 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %228), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:165:0
  %230 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %230), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:195:0
  %input.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:198:0
  %input.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self/__module.roberta.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:211:0
  %237 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %238 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %237), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.4 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%238, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:213:0
  %240 : int = aten::size(%context_layer.4, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %241 : int = aten::size(%context_layer.4, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:214:0
  %242 : int[] = prim::ListConstruct(%240, %241, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self
  %input.18 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.4, %242), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.self # transformers/modeling_roberta.py:215:0
  %244 : __torch__.torch.nn.modules.normalization.___torch_mangle_10916.LayerNorm = prim::GetAttr[name="LayerNorm"](%192)
  %245 : __torch__.torch.nn.modules.linear.___torch_mangle_10915.Linear = prim::GetAttr[name="dense"](%192)
  %246 : Tensor = prim::GetAttr[name="bias"](%245)
  %247 : Tensor = prim::GetAttr[name="weight"](%245)
  %248 : Float(768:1, 768:768) = aten::t(%247), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.18, %248), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.10, %246, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.19, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output # transformers/modeling_roberta.py:232:0
  %253 : Tensor = prim::GetAttr[name="bias"](%244)
  %254 : Tensor = prim::GetAttr[name="weight"](%244)
  %255 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %255, %254, %253, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.attention/__module.roberta.encoder.layer.1.attention.output/__module.roberta.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %257 : __torch__.torch.nn.modules.linear.___torch_mangle_10920.Linear = prim::GetAttr[name="dense"](%190)
  %258 : Tensor = prim::GetAttr[name="bias"](%257)
  %259 : Tensor = prim::GetAttr[name="weight"](%257)
  %260 : Float(768:1, 3072:768) = aten::t(%259), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %260), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.11, %258, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate/__module.roberta.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %264 : __torch__.torch.nn.modules.normalization.___torch_mangle_10923.LayerNorm = prim::GetAttr[name="LayerNorm"](%189)
  %265 : __torch__.torch.nn.modules.linear.___torch_mangle_10922.Linear = prim::GetAttr[name="dense"](%189)
  %266 : Tensor = prim::GetAttr[name="bias"](%265)
  %267 : Tensor = prim::GetAttr[name="weight"](%265)
  %268 : Float(3072:1, 768:3072) = aten::t(%267), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.22, %268), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.12, %266, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.23, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output # transformers/modeling_roberta.py:311:0
  %273 : Tensor = prim::GetAttr[name="bias"](%264)
  %274 : Tensor = prim::GetAttr[name="weight"](%264)
  %275 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm
  %input.25 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %275, %274, %273, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.1/__module.roberta.encoder.layer.1.output/__module.roberta.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %277 : __torch__.transformers.modeling_roberta.___torch_mangle_10942.RobertaOutput = prim::GetAttr[name="output"](%96)
  %278 : __torch__.transformers.modeling_roberta.___torch_mangle_10938.RobertaIntermediate = prim::GetAttr[name="intermediate"](%96)
  %279 : __torch__.transformers.modeling_roberta.___torch_mangle_10936.RobertaAttention = prim::GetAttr[name="attention"](%96)
  %280 : __torch__.transformers.modeling_roberta.___torch_mangle_10935.RobertaSelfOutput = prim::GetAttr[name="output"](%279)
  %281 : __torch__.transformers.modeling_roberta.___torch_mangle_10931.RobertaSelfAttention = prim::GetAttr[name="self"](%279)
  %282 : __torch__.torch.nn.modules.linear.___torch_mangle_10929.Linear = prim::GetAttr[name="value"](%281)
  %283 : __torch__.torch.nn.modules.linear.___torch_mangle_10928.Linear = prim::GetAttr[name="key"](%281)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_10927.Linear = prim::GetAttr[name="query"](%281)
  %285 : Tensor = prim::GetAttr[name="bias"](%284)
  %286 : Tensor = prim::GetAttr[name="weight"](%284)
  %287 : Float(768:1, 768:768) = aten::t(%286), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %287), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.13, %285, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %290 : Tensor = prim::GetAttr[name="bias"](%283)
  %291 : Tensor = prim::GetAttr[name="weight"](%283)
  %292 : Float(768:1, 768:768) = aten::t(%291), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %292), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.14, %290, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %295 : Tensor = prim::GetAttr[name="bias"](%282)
  %296 : Tensor = prim::GetAttr[name="weight"](%282)
  %297 : Float(768:1, 768:768) = aten::t(%296), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.25, %297), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.15, %295, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %300 : int = aten::size(%x.13, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %301 : int = aten::size(%x.13, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %302 : int[] = prim::ListConstruct(%300, %301, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %302), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %304 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %query_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %304), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %306 : int = aten::size(%x.15, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %307 : int = aten::size(%x.15, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %308 : int[] = prim::ListConstruct(%306, %307, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %308), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %310 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %key_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %310), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %312 : int = aten::size(%x.17, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %313 : int = aten::size(%x.17, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:163:0
  %314 : int[] = prim::ListConstruct(%312, %313, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %x.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %314), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:164:0
  %316 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %value_layer.3 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %316), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:165:0
  %318 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %318), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:195:0
  %input.26 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:198:0
  %input.27 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self/__module.roberta.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:211:0
  %325 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %326 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %325), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.6 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%326, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:213:0
  %328 : int = aten::size(%context_layer.6, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %329 : int = aten::size(%context_layer.6, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:214:0
  %330 : int[] = prim::ListConstruct(%328, %329, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self
  %input.28 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.6, %330), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.self # transformers/modeling_roberta.py:215:0
  %332 : __torch__.torch.nn.modules.normalization.___torch_mangle_10933.LayerNorm = prim::GetAttr[name="LayerNorm"](%280)
  %333 : __torch__.torch.nn.modules.linear.___torch_mangle_10932.Linear = prim::GetAttr[name="dense"](%280)
  %334 : Tensor = prim::GetAttr[name="bias"](%333)
  %335 : Tensor = prim::GetAttr[name="weight"](%333)
  %336 : Float(768:1, 768:768) = aten::t(%335), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.28, %336), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.16, %334, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.29, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output # transformers/modeling_roberta.py:232:0
  %341 : Tensor = prim::GetAttr[name="bias"](%332)
  %342 : Tensor = prim::GetAttr[name="weight"](%332)
  %343 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %343, %342, %341, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.attention/__module.roberta.encoder.layer.2.attention.output/__module.roberta.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %345 : __torch__.torch.nn.modules.linear.___torch_mangle_10937.Linear = prim::GetAttr[name="dense"](%278)
  %346 : Tensor = prim::GetAttr[name="bias"](%345)
  %347 : Tensor = prim::GetAttr[name="weight"](%345)
  %348 : Float(768:1, 3072:768) = aten::t(%347), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %348), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.17, %346, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate/__module.roberta.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %352 : __torch__.torch.nn.modules.normalization.___torch_mangle_10940.LayerNorm = prim::GetAttr[name="LayerNorm"](%277)
  %353 : __torch__.torch.nn.modules.linear.___torch_mangle_10939.Linear = prim::GetAttr[name="dense"](%277)
  %354 : Tensor = prim::GetAttr[name="bias"](%353)
  %355 : Tensor = prim::GetAttr[name="weight"](%353)
  %356 : Float(3072:1, 768:3072) = aten::t(%355), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.32, %356), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.18, %354, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.33, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output # transformers/modeling_roberta.py:311:0
  %361 : Tensor = prim::GetAttr[name="bias"](%352)
  %362 : Tensor = prim::GetAttr[name="weight"](%352)
  %363 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm
  %input.35 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %363, %362, %361, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.2/__module.roberta.encoder.layer.2.output/__module.roberta.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %365 : __torch__.transformers.modeling_roberta.___torch_mangle_10959.RobertaOutput = prim::GetAttr[name="output"](%94)
  %366 : __torch__.transformers.modeling_roberta.___torch_mangle_10955.RobertaIntermediate = prim::GetAttr[name="intermediate"](%94)
  %367 : __torch__.transformers.modeling_roberta.___torch_mangle_10953.RobertaAttention = prim::GetAttr[name="attention"](%94)
  %368 : __torch__.transformers.modeling_roberta.___torch_mangle_10952.RobertaSelfOutput = prim::GetAttr[name="output"](%367)
  %369 : __torch__.transformers.modeling_roberta.___torch_mangle_10948.RobertaSelfAttention = prim::GetAttr[name="self"](%367)
  %370 : __torch__.torch.nn.modules.linear.___torch_mangle_10946.Linear = prim::GetAttr[name="value"](%369)
  %371 : __torch__.torch.nn.modules.linear.___torch_mangle_10945.Linear = prim::GetAttr[name="key"](%369)
  %372 : __torch__.torch.nn.modules.linear.___torch_mangle_10944.Linear = prim::GetAttr[name="query"](%369)
  %373 : Tensor = prim::GetAttr[name="bias"](%372)
  %374 : Tensor = prim::GetAttr[name="weight"](%372)
  %375 : Float(768:1, 768:768) = aten::t(%374), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %375), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.19, %373, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %378 : Tensor = prim::GetAttr[name="bias"](%371)
  %379 : Tensor = prim::GetAttr[name="weight"](%371)
  %380 : Float(768:1, 768:768) = aten::t(%379), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %380), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.20, %378, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %383 : Tensor = prim::GetAttr[name="bias"](%370)
  %384 : Tensor = prim::GetAttr[name="weight"](%370)
  %385 : Float(768:1, 768:768) = aten::t(%384), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.35, %385), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.21, %383, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %388 : int = aten::size(%x.19, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %389 : int = aten::size(%x.19, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %390 : int[] = prim::ListConstruct(%388, %389, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %390), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %392 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %query_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %392), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %394 : int = aten::size(%x.21, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %395 : int = aten::size(%x.21, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %396 : int[] = prim::ListConstruct(%394, %395, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %396), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %398 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %key_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %398), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %400 : int = aten::size(%x.23, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %401 : int = aten::size(%x.23, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:163:0
  %402 : int[] = prim::ListConstruct(%400, %401, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %x.24 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %402), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:164:0
  %404 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %value_layer.4 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %404), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:165:0
  %406 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %406), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:195:0
  %input.36 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:198:0
  %input.37 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self/__module.roberta.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:211:0
  %413 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %414 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %413), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.8 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%414, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:213:0
  %416 : int = aten::size(%context_layer.8, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %417 : int = aten::size(%context_layer.8, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:214:0
  %418 : int[] = prim::ListConstruct(%416, %417, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self
  %input.38 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.8, %418), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.self # transformers/modeling_roberta.py:215:0
  %420 : __torch__.torch.nn.modules.normalization.___torch_mangle_10950.LayerNorm = prim::GetAttr[name="LayerNorm"](%368)
  %421 : __torch__.torch.nn.modules.linear.___torch_mangle_10949.Linear = prim::GetAttr[name="dense"](%368)
  %422 : Tensor = prim::GetAttr[name="bias"](%421)
  %423 : Tensor = prim::GetAttr[name="weight"](%421)
  %424 : Float(768:1, 768:768) = aten::t(%423), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.38, %424), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.22, %422, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.39, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output # transformers/modeling_roberta.py:232:0
  %429 : Tensor = prim::GetAttr[name="bias"](%420)
  %430 : Tensor = prim::GetAttr[name="weight"](%420)
  %431 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %431, %430, %429, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.attention/__module.roberta.encoder.layer.3.attention.output/__module.roberta.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %433 : __torch__.torch.nn.modules.linear.___torch_mangle_10954.Linear = prim::GetAttr[name="dense"](%366)
  %434 : Tensor = prim::GetAttr[name="bias"](%433)
  %435 : Tensor = prim::GetAttr[name="weight"](%433)
  %436 : Float(768:1, 3072:768) = aten::t(%435), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %436), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.23, %434, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate/__module.roberta.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %440 : __torch__.torch.nn.modules.normalization.___torch_mangle_10957.LayerNorm = prim::GetAttr[name="LayerNorm"](%365)
  %441 : __torch__.torch.nn.modules.linear.___torch_mangle_10956.Linear = prim::GetAttr[name="dense"](%365)
  %442 : Tensor = prim::GetAttr[name="bias"](%441)
  %443 : Tensor = prim::GetAttr[name="weight"](%441)
  %444 : Float(3072:1, 768:3072) = aten::t(%443), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.42, %444), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.24, %442, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.43, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output # transformers/modeling_roberta.py:311:0
  %449 : Tensor = prim::GetAttr[name="bias"](%440)
  %450 : Tensor = prim::GetAttr[name="weight"](%440)
  %451 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm
  %input.45 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %451, %450, %449, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.3/__module.roberta.encoder.layer.3.output/__module.roberta.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %453 : __torch__.transformers.modeling_roberta.___torch_mangle_10976.RobertaOutput = prim::GetAttr[name="output"](%92)
  %454 : __torch__.transformers.modeling_roberta.___torch_mangle_10972.RobertaIntermediate = prim::GetAttr[name="intermediate"](%92)
  %455 : __torch__.transformers.modeling_roberta.___torch_mangle_10970.RobertaAttention = prim::GetAttr[name="attention"](%92)
  %456 : __torch__.transformers.modeling_roberta.___torch_mangle_10969.RobertaSelfOutput = prim::GetAttr[name="output"](%455)
  %457 : __torch__.transformers.modeling_roberta.___torch_mangle_10965.RobertaSelfAttention = prim::GetAttr[name="self"](%455)
  %458 : __torch__.torch.nn.modules.linear.___torch_mangle_10963.Linear = prim::GetAttr[name="value"](%457)
  %459 : __torch__.torch.nn.modules.linear.___torch_mangle_10962.Linear = prim::GetAttr[name="key"](%457)
  %460 : __torch__.torch.nn.modules.linear.___torch_mangle_10961.Linear = prim::GetAttr[name="query"](%457)
  %461 : Tensor = prim::GetAttr[name="bias"](%460)
  %462 : Tensor = prim::GetAttr[name="weight"](%460)
  %463 : Float(768:1, 768:768) = aten::t(%462), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %463), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.25, %461, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %466 : Tensor = prim::GetAttr[name="bias"](%459)
  %467 : Tensor = prim::GetAttr[name="weight"](%459)
  %468 : Float(768:1, 768:768) = aten::t(%467), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %468), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.26, %466, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %471 : Tensor = prim::GetAttr[name="bias"](%458)
  %472 : Tensor = prim::GetAttr[name="weight"](%458)
  %473 : Float(768:1, 768:768) = aten::t(%472), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.45, %473), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.27, %471, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %476 : int = aten::size(%x.25, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %477 : int = aten::size(%x.25, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %478 : int[] = prim::ListConstruct(%476, %477, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.26 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %478), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %480 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %query_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %480), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %482 : int = aten::size(%x.27, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %483 : int = aten::size(%x.27, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %484 : int[] = prim::ListConstruct(%482, %483, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.28 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %484), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %486 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %key_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %486), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %488 : int = aten::size(%x.29, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %489 : int = aten::size(%x.29, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:163:0
  %490 : int[] = prim::ListConstruct(%488, %489, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %x.30 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %490), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:164:0
  %492 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %value_layer.5 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %492), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:165:0
  %494 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %494), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:195:0
  %input.46 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:198:0
  %input.47 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self/__module.roberta.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:211:0
  %501 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %502 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %501), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.10 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%502, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:213:0
  %504 : int = aten::size(%context_layer.10, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %505 : int = aten::size(%context_layer.10, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:214:0
  %506 : int[] = prim::ListConstruct(%504, %505, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self
  %input.48 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.10, %506), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.self # transformers/modeling_roberta.py:215:0
  %508 : __torch__.torch.nn.modules.normalization.___torch_mangle_10967.LayerNorm = prim::GetAttr[name="LayerNorm"](%456)
  %509 : __torch__.torch.nn.modules.linear.___torch_mangle_10966.Linear = prim::GetAttr[name="dense"](%456)
  %510 : Tensor = prim::GetAttr[name="bias"](%509)
  %511 : Tensor = prim::GetAttr[name="weight"](%509)
  %512 : Float(768:1, 768:768) = aten::t(%511), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.48, %512), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.28, %510, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.49, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output # transformers/modeling_roberta.py:232:0
  %517 : Tensor = prim::GetAttr[name="bias"](%508)
  %518 : Tensor = prim::GetAttr[name="weight"](%508)
  %519 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %519, %518, %517, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.attention/__module.roberta.encoder.layer.4.attention.output/__module.roberta.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %521 : __torch__.torch.nn.modules.linear.___torch_mangle_10971.Linear = prim::GetAttr[name="dense"](%454)
  %522 : Tensor = prim::GetAttr[name="bias"](%521)
  %523 : Tensor = prim::GetAttr[name="weight"](%521)
  %524 : Float(768:1, 3072:768) = aten::t(%523), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %524), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.29, %522, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate/__module.roberta.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %528 : __torch__.torch.nn.modules.normalization.___torch_mangle_10974.LayerNorm = prim::GetAttr[name="LayerNorm"](%453)
  %529 : __torch__.torch.nn.modules.linear.___torch_mangle_10973.Linear = prim::GetAttr[name="dense"](%453)
  %530 : Tensor = prim::GetAttr[name="bias"](%529)
  %531 : Tensor = prim::GetAttr[name="weight"](%529)
  %532 : Float(3072:1, 768:3072) = aten::t(%531), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.52, %532), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.30, %530, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.53, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output # transformers/modeling_roberta.py:311:0
  %537 : Tensor = prim::GetAttr[name="bias"](%528)
  %538 : Tensor = prim::GetAttr[name="weight"](%528)
  %539 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm
  %input.55 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %539, %538, %537, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.4/__module.roberta.encoder.layer.4.output/__module.roberta.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %541 : __torch__.transformers.modeling_roberta.___torch_mangle_10993.RobertaOutput = prim::GetAttr[name="output"](%90)
  %542 : __torch__.transformers.modeling_roberta.___torch_mangle_10989.RobertaIntermediate = prim::GetAttr[name="intermediate"](%90)
  %543 : __torch__.transformers.modeling_roberta.___torch_mangle_10987.RobertaAttention = prim::GetAttr[name="attention"](%90)
  %544 : __torch__.transformers.modeling_roberta.___torch_mangle_10986.RobertaSelfOutput = prim::GetAttr[name="output"](%543)
  %545 : __torch__.transformers.modeling_roberta.___torch_mangle_10982.RobertaSelfAttention = prim::GetAttr[name="self"](%543)
  %546 : __torch__.torch.nn.modules.linear.___torch_mangle_10980.Linear = prim::GetAttr[name="value"](%545)
  %547 : __torch__.torch.nn.modules.linear.___torch_mangle_10979.Linear = prim::GetAttr[name="key"](%545)
  %548 : __torch__.torch.nn.modules.linear.___torch_mangle_10978.Linear = prim::GetAttr[name="query"](%545)
  %549 : Tensor = prim::GetAttr[name="bias"](%548)
  %550 : Tensor = prim::GetAttr[name="weight"](%548)
  %551 : Float(768:1, 768:768) = aten::t(%550), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %551), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.31, %549, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %554 : Tensor = prim::GetAttr[name="bias"](%547)
  %555 : Tensor = prim::GetAttr[name="weight"](%547)
  %556 : Float(768:1, 768:768) = aten::t(%555), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %556), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.32, %554, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %559 : Tensor = prim::GetAttr[name="bias"](%546)
  %560 : Tensor = prim::GetAttr[name="weight"](%546)
  %561 : Float(768:1, 768:768) = aten::t(%560), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.55, %561), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.33, %559, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %564 : int = aten::size(%x.31, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %565 : int = aten::size(%x.31, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %566 : int[] = prim::ListConstruct(%564, %565, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.32 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %566), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %568 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %query_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %568), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %570 : int = aten::size(%x.33, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %571 : int = aten::size(%x.33, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %572 : int[] = prim::ListConstruct(%570, %571, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.34 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %572), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %574 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %key_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %574), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %576 : int = aten::size(%x.35, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %577 : int = aten::size(%x.35, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:163:0
  %578 : int[] = prim::ListConstruct(%576, %577, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %x.36 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %578), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:164:0
  %580 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %value_layer.6 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %580), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:165:0
  %582 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %582), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.12 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:195:0
  %input.56 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:198:0
  %input.57 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self/__module.roberta.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:211:0
  %589 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %590 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %589), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.12 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%590, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:213:0
  %592 : int = aten::size(%context_layer.12, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %593 : int = aten::size(%context_layer.12, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:214:0
  %594 : int[] = prim::ListConstruct(%592, %593, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self
  %input.58 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.12, %594), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.self # transformers/modeling_roberta.py:215:0
  %596 : __torch__.torch.nn.modules.normalization.___torch_mangle_10984.LayerNorm = prim::GetAttr[name="LayerNorm"](%544)
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_10983.Linear = prim::GetAttr[name="dense"](%544)
  %598 : Tensor = prim::GetAttr[name="bias"](%597)
  %599 : Tensor = prim::GetAttr[name="weight"](%597)
  %600 : Float(768:1, 768:768) = aten::t(%599), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.58, %600), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.34, %598, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.59, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output # transformers/modeling_roberta.py:232:0
  %605 : Tensor = prim::GetAttr[name="bias"](%596)
  %606 : Tensor = prim::GetAttr[name="weight"](%596)
  %607 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %607, %606, %605, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.attention/__module.roberta.encoder.layer.5.attention.output/__module.roberta.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %609 : __torch__.torch.nn.modules.linear.___torch_mangle_10988.Linear = prim::GetAttr[name="dense"](%542)
  %610 : Tensor = prim::GetAttr[name="bias"](%609)
  %611 : Tensor = prim::GetAttr[name="weight"](%609)
  %612 : Float(768:1, 3072:768) = aten::t(%611), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %612), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.35, %610, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate/__module.roberta.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %616 : __torch__.torch.nn.modules.normalization.___torch_mangle_10991.LayerNorm = prim::GetAttr[name="LayerNorm"](%541)
  %617 : __torch__.torch.nn.modules.linear.___torch_mangle_10990.Linear = prim::GetAttr[name="dense"](%541)
  %618 : Tensor = prim::GetAttr[name="bias"](%617)
  %619 : Tensor = prim::GetAttr[name="weight"](%617)
  %620 : Float(3072:1, 768:3072) = aten::t(%619), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.62, %620), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.36, %618, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.63, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output # transformers/modeling_roberta.py:311:0
  %625 : Tensor = prim::GetAttr[name="bias"](%616)
  %626 : Tensor = prim::GetAttr[name="weight"](%616)
  %627 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm
  %input.65 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %627, %626, %625, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.5/__module.roberta.encoder.layer.5.output/__module.roberta.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %629 : __torch__.transformers.modeling_roberta.___torch_mangle_11010.RobertaOutput = prim::GetAttr[name="output"](%88)
  %630 : __torch__.transformers.modeling_roberta.___torch_mangle_11006.RobertaIntermediate = prim::GetAttr[name="intermediate"](%88)
  %631 : __torch__.transformers.modeling_roberta.___torch_mangle_11004.RobertaAttention = prim::GetAttr[name="attention"](%88)
  %632 : __torch__.transformers.modeling_roberta.___torch_mangle_11003.RobertaSelfOutput = prim::GetAttr[name="output"](%631)
  %633 : __torch__.transformers.modeling_roberta.___torch_mangle_10999.RobertaSelfAttention = prim::GetAttr[name="self"](%631)
  %634 : __torch__.torch.nn.modules.linear.___torch_mangle_10997.Linear = prim::GetAttr[name="value"](%633)
  %635 : __torch__.torch.nn.modules.linear.___torch_mangle_10996.Linear = prim::GetAttr[name="key"](%633)
  %636 : __torch__.torch.nn.modules.linear.___torch_mangle_10995.Linear = prim::GetAttr[name="query"](%633)
  %637 : Tensor = prim::GetAttr[name="bias"](%636)
  %638 : Tensor = prim::GetAttr[name="weight"](%636)
  %639 : Float(768:1, 768:768) = aten::t(%638), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %639), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.37, %637, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %642 : Tensor = prim::GetAttr[name="bias"](%635)
  %643 : Tensor = prim::GetAttr[name="weight"](%635)
  %644 : Float(768:1, 768:768) = aten::t(%643), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %644), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.38, %642, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %647 : Tensor = prim::GetAttr[name="bias"](%634)
  %648 : Tensor = prim::GetAttr[name="weight"](%634)
  %649 : Float(768:1, 768:768) = aten::t(%648), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.65, %649), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.39, %647, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %652 : int = aten::size(%x.37, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %653 : int = aten::size(%x.37, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %654 : int[] = prim::ListConstruct(%652, %653, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.38 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %654), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %656 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %query_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %656), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %658 : int = aten::size(%x.39, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %659 : int = aten::size(%x.39, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %660 : int[] = prim::ListConstruct(%658, %659, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.40 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %660), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %662 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %key_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %662), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %664 : int = aten::size(%x.41, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %665 : int = aten::size(%x.41, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:163:0
  %666 : int[] = prim::ListConstruct(%664, %665, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %x.42 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %666), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:164:0
  %668 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %value_layer.7 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %668), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:165:0
  %670 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.13 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %670), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.14 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:195:0
  %input.66 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:198:0
  %input.67 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self/__module.roberta.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:211:0
  %677 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %678 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %677), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.14 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%678, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:213:0
  %680 : int = aten::size(%context_layer.14, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %681 : int = aten::size(%context_layer.14, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:214:0
  %682 : int[] = prim::ListConstruct(%680, %681, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self
  %input.68 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.14, %682), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.self # transformers/modeling_roberta.py:215:0
  %684 : __torch__.torch.nn.modules.normalization.___torch_mangle_11001.LayerNorm = prim::GetAttr[name="LayerNorm"](%632)
  %685 : __torch__.torch.nn.modules.linear.___torch_mangle_11000.Linear = prim::GetAttr[name="dense"](%632)
  %686 : Tensor = prim::GetAttr[name="bias"](%685)
  %687 : Tensor = prim::GetAttr[name="weight"](%685)
  %688 : Float(768:1, 768:768) = aten::t(%687), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.68, %688), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.40, %686, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.69, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output # transformers/modeling_roberta.py:232:0
  %693 : Tensor = prim::GetAttr[name="bias"](%684)
  %694 : Tensor = prim::GetAttr[name="weight"](%684)
  %695 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %695, %694, %693, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.attention/__module.roberta.encoder.layer.6.attention.output/__module.roberta.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_11005.Linear = prim::GetAttr[name="dense"](%630)
  %698 : Tensor = prim::GetAttr[name="bias"](%697)
  %699 : Tensor = prim::GetAttr[name="weight"](%697)
  %700 : Float(768:1, 3072:768) = aten::t(%699), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %700), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.41, %698, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate/__module.roberta.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %704 : __torch__.torch.nn.modules.normalization.___torch_mangle_11008.LayerNorm = prim::GetAttr[name="LayerNorm"](%629)
  %705 : __torch__.torch.nn.modules.linear.___torch_mangle_11007.Linear = prim::GetAttr[name="dense"](%629)
  %706 : Tensor = prim::GetAttr[name="bias"](%705)
  %707 : Tensor = prim::GetAttr[name="weight"](%705)
  %708 : Float(3072:1, 768:3072) = aten::t(%707), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.72, %708), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.42, %706, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.73, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output # transformers/modeling_roberta.py:311:0
  %713 : Tensor = prim::GetAttr[name="bias"](%704)
  %714 : Tensor = prim::GetAttr[name="weight"](%704)
  %715 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm
  %input.75 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %715, %714, %713, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.6/__module.roberta.encoder.layer.6.output/__module.roberta.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %717 : __torch__.transformers.modeling_roberta.___torch_mangle_11027.RobertaOutput = prim::GetAttr[name="output"](%86)
  %718 : __torch__.transformers.modeling_roberta.___torch_mangle_11023.RobertaIntermediate = prim::GetAttr[name="intermediate"](%86)
  %719 : __torch__.transformers.modeling_roberta.___torch_mangle_11021.RobertaAttention = prim::GetAttr[name="attention"](%86)
  %720 : __torch__.transformers.modeling_roberta.___torch_mangle_11020.RobertaSelfOutput = prim::GetAttr[name="output"](%719)
  %721 : __torch__.transformers.modeling_roberta.___torch_mangle_11016.RobertaSelfAttention = prim::GetAttr[name="self"](%719)
  %722 : __torch__.torch.nn.modules.linear.___torch_mangle_11014.Linear = prim::GetAttr[name="value"](%721)
  %723 : __torch__.torch.nn.modules.linear.___torch_mangle_11013.Linear = prim::GetAttr[name="key"](%721)
  %724 : __torch__.torch.nn.modules.linear.___torch_mangle_11012.Linear = prim::GetAttr[name="query"](%721)
  %725 : Tensor = prim::GetAttr[name="bias"](%724)
  %726 : Tensor = prim::GetAttr[name="weight"](%724)
  %727 : Float(768:1, 768:768) = aten::t(%726), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %727), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.43, %725, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %730 : Tensor = prim::GetAttr[name="bias"](%723)
  %731 : Tensor = prim::GetAttr[name="weight"](%723)
  %732 : Float(768:1, 768:768) = aten::t(%731), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %732), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.44, %730, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %735 : Tensor = prim::GetAttr[name="bias"](%722)
  %736 : Tensor = prim::GetAttr[name="weight"](%722)
  %737 : Float(768:1, 768:768) = aten::t(%736), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.75, %737), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.45, %735, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %740 : int = aten::size(%x.43, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %741 : int = aten::size(%x.43, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %742 : int[] = prim::ListConstruct(%740, %741, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.44 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %742), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %744 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %query_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %744), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %746 : int = aten::size(%x.45, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %747 : int = aten::size(%x.45, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %748 : int[] = prim::ListConstruct(%746, %747, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.46 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %748), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %750 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %key_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %750), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %752 : int = aten::size(%x.47, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %753 : int = aten::size(%x.47, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:163:0
  %754 : int[] = prim::ListConstruct(%752, %753, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %x.48 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %754), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:164:0
  %756 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %value_layer.8 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %756), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:165:0
  %758 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.15 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %758), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.16 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:195:0
  %input.76 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:198:0
  %input.77 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self/__module.roberta.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:211:0
  %765 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %766 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %765), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.16 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%766, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:213:0
  %768 : int = aten::size(%context_layer.16, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %769 : int = aten::size(%context_layer.16, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:214:0
  %770 : int[] = prim::ListConstruct(%768, %769, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self
  %input.78 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.16, %770), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.self # transformers/modeling_roberta.py:215:0
  %772 : __torch__.torch.nn.modules.normalization.___torch_mangle_11018.LayerNorm = prim::GetAttr[name="LayerNorm"](%720)
  %773 : __torch__.torch.nn.modules.linear.___torch_mangle_11017.Linear = prim::GetAttr[name="dense"](%720)
  %774 : Tensor = prim::GetAttr[name="bias"](%773)
  %775 : Tensor = prim::GetAttr[name="weight"](%773)
  %776 : Float(768:1, 768:768) = aten::t(%775), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.78, %776), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.46, %774, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.79, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output # transformers/modeling_roberta.py:232:0
  %781 : Tensor = prim::GetAttr[name="bias"](%772)
  %782 : Tensor = prim::GetAttr[name="weight"](%772)
  %783 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %783, %782, %781, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.attention/__module.roberta.encoder.layer.7.attention.output/__module.roberta.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %785 : __torch__.torch.nn.modules.linear.___torch_mangle_11022.Linear = prim::GetAttr[name="dense"](%718)
  %786 : Tensor = prim::GetAttr[name="bias"](%785)
  %787 : Tensor = prim::GetAttr[name="weight"](%785)
  %788 : Float(768:1, 3072:768) = aten::t(%787), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %788), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.47, %786, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate/__module.roberta.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %792 : __torch__.torch.nn.modules.normalization.___torch_mangle_11025.LayerNorm = prim::GetAttr[name="LayerNorm"](%717)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_11024.Linear = prim::GetAttr[name="dense"](%717)
  %794 : Tensor = prim::GetAttr[name="bias"](%793)
  %795 : Tensor = prim::GetAttr[name="weight"](%793)
  %796 : Float(3072:1, 768:3072) = aten::t(%795), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.82, %796), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.48, %794, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.83, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output # transformers/modeling_roberta.py:311:0
  %801 : Tensor = prim::GetAttr[name="bias"](%792)
  %802 : Tensor = prim::GetAttr[name="weight"](%792)
  %803 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm
  %input.85 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %803, %802, %801, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.7/__module.roberta.encoder.layer.7.output/__module.roberta.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %805 : __torch__.transformers.modeling_roberta.___torch_mangle_11044.RobertaOutput = prim::GetAttr[name="output"](%84)
  %806 : __torch__.transformers.modeling_roberta.___torch_mangle_11040.RobertaIntermediate = prim::GetAttr[name="intermediate"](%84)
  %807 : __torch__.transformers.modeling_roberta.___torch_mangle_11038.RobertaAttention = prim::GetAttr[name="attention"](%84)
  %808 : __torch__.transformers.modeling_roberta.___torch_mangle_11037.RobertaSelfOutput = prim::GetAttr[name="output"](%807)
  %809 : __torch__.transformers.modeling_roberta.___torch_mangle_11033.RobertaSelfAttention = prim::GetAttr[name="self"](%807)
  %810 : __torch__.torch.nn.modules.linear.___torch_mangle_11031.Linear = prim::GetAttr[name="value"](%809)
  %811 : __torch__.torch.nn.modules.linear.___torch_mangle_11030.Linear = prim::GetAttr[name="key"](%809)
  %812 : __torch__.torch.nn.modules.linear.___torch_mangle_11029.Linear = prim::GetAttr[name="query"](%809)
  %813 : Tensor = prim::GetAttr[name="bias"](%812)
  %814 : Tensor = prim::GetAttr[name="weight"](%812)
  %815 : Float(768:1, 768:768) = aten::t(%814), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %815), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.49, %813, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %818 : Tensor = prim::GetAttr[name="bias"](%811)
  %819 : Tensor = prim::GetAttr[name="weight"](%811)
  %820 : Float(768:1, 768:768) = aten::t(%819), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %820), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.50, %818, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %823 : Tensor = prim::GetAttr[name="bias"](%810)
  %824 : Tensor = prim::GetAttr[name="weight"](%810)
  %825 : Float(768:1, 768:768) = aten::t(%824), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.85, %825), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.51, %823, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %828 : int = aten::size(%x.49, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %829 : int = aten::size(%x.49, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %830 : int[] = prim::ListConstruct(%828, %829, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.50 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %830), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %832 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %query_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %832), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %834 : int = aten::size(%x.51, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %835 : int = aten::size(%x.51, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %836 : int[] = prim::ListConstruct(%834, %835, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.52 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %836), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %838 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %key_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %838), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %840 : int = aten::size(%x.53, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %841 : int = aten::size(%x.53, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:163:0
  %842 : int[] = prim::ListConstruct(%840, %841, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %x.54 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %842), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:164:0
  %844 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %value_layer.9 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %844), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:165:0
  %846 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.17 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %846), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.18 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:195:0
  %input.86 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:198:0
  %input.87 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self/__module.roberta.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:211:0
  %853 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %854 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %853), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.18 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%854, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:213:0
  %856 : int = aten::size(%context_layer.18, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %857 : int = aten::size(%context_layer.18, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:214:0
  %858 : int[] = prim::ListConstruct(%856, %857, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self
  %input.88 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.18, %858), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.self # transformers/modeling_roberta.py:215:0
  %860 : __torch__.torch.nn.modules.normalization.___torch_mangle_11035.LayerNorm = prim::GetAttr[name="LayerNorm"](%808)
  %861 : __torch__.torch.nn.modules.linear.___torch_mangle_11034.Linear = prim::GetAttr[name="dense"](%808)
  %862 : Tensor = prim::GetAttr[name="bias"](%861)
  %863 : Tensor = prim::GetAttr[name="weight"](%861)
  %864 : Float(768:1, 768:768) = aten::t(%863), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.88, %864), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.52, %862, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.89, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output # transformers/modeling_roberta.py:232:0
  %869 : Tensor = prim::GetAttr[name="bias"](%860)
  %870 : Tensor = prim::GetAttr[name="weight"](%860)
  %871 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %871, %870, %869, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.attention/__module.roberta.encoder.layer.8.attention.output/__module.roberta.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %873 : __torch__.torch.nn.modules.linear.___torch_mangle_11039.Linear = prim::GetAttr[name="dense"](%806)
  %874 : Tensor = prim::GetAttr[name="bias"](%873)
  %875 : Tensor = prim::GetAttr[name="weight"](%873)
  %876 : Float(768:1, 3072:768) = aten::t(%875), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %876), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.53, %874, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate/__module.roberta.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %880 : __torch__.torch.nn.modules.normalization.___torch_mangle_11042.LayerNorm = prim::GetAttr[name="LayerNorm"](%805)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_11041.Linear = prim::GetAttr[name="dense"](%805)
  %882 : Tensor = prim::GetAttr[name="bias"](%881)
  %883 : Tensor = prim::GetAttr[name="weight"](%881)
  %884 : Float(3072:1, 768:3072) = aten::t(%883), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.92, %884), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.54, %882, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.93, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output # transformers/modeling_roberta.py:311:0
  %889 : Tensor = prim::GetAttr[name="bias"](%880)
  %890 : Tensor = prim::GetAttr[name="weight"](%880)
  %891 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm
  %input.95 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %891, %890, %889, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.8/__module.roberta.encoder.layer.8.output/__module.roberta.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %893 : __torch__.transformers.modeling_roberta.___torch_mangle_11061.RobertaOutput = prim::GetAttr[name="output"](%82)
  %894 : __torch__.transformers.modeling_roberta.___torch_mangle_11057.RobertaIntermediate = prim::GetAttr[name="intermediate"](%82)
  %895 : __torch__.transformers.modeling_roberta.___torch_mangle_11055.RobertaAttention = prim::GetAttr[name="attention"](%82)
  %896 : __torch__.transformers.modeling_roberta.___torch_mangle_11054.RobertaSelfOutput = prim::GetAttr[name="output"](%895)
  %897 : __torch__.transformers.modeling_roberta.___torch_mangle_11050.RobertaSelfAttention = prim::GetAttr[name="self"](%895)
  %898 : __torch__.torch.nn.modules.linear.___torch_mangle_11048.Linear = prim::GetAttr[name="value"](%897)
  %899 : __torch__.torch.nn.modules.linear.___torch_mangle_11047.Linear = prim::GetAttr[name="key"](%897)
  %900 : __torch__.torch.nn.modules.linear.___torch_mangle_11046.Linear = prim::GetAttr[name="query"](%897)
  %901 : Tensor = prim::GetAttr[name="bias"](%900)
  %902 : Tensor = prim::GetAttr[name="weight"](%900)
  %903 : Float(768:1, 768:768) = aten::t(%902), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %903), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.55, %901, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %906 : Tensor = prim::GetAttr[name="bias"](%899)
  %907 : Tensor = prim::GetAttr[name="weight"](%899)
  %908 : Float(768:1, 768:768) = aten::t(%907), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %908), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.56, %906, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %911 : Tensor = prim::GetAttr[name="bias"](%898)
  %912 : Tensor = prim::GetAttr[name="weight"](%898)
  %913 : Float(768:1, 768:768) = aten::t(%912), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.95, %913), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.57, %911, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %916 : int = aten::size(%x.55, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %917 : int = aten::size(%x.55, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %918 : int[] = prim::ListConstruct(%916, %917, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.56 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %918), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %920 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %query_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %920), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %922 : int = aten::size(%x.57, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %923 : int = aten::size(%x.57, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %924 : int[] = prim::ListConstruct(%922, %923, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.58 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %924), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %926 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %key_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %926), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %928 : int = aten::size(%x.59, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %929 : int = aten::size(%x.59, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:163:0
  %930 : int[] = prim::ListConstruct(%928, %929, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %x.60 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %930), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:164:0
  %932 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %value_layer.10 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %932), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:165:0
  %934 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.19 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %934), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.20 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:195:0
  %input.96 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:198:0
  %input.97 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self/__module.roberta.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:211:0
  %941 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %942 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %941), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.20 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%942, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:213:0
  %944 : int = aten::size(%context_layer.20, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %945 : int = aten::size(%context_layer.20, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:214:0
  %946 : int[] = prim::ListConstruct(%944, %945, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self
  %input.98 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.20, %946), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.self # transformers/modeling_roberta.py:215:0
  %948 : __torch__.torch.nn.modules.normalization.___torch_mangle_11052.LayerNorm = prim::GetAttr[name="LayerNorm"](%896)
  %949 : __torch__.torch.nn.modules.linear.___torch_mangle_11051.Linear = prim::GetAttr[name="dense"](%896)
  %950 : Tensor = prim::GetAttr[name="bias"](%949)
  %951 : Tensor = prim::GetAttr[name="weight"](%949)
  %952 : Float(768:1, 768:768) = aten::t(%951), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.98, %952), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.58, %950, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.99, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output # transformers/modeling_roberta.py:232:0
  %957 : Tensor = prim::GetAttr[name="bias"](%948)
  %958 : Tensor = prim::GetAttr[name="weight"](%948)
  %959 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %959, %958, %957, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.attention/__module.roberta.encoder.layer.9.attention.output/__module.roberta.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %961 : __torch__.torch.nn.modules.linear.___torch_mangle_11056.Linear = prim::GetAttr[name="dense"](%894)
  %962 : Tensor = prim::GetAttr[name="bias"](%961)
  %963 : Tensor = prim::GetAttr[name="weight"](%961)
  %964 : Float(768:1, 3072:768) = aten::t(%963), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %964), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.59, %962, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate/__module.roberta.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %968 : __torch__.torch.nn.modules.normalization.___torch_mangle_11059.LayerNorm = prim::GetAttr[name="LayerNorm"](%893)
  %969 : __torch__.torch.nn.modules.linear.___torch_mangle_11058.Linear = prim::GetAttr[name="dense"](%893)
  %970 : Tensor = prim::GetAttr[name="bias"](%969)
  %971 : Tensor = prim::GetAttr[name="weight"](%969)
  %972 : Float(3072:1, 768:3072) = aten::t(%971), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.102, %972), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.60, %970, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.103, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output # transformers/modeling_roberta.py:311:0
  %977 : Tensor = prim::GetAttr[name="bias"](%968)
  %978 : Tensor = prim::GetAttr[name="weight"](%968)
  %979 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm
  %input.105 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %979, %978, %977, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.9/__module.roberta.encoder.layer.9.output/__module.roberta.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %981 : __torch__.transformers.modeling_roberta.___torch_mangle_11078.RobertaOutput = prim::GetAttr[name="output"](%80)
  %982 : __torch__.transformers.modeling_roberta.___torch_mangle_11074.RobertaIntermediate = prim::GetAttr[name="intermediate"](%80)
  %983 : __torch__.transformers.modeling_roberta.___torch_mangle_11072.RobertaAttention = prim::GetAttr[name="attention"](%80)
  %984 : __torch__.transformers.modeling_roberta.___torch_mangle_11071.RobertaSelfOutput = prim::GetAttr[name="output"](%983)
  %985 : __torch__.transformers.modeling_roberta.___torch_mangle_11067.RobertaSelfAttention = prim::GetAttr[name="self"](%983)
  %986 : __torch__.torch.nn.modules.linear.___torch_mangle_11065.Linear = prim::GetAttr[name="value"](%985)
  %987 : __torch__.torch.nn.modules.linear.___torch_mangle_11064.Linear = prim::GetAttr[name="key"](%985)
  %988 : __torch__.torch.nn.modules.linear.___torch_mangle_11063.Linear = prim::GetAttr[name="query"](%985)
  %989 : Tensor = prim::GetAttr[name="bias"](%988)
  %990 : Tensor = prim::GetAttr[name="weight"](%988)
  %991 : Float(768:1, 768:768) = aten::t(%990), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %991), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.61, %989, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %994 : Tensor = prim::GetAttr[name="bias"](%987)
  %995 : Tensor = prim::GetAttr[name="weight"](%987)
  %996 : Float(768:1, 768:768) = aten::t(%995), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %996), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.62, %994, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %999 : Tensor = prim::GetAttr[name="bias"](%986)
  %1000 : Tensor = prim::GetAttr[name="weight"](%986)
  %1001 : Float(768:1, 768:768) = aten::t(%1000), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.105, %1001), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.63, %999, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1004 : int = aten::size(%x.61, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1005 : int = aten::size(%x.61, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1006 : int[] = prim::ListConstruct(%1004, %1005, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.62 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %1006), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1008 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %query_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1008), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1010 : int = aten::size(%x.63, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1011 : int = aten::size(%x.63, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1012 : int[] = prim::ListConstruct(%1010, %1011, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.64 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1012), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1014 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %key_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1014), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1016 : int = aten::size(%x.65, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1017 : int = aten::size(%x.65, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:163:0
  %1018 : int[] = prim::ListConstruct(%1016, %1017, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %x.66 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1018), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:164:0
  %1020 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %value_layer.11 : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1020), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:165:0
  %1022 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.21 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1022), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.22 : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:195:0
  %input.106 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:198:0
  %input.107 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self/__module.roberta.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:211:0
  %1029 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %1030 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1029), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer.22 : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1030, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:213:0
  %1032 : int = aten::size(%context_layer.22, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1033 : int = aten::size(%context_layer.22, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:214:0
  %1034 : int[] = prim::ListConstruct(%1032, %1033, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self
  %input.108 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1034), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.self # transformers/modeling_roberta.py:215:0
  %1036 : __torch__.torch.nn.modules.normalization.___torch_mangle_11069.LayerNorm = prim::GetAttr[name="LayerNorm"](%984)
  %1037 : __torch__.torch.nn.modules.linear.___torch_mangle_11068.Linear = prim::GetAttr[name="dense"](%984)
  %1038 : Tensor = prim::GetAttr[name="bias"](%1037)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1037)
  %1040 : Float(768:1, 768:768) = aten::t(%1039), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.108, %1040), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.64, %1038, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.109, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output # transformers/modeling_roberta.py:232:0
  %1045 : Tensor = prim::GetAttr[name="bias"](%1036)
  %1046 : Tensor = prim::GetAttr[name="weight"](%1036)
  %1047 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1047, %1046, %1045, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.attention/__module.roberta.encoder.layer.10.attention.output/__module.roberta.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1049 : __torch__.torch.nn.modules.linear.___torch_mangle_11073.Linear = prim::GetAttr[name="dense"](%982)
  %1050 : Tensor = prim::GetAttr[name="bias"](%1049)
  %1051 : Tensor = prim::GetAttr[name="weight"](%1049)
  %1052 : Float(768:1, 3072:768) = aten::t(%1051), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1052), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1050, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate/__module.roberta.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1056 : __torch__.torch.nn.modules.normalization.___torch_mangle_11076.LayerNorm = prim::GetAttr[name="LayerNorm"](%981)
  %1057 : __torch__.torch.nn.modules.linear.___torch_mangle_11075.Linear = prim::GetAttr[name="dense"](%981)
  %1058 : Tensor = prim::GetAttr[name="bias"](%1057)
  %1059 : Tensor = prim::GetAttr[name="weight"](%1057)
  %1060 : Float(3072:1, 768:3072) = aten::t(%1059), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.112, %1060), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.66, %1058, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.113, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output # transformers/modeling_roberta.py:311:0
  %1065 : Tensor = prim::GetAttr[name="bias"](%1056)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1056)
  %1067 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm
  %input.115 : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1067, %1066, %1065, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.10/__module.roberta.encoder.layer.10.output/__module.roberta.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1069 : __torch__.transformers.modeling_roberta.___torch_mangle_11095.RobertaOutput = prim::GetAttr[name="output"](%78)
  %1070 : __torch__.transformers.modeling_roberta.___torch_mangle_11091.RobertaIntermediate = prim::GetAttr[name="intermediate"](%78)
  %1071 : __torch__.transformers.modeling_roberta.___torch_mangle_11089.RobertaAttention = prim::GetAttr[name="attention"](%78)
  %1072 : __torch__.transformers.modeling_roberta.___torch_mangle_11088.RobertaSelfOutput = prim::GetAttr[name="output"](%1071)
  %1073 : __torch__.transformers.modeling_roberta.___torch_mangle_11084.RobertaSelfAttention = prim::GetAttr[name="self"](%1071)
  %1074 : __torch__.torch.nn.modules.linear.___torch_mangle_11082.Linear = prim::GetAttr[name="value"](%1073)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_11081.Linear = prim::GetAttr[name="key"](%1073)
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_11080.Linear = prim::GetAttr[name="query"](%1073)
  %1077 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1078 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1079 : Float(768:1, 768:768) = aten::t(%1078), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1079), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.67, %1077, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1082 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1083 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1084 : Float(768:1, 768:768) = aten::t(%1083), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1084), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.68, %1082, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1087 : Tensor = prim::GetAttr[name="bias"](%1074)
  %1088 : Tensor = prim::GetAttr[name="weight"](%1074)
  %1089 : Float(768:1, 768:768) = aten::t(%1088), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.115, %1089), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.69, %1087, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1092 : int = aten::size(%x.67, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1093 : int = aten::size(%x.67, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1094 : int[] = prim::ListConstruct(%1092, %1093, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.68 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1094), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1096 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %query_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1096), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1098 : int = aten::size(%x.69, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1099 : int = aten::size(%x.69, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1100 : int[] = prim::ListConstruct(%1098, %1099, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x.70 : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1100), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1102 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %key_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1102), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1104 : int = aten::size(%x.71, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1105 : int = aten::size(%x.71, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:163:0
  %1106 : int[] = prim::ListConstruct(%1104, %1105, %20, %19), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %x : Float(17:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1106), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:164:0
  %1108 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %value_layer : Float(17:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1108), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:165:0
  %1110 : Float(17:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %22, %18), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores.23 : Float(17:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1110), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:194:0
  %attention_scores : Float(17:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %17), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:195:0
  %input.116 : Float(17:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:198:0
  %input.117 : Float(17:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %22, %29), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self/__module.roberta.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(17:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:211:0
  %1117 : int[] = prim::ListConstruct(%38, %32, %37, %31), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %1118 : Float(17:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1117), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %context_layer : Float(17:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1118, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:213:0
  %1120 : int = aten::size(%context_layer, %38), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1121 : int = aten::size(%context_layer, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:214:0
  %1122 : int[] = prim::ListConstruct(%1120, %1121, %25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self
  %input.118 : Float(17:9984, 13:768, 768:1) = aten::view(%context_layer, %1122), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.self # transformers/modeling_roberta.py:215:0
  %1124 : __torch__.torch.nn.modules.normalization.___torch_mangle_11086.LayerNorm = prim::GetAttr[name="LayerNorm"](%1072)
  %1125 : __torch__.torch.nn.modules.linear.___torch_mangle_11085.Linear = prim::GetAttr[name="dense"](%1072)
  %1126 : Tensor = prim::GetAttr[name="bias"](%1125)
  %1127 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1128 : Float(768:1, 768:768) = aten::t(%1127), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.118, %1128), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.70, %1126, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.119, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output # transformers/modeling_roberta.py:232:0
  %1133 : Tensor = prim::GetAttr[name="bias"](%1124)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1124)
  %1135 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1135, %1134, %1133, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.attention/__module.roberta.encoder.layer.11.attention.output/__module.roberta.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1137 : __torch__.torch.nn.modules.linear.___torch_mangle_11090.Linear = prim::GetAttr[name="dense"](%1070)
  %1138 : Tensor = prim::GetAttr[name="bias"](%1137)
  %1139 : Tensor = prim::GetAttr[name="weight"](%1137)
  %1140 : Float(768:1, 3072:768) = aten::t(%1139), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(17:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1140), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(17:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1138, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate/__module.roberta.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(17:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1144 : __torch__.torch.nn.modules.normalization.___torch_mangle_11093.LayerNorm = prim::GetAttr[name="LayerNorm"](%1069)
  %1145 : __torch__.torch.nn.modules.linear.___torch_mangle_11092.Linear = prim::GetAttr[name="dense"](%1069)
  %1146 : Tensor = prim::GetAttr[name="bias"](%1145)
  %1147 : Tensor = prim::GetAttr[name="weight"](%1145)
  %1148 : Float(3072:1, 768:3072) = aten::t(%1147), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output.72 : Float(17:9984, 13:768, 768:1) = aten::matmul(%input.122, %1148), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(17:9984, 13:768, 768:1) = aten::add_(%output.72, %1146, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states : Float(17:9984, 13:768, 768:1) = aten::dropout(%input.123, %26, %34), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(17:9984, 13:768, 768:1) = aten::add(%hidden_states, %input_tensor, %37), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output # transformers/modeling_roberta.py:311:0
  %1153 : Tensor = prim::GetAttr[name="bias"](%1144)
  %1154 : Tensor = prim::GetAttr[name="weight"](%1144)
  %1155 : int[] = prim::ListConstruct(%25), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm
  %input : Float(17:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1155, %1154, %1153, %24, %23), scope: __module.roberta/__module.roberta.encoder/__module.roberta.encoder.layer.11/__module.roberta.encoder.layer.11.output/__module.roberta.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1157 : int = prim::Constant[value=1](), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %1158 : Tensor = prim::GetAttr[name="bias"](%3)
  %1159 : Tensor = prim::GetAttr[name="weight"](%3)
  %1160 : Float(768:1, 2:768) = aten::t(%1159), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1160), scope: __module.qa_outputs # torch/nn/functional.py:1676:0
  %1162 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1158, %1157), scope: __module.qa_outputs # torch/nn/functional.py:1678:0
  %7 : int = prim::Constant[value=1]() # torch/tensor.py:371:0
  %8 : int = prim::Constant[value=-1]() # torch/tensor.py:371:0
  %9 : Tensor[] = aten::split(%1162, %7, %8) # torch/tensor.py:371:0
  %start_logits : Float(17:26, 13:2, 1:1), %end_logits : Float(17:26, 13:2, 1:1) = prim::ListUnpack(%9)
  %12 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1290:0
  %13 : Float(17:26, 13:2) = aten::squeeze(%start_logits, %12) # transformers/modeling_roberta.py:1290:0
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_roberta.py:1291:0
  %15 : Float(17:26, 13:2) = aten::squeeze(%end_logits, %14) # transformers/modeling_roberta.py:1291:0
  %16 : (Float(17:26, 13:2), Float(17:26, 13:2)) = prim::TupleConstruct(%13, %15)
  return (%16)
