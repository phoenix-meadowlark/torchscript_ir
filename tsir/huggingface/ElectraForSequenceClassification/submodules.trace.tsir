ElectraForSequenceClassification(
  (electra): ElectraModel(
    (embeddings): ElectraEmbeddings(
      (word_embeddings): Embedding(30522, 128, padding_idx=0)
      (position_embeddings): Embedding(512, 128)
      (token_type_embeddings): Embedding(2, 128)
      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)
    (encoder): ElectraEncoder(
      (layer): ModuleList(
        (0): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (1): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (2): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (3): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (4): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (5): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (6): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (7): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (8): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (9): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (10): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (11): ElectraLayer(
          (attention): ElectraAttention(
            (self): ElectraSelfAttention(
              (query): Linear(in_features=256, out_features=256, bias=True)
              (key): Linear(in_features=256, out_features=256, bias=True)
              (value): Linear(in_features=256, out_features=256, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): ElectraSelfOutput(
              (dense): Linear(in_features=256, out_features=256, bias=True)
              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): ElectraIntermediate(
            (dense): Linear(in_features=256, out_features=1024, bias=True)
          )
          (output): ElectraOutput(
            (dense): Linear(in_features=1024, out_features=256, bias=True)
            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
  )
  (classifier): ElectraClassificationHead(
    (dense): Linear(in_features=256, out_features=256, bias=True)
    (dropout): Dropout(p=0.1, inplace=False)
    (out_proj): Linear(in_features=256, out_features=2, bias=True)
  )
)

ElectraForSequenceClassification._actual_script_module
ElectraForSequenceClassification.forward
  graph(%self.1 : __torch__.transformers.modeling_electra.ElectraForSequenceClassification,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %3433 : __torch__.transformers.modeling_electra.ElectraClassificationHead = prim::GetAttr[name="classifier"](%self.1)
    %3425 : __torch__.transformers.modeling_electra.ElectraModel = prim::GetAttr[name="electra"](%self.1)
    %3652 : Tensor = prim::CallMethod[name="forward"](%3425, %input_ids, %attention_mask.1)
    %3653 : Tensor = prim::CallMethod[name="forward"](%3433, %3652)
    %2782 : (Float(17:2, 2:1)) = prim::TupleConstruct(%3653)
    return (%2782)

ElectraForSequenceClassification.classifier
ElectraClassificationHead._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_electra.ElectraClassificationHead,
        %9 : Float(17:3328, 13:256, 256:1)):
    %1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="out_proj"](%self.215)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.215)
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.215)
    %4 : int = prim::Constant[value=0](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %5 : int = prim::Constant[value=0](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %6 : int = prim::Constant[value=9223372036854775807](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %7 : int = prim::Constant[value=1](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %8 : Float(17:3328, 13:256, 256:1) = aten::slice(%9, %4, %5, %6, %7), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %10 : int = prim::Constant[value=1](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %11 : int = prim::Constant[value=0](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %12 : Float(17:3328, 256:1) = aten::select(%8, %10, %11), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %13 : int = prim::Constant[value=1](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %14 : int = prim::Constant[value=0](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %15 : int = prim::Constant[value=9223372036854775807](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %16 : int = prim::Constant[value=1](), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %input.126 : Float(17:3328, 256:1) = aten::slice(%12, %13, %14, %15, %16), scope: __module.classifier # transformers/modeling_electra.py:769:0
    %23 : Tensor = prim::CallMethod[name="forward"](%3, %input.126)
    %24 : Tensor = prim::CallMethod[name="forward"](%2, %23)
    %input.129 : Float(17:256, 256:1) = aten::gelu(%24), scope: __module.classifier # torch/nn/functional.py:1369:0
    %25 : Tensor = prim::CallMethod[name="forward1"](%3, %input.129)
    %26 : Tensor = prim::CallMethod[name="forward"](%1, %25)
    return (%26)

ElectraForSequenceClassification.electra
ElectraModel._actual_script_module
  graph(%self.2 : __torch__.transformers.modeling_electra.ElectraModel,
        %input_ids : Long(17:13, 13:1),
        %attention_mask.1 : Long(17:13, 13:1)):
    %1 : __torch__.transformers.modeling_electra.ElectraEncoder = prim::GetAttr[name="encoder"](%self.2)
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="embeddings_project"](%self.2)
    %3 : __torch__.transformers.modeling_electra.ElectraEmbeddings = prim::GetAttr[name="embeddings"](%self.2)
    %4 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:724:0
    %5 : int = aten::size(%input_ids, %4), scope: __module.electra # transformers/modeling_electra.py:724:0
    %7 : Long() = prim::NumToTensor(%5), scope: __module.electra
    %8 : int = aten::Int(%7), scope: __module.electra
    %9 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_electra.py:724:0
    %10 : int = aten::size(%input_ids, %9), scope: __module.electra # transformers/modeling_electra.py:724:0
    %11 : Long() = prim::NumToTensor(%10), scope: __module.electra
    %12 : int = aten::Int(%11), scope: __module.electra
    %13 : int[] = prim::ListConstruct(%8, %12), scope: __module.electra
    %14 : int = prim::Constant[value=4](), scope: __module.electra # transformers/modeling_electra.py:735:0
    %15 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:735:0
    %16 : Device = prim::Constant[value="cpu"](), scope: __module.electra # transformers/modeling_electra.py:735:0
    %17 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_electra.py:735:0
    %input.2 : Long(17:13, 13:1) = aten::zeros(%13, %14, %15, %16, %17), scope: __module.electra # transformers/modeling_electra.py:735:0
    %19 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %20 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %21 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %22 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %23 : Long(17:13, 13:1) = aten::slice(%attention_mask.1, %19, %20, %21, %22), scope: __module.electra # transformers/modeling_utils.py:244:0
    %25 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %26 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%23, %25), scope: __module.electra # transformers/modeling_utils.py:244:0
    %27 : int = prim::Constant[value=2](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %28 : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%26, %27), scope: __module.electra # transformers/modeling_utils.py:244:0
    %29 : int = prim::Constant[value=3](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %30 : int = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %31 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %32 : int = prim::Constant[value=1](), scope: __module.electra # transformers/modeling_utils.py:244:0
    %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::slice(%28, %29, %30, %31, %32), scope: __module.electra # transformers/modeling_utils.py:244:0
    %34 : int = prim::Constant[value=6](), scope: __module.electra # transformers/modeling_utils.py:257:0
    %35 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_utils.py:257:0
    %36 : bool = prim::Constant[value=0](), scope: __module.electra # transformers/modeling_utils.py:257:0
    %37 : None = prim::Constant(), scope: __module.electra
    %38 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %34, %35, %36, %37), scope: __module.electra # transformers/modeling_utils.py:257:0
    %39 : float = prim::Constant[value=1.](), scope: __module.electra # torch/tensor.py:396:0
    %40 : int = prim::Constant[value=1](), scope: __module.electra # torch/tensor.py:396:0
    %41 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%38, %39, %40), scope: __module.electra # torch/tensor.py:396:0
    %42 : Double() = prim::Constant[value={-10000}](), scope: __module.electra # transformers/modeling_utils.py:258:0
    %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%41, %42), scope: __module.electra # transformers/modeling_utils.py:258:0
    %47 : Tensor = prim::CallMethod[name="forward"](%3, %input_ids, %input.2)
    %48 : Tensor = prim::CallMethod[name="forward"](%2, %47)
    %49 : Tensor = prim::CallMethod[name="forward"](%1, %48, %attention_mask)
    return (%49)

ElectraModel.embeddings
ElectraEmbeddings._actual_script_module
  graph(%self.3 : __torch__.transformers.modeling_electra.ElectraEmbeddings,
        %input_ids : Long(17:13, 13:1),
        %input.2 : Long(17:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.3)
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.3)
    %5 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="token_type_embeddings"](%self.3)
    %6 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="position_embeddings"](%self.3)
    %7 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embeddings"](%self.3)
    %8 : Tensor = prim::GetAttr[name="position_ids"](%self.3)
    %12 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:173:0
    %13 : int = aten::size(%input_ids, %12), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:173:0
    %seq_length : Long() = prim::NumToTensor(%13), scope: __module.electra/__module.electra.embeddings
    %15 : int = aten::Int(%seq_length), scope: __module.electra/__module.electra.embeddings
    %16 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %17 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %18 : int = prim::Constant[value=9223372036854775807](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %19 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %20 : Long(1:512, 512:1) = aten::slice(%8, %16, %17, %18, %19), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %21 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %22 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %23 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %input.1 : Long(1:512, 13:1) = aten::slice(%20, %21, %22, %15, %23), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:180:0
    %34 : Tensor = prim::CallMethod[name="forward"](%7, %input_ids)
    %35 : Tensor = prim::CallMethod[name="forward"](%6, %input.1)
    %36 : Tensor = prim::CallMethod[name="forward"](%5, %input.2)
    %28 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
    %29 : Float(17:1664, 13:128, 128:1) = aten::add(%34, %35, %28), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
    %30 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
    %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%29, %36, %30), scope: __module.electra/__module.electra.embeddings # transformers/modeling_electra.py:190:0
    %37 : Tensor = prim::CallMethod[name="forward"](%4, %input.3)
    %38 : Tensor = prim::CallMethod[name="forward"](%3, %37)
    return (%38)

ElectraModel.embeddings_project
Linear._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : Float(128:1, 256:128) = aten::t(%3), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
    %output.1 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1678:0
    %input.6 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.1, %2, %6), scope: __module.electra/__module.electra.embeddings_project # torch/nn/functional.py:1678:0
    return (%input.6)

ElectraModel.encoder
ElectraEncoder._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_electra.ElectraEncoder,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %4 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="11"](%3)
    %5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %6 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="10"](%5)
    %7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %8 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="9"](%7)
    %9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %10 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="8"](%9)
    %11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %12 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="7"](%11)
    %13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %14 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="6"](%13)
    %15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %16 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="5"](%15)
    %17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %18 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="4"](%17)
    %19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %20 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="3"](%19)
    %21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %22 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="2"](%21)
    %23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %24 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="1"](%23)
    %25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.10)
    %26 : __torch__.transformers.modeling_electra.ElectraLayer = prim::GetAttr[name="0"](%25)
    %39 : Tensor = prim::CallMethod[name="forward"](%26, %1, %attention_mask)
    %40 : Tensor = prim::CallMethod[name="forward"](%24, %39, %attention_mask)
    %41 : Tensor = prim::CallMethod[name="forward"](%22, %40, %attention_mask)
    %42 : Tensor = prim::CallMethod[name="forward"](%20, %41, %attention_mask)
    %43 : Tensor = prim::CallMethod[name="forward"](%18, %42, %attention_mask)
    %44 : Tensor = prim::CallMethod[name="forward"](%16, %43, %attention_mask)
    %45 : Tensor = prim::CallMethod[name="forward"](%14, %44, %attention_mask)
    %46 : Tensor = prim::CallMethod[name="forward"](%12, %45, %attention_mask)
    %47 : Tensor = prim::CallMethod[name="forward"](%10, %46, %attention_mask)
    %48 : Tensor = prim::CallMethod[name="forward"](%8, %47, %attention_mask)
    %49 : Tensor = prim::CallMethod[name="forward"](%6, %48, %attention_mask)
    %50 : Tensor = prim::CallMethod[name="forward"](%4, %49, %attention_mask)
    return (%50)

ElectraEmbeddings.LayerNorm
LayerNorm._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.3 : Float(17:1664, 13:128, 128:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.7)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.7)
    %4 : int = prim::Constant[value=128](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.4)

ElectraEmbeddings.dropout
Dropout._actual_script_module
  graph(%self.8 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:1664, 13:128, 128:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
    %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.dropout # torch/nn/functional.py:973:0
    return (%input.5)

ElectraEmbeddings.position_embeddings
Embedding._actual_script_module
  graph(%self.5 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.1 : Long(1:512, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.5)
    %3 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%2, %input.1, %3, %4, %5), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.position_embeddings # torch/nn/functional.py:1814:0
    return (%position_embeddings)

ElectraEmbeddings.token_type_embeddings
Embedding._actual_script_module
  graph(%self.6 : __torch__.torch.nn.modules.sparse.Embedding,
        %input.2 : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.6)
    %3 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%2, %input.2, %3, %4, %5), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
    return (%token_type_embeddings)

ElectraEmbeddings.word_embeddings
Embedding._actual_script_module
  graph(%self.4 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(17:13, 13:1)):
    %2 : Tensor = prim::GetAttr[name="weight"](%self.4)
    %3 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %5 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%2, %input_ids, %3, %4, %5), scope: __module.electra/__module.electra.embeddings/__module.electra.embeddings.word_embeddings # torch/nn/functional.py:1814:0
    return (%inputs_embeds)

ModuleList.*
  module had no methods with graph attrs.

ElectraLayer._actual_script_module
  graph(%self.11 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.11)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.11)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.11)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.12 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.12)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.12)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.22 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.22)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.13 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
    return (%input.13)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.24 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.24)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.24)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.24)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output # transformers/modeling_electra.py:365:0
    %input.15 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.15)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.18 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.18)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.18)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.18)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
    %input.11 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.11)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.13 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.13)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.13)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.13)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.13)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %x.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %query_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.2, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %x.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %key_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.4, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %x.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %value_layer.1 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.6, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.1, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.1, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
    %input.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %input.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.7, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.8)
    %context_layer.1 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.1), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.1, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.2 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.2, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.2, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self
    %input.9 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.2, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self # transformers/modeling_electra.py:269:0
    return (%input.9)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.17 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.8 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.1 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.8, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.1)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.15)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.15)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %output.3 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    %x.3 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.3, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.3)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.14 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.14)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.14)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %output.2 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    %x.1 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.2, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.1)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.16 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.16)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.16)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %output.4 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    %x.5 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.4, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.self/__module.electra.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.5)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.21 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.11 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.21)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.21)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.11, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.19 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.19)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.19)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %output.5 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    %input.10 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.5, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.10)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.1 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.attention/__module.electra.encoder.layer.0.attention.output/__module.electra.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.1)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.23)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.23)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %output.6 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    %input.12 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.6, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.intermediate/__module.electra.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.12)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.27 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.15 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.27)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.27)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.16 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.15, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.16)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.25 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.25)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.25)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %output.7 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    %input.14 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.7, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
    return (%input.14)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.2 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.0/__module.electra.encoder.layer.0.output/__module.electra.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.2)

ElectraLayer._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.28)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.28)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.28)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.29 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.29)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.29)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.39)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.23 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
    return (%input.23)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.41 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.41)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.41)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.41)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output # transformers/modeling_electra.py:365:0
    %input.25 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.25)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.35 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.35)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.35)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.35)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
    %input.21 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.21)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.30 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.30)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.30)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.30)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.30)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %x.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %query_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.8, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %x.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %key_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.10, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %x.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %value_layer.2 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.12, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.2, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.3, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
    %input.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %input.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.17, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.18)
    %context_layer.3 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.2), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.3, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.4 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.4, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.4, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self
    %input.19 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.4, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self # transformers/modeling_electra.py:269:0
    return (%input.19)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.18 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.2 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.18, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.2)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.32 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.32)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.32)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %output.9 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    %x.9 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.9, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.9)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %output.8 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    %x.7 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.8, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.7)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %output.10 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    %x.11 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.10, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.self/__module.electra.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.11)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.38 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.21 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.38)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.38)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.21, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.36 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.36)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.36)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %output.11 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    %input.20 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.11, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.20)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.3 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.attention/__module.electra.encoder.layer.1.attention.output/__module.electra.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.3)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.40)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.40)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %output.12 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    %input.22 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.12, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.intermediate/__module.electra.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.22)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.25 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.26 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.25, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.26)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %output.13 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    %input.24 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.13, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
    return (%input.24)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.43 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.4 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.1/__module.electra.encoder.layer.1.output/__module.electra.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.4)

ElectraLayer._actual_script_module
  graph(%self.45 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.45)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.45)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.45)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.46 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.46)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.46)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.56 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.56)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.33 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
    return (%input.33)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.58 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.58)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.58)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.58)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output # transformers/modeling_electra.py:365:0
    %input.35 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.35)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.52 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.52)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.52)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.52)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
    %input.31 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.31)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.47 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.47)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.47)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.47)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.47)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %x.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %query_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.14, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %x.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %key_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.16, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %x.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %value_layer.3 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.18, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.3, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.5, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
    %input.27 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %input.28 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.27, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.28)
    %context_layer.5 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.5, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.6 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.6, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.6, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self
    %input.29 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.6, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self # transformers/modeling_electra.py:269:0
    return (%input.29)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.28 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.3 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.28, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.3)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.49 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.49)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.49)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %output.15 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    %x.15 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.15, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.15)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %output.14 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    %x.13 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.14, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.13)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.50 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.50)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.50)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %output.16 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    %x.17 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.16, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.self/__module.electra.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.17)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.31 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.31, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %output.17 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    %input.30 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.17, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.30)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.54 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.5 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.attention/__module.electra.encoder.layer.2.attention.output/__module.electra.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.5)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %output.18 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    %input.32 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.18, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.intermediate/__module.electra.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.32)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.61 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.35 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.61)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.61)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.36 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.35, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.36)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %output.19 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    %input.34 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.19, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
    return (%input.34)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.60 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.6 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.2/__module.electra.encoder.layer.2.output/__module.electra.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.6)

ElectraLayer._actual_script_module
  graph(%self.62 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.62)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.62)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.62)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.63 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.63)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.63)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.73 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.73)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.43 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
    return (%input.43)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.75 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.75)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.75)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.75)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output # transformers/modeling_electra.py:365:0
    %input.45 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.45)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.69 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.69)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.69)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.69)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
    %input.41 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.41)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.64 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.64)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.64)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.64)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.64)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %x.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %query_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.20, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %x.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %key_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.22, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %x.24 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %value_layer.4 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.24, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.4, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.7, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
    %input.37 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %input.38 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.37, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.38)
    %context_layer.7 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.7, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.8 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.8, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.8, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self
    %input.39 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.8, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self # transformers/modeling_electra.py:269:0
    return (%input.39)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.38 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.4 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.38, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.4)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %output.21 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    %x.21 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.21, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.21)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.65 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.65)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.65)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %output.20 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    %x.19 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.20, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.19)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.67)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.67)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %output.22 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    %x.23 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.22, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.self/__module.electra.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.23)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.72 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.41 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.72)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.72)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.41, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %output.23 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    %input.40 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.23, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.40)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.71 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.7 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.attention/__module.electra.encoder.layer.3.attention.output/__module.electra.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.7)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.74 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.74)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.74)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %output.24 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    %input.42 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.24, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.intermediate/__module.electra.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.42)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.45 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.78)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.78)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.46 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.45, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.46)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.76 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.76)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.76)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %output.25 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    %input.44 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.25, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
    return (%input.44)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.8 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.3/__module.electra.encoder.layer.3.output/__module.electra.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.8)

ElectraLayer._actual_script_module
  graph(%self.79 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.79)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.79)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.79)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.80 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.80)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.80)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.90 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.90)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.53 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
    return (%input.53)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.92 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.92)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.92)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.92)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output # transformers/modeling_electra.py:365:0
    %input.55 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.55)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.86 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.86)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.86)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.86)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
    %input.51 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.51)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.81 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.81)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.81)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.81)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.81)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %x.26 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %query_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.26, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %x.28 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %key_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.28, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %x.30 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %value_layer.5 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.30, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.5, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.9, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
    %input.47 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %input.48 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.47, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.48)
    %context_layer.9 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.9, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.10 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.10, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.10, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self
    %input.49 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.10, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self # transformers/modeling_electra.py:269:0
    return (%input.49)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.85 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.48 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.5 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.48, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.5)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.83 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.83)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.83)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %output.27 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    %x.27 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.27, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.27)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.82 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.82)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.82)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %output.26 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    %x.25 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.26, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.25)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.84)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.84)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %output.28 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    %x.29 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.28, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.self/__module.electra.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.29)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.51 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.89)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.89)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.51, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.87 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.87)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.87)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %output.29 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    %input.50 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.29, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.50)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.9 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.attention/__module.electra.encoder.layer.4.attention.output/__module.electra.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.9)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.91 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.91)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.91)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %output.30 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    %input.52 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.30, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.intermediate/__module.electra.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.52)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.55 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.95)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.95)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.56 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.55, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.56)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.93 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.93)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.93)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %output.31 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    %input.54 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.31, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
    return (%input.54)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.94 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.10 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.4/__module.electra.encoder.layer.4.output/__module.electra.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.10)

ElectraLayer._actual_script_module
  graph(%self.96 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.96)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.96)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.96)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.97 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.97)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.97)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.107 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.107)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.63 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
    return (%input.63)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.109 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.109)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.109)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.109)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output # transformers/modeling_electra.py:365:0
    %input.65 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.65)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.103 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.103)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.103)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.103)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
    %input.61 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.61)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.98 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.98)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.98)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.98)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.98)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %x.32 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %query_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.32, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %x.34 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %key_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.34, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %x.36 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %value_layer.6 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.36, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.6, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.12 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.11, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
    %input.57 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %input.58 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.57, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.58)
    %context_layer.11 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.11, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.12 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.12, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.12, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self
    %input.59 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.12, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self # transformers/modeling_electra.py:269:0
    return (%input.59)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.102 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.58 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.6 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.58, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.6)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.100)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.100)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %output.33 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    %x.33 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.33, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.33)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %output.32 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    %x.31 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.32, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.31)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.101)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.101)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %output.34 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    %x.35 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.34, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.self/__module.electra.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.35)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.61 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.106)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.106)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.61, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.104 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.104)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.104)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %output.35 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    %input.60 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.35, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.60)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.105 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.11 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.attention/__module.electra.encoder.layer.5.attention.output/__module.electra.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.11)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %output.36 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    %input.62 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.36, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.intermediate/__module.electra.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.62)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.65 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.66 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.65, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.66)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %output.37 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    %input.64 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.37, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
    return (%input.64)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.12 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.5/__module.electra.encoder.layer.5.output/__module.electra.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.12)

ElectraLayer._actual_script_module
  graph(%self.113 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.113)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.113)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.113)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.114 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.114)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.114)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.124 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.124)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.73 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
    return (%input.73)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.126 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.126)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.126)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.126)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output # transformers/modeling_electra.py:365:0
    %input.75 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.75)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.120 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.120)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.120)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.120)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
    %input.71 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.71)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.115)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.115)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.115)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.115)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %x.38 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %query_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.38, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %x.40 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %key_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.40, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %x.42 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %value_layer.7 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.42, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.7, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.13 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.14 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.13, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
    %input.67 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %input.68 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.67, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.68)
    %context_layer.13 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.13, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.14 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.14, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.14, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self
    %input.69 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.14, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self # transformers/modeling_electra.py:269:0
    return (%input.69)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.68 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.7 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.68, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.7)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.117)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.117)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %output.39 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    %x.39 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.39, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.39)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.116 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.116)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.116)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %output.38 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    %x.37 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.38, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.37)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.118 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.118)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.118)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %output.40 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    %x.41 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.40, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.self/__module.electra.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.41)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.71 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.71, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %output.41 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    %input.70 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.41, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.70)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.13 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.attention/__module.electra.encoder.layer.6.attention.output/__module.electra.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.13)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %output.42 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    %input.72 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.42, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.intermediate/__module.electra.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.72)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.129 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.75 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.129)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.129)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.76 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.75, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.76)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.127 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.127)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.127)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %output.43 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    %input.74 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.43, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
    return (%input.74)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.128 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.14 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.6/__module.electra.encoder.layer.6.output/__module.electra.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.14)

ElectraLayer._actual_script_module
  graph(%self.130 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.130)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.130)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.130)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.131 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.131)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.131)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.141 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.141)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.83 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
    return (%input.83)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.143 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.143)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.143)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.143)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output # transformers/modeling_electra.py:365:0
    %input.85 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.85)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.137 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.137)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.137)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.137)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
    %input.81 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.81)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.132 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.132)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.132)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.132)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.132)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %x.44 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %query_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.44, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %x.46 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %key_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.46, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %x.48 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %value_layer.8 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.48, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.8, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.15 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.16 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.15, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
    %input.77 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %input.78 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.77, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.78)
    %context_layer.15 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.15, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.16 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.16, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.16, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self
    %input.79 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.16, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self # transformers/modeling_electra.py:269:0
    return (%input.79)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.78 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.8 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.78, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.8)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %output.45 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    %x.45 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.45, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.45)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.133)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.133)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %output.44 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    %x.43 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.44, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.43)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.135 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.135)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.135)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %output.46 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    %x.47 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.46, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.self/__module.electra.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.47)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.140 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.81 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.140)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.140)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.81, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.138 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.138)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.138)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %output.47 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    %input.80 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.47, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.80)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.139 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.15 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.attention/__module.electra.encoder.layer.7.attention.output/__module.electra.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.15)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.142 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.142)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.142)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %output.48 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    %input.82 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.48, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.intermediate/__module.electra.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.82)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.146 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.85 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.146)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.146)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.86 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.85, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.86)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.144)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.144)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %output.49 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    %input.84 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.49, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
    return (%input.84)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.16 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.7/__module.electra.encoder.layer.7.output/__module.electra.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.16)

ElectraLayer._actual_script_module
  graph(%self.147 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.147)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.147)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.147)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.148 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.148)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.148)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.158 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.158)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.93 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
    return (%input.93)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.160)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.160)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.160)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output # transformers/modeling_electra.py:365:0
    %input.95 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.95)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.154 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.154)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.154)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.154)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
    %input.91 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.91)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.149)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.149)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.149)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.149)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %x.50 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %query_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.50, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %x.52 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %key_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.52, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %x.54 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %value_layer.9 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.54, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.9, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.17 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.18 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.17, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
    %input.87 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %input.88 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.87, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.88)
    %context_layer.17 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.9), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.17, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.18 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.18, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.18, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self
    %input.89 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.18, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self # transformers/modeling_electra.py:269:0
    return (%input.89)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.153 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.88 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.9 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.88, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.9)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.151 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.151)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.151)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %output.51 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    %x.51 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.51, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.51)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.150)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.150)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %output.50 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    %x.49 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.50, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.49)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.152)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.152)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %output.52 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    %x.53 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.52, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.self/__module.electra.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.53)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.157 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.91 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.157)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.157)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.91, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.155)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.155)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %output.53 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    %input.90 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.53, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.90)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.17 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.attention/__module.electra.encoder.layer.8.attention.output/__module.electra.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.17)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.159 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.159)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.159)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %output.54 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    %input.92 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.54, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.intermediate/__module.electra.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.92)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.163 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.95 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.163)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.163)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.96 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.95, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.96)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.161)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.161)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %output.55 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    %input.94 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.55, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
    return (%input.94)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.162 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.18 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.8/__module.electra.encoder.layer.8.output/__module.electra.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.18)

ElectraLayer._actual_script_module
  graph(%self.164 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.164)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.164)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.164)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.165 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.165)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.165)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.175)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.103 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
    return (%input.103)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.177 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.177)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.177)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.177)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output # transformers/modeling_electra.py:365:0
    %input.105 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.105)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.171)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.171)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.171)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
    %input.101 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.101)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.166 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.166)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.166)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.166)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.166)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %x.56 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %query_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.56, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %x.58 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %key_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.58, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %x.60 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %value_layer.10 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.60, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.10, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.19 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.20 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.19, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
    %input.97 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %input.98 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.97, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.98)
    %context_layer.19 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.19, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.20 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.20, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.20, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self
    %input.99 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.20, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self # transformers/modeling_electra.py:269:0
    return (%input.99)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.170 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.98 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.10 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.98, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.10)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.168 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.168)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.168)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %output.57 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    %x.57 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.57, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.57)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.167)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %output.56 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    %x.55 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.56, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.55)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.169 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.169)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.169)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %output.58 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    %x.59 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.58, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.self/__module.electra.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.59)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.101 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.101, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.172)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.172)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %output.59 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    %input.100 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.59, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.100)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.173 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.19 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.attention/__module.electra.encoder.layer.9.attention.output/__module.electra.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.19)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.176 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %output.60 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    %input.102 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.60, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.intermediate/__module.electra.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.102)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.105 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.180)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.180)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.106 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.105, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.106)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %output.61 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    %input.104 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.61, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
    return (%input.104)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.179 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.20 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.9/__module.electra.encoder.layer.9.output/__module.electra.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.20)

ElectraLayer._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.181)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.181)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.181)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.182)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.182)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.192 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.192)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.113 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
    return (%input.113)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.194 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.194)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.194)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.194)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output # transformers/modeling_electra.py:365:0
    %input.115 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.115)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.188 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.188)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.188)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.188)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
    %input.111 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.111)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.183 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.183)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.183)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.183)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.183)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %x.62 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %query_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.62, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %x.64 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %key_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.64, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %x.66 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %value_layer.11 : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.66, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer.11, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.21 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores.22 : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.21, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
    %input.107 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %input.108 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.107, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.108)
    %context_layer.21 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer.11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.21, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %context_layer.22 : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer.22, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer.22, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self
    %input.109 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer.22, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self # transformers/modeling_electra.py:269:0
    return (%input.109)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.108 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs.11 : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.108, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs.11)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.185)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %output.63 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    %x.63 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.63, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.63)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.184 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.184)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.184)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %output.62 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    %x.61 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.62, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.61)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.186 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.186)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.186)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %output.64 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    %x.65 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.64, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.self/__module.electra.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.65)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.191 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.111 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.191)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.191)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.111, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %output.65 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    %input.110 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.65, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.110)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.190 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.21 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.attention/__module.electra.encoder.layer.10.attention.output/__module.electra.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.21)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.193 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.193)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.193)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %output.66 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    %input.112 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.66, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.intermediate/__module.electra.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.112)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.197 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.115 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.197)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.197)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    %input.116 : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.115, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input.116)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.195 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.195)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.195)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %output.67 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    %input.114 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.67, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
    return (%input.114)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.22 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.10/__module.electra.encoder.layer.10.output/__module.electra.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.22)

ElectraLayer._actual_script_module
  graph(%self.198 : __torch__.transformers.modeling_electra.ElectraLayer,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraOutput = prim::GetAttr[name="output"](%self.198)
    %4 : __torch__.transformers.modeling_electra.ElectraIntermediate = prim::GetAttr[name="intermediate"](%self.198)
    %5 : __torch__.transformers.modeling_electra.ElectraAttention = prim::GetAttr[name="attention"](%self.198)
    %30 : Tensor = prim::CallMethod[name="forward"](%5, %1, %attention_mask)
    %31 : Tensor = prim::CallMethod[name="forward"](%4, %30)
    %32 : Tensor = prim::CallMethod[name="forward"](%3, %31, %30)
    return (%32)

ElectraLayer.attention
ElectraAttention._actual_script_module
  graph(%self.199 : __torch__.transformers.modeling_electra.ElectraAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.transformers.modeling_electra.ElectraSelfOutput = prim::GetAttr[name="output"](%self.199)
    %4 : __torch__.transformers.modeling_electra.ElectraSelfAttention = prim::GetAttr[name="self"](%self.199)
    %7 : Tensor = prim::CallMethod[name="forward"](%4, %1, %attention_mask)
    %8 : Tensor = prim::CallMethod[name="forward"](%3, %7, %1)
    return (%8)

ElectraLayer.intermediate
ElectraIntermediate._actual_script_module
  graph(%self.209 : __torch__.transformers.modeling_electra.ElectraIntermediate,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.209)
    %5 : Tensor = prim::CallMethod[name="forward"](%2, %1)
    %input.123 : Float(17:13312, 13:1024, 1024:1) = aten::gelu(%5), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
    return (%input.123)

ElectraLayer.output
ElectraOutput._actual_script_module
  graph(%self.211 : __torch__.transformers.modeling_electra.ElectraOutput,
        %1 : Float(17:13312, 13:1024, 1024:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.211)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.211)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.211)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output # transformers/modeling_electra.py:365:0
    %input.125 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output # transformers/modeling_electra.py:365:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.125)
    return (%13)

ElectraAttention.output
ElectraSelfOutput._actual_script_module
  graph(%self.205 : __torch__.transformers.modeling_electra.ElectraSelfOutput,
        %1 : Float(17:3328, 13:256, 256:1),
        %2 : Float(17:3328, 13:256, 256:1)):
    %3 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="LayerNorm"](%self.205)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.205)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="dense"](%self.205)
    %11 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %12 : Tensor = prim::CallMethod[name="forward"](%4, %11)
    %8 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
    %input.121 : Float(17:3328, 13:256, 256:1) = aten::add(%12, %2, %8), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output # transformers/modeling_electra.py:286:0
    %13 : Tensor = prim::CallMethod[name="forward"](%3, %input.121)
    return (%13)

ElectraAttention.self
ElectraSelfAttention._actual_script_module
  graph(%self.200 : __torch__.transformers.modeling_electra.ElectraSelfAttention,
        %1 : Float(17:3328, 13:256, 256:1),
        %attention_mask : Float(17:13, 1:13, 1:13, 13:1)):
    %3 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.200)
    %4 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="value"](%self.200)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="key"](%self.200)
    %6 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="query"](%self.200)
    %111 : Tensor = prim::CallMethod[name="forward"](%6, %1)
    %112 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %113 : Tensor = prim::CallMethod[name="forward"](%4, %1)
    %10 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %11 : int = aten::size(%111, %10), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %12 : Long() = prim::NumToTensor(%11), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %13 : int = aten::Int(%12), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %14 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %15 : int = aten::size(%111, %14), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %16 : Long() = prim::NumToTensor(%15), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %17 : int = aten::Int(%16), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %21 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %22 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %23 : int[] = prim::ListConstruct(%13, %17, %21, %22), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %x.68 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%111, %23), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %25 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %26 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %27 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %28 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %29 : int[] = prim::ListConstruct(%25, %26, %27, %28), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %query_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.68, %29), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %31 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %32 : int = aten::size(%112, %31), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %33 : Long() = prim::NumToTensor(%32), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %34 : int = aten::Int(%33), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %35 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %36 : int = aten::size(%112, %35), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %37 : Long() = prim::NumToTensor(%36), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %38 : int = aten::Int(%37), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %42 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %43 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %44 : int[] = prim::ListConstruct(%34, %38, %42, %43), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %x.70 : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%112, %44), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %46 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %47 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %48 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %49 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %50 : int[] = prim::ListConstruct(%46, %47, %48, %49), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %key_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x.70, %50), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %52 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %53 : int = aten::size(%113, %52), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %54 : Long() = prim::NumToTensor(%53), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %55 : int = aten::Int(%54), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %56 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %57 : int = aten::size(%113, %56), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:217:0
    %58 : Long() = prim::NumToTensor(%57), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %59 : int = aten::Int(%58), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %63 : int = prim::Constant[value=4](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %64 : int = prim::Constant[value=64](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %65 : int[] = prim::ListConstruct(%55, %59, %63, %64), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %x : Float(17:3328, 13:256, 4:64, 64:1) = aten::view(%113, %65), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:218:0
    %67 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %68 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %69 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %70 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %71 : int[] = prim::ListConstruct(%67, %68, %69, %70), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %value_layer : Float(17:3328, 4:64, 13:256, 64:1) = aten::permute(%x, %71), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:219:0
    %73 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
    %74 : int = prim::Constant[value=-2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
    %75 : Float(17:3328, 4:64, 64:1, 13:256) = aten::transpose(%key_layer, %73, %74), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
    %attention_scores.23 : Float(17:676, 4:169, 13:13, 13:1) = aten::matmul(%query_layer, %75), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:248:0
    %77 : Double() = prim::Constant[value={8}](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
    %attention_scores : Float(17:676, 4:169, 13:13, 13:1) = aten::div(%attention_scores.23, %77), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:249:0
    %79 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
    %input.117 : Float(17:676, 4:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %79), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:252:0
    %81 : int = prim::Constant[value=-1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %82 : None = prim::Constant(), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %input.118 : Float(17:676, 4:169, 13:13, 13:1) = aten::softmax(%input.117, %81, %82), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
    %114 : Tensor = prim::CallMethod[name="forward"](%3, %input.118)
    %context_layer.23 : Float(17:3328, 4:832, 13:64, 64:1) = aten::matmul(%114, %value_layer), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:265:0
    %86 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %87 : int = prim::Constant[value=2](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %88 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %89 : int = prim::Constant[value=3](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %90 : int[] = prim::ListConstruct(%86, %87, %88, %89), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %91 : Float(17:3328, 13:64, 4:832, 64:1) = aten::permute(%context_layer.23, %90), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %92 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %context_layer : Float(17:3328, 13:256, 4:64, 64:1) = aten::contiguous(%91, %92), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:267:0
    %94 : int = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
    %95 : int = aten::size(%context_layer, %94), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
    %96 : Long() = prim::NumToTensor(%95), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %97 : int = aten::Int(%96), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %98 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
    %99 : int = aten::size(%context_layer, %98), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:268:0
    %100 : Long() = prim::NumToTensor(%99), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %101 : int = aten::Int(%100), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %108 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
    %109 : int[] = prim::ListConstruct(%97, %101, %108), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self
    %input.119 : Float(17:3328, 13:256, 256:1) = aten::view(%context_layer, %109), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self # transformers/modeling_electra.py:269:0
    return (%input.119)

ElectraSelfAttention.dropout
Dropout._actual_script_module
  graph(%self.204 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.118 : Float(17:676, 4:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    %attention_probs : Float(17:676, 4:169, 13:13, 13:1) = aten::dropout(%input.118, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
    return (%attention_probs)

ElectraSelfAttention.key
Linear._actual_script_module
  graph(%self.202 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.202)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.202)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %output.69 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    %x.69 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.69, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
    return (%x.69)

ElectraSelfAttention.query
Linear._actual_script_module
  graph(%self.201 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.201)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.201)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %output.68 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    %x.67 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.68, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
    return (%x.67)

ElectraSelfAttention.value
Linear._actual_script_module
  graph(%self.203 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.203)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.203)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %output.70 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    %x.71 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.70, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.self/__module.electra.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
    return (%x.71)

ElectraSelfOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.208 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.121 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.208)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.208)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    %input_tensor : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.121, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%input_tensor)

ElectraSelfOutput.dense
Linear._actual_script_module
  graph(%self.206 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.206)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.206)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %output.71 : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    %input.120 : Float(17:3328, 13:256, 256:1) = aten::add_(%output.71, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
    return (%input.120)

ElectraSelfOutput.dropout
Dropout._actual_script_module
  graph(%self.207 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    %hidden_states.23 : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.attention/__module.electra.encoder.layer.11.attention.output/__module.electra.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states.23)

ElectraIntermediate.dense
Linear._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.210)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.210)
    %4 : Float(256:1, 1024:256) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %output.72 : Float(17:13312, 13:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    %input.122 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%output.72, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.intermediate/__module.electra.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
    return (%input.122)

ElectraOutput.LayerNorm
LayerNorm._actual_script_module
  graph(%self.214 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.125 : Float(17:3328, 13:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.214)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.214)
    %4 : int = prim::Constant[value=256](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    %features : Float(17:3328, 13:256, 256:1) = aten::layer_norm(%input.125, %5, %3, %2, %6, %7), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
    return (%features)

ElectraOutput.dense
Linear._actual_script_module
  graph(%self.212 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:13312, 13:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.212)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.212)
    %4 : Float(1024:1, 256:1024) = aten::t(%3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %output : Float(17:3328, 13:256, 256:1) = aten::matmul(%1, %4), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    %input.124 : Float(17:3328, 13:256, 256:1) = aten::add_(%output, %2, %6), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
    return (%input.124)

ElectraOutput.dropout
Dropout._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.dropout.Dropout,
        %1 : Float(17:3328, 13:256, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    %hidden_states : Float(17:3328, 13:256, 256:1) = aten::dropout(%1, %2, %3), scope: __module.electra/__module.electra.encoder/__module.electra.encoder.layer.11/__module.electra.encoder.layer.11.output/__module.electra.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
    return (%hidden_states)

ElectraClassificationHead.dense
Linear._actual_script_module
  graph(%self.217 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:3328, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.217)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.217)
    %4 : Float(256:1, 256:256) = aten::t(%3), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
    %6 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
    %input.128 : Float(17:256, 256:1) = aten::addmm(%2, %1, %4, %5, %6), scope: __module.classifier/__module.classifier.dense # torch/nn/functional.py:1674:0
    return (%input.128)

ElectraClassificationHead.dropout
Dropout._actual_script_module
  graph(%self.216 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.126 : Float(17:3328, 256:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
    %input.127 : Float(17:3328, 256:1) = aten::dropout(%input.126, %2, %3), scope: __module.classifier/__module.classifier.dropout # torch/nn/functional.py:973:0
    return (%input.127)

ElectraClassificationHead.out_proj
Linear._actual_script_module
  graph(%self : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(17:256, 256:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self)
    %3 : Tensor = prim::GetAttr[name="weight"](%self)
    %4 : Float(256:1, 2:256) = aten::t(%3), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
    %5 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
    %6 : int = prim::Constant[value=1](), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
    %7 : Float(17:2, 2:1) = aten::addmm(%2, %1, %4, %5, %6), scope: __module.classifier/__module.classifier.out_proj # torch/nn/functional.py:1674:0
    return (%7)

