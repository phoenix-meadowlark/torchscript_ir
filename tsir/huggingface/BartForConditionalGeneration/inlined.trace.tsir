graph(%self.1 : __torch__.transformers.modeling_bart.BartForConditionalGeneration,
      %input_ids : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : Tensor = prim::GetAttr[name="final_logits_bias"](%self.1)
  %4 : __torch__.transformers.modeling_bart.BartModel = prim::GetAttr[name="model"](%self.1)
  %5 : __torch__.transformers.modeling_bart.BartModel = prim::GetAttr[name="model"](%self.1)
  %6 : __torch__.torch.nn.modules.sparse.___torch_mangle_394.Embedding = prim::GetAttr[name="shared"](%5)
  %7 : Tensor = prim::GetAttr[name="weight"](%6)
  %16 : Double() = prim::Constant[value={1}](), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
  %17 : Long() = prim::Constant[value={2}](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %18 : int = prim::Constant[value=4](), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %19 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %20 : float = prim::Constant[value=1.0000000000000001e-05](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %21 : int = prim::Constant[value=1024](), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %22 : float = prim::Constant[value=0.10000000000000001](), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
  %23 : int = prim::Constant[value=2](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %24 : Double() = prim::Constant[value={0.125}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %25 : Long() = prim::Constant[value={16}](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %26 : int = prim::Constant[value=64](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %27 : int = prim::Constant[value=16](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %28 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %29 : float = prim::Constant[value=-inf](), scope: __module.model # transformers/modeling_bart.py:838:0
  %30 : Device = prim::Constant[value="cpu"](), scope: __module.model # transformers/modeling_bart.py:157:0
  %31 : int = prim::Constant[value=6](), scope: __module.model # transformers/modeling_bart.py:157:0
  %32 : int = prim::Constant[value=12](), scope: __module.model # transformers/modeling_bart.py:210:0
  %33 : int = prim::Constant[value=17](), scope: __module.model # transformers/modeling_bart.py:209:0
  %34 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # transformers/modeling_bart.py:209:0
  %35 : int = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:209:0
  %36 : int = prim::Constant[value=-1](), scope: __module.model # transformers/modeling_bart.py:208:0
  %37 : Long() = prim::Constant[value={1}](), scope: __module.model # transformers/modeling_bart.py:208:0
  %38 : bool = prim::Constant[value=0](), scope: __module.model # transformers/modeling_bart.py:208:0
  %39 : int = prim::Constant[value=1](), scope: __module.model # transformers/modeling_bart.py:208:0
  %40 : None = prim::Constant(), scope: __module.model
  %41 : __torch__.transformers.modeling_bart.BartDecoder = prim::GetAttr[name="decoder"](%4)
  %42 : __torch__.torch.nn.modules.sparse.___torch_mangle_394.Embedding = prim::GetAttr[name="shared"](%4)
  %43 : Tensor = prim::GetAttr[name="weight"](%42)
  %44 : __torch__.transformers.modeling_bart.BartEncoder = prim::GetAttr[name="encoder"](%4)
  %prev_output_tokens : Long(17:13, 13:1) = aten::clone(%input_ids, %40), scope: __module.model # transformers/modeling_bart.py:207:0
  %46 : Bool(17:13, 13:1) = aten::ne(%input_ids, %39), scope: __module.model # transformers/modeling_bart.py:208:0
  %47 : int[] = prim::ListConstruct(%39), scope: __module.model
  %48 : Long(17:1) = aten::sum(%46, %47, %38, %40), scope: __module.model # transformers/modeling_bart.py:208:0
  %49 : Long(17:1) = aten::sub(%48, %37, %39), scope: __module.model # transformers/modeling_bart.py:208:0
  %index_of_eos : Long(17:1, 1:1) = aten::unsqueeze(%49, %36), scope: __module.model # transformers/modeling_bart.py:208:0
  %51 : Long(17:1, 1:1) = aten::gather(%input_ids, %39, %index_of_eos, %38), scope: __module.model # transformers/modeling_bart.py:209:0
  %52 : Long(17:1) = aten::squeeze(%51), scope: __module.model # transformers/modeling_bart.py:209:0
  %53 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %35, %35, %34, %39), scope: __module.model # transformers/modeling_bart.py:209:0
  %54 : Long(17:13) = aten::select(%53, %39, %35), scope: __module.model # transformers/modeling_bart.py:209:0
  %55 : int[] = prim::ListConstruct(%33), scope: __module.model
  %56 : Long(17:1) = aten::view(%52, %55), scope: __module.model # transformers/modeling_bart.py:209:0
  %57 : Long(17:13) = aten::copy_(%54, %56, %38), scope: __module.model # transformers/modeling_bart.py:209:0
  %58 : Long(17:13, 13:1) = aten::slice(%input_ids, %35, %35, %34, %39), scope: __module.model # transformers/modeling_bart.py:210:0
  %59 : Long(17:13, 12:1) = aten::slice(%58, %39, %35, %36, %39), scope: __module.model # transformers/modeling_bart.py:210:0
  %60 : Long(17:13, 13:1) = aten::slice(%prev_output_tokens, %35, %35, %34, %39), scope: __module.model # transformers/modeling_bart.py:210:0
  %61 : Long(17:13, 12:1) = aten::slice(%60, %39, %39, %34, %39), scope: __module.model # transformers/modeling_bart.py:210:0
  %62 : int[] = prim::ListConstruct(%33, %32), scope: __module.model
  %63 : Long(17:13, 12:1) = aten::view(%59, %62), scope: __module.model # transformers/modeling_bart.py:210:0
  %64 : Long(17:13, 12:1) = aten::copy_(%61, %63, %38), scope: __module.model # transformers/modeling_bart.py:210:0
  %65 : int = aten::size(%prev_output_tokens, %39), scope: __module.model # transformers/modeling_bart.py:149:0
  %66 : int[] = prim::ListConstruct(%65, %65), scope: __module.model
  %t.1 : Float(13:13, 13:1) = aten::zeros(%66, %31, %35, %30, %38), scope: __module.model # transformers/modeling_bart.py:157:0
  %t.2 : Float(13:13, 13:1) = aten::to(%t.1, %31, %38, %38, %40), scope: __module.model # transformers/modeling_bart.py:838:0
  %t : Float(13:13, 13:1) = aten::fill_(%t.2, %29), scope: __module.model # transformers/modeling_bart.py:838:0
  %70 : int = aten::size(%t, %36), scope: __module.model # transformers/modeling_bart.py:158:0
  %mask : Long(13:1) = aten::arange(%70, %40, %35, %30, %38), scope: __module.model # transformers/modeling_bart.py:158:0
  %72 : Long(13:1) = aten::add(%mask, %37, %39), scope: __module.model # transformers/modeling_bart.py:159:0
  %73 : int = aten::size(%t, %36), scope: __module.model # transformers/modeling_bart.py:159:0
  %74 : int[] = prim::ListConstruct(%73, %39), scope: __module.model
  %75 : Long(13:1, 1:1) = aten::view(%72, %74), scope: __module.model # transformers/modeling_bart.py:159:0
  %76 : Bool(13:13, 13:1) = aten::lt(%mask, %75), scope: __module.model # torch/tensor.py:22:0
  %tmp : Float(13:13, 13:1) = aten::masked_fill_(%t, %76, %35), scope: __module.model # transformers/modeling_bart.py:159:0
  %attn_mask : Float(13:13, 13:1) = aten::to(%tmp, %30, %31, %38, %38, %40), scope: __module.model # transformers/modeling_bart.py:160:0
  %79 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %80 : __torch__.transformers.modeling_bart.___torch_mangle_513.EncoderLayer = prim::GetAttr[name="11"](%79)
  %81 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %82 : __torch__.transformers.modeling_bart.___torch_mangle_503.EncoderLayer = prim::GetAttr[name="10"](%81)
  %83 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %84 : __torch__.transformers.modeling_bart.___torch_mangle_493.EncoderLayer = prim::GetAttr[name="9"](%83)
  %85 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %86 : __torch__.transformers.modeling_bart.___torch_mangle_483.EncoderLayer = prim::GetAttr[name="8"](%85)
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %88 : __torch__.transformers.modeling_bart.___torch_mangle_473.EncoderLayer = prim::GetAttr[name="7"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %90 : __torch__.transformers.modeling_bart.___torch_mangle_463.EncoderLayer = prim::GetAttr[name="6"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %92 : __torch__.transformers.modeling_bart.___torch_mangle_453.EncoderLayer = prim::GetAttr[name="5"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %94 : __torch__.transformers.modeling_bart.___torch_mangle_443.EncoderLayer = prim::GetAttr[name="4"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %96 : __torch__.transformers.modeling_bart.___torch_mangle_433.EncoderLayer = prim::GetAttr[name="3"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %98 : __torch__.transformers.modeling_bart.___torch_mangle_423.EncoderLayer = prim::GetAttr[name="2"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %100 : __torch__.transformers.modeling_bart.___torch_mangle_413.EncoderLayer = prim::GetAttr[name="1"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_514.ModuleList = prim::GetAttr[name="layers"](%44)
  %102 : __torch__.transformers.modeling_bart.EncoderLayer = prim::GetAttr[name="0"](%101)
  %103 : __torch__.torch.nn.modules.normalization.___torch_mangle_515.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%44)
  %104 : __torch__.transformers.modeling_bart.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%44)
  %key_padding_mask.1 : Bool(17:13, 13:1) = aten::eq(%attention_mask, %35), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:136:0
  %106 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%43, %input_ids, %39, %38, %38), scope: __module.model/__module.model.encoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %inputs_embeds : Float(17:13312, 13:1024, 1024:1) = aten::mul(%106, %16), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:334:0
  %108 : Tensor = prim::GetAttr[name="weight"](%104)
  %109 : int = aten::size(%input_ids, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.1 : Long(13:1) = aten::arange(%109, %18, %35, %30, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.1 : Long(13:1) = aten::add(%positions.1, %17, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # transformers/modeling_bart.py:822:0
  %embed_pos : Float(13:1024, 1024:1) = aten::embedding(%108, %input.1, %39, %38, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.embed_positions # torch/nn/functional.py:1814:0
  %input.2 : Float(17:13312, 13:1024, 1024:1) = aten::add(%inputs_embeds, %embed_pos, %39), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:336:0
  %114 : Tensor = prim::GetAttr[name="bias"](%103)
  %115 : Tensor = prim::GetAttr[name="weight"](%103)
  %116 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding
  %input.3 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.2, %116, %115, %114, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.1 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.3, %22, %38), scope: __module.model/__module.model.encoder # torch/nn/functional.py:973:0
  %query.1 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.1, %35, %39), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:341:0
  %120 : __torch__.torch.nn.modules.normalization.___torch_mangle_403.LayerNorm = prim::GetAttr[name="final_layer_norm"](%102)
  %121 : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear = prim::GetAttr[name="fc2"](%102)
  %122 : __torch__.torch.nn.modules.linear.___torch_mangle_401.Linear = prim::GetAttr[name="fc1"](%102)
  %123 : __torch__.torch.nn.modules.normalization.___torch_mangle_400.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%102)
  %124 : __torch__.transformers.modeling_bart.Attention = prim::GetAttr[name="self_attn"](%102)
  %125 : __torch__.torch.nn.modules.linear.___torch_mangle_399.Linear = prim::GetAttr[name="out_proj"](%124)
  %126 : __torch__.torch.nn.modules.linear.___torch_mangle_397.Linear = prim::GetAttr[name="v_proj"](%124)
  %127 : __torch__.torch.nn.modules.linear.___torch_mangle_396.Linear = prim::GetAttr[name="k_proj"](%124)
  %128 : __torch__.torch.nn.modules.linear.___torch_mangle_398.Linear = prim::GetAttr[name="q_proj"](%124)
  %129 : int = aten::size(%query.1, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %130 : int = aten::size(%query.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.1 : Long() = prim::NumToTensor(%130), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %132 : int = aten::size(%query.1, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %133 : Tensor = prim::GetAttr[name="bias"](%128)
  %134 : Tensor = prim::GetAttr[name="weight"](%128)
  %135 : Float(1024:1, 1024:1024) = aten::t(%134), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.1 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %135), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.1, %133, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%137, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %139 : Tensor = prim::GetAttr[name="bias"](%127)
  %140 : Tensor = prim::GetAttr[name="weight"](%127)
  %141 : Float(1024:1, 1024:1024) = aten::t(%140), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %141), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %139, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %144 : Tensor = prim::GetAttr[name="bias"](%126)
  %145 : Tensor = prim::GetAttr[name="weight"](%126)
  %146 : Float(1024:1, 1024:1024) = aten::t(%145), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.1, %146), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.3, %144, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.1, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %150 : Long() = aten::mul(%bsz.1, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %151 : int = aten::Int(%150), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %152 : int[] = prim::ListConstruct(%129, %151, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %153 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.2, %152), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%153, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.3, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %156 : Long() = aten::mul(%bsz.1, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %157 : int = aten::Int(%156), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %158 : int[] = prim::ListConstruct(%36, %157, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %159 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.4, %158), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%159, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.5, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %162 : Long() = aten::mul(%bsz.1, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %163 : int = aten::Int(%162), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %164 : int[] = prim::ListConstruct(%36, %163, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %165 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.6, %164), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.1 : Float(272:64, 13:17408, 64:1) = aten::transpose(%165, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %167 : int = aten::size(%k.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %168 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.1, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.1 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.1, %168), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %170 : int[] = prim::ListConstruct(%130, %27, %129, %167), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %attn_weights.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.1, %170), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:718:0
  %172 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.1 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%172, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.2, %reshaped.1, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:720:0
  %175 : Long() = aten::mul(%bsz.1, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %176 : int = aten::Int(%175), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %177 : int[] = prim::ListConstruct(%176, %129, %167), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %input.4 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.3, %177), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:721:0
  %input.5 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.4, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.4 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.5, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.1 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.4, %v.1), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %182 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.1, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %183 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%182, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %184 : int[] = prim::ListConstruct(%129, %130, %132), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::view(%183, %184), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %186 : Tensor = prim::GetAttr[name="bias"](%125)
  %187 : Tensor = prim::GetAttr[name="weight"](%125)
  %188 : Float(1024:1, 1024:1024) = aten::t(%187), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.4 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.6, %188), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.7 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.4, %186, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn/__module.model.encoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.7, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.8 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.1, %x.2, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:258:0
  %193 : Tensor = prim::GetAttr[name="bias"](%123)
  %194 : Tensor = prim::GetAttr[name="weight"](%123)
  %195 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm
  %input.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.8, %195, %194, %193, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %197 : Tensor = prim::GetAttr[name="bias"](%122)
  %198 : Tensor = prim::GetAttr[name="weight"](%122)
  %199 : Float(1024:1, 4096:1024) = aten::t(%198), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.9, %199), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.5, %197, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.11 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:1369:0
  %input.12 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.11, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %204 : Tensor = prim::GetAttr[name="bias"](%121)
  %205 : Tensor = prim::GetAttr[name="weight"](%121)
  %206 : Float(4096:1, 1024:4096) = aten::t(%205), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.12, %206), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.13 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.6, %204, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.13, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # torch/nn/functional.py:973:0
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.9, %x.3, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0 # transformers/modeling_bart.py:269:0
  %211 : Tensor = prim::GetAttr[name="bias"](%120)
  %212 : Tensor = prim::GetAttr[name="weight"](%120)
  %213 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm
  %query.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.14, %213, %212, %211, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.0/__module.model.encoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %215 : __torch__.torch.nn.modules.normalization.___torch_mangle_412.LayerNorm = prim::GetAttr[name="final_layer_norm"](%100)
  %216 : __torch__.torch.nn.modules.linear.___torch_mangle_411.Linear = prim::GetAttr[name="fc2"](%100)
  %217 : __torch__.torch.nn.modules.linear.___torch_mangle_410.Linear = prim::GetAttr[name="fc1"](%100)
  %218 : __torch__.torch.nn.modules.normalization.___torch_mangle_409.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%100)
  %219 : __torch__.transformers.modeling_bart.___torch_mangle_408.Attention = prim::GetAttr[name="self_attn"](%100)
  %220 : __torch__.torch.nn.modules.linear.___torch_mangle_407.Linear = prim::GetAttr[name="out_proj"](%219)
  %221 : __torch__.torch.nn.modules.linear.___torch_mangle_405.Linear = prim::GetAttr[name="v_proj"](%219)
  %222 : __torch__.torch.nn.modules.linear.___torch_mangle_404.Linear = prim::GetAttr[name="k_proj"](%219)
  %223 : __torch__.torch.nn.modules.linear.___torch_mangle_406.Linear = prim::GetAttr[name="q_proj"](%219)
  %224 : int = aten::size(%query.2, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %225 : int = aten::size(%query.2, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.2 : Long() = prim::NumToTensor(%225), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %227 : int = aten::size(%query.2, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %228 : Tensor = prim::GetAttr[name="bias"](%223)
  %229 : Tensor = prim::GetAttr[name="weight"](%223)
  %230 : Float(1024:1, 1024:1024) = aten::t(%229), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.7 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %230), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %232 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.7, %228, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%232, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %234 : Tensor = prim::GetAttr[name="bias"](%222)
  %235 : Tensor = prim::GetAttr[name="weight"](%222)
  %236 : Float(1024:1, 1024:1024) = aten::t(%235), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %236), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %234, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %239 : Tensor = prim::GetAttr[name="bias"](%221)
  %240 : Tensor = prim::GetAttr[name="weight"](%221)
  %241 : Float(1024:1, 1024:1024) = aten::t(%240), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.2, %241), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.9, %239, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.7, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %245 : Long() = aten::mul(%bsz.2, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %246 : int = aten::Int(%245), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %247 : int[] = prim::ListConstruct(%224, %246, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %248 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.8, %247), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%248, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.9, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %251 : Long() = aten::mul(%bsz.2, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %252 : int = aten::Int(%251), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %253 : int[] = prim::ListConstruct(%36, %252, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %254 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.10, %253), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%254, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.11, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %257 : Long() = aten::mul(%bsz.2, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %258 : int = aten::Int(%257), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %259 : int[] = prim::ListConstruct(%36, %258, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %260 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.12, %259), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.2 : Float(272:64, 13:17408, 64:1) = aten::transpose(%260, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %262 : int = aten::size(%k.2, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %263 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.2, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.5 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.2, %263), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %265 : int[] = prim::ListConstruct(%225, %27, %224, %262), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %attn_weights.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.5, %265), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:718:0
  %267 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.2 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%267, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.6, %reshaped.2, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:720:0
  %270 : Long() = aten::mul(%bsz.2, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %271 : int = aten::Int(%270), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %272 : int[] = prim::ListConstruct(%271, %224, %262), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %input.15 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.7, %272), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:721:0
  %input.16 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.15, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.8 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.16, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.2 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.8, %v.2), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %277 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.2, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %278 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%277, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %279 : int[] = prim::ListConstruct(%224, %225, %227), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn
  %input.17 : Float(13:17408, 17:1024, 1024:1) = aten::view(%278, %279), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %281 : Tensor = prim::GetAttr[name="bias"](%220)
  %282 : Tensor = prim::GetAttr[name="weight"](%220)
  %283 : Float(1024:1, 1024:1024) = aten::t(%282), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.10 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.17, %283), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.18 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.10, %281, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn/__module.model.encoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.18, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.2, %x.4, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:258:0
  %288 : Tensor = prim::GetAttr[name="bias"](%218)
  %289 : Tensor = prim::GetAttr[name="weight"](%218)
  %290 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.19, %290, %289, %288, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %292 : Tensor = prim::GetAttr[name="bias"](%217)
  %293 : Tensor = prim::GetAttr[name="weight"](%217)
  %294 : Float(1024:1, 4096:1024) = aten::t(%293), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.20, %294), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.21 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.11, %292, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.22 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:1369:0
  %input.23 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.22, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %299 : Tensor = prim::GetAttr[name="bias"](%216)
  %300 : Tensor = prim::GetAttr[name="weight"](%216)
  %301 : Float(4096:1, 1024:4096) = aten::t(%300), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.23, %301), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.12, %299, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.24, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # torch/nn/functional.py:973:0
  %input.25 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.20, %x.5, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1 # transformers/modeling_bart.py:269:0
  %306 : Tensor = prim::GetAttr[name="bias"](%215)
  %307 : Tensor = prim::GetAttr[name="weight"](%215)
  %308 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm
  %query.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.25, %308, %307, %306, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.1/__module.model.encoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %310 : __torch__.torch.nn.modules.normalization.___torch_mangle_422.LayerNorm = prim::GetAttr[name="final_layer_norm"](%98)
  %311 : __torch__.torch.nn.modules.linear.___torch_mangle_421.Linear = prim::GetAttr[name="fc2"](%98)
  %312 : __torch__.torch.nn.modules.linear.___torch_mangle_420.Linear = prim::GetAttr[name="fc1"](%98)
  %313 : __torch__.torch.nn.modules.normalization.___torch_mangle_419.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%98)
  %314 : __torch__.transformers.modeling_bart.___torch_mangle_418.Attention = prim::GetAttr[name="self_attn"](%98)
  %315 : __torch__.torch.nn.modules.linear.___torch_mangle_417.Linear = prim::GetAttr[name="out_proj"](%314)
  %316 : __torch__.torch.nn.modules.linear.___torch_mangle_415.Linear = prim::GetAttr[name="v_proj"](%314)
  %317 : __torch__.torch.nn.modules.linear.___torch_mangle_414.Linear = prim::GetAttr[name="k_proj"](%314)
  %318 : __torch__.torch.nn.modules.linear.___torch_mangle_416.Linear = prim::GetAttr[name="q_proj"](%314)
  %319 : int = aten::size(%query.3, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %320 : int = aten::size(%query.3, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.3 : Long() = prim::NumToTensor(%320), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %322 : int = aten::size(%query.3, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %323 : Tensor = prim::GetAttr[name="bias"](%318)
  %324 : Tensor = prim::GetAttr[name="weight"](%318)
  %325 : Float(1024:1, 1024:1024) = aten::t(%324), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.13 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %325), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %327 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.13, %323, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%327, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %329 : Tensor = prim::GetAttr[name="bias"](%317)
  %330 : Tensor = prim::GetAttr[name="weight"](%317)
  %331 : Float(1024:1, 1024:1024) = aten::t(%330), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %331), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %329, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %334 : Tensor = prim::GetAttr[name="bias"](%316)
  %335 : Tensor = prim::GetAttr[name="weight"](%316)
  %336 : Float(1024:1, 1024:1024) = aten::t(%335), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.3, %336), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.15, %334, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.13, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %340 : Long() = aten::mul(%bsz.3, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %341 : int = aten::Int(%340), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %342 : int[] = prim::ListConstruct(%319, %341, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %343 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.14, %342), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%343, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.15, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %346 : Long() = aten::mul(%bsz.3, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %347 : int = aten::Int(%346), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %348 : int[] = prim::ListConstruct(%36, %347, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %349 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.16, %348), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%349, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.17, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %352 : Long() = aten::mul(%bsz.3, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %353 : int = aten::Int(%352), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %354 : int[] = prim::ListConstruct(%36, %353, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %355 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.18, %354), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.3 : Float(272:64, 13:17408, 64:1) = aten::transpose(%355, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %357 : int = aten::size(%k.3, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %358 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.3, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.9 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.3, %358), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %360 : int[] = prim::ListConstruct(%320, %27, %319, %357), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %attn_weights.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.9, %360), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:718:0
  %362 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.3 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%362, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.10, %reshaped.3, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:720:0
  %365 : Long() = aten::mul(%bsz.3, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %366 : int = aten::Int(%365), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %367 : int[] = prim::ListConstruct(%366, %319, %357), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %input.26 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.11, %367), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:721:0
  %input.27 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.26, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.12 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.27, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.3 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.12, %v.3), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %372 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.3, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %373 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%372, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %374 : int[] = prim::ListConstruct(%319, %320, %322), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::view(%373, %374), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %376 : Tensor = prim::GetAttr[name="bias"](%315)
  %377 : Tensor = prim::GetAttr[name="weight"](%315)
  %378 : Float(1024:1, 1024:1024) = aten::t(%377), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.16 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.28, %378), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.16, %376, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn/__module.model.encoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.29, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.30 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.3, %x.6, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:258:0
  %383 : Tensor = prim::GetAttr[name="bias"](%313)
  %384 : Tensor = prim::GetAttr[name="weight"](%313)
  %385 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm
  %input.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.30, %385, %384, %383, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %387 : Tensor = prim::GetAttr[name="bias"](%312)
  %388 : Tensor = prim::GetAttr[name="weight"](%312)
  %389 : Float(1024:1, 4096:1024) = aten::t(%388), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.31, %389), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.32 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.17, %387, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.33 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.32), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:1369:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.33, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %394 : Tensor = prim::GetAttr[name="bias"](%311)
  %395 : Tensor = prim::GetAttr[name="weight"](%311)
  %396 : Float(4096:1, 1024:4096) = aten::t(%395), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.34, %396), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.18, %394, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.35, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # torch/nn/functional.py:973:0
  %input.36 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.31, %x.7, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2 # transformers/modeling_bart.py:269:0
  %401 : Tensor = prim::GetAttr[name="bias"](%310)
  %402 : Tensor = prim::GetAttr[name="weight"](%310)
  %403 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm
  %query.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.36, %403, %402, %401, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.2/__module.model.encoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %405 : __torch__.torch.nn.modules.normalization.___torch_mangle_432.LayerNorm = prim::GetAttr[name="final_layer_norm"](%96)
  %406 : __torch__.torch.nn.modules.linear.___torch_mangle_431.Linear = prim::GetAttr[name="fc2"](%96)
  %407 : __torch__.torch.nn.modules.linear.___torch_mangle_430.Linear = prim::GetAttr[name="fc1"](%96)
  %408 : __torch__.torch.nn.modules.normalization.___torch_mangle_429.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%96)
  %409 : __torch__.transformers.modeling_bart.___torch_mangle_428.Attention = prim::GetAttr[name="self_attn"](%96)
  %410 : __torch__.torch.nn.modules.linear.___torch_mangle_427.Linear = prim::GetAttr[name="out_proj"](%409)
  %411 : __torch__.torch.nn.modules.linear.___torch_mangle_425.Linear = prim::GetAttr[name="v_proj"](%409)
  %412 : __torch__.torch.nn.modules.linear.___torch_mangle_424.Linear = prim::GetAttr[name="k_proj"](%409)
  %413 : __torch__.torch.nn.modules.linear.___torch_mangle_426.Linear = prim::GetAttr[name="q_proj"](%409)
  %414 : int = aten::size(%query.4, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %415 : int = aten::size(%query.4, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.4 : Long() = prim::NumToTensor(%415), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %417 : int = aten::size(%query.4, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %418 : Tensor = prim::GetAttr[name="bias"](%413)
  %419 : Tensor = prim::GetAttr[name="weight"](%413)
  %420 : Float(1024:1, 1024:1024) = aten::t(%419), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.19 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %420), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %422 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.19, %418, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%422, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %424 : Tensor = prim::GetAttr[name="bias"](%412)
  %425 : Tensor = prim::GetAttr[name="weight"](%412)
  %426 : Float(1024:1, 1024:1024) = aten::t(%425), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %426), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %424, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %429 : Tensor = prim::GetAttr[name="bias"](%411)
  %430 : Tensor = prim::GetAttr[name="weight"](%411)
  %431 : Float(1024:1, 1024:1024) = aten::t(%430), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.4, %431), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.21, %429, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.19, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %435 : Long() = aten::mul(%bsz.4, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %436 : int = aten::Int(%435), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %437 : int[] = prim::ListConstruct(%414, %436, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %438 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.20, %437), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%438, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.21, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %441 : Long() = aten::mul(%bsz.4, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %442 : int = aten::Int(%441), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %443 : int[] = prim::ListConstruct(%36, %442, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %444 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.22, %443), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%444, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.24 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.23, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %447 : Long() = aten::mul(%bsz.4, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %448 : int = aten::Int(%447), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %449 : int[] = prim::ListConstruct(%36, %448, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %450 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.24, %449), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.4 : Float(272:64, 13:17408, 64:1) = aten::transpose(%450, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %452 : int = aten::size(%k.4, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %453 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.4, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.13 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.4, %453), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %455 : int[] = prim::ListConstruct(%415, %27, %414, %452), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %attn_weights.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.13, %455), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:718:0
  %457 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.4 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%457, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.14, %reshaped.4, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:720:0
  %460 : Long() = aten::mul(%bsz.4, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %461 : int = aten::Int(%460), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %462 : int[] = prim::ListConstruct(%461, %414, %452), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %input.37 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.15, %462), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:721:0
  %input.38 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.37, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.16 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.38, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.4 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.16, %v.4), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %467 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.4, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %468 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%467, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %469 : int[] = prim::ListConstruct(%414, %415, %417), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn
  %input.39 : Float(13:17408, 17:1024, 1024:1) = aten::view(%468, %469), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %471 : Tensor = prim::GetAttr[name="bias"](%410)
  %472 : Tensor = prim::GetAttr[name="weight"](%410)
  %473 : Float(1024:1, 1024:1024) = aten::t(%472), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.22 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.39, %473), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.40 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.22, %471, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn/__module.model.encoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.40, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.4, %x.8, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:258:0
  %478 : Tensor = prim::GetAttr[name="bias"](%408)
  %479 : Tensor = prim::GetAttr[name="weight"](%408)
  %480 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.41, %480, %479, %478, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %482 : Tensor = prim::GetAttr[name="bias"](%407)
  %483 : Tensor = prim::GetAttr[name="weight"](%407)
  %484 : Float(1024:1, 4096:1024) = aten::t(%483), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.42, %484), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.23, %482, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %489 : Tensor = prim::GetAttr[name="bias"](%406)
  %490 : Tensor = prim::GetAttr[name="weight"](%406)
  %491 : Float(4096:1, 1024:4096) = aten::t(%490), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %491), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.24, %489, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.42, %x.9, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3 # transformers/modeling_bart.py:269:0
  %496 : Tensor = prim::GetAttr[name="bias"](%405)
  %497 : Tensor = prim::GetAttr[name="weight"](%405)
  %498 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm
  %query.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %498, %497, %496, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.3/__module.model.encoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %500 : __torch__.torch.nn.modules.normalization.___torch_mangle_442.LayerNorm = prim::GetAttr[name="final_layer_norm"](%94)
  %501 : __torch__.torch.nn.modules.linear.___torch_mangle_441.Linear = prim::GetAttr[name="fc2"](%94)
  %502 : __torch__.torch.nn.modules.linear.___torch_mangle_440.Linear = prim::GetAttr[name="fc1"](%94)
  %503 : __torch__.torch.nn.modules.normalization.___torch_mangle_439.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%94)
  %504 : __torch__.transformers.modeling_bart.___torch_mangle_438.Attention = prim::GetAttr[name="self_attn"](%94)
  %505 : __torch__.torch.nn.modules.linear.___torch_mangle_437.Linear = prim::GetAttr[name="out_proj"](%504)
  %506 : __torch__.torch.nn.modules.linear.___torch_mangle_435.Linear = prim::GetAttr[name="v_proj"](%504)
  %507 : __torch__.torch.nn.modules.linear.___torch_mangle_434.Linear = prim::GetAttr[name="k_proj"](%504)
  %508 : __torch__.torch.nn.modules.linear.___torch_mangle_436.Linear = prim::GetAttr[name="q_proj"](%504)
  %509 : int = aten::size(%query.5, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %510 : int = aten::size(%query.5, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.5 : Long() = prim::NumToTensor(%510), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %512 : int = aten::size(%query.5, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %513 : Tensor = prim::GetAttr[name="bias"](%508)
  %514 : Tensor = prim::GetAttr[name="weight"](%508)
  %515 : Float(1024:1, 1024:1024) = aten::t(%514), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.25 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %515), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %517 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.25, %513, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.25 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%517, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %519 : Tensor = prim::GetAttr[name="bias"](%507)
  %520 : Tensor = prim::GetAttr[name="weight"](%507)
  %521 : Float(1024:1, 1024:1024) = aten::t(%520), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %521), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.27 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %519, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %524 : Tensor = prim::GetAttr[name="bias"](%506)
  %525 : Tensor = prim::GetAttr[name="weight"](%506)
  %526 : Float(1024:1, 1024:1024) = aten::t(%525), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.5, %526), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.29 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.27, %524, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.26 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.25, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %530 : Long() = aten::mul(%bsz.5, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %531 : int = aten::Int(%530), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %532 : int[] = prim::ListConstruct(%509, %531, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %533 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.26, %532), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%533, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.28 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.27, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %536 : Long() = aten::mul(%bsz.5, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %537 : int = aten::Int(%536), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %538 : int[] = prim::ListConstruct(%36, %537, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %539 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.28, %538), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%539, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.30 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.29, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %542 : Long() = aten::mul(%bsz.5, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %543 : int = aten::Int(%542), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %544 : int[] = prim::ListConstruct(%36, %543, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %545 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.30, %544), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.5 : Float(272:64, 13:17408, 64:1) = aten::transpose(%545, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %547 : int = aten::size(%k.5, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %548 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.5, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.17 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.5, %548), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %550 : int[] = prim::ListConstruct(%510, %27, %509, %547), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %attn_weights.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.17, %550), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:718:0
  %552 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.5 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%552, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.18, %reshaped.5, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:720:0
  %555 : Long() = aten::mul(%bsz.5, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %556 : int = aten::Int(%555), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %557 : int[] = prim::ListConstruct(%556, %509, %547), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %input.48 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.19, %557), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:721:0
  %input.49 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.48, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.20 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.49, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.5 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.20, %v.5), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %562 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.5, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %563 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%562, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %564 : int[] = prim::ListConstruct(%509, %510, %512), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::view(%563, %564), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %566 : Tensor = prim::GetAttr[name="bias"](%505)
  %567 : Tensor = prim::GetAttr[name="weight"](%505)
  %568 : Float(1024:1, 1024:1024) = aten::t(%567), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.28 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.50, %568), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.28, %566, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn/__module.model.encoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.51, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.52 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.5, %x.10, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:258:0
  %573 : Tensor = prim::GetAttr[name="bias"](%503)
  %574 : Tensor = prim::GetAttr[name="weight"](%503)
  %575 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm
  %input.53 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.52, %575, %574, %573, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %577 : Tensor = prim::GetAttr[name="bias"](%502)
  %578 : Tensor = prim::GetAttr[name="weight"](%502)
  %579 : Float(1024:1, 4096:1024) = aten::t(%578), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.53, %579), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.29, %577, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.55 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.54), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:1369:0
  %input.56 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.55, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %584 : Tensor = prim::GetAttr[name="bias"](%501)
  %585 : Tensor = prim::GetAttr[name="weight"](%501)
  %586 : Float(4096:1, 1024:4096) = aten::t(%585), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.56, %586), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.30, %584, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.57, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # torch/nn/functional.py:973:0
  %input.58 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.53, %x.11, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4 # transformers/modeling_bart.py:269:0
  %591 : Tensor = prim::GetAttr[name="bias"](%500)
  %592 : Tensor = prim::GetAttr[name="weight"](%500)
  %593 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm
  %query.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.58, %593, %592, %591, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.4/__module.model.encoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %595 : __torch__.torch.nn.modules.normalization.___torch_mangle_452.LayerNorm = prim::GetAttr[name="final_layer_norm"](%92)
  %596 : __torch__.torch.nn.modules.linear.___torch_mangle_451.Linear = prim::GetAttr[name="fc2"](%92)
  %597 : __torch__.torch.nn.modules.linear.___torch_mangle_450.Linear = prim::GetAttr[name="fc1"](%92)
  %598 : __torch__.torch.nn.modules.normalization.___torch_mangle_449.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%92)
  %599 : __torch__.transformers.modeling_bart.___torch_mangle_448.Attention = prim::GetAttr[name="self_attn"](%92)
  %600 : __torch__.torch.nn.modules.linear.___torch_mangle_447.Linear = prim::GetAttr[name="out_proj"](%599)
  %601 : __torch__.torch.nn.modules.linear.___torch_mangle_445.Linear = prim::GetAttr[name="v_proj"](%599)
  %602 : __torch__.torch.nn.modules.linear.___torch_mangle_444.Linear = prim::GetAttr[name="k_proj"](%599)
  %603 : __torch__.torch.nn.modules.linear.___torch_mangle_446.Linear = prim::GetAttr[name="q_proj"](%599)
  %604 : int = aten::size(%query.6, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %605 : int = aten::size(%query.6, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.6 : Long() = prim::NumToTensor(%605), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %607 : int = aten::size(%query.6, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %608 : Tensor = prim::GetAttr[name="bias"](%603)
  %609 : Tensor = prim::GetAttr[name="weight"](%603)
  %610 : Float(1024:1, 1024:1024) = aten::t(%609), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.31 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %610), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %612 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.31, %608, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.31 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%612, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %614 : Tensor = prim::GetAttr[name="bias"](%602)
  %615 : Tensor = prim::GetAttr[name="weight"](%602)
  %616 : Float(1024:1, 1024:1024) = aten::t(%615), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %616), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.33 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %614, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %619 : Tensor = prim::GetAttr[name="bias"](%601)
  %620 : Tensor = prim::GetAttr[name="weight"](%601)
  %621 : Float(1024:1, 1024:1024) = aten::t(%620), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.6, %621), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.35 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.33, %619, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.32 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.31, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %625 : Long() = aten::mul(%bsz.6, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %626 : int = aten::Int(%625), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %627 : int[] = prim::ListConstruct(%604, %626, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %628 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.32, %627), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%628, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.34 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.33, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %631 : Long() = aten::mul(%bsz.6, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %632 : int = aten::Int(%631), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %633 : int[] = prim::ListConstruct(%36, %632, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %634 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.34, %633), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%634, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.36 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.35, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %637 : Long() = aten::mul(%bsz.6, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %638 : int = aten::Int(%637), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %639 : int[] = prim::ListConstruct(%36, %638, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %640 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.36, %639), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.6 : Float(272:64, 13:17408, 64:1) = aten::transpose(%640, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %642 : int = aten::size(%k.6, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %643 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.6, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.21 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.6, %643), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %645 : int[] = prim::ListConstruct(%605, %27, %604, %642), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %attn_weights.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.21, %645), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:718:0
  %647 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.6 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%647, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.22, %reshaped.6, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:720:0
  %650 : Long() = aten::mul(%bsz.6, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %651 : int = aten::Int(%650), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %652 : int[] = prim::ListConstruct(%651, %604, %642), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %input.59 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.23, %652), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:721:0
  %input.60 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.59, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.24 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.60, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.6 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.24, %v.6), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %657 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.6, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %658 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%657, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %659 : int[] = prim::ListConstruct(%604, %605, %607), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn
  %input.61 : Float(13:17408, 17:1024, 1024:1) = aten::view(%658, %659), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %661 : Tensor = prim::GetAttr[name="bias"](%600)
  %662 : Tensor = prim::GetAttr[name="weight"](%600)
  %663 : Float(1024:1, 1024:1024) = aten::t(%662), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.34 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.61, %663), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.62 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.34, %661, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn/__module.model.encoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.62, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.63 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.6, %x.12, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:258:0
  %668 : Tensor = prim::GetAttr[name="bias"](%598)
  %669 : Tensor = prim::GetAttr[name="weight"](%598)
  %670 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.63, %670, %669, %668, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %672 : Tensor = prim::GetAttr[name="bias"](%597)
  %673 : Tensor = prim::GetAttr[name="weight"](%597)
  %674 : Float(1024:1, 4096:1024) = aten::t(%673), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.64, %674), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.65 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.35, %672, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.66 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.65), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:1369:0
  %input.67 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.66, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %679 : Tensor = prim::GetAttr[name="bias"](%596)
  %680 : Tensor = prim::GetAttr[name="weight"](%596)
  %681 : Float(4096:1, 1024:4096) = aten::t(%680), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.67, %681), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.36, %679, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.64, %x.13, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5 # transformers/modeling_bart.py:269:0
  %686 : Tensor = prim::GetAttr[name="bias"](%595)
  %687 : Tensor = prim::GetAttr[name="weight"](%595)
  %688 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm
  %query.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %688, %687, %686, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.5/__module.model.encoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %690 : __torch__.torch.nn.modules.normalization.___torch_mangle_462.LayerNorm = prim::GetAttr[name="final_layer_norm"](%90)
  %691 : __torch__.torch.nn.modules.linear.___torch_mangle_461.Linear = prim::GetAttr[name="fc2"](%90)
  %692 : __torch__.torch.nn.modules.linear.___torch_mangle_460.Linear = prim::GetAttr[name="fc1"](%90)
  %693 : __torch__.torch.nn.modules.normalization.___torch_mangle_459.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%90)
  %694 : __torch__.transformers.modeling_bart.___torch_mangle_458.Attention = prim::GetAttr[name="self_attn"](%90)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_457.Linear = prim::GetAttr[name="out_proj"](%694)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_455.Linear = prim::GetAttr[name="v_proj"](%694)
  %697 : __torch__.torch.nn.modules.linear.___torch_mangle_454.Linear = prim::GetAttr[name="k_proj"](%694)
  %698 : __torch__.torch.nn.modules.linear.___torch_mangle_456.Linear = prim::GetAttr[name="q_proj"](%694)
  %699 : int = aten::size(%query.7, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %700 : int = aten::size(%query.7, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.7 : Long() = prim::NumToTensor(%700), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %702 : int = aten::size(%query.7, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %703 : Tensor = prim::GetAttr[name="bias"](%698)
  %704 : Tensor = prim::GetAttr[name="weight"](%698)
  %705 : Float(1024:1, 1024:1024) = aten::t(%704), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.37 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %705), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %707 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.37, %703, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.37 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%707, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %709 : Tensor = prim::GetAttr[name="bias"](%697)
  %710 : Tensor = prim::GetAttr[name="weight"](%697)
  %711 : Float(1024:1, 1024:1024) = aten::t(%710), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %711), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.39 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %709, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %714 : Tensor = prim::GetAttr[name="bias"](%696)
  %715 : Tensor = prim::GetAttr[name="weight"](%696)
  %716 : Float(1024:1, 1024:1024) = aten::t(%715), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.7, %716), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.41 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.39, %714, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.38 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.37, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %720 : Long() = aten::mul(%bsz.7, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %721 : int = aten::Int(%720), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %722 : int[] = prim::ListConstruct(%699, %721, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %723 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.38, %722), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%723, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.40 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.39, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %726 : Long() = aten::mul(%bsz.7, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %727 : int = aten::Int(%726), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %728 : int[] = prim::ListConstruct(%36, %727, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %729 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.40, %728), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%729, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.42 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.41, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %732 : Long() = aten::mul(%bsz.7, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %733 : int = aten::Int(%732), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %734 : int[] = prim::ListConstruct(%36, %733, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %735 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.42, %734), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.7 : Float(272:64, 13:17408, 64:1) = aten::transpose(%735, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %737 : int = aten::size(%k.7, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %738 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.7, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.25 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.7, %738), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %740 : int[] = prim::ListConstruct(%700, %27, %699, %737), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %attn_weights.26 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.25, %740), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:718:0
  %742 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.7 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%742, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.27 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.26, %reshaped.7, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:720:0
  %745 : Long() = aten::mul(%bsz.7, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %746 : int = aten::Int(%745), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %747 : int[] = prim::ListConstruct(%746, %699, %737), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %input.70 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.27, %747), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:721:0
  %input.71 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.70, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.28 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.71, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.7 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.28, %v.7), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %752 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.7, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %753 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%752, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %754 : int[] = prim::ListConstruct(%699, %700, %702), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn
  %input.72 : Float(13:17408, 17:1024, 1024:1) = aten::view(%753, %754), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %756 : Tensor = prim::GetAttr[name="bias"](%695)
  %757 : Tensor = prim::GetAttr[name="weight"](%695)
  %758 : Float(1024:1, 1024:1024) = aten::t(%757), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.40 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %758), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.40, %756, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn/__module.model.encoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.7, %x.14, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:258:0
  %763 : Tensor = prim::GetAttr[name="bias"](%693)
  %764 : Tensor = prim::GetAttr[name="weight"](%693)
  %765 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm
  %input.75 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %765, %764, %763, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %767 : Tensor = prim::GetAttr[name="bias"](%692)
  %768 : Tensor = prim::GetAttr[name="weight"](%692)
  %769 : Float(1024:1, 4096:1024) = aten::t(%768), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.75, %769), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.76 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.41, %767, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.77 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.76), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:1369:0
  %input.78 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.77, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %774 : Tensor = prim::GetAttr[name="bias"](%691)
  %775 : Tensor = prim::GetAttr[name="weight"](%691)
  %776 : Float(4096:1, 1024:4096) = aten::t(%775), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.78, %776), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.42, %774, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.79, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # torch/nn/functional.py:973:0
  %input.80 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.75, %x.15, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6 # transformers/modeling_bart.py:269:0
  %781 : Tensor = prim::GetAttr[name="bias"](%690)
  %782 : Tensor = prim::GetAttr[name="weight"](%690)
  %783 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm
  %query.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.80, %783, %782, %781, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.6/__module.model.encoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %785 : __torch__.torch.nn.modules.normalization.___torch_mangle_472.LayerNorm = prim::GetAttr[name="final_layer_norm"](%88)
  %786 : __torch__.torch.nn.modules.linear.___torch_mangle_471.Linear = prim::GetAttr[name="fc2"](%88)
  %787 : __torch__.torch.nn.modules.linear.___torch_mangle_470.Linear = prim::GetAttr[name="fc1"](%88)
  %788 : __torch__.torch.nn.modules.normalization.___torch_mangle_469.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%88)
  %789 : __torch__.transformers.modeling_bart.___torch_mangle_468.Attention = prim::GetAttr[name="self_attn"](%88)
  %790 : __torch__.torch.nn.modules.linear.___torch_mangle_467.Linear = prim::GetAttr[name="out_proj"](%789)
  %791 : __torch__.torch.nn.modules.linear.___torch_mangle_465.Linear = prim::GetAttr[name="v_proj"](%789)
  %792 : __torch__.torch.nn.modules.linear.___torch_mangle_464.Linear = prim::GetAttr[name="k_proj"](%789)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_466.Linear = prim::GetAttr[name="q_proj"](%789)
  %794 : int = aten::size(%query.8, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %795 : int = aten::size(%query.8, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.8 : Long() = prim::NumToTensor(%795), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %797 : int = aten::size(%query.8, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %798 : Tensor = prim::GetAttr[name="bias"](%793)
  %799 : Tensor = prim::GetAttr[name="weight"](%793)
  %800 : Float(1024:1, 1024:1024) = aten::t(%799), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.43 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %800), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %802 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.43, %798, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.43 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%802, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %804 : Tensor = prim::GetAttr[name="bias"](%792)
  %805 : Tensor = prim::GetAttr[name="weight"](%792)
  %806 : Float(1024:1, 1024:1024) = aten::t(%805), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %806), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.45 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %804, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %809 : Tensor = prim::GetAttr[name="bias"](%791)
  %810 : Tensor = prim::GetAttr[name="weight"](%791)
  %811 : Float(1024:1, 1024:1024) = aten::t(%810), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.8, %811), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.47 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.45, %809, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.44 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.43, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %815 : Long() = aten::mul(%bsz.8, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %816 : int = aten::Int(%815), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %817 : int[] = prim::ListConstruct(%794, %816, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %818 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.44, %817), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%818, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.46 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.45, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %821 : Long() = aten::mul(%bsz.8, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %822 : int = aten::Int(%821), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %823 : int[] = prim::ListConstruct(%36, %822, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %824 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.46, %823), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%824, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.48 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.47, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %827 : Long() = aten::mul(%bsz.8, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %828 : int = aten::Int(%827), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %829 : int[] = prim::ListConstruct(%36, %828, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %830 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.48, %829), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.8 : Float(272:64, 13:17408, 64:1) = aten::transpose(%830, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %832 : int = aten::size(%k.8, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %833 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.8, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.29 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.8, %833), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %835 : int[] = prim::ListConstruct(%795, %27, %794, %832), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %attn_weights.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.29, %835), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:718:0
  %837 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.8 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%837, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.30, %reshaped.8, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:720:0
  %840 : Long() = aten::mul(%bsz.8, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %841 : int = aten::Int(%840), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %842 : int[] = prim::ListConstruct(%841, %794, %832), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %input.81 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.31, %842), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:721:0
  %input.82 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.81, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.32 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.82, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.8 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.32, %v.8), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %847 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.8, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %848 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%847, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %849 : int[] = prim::ListConstruct(%794, %795, %797), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::view(%848, %849), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %851 : Tensor = prim::GetAttr[name="bias"](%790)
  %852 : Tensor = prim::GetAttr[name="weight"](%790)
  %853 : Float(1024:1, 1024:1024) = aten::t(%852), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.46 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.83, %853), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.84 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.46, %851, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn/__module.model.encoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.84, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.85 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.8, %x.16, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:258:0
  %858 : Tensor = prim::GetAttr[name="bias"](%788)
  %859 : Tensor = prim::GetAttr[name="weight"](%788)
  %860 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.85, %860, %859, %858, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %862 : Tensor = prim::GetAttr[name="bias"](%787)
  %863 : Tensor = prim::GetAttr[name="weight"](%787)
  %864 : Float(1024:1, 4096:1024) = aten::t(%863), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.86, %864), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.87 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.47, %862, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.87), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:1369:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.88, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %869 : Tensor = prim::GetAttr[name="bias"](%786)
  %870 : Tensor = prim::GetAttr[name="weight"](%786)
  %871 : Float(4096:1, 1024:4096) = aten::t(%870), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.89, %871), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.90 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.48, %869, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.90, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # torch/nn/functional.py:973:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.86, %x.17, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7 # transformers/modeling_bart.py:269:0
  %876 : Tensor = prim::GetAttr[name="bias"](%785)
  %877 : Tensor = prim::GetAttr[name="weight"](%785)
  %878 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm
  %query.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.91, %878, %877, %876, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.7/__module.model.encoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %880 : __torch__.torch.nn.modules.normalization.___torch_mangle_482.LayerNorm = prim::GetAttr[name="final_layer_norm"](%86)
  %881 : __torch__.torch.nn.modules.linear.___torch_mangle_481.Linear = prim::GetAttr[name="fc2"](%86)
  %882 : __torch__.torch.nn.modules.linear.___torch_mangle_480.Linear = prim::GetAttr[name="fc1"](%86)
  %883 : __torch__.torch.nn.modules.normalization.___torch_mangle_479.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%86)
  %884 : __torch__.transformers.modeling_bart.___torch_mangle_478.Attention = prim::GetAttr[name="self_attn"](%86)
  %885 : __torch__.torch.nn.modules.linear.___torch_mangle_477.Linear = prim::GetAttr[name="out_proj"](%884)
  %886 : __torch__.torch.nn.modules.linear.___torch_mangle_475.Linear = prim::GetAttr[name="v_proj"](%884)
  %887 : __torch__.torch.nn.modules.linear.___torch_mangle_474.Linear = prim::GetAttr[name="k_proj"](%884)
  %888 : __torch__.torch.nn.modules.linear.___torch_mangle_476.Linear = prim::GetAttr[name="q_proj"](%884)
  %889 : int = aten::size(%query.9, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %890 : int = aten::size(%query.9, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.9 : Long() = prim::NumToTensor(%890), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %892 : int = aten::size(%query.9, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %893 : Tensor = prim::GetAttr[name="bias"](%888)
  %894 : Tensor = prim::GetAttr[name="weight"](%888)
  %895 : Float(1024:1, 1024:1024) = aten::t(%894), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.49 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %895), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %897 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.49, %893, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.49 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%897, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %899 : Tensor = prim::GetAttr[name="bias"](%887)
  %900 : Tensor = prim::GetAttr[name="weight"](%887)
  %901 : Float(1024:1, 1024:1024) = aten::t(%900), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %901), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.51 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %899, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %904 : Tensor = prim::GetAttr[name="bias"](%886)
  %905 : Tensor = prim::GetAttr[name="weight"](%886)
  %906 : Float(1024:1, 1024:1024) = aten::t(%905), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.9, %906), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.53 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.51, %904, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.50 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.49, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %910 : Long() = aten::mul(%bsz.9, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %911 : int = aten::Int(%910), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %912 : int[] = prim::ListConstruct(%889, %911, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %913 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.50, %912), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%913, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.52 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.51, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %916 : Long() = aten::mul(%bsz.9, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %917 : int = aten::Int(%916), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %918 : int[] = prim::ListConstruct(%36, %917, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %919 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.52, %918), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%919, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.54 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.53, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %922 : Long() = aten::mul(%bsz.9, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %923 : int = aten::Int(%922), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %924 : int[] = prim::ListConstruct(%36, %923, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %925 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.54, %924), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.9 : Float(272:64, 13:17408, 64:1) = aten::transpose(%925, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %927 : int = aten::size(%k.9, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %928 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.9, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.33 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.9, %928), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %930 : int[] = prim::ListConstruct(%890, %27, %889, %927), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %attn_weights.34 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.33, %930), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:718:0
  %932 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.9 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%932, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.35 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.34, %reshaped.9, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:720:0
  %935 : Long() = aten::mul(%bsz.9, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %936 : int = aten::Int(%935), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %937 : int[] = prim::ListConstruct(%936, %889, %927), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %input.92 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.35, %937), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:721:0
  %input.93 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.92, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.36 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.93, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.9 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.36, %v.9), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %942 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.9, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %943 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%942, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %944 : int[] = prim::ListConstruct(%889, %890, %892), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn
  %input.94 : Float(13:17408, 17:1024, 1024:1) = aten::view(%943, %944), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %946 : Tensor = prim::GetAttr[name="bias"](%885)
  %947 : Tensor = prim::GetAttr[name="weight"](%885)
  %948 : Float(1024:1, 1024:1024) = aten::t(%947), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.52 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.94, %948), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.52, %946, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn/__module.model.encoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.9, %x.18, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:258:0
  %953 : Tensor = prim::GetAttr[name="bias"](%883)
  %954 : Tensor = prim::GetAttr[name="weight"](%883)
  %955 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm
  %input.97 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %955, %954, %953, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %957 : Tensor = prim::GetAttr[name="bias"](%882)
  %958 : Tensor = prim::GetAttr[name="weight"](%882)
  %959 : Float(1024:1, 4096:1024) = aten::t(%958), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.97, %959), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.53, %957, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.98), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:1369:0
  %input.100 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.99, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %964 : Tensor = prim::GetAttr[name="bias"](%881)
  %965 : Tensor = prim::GetAttr[name="weight"](%881)
  %966 : Float(4096:1, 1024:4096) = aten::t(%965), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.100, %966), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.54, %964, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.101, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # torch/nn/functional.py:973:0
  %input.102 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.97, %x.19, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8 # transformers/modeling_bart.py:269:0
  %971 : Tensor = prim::GetAttr[name="bias"](%880)
  %972 : Tensor = prim::GetAttr[name="weight"](%880)
  %973 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm
  %query.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.102, %973, %972, %971, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.8/__module.model.encoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %975 : __torch__.torch.nn.modules.normalization.___torch_mangle_492.LayerNorm = prim::GetAttr[name="final_layer_norm"](%84)
  %976 : __torch__.torch.nn.modules.linear.___torch_mangle_491.Linear = prim::GetAttr[name="fc2"](%84)
  %977 : __torch__.torch.nn.modules.linear.___torch_mangle_490.Linear = prim::GetAttr[name="fc1"](%84)
  %978 : __torch__.torch.nn.modules.normalization.___torch_mangle_489.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%84)
  %979 : __torch__.transformers.modeling_bart.___torch_mangle_488.Attention = prim::GetAttr[name="self_attn"](%84)
  %980 : __torch__.torch.nn.modules.linear.___torch_mangle_487.Linear = prim::GetAttr[name="out_proj"](%979)
  %981 : __torch__.torch.nn.modules.linear.___torch_mangle_485.Linear = prim::GetAttr[name="v_proj"](%979)
  %982 : __torch__.torch.nn.modules.linear.___torch_mangle_484.Linear = prim::GetAttr[name="k_proj"](%979)
  %983 : __torch__.torch.nn.modules.linear.___torch_mangle_486.Linear = prim::GetAttr[name="q_proj"](%979)
  %984 : int = aten::size(%query.10, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %985 : int = aten::size(%query.10, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.10 : Long() = prim::NumToTensor(%985), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %987 : int = aten::size(%query.10, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %988 : Tensor = prim::GetAttr[name="bias"](%983)
  %989 : Tensor = prim::GetAttr[name="weight"](%983)
  %990 : Float(1024:1, 1024:1024) = aten::t(%989), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.55 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %990), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %992 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.55, %988, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.55 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%992, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %994 : Tensor = prim::GetAttr[name="bias"](%982)
  %995 : Tensor = prim::GetAttr[name="weight"](%982)
  %996 : Float(1024:1, 1024:1024) = aten::t(%995), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %996), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.57 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %994, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %999 : Tensor = prim::GetAttr[name="bias"](%981)
  %1000 : Tensor = prim::GetAttr[name="weight"](%981)
  %1001 : Float(1024:1, 1024:1024) = aten::t(%1000), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.10, %1001), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.59 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.57, %999, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.56 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.55, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1005 : Long() = aten::mul(%bsz.10, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1006 : int = aten::Int(%1005), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1007 : int[] = prim::ListConstruct(%984, %1006, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1008 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.56, %1007), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1008, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.58 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.57, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1011 : Long() = aten::mul(%bsz.10, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1012 : int = aten::Int(%1011), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1013 : int[] = prim::ListConstruct(%36, %1012, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1014 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.58, %1013), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1014, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.60 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.59, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1017 : Long() = aten::mul(%bsz.10, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1018 : int = aten::Int(%1017), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1019 : int[] = prim::ListConstruct(%36, %1018, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1020 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.60, %1019), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.10 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1020, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %1022 : int = aten::size(%k.10, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %1023 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.10, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.37 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.10, %1023), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %1025 : int[] = prim::ListConstruct(%985, %27, %984, %1022), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %attn_weights.38 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.37, %1025), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:718:0
  %1027 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.10 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1027, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.38, %reshaped.10, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:720:0
  %1030 : Long() = aten::mul(%bsz.10, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %1031 : int = aten::Int(%1030), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %1032 : int[] = prim::ListConstruct(%1031, %984, %1022), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %input.103 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.39, %1032), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:721:0
  %input.104 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.103, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.40 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.104, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.10 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.40, %v.10), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %1037 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.10, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1038 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1037, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1039 : int[] = prim::ListConstruct(%984, %985, %987), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1038, %1039), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %1041 : Tensor = prim::GetAttr[name="bias"](%980)
  %1042 : Tensor = prim::GetAttr[name="weight"](%980)
  %1043 : Float(1024:1, 1024:1024) = aten::t(%1042), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.58 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.105, %1043), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.106 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.58, %1041, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn/__module.model.encoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.106, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.107 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.10, %x.20, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:258:0
  %1048 : Tensor = prim::GetAttr[name="bias"](%978)
  %1049 : Tensor = prim::GetAttr[name="weight"](%978)
  %1050 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm
  %input.108 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.107, %1050, %1049, %1048, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1052 : Tensor = prim::GetAttr[name="bias"](%977)
  %1053 : Tensor = prim::GetAttr[name="weight"](%977)
  %1054 : Float(1024:1, 4096:1024) = aten::t(%1053), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.108, %1054), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.59, %1052, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.110 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.109), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:1369:0
  %input.111 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.110, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %1059 : Tensor = prim::GetAttr[name="bias"](%976)
  %1060 : Tensor = prim::GetAttr[name="weight"](%976)
  %1061 : Float(4096:1, 1024:4096) = aten::t(%1060), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.111, %1061), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.112 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.60, %1059, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.112, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # torch/nn/functional.py:973:0
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.108, %x.21, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9 # transformers/modeling_bart.py:269:0
  %1066 : Tensor = prim::GetAttr[name="bias"](%975)
  %1067 : Tensor = prim::GetAttr[name="weight"](%975)
  %1068 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm
  %query.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.113, %1068, %1067, %1066, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.9/__module.model.encoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %1070 : __torch__.torch.nn.modules.normalization.___torch_mangle_502.LayerNorm = prim::GetAttr[name="final_layer_norm"](%82)
  %1071 : __torch__.torch.nn.modules.linear.___torch_mangle_501.Linear = prim::GetAttr[name="fc2"](%82)
  %1072 : __torch__.torch.nn.modules.linear.___torch_mangle_500.Linear = prim::GetAttr[name="fc1"](%82)
  %1073 : __torch__.torch.nn.modules.normalization.___torch_mangle_499.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%82)
  %1074 : __torch__.transformers.modeling_bart.___torch_mangle_498.Attention = prim::GetAttr[name="self_attn"](%82)
  %1075 : __torch__.torch.nn.modules.linear.___torch_mangle_497.Linear = prim::GetAttr[name="out_proj"](%1074)
  %1076 : __torch__.torch.nn.modules.linear.___torch_mangle_495.Linear = prim::GetAttr[name="v_proj"](%1074)
  %1077 : __torch__.torch.nn.modules.linear.___torch_mangle_494.Linear = prim::GetAttr[name="k_proj"](%1074)
  %1078 : __torch__.torch.nn.modules.linear.___torch_mangle_496.Linear = prim::GetAttr[name="q_proj"](%1074)
  %1079 : int = aten::size(%query.11, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1080 : int = aten::size(%query.11, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.11 : Long() = prim::NumToTensor(%1080), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1082 : int = aten::size(%query.11, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %1083 : Tensor = prim::GetAttr[name="bias"](%1078)
  %1084 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1085 : Float(1024:1, 1024:1024) = aten::t(%1084), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.61 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1085), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1087 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.61, %1083, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.61 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1087, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %1089 : Tensor = prim::GetAttr[name="bias"](%1077)
  %1090 : Tensor = prim::GetAttr[name="weight"](%1077)
  %1091 : Float(1024:1, 1024:1024) = aten::t(%1090), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1091), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.63 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %1089, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1094 : Tensor = prim::GetAttr[name="bias"](%1076)
  %1095 : Tensor = prim::GetAttr[name="weight"](%1076)
  %1096 : Float(1024:1, 1024:1024) = aten::t(%1095), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.11, %1096), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.65 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.63, %1094, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.62 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.61, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1100 : Long() = aten::mul(%bsz.11, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1101 : int = aten::Int(%1100), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1102 : int[] = prim::ListConstruct(%1079, %1101, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1103 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.62, %1102), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1103, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.64 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.63, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1106 : Long() = aten::mul(%bsz.11, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1107 : int = aten::Int(%1106), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1108 : int[] = prim::ListConstruct(%36, %1107, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1109 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.64, %1108), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1109, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.66 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.65, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1112 : Long() = aten::mul(%bsz.11, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1113 : int = aten::Int(%1112), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1114 : int[] = prim::ListConstruct(%36, %1113, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1115 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.66, %1114), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.11 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1115, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %1117 : int = aten::size(%k.11, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %1118 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.11, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.41 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.11, %1118), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %1120 : int[] = prim::ListConstruct(%1080, %27, %1079, %1117), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %attn_weights.42 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.41, %1120), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:718:0
  %1122 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.11 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1122, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.43 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.42, %reshaped.11, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:720:0
  %1125 : Long() = aten::mul(%bsz.11, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %1126 : int = aten::Int(%1125), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %1127 : int[] = prim::ListConstruct(%1126, %1079, %1117), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %input.114 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.43, %1127), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:721:0
  %input.115 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.114, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.44 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.115, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.11 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.44, %v.11), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %1132 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.11, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1133 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1132, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1134 : int[] = prim::ListConstruct(%1079, %1080, %1082), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn
  %input.116 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1133, %1134), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %1136 : Tensor = prim::GetAttr[name="bias"](%1075)
  %1137 : Tensor = prim::GetAttr[name="weight"](%1075)
  %1138 : Float(1024:1, 1024:1024) = aten::t(%1137), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.64 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.116, %1138), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.64, %1136, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn/__module.model.encoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.117, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.11, %x.22, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:258:0
  %1143 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1144 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1145 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.118, %1145, %1144, %1143, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1147 : Tensor = prim::GetAttr[name="bias"](%1072)
  %1148 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1149 : Float(1024:1, 4096:1024) = aten::t(%1148), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.119, %1149), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.120 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.65, %1147, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.121 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.120), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:1369:0
  %input.122 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.121, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %1154 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1155 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1156 : Float(4096:1, 1024:4096) = aten::t(%1155), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.122, %1156), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.66, %1154, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.123, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # torch/nn/functional.py:973:0
  %input.124 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.119, %x.23, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10 # transformers/modeling_bart.py:269:0
  %1161 : Tensor = prim::GetAttr[name="bias"](%1070)
  %1162 : Tensor = prim::GetAttr[name="weight"](%1070)
  %1163 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm
  %query.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.124, %1163, %1162, %1161, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.10/__module.model.encoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %1165 : __torch__.torch.nn.modules.normalization.___torch_mangle_512.LayerNorm = prim::GetAttr[name="final_layer_norm"](%80)
  %1166 : __torch__.torch.nn.modules.linear.___torch_mangle_511.Linear = prim::GetAttr[name="fc2"](%80)
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_510.Linear = prim::GetAttr[name="fc1"](%80)
  %1168 : __torch__.torch.nn.modules.normalization.___torch_mangle_509.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%80)
  %1169 : __torch__.transformers.modeling_bart.___torch_mangle_508.Attention = prim::GetAttr[name="self_attn"](%80)
  %1170 : __torch__.torch.nn.modules.linear.___torch_mangle_507.Linear = prim::GetAttr[name="out_proj"](%1169)
  %1171 : __torch__.torch.nn.modules.linear.___torch_mangle_505.Linear = prim::GetAttr[name="v_proj"](%1169)
  %1172 : __torch__.torch.nn.modules.linear.___torch_mangle_504.Linear = prim::GetAttr[name="k_proj"](%1169)
  %1173 : __torch__.torch.nn.modules.linear.___torch_mangle_506.Linear = prim::GetAttr[name="q_proj"](%1169)
  %1174 : int = aten::size(%query.12, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1175 : int = aten::size(%query.12, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.12 : Long() = prim::NumToTensor(%1175), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1177 : int = aten::size(%query.12, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %1178 : Tensor = prim::GetAttr[name="bias"](%1173)
  %1179 : Tensor = prim::GetAttr[name="weight"](%1173)
  %1180 : Float(1024:1, 1024:1024) = aten::t(%1179), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.67 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1180), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1182 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.67, %1178, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.67 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1182, %24), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %1184 : Tensor = prim::GetAttr[name="bias"](%1172)
  %1185 : Tensor = prim::GetAttr[name="weight"](%1172)
  %1186 : Float(1024:1, 1024:1024) = aten::t(%1185), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1186), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.69 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %1184, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1189 : Tensor = prim::GetAttr[name="bias"](%1171)
  %1190 : Tensor = prim::GetAttr[name="weight"](%1171)
  %1191 : Float(1024:1, 1024:1024) = aten::t(%1190), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.12, %1191), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.71 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.69, %1189, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.68 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.67, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1195 : Long() = aten::mul(%bsz.12, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1196 : int = aten::Int(%1195), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1197 : int[] = prim::ListConstruct(%1174, %1196, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1198 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.68, %1197), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1198, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.70 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.69, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1201 : Long() = aten::mul(%bsz.12, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1202 : int = aten::Int(%1201), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1203 : int[] = prim::ListConstruct(%36, %1202, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1204 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.70, %1203), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1204, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.72 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.71, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1207 : Long() = aten::mul(%bsz.12, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1208 : int = aten::Int(%1207), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1209 : int[] = prim::ListConstruct(%36, %1208, %26), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1210 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.72, %1209), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.12 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1210, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %1212 : int = aten::size(%k.12, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %1213 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.12, %39, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.45 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.12, %1213), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %1215 : int[] = prim::ListConstruct(%1175, %27, %1174, %1212), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %attn_weights.46 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.45, %1215), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:718:0
  %1217 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask.1, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %reshaped.12 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1217, %23), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:719:0
  %attn_weights.47 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.46, %reshaped.12, %29), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:720:0
  %1220 : Long() = aten::mul(%bsz.12, %25), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %1221 : int = aten::Int(%1220), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %1222 : int[] = prim::ListConstruct(%1221, %1174, %1212), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %input.125 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.47, %1222), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:721:0
  %input.126 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.125, %36, %40), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.48 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.126, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.12 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.48, %v.12), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %1227 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.12, %35, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1228 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1227, %35), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1229 : int[] = prim::ListConstruct(%1174, %1175, %1177), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1228, %1229), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %1231 : Tensor = prim::GetAttr[name="bias"](%1170)
  %1232 : Tensor = prim::GetAttr[name="weight"](%1170)
  %1233 : Float(1024:1, 1024:1024) = aten::t(%1232), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.70 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.127, %1233), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.70, %1231, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn/__module.model.encoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.128, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.129 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.12, %x.24, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:258:0
  %1238 : Tensor = prim::GetAttr[name="bias"](%1168)
  %1239 : Tensor = prim::GetAttr[name="weight"](%1168)
  %1240 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm
  %input.130 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.129, %1240, %1239, %1238, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1242 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1244 : Float(1024:1, 4096:1024) = aten::t(%1243), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.130, %1244), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.131 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.71, %1242, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.132 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.131), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:1369:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.132, %28, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %1249 : Tensor = prim::GetAttr[name="bias"](%1166)
  %1250 : Tensor = prim::GetAttr[name="weight"](%1166)
  %1251 : Float(4096:1, 1024:4096) = aten::t(%1250), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output.72 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.133, %1251), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.134 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.72, %1249, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.25 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.134, %22, %38), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # torch/nn/functional.py:973:0
  %input.135 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.130, %x.25, %39), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11 # transformers/modeling_bart.py:269:0
  %1256 : Tensor = prim::GetAttr[name="bias"](%1165)
  %1257 : Tensor = prim::GetAttr[name="weight"](%1165)
  %1258 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm
  %x.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.135, %1258, %1257, %1256, %20, %19), scope: __module.model/__module.model.encoder/__module.model.encoder.layers.11/__module.model.encoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %encoder_hidden_states : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x.26, %35, %39), scope: __module.model/__module.model.encoder # transformers/modeling_bart.py:366:0
  %1261 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1262 : __torch__.transformers.modeling_bart.___torch_mangle_708.DecoderLayer = prim::GetAttr[name="11"](%1261)
  %1263 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1264 : __torch__.transformers.modeling_bart.___torch_mangle_692.DecoderLayer = prim::GetAttr[name="10"](%1263)
  %1265 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1266 : __torch__.transformers.modeling_bart.___torch_mangle_676.DecoderLayer = prim::GetAttr[name="9"](%1265)
  %1267 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1268 : __torch__.transformers.modeling_bart.___torch_mangle_660.DecoderLayer = prim::GetAttr[name="8"](%1267)
  %1269 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1270 : __torch__.transformers.modeling_bart.___torch_mangle_644.DecoderLayer = prim::GetAttr[name="7"](%1269)
  %1271 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1272 : __torch__.transformers.modeling_bart.___torch_mangle_628.DecoderLayer = prim::GetAttr[name="6"](%1271)
  %1273 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1274 : __torch__.transformers.modeling_bart.___torch_mangle_612.DecoderLayer = prim::GetAttr[name="5"](%1273)
  %1275 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1276 : __torch__.transformers.modeling_bart.___torch_mangle_596.DecoderLayer = prim::GetAttr[name="4"](%1275)
  %1277 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1278 : __torch__.transformers.modeling_bart.___torch_mangle_580.DecoderLayer = prim::GetAttr[name="3"](%1277)
  %1279 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1280 : __torch__.transformers.modeling_bart.___torch_mangle_564.DecoderLayer = prim::GetAttr[name="2"](%1279)
  %1281 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1282 : __torch__.transformers.modeling_bart.___torch_mangle_548.DecoderLayer = prim::GetAttr[name="1"](%1281)
  %1283 : __torch__.torch.nn.modules.container.___torch_mangle_709.ModuleList = prim::GetAttr[name="layers"](%41)
  %1284 : __torch__.transformers.modeling_bart.DecoderLayer = prim::GetAttr[name="0"](%1283)
  %1285 : __torch__.torch.nn.modules.normalization.___torch_mangle_710.LayerNorm = prim::GetAttr[name="layernorm_embedding"](%41)
  %1286 : __torch__.transformers.modeling_bart.___torch_mangle_517.LearnedPositionalEmbedding = prim::GetAttr[name="embed_positions"](%41)
  %key_padding_mask : Bool(17:13, 13:1) = aten::eq(%attention_mask, %35), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:136:0
  %1288 : Tensor = prim::GetAttr[name="weight"](%1286)
  %1289 : int = aten::size(%prev_output_tokens, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:816:0
  %positions.2 : Long(13:1) = aten::arange(%1289, %18, %35, %30, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:821:0
  %input.136 : Long(13:1) = aten::add(%positions.2, %17, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # transformers/modeling_bart.py:822:0
  %positions : Float(13:1024, 1024:1) = aten::embedding(%1288, %input.136, %39, %38, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_positions # torch/nn/functional.py:1814:0
  %1293 : Float(17:13312, 13:1024, 1024:1) = aten::embedding(%43, %prev_output_tokens, %39, %38, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.embed_tokens # torch/nn/functional.py:1814:0
  %x.27 : Float(17:13312, 13:1024, 1024:1) = aten::mul(%1293, %16), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:556:0
  %input.137 : Float(17:13312, 13:1024, 1024:1) = aten::add_(%x.27, %positions, %39), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:557:0
  %1296 : Tensor = prim::GetAttr[name="bias"](%1285)
  %1297 : Tensor = prim::GetAttr[name="weight"](%1285)
  %1298 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding
  %input.138 : Float(17:13312, 13:1024, 1024:1) = aten::layer_norm(%input.137, %1298, %1297, %1296, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layernorm_embedding # torch/nn/functional.py:2048:0
  %x.28 : Float(17:13312, 13:1024, 1024:1) = aten::dropout(%input.138, %22, %38), scope: __module.model/__module.model.decoder # torch/nn/functional.py:973:0
  %query.13 : Float(13:1024, 17:13312, 1024:1) = aten::transpose(%x.28, %35, %39), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:562:0
  %input.144 : Float(13:17408, 17:1024, 1024:1) = aten::transpose(%encoder_hidden_states, %35, %39), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:563:0
  %1303 : __torch__.torch.nn.modules.normalization.___torch_mangle_532.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1284)
  %1304 : __torch__.torch.nn.modules.linear.___torch_mangle_531.Linear = prim::GetAttr[name="fc2"](%1284)
  %1305 : __torch__.torch.nn.modules.linear.___torch_mangle_530.Linear = prim::GetAttr[name="fc1"](%1284)
  %1306 : __torch__.torch.nn.modules.normalization.___torch_mangle_529.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1284)
  %1307 : __torch__.transformers.modeling_bart.___torch_mangle_528.Attention = prim::GetAttr[name="encoder_attn"](%1284)
  %1308 : __torch__.torch.nn.modules.normalization.___torch_mangle_523.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1284)
  %1309 : __torch__.transformers.modeling_bart.___torch_mangle_522.Attention = prim::GetAttr[name="self_attn"](%1284)
  %1310 : __torch__.torch.nn.modules.linear.___torch_mangle_521.Linear = prim::GetAttr[name="out_proj"](%1309)
  %1311 : __torch__.torch.nn.modules.linear.___torch_mangle_519.Linear = prim::GetAttr[name="v_proj"](%1309)
  %1312 : __torch__.torch.nn.modules.linear.___torch_mangle_518.Linear = prim::GetAttr[name="k_proj"](%1309)
  %1313 : __torch__.torch.nn.modules.linear.___torch_mangle_520.Linear = prim::GetAttr[name="q_proj"](%1309)
  %1314 : int = aten::size(%query.13, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1315 : int = aten::size(%query.13, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %bsz.13 : Long() = prim::NumToTensor(%1315), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1317 : int = aten::size(%query.13, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:660:0
  %1318 : Tensor = prim::GetAttr[name="bias"](%1313)
  %1319 : Tensor = prim::GetAttr[name="weight"](%1313)
  %1320 : Float(1024:1, 1024:1024) = aten::t(%1319), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.73 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1320), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1322 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.73, %1318, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.73 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1322, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:673:0
  %1324 : Tensor = prim::GetAttr[name="bias"](%1312)
  %1325 : Tensor = prim::GetAttr[name="weight"](%1312)
  %1326 : Float(1024:1, 1024:1024) = aten::t(%1325), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.74 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1326), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.75 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.74, %1324, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1329 : Tensor = prim::GetAttr[name="bias"](%1311)
  %1330 : Tensor = prim::GetAttr[name="weight"](%1311)
  %1331 : Float(1024:1, 1024:1024) = aten::t(%1330), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.75 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.13, %1331), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.77 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.75, %1329, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.74 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.73, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1335 : Long() = aten::mul(%bsz.13, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1336 : int = aten::Int(%1335), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1337 : int[] = prim::ListConstruct(%1314, %1336, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1338 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.74, %1337), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %q.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1338, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.76 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.75, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1341 : Long() = aten::mul(%bsz.13, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1342 : int = aten::Int(%1341), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1343 : int[] = prim::ListConstruct(%36, %1342, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1344 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.76, %1343), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %k.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1344, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %tensor.78 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.77, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1347 : Long() = aten::mul(%bsz.13, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1348 : int = aten::Int(%1347), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1349 : int[] = prim::ListConstruct(%36, %1348, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1350 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.78, %1349), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %v.13 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1350, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:647:0
  %1352 : int = aten::size(%k.13, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:701:0
  %1353 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.13, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.49 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.13, %1353), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:702:0
  %1355 : int[] = prim::ListConstruct(%1315, %27, %1314, %1352), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1356 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.49, %1355), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.50 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1356, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:706:0
  %1358 : Long() = aten::mul(%bsz.13, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %1359 : int = aten::Int(%1358), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %1360 : int[] = prim::ListConstruct(%1359, %1314, %1352), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %input.139 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.50, %1360), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:707:0
  %input.140 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.139, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.51 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.140, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # torch/nn/functional.py:973:0
  %attn_output.13 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.51, %v.13), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:730:0
  %1365 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.13, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1366 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1365, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1367 : int[] = prim::ListConstruct(%1314, %1315, %1317), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1366, %1367), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn # transformers/modeling_bart.py:732:0
  %1369 : Tensor = prim::GetAttr[name="bias"](%1310)
  %1370 : Tensor = prim::GetAttr[name="weight"](%1310)
  %1371 : Float(1024:1, 1024:1024) = aten::t(%1370), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.76 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.141, %1371), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.142 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.76, %1369, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn/__module.model.decoder.layers.0.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.29 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.142, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.143 : Float(13:1024, 17:13312, 1024:1) = aten::add(%query.13, %x.29, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:427:0
  %1376 : Tensor = prim::GetAttr[name="bias"](%1308)
  %1377 : Tensor = prim::GetAttr[name="weight"](%1308)
  %1378 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm
  %query.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.143, %1378, %1377, %1376, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1380 : __torch__.torch.nn.modules.linear.___torch_mangle_527.Linear = prim::GetAttr[name="out_proj"](%1307)
  %1381 : __torch__.torch.nn.modules.linear.___torch_mangle_525.Linear = prim::GetAttr[name="v_proj"](%1307)
  %1382 : __torch__.torch.nn.modules.linear.___torch_mangle_524.Linear = prim::GetAttr[name="k_proj"](%1307)
  %1383 : __torch__.torch.nn.modules.linear.___torch_mangle_526.Linear = prim::GetAttr[name="q_proj"](%1307)
  %1384 : int = aten::size(%query.14, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1385 : int = aten::size(%query.14, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.14 : Long() = prim::NumToTensor(%1385), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1387 : int = aten::size(%query.14, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:660:0
  %1388 : Tensor = prim::GetAttr[name="bias"](%1383)
  %1389 : Tensor = prim::GetAttr[name="weight"](%1383)
  %1390 : Float(1024:1, 1024:1024) = aten::t(%1389), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.77 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.14, %1390), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1392 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.77, %1388, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.79 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1392, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:673:0
  %1394 : Tensor = prim::GetAttr[name="bias"](%1382)
  %1395 : Tensor = prim::GetAttr[name="weight"](%1382)
  %1396 : Float(1024:1, 1024:1024) = aten::t(%1395), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.78 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1396), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.81 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.78, %1394, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1399 : Tensor = prim::GetAttr[name="bias"](%1381)
  %1400 : Tensor = prim::GetAttr[name="weight"](%1381)
  %1401 : Float(1024:1, 1024:1024) = aten::t(%1400), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.79 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1401), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.83 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.79, %1399, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.80 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.79, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1405 : Long() = aten::mul(%bsz.14, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1406 : int = aten::Int(%1405), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1407 : int[] = prim::ListConstruct(%1384, %1406, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1408 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.80, %1407), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %q.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1408, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.82 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.81, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1411 : Long() = aten::mul(%bsz.14, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1412 : int = aten::Int(%1411), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1413 : int[] = prim::ListConstruct(%36, %1412, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1414 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.82, %1413), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %k.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1414, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.84 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.83, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1417 : Long() = aten::mul(%bsz.14, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1418 : int = aten::Int(%1417), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1419 : int[] = prim::ListConstruct(%36, %1418, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1420 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.84, %1419), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %v.14 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1420, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:647:0
  %1422 : int = aten::size(%k.14, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:701:0
  %1423 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.14, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.52 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.14, %1423), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:702:0
  %1425 : int[] = prim::ListConstruct(%1385, %27, %1384, %1422), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %attn_weights.53 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.52, %1425), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:718:0
  %1427 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.13 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1427, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.54 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.53, %reshaped.13, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:720:0
  %1430 : Long() = aten::mul(%bsz.14, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %1431 : int = aten::Int(%1430), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %1432 : int[] = prim::ListConstruct(%1431, %1384, %1422), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %input.145 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.54, %1432), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:721:0
  %input.146 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.145, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.55 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.146, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.14 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.55, %v.14), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:730:0
  %1437 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.14, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1438 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1437, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1439 : int[] = prim::ListConstruct(%1384, %1385, %1387), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn
  %input.147 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1438, %1439), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn # transformers/modeling_bart.py:732:0
  %1441 : Tensor = prim::GetAttr[name="bias"](%1380)
  %1442 : Tensor = prim::GetAttr[name="weight"](%1380)
  %1443 : Float(1024:1, 1024:1024) = aten::t(%1442), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.80 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.147, %1443), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.148 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.80, %1441, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn/__module.model.decoder.layers.0.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.148, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.14, %x.30, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:443:0
  %1448 : Tensor = prim::GetAttr[name="bias"](%1306)
  %1449 : Tensor = prim::GetAttr[name="weight"](%1306)
  %1450 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.149, %1450, %1449, %1448, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1452 : Tensor = prim::GetAttr[name="bias"](%1305)
  %1453 : Tensor = prim::GetAttr[name="weight"](%1305)
  %1454 : Float(1024:1, 4096:1024) = aten::t(%1453), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %output.81 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.150, %1454), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.81, %1452, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %1459 : Tensor = prim::GetAttr[name="bias"](%1304)
  %1460 : Tensor = prim::GetAttr[name="weight"](%1304)
  %1461 : Float(4096:1, 1024:4096) = aten::t(%1460), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %output.82 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %1461), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.82, %1459, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.fc2 # torch/nn/functional.py:1678:0
  %x.31 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.150, %x.31, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0 # transformers/modeling_bart.py:455:0
  %1466 : Tensor = prim::GetAttr[name="bias"](%1303)
  %1467 : Tensor = prim::GetAttr[name="weight"](%1303)
  %1468 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm
  %query.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %1468, %1467, %1466, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.0/__module.model.decoder.layers.0.final_layer_norm # torch/nn/functional.py:2048:0
  %1470 : __torch__.torch.nn.modules.normalization.___torch_mangle_547.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1282)
  %1471 : __torch__.torch.nn.modules.linear.___torch_mangle_546.Linear = prim::GetAttr[name="fc2"](%1282)
  %1472 : __torch__.torch.nn.modules.linear.___torch_mangle_545.Linear = prim::GetAttr[name="fc1"](%1282)
  %1473 : __torch__.torch.nn.modules.normalization.___torch_mangle_544.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1282)
  %1474 : __torch__.transformers.modeling_bart.___torch_mangle_543.Attention = prim::GetAttr[name="encoder_attn"](%1282)
  %1475 : __torch__.torch.nn.modules.normalization.___torch_mangle_538.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1282)
  %1476 : __torch__.transformers.modeling_bart.___torch_mangle_537.Attention = prim::GetAttr[name="self_attn"](%1282)
  %1477 : __torch__.torch.nn.modules.linear.___torch_mangle_536.Linear = prim::GetAttr[name="out_proj"](%1476)
  %1478 : __torch__.torch.nn.modules.linear.___torch_mangle_534.Linear = prim::GetAttr[name="v_proj"](%1476)
  %1479 : __torch__.torch.nn.modules.linear.___torch_mangle_533.Linear = prim::GetAttr[name="k_proj"](%1476)
  %1480 : __torch__.torch.nn.modules.linear.___torch_mangle_535.Linear = prim::GetAttr[name="q_proj"](%1476)
  %1481 : int = aten::size(%query.15, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1482 : int = aten::size(%query.15, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %bsz.15 : Long() = prim::NumToTensor(%1482), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1484 : int = aten::size(%query.15, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:660:0
  %1485 : Tensor = prim::GetAttr[name="bias"](%1480)
  %1486 : Tensor = prim::GetAttr[name="weight"](%1480)
  %1487 : Float(1024:1, 1024:1024) = aten::t(%1486), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.83 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1487), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1489 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.83, %1485, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.85 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1489, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:673:0
  %1491 : Tensor = prim::GetAttr[name="bias"](%1479)
  %1492 : Tensor = prim::GetAttr[name="weight"](%1479)
  %1493 : Float(1024:1, 1024:1024) = aten::t(%1492), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.84 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1493), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.87 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.84, %1491, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1496 : Tensor = prim::GetAttr[name="bias"](%1478)
  %1497 : Tensor = prim::GetAttr[name="weight"](%1478)
  %1498 : Float(1024:1, 1024:1024) = aten::t(%1497), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.85 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.15, %1498), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.89 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.85, %1496, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.86 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.85, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1502 : Long() = aten::mul(%bsz.15, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1503 : int = aten::Int(%1502), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1504 : int[] = prim::ListConstruct(%1481, %1503, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1505 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.86, %1504), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %q.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1505, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.88 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.87, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1508 : Long() = aten::mul(%bsz.15, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1509 : int = aten::Int(%1508), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1510 : int[] = prim::ListConstruct(%36, %1509, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1511 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.88, %1510), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %k.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1511, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %tensor.90 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.89, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1514 : Long() = aten::mul(%bsz.15, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1515 : int = aten::Int(%1514), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1516 : int[] = prim::ListConstruct(%36, %1515, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1517 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.90, %1516), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %v.15 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1517, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:647:0
  %1519 : int = aten::size(%k.15, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:701:0
  %1520 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.15, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.56 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.15, %1520), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:702:0
  %1522 : int[] = prim::ListConstruct(%1482, %27, %1481, %1519), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1523 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.56, %1522), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1523, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:706:0
  %1525 : Long() = aten::mul(%bsz.15, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %1526 : int = aten::Int(%1525), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %1527 : int[] = prim::ListConstruct(%1526, %1481, %1519), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %input.156 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.57, %1527), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:707:0
  %input.157 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.156, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.58 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.157, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # torch/nn/functional.py:973:0
  %attn_output.15 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.58, %v.15), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:730:0
  %1532 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.15, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1533 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1532, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1534 : int[] = prim::ListConstruct(%1481, %1482, %1484), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1533, %1534), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn # transformers/modeling_bart.py:732:0
  %1536 : Tensor = prim::GetAttr[name="bias"](%1477)
  %1537 : Tensor = prim::GetAttr[name="weight"](%1477)
  %1538 : Float(1024:1, 1024:1024) = aten::t(%1537), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.86 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.158, %1538), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.86, %1536, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn/__module.model.decoder.layers.1.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.32 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.159, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.160 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.15, %x.32, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:427:0
  %1543 : Tensor = prim::GetAttr[name="bias"](%1475)
  %1544 : Tensor = prim::GetAttr[name="weight"](%1475)
  %1545 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm
  %query.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.160, %1545, %1544, %1543, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1547 : __torch__.torch.nn.modules.linear.___torch_mangle_542.Linear = prim::GetAttr[name="out_proj"](%1474)
  %1548 : __torch__.torch.nn.modules.linear.___torch_mangle_540.Linear = prim::GetAttr[name="v_proj"](%1474)
  %1549 : __torch__.torch.nn.modules.linear.___torch_mangle_539.Linear = prim::GetAttr[name="k_proj"](%1474)
  %1550 : __torch__.torch.nn.modules.linear.___torch_mangle_541.Linear = prim::GetAttr[name="q_proj"](%1474)
  %1551 : int = aten::size(%query.16, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1552 : int = aten::size(%query.16, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.16 : Long() = prim::NumToTensor(%1552), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1554 : int = aten::size(%query.16, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:660:0
  %1555 : Tensor = prim::GetAttr[name="bias"](%1550)
  %1556 : Tensor = prim::GetAttr[name="weight"](%1550)
  %1557 : Float(1024:1, 1024:1024) = aten::t(%1556), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.87 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.16, %1557), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1559 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.87, %1555, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.91 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1559, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:673:0
  %1561 : Tensor = prim::GetAttr[name="bias"](%1549)
  %1562 : Tensor = prim::GetAttr[name="weight"](%1549)
  %1563 : Float(1024:1, 1024:1024) = aten::t(%1562), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.88 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1563), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.93 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.88, %1561, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1566 : Tensor = prim::GetAttr[name="bias"](%1548)
  %1567 : Tensor = prim::GetAttr[name="weight"](%1548)
  %1568 : Float(1024:1, 1024:1024) = aten::t(%1567), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.89 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1568), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.95 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.89, %1566, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.92 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.91, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1572 : Long() = aten::mul(%bsz.16, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1573 : int = aten::Int(%1572), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1574 : int[] = prim::ListConstruct(%1551, %1573, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1575 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.92, %1574), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %q.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1575, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.94 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.93, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1578 : Long() = aten::mul(%bsz.16, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1579 : int = aten::Int(%1578), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1580 : int[] = prim::ListConstruct(%36, %1579, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1581 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.94, %1580), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %k.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1581, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.96 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.95, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1584 : Long() = aten::mul(%bsz.16, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1585 : int = aten::Int(%1584), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1586 : int[] = prim::ListConstruct(%36, %1585, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1587 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.96, %1586), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %v.16 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1587, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:647:0
  %1589 : int = aten::size(%k.16, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:701:0
  %1590 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.16, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.59 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.16, %1590), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:702:0
  %1592 : int[] = prim::ListConstruct(%1552, %27, %1551, %1589), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %attn_weights.60 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.59, %1592), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:718:0
  %1594 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.14 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1594, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.61 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.60, %reshaped.14, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:720:0
  %1597 : Long() = aten::mul(%bsz.16, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %1598 : int = aten::Int(%1597), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %1599 : int[] = prim::ListConstruct(%1598, %1551, %1589), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %input.161 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.61, %1599), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:721:0
  %input.162 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.161, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.62 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.162, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.16 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.62, %v.16), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:730:0
  %1604 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.16, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1605 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1604, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1606 : int[] = prim::ListConstruct(%1551, %1552, %1554), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1605, %1606), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn # transformers/modeling_bart.py:732:0
  %1608 : Tensor = prim::GetAttr[name="bias"](%1547)
  %1609 : Tensor = prim::GetAttr[name="weight"](%1547)
  %1610 : Float(1024:1, 1024:1024) = aten::t(%1609), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.90 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.163, %1610), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.90, %1608, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn/__module.model.decoder.layers.1.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.164, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.165 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.16, %x.33, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:443:0
  %1615 : Tensor = prim::GetAttr[name="bias"](%1473)
  %1616 : Tensor = prim::GetAttr[name="weight"](%1473)
  %1617 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm
  %input.166 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.165, %1617, %1616, %1615, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1619 : Tensor = prim::GetAttr[name="bias"](%1472)
  %1620 : Tensor = prim::GetAttr[name="weight"](%1472)
  %1621 : Float(1024:1, 4096:1024) = aten::t(%1620), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %output.91 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.166, %1621), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1676:0
  %input.167 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.91, %1619, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc1 # torch/nn/functional.py:1678:0
  %input.168 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.167), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:1369:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.168, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %1626 : Tensor = prim::GetAttr[name="bias"](%1471)
  %1627 : Tensor = prim::GetAttr[name="weight"](%1471)
  %1628 : Float(4096:1, 1024:4096) = aten::t(%1627), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %output.92 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.169, %1628), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1676:0
  %input.170 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.92, %1626, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.fc2 # torch/nn/functional.py:1678:0
  %x.34 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.170, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # torch/nn/functional.py:973:0
  %input.171 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.166, %x.34, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1 # transformers/modeling_bart.py:455:0
  %1633 : Tensor = prim::GetAttr[name="bias"](%1470)
  %1634 : Tensor = prim::GetAttr[name="weight"](%1470)
  %1635 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm
  %query.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.171, %1635, %1634, %1633, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.1/__module.model.decoder.layers.1.final_layer_norm # torch/nn/functional.py:2048:0
  %1637 : __torch__.torch.nn.modules.normalization.___torch_mangle_563.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1280)
  %1638 : __torch__.torch.nn.modules.linear.___torch_mangle_562.Linear = prim::GetAttr[name="fc2"](%1280)
  %1639 : __torch__.torch.nn.modules.linear.___torch_mangle_561.Linear = prim::GetAttr[name="fc1"](%1280)
  %1640 : __torch__.torch.nn.modules.normalization.___torch_mangle_560.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1280)
  %1641 : __torch__.transformers.modeling_bart.___torch_mangle_559.Attention = prim::GetAttr[name="encoder_attn"](%1280)
  %1642 : __torch__.torch.nn.modules.normalization.___torch_mangle_554.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1280)
  %1643 : __torch__.transformers.modeling_bart.___torch_mangle_553.Attention = prim::GetAttr[name="self_attn"](%1280)
  %1644 : __torch__.torch.nn.modules.linear.___torch_mangle_552.Linear = prim::GetAttr[name="out_proj"](%1643)
  %1645 : __torch__.torch.nn.modules.linear.___torch_mangle_550.Linear = prim::GetAttr[name="v_proj"](%1643)
  %1646 : __torch__.torch.nn.modules.linear.___torch_mangle_549.Linear = prim::GetAttr[name="k_proj"](%1643)
  %1647 : __torch__.torch.nn.modules.linear.___torch_mangle_551.Linear = prim::GetAttr[name="q_proj"](%1643)
  %1648 : int = aten::size(%query.17, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1649 : int = aten::size(%query.17, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %bsz.17 : Long() = prim::NumToTensor(%1649), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1651 : int = aten::size(%query.17, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:660:0
  %1652 : Tensor = prim::GetAttr[name="bias"](%1647)
  %1653 : Tensor = prim::GetAttr[name="weight"](%1647)
  %1654 : Float(1024:1, 1024:1024) = aten::t(%1653), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.93 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1654), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1656 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.93, %1652, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.97 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1656, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:673:0
  %1658 : Tensor = prim::GetAttr[name="bias"](%1646)
  %1659 : Tensor = prim::GetAttr[name="weight"](%1646)
  %1660 : Float(1024:1, 1024:1024) = aten::t(%1659), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.94 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1660), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.99 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.94, %1658, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1663 : Tensor = prim::GetAttr[name="bias"](%1645)
  %1664 : Tensor = prim::GetAttr[name="weight"](%1645)
  %1665 : Float(1024:1, 1024:1024) = aten::t(%1664), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.95 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.17, %1665), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.101 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.95, %1663, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.98 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.97, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1669 : Long() = aten::mul(%bsz.17, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1670 : int = aten::Int(%1669), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1671 : int[] = prim::ListConstruct(%1648, %1670, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1672 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.98, %1671), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %q.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1672, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.100 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.99, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1675 : Long() = aten::mul(%bsz.17, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1676 : int = aten::Int(%1675), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1677 : int[] = prim::ListConstruct(%36, %1676, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1678 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.100, %1677), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %k.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1678, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %tensor.102 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.101, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1681 : Long() = aten::mul(%bsz.17, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1682 : int = aten::Int(%1681), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1683 : int[] = prim::ListConstruct(%36, %1682, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1684 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.102, %1683), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %v.17 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1684, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:647:0
  %1686 : int = aten::size(%k.17, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:701:0
  %1687 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.17, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.63 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.17, %1687), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:702:0
  %1689 : int[] = prim::ListConstruct(%1649, %27, %1648, %1686), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1690 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.63, %1689), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.64 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1690, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:706:0
  %1692 : Long() = aten::mul(%bsz.17, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %1693 : int = aten::Int(%1692), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %1694 : int[] = prim::ListConstruct(%1693, %1648, %1686), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %input.172 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.64, %1694), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:707:0
  %input.173 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.172, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.65 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.173, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # torch/nn/functional.py:973:0
  %attn_output.17 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.65, %v.17), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:730:0
  %1699 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.17, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1700 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1699, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1701 : int[] = prim::ListConstruct(%1648, %1649, %1651), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn
  %input.174 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1700, %1701), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn # transformers/modeling_bart.py:732:0
  %1703 : Tensor = prim::GetAttr[name="bias"](%1644)
  %1704 : Tensor = prim::GetAttr[name="weight"](%1644)
  %1705 : Float(1024:1, 1024:1024) = aten::t(%1704), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.96 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.174, %1705), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.175 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.96, %1703, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn/__module.model.decoder.layers.2.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.35 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.175, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.17, %x.35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:427:0
  %1710 : Tensor = prim::GetAttr[name="bias"](%1642)
  %1711 : Tensor = prim::GetAttr[name="weight"](%1642)
  %1712 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm
  %query.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.176, %1712, %1711, %1710, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1714 : __torch__.torch.nn.modules.linear.___torch_mangle_558.Linear = prim::GetAttr[name="out_proj"](%1641)
  %1715 : __torch__.torch.nn.modules.linear.___torch_mangle_556.Linear = prim::GetAttr[name="v_proj"](%1641)
  %1716 : __torch__.torch.nn.modules.linear.___torch_mangle_555.Linear = prim::GetAttr[name="k_proj"](%1641)
  %1717 : __torch__.torch.nn.modules.linear.___torch_mangle_557.Linear = prim::GetAttr[name="q_proj"](%1641)
  %1718 : int = aten::size(%query.18, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1719 : int = aten::size(%query.18, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.18 : Long() = prim::NumToTensor(%1719), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1721 : int = aten::size(%query.18, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:660:0
  %1722 : Tensor = prim::GetAttr[name="bias"](%1717)
  %1723 : Tensor = prim::GetAttr[name="weight"](%1717)
  %1724 : Float(1024:1, 1024:1024) = aten::t(%1723), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.97 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.18, %1724), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1726 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.97, %1722, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.103 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1726, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:673:0
  %1728 : Tensor = prim::GetAttr[name="bias"](%1716)
  %1729 : Tensor = prim::GetAttr[name="weight"](%1716)
  %1730 : Float(1024:1, 1024:1024) = aten::t(%1729), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.98 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1730), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.105 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.98, %1728, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1733 : Tensor = prim::GetAttr[name="bias"](%1715)
  %1734 : Tensor = prim::GetAttr[name="weight"](%1715)
  %1735 : Float(1024:1, 1024:1024) = aten::t(%1734), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.99 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1735), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.107 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.99, %1733, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.104 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.103, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1739 : Long() = aten::mul(%bsz.18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1740 : int = aten::Int(%1739), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1741 : int[] = prim::ListConstruct(%1718, %1740, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1742 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.104, %1741), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %q.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1742, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.106 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.105, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1745 : Long() = aten::mul(%bsz.18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1746 : int = aten::Int(%1745), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1747 : int[] = prim::ListConstruct(%36, %1746, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1748 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.106, %1747), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %k.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1748, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.108 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.107, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1751 : Long() = aten::mul(%bsz.18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1752 : int = aten::Int(%1751), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1753 : int[] = prim::ListConstruct(%36, %1752, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1754 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.108, %1753), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %v.18 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1754, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:647:0
  %1756 : int = aten::size(%k.18, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:701:0
  %1757 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.18, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.66 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.18, %1757), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:702:0
  %1759 : int[] = prim::ListConstruct(%1719, %27, %1718, %1756), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %attn_weights.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.66, %1759), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:718:0
  %1761 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.15 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1761, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.68 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.67, %reshaped.15, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:720:0
  %1764 : Long() = aten::mul(%bsz.18, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %1765 : int = aten::Int(%1764), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %1766 : int[] = prim::ListConstruct(%1765, %1718, %1756), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %input.177 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.68, %1766), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:721:0
  %input.178 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.177, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.69 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.178, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.18 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.69, %v.18), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:730:0
  %1771 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.18, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1772 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1771, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1773 : int[] = prim::ListConstruct(%1718, %1719, %1721), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn
  %input.179 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1772, %1773), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn # transformers/modeling_bart.py:732:0
  %1775 : Tensor = prim::GetAttr[name="bias"](%1714)
  %1776 : Tensor = prim::GetAttr[name="weight"](%1714)
  %1777 : Float(1024:1, 1024:1024) = aten::t(%1776), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.100 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.179, %1777), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.180 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.100, %1775, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn/__module.model.decoder.layers.2.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.180, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.18, %x.36, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:443:0
  %1782 : Tensor = prim::GetAttr[name="bias"](%1640)
  %1783 : Tensor = prim::GetAttr[name="weight"](%1640)
  %1784 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.181, %1784, %1783, %1782, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1786 : Tensor = prim::GetAttr[name="bias"](%1639)
  %1787 : Tensor = prim::GetAttr[name="weight"](%1639)
  %1788 : Float(1024:1, 4096:1024) = aten::t(%1787), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %output.101 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.182, %1788), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1676:0
  %input.183 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.101, %1786, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc1 # torch/nn/functional.py:1678:0
  %input.184 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.183), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:1369:0
  %input.185 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.184, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %1793 : Tensor = prim::GetAttr[name="bias"](%1638)
  %1794 : Tensor = prim::GetAttr[name="weight"](%1638)
  %1795 : Float(4096:1, 1024:4096) = aten::t(%1794), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %output.102 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.185, %1795), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1676:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.102, %1793, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.fc2 # torch/nn/functional.py:1678:0
  %x.37 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.186, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # torch/nn/functional.py:973:0
  %input.187 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.182, %x.37, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2 # transformers/modeling_bart.py:455:0
  %1800 : Tensor = prim::GetAttr[name="bias"](%1637)
  %1801 : Tensor = prim::GetAttr[name="weight"](%1637)
  %1802 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm
  %query.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.187, %1802, %1801, %1800, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.2/__module.model.decoder.layers.2.final_layer_norm # torch/nn/functional.py:2048:0
  %1804 : __torch__.torch.nn.modules.normalization.___torch_mangle_579.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1278)
  %1805 : __torch__.torch.nn.modules.linear.___torch_mangle_578.Linear = prim::GetAttr[name="fc2"](%1278)
  %1806 : __torch__.torch.nn.modules.linear.___torch_mangle_577.Linear = prim::GetAttr[name="fc1"](%1278)
  %1807 : __torch__.torch.nn.modules.normalization.___torch_mangle_576.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1278)
  %1808 : __torch__.transformers.modeling_bart.___torch_mangle_575.Attention = prim::GetAttr[name="encoder_attn"](%1278)
  %1809 : __torch__.torch.nn.modules.normalization.___torch_mangle_570.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1278)
  %1810 : __torch__.transformers.modeling_bart.___torch_mangle_569.Attention = prim::GetAttr[name="self_attn"](%1278)
  %1811 : __torch__.torch.nn.modules.linear.___torch_mangle_568.Linear = prim::GetAttr[name="out_proj"](%1810)
  %1812 : __torch__.torch.nn.modules.linear.___torch_mangle_566.Linear = prim::GetAttr[name="v_proj"](%1810)
  %1813 : __torch__.torch.nn.modules.linear.___torch_mangle_565.Linear = prim::GetAttr[name="k_proj"](%1810)
  %1814 : __torch__.torch.nn.modules.linear.___torch_mangle_567.Linear = prim::GetAttr[name="q_proj"](%1810)
  %1815 : int = aten::size(%query.19, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1816 : int = aten::size(%query.19, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %bsz.19 : Long() = prim::NumToTensor(%1816), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1818 : int = aten::size(%query.19, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:660:0
  %1819 : Tensor = prim::GetAttr[name="bias"](%1814)
  %1820 : Tensor = prim::GetAttr[name="weight"](%1814)
  %1821 : Float(1024:1, 1024:1024) = aten::t(%1820), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.103 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1821), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1823 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.103, %1819, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.109 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1823, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:673:0
  %1825 : Tensor = prim::GetAttr[name="bias"](%1813)
  %1826 : Tensor = prim::GetAttr[name="weight"](%1813)
  %1827 : Float(1024:1, 1024:1024) = aten::t(%1826), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.104 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1827), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.111 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.104, %1825, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1830 : Tensor = prim::GetAttr[name="bias"](%1812)
  %1831 : Tensor = prim::GetAttr[name="weight"](%1812)
  %1832 : Float(1024:1, 1024:1024) = aten::t(%1831), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.105 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.19, %1832), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.113 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.105, %1830, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.110 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.109, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1836 : Long() = aten::mul(%bsz.19, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1837 : int = aten::Int(%1836), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1838 : int[] = prim::ListConstruct(%1815, %1837, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1839 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.110, %1838), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %q.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1839, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.112 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.111, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1842 : Long() = aten::mul(%bsz.19, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1843 : int = aten::Int(%1842), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1844 : int[] = prim::ListConstruct(%36, %1843, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1845 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.112, %1844), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %k.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1845, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %tensor.114 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.113, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1848 : Long() = aten::mul(%bsz.19, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1849 : int = aten::Int(%1848), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1850 : int[] = prim::ListConstruct(%36, %1849, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1851 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.114, %1850), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %v.19 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1851, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:647:0
  %1853 : int = aten::size(%k.19, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:701:0
  %1854 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.19, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.70 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.19, %1854), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:702:0
  %1856 : int[] = prim::ListConstruct(%1816, %27, %1815, %1853), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1857 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.70, %1856), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.71 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1857, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:706:0
  %1859 : Long() = aten::mul(%bsz.19, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %1860 : int = aten::Int(%1859), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %1861 : int[] = prim::ListConstruct(%1860, %1815, %1853), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %input.188 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.71, %1861), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:707:0
  %input.189 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.188, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.72 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.189, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # torch/nn/functional.py:973:0
  %attn_output.19 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.72, %v.19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:730:0
  %1866 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.19, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1867 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1866, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1868 : int[] = prim::ListConstruct(%1815, %1816, %1818), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1867, %1868), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn # transformers/modeling_bart.py:732:0
  %1870 : Tensor = prim::GetAttr[name="bias"](%1811)
  %1871 : Tensor = prim::GetAttr[name="weight"](%1811)
  %1872 : Float(1024:1, 1024:1024) = aten::t(%1871), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.106 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.190, %1872), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.106, %1870, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn/__module.model.decoder.layers.3.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.38 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.191, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.192 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.19, %x.38, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:427:0
  %1877 : Tensor = prim::GetAttr[name="bias"](%1809)
  %1878 : Tensor = prim::GetAttr[name="weight"](%1809)
  %1879 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm
  %query.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.192, %1879, %1878, %1877, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %1881 : __torch__.torch.nn.modules.linear.___torch_mangle_574.Linear = prim::GetAttr[name="out_proj"](%1808)
  %1882 : __torch__.torch.nn.modules.linear.___torch_mangle_572.Linear = prim::GetAttr[name="v_proj"](%1808)
  %1883 : __torch__.torch.nn.modules.linear.___torch_mangle_571.Linear = prim::GetAttr[name="k_proj"](%1808)
  %1884 : __torch__.torch.nn.modules.linear.___torch_mangle_573.Linear = prim::GetAttr[name="q_proj"](%1808)
  %1885 : int = aten::size(%query.20, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1886 : int = aten::size(%query.20, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.20 : Long() = prim::NumToTensor(%1886), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1888 : int = aten::size(%query.20, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:660:0
  %1889 : Tensor = prim::GetAttr[name="bias"](%1884)
  %1890 : Tensor = prim::GetAttr[name="weight"](%1884)
  %1891 : Float(1024:1, 1024:1024) = aten::t(%1890), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.107 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.20, %1891), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %1893 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.107, %1889, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.115 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1893, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:673:0
  %1895 : Tensor = prim::GetAttr[name="bias"](%1883)
  %1896 : Tensor = prim::GetAttr[name="weight"](%1883)
  %1897 : Float(1024:1, 1024:1024) = aten::t(%1896), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.108 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1897), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.117 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.108, %1895, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %1900 : Tensor = prim::GetAttr[name="bias"](%1882)
  %1901 : Tensor = prim::GetAttr[name="weight"](%1882)
  %1902 : Float(1024:1, 1024:1024) = aten::t(%1901), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.109 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %1902), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.119 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.109, %1900, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.116 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.115, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1906 : Long() = aten::mul(%bsz.20, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1907 : int = aten::Int(%1906), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1908 : int[] = prim::ListConstruct(%1885, %1907, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1909 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.116, %1908), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %q.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1909, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.118 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.117, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1912 : Long() = aten::mul(%bsz.20, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1913 : int = aten::Int(%1912), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1914 : int[] = prim::ListConstruct(%36, %1913, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1915 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.118, %1914), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %k.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1915, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.120 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.119, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1918 : Long() = aten::mul(%bsz.20, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1919 : int = aten::Int(%1918), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1920 : int[] = prim::ListConstruct(%36, %1919, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1921 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.120, %1920), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %v.20 : Float(272:64, 13:17408, 64:1) = aten::transpose(%1921, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:647:0
  %1923 : int = aten::size(%k.20, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:701:0
  %1924 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.20, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.73 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.20, %1924), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:702:0
  %1926 : int[] = prim::ListConstruct(%1886, %27, %1885, %1923), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %attn_weights.74 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.73, %1926), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:718:0
  %1928 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.16 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%1928, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.74, %reshaped.16, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:720:0
  %1931 : Long() = aten::mul(%bsz.20, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %1932 : int = aten::Int(%1931), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %1933 : int[] = prim::ListConstruct(%1932, %1885, %1923), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %input.193 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.75, %1933), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:721:0
  %input.194 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.193, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.76 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.194, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.20 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.76, %v.20), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:730:0
  %1938 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.20, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1939 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%1938, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1940 : int[] = prim::ListConstruct(%1885, %1886, %1888), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::view(%1939, %1940), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn # transformers/modeling_bart.py:732:0
  %1942 : Tensor = prim::GetAttr[name="bias"](%1881)
  %1943 : Tensor = prim::GetAttr[name="weight"](%1881)
  %1944 : Float(1024:1, 1024:1024) = aten::t(%1943), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.110 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.195, %1944), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.196 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.110, %1942, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn/__module.model.decoder.layers.3.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.196, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.197 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.20, %x.39, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:443:0
  %1949 : Tensor = prim::GetAttr[name="bias"](%1807)
  %1950 : Tensor = prim::GetAttr[name="weight"](%1807)
  %1951 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm
  %input.198 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.197, %1951, %1950, %1949, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %1953 : Tensor = prim::GetAttr[name="bias"](%1806)
  %1954 : Tensor = prim::GetAttr[name="weight"](%1806)
  %1955 : Float(1024:1, 4096:1024) = aten::t(%1954), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %output.111 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.198, %1955), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.111, %1953, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc1 # torch/nn/functional.py:1678:0
  %input.200 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.199), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:1369:0
  %input.201 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.200, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %1960 : Tensor = prim::GetAttr[name="bias"](%1805)
  %1961 : Tensor = prim::GetAttr[name="weight"](%1805)
  %1962 : Float(4096:1, 1024:4096) = aten::t(%1961), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %output.112 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.201, %1962), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1676:0
  %input.202 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.112, %1960, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.fc2 # torch/nn/functional.py:1678:0
  %x.40 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.202, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # torch/nn/functional.py:973:0
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.198, %x.40, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3 # transformers/modeling_bart.py:455:0
  %1967 : Tensor = prim::GetAttr[name="bias"](%1804)
  %1968 : Tensor = prim::GetAttr[name="weight"](%1804)
  %1969 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm
  %query.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.203, %1969, %1968, %1967, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.3/__module.model.decoder.layers.3.final_layer_norm # torch/nn/functional.py:2048:0
  %1971 : __torch__.torch.nn.modules.normalization.___torch_mangle_595.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1276)
  %1972 : __torch__.torch.nn.modules.linear.___torch_mangle_594.Linear = prim::GetAttr[name="fc2"](%1276)
  %1973 : __torch__.torch.nn.modules.linear.___torch_mangle_593.Linear = prim::GetAttr[name="fc1"](%1276)
  %1974 : __torch__.torch.nn.modules.normalization.___torch_mangle_592.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1276)
  %1975 : __torch__.transformers.modeling_bart.___torch_mangle_591.Attention = prim::GetAttr[name="encoder_attn"](%1276)
  %1976 : __torch__.torch.nn.modules.normalization.___torch_mangle_586.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1276)
  %1977 : __torch__.transformers.modeling_bart.___torch_mangle_585.Attention = prim::GetAttr[name="self_attn"](%1276)
  %1978 : __torch__.torch.nn.modules.linear.___torch_mangle_584.Linear = prim::GetAttr[name="out_proj"](%1977)
  %1979 : __torch__.torch.nn.modules.linear.___torch_mangle_582.Linear = prim::GetAttr[name="v_proj"](%1977)
  %1980 : __torch__.torch.nn.modules.linear.___torch_mangle_581.Linear = prim::GetAttr[name="k_proj"](%1977)
  %1981 : __torch__.torch.nn.modules.linear.___torch_mangle_583.Linear = prim::GetAttr[name="q_proj"](%1977)
  %1982 : int = aten::size(%query.21, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %1983 : int = aten::size(%query.21, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %bsz.21 : Long() = prim::NumToTensor(%1983), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %1985 : int = aten::size(%query.21, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:660:0
  %1986 : Tensor = prim::GetAttr[name="bias"](%1981)
  %1987 : Tensor = prim::GetAttr[name="weight"](%1981)
  %1988 : Float(1024:1, 1024:1024) = aten::t(%1987), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.113 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %1988), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1676:0
  %1990 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.113, %1986, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.121 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%1990, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:673:0
  %1992 : Tensor = prim::GetAttr[name="bias"](%1980)
  %1993 : Tensor = prim::GetAttr[name="weight"](%1980)
  %1994 : Float(1024:1, 1024:1024) = aten::t(%1993), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.114 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %1994), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.123 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.114, %1992, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.k_proj # torch/nn/functional.py:1678:0
  %1997 : Tensor = prim::GetAttr[name="bias"](%1979)
  %1998 : Tensor = prim::GetAttr[name="weight"](%1979)
  %1999 : Float(1024:1, 1024:1024) = aten::t(%1998), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.115 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.21, %1999), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.125 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.115, %1997, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.122 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.121, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2003 : Long() = aten::mul(%bsz.21, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2004 : int = aten::Int(%2003), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2005 : int[] = prim::ListConstruct(%1982, %2004, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2006 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.122, %2005), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %q.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2006, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.124 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.123, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2009 : Long() = aten::mul(%bsz.21, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2010 : int = aten::Int(%2009), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2011 : int[] = prim::ListConstruct(%36, %2010, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2012 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.124, %2011), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %k.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2012, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %tensor.126 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.125, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2015 : Long() = aten::mul(%bsz.21, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2016 : int = aten::Int(%2015), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2017 : int[] = prim::ListConstruct(%36, %2016, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2018 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.126, %2017), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %v.21 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2018, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:647:0
  %2020 : int = aten::size(%k.21, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:701:0
  %2021 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.21, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.77 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.21, %2021), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:702:0
  %2023 : int[] = prim::ListConstruct(%1983, %27, %1982, %2020), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2024 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.77, %2023), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.78 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2024, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:706:0
  %2026 : Long() = aten::mul(%bsz.21, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %2027 : int = aten::Int(%2026), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %2028 : int[] = prim::ListConstruct(%2027, %1982, %2020), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %input.204 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.78, %2028), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:707:0
  %input.205 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.204, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.79 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.205, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # torch/nn/functional.py:973:0
  %attn_output.21 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.79, %v.21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:730:0
  %2033 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.21, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2034 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2033, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2035 : int[] = prim::ListConstruct(%1982, %1983, %1985), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn
  %input.206 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2034, %2035), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn # transformers/modeling_bart.py:732:0
  %2037 : Tensor = prim::GetAttr[name="bias"](%1978)
  %2038 : Tensor = prim::GetAttr[name="weight"](%1978)
  %2039 : Float(1024:1, 1024:1024) = aten::t(%2038), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.116 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.206, %2039), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.116, %2037, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn/__module.model.decoder.layers.4.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.41 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.207, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.21, %x.41, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:427:0
  %2044 : Tensor = prim::GetAttr[name="bias"](%1976)
  %2045 : Tensor = prim::GetAttr[name="weight"](%1976)
  %2046 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm
  %query.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.208, %2046, %2045, %2044, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2048 : __torch__.torch.nn.modules.linear.___torch_mangle_590.Linear = prim::GetAttr[name="out_proj"](%1975)
  %2049 : __torch__.torch.nn.modules.linear.___torch_mangle_588.Linear = prim::GetAttr[name="v_proj"](%1975)
  %2050 : __torch__.torch.nn.modules.linear.___torch_mangle_587.Linear = prim::GetAttr[name="k_proj"](%1975)
  %2051 : __torch__.torch.nn.modules.linear.___torch_mangle_589.Linear = prim::GetAttr[name="q_proj"](%1975)
  %2052 : int = aten::size(%query.22, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2053 : int = aten::size(%query.22, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.22 : Long() = prim::NumToTensor(%2053), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2055 : int = aten::size(%query.22, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:660:0
  %2056 : Tensor = prim::GetAttr[name="bias"](%2051)
  %2057 : Tensor = prim::GetAttr[name="weight"](%2051)
  %2058 : Float(1024:1, 1024:1024) = aten::t(%2057), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.117 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.22, %2058), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2060 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.117, %2056, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.127 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2060, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:673:0
  %2062 : Tensor = prim::GetAttr[name="bias"](%2050)
  %2063 : Tensor = prim::GetAttr[name="weight"](%2050)
  %2064 : Float(1024:1, 1024:1024) = aten::t(%2063), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.118 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2064), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.129 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.118, %2062, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2067 : Tensor = prim::GetAttr[name="bias"](%2049)
  %2068 : Tensor = prim::GetAttr[name="weight"](%2049)
  %2069 : Float(1024:1, 1024:1024) = aten::t(%2068), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.119 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2069), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.131 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.119, %2067, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.128 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.127, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2073 : Long() = aten::mul(%bsz.22, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2074 : int = aten::Int(%2073), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2075 : int[] = prim::ListConstruct(%2052, %2074, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2076 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.128, %2075), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %q.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2076, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.130 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.129, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2079 : Long() = aten::mul(%bsz.22, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2080 : int = aten::Int(%2079), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2081 : int[] = prim::ListConstruct(%36, %2080, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2082 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.130, %2081), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %k.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2082, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.132 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.131, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2085 : Long() = aten::mul(%bsz.22, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2086 : int = aten::Int(%2085), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2087 : int[] = prim::ListConstruct(%36, %2086, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2088 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.132, %2087), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %v.22 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2088, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:647:0
  %2090 : int = aten::size(%k.22, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:701:0
  %2091 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.22, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.80 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.22, %2091), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:702:0
  %2093 : int[] = prim::ListConstruct(%2053, %27, %2052, %2090), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %attn_weights.81 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.80, %2093), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:718:0
  %2095 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.17 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2095, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.82 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.81, %reshaped.17, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:720:0
  %2098 : Long() = aten::mul(%bsz.22, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %2099 : int = aten::Int(%2098), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %2100 : int[] = prim::ListConstruct(%2099, %2052, %2090), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %input.209 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.82, %2100), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:721:0
  %input.210 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.209, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.83 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.210, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.22 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.83, %v.22), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:730:0
  %2105 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.22, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2106 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2105, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2107 : int[] = prim::ListConstruct(%2052, %2053, %2055), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn
  %input.211 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2106, %2107), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn # transformers/modeling_bart.py:732:0
  %2109 : Tensor = prim::GetAttr[name="bias"](%2048)
  %2110 : Tensor = prim::GetAttr[name="weight"](%2048)
  %2111 : Float(1024:1, 1024:1024) = aten::t(%2110), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.120 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.211, %2111), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.120, %2109, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn/__module.model.decoder.layers.4.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.22, %x.42, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:443:0
  %2116 : Tensor = prim::GetAttr[name="bias"](%1974)
  %2117 : Tensor = prim::GetAttr[name="weight"](%1974)
  %2118 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm
  %input.214 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %2118, %2117, %2116, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2120 : Tensor = prim::GetAttr[name="bias"](%1973)
  %2121 : Tensor = prim::GetAttr[name="weight"](%1973)
  %2122 : Float(1024:1, 4096:1024) = aten::t(%2121), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %output.121 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.214, %2122), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1676:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.121, %2120, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc1 # torch/nn/functional.py:1678:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.215), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:1369:0
  %input.217 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.216, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %2127 : Tensor = prim::GetAttr[name="bias"](%1972)
  %2128 : Tensor = prim::GetAttr[name="weight"](%1972)
  %2129 : Float(4096:1, 1024:4096) = aten::t(%2128), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %output.122 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.217, %2129), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1676:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.122, %2127, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.fc2 # torch/nn/functional.py:1678:0
  %x.43 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.218, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # torch/nn/functional.py:973:0
  %input.219 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.214, %x.43, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4 # transformers/modeling_bart.py:455:0
  %2134 : Tensor = prim::GetAttr[name="bias"](%1971)
  %2135 : Tensor = prim::GetAttr[name="weight"](%1971)
  %2136 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm
  %query.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.219, %2136, %2135, %2134, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.4/__module.model.decoder.layers.4.final_layer_norm # torch/nn/functional.py:2048:0
  %2138 : __torch__.torch.nn.modules.normalization.___torch_mangle_611.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1274)
  %2139 : __torch__.torch.nn.modules.linear.___torch_mangle_610.Linear = prim::GetAttr[name="fc2"](%1274)
  %2140 : __torch__.torch.nn.modules.linear.___torch_mangle_609.Linear = prim::GetAttr[name="fc1"](%1274)
  %2141 : __torch__.torch.nn.modules.normalization.___torch_mangle_608.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1274)
  %2142 : __torch__.transformers.modeling_bart.___torch_mangle_607.Attention = prim::GetAttr[name="encoder_attn"](%1274)
  %2143 : __torch__.torch.nn.modules.normalization.___torch_mangle_602.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1274)
  %2144 : __torch__.transformers.modeling_bart.___torch_mangle_601.Attention = prim::GetAttr[name="self_attn"](%1274)
  %2145 : __torch__.torch.nn.modules.linear.___torch_mangle_600.Linear = prim::GetAttr[name="out_proj"](%2144)
  %2146 : __torch__.torch.nn.modules.linear.___torch_mangle_598.Linear = prim::GetAttr[name="v_proj"](%2144)
  %2147 : __torch__.torch.nn.modules.linear.___torch_mangle_597.Linear = prim::GetAttr[name="k_proj"](%2144)
  %2148 : __torch__.torch.nn.modules.linear.___torch_mangle_599.Linear = prim::GetAttr[name="q_proj"](%2144)
  %2149 : int = aten::size(%query.23, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2150 : int = aten::size(%query.23, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %bsz.23 : Long() = prim::NumToTensor(%2150), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2152 : int = aten::size(%query.23, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:660:0
  %2153 : Tensor = prim::GetAttr[name="bias"](%2148)
  %2154 : Tensor = prim::GetAttr[name="weight"](%2148)
  %2155 : Float(1024:1, 1024:1024) = aten::t(%2154), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.123 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2155), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2157 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.123, %2153, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.133 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2157, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:673:0
  %2159 : Tensor = prim::GetAttr[name="bias"](%2147)
  %2160 : Tensor = prim::GetAttr[name="weight"](%2147)
  %2161 : Float(1024:1, 1024:1024) = aten::t(%2160), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.124 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2161), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.135 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.124, %2159, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2164 : Tensor = prim::GetAttr[name="bias"](%2146)
  %2165 : Tensor = prim::GetAttr[name="weight"](%2146)
  %2166 : Float(1024:1, 1024:1024) = aten::t(%2165), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.125 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.23, %2166), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.137 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.125, %2164, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.134 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.133, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2170 : Long() = aten::mul(%bsz.23, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2171 : int = aten::Int(%2170), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2172 : int[] = prim::ListConstruct(%2149, %2171, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2173 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.134, %2172), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %q.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2173, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.136 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.135, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2176 : Long() = aten::mul(%bsz.23, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2177 : int = aten::Int(%2176), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2178 : int[] = prim::ListConstruct(%36, %2177, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2179 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.136, %2178), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %k.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2179, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %tensor.138 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.137, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2182 : Long() = aten::mul(%bsz.23, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2183 : int = aten::Int(%2182), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2184 : int[] = prim::ListConstruct(%36, %2183, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2185 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.138, %2184), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %v.23 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2185, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:647:0
  %2187 : int = aten::size(%k.23, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:701:0
  %2188 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.23, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.84 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.23, %2188), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:702:0
  %2190 : int[] = prim::ListConstruct(%2150, %27, %2149, %2187), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2191 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.84, %2190), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2191, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:706:0
  %2193 : Long() = aten::mul(%bsz.23, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %2194 : int = aten::Int(%2193), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %2195 : int[] = prim::ListConstruct(%2194, %2149, %2187), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %input.220 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.85, %2195), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:707:0
  %input.221 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.220, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.86 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.221, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # torch/nn/functional.py:973:0
  %attn_output.23 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.86, %v.23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:730:0
  %2200 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.23, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2201 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2200, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2202 : int[] = prim::ListConstruct(%2149, %2150, %2152), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn
  %input.222 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2201, %2202), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn # transformers/modeling_bart.py:732:0
  %2204 : Tensor = prim::GetAttr[name="bias"](%2145)
  %2205 : Tensor = prim::GetAttr[name="weight"](%2145)
  %2206 : Float(1024:1, 1024:1024) = aten::t(%2205), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.126 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.222, %2206), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.223 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.126, %2204, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn/__module.model.decoder.layers.5.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.44 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.223, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.224 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.23, %x.44, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:427:0
  %2211 : Tensor = prim::GetAttr[name="bias"](%2143)
  %2212 : Tensor = prim::GetAttr[name="weight"](%2143)
  %2213 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm
  %query.24 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.224, %2213, %2212, %2211, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2215 : __torch__.torch.nn.modules.linear.___torch_mangle_606.Linear = prim::GetAttr[name="out_proj"](%2142)
  %2216 : __torch__.torch.nn.modules.linear.___torch_mangle_604.Linear = prim::GetAttr[name="v_proj"](%2142)
  %2217 : __torch__.torch.nn.modules.linear.___torch_mangle_603.Linear = prim::GetAttr[name="k_proj"](%2142)
  %2218 : __torch__.torch.nn.modules.linear.___torch_mangle_605.Linear = prim::GetAttr[name="q_proj"](%2142)
  %2219 : int = aten::size(%query.24, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2220 : int = aten::size(%query.24, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.24 : Long() = prim::NumToTensor(%2220), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2222 : int = aten::size(%query.24, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:660:0
  %2223 : Tensor = prim::GetAttr[name="bias"](%2218)
  %2224 : Tensor = prim::GetAttr[name="weight"](%2218)
  %2225 : Float(1024:1, 1024:1024) = aten::t(%2224), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.127 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.24, %2225), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2227 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.127, %2223, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.139 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2227, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:673:0
  %2229 : Tensor = prim::GetAttr[name="bias"](%2217)
  %2230 : Tensor = prim::GetAttr[name="weight"](%2217)
  %2231 : Float(1024:1, 1024:1024) = aten::t(%2230), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.128 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2231), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.141 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.128, %2229, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2234 : Tensor = prim::GetAttr[name="bias"](%2216)
  %2235 : Tensor = prim::GetAttr[name="weight"](%2216)
  %2236 : Float(1024:1, 1024:1024) = aten::t(%2235), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.129 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2236), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.143 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.129, %2234, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.140 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.139, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2240 : Long() = aten::mul(%bsz.24, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2241 : int = aten::Int(%2240), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2242 : int[] = prim::ListConstruct(%2219, %2241, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2243 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.140, %2242), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %q.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2243, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.142 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.141, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2246 : Long() = aten::mul(%bsz.24, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2247 : int = aten::Int(%2246), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2248 : int[] = prim::ListConstruct(%36, %2247, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2249 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.142, %2248), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %k.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2249, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.144 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.143, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2252 : Long() = aten::mul(%bsz.24, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2253 : int = aten::Int(%2252), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2254 : int[] = prim::ListConstruct(%36, %2253, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2255 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.144, %2254), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %v.24 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2255, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:647:0
  %2257 : int = aten::size(%k.24, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:701:0
  %2258 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.24, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.87 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.24, %2258), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:702:0
  %2260 : int[] = prim::ListConstruct(%2220, %27, %2219, %2257), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %attn_weights.88 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.87, %2260), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:718:0
  %2262 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.18 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2262, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.89 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.88, %reshaped.18, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:720:0
  %2265 : Long() = aten::mul(%bsz.24, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %2266 : int = aten::Int(%2265), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %2267 : int[] = prim::ListConstruct(%2266, %2219, %2257), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %input.225 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.89, %2267), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:721:0
  %input.226 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.225, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.90 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.226, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.24 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.90, %v.24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:730:0
  %2272 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.24, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2273 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2272, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2274 : int[] = prim::ListConstruct(%2219, %2220, %2222), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn
  %input.227 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2273, %2274), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn # transformers/modeling_bart.py:732:0
  %2276 : Tensor = prim::GetAttr[name="bias"](%2215)
  %2277 : Tensor = prim::GetAttr[name="weight"](%2215)
  %2278 : Float(1024:1, 1024:1024) = aten::t(%2277), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.130 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.227, %2278), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.228 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.130, %2276, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn/__module.model.decoder.layers.5.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.228, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.229 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.24, %x.45, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:443:0
  %2283 : Tensor = prim::GetAttr[name="bias"](%2141)
  %2284 : Tensor = prim::GetAttr[name="weight"](%2141)
  %2285 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm
  %input.230 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.229, %2285, %2284, %2283, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2287 : Tensor = prim::GetAttr[name="bias"](%2140)
  %2288 : Tensor = prim::GetAttr[name="weight"](%2140)
  %2289 : Float(1024:1, 4096:1024) = aten::t(%2288), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %output.131 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.230, %2289), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1676:0
  %input.231 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.131, %2287, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc1 # torch/nn/functional.py:1678:0
  %input.232 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.231), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:1369:0
  %input.233 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.232, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %2294 : Tensor = prim::GetAttr[name="bias"](%2139)
  %2295 : Tensor = prim::GetAttr[name="weight"](%2139)
  %2296 : Float(4096:1, 1024:4096) = aten::t(%2295), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %output.132 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.233, %2296), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1676:0
  %input.234 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.132, %2294, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.fc2 # torch/nn/functional.py:1678:0
  %x.46 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.234, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # torch/nn/functional.py:973:0
  %input.235 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.230, %x.46, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5 # transformers/modeling_bart.py:455:0
  %2301 : Tensor = prim::GetAttr[name="bias"](%2138)
  %2302 : Tensor = prim::GetAttr[name="weight"](%2138)
  %2303 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm
  %query.25 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.235, %2303, %2302, %2301, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.5/__module.model.decoder.layers.5.final_layer_norm # torch/nn/functional.py:2048:0
  %2305 : __torch__.torch.nn.modules.normalization.___torch_mangle_627.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1272)
  %2306 : __torch__.torch.nn.modules.linear.___torch_mangle_626.Linear = prim::GetAttr[name="fc2"](%1272)
  %2307 : __torch__.torch.nn.modules.linear.___torch_mangle_625.Linear = prim::GetAttr[name="fc1"](%1272)
  %2308 : __torch__.torch.nn.modules.normalization.___torch_mangle_624.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1272)
  %2309 : __torch__.transformers.modeling_bart.___torch_mangle_623.Attention = prim::GetAttr[name="encoder_attn"](%1272)
  %2310 : __torch__.torch.nn.modules.normalization.___torch_mangle_618.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1272)
  %2311 : __torch__.transformers.modeling_bart.___torch_mangle_617.Attention = prim::GetAttr[name="self_attn"](%1272)
  %2312 : __torch__.torch.nn.modules.linear.___torch_mangle_616.Linear = prim::GetAttr[name="out_proj"](%2311)
  %2313 : __torch__.torch.nn.modules.linear.___torch_mangle_614.Linear = prim::GetAttr[name="v_proj"](%2311)
  %2314 : __torch__.torch.nn.modules.linear.___torch_mangle_613.Linear = prim::GetAttr[name="k_proj"](%2311)
  %2315 : __torch__.torch.nn.modules.linear.___torch_mangle_615.Linear = prim::GetAttr[name="q_proj"](%2311)
  %2316 : int = aten::size(%query.25, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2317 : int = aten::size(%query.25, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %bsz.25 : Long() = prim::NumToTensor(%2317), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2319 : int = aten::size(%query.25, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:660:0
  %2320 : Tensor = prim::GetAttr[name="bias"](%2315)
  %2321 : Tensor = prim::GetAttr[name="weight"](%2315)
  %2322 : Float(1024:1, 1024:1024) = aten::t(%2321), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.133 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2322), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2324 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.133, %2320, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.145 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2324, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:673:0
  %2326 : Tensor = prim::GetAttr[name="bias"](%2314)
  %2327 : Tensor = prim::GetAttr[name="weight"](%2314)
  %2328 : Float(1024:1, 1024:1024) = aten::t(%2327), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.134 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2328), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.147 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.134, %2326, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2331 : Tensor = prim::GetAttr[name="bias"](%2313)
  %2332 : Tensor = prim::GetAttr[name="weight"](%2313)
  %2333 : Float(1024:1, 1024:1024) = aten::t(%2332), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.135 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.25, %2333), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.149 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.135, %2331, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.146 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.145, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2337 : Long() = aten::mul(%bsz.25, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2338 : int = aten::Int(%2337), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2339 : int[] = prim::ListConstruct(%2316, %2338, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2340 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.146, %2339), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %q.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2340, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.148 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.147, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2343 : Long() = aten::mul(%bsz.25, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2344 : int = aten::Int(%2343), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2345 : int[] = prim::ListConstruct(%36, %2344, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2346 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.148, %2345), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %k.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2346, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %tensor.150 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.149, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2349 : Long() = aten::mul(%bsz.25, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2350 : int = aten::Int(%2349), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2351 : int[] = prim::ListConstruct(%36, %2350, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2352 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.150, %2351), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %v.25 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2352, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:647:0
  %2354 : int = aten::size(%k.25, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:701:0
  %2355 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.25, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.91 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.25, %2355), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:702:0
  %2357 : int[] = prim::ListConstruct(%2317, %27, %2316, %2354), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2358 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.91, %2357), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.92 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2358, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:706:0
  %2360 : Long() = aten::mul(%bsz.25, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %2361 : int = aten::Int(%2360), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %2362 : int[] = prim::ListConstruct(%2361, %2316, %2354), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %input.236 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.92, %2362), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:707:0
  %input.237 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.236, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.93 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.237, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # torch/nn/functional.py:973:0
  %attn_output.25 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.93, %v.25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:730:0
  %2367 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.25, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2368 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2367, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2369 : int[] = prim::ListConstruct(%2316, %2317, %2319), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn
  %input.238 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2368, %2369), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn # transformers/modeling_bart.py:732:0
  %2371 : Tensor = prim::GetAttr[name="bias"](%2312)
  %2372 : Tensor = prim::GetAttr[name="weight"](%2312)
  %2373 : Float(1024:1, 1024:1024) = aten::t(%2372), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.136 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.238, %2373), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.239 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.136, %2371, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn/__module.model.decoder.layers.6.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.47 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.239, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.240 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.25, %x.47, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:427:0
  %2378 : Tensor = prim::GetAttr[name="bias"](%2310)
  %2379 : Tensor = prim::GetAttr[name="weight"](%2310)
  %2380 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm
  %query.26 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.240, %2380, %2379, %2378, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2382 : __torch__.torch.nn.modules.linear.___torch_mangle_622.Linear = prim::GetAttr[name="out_proj"](%2309)
  %2383 : __torch__.torch.nn.modules.linear.___torch_mangle_620.Linear = prim::GetAttr[name="v_proj"](%2309)
  %2384 : __torch__.torch.nn.modules.linear.___torch_mangle_619.Linear = prim::GetAttr[name="k_proj"](%2309)
  %2385 : __torch__.torch.nn.modules.linear.___torch_mangle_621.Linear = prim::GetAttr[name="q_proj"](%2309)
  %2386 : int = aten::size(%query.26, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2387 : int = aten::size(%query.26, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.26 : Long() = prim::NumToTensor(%2387), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2389 : int = aten::size(%query.26, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:660:0
  %2390 : Tensor = prim::GetAttr[name="bias"](%2385)
  %2391 : Tensor = prim::GetAttr[name="weight"](%2385)
  %2392 : Float(1024:1, 1024:1024) = aten::t(%2391), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.137 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.26, %2392), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2394 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.137, %2390, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.151 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2394, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:673:0
  %2396 : Tensor = prim::GetAttr[name="bias"](%2384)
  %2397 : Tensor = prim::GetAttr[name="weight"](%2384)
  %2398 : Float(1024:1, 1024:1024) = aten::t(%2397), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.138 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2398), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.153 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.138, %2396, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2401 : Tensor = prim::GetAttr[name="bias"](%2383)
  %2402 : Tensor = prim::GetAttr[name="weight"](%2383)
  %2403 : Float(1024:1, 1024:1024) = aten::t(%2402), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.139 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2403), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.155 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.139, %2401, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.152 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.151, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2407 : Long() = aten::mul(%bsz.26, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2408 : int = aten::Int(%2407), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2409 : int[] = prim::ListConstruct(%2386, %2408, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2410 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.152, %2409), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %q.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2410, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.154 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.153, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2413 : Long() = aten::mul(%bsz.26, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2414 : int = aten::Int(%2413), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2415 : int[] = prim::ListConstruct(%36, %2414, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2416 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.154, %2415), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %k.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2416, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.156 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.155, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2419 : Long() = aten::mul(%bsz.26, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2420 : int = aten::Int(%2419), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2421 : int[] = prim::ListConstruct(%36, %2420, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2422 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.156, %2421), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %v.26 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2422, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:647:0
  %2424 : int = aten::size(%k.26, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:701:0
  %2425 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.26, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.94 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.26, %2425), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:702:0
  %2427 : int[] = prim::ListConstruct(%2387, %27, %2386, %2424), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %attn_weights.95 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.94, %2427), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:718:0
  %2429 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.19 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2429, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.96 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.95, %reshaped.19, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:720:0
  %2432 : Long() = aten::mul(%bsz.26, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %2433 : int = aten::Int(%2432), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %2434 : int[] = prim::ListConstruct(%2433, %2386, %2424), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %input.241 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.96, %2434), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:721:0
  %input.242 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.241, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.97 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.242, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.26 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.97, %v.26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:730:0
  %2439 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.26, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2440 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2439, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2441 : int[] = prim::ListConstruct(%2386, %2387, %2389), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn
  %input.243 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2440, %2441), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn # transformers/modeling_bart.py:732:0
  %2443 : Tensor = prim::GetAttr[name="bias"](%2382)
  %2444 : Tensor = prim::GetAttr[name="weight"](%2382)
  %2445 : Float(1024:1, 1024:1024) = aten::t(%2444), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.140 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.243, %2445), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.244 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.140, %2443, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn/__module.model.decoder.layers.6.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.244, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.245 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.26, %x.48, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:443:0
  %2450 : Tensor = prim::GetAttr[name="bias"](%2308)
  %2451 : Tensor = prim::GetAttr[name="weight"](%2308)
  %2452 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm
  %input.246 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.245, %2452, %2451, %2450, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2454 : Tensor = prim::GetAttr[name="bias"](%2307)
  %2455 : Tensor = prim::GetAttr[name="weight"](%2307)
  %2456 : Float(1024:1, 4096:1024) = aten::t(%2455), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %output.141 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.246, %2456), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1676:0
  %input.247 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.141, %2454, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc1 # torch/nn/functional.py:1678:0
  %input.248 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.247), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:1369:0
  %input.249 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.248, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %2461 : Tensor = prim::GetAttr[name="bias"](%2306)
  %2462 : Tensor = prim::GetAttr[name="weight"](%2306)
  %2463 : Float(4096:1, 1024:4096) = aten::t(%2462), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %output.142 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.249, %2463), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1676:0
  %input.250 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.142, %2461, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.fc2 # torch/nn/functional.py:1678:0
  %x.49 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.250, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # torch/nn/functional.py:973:0
  %input.251 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.246, %x.49, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6 # transformers/modeling_bart.py:455:0
  %2468 : Tensor = prim::GetAttr[name="bias"](%2305)
  %2469 : Tensor = prim::GetAttr[name="weight"](%2305)
  %2470 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm
  %query.27 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.251, %2470, %2469, %2468, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.6/__module.model.decoder.layers.6.final_layer_norm # torch/nn/functional.py:2048:0
  %2472 : __torch__.torch.nn.modules.normalization.___torch_mangle_643.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1270)
  %2473 : __torch__.torch.nn.modules.linear.___torch_mangle_642.Linear = prim::GetAttr[name="fc2"](%1270)
  %2474 : __torch__.torch.nn.modules.linear.___torch_mangle_641.Linear = prim::GetAttr[name="fc1"](%1270)
  %2475 : __torch__.torch.nn.modules.normalization.___torch_mangle_640.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1270)
  %2476 : __torch__.transformers.modeling_bart.___torch_mangle_639.Attention = prim::GetAttr[name="encoder_attn"](%1270)
  %2477 : __torch__.torch.nn.modules.normalization.___torch_mangle_634.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1270)
  %2478 : __torch__.transformers.modeling_bart.___torch_mangle_633.Attention = prim::GetAttr[name="self_attn"](%1270)
  %2479 : __torch__.torch.nn.modules.linear.___torch_mangle_632.Linear = prim::GetAttr[name="out_proj"](%2478)
  %2480 : __torch__.torch.nn.modules.linear.___torch_mangle_630.Linear = prim::GetAttr[name="v_proj"](%2478)
  %2481 : __torch__.torch.nn.modules.linear.___torch_mangle_629.Linear = prim::GetAttr[name="k_proj"](%2478)
  %2482 : __torch__.torch.nn.modules.linear.___torch_mangle_631.Linear = prim::GetAttr[name="q_proj"](%2478)
  %2483 : int = aten::size(%query.27, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2484 : int = aten::size(%query.27, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %bsz.27 : Long() = prim::NumToTensor(%2484), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2486 : int = aten::size(%query.27, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:660:0
  %2487 : Tensor = prim::GetAttr[name="bias"](%2482)
  %2488 : Tensor = prim::GetAttr[name="weight"](%2482)
  %2489 : Float(1024:1, 1024:1024) = aten::t(%2488), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.143 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2489), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2491 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.143, %2487, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.157 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2491, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:673:0
  %2493 : Tensor = prim::GetAttr[name="bias"](%2481)
  %2494 : Tensor = prim::GetAttr[name="weight"](%2481)
  %2495 : Float(1024:1, 1024:1024) = aten::t(%2494), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.144 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2495), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.144, %2493, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2498 : Tensor = prim::GetAttr[name="bias"](%2480)
  %2499 : Tensor = prim::GetAttr[name="weight"](%2480)
  %2500 : Float(1024:1, 1024:1024) = aten::t(%2499), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.145 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.27, %2500), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.161 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.145, %2498, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.158 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.157, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2504 : Long() = aten::mul(%bsz.27, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2505 : int = aten::Int(%2504), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2506 : int[] = prim::ListConstruct(%2483, %2505, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2507 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.158, %2506), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %q.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2507, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.160 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.159, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2510 : Long() = aten::mul(%bsz.27, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2511 : int = aten::Int(%2510), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2512 : int[] = prim::ListConstruct(%36, %2511, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2513 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.160, %2512), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %k.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2513, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %tensor.162 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.161, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2516 : Long() = aten::mul(%bsz.27, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2517 : int = aten::Int(%2516), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2518 : int[] = prim::ListConstruct(%36, %2517, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2519 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.162, %2518), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %v.27 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2519, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:647:0
  %2521 : int = aten::size(%k.27, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:701:0
  %2522 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.27, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.98 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.27, %2522), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:702:0
  %2524 : int[] = prim::ListConstruct(%2484, %27, %2483, %2521), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2525 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.98, %2524), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.99 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2525, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:706:0
  %2527 : Long() = aten::mul(%bsz.27, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %2528 : int = aten::Int(%2527), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %2529 : int[] = prim::ListConstruct(%2528, %2483, %2521), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %input.252 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.99, %2529), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:707:0
  %input.253 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.252, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.100 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.253, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # torch/nn/functional.py:973:0
  %attn_output.27 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.100, %v.27), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:730:0
  %2534 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.27, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2535 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2534, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2536 : int[] = prim::ListConstruct(%2483, %2484, %2486), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn
  %input.254 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2535, %2536), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn # transformers/modeling_bart.py:732:0
  %2538 : Tensor = prim::GetAttr[name="bias"](%2479)
  %2539 : Tensor = prim::GetAttr[name="weight"](%2479)
  %2540 : Float(1024:1, 1024:1024) = aten::t(%2539), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.146 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.254, %2540), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.255 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.146, %2538, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn/__module.model.decoder.layers.7.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.50 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.255, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.256 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.27, %x.50, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:427:0
  %2545 : Tensor = prim::GetAttr[name="bias"](%2477)
  %2546 : Tensor = prim::GetAttr[name="weight"](%2477)
  %2547 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm
  %query.28 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.256, %2547, %2546, %2545, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2549 : __torch__.torch.nn.modules.linear.___torch_mangle_638.Linear = prim::GetAttr[name="out_proj"](%2476)
  %2550 : __torch__.torch.nn.modules.linear.___torch_mangle_636.Linear = prim::GetAttr[name="v_proj"](%2476)
  %2551 : __torch__.torch.nn.modules.linear.___torch_mangle_635.Linear = prim::GetAttr[name="k_proj"](%2476)
  %2552 : __torch__.torch.nn.modules.linear.___torch_mangle_637.Linear = prim::GetAttr[name="q_proj"](%2476)
  %2553 : int = aten::size(%query.28, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2554 : int = aten::size(%query.28, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.28 : Long() = prim::NumToTensor(%2554), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2556 : int = aten::size(%query.28, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:660:0
  %2557 : Tensor = prim::GetAttr[name="bias"](%2552)
  %2558 : Tensor = prim::GetAttr[name="weight"](%2552)
  %2559 : Float(1024:1, 1024:1024) = aten::t(%2558), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.147 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.28, %2559), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2561 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.147, %2557, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.163 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2561, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:673:0
  %2563 : Tensor = prim::GetAttr[name="bias"](%2551)
  %2564 : Tensor = prim::GetAttr[name="weight"](%2551)
  %2565 : Float(1024:1, 1024:1024) = aten::t(%2564), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.148 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2565), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.165 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.148, %2563, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2568 : Tensor = prim::GetAttr[name="bias"](%2550)
  %2569 : Tensor = prim::GetAttr[name="weight"](%2550)
  %2570 : Float(1024:1, 1024:1024) = aten::t(%2569), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.149 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2570), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.167 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.149, %2568, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.164 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.163, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2574 : Long() = aten::mul(%bsz.28, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2575 : int = aten::Int(%2574), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2576 : int[] = prim::ListConstruct(%2553, %2575, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2577 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.164, %2576), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %q.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2577, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.166 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.165, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2580 : Long() = aten::mul(%bsz.28, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2581 : int = aten::Int(%2580), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2582 : int[] = prim::ListConstruct(%36, %2581, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2583 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.166, %2582), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %k.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2583, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.168 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.167, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2586 : Long() = aten::mul(%bsz.28, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2587 : int = aten::Int(%2586), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2588 : int[] = prim::ListConstruct(%36, %2587, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2589 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.168, %2588), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %v.28 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2589, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:647:0
  %2591 : int = aten::size(%k.28, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:701:0
  %2592 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.28, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.101 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.28, %2592), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:702:0
  %2594 : int[] = prim::ListConstruct(%2554, %27, %2553, %2591), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %attn_weights.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.101, %2594), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:718:0
  %2596 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.20 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2596, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.102, %reshaped.20, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:720:0
  %2599 : Long() = aten::mul(%bsz.28, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %2600 : int = aten::Int(%2599), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %2601 : int[] = prim::ListConstruct(%2600, %2553, %2591), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %input.257 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.103, %2601), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:721:0
  %input.258 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.257, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.104 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.258, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.28 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.104, %v.28), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:730:0
  %2606 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.28, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2607 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2606, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2608 : int[] = prim::ListConstruct(%2553, %2554, %2556), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn
  %input.259 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2607, %2608), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn # transformers/modeling_bart.py:732:0
  %2610 : Tensor = prim::GetAttr[name="bias"](%2549)
  %2611 : Tensor = prim::GetAttr[name="weight"](%2549)
  %2612 : Float(1024:1, 1024:1024) = aten::t(%2611), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.150 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.259, %2612), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.260 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.150, %2610, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn/__module.model.decoder.layers.7.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.260, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.261 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.28, %x.51, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:443:0
  %2617 : Tensor = prim::GetAttr[name="bias"](%2475)
  %2618 : Tensor = prim::GetAttr[name="weight"](%2475)
  %2619 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm
  %input.262 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.261, %2619, %2618, %2617, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2621 : Tensor = prim::GetAttr[name="bias"](%2474)
  %2622 : Tensor = prim::GetAttr[name="weight"](%2474)
  %2623 : Float(1024:1, 4096:1024) = aten::t(%2622), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %output.151 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.262, %2623), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1676:0
  %input.263 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.151, %2621, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc1 # torch/nn/functional.py:1678:0
  %input.264 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.263), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:1369:0
  %input.265 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.264, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %2628 : Tensor = prim::GetAttr[name="bias"](%2473)
  %2629 : Tensor = prim::GetAttr[name="weight"](%2473)
  %2630 : Float(4096:1, 1024:4096) = aten::t(%2629), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %output.152 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.265, %2630), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1676:0
  %input.266 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.152, %2628, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.fc2 # torch/nn/functional.py:1678:0
  %x.52 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.266, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # torch/nn/functional.py:973:0
  %input.267 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.262, %x.52, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7 # transformers/modeling_bart.py:455:0
  %2635 : Tensor = prim::GetAttr[name="bias"](%2472)
  %2636 : Tensor = prim::GetAttr[name="weight"](%2472)
  %2637 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm
  %query.29 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.267, %2637, %2636, %2635, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.7/__module.model.decoder.layers.7.final_layer_norm # torch/nn/functional.py:2048:0
  %2639 : __torch__.torch.nn.modules.normalization.___torch_mangle_659.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1268)
  %2640 : __torch__.torch.nn.modules.linear.___torch_mangle_658.Linear = prim::GetAttr[name="fc2"](%1268)
  %2641 : __torch__.torch.nn.modules.linear.___torch_mangle_657.Linear = prim::GetAttr[name="fc1"](%1268)
  %2642 : __torch__.torch.nn.modules.normalization.___torch_mangle_656.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1268)
  %2643 : __torch__.transformers.modeling_bart.___torch_mangle_655.Attention = prim::GetAttr[name="encoder_attn"](%1268)
  %2644 : __torch__.torch.nn.modules.normalization.___torch_mangle_650.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1268)
  %2645 : __torch__.transformers.modeling_bart.___torch_mangle_649.Attention = prim::GetAttr[name="self_attn"](%1268)
  %2646 : __torch__.torch.nn.modules.linear.___torch_mangle_648.Linear = prim::GetAttr[name="out_proj"](%2645)
  %2647 : __torch__.torch.nn.modules.linear.___torch_mangle_646.Linear = prim::GetAttr[name="v_proj"](%2645)
  %2648 : __torch__.torch.nn.modules.linear.___torch_mangle_645.Linear = prim::GetAttr[name="k_proj"](%2645)
  %2649 : __torch__.torch.nn.modules.linear.___torch_mangle_647.Linear = prim::GetAttr[name="q_proj"](%2645)
  %2650 : int = aten::size(%query.29, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2651 : int = aten::size(%query.29, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %bsz.29 : Long() = prim::NumToTensor(%2651), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2653 : int = aten::size(%query.29, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:660:0
  %2654 : Tensor = prim::GetAttr[name="bias"](%2649)
  %2655 : Tensor = prim::GetAttr[name="weight"](%2649)
  %2656 : Float(1024:1, 1024:1024) = aten::t(%2655), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.153 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2656), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2658 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.153, %2654, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.169 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2658, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:673:0
  %2660 : Tensor = prim::GetAttr[name="bias"](%2648)
  %2661 : Tensor = prim::GetAttr[name="weight"](%2648)
  %2662 : Float(1024:1, 1024:1024) = aten::t(%2661), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.154 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2662), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.171 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.154, %2660, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2665 : Tensor = prim::GetAttr[name="bias"](%2647)
  %2666 : Tensor = prim::GetAttr[name="weight"](%2647)
  %2667 : Float(1024:1, 1024:1024) = aten::t(%2666), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.155 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.29, %2667), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.173 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.155, %2665, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.170 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.169, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2671 : Long() = aten::mul(%bsz.29, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2672 : int = aten::Int(%2671), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2673 : int[] = prim::ListConstruct(%2650, %2672, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2674 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.170, %2673), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %q.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2674, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.172 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.171, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2677 : Long() = aten::mul(%bsz.29, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2678 : int = aten::Int(%2677), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2679 : int[] = prim::ListConstruct(%36, %2678, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2680 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.172, %2679), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %k.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2680, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %tensor.174 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.173, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2683 : Long() = aten::mul(%bsz.29, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2684 : int = aten::Int(%2683), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2685 : int[] = prim::ListConstruct(%36, %2684, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2686 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.174, %2685), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %v.29 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2686, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:647:0
  %2688 : int = aten::size(%k.29, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:701:0
  %2689 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.29, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.105 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.29, %2689), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:702:0
  %2691 : int[] = prim::ListConstruct(%2651, %27, %2650, %2688), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2692 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.105, %2691), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.106 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2692, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:706:0
  %2694 : Long() = aten::mul(%bsz.29, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %2695 : int = aten::Int(%2694), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %2696 : int[] = prim::ListConstruct(%2695, %2650, %2688), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %input.268 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.106, %2696), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:707:0
  %input.269 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.268, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.107 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.269, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # torch/nn/functional.py:973:0
  %attn_output.29 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.107, %v.29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:730:0
  %2701 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.29, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2702 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2701, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2703 : int[] = prim::ListConstruct(%2650, %2651, %2653), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn
  %input.270 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2702, %2703), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn # transformers/modeling_bart.py:732:0
  %2705 : Tensor = prim::GetAttr[name="bias"](%2646)
  %2706 : Tensor = prim::GetAttr[name="weight"](%2646)
  %2707 : Float(1024:1, 1024:1024) = aten::t(%2706), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.156 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.270, %2707), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.271 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.156, %2705, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn/__module.model.decoder.layers.8.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.53 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.271, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.272 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.29, %x.53, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:427:0
  %2712 : Tensor = prim::GetAttr[name="bias"](%2644)
  %2713 : Tensor = prim::GetAttr[name="weight"](%2644)
  %2714 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm
  %query.30 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.272, %2714, %2713, %2712, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2716 : __torch__.torch.nn.modules.linear.___torch_mangle_654.Linear = prim::GetAttr[name="out_proj"](%2643)
  %2717 : __torch__.torch.nn.modules.linear.___torch_mangle_652.Linear = prim::GetAttr[name="v_proj"](%2643)
  %2718 : __torch__.torch.nn.modules.linear.___torch_mangle_651.Linear = prim::GetAttr[name="k_proj"](%2643)
  %2719 : __torch__.torch.nn.modules.linear.___torch_mangle_653.Linear = prim::GetAttr[name="q_proj"](%2643)
  %2720 : int = aten::size(%query.30, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2721 : int = aten::size(%query.30, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.30 : Long() = prim::NumToTensor(%2721), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2723 : int = aten::size(%query.30, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:660:0
  %2724 : Tensor = prim::GetAttr[name="bias"](%2719)
  %2725 : Tensor = prim::GetAttr[name="weight"](%2719)
  %2726 : Float(1024:1, 1024:1024) = aten::t(%2725), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.157 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.30, %2726), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2728 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.157, %2724, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.175 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2728, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:673:0
  %2730 : Tensor = prim::GetAttr[name="bias"](%2718)
  %2731 : Tensor = prim::GetAttr[name="weight"](%2718)
  %2732 : Float(1024:1, 1024:1024) = aten::t(%2731), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.158 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2732), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.177 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.158, %2730, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2735 : Tensor = prim::GetAttr[name="bias"](%2717)
  %2736 : Tensor = prim::GetAttr[name="weight"](%2717)
  %2737 : Float(1024:1, 1024:1024) = aten::t(%2736), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.159 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2737), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.179 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.159, %2735, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.176 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.175, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2741 : Long() = aten::mul(%bsz.30, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2742 : int = aten::Int(%2741), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2743 : int[] = prim::ListConstruct(%2720, %2742, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2744 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.176, %2743), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %q.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2744, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.178 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.177, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2747 : Long() = aten::mul(%bsz.30, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2748 : int = aten::Int(%2747), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2749 : int[] = prim::ListConstruct(%36, %2748, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2750 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.178, %2749), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %k.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2750, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.180 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.179, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2753 : Long() = aten::mul(%bsz.30, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2754 : int = aten::Int(%2753), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2755 : int[] = prim::ListConstruct(%36, %2754, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2756 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.180, %2755), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %v.30 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2756, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:647:0
  %2758 : int = aten::size(%k.30, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:701:0
  %2759 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.30, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.108 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.30, %2759), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:702:0
  %2761 : int[] = prim::ListConstruct(%2721, %27, %2720, %2758), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %attn_weights.109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.108, %2761), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:718:0
  %2763 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.21 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2763, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.110 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.109, %reshaped.21, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:720:0
  %2766 : Long() = aten::mul(%bsz.30, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %2767 : int = aten::Int(%2766), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %2768 : int[] = prim::ListConstruct(%2767, %2720, %2758), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %input.273 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.110, %2768), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:721:0
  %input.274 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.273, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.111 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.274, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.30 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.111, %v.30), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:730:0
  %2773 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.30, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2774 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2773, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2775 : int[] = prim::ListConstruct(%2720, %2721, %2723), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn
  %input.275 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2774, %2775), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn # transformers/modeling_bart.py:732:0
  %2777 : Tensor = prim::GetAttr[name="bias"](%2716)
  %2778 : Tensor = prim::GetAttr[name="weight"](%2716)
  %2779 : Float(1024:1, 1024:1024) = aten::t(%2778), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.160 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.275, %2779), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.276 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.160, %2777, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn/__module.model.decoder.layers.8.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.276, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.277 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.30, %x.54, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:443:0
  %2784 : Tensor = prim::GetAttr[name="bias"](%2642)
  %2785 : Tensor = prim::GetAttr[name="weight"](%2642)
  %2786 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm
  %input.278 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.277, %2786, %2785, %2784, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2788 : Tensor = prim::GetAttr[name="bias"](%2641)
  %2789 : Tensor = prim::GetAttr[name="weight"](%2641)
  %2790 : Float(1024:1, 4096:1024) = aten::t(%2789), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %output.161 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.278, %2790), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1676:0
  %input.279 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.161, %2788, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc1 # torch/nn/functional.py:1678:0
  %input.280 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.279), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:1369:0
  %input.281 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.280, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %2795 : Tensor = prim::GetAttr[name="bias"](%2640)
  %2796 : Tensor = prim::GetAttr[name="weight"](%2640)
  %2797 : Float(4096:1, 1024:4096) = aten::t(%2796), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %output.162 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.281, %2797), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1676:0
  %input.282 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.162, %2795, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.fc2 # torch/nn/functional.py:1678:0
  %x.55 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.282, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # torch/nn/functional.py:973:0
  %input.283 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.278, %x.55, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8 # transformers/modeling_bart.py:455:0
  %2802 : Tensor = prim::GetAttr[name="bias"](%2639)
  %2803 : Tensor = prim::GetAttr[name="weight"](%2639)
  %2804 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm
  %query.31 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.283, %2804, %2803, %2802, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.8/__module.model.decoder.layers.8.final_layer_norm # torch/nn/functional.py:2048:0
  %2806 : __torch__.torch.nn.modules.normalization.___torch_mangle_675.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1266)
  %2807 : __torch__.torch.nn.modules.linear.___torch_mangle_674.Linear = prim::GetAttr[name="fc2"](%1266)
  %2808 : __torch__.torch.nn.modules.linear.___torch_mangle_673.Linear = prim::GetAttr[name="fc1"](%1266)
  %2809 : __torch__.torch.nn.modules.normalization.___torch_mangle_672.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1266)
  %2810 : __torch__.transformers.modeling_bart.___torch_mangle_671.Attention = prim::GetAttr[name="encoder_attn"](%1266)
  %2811 : __torch__.torch.nn.modules.normalization.___torch_mangle_666.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1266)
  %2812 : __torch__.transformers.modeling_bart.___torch_mangle_665.Attention = prim::GetAttr[name="self_attn"](%1266)
  %2813 : __torch__.torch.nn.modules.linear.___torch_mangle_664.Linear = prim::GetAttr[name="out_proj"](%2812)
  %2814 : __torch__.torch.nn.modules.linear.___torch_mangle_662.Linear = prim::GetAttr[name="v_proj"](%2812)
  %2815 : __torch__.torch.nn.modules.linear.___torch_mangle_661.Linear = prim::GetAttr[name="k_proj"](%2812)
  %2816 : __torch__.torch.nn.modules.linear.___torch_mangle_663.Linear = prim::GetAttr[name="q_proj"](%2812)
  %2817 : int = aten::size(%query.31, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2818 : int = aten::size(%query.31, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %bsz.31 : Long() = prim::NumToTensor(%2818), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2820 : int = aten::size(%query.31, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:660:0
  %2821 : Tensor = prim::GetAttr[name="bias"](%2816)
  %2822 : Tensor = prim::GetAttr[name="weight"](%2816)
  %2823 : Float(1024:1, 1024:1024) = aten::t(%2822), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.163 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2823), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2825 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.163, %2821, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.181 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2825, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:673:0
  %2827 : Tensor = prim::GetAttr[name="bias"](%2815)
  %2828 : Tensor = prim::GetAttr[name="weight"](%2815)
  %2829 : Float(1024:1, 1024:1024) = aten::t(%2828), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.164 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2829), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.183 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.164, %2827, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2832 : Tensor = prim::GetAttr[name="bias"](%2814)
  %2833 : Tensor = prim::GetAttr[name="weight"](%2814)
  %2834 : Float(1024:1, 1024:1024) = aten::t(%2833), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.165 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.31, %2834), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.185 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.165, %2832, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.182 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.181, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2838 : Long() = aten::mul(%bsz.31, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2839 : int = aten::Int(%2838), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2840 : int[] = prim::ListConstruct(%2817, %2839, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2841 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.182, %2840), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %q.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2841, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.184 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.183, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2844 : Long() = aten::mul(%bsz.31, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2845 : int = aten::Int(%2844), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2846 : int[] = prim::ListConstruct(%36, %2845, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2847 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.184, %2846), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %k.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2847, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %tensor.186 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.185, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2850 : Long() = aten::mul(%bsz.31, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2851 : int = aten::Int(%2850), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2852 : int[] = prim::ListConstruct(%36, %2851, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2853 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.186, %2852), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %v.31 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2853, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:647:0
  %2855 : int = aten::size(%k.31, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:701:0
  %2856 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.31, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.112 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.31, %2856), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:702:0
  %2858 : int[] = prim::ListConstruct(%2818, %27, %2817, %2855), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2859 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.112, %2858), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.113 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2859, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:706:0
  %2861 : Long() = aten::mul(%bsz.31, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %2862 : int = aten::Int(%2861), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %2863 : int[] = prim::ListConstruct(%2862, %2817, %2855), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %input.284 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.113, %2863), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:707:0
  %input.285 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.284, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.114 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.285, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # torch/nn/functional.py:973:0
  %attn_output.31 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.114, %v.31), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:730:0
  %2868 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.31, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2869 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2868, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2870 : int[] = prim::ListConstruct(%2817, %2818, %2820), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn
  %input.286 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2869, %2870), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn # transformers/modeling_bart.py:732:0
  %2872 : Tensor = prim::GetAttr[name="bias"](%2813)
  %2873 : Tensor = prim::GetAttr[name="weight"](%2813)
  %2874 : Float(1024:1, 1024:1024) = aten::t(%2873), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.166 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.286, %2874), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.287 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.166, %2872, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn/__module.model.decoder.layers.9.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.56 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.287, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.288 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.31, %x.56, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:427:0
  %2879 : Tensor = prim::GetAttr[name="bias"](%2811)
  %2880 : Tensor = prim::GetAttr[name="weight"](%2811)
  %2881 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm
  %query.32 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.288, %2881, %2880, %2879, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %2883 : __torch__.torch.nn.modules.linear.___torch_mangle_670.Linear = prim::GetAttr[name="out_proj"](%2810)
  %2884 : __torch__.torch.nn.modules.linear.___torch_mangle_668.Linear = prim::GetAttr[name="v_proj"](%2810)
  %2885 : __torch__.torch.nn.modules.linear.___torch_mangle_667.Linear = prim::GetAttr[name="k_proj"](%2810)
  %2886 : __torch__.torch.nn.modules.linear.___torch_mangle_669.Linear = prim::GetAttr[name="q_proj"](%2810)
  %2887 : int = aten::size(%query.32, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2888 : int = aten::size(%query.32, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.32 : Long() = prim::NumToTensor(%2888), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2890 : int = aten::size(%query.32, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:660:0
  %2891 : Tensor = prim::GetAttr[name="bias"](%2886)
  %2892 : Tensor = prim::GetAttr[name="weight"](%2886)
  %2893 : Float(1024:1, 1024:1024) = aten::t(%2892), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.167 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.32, %2893), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %2895 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.167, %2891, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.187 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2895, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:673:0
  %2897 : Tensor = prim::GetAttr[name="bias"](%2885)
  %2898 : Tensor = prim::GetAttr[name="weight"](%2885)
  %2899 : Float(1024:1, 1024:1024) = aten::t(%2898), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.168 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2899), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.189 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.168, %2897, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %2902 : Tensor = prim::GetAttr[name="bias"](%2884)
  %2903 : Tensor = prim::GetAttr[name="weight"](%2884)
  %2904 : Float(1024:1, 1024:1024) = aten::t(%2903), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.169 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2904), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.191 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.169, %2902, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.188 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.187, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2908 : Long() = aten::mul(%bsz.32, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2909 : int = aten::Int(%2908), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2910 : int[] = prim::ListConstruct(%2887, %2909, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2911 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.188, %2910), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %q.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2911, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.190 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.189, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2914 : Long() = aten::mul(%bsz.32, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2915 : int = aten::Int(%2914), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2916 : int[] = prim::ListConstruct(%36, %2915, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2917 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.190, %2916), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %k.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2917, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.192 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.191, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2920 : Long() = aten::mul(%bsz.32, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2921 : int = aten::Int(%2920), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2922 : int[] = prim::ListConstruct(%36, %2921, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2923 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.192, %2922), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %v.32 : Float(272:64, 13:17408, 64:1) = aten::transpose(%2923, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:647:0
  %2925 : int = aten::size(%k.32, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:701:0
  %2926 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.32, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.115 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.32, %2926), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:702:0
  %2928 : int[] = prim::ListConstruct(%2888, %27, %2887, %2925), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %attn_weights.116 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.115, %2928), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:718:0
  %2930 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.22 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%2930, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.117 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.116, %reshaped.22, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:720:0
  %2933 : Long() = aten::mul(%bsz.32, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %2934 : int = aten::Int(%2933), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %2935 : int[] = prim::ListConstruct(%2934, %2887, %2925), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %input.289 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.117, %2935), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:721:0
  %input.290 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.289, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.118 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.290, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.32 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.118, %v.32), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:730:0
  %2940 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.32, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2941 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%2940, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2942 : int[] = prim::ListConstruct(%2887, %2888, %2890), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn
  %input.291 : Float(13:17408, 17:1024, 1024:1) = aten::view(%2941, %2942), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn # transformers/modeling_bart.py:732:0
  %2944 : Tensor = prim::GetAttr[name="bias"](%2883)
  %2945 : Tensor = prim::GetAttr[name="weight"](%2883)
  %2946 : Float(1024:1, 1024:1024) = aten::t(%2945), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.170 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.291, %2946), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.292 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.170, %2944, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn/__module.model.decoder.layers.9.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.292, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.293 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.32, %x.57, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:443:0
  %2951 : Tensor = prim::GetAttr[name="bias"](%2809)
  %2952 : Tensor = prim::GetAttr[name="weight"](%2809)
  %2953 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm
  %input.294 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.293, %2953, %2952, %2951, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %2955 : Tensor = prim::GetAttr[name="bias"](%2808)
  %2956 : Tensor = prim::GetAttr[name="weight"](%2808)
  %2957 : Float(1024:1, 4096:1024) = aten::t(%2956), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %output.171 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.294, %2957), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1676:0
  %input.295 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.171, %2955, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc1 # torch/nn/functional.py:1678:0
  %input.296 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.295), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:1369:0
  %input.297 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.296, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %2962 : Tensor = prim::GetAttr[name="bias"](%2807)
  %2963 : Tensor = prim::GetAttr[name="weight"](%2807)
  %2964 : Float(4096:1, 1024:4096) = aten::t(%2963), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %output.172 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.297, %2964), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1676:0
  %input.298 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.172, %2962, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.fc2 # torch/nn/functional.py:1678:0
  %x.58 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.298, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # torch/nn/functional.py:973:0
  %input.299 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.294, %x.58, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9 # transformers/modeling_bart.py:455:0
  %2969 : Tensor = prim::GetAttr[name="bias"](%2806)
  %2970 : Tensor = prim::GetAttr[name="weight"](%2806)
  %2971 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm
  %query.33 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.299, %2971, %2970, %2969, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.9/__module.model.decoder.layers.9.final_layer_norm # torch/nn/functional.py:2048:0
  %2973 : __torch__.torch.nn.modules.normalization.___torch_mangle_691.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1264)
  %2974 : __torch__.torch.nn.modules.linear.___torch_mangle_690.Linear = prim::GetAttr[name="fc2"](%1264)
  %2975 : __torch__.torch.nn.modules.linear.___torch_mangle_689.Linear = prim::GetAttr[name="fc1"](%1264)
  %2976 : __torch__.torch.nn.modules.normalization.___torch_mangle_688.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1264)
  %2977 : __torch__.transformers.modeling_bart.___torch_mangle_687.Attention = prim::GetAttr[name="encoder_attn"](%1264)
  %2978 : __torch__.torch.nn.modules.normalization.___torch_mangle_682.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1264)
  %2979 : __torch__.transformers.modeling_bart.___torch_mangle_681.Attention = prim::GetAttr[name="self_attn"](%1264)
  %2980 : __torch__.torch.nn.modules.linear.___torch_mangle_680.Linear = prim::GetAttr[name="out_proj"](%2979)
  %2981 : __torch__.torch.nn.modules.linear.___torch_mangle_678.Linear = prim::GetAttr[name="v_proj"](%2979)
  %2982 : __torch__.torch.nn.modules.linear.___torch_mangle_677.Linear = prim::GetAttr[name="k_proj"](%2979)
  %2983 : __torch__.torch.nn.modules.linear.___torch_mangle_679.Linear = prim::GetAttr[name="q_proj"](%2979)
  %2984 : int = aten::size(%query.33, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %2985 : int = aten::size(%query.33, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %bsz.33 : Long() = prim::NumToTensor(%2985), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %2987 : int = aten::size(%query.33, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:660:0
  %2988 : Tensor = prim::GetAttr[name="bias"](%2983)
  %2989 : Tensor = prim::GetAttr[name="weight"](%2983)
  %2990 : Float(1024:1, 1024:1024) = aten::t(%2989), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.173 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %2990), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1676:0
  %2992 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.173, %2988, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.193 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%2992, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:673:0
  %2994 : Tensor = prim::GetAttr[name="bias"](%2982)
  %2995 : Tensor = prim::GetAttr[name="weight"](%2982)
  %2996 : Float(1024:1, 1024:1024) = aten::t(%2995), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.174 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %2996), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.195 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.174, %2994, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.k_proj # torch/nn/functional.py:1678:0
  %2999 : Tensor = prim::GetAttr[name="bias"](%2981)
  %3000 : Tensor = prim::GetAttr[name="weight"](%2981)
  %3001 : Float(1024:1, 1024:1024) = aten::t(%3000), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.175 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.33, %3001), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.197 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.175, %2999, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.194 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.193, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3005 : Long() = aten::mul(%bsz.33, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3006 : int = aten::Int(%3005), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3007 : int[] = prim::ListConstruct(%2984, %3006, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3008 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.194, %3007), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %q.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3008, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.196 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.195, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3011 : Long() = aten::mul(%bsz.33, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3012 : int = aten::Int(%3011), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3013 : int[] = prim::ListConstruct(%36, %3012, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3014 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.196, %3013), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %k.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3014, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %tensor.198 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.197, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3017 : Long() = aten::mul(%bsz.33, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3018 : int = aten::Int(%3017), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3019 : int[] = prim::ListConstruct(%36, %3018, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3020 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.198, %3019), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %v.33 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3020, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:647:0
  %3022 : int = aten::size(%k.33, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:701:0
  %3023 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.33, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.119 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.33, %3023), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:702:0
  %3025 : int[] = prim::ListConstruct(%2985, %27, %2984, %3022), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3026 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.119, %3025), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3026, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:706:0
  %3028 : Long() = aten::mul(%bsz.33, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %3029 : int = aten::Int(%3028), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %3030 : int[] = prim::ListConstruct(%3029, %2984, %3022), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %input.300 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.120, %3030), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:707:0
  %input.301 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.300, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.121 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.301, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # torch/nn/functional.py:973:0
  %attn_output.33 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.121, %v.33), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:730:0
  %3035 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.33, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3036 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3035, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3037 : int[] = prim::ListConstruct(%2984, %2985, %2987), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn
  %input.302 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3036, %3037), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn # transformers/modeling_bart.py:732:0
  %3039 : Tensor = prim::GetAttr[name="bias"](%2980)
  %3040 : Tensor = prim::GetAttr[name="weight"](%2980)
  %3041 : Float(1024:1, 1024:1024) = aten::t(%3040), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.176 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.302, %3041), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.303 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.176, %3039, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn/__module.model.decoder.layers.10.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.59 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.303, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.304 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.33, %x.59, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:427:0
  %3046 : Tensor = prim::GetAttr[name="bias"](%2978)
  %3047 : Tensor = prim::GetAttr[name="weight"](%2978)
  %3048 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm
  %query.34 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.304, %3048, %3047, %3046, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3050 : __torch__.torch.nn.modules.linear.___torch_mangle_686.Linear = prim::GetAttr[name="out_proj"](%2977)
  %3051 : __torch__.torch.nn.modules.linear.___torch_mangle_684.Linear = prim::GetAttr[name="v_proj"](%2977)
  %3052 : __torch__.torch.nn.modules.linear.___torch_mangle_683.Linear = prim::GetAttr[name="k_proj"](%2977)
  %3053 : __torch__.torch.nn.modules.linear.___torch_mangle_685.Linear = prim::GetAttr[name="q_proj"](%2977)
  %3054 : int = aten::size(%query.34, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3055 : int = aten::size(%query.34, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz.34 : Long() = prim::NumToTensor(%3055), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3057 : int = aten::size(%query.34, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:660:0
  %3058 : Tensor = prim::GetAttr[name="bias"](%3053)
  %3059 : Tensor = prim::GetAttr[name="weight"](%3053)
  %3060 : Float(1024:1, 1024:1024) = aten::t(%3059), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.177 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.34, %3060), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3062 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.177, %3058, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.199 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3062, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:673:0
  %3064 : Tensor = prim::GetAttr[name="bias"](%3052)
  %3065 : Tensor = prim::GetAttr[name="weight"](%3052)
  %3066 : Float(1024:1, 1024:1024) = aten::t(%3065), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.178 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3066), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.201 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.178, %3064, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3069 : Tensor = prim::GetAttr[name="bias"](%3051)
  %3070 : Tensor = prim::GetAttr[name="weight"](%3051)
  %3071 : Float(1024:1, 1024:1024) = aten::t(%3070), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.179 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3071), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.203 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.179, %3069, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.200 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.199, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3075 : Long() = aten::mul(%bsz.34, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3076 : int = aten::Int(%3075), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3077 : int[] = prim::ListConstruct(%3054, %3076, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3078 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.200, %3077), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %q.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3078, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.202 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.201, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3081 : Long() = aten::mul(%bsz.34, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3082 : int = aten::Int(%3081), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3083 : int[] = prim::ListConstruct(%36, %3082, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3084 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.202, %3083), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %k.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3084, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.204 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.203, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3087 : Long() = aten::mul(%bsz.34, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3088 : int = aten::Int(%3087), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3089 : int[] = prim::ListConstruct(%36, %3088, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3090 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.204, %3089), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %v.34 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3090, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:647:0
  %3092 : int = aten::size(%k.34, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:701:0
  %3093 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.34, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.122 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.34, %3093), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:702:0
  %3095 : int[] = prim::ListConstruct(%3055, %27, %3054, %3092), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %attn_weights.123 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.122, %3095), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:718:0
  %3097 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped.23 : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3097, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.124 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.123, %reshaped.23, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:720:0
  %3100 : Long() = aten::mul(%bsz.34, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %3101 : int = aten::Int(%3100), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %3102 : int[] = prim::ListConstruct(%3101, %3054, %3092), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %input.305 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.124, %3102), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:721:0
  %input.306 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.305, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights.125 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.306, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # torch/nn/functional.py:973:0
  %attn_output.34 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.125, %v.34), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:730:0
  %3107 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.34, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3108 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3107, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3109 : int[] = prim::ListConstruct(%3054, %3055, %3057), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn
  %input.307 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3108, %3109), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn # transformers/modeling_bart.py:732:0
  %3111 : Tensor = prim::GetAttr[name="bias"](%3050)
  %3112 : Tensor = prim::GetAttr[name="weight"](%3050)
  %3113 : Float(1024:1, 1024:1024) = aten::t(%3112), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.180 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.307, %3113), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.308 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.180, %3111, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn/__module.model.decoder.layers.10.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.308, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.309 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.34, %x.60, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:443:0
  %3118 : Tensor = prim::GetAttr[name="bias"](%2976)
  %3119 : Tensor = prim::GetAttr[name="weight"](%2976)
  %3120 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm
  %input.310 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.309, %3120, %3119, %3118, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3122 : Tensor = prim::GetAttr[name="bias"](%2975)
  %3123 : Tensor = prim::GetAttr[name="weight"](%2975)
  %3124 : Float(1024:1, 4096:1024) = aten::t(%3123), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %output.181 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.310, %3124), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1676:0
  %input.311 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.181, %3122, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc1 # torch/nn/functional.py:1678:0
  %input.312 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.311), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:1369:0
  %input.313 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.312, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %3129 : Tensor = prim::GetAttr[name="bias"](%2974)
  %3130 : Tensor = prim::GetAttr[name="weight"](%2974)
  %3131 : Float(4096:1, 1024:4096) = aten::t(%3130), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %output.182 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.313, %3131), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1676:0
  %input.314 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.182, %3129, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.fc2 # torch/nn/functional.py:1678:0
  %x.61 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.314, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # torch/nn/functional.py:973:0
  %input.315 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.310, %x.61, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10 # transformers/modeling_bart.py:455:0
  %3136 : Tensor = prim::GetAttr[name="bias"](%2973)
  %3137 : Tensor = prim::GetAttr[name="weight"](%2973)
  %3138 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm
  %query.35 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.315, %3138, %3137, %3136, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.10/__module.model.decoder.layers.10.final_layer_norm # torch/nn/functional.py:2048:0
  %3140 : __torch__.torch.nn.modules.normalization.___torch_mangle_707.LayerNorm = prim::GetAttr[name="final_layer_norm"](%1262)
  %3141 : __torch__.torch.nn.modules.linear.___torch_mangle_706.Linear = prim::GetAttr[name="fc2"](%1262)
  %3142 : __torch__.torch.nn.modules.linear.___torch_mangle_705.Linear = prim::GetAttr[name="fc1"](%1262)
  %3143 : __torch__.torch.nn.modules.normalization.___torch_mangle_704.LayerNorm = prim::GetAttr[name="encoder_attn_layer_norm"](%1262)
  %3144 : __torch__.transformers.modeling_bart.___torch_mangle_703.Attention = prim::GetAttr[name="encoder_attn"](%1262)
  %3145 : __torch__.torch.nn.modules.normalization.___torch_mangle_698.LayerNorm = prim::GetAttr[name="self_attn_layer_norm"](%1262)
  %3146 : __torch__.transformers.modeling_bart.___torch_mangle_697.Attention = prim::GetAttr[name="self_attn"](%1262)
  %3147 : __torch__.torch.nn.modules.linear.___torch_mangle_696.Linear = prim::GetAttr[name="out_proj"](%3146)
  %3148 : __torch__.torch.nn.modules.linear.___torch_mangle_694.Linear = prim::GetAttr[name="v_proj"](%3146)
  %3149 : __torch__.torch.nn.modules.linear.___torch_mangle_693.Linear = prim::GetAttr[name="k_proj"](%3146)
  %3150 : __torch__.torch.nn.modules.linear.___torch_mangle_695.Linear = prim::GetAttr[name="q_proj"](%3146)
  %3151 : int = aten::size(%query.35, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3152 : int = aten::size(%query.35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %bsz.35 : Long() = prim::NumToTensor(%3152), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3154 : int = aten::size(%query.35, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:660:0
  %3155 : Tensor = prim::GetAttr[name="bias"](%3150)
  %3156 : Tensor = prim::GetAttr[name="weight"](%3150)
  %3157 : Float(1024:1, 1024:1024) = aten::t(%3156), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %output.183 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3157), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1676:0
  %3159 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.183, %3155, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.205 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3159, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:673:0
  %3161 : Tensor = prim::GetAttr[name="bias"](%3149)
  %3162 : Tensor = prim::GetAttr[name="weight"](%3149)
  %3163 : Float(1024:1, 1024:1024) = aten::t(%3162), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %output.184 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3163), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.207 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.184, %3161, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.k_proj # torch/nn/functional.py:1678:0
  %3166 : Tensor = prim::GetAttr[name="bias"](%3148)
  %3167 : Tensor = prim::GetAttr[name="weight"](%3148)
  %3168 : Float(1024:1, 1024:1024) = aten::t(%3167), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %output.185 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query.35, %3168), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.209 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.185, %3166, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.206 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.205, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3172 : Long() = aten::mul(%bsz.35, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3173 : int = aten::Int(%3172), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3174 : int[] = prim::ListConstruct(%3151, %3173, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3175 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.206, %3174), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %q.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3175, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.208 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.207, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3178 : Long() = aten::mul(%bsz.35, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3179 : int = aten::Int(%3178), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3180 : int[] = prim::ListConstruct(%36, %3179, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3181 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.208, %3180), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %k.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3181, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %tensor.210 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.209, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3184 : Long() = aten::mul(%bsz.35, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3185 : int = aten::Int(%3184), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3186 : int[] = prim::ListConstruct(%36, %3185, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3187 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.210, %3186), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %v.35 : Float(272:64, 13:17408, 64:1) = aten::transpose(%3187, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:647:0
  %3189 : int = aten::size(%k.35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:701:0
  %3190 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k.35, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %attn_weights.126 : Float(272:169, 13:13, 13:1) = aten::bmm(%q.35, %3190), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:702:0
  %3192 : int[] = prim::ListConstruct(%3152, %27, %3151, %3189), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.126, %3192), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %attn_weights.127 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3193, %attn_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:706:0
  %3195 : Long() = aten::mul(%bsz.35, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %3196 : int = aten::Int(%3195), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %3197 : int[] = prim::ListConstruct(%3196, %3151, %3189), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %input.316 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.127, %3197), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:707:0
  %input.317 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.316, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:1498:0
  %attn_weights.128 : Float(272:169, 13:13, 13:1) = aten::dropout(%input.317, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # torch/nn/functional.py:973:0
  %attn_output.35 : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights.128, %v.35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:730:0
  %3202 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output.35, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3203 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3202, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3204 : int[] = prim::ListConstruct(%3151, %3152, %3154), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn
  %input.318 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3203, %3204), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn # transformers/modeling_bart.py:732:0
  %3206 : Tensor = prim::GetAttr[name="bias"](%3147)
  %3207 : Tensor = prim::GetAttr[name="weight"](%3147)
  %3208 : Float(1024:1, 1024:1024) = aten::t(%3207), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %output.186 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.318, %3208), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1676:0
  %input.319 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.186, %3206, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn/__module.model.decoder.layers.11.self_attn.out_proj # torch/nn/functional.py:1678:0
  %x.62 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.319, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.320 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query.35, %x.62, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:427:0
  %3213 : Tensor = prim::GetAttr[name="bias"](%3145)
  %3214 : Tensor = prim::GetAttr[name="weight"](%3145)
  %3215 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm
  %query : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.320, %3215, %3214, %3213, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.self_attn_layer_norm # torch/nn/functional.py:2048:0
  %3217 : __torch__.torch.nn.modules.linear.___torch_mangle_702.Linear = prim::GetAttr[name="out_proj"](%3144)
  %3218 : __torch__.torch.nn.modules.linear.___torch_mangle_700.Linear = prim::GetAttr[name="v_proj"](%3144)
  %3219 : __torch__.torch.nn.modules.linear.___torch_mangle_699.Linear = prim::GetAttr[name="k_proj"](%3144)
  %3220 : __torch__.torch.nn.modules.linear.___torch_mangle_701.Linear = prim::GetAttr[name="q_proj"](%3144)
  %3221 : int = aten::size(%query, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3222 : int = aten::size(%query, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %bsz : Long() = prim::NumToTensor(%3222), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3224 : int = aten::size(%query, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:660:0
  %3225 : Tensor = prim::GetAttr[name="bias"](%3220)
  %3226 : Tensor = prim::GetAttr[name="weight"](%3220)
  %3227 : Float(1024:1, 1024:1024) = aten::t(%3226), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %output.187 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%query, %3227), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1676:0
  %3229 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.187, %3225, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.q_proj # torch/nn/functional.py:1678:0
  %tensor.211 : Float(13:17408, 17:1024, 1024:1) = aten::mul(%3229, %24), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:673:0
  %3231 : Tensor = prim::GetAttr[name="bias"](%3219)
  %3232 : Tensor = prim::GetAttr[name="weight"](%3219)
  %3233 : Float(1024:1, 1024:1024) = aten::t(%3232), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %output.188 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3233), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1676:0
  %tensor.213 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.188, %3231, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.k_proj # torch/nn/functional.py:1678:0
  %3236 : Tensor = prim::GetAttr[name="bias"](%3218)
  %3237 : Tensor = prim::GetAttr[name="weight"](%3218)
  %3238 : Float(1024:1, 1024:1024) = aten::t(%3237), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %output.189 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %3238), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1676:0
  %tensor.215 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.189, %3236, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.v_proj # torch/nn/functional.py:1678:0
  %tensor.212 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.211, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3242 : Long() = aten::mul(%bsz, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3243 : int = aten::Int(%3242), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3244 : int[] = prim::ListConstruct(%3221, %3243, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3245 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.212, %3244), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %q : Float(272:64, 13:17408, 64:1) = aten::transpose(%3245, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor.214 : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.213, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3248 : Long() = aten::mul(%bsz, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3249 : int = aten::Int(%3248), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3250 : int[] = prim::ListConstruct(%36, %3249, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3251 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor.214, %3250), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %k : Float(272:64, 13:17408, 64:1) = aten::transpose(%3251, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %tensor : Float(13:17408, 17:1024, 1024:1) = aten::contiguous(%tensor.215, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3254 : Long() = aten::mul(%bsz, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3255 : int = aten::Int(%3254), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3256 : int[] = prim::ListConstruct(%36, %3255, %26), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3257 : Float(13:17408, 272:64, 64:1) = aten::view(%tensor, %3256), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %v : Float(272:64, 13:17408, 64:1) = aten::transpose(%3257, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:647:0
  %3259 : int = aten::size(%k, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:701:0
  %3260 : Float(272:64, 64:1, 13:17408) = aten::transpose(%k, %39, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %attn_weights.129 : Float(272:169, 13:13, 13:1) = aten::bmm(%q, %3260), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:702:0
  %3262 : int[] = prim::ListConstruct(%3222, %27, %3221, %3259), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %attn_weights.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::view(%attn_weights.129, %3262), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:718:0
  %3264 : Bool(17:13, 1:13, 13:1) = aten::unsqueeze(%key_padding_mask, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %reshaped : Bool(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%3264, %23), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:719:0
  %attn_weights.131 : Float(17:2704, 16:169, 13:13, 13:1) = aten::masked_fill(%attn_weights.130, %reshaped, %29), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:720:0
  %3267 : Long() = aten::mul(%bsz, %25), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %3268 : int = aten::Int(%3267), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %3269 : int[] = prim::ListConstruct(%3268, %3221, %3259), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %input.321 : Float(272:169, 13:13, 13:1) = aten::view(%attn_weights.131, %3269), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:721:0
  %input.322 : Float(272:169, 13:13, 13:1) = aten::softmax(%input.321, %36, %40), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:1498:0
  %attn_weights : Float(272:169, 13:13, 13:1) = aten::dropout(%input.322, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # torch/nn/functional.py:973:0
  %attn_output : Float(272:832, 13:64, 64:1) = aten::bmm(%attn_weights, %v), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:730:0
  %3274 : Float(13:64, 272:832, 64:1) = aten::transpose(%attn_output, %35, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3275 : Float(13:17408, 272:64, 64:1) = aten::contiguous(%3274, %35), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3276 : int[] = prim::ListConstruct(%3221, %3222, %3224), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn
  %input.323 : Float(13:17408, 17:1024, 1024:1) = aten::view(%3275, %3276), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn # transformers/modeling_bart.py:732:0
  %3278 : Tensor = prim::GetAttr[name="bias"](%3217)
  %3279 : Tensor = prim::GetAttr[name="weight"](%3217)
  %3280 : Float(1024:1, 1024:1024) = aten::t(%3279), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %output.190 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.323, %3280), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1676:0
  %input.324 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.190, %3278, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn/__module.model.decoder.layers.11.encoder_attn.out_proj # torch/nn/functional.py:1678:0
  %x.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.324, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.325 : Float(13:17408, 17:1024, 1024:1) = aten::add(%query, %x.63, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:443:0
  %3285 : Tensor = prim::GetAttr[name="bias"](%3143)
  %3286 : Tensor = prim::GetAttr[name="weight"](%3143)
  %3287 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm
  %input.326 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.325, %3287, %3286, %3285, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.encoder_attn_layer_norm # torch/nn/functional.py:2048:0
  %3289 : Tensor = prim::GetAttr[name="bias"](%3142)
  %3290 : Tensor = prim::GetAttr[name="weight"](%3142)
  %3291 : Float(1024:1, 4096:1024) = aten::t(%3290), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %output.191 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input.326, %3291), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1676:0
  %input.327 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.191, %3289, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc1 # torch/nn/functional.py:1678:0
  %input.328 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.327), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:1369:0
  %input.329 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.328, %28, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %3296 : Tensor = prim::GetAttr[name="bias"](%3141)
  %3297 : Tensor = prim::GetAttr[name="weight"](%3141)
  %3298 : Float(4096:1, 1024:4096) = aten::t(%3297), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %output.192 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.329, %3298), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1676:0
  %input.330 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.192, %3296, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.fc2 # torch/nn/functional.py:1678:0
  %x.64 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.330, %22, %38), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # torch/nn/functional.py:973:0
  %input.331 : Float(13:17408, 17:1024, 1024:1) = aten::add(%input.326, %x.64, %39), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11 # transformers/modeling_bart.py:455:0
  %3303 : Tensor = prim::GetAttr[name="bias"](%3140)
  %3304 : Tensor = prim::GetAttr[name="weight"](%3140)
  %3305 : int[] = prim::ListConstruct(%21), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm
  %x : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.331, %3305, %3304, %3303, %20, %19), scope: __module.model/__module.model.decoder/__module.model.decoder.layers.11/__module.model.decoder.layers.11.final_layer_norm # torch/nn/functional.py:2048:0
  %input : Float(17:1024, 13:17408, 1024:1) = aten::transpose(%x, %35, %39), scope: __module.model/__module.model.decoder # transformers/modeling_bart.py:601:0
  %3308 : (Float(17:1024, 13:17408, 1024:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%input, %encoder_hidden_states)
  %9 : Float(17:1024, 13:17408, 1024:1), %10 : Float(17:1024, 13:17408, 1024:1) = prim::TupleUnpack(%3308)
  %11 : Float(1024:1, 50265:1024) = aten::t(%7) # torch/nn/functional.py:1676:0
  %output : Float(17:653445, 13:50265, 50265:1) = aten::matmul(%9, %11) # torch/nn/functional.py:1676:0
  %13 : int = prim::Constant[value=1]() # torch/nn/functional.py:1678:0
  %14 : Float(17:653445, 13:50265, 50265:1) = aten::add_(%output, %3, %13) # torch/nn/functional.py:1678:0
  %15 : (Float(17:653445, 13:50265, 50265:1), Float(17:1024, 13:17408, 1024:1)) = prim::TupleConstruct(%14, %10)
  return (%15)
