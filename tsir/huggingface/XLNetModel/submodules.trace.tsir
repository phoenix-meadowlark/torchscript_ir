XLNetModel(
  (word_embedding): Embedding(32000, 1024)
  (layer): ModuleList(
    (0): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (1): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (2): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (3): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (4): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (5): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (6): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (7): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (8): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (9): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (10): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (11): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (12): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (13): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (14): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (15): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (16): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (17): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (18): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (19): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (20): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (21): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (22): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (23): XLNetLayer(
      (rel_attn): XLNetRelativeAttention(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (ff): XLNetFeedForward(
        (layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)
        (layer_1): Linear(in_features=1024, out_features=4096, bias=True)
        (layer_2): Linear(in_features=4096, out_features=1024, bias=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (dropout): Dropout(p=0.1, inplace=False)
)

XLNetModel._actual_script_module
XLNetModel.forward
  graph(%self.1 : __torch__.transformers.modeling_xlnet.XLNetModel,
        %input_ids.1 : Long(17:13, 13:1),
        %attention_mask : Long(17:13, 13:1)):
    %6260 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6261 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="23"](%6260)
    %6235 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6236 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="22"](%6235)
    %6210 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6211 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="21"](%6210)
    %6185 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6186 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="20"](%6185)
    %6160 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6161 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="19"](%6160)
    %6135 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6136 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="18"](%6135)
    %6110 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6111 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="17"](%6110)
    %6085 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6086 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="16"](%6085)
    %6060 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6061 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="15"](%6060)
    %6035 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6036 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="14"](%6035)
    %6010 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %6011 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="13"](%6010)
    %5985 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5986 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="12"](%5985)
    %5960 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5961 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="11"](%5960)
    %5935 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5936 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="10"](%5935)
    %5910 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5911 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="9"](%5910)
    %5885 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5886 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="8"](%5885)
    %5860 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5861 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="7"](%5860)
    %5835 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5836 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="6"](%5835)
    %5810 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5811 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="5"](%5810)
    %5785 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5786 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="4"](%5785)
    %5760 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5761 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="3"](%5760)
    %5735 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5736 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="2"](%5735)
    %5710 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5711 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="1"](%5710)
    %5685 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="layer"](%self.1)
    %5686 : __torch__.transformers.modeling_xlnet.XLNetLayer = prim::GetAttr[name="0"](%5685)
    %5661 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.1)
    %5660 : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name="word_embedding"](%self.1)
    %900 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1101:0
    %901 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1101:0
    %902 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %900, %901) # transformers/modeling_xlnet.py:1101:0
    %903 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1101:0
    %input_ids : Long(13:17, 17:1) = aten::contiguous(%902, %903) # transformers/modeling_xlnet.py:1101:0
    %905 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1102:0
    %906 : int = aten::size(%input_ids, %905) # transformers/modeling_xlnet.py:1102:0
    %qlen : Long() = prim::NumToTensor(%906)
    %954 : int = aten::Int(%qlen)
    %914 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1102:0
    %915 : int = aten::size(%input_ids, %914) # transformers/modeling_xlnet.py:1102:0
    %bsz : Long() = prim::NumToTensor(%915)
    %1042 : int = aten::Int(%bsz)
    %917 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1111:0
    %918 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1111:0
    %919 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %917, %918) # transformers/modeling_xlnet.py:1111:0
    %920 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1111:0
    %921 : Long(13:17, 17:1) = aten::contiguous(%919, %920) # transformers/modeling_xlnet.py:1111:0
    %922 : Long() = prim::Constant[value={0}]() # transformers/modeling_xlnet.py:1116:0
    %923 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1116:0
    %klen.1 : Long() = aten::add(%qlen, %922, %923) # transformers/modeling_xlnet.py:1116:0
    %1014 : Scalar = aten::ScalarImplicit(%klen.1)
    %925 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
    %926 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
    %input_mask : Float(13:17, 17:1) = aten::rsub(%921, %925, %926) # torch/tensor.py:396:0
    %928 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1139:0
    %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %928) # transformers/modeling_xlnet.py:1139:0
    %930 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
    %931 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
    %932 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
    %933 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
    %934 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %930, %931, %932, %933) # transformers/modeling_xlnet.py:1151:0
    %935 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
    %936 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
    %937 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
    %938 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
    %939 : Float(1:221, 13:17, 17:1) = aten::slice(%934, %935, %936, %937, %938) # transformers/modeling_xlnet.py:1151:0
    %940 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1151:0
    %941 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
    %942 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
    %943 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
    %944 : Float(1:221, 13:17, 17:1) = aten::slice(%939, %940, %941, %942, %943) # transformers/modeling_xlnet.py:1151:0
    %945 : int = prim::Constant[value=3]() # transformers/modeling_xlnet.py:1151:0
    %946 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%944, %945) # transformers/modeling_xlnet.py:1151:0
    %947 : int = prim::Constant[value=0]() # torch/tensor.py:22:0
    %948 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%946, %947) # torch/tensor.py:22:0
    %949 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1156:0
    %950 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1156:0
    %951 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1156:0
    %952 : None = prim::Constant()
    %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%948, %949, %950, %951, %952) # transformers/modeling_xlnet.py:1156:0
    %955 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1159:0
    %956 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
    %957 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1159:0
    %958 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
    %959 : Float(13:13, 13:1) = aten::eye(%954, %955, %956, %957, %958) # transformers/modeling_xlnet.py:1159:0
    %960 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1159:0
    %961 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1159:0
    %962 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
    %963 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
    %964 : None = prim::Constant()
    %965 : Float(13:13, 13:1) = aten::to(%959, %960, %961, %962, %963, %964) # transformers/modeling_xlnet.py:1159:0
    %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%965) # transformers/modeling_xlnet.py:1159:0
    %967 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
    %968 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
    %969 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1162:0
    %970 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
    %971 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %967, %968, %969, %970) # transformers/modeling_xlnet.py:1162:0
    %972 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
    %973 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
    %974 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1162:0
    %975 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
    %976 : Float(13:13, 13:1) = aten::slice(%971, %972, %973, %974, %975) # transformers/modeling_xlnet.py:1162:0
    %977 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1162:0
    %978 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%976, %977) # transformers/modeling_xlnet.py:1162:0
    %979 : int = prim::Constant[value=3]() # transformers/modeling_xlnet.py:1162:0
    %980 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%978, %979) # transformers/modeling_xlnet.py:1162:0
    %981 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
    %982 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %980, %981) # transformers/modeling_xlnet.py:1162:0
    %983 : int = prim::Constant[value=0]() # torch/tensor.py:22:0
    %984 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%982, %983) # torch/tensor.py:22:0
    %985 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1162:0
    %986 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1162:0
    %987 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
    %988 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
    %989 : None = prim::Constant()
    %990 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%984, %985, %986, %987, %988, %989) # transformers/modeling_xlnet.py:1162:0
    %6760 : Tensor = prim::CallMethod[name="forward"](%5660, %input_ids)
    %6761 : Tensor = prim::CallMethod[name="forward"](%5661, %6760)
    %998 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
    %999 : int = prim::Constant[value=1024]() # transformers/modeling_xlnet.py:1028:0
    %1000 : float = prim::Constant[value=2.]() # transformers/modeling_xlnet.py:1028:0
    %1001 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1028:0
    %1002 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
    %1003 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1028:0
    %1004 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
    %freq_seq : Float(512:1) = aten::arange(%998, %999, %1000, %1001, %1002, %1003, %1004) # transformers/modeling_xlnet.py:1028:0
    %1006 : Long() = prim::Constant[value={1024}]() # transformers/modeling_xlnet.py:1029:0
    %1007 : Float(512:1) = aten::div(%freq_seq, %1006) # transformers/modeling_xlnet.py:1029:0
    %1008 : int = prim::Constant[value=10000]() # transformers/modeling_xlnet.py:1029:0
    %1009 : Float(512:1) = aten::pow(%1008, %1007) # transformers/modeling_xlnet.py:1029:0
    %1010 : Float(512:1) = aten::reciprocal(%1009) # torch/tensor.py:400:0
    %1011 : Long() = prim::Constant[value={1}]() # torch/tensor.py:400:0
    %1012 : Float(512:1) = aten::mul(%1010, %1011) # torch/tensor.py:400:0
    %end : Long() = aten::neg(%qlen) # transformers/modeling_xlnet.py:1033:0
    %1015 : Scalar = aten::ScalarImplicit(%end)
    %1016 : float = prim::Constant[value=-1.]() # transformers/modeling_xlnet.py:1057:0
    %1017 : None = prim::Constant()
    %1018 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1057:0
    %1019 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1057:0
    %1020 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1057:0
    %1021 : Float(26:1) = aten::arange(%1014, %1015, %1016, %1017, %1018, %1019, %1020) # transformers/modeling_xlnet.py:1057:0
    %1022 : str = prim::Constant[value="i,d->id"]() # torch/functional.py:327:0
    %1023 : Tensor[] = prim::ListConstruct(%1021, %1012)
    %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%1022, %1023) # torch/functional.py:327:0
    %1025 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp) # transformers/modeling_xlnet.py:1018:0
    %1026 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp) # transformers/modeling_xlnet.py:1018:0
    %1027 : Tensor[] = prim::ListConstruct(%1025, %1026)
    %1028 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1018:0
    %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%1027, %1028) # transformers/modeling_xlnet.py:1018:0
    %1030 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
    %1031 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
    %1032 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1019:0
    %1033 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
    %1034 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %1030, %1031, %1032, %1033) # transformers/modeling_xlnet.py:1019:0
    %1035 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
    %1036 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%1034, %1035) # transformers/modeling_xlnet.py:1019:0
    %1037 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1019:0
    %1038 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
    %1039 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1019:0
    %1040 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
    %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%1036, %1037, %1038, %1039, %1040) # transformers/modeling_xlnet.py:1019:0
    %1043 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1022:0
    %1044 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1022:0
    %1045 : int[] = prim::ListConstruct(%1043, %1042, %1044)
    %1046 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1022:0
    %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %1045, %1046) # transformers/modeling_xlnet.py:1022:0
    %1048 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1062:0
    %1049 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
    %1050 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1062:0
    %1051 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
    %1052 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
    %1053 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
    %1054 : None = prim::Constant()
    %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %1048, %1049, %1050, %1051, %1052, %1053, %1054) # transformers/modeling_xlnet.py:1062:0
    %6762 : Tensor = prim::CallMethod[name="forward1"](%5661, %input.2)
    %1059 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1060 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1061 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1062 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6761, %1059, %1060, %1061, %1062) # transformers/modeling_xlnet.py:1009:0
    %1064 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1) # transformers/modeling_xlnet.py:1013:0
    %6763 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5686, %6761, %6762, %990)
    %6582 : Float(13:17408, 17:1024, 1024:1), %6583 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6763)
    %1239 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1240 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1241 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1242 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6582, %1239, %1240, %1241, %1242) # transformers/modeling_xlnet.py:1009:0
    %1244 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2) # transformers/modeling_xlnet.py:1013:0
    %6764 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5711, %6582, %6583, %990)
    %6590 : Float(13:17408, 17:1024, 1024:1), %6591 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6764)
    %1419 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1420 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1421 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1422 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6590, %1419, %1420, %1421, %1422) # transformers/modeling_xlnet.py:1009:0
    %1424 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3) # transformers/modeling_xlnet.py:1013:0
    %6765 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5736, %6590, %6591, %990)
    %6598 : Float(13:17408, 17:1024, 1024:1), %6599 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6765)
    %1599 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1600 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1601 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1602 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6598, %1599, %1600, %1601, %1602) # transformers/modeling_xlnet.py:1009:0
    %1604 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4) # transformers/modeling_xlnet.py:1013:0
    %6766 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5761, %6598, %6599, %990)
    %6606 : Float(13:17408, 17:1024, 1024:1), %6607 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6766)
    %1779 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1780 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1781 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1782 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6606, %1779, %1780, %1781, %1782) # transformers/modeling_xlnet.py:1009:0
    %1784 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5) # transformers/modeling_xlnet.py:1013:0
    %6767 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5786, %6606, %6607, %990)
    %6614 : Float(13:17408, 17:1024, 1024:1), %6615 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6767)
    %1959 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1960 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %1961 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %1962 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6614, %1959, %1960, %1961, %1962) # transformers/modeling_xlnet.py:1009:0
    %1964 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6) # transformers/modeling_xlnet.py:1013:0
    %6768 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5811, %6614, %6615, %990)
    %6622 : Float(13:17408, 17:1024, 1024:1), %6623 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6768)
    %2139 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2140 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2141 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %2142 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6622, %2139, %2140, %2141, %2142) # transformers/modeling_xlnet.py:1009:0
    %2144 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7) # transformers/modeling_xlnet.py:1013:0
    %6769 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5836, %6622, %6623, %990)
    %6630 : Float(13:17408, 17:1024, 1024:1), %6631 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6769)
    %2319 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2320 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2321 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %2322 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6630, %2319, %2320, %2321, %2322) # transformers/modeling_xlnet.py:1009:0
    %2324 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8) # transformers/modeling_xlnet.py:1013:0
    %6770 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5861, %6630, %6631, %990)
    %6638 : Float(13:17408, 17:1024, 1024:1), %6639 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6770)
    %2499 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2500 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2501 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %2502 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6638, %2499, %2500, %2501, %2502) # transformers/modeling_xlnet.py:1009:0
    %2504 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9) # transformers/modeling_xlnet.py:1013:0
    %6771 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5886, %6638, %6639, %990)
    %6646 : Float(13:17408, 17:1024, 1024:1), %6647 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6771)
    %2679 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2680 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2681 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %2682 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6646, %2679, %2680, %2681, %2682) # transformers/modeling_xlnet.py:1009:0
    %2684 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10) # transformers/modeling_xlnet.py:1013:0
    %6772 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5911, %6646, %6647, %990)
    %6654 : Float(13:17408, 17:1024, 1024:1), %6655 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6772)
    %2859 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2860 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %2861 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %2862 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6654, %2859, %2860, %2861, %2862) # transformers/modeling_xlnet.py:1009:0
    %2864 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11) # transformers/modeling_xlnet.py:1013:0
    %6773 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5936, %6654, %6655, %990)
    %6662 : Float(13:17408, 17:1024, 1024:1), %6663 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6773)
    %3039 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3040 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3041 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3042 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6662, %3039, %3040, %3041, %3042) # transformers/modeling_xlnet.py:1009:0
    %3044 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12) # transformers/modeling_xlnet.py:1013:0
    %6774 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5961, %6662, %6663, %990)
    %6670 : Float(13:17408, 17:1024, 1024:1), %6671 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6774)
    %3219 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3220 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3221 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3222 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6670, %3219, %3220, %3221, %3222) # transformers/modeling_xlnet.py:1009:0
    %3224 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13) # transformers/modeling_xlnet.py:1013:0
    %6775 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%5986, %6670, %6671, %990)
    %6678 : Float(13:17408, 17:1024, 1024:1), %6679 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6775)
    %3399 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3400 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3401 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3402 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6678, %3399, %3400, %3401, %3402) # transformers/modeling_xlnet.py:1009:0
    %3404 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14) # transformers/modeling_xlnet.py:1013:0
    %6776 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6011, %6678, %6679, %990)
    %6686 : Float(13:17408, 17:1024, 1024:1), %6687 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6776)
    %3579 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3580 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3581 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3582 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6686, %3579, %3580, %3581, %3582) # transformers/modeling_xlnet.py:1009:0
    %3584 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15) # transformers/modeling_xlnet.py:1013:0
    %6777 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6036, %6686, %6687, %990)
    %6694 : Float(13:17408, 17:1024, 1024:1), %6695 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6777)
    %3759 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3760 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3761 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3762 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6694, %3759, %3760, %3761, %3762) # transformers/modeling_xlnet.py:1009:0
    %3764 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16) # transformers/modeling_xlnet.py:1013:0
    %6778 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6061, %6694, %6695, %990)
    %6702 : Float(13:17408, 17:1024, 1024:1), %6703 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6778)
    %3939 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3940 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %3941 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %3942 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6702, %3939, %3940, %3941, %3942) # transformers/modeling_xlnet.py:1009:0
    %3944 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17) # transformers/modeling_xlnet.py:1013:0
    %6779 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6086, %6702, %6703, %990)
    %6710 : Float(13:17408, 17:1024, 1024:1), %6711 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6779)
    %4119 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4120 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4121 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %4122 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6710, %4119, %4120, %4121, %4122) # transformers/modeling_xlnet.py:1009:0
    %4124 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18) # transformers/modeling_xlnet.py:1013:0
    %6780 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6111, %6710, %6711, %990)
    %6718 : Float(13:17408, 17:1024, 1024:1), %6719 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6780)
    %4299 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4300 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4301 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %4302 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6718, %4299, %4300, %4301, %4302) # transformers/modeling_xlnet.py:1009:0
    %4304 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19) # transformers/modeling_xlnet.py:1013:0
    %6781 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6136, %6718, %6719, %990)
    %6726 : Float(13:17408, 17:1024, 1024:1), %6727 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6781)
    %4479 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4480 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4481 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %4482 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6726, %4479, %4480, %4481, %4482) # transformers/modeling_xlnet.py:1009:0
    %4484 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20) # transformers/modeling_xlnet.py:1013:0
    %6782 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6161, %6726, %6727, %990)
    %6734 : Float(13:17408, 17:1024, 1024:1), %6735 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6782)
    %4659 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4660 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4661 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %4662 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6734, %4659, %4660, %4661, %4662) # transformers/modeling_xlnet.py:1009:0
    %4664 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21) # transformers/modeling_xlnet.py:1013:0
    %6783 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6186, %6734, %6735, %990)
    %6742 : Float(13:17408, 17:1024, 1024:1), %6743 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6783)
    %4839 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4840 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %4841 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %4842 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6742, %4839, %4840, %4841, %4842) # transformers/modeling_xlnet.py:1009:0
    %4844 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22) # transformers/modeling_xlnet.py:1013:0
    %6784 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6211, %6742, %6743, %990)
    %6750 : Float(13:17408, 17:1024, 1024:1), %6751 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6784)
    %5019 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %5020 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %5021 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %5022 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6750, %5019, %5020, %5021, %5022) # transformers/modeling_xlnet.py:1009:0
    %5024 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23) # transformers/modeling_xlnet.py:1013:0
    %6785 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%6236, %6750, %6751, %990)
    %6758 : Float(13:17408, 17:1024, 1024:1), %6759 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%6785)
    %5199 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %5200 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
    %5201 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
    %5202 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
    %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%6758, %5199, %5200, %5201, %5202) # transformers/modeling_xlnet.py:1009:0
    %5204 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem) # transformers/modeling_xlnet.py:1013:0
    %6786 : Tensor = prim::CallMethod[name="forward"](%6261, %6758, %6759, %990)
    %6787 : Tensor = prim::CallMethod[name="forward2"](%5661, %6786)
    %5382 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1253:0
    %5383 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1253:0
    %5384 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1253:0
    %5385 : int[] = prim::ListConstruct(%5382, %5383, %5384)
    %5386 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%6787, %5385) # transformers/modeling_xlnet.py:1253:0
    %5387 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1253:0
    %5388 : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%5386, %5387) # transformers/modeling_xlnet.py:1253:0
    %5389 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%1064, %1244, %1424, %1604, %1784, %1964, %2144, %2324, %2504, %2684, %2864, %3044, %3224, %3404, %3584, %3764, %3944, %4124, %4304, %4484, %4664, %4844, %5024, %5204)
    %5390 : (Float(17:13312, 13:1024, 1024:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%5388, %5389)
    return (%5390)

XLNetModel.dropout
Dropout._actual_script_module
  graph(%self.3 : __torch__.torch.nn.modules.dropout.Dropout,
        %4 : Float(13:17408, 17:1024, 1024:1)):
    %1 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
    %2 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
    %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%4, %1, %2), scope: __module.dropout # torch/nn/functional.py:973:0
    return (%curr_out.1)

XLNetModel.word_embedding
Embedding._actual_script_module
  graph(%self.2 : __torch__.torch.nn.modules.sparse.Embedding,
        %input_ids : Long(13:17, 17:1)):
    %1 : Tensor = prim::GetAttr[name="weight"](%self.2)
    %2 : int = prim::Constant[value=-1](), scope: __module.word_embedding # torch/nn/functional.py:1814:0
    %3 : bool = prim::Constant[value=0](), scope: __module.word_embedding # torch/nn/functional.py:1814:0
    %4 : bool = prim::Constant[value=0](), scope: __module.word_embedding # torch/nn/functional.py:1814:0
    %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%1, %input_ids, %2, %3, %4), scope: __module.word_embedding # torch/nn/functional.py:1814:0
    return (%input.1)

ModuleList.*
Dropout.*
  module had no methods with graph attrs.

XLNetLayer._actual_script_module
  graph(%self.5 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.5)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.5)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.10 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.10)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.10)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.10)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.10)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.0/__module.layer.0.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.8)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.0/__module.layer.0.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.11)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.6 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.6)
    %5 : Tensor = prim::GetAttr[name="o"](%self.6)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.6)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.6)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.6)
    %9 : Tensor = prim::GetAttr[name="r"](%self.6)
    %10 : Tensor = prim::GetAttr[name="v"](%self.6)
    %11 : Tensor = prim::GetAttr[name="k"](%self.6)
    %12 : Tensor = prim::GetAttr[name="q"](%self.6)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.0/__module.layer.0.rel_attn
    %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.0/__module.layer.0.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.0/__module.layer.0.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.0/__module.layer.0.rel_attn
    %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.2, %9), scope: __module.layer.0/__module.layer.0.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %8, %31), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.0/__module.layer.0.rel_attn
    %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %7, %36), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.0/__module.layer.0.rel_attn
    %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.1, %50), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.2 : Long() = prim::NumToTensor(%51), scope: __module.layer.0/__module.layer.0.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.2), scope: __module.layer.0/__module.layer.0.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.1, %54), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.0/__module.layer.0.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.0/__module.layer.0.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.0/__module.layer.0.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.1, %59), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.0/__module.layer.0.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.0/__module.layer.0.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.0/__module.layer.0.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.1, %64), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.0/__module.layer.0.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.0/__module.layer.0.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.0/__module.layer.0.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.1, %69), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.0/__module.layer.0.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.0/__module.layer.0.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.0/__module.layer.0.rel_attn
    %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %73), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %75, %76, %77, %78), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.0/__module.layer.0.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.0/__module.layer.0.rel_attn
    %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %99), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %106, %105), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %108), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.0/__module.layer.0.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %119, %120), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.0/__module.layer.0.rel_attn
    %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %122, %123), scope: __module.layer.0/__module.layer.0.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.4)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.0/__module.layer.0.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.0/__module.layer.0.rel_attn
    %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.5)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.6)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.2)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.7 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.4 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %2, %3), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.9 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.6 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.9)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.9)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %5, %3, %2, %6, %7), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.1)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.12 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.8 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.dropout # torch/nn/functional.py:973:0
    %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %2, %3), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.dropout # torch/nn/functional.py:973:0
    return (%input.9)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.11 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.11)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.11)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %2, %6), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.7)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.13 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.13)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.13)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %2, %6), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.10)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.15 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.11 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.15)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.15)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %5, %3, %2, %6, %7), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.2)

XLNetLayer._actual_script_module
  graph(%self.16 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.16)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.16)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.21 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.21)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.21)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.21)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.21)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.1/__module.layer.1.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.17)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.1/__module.layer.1.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.20)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.17 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.17)
    %5 : Tensor = prim::GetAttr[name="o"](%self.17)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.17)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.17)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.17)
    %9 : Tensor = prim::GetAttr[name="r"](%self.17)
    %10 : Tensor = prim::GetAttr[name="v"](%self.17)
    %11 : Tensor = prim::GetAttr[name="k"](%self.17)
    %12 : Tensor = prim::GetAttr[name="q"](%self.17)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.1/__module.layer.1.rel_attn
    %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.1/__module.layer.1.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.1/__module.layer.1.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.1/__module.layer.1.rel_attn
    %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.3, %9), scope: __module.layer.1/__module.layer.1.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %8, %31), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.1/__module.layer.1.rel_attn
    %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %7, %36), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.1/__module.layer.1.rel_attn
    %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.2, %50), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.3 : Long() = prim::NumToTensor(%51), scope: __module.layer.1/__module.layer.1.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.3), scope: __module.layer.1/__module.layer.1.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.5, %54), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.1/__module.layer.1.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.1/__module.layer.1.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.1/__module.layer.1.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.5, %59), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.1/__module.layer.1.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.1/__module.layer.1.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.1/__module.layer.1.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.5, %64), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.1/__module.layer.1.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.1/__module.layer.1.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.1/__module.layer.1.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.5, %69), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.1/__module.layer.1.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.1/__module.layer.1.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.1/__module.layer.1.rel_attn
    %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %73), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %75, %76, %77, %78), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.1/__module.layer.1.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.1/__module.layer.1.rel_attn
    %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %99), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %106, %105), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %108), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.1/__module.layer.1.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %119, %120), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.1/__module.layer.1.rel_attn
    %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %122, %123), scope: __module.layer.1/__module.layer.1.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.13)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.1/__module.layer.1.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.1/__module.layer.1.rel_attn
    %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.14)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.15)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.3)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.18 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.13 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %2, %3), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.20 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.15 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.20)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.20)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %5, %3, %2, %6, %7), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.2)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.23 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.17 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.dropout # torch/nn/functional.py:973:0
    %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %2, %3), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.dropout # torch/nn/functional.py:973:0
    return (%input.18)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.22 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.22)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.22)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %2, %6), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.16)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.24 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.24)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.24)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %2, %6), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.19)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.26 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.20 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.26)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.26)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %5, %3, %2, %6, %7), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.3)

XLNetLayer._actual_script_module
  graph(%self.27 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.27)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.27)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.32 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.32)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.32)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.32)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.32)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.2/__module.layer.2.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.26)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.2/__module.layer.2.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.29)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.28 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.28)
    %5 : Tensor = prim::GetAttr[name="o"](%self.28)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.28)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.28)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.28)
    %9 : Tensor = prim::GetAttr[name="r"](%self.28)
    %10 : Tensor = prim::GetAttr[name="v"](%self.28)
    %11 : Tensor = prim::GetAttr[name="k"](%self.28)
    %12 : Tensor = prim::GetAttr[name="q"](%self.28)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.2/__module.layer.2.rel_attn
    %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.2/__module.layer.2.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.2/__module.layer.2.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.2/__module.layer.2.rel_attn
    %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.4, %9), scope: __module.layer.2/__module.layer.2.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %8, %31), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.2/__module.layer.2.rel_attn
    %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %7, %36), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.2/__module.layer.2.rel_attn
    %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.3, %50), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.4 : Long() = prim::NumToTensor(%51), scope: __module.layer.2/__module.layer.2.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.4), scope: __module.layer.2/__module.layer.2.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.9, %54), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.2/__module.layer.2.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.2/__module.layer.2.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.2/__module.layer.2.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.9, %59), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.2/__module.layer.2.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.2/__module.layer.2.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.2/__module.layer.2.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.9, %64), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.2/__module.layer.2.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.2/__module.layer.2.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.2/__module.layer.2.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.9, %69), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.2/__module.layer.2.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.2/__module.layer.2.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.2/__module.layer.2.rel_attn
    %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %73), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %75, %76, %77, %78), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.2/__module.layer.2.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.2/__module.layer.2.rel_attn
    %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %99), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %106, %105), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %108), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.2/__module.layer.2.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %119, %120), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.2/__module.layer.2.rel_attn
    %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %122, %123), scope: __module.layer.2/__module.layer.2.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.22)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.2/__module.layer.2.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.2/__module.layer.2.rel_attn
    %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.23)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.24)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.4)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.29 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.22 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %2, %3), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.31 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.24 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.31)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.31)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %5, %3, %2, %6, %7), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.3)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.34 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.26 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.dropout # torch/nn/functional.py:973:0
    %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %2, %3), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.dropout # torch/nn/functional.py:973:0
    return (%input.27)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.33 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.33)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.33)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %2, %6), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.25)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.35 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.35)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.35)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %2, %6), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.28)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.37 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.29 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.37)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.37)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %5, %3, %2, %6, %7), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.4)

XLNetLayer._actual_script_module
  graph(%self.38 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.38)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.38)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.43 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.43)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.43)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.43)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.43)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.3/__module.layer.3.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.35)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.3/__module.layer.3.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.38)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.39 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.39)
    %5 : Tensor = prim::GetAttr[name="o"](%self.39)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.39)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.39)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.39)
    %9 : Tensor = prim::GetAttr[name="r"](%self.39)
    %10 : Tensor = prim::GetAttr[name="v"](%self.39)
    %11 : Tensor = prim::GetAttr[name="k"](%self.39)
    %12 : Tensor = prim::GetAttr[name="q"](%self.39)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.3/__module.layer.3.rel_attn
    %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.3/__module.layer.3.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.3/__module.layer.3.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.3/__module.layer.3.rel_attn
    %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.5, %9), scope: __module.layer.3/__module.layer.3.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %8, %31), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.3/__module.layer.3.rel_attn
    %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %7, %36), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.3/__module.layer.3.rel_attn
    %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.4, %50), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.5 : Long() = prim::NumToTensor(%51), scope: __module.layer.3/__module.layer.3.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.5), scope: __module.layer.3/__module.layer.3.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.13, %54), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.3/__module.layer.3.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.3/__module.layer.3.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.3/__module.layer.3.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.13, %59), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.3/__module.layer.3.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.3/__module.layer.3.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.3/__module.layer.3.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.13, %64), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.3/__module.layer.3.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.3/__module.layer.3.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.3/__module.layer.3.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.13, %69), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.3/__module.layer.3.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.3/__module.layer.3.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.3/__module.layer.3.rel_attn
    %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %73), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %75, %76, %77, %78), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.3/__module.layer.3.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.3/__module.layer.3.rel_attn
    %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %99), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %106, %105), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %108), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.3/__module.layer.3.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %119, %120), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.3/__module.layer.3.rel_attn
    %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %122, %123), scope: __module.layer.3/__module.layer.3.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.31)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.3/__module.layer.3.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.3/__module.layer.3.rel_attn
    %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.32)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.33)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.5)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.40 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.31 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %2, %3), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.42 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.33 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.42)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.42)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %5, %3, %2, %6, %7), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.4)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.45 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.35 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.dropout # torch/nn/functional.py:973:0
    %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %2, %3), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.dropout # torch/nn/functional.py:973:0
    return (%input.36)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.44 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.44)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.44)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %2, %6), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.34)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.46 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.46)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.46)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %2, %6), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.37)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.48 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.38 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.48)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.48)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %5, %3, %2, %6, %7), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.5)

XLNetLayer._actual_script_module
  graph(%self.49 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.49)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.49)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.54 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.54)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.54)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.54)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.54)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.4/__module.layer.4.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.44)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.4/__module.layer.4.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.47)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.50 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.50)
    %5 : Tensor = prim::GetAttr[name="o"](%self.50)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.50)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.50)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.50)
    %9 : Tensor = prim::GetAttr[name="r"](%self.50)
    %10 : Tensor = prim::GetAttr[name="v"](%self.50)
    %11 : Tensor = prim::GetAttr[name="k"](%self.50)
    %12 : Tensor = prim::GetAttr[name="q"](%self.50)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.4/__module.layer.4.rel_attn
    %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.4/__module.layer.4.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.4/__module.layer.4.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.4/__module.layer.4.rel_attn
    %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.6, %9), scope: __module.layer.4/__module.layer.4.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %8, %31), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.4/__module.layer.4.rel_attn
    %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %7, %36), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.4/__module.layer.4.rel_attn
    %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.5, %50), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.6 : Long() = prim::NumToTensor(%51), scope: __module.layer.4/__module.layer.4.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.6), scope: __module.layer.4/__module.layer.4.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.17, %54), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.4/__module.layer.4.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.4/__module.layer.4.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.4/__module.layer.4.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.17, %59), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.4/__module.layer.4.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.4/__module.layer.4.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.4/__module.layer.4.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.17, %64), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.4/__module.layer.4.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.4/__module.layer.4.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.4/__module.layer.4.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.17, %69), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.4/__module.layer.4.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.4/__module.layer.4.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.4/__module.layer.4.rel_attn
    %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %73), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %75, %76, %77, %78), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.4/__module.layer.4.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.4/__module.layer.4.rel_attn
    %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %99), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %106, %105), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %108), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.4/__module.layer.4.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %119, %120), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.4/__module.layer.4.rel_attn
    %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %122, %123), scope: __module.layer.4/__module.layer.4.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.40)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.4/__module.layer.4.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.4/__module.layer.4.rel_attn
    %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.41)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.42)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.6)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.51 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.40 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %2, %3), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.53 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.42 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.53)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.53)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %5, %3, %2, %6, %7), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.5)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.56 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.44 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.dropout # torch/nn/functional.py:973:0
    %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %2, %3), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.dropout # torch/nn/functional.py:973:0
    return (%input.45)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.55 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.55)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.55)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %2, %6), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.43)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.57 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.57)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.57)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %2, %6), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.46)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.59 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.47 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.59)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.59)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %5, %3, %2, %6, %7), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.6)

XLNetLayer._actual_script_module
  graph(%self.60 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.60)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.60)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.65 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.65)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.65)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.65)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.65)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.5/__module.layer.5.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.53)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.5/__module.layer.5.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.56)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.61 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.61)
    %5 : Tensor = prim::GetAttr[name="o"](%self.61)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.61)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.61)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.61)
    %9 : Tensor = prim::GetAttr[name="r"](%self.61)
    %10 : Tensor = prim::GetAttr[name="v"](%self.61)
    %11 : Tensor = prim::GetAttr[name="k"](%self.61)
    %12 : Tensor = prim::GetAttr[name="q"](%self.61)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.5/__module.layer.5.rel_attn
    %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.5/__module.layer.5.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.5/__module.layer.5.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.5/__module.layer.5.rel_attn
    %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.7, %9), scope: __module.layer.5/__module.layer.5.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %8, %31), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.5/__module.layer.5.rel_attn
    %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %7, %36), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.5/__module.layer.5.rel_attn
    %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.6, %50), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.7 : Long() = prim::NumToTensor(%51), scope: __module.layer.5/__module.layer.5.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.7), scope: __module.layer.5/__module.layer.5.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.21, %54), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.5/__module.layer.5.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.5/__module.layer.5.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.5/__module.layer.5.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.21, %59), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.5/__module.layer.5.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.5/__module.layer.5.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.5/__module.layer.5.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.21, %64), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.5/__module.layer.5.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.5/__module.layer.5.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.5/__module.layer.5.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.21, %69), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.5/__module.layer.5.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.5/__module.layer.5.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.5/__module.layer.5.rel_attn
    %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %73), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %75, %76, %77, %78), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.5/__module.layer.5.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.5/__module.layer.5.rel_attn
    %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %99), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %106, %105), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %108), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.5/__module.layer.5.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %119, %120), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.5/__module.layer.5.rel_attn
    %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %122, %123), scope: __module.layer.5/__module.layer.5.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.49)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.5/__module.layer.5.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.5/__module.layer.5.rel_attn
    %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.50)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.51)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.7)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.62 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.49 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %2, %3), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.64 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.51 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.64)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.64)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %5, %3, %2, %6, %7), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.6)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.67 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.53 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.dropout # torch/nn/functional.py:973:0
    %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %2, %3), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.dropout # torch/nn/functional.py:973:0
    return (%input.54)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.66 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.66)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.66)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %2, %6), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.52)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.68 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.68)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.68)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %2, %6), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.55)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.70 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.56 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.70)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.70)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %5, %3, %2, %6, %7), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.7)

XLNetLayer._actual_script_module
  graph(%self.71 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.71)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.71)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.76 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.76)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.76)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.76)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.76)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.6/__module.layer.6.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.62)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.6/__module.layer.6.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.65)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.72 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.72)
    %5 : Tensor = prim::GetAttr[name="o"](%self.72)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.72)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.72)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.72)
    %9 : Tensor = prim::GetAttr[name="r"](%self.72)
    %10 : Tensor = prim::GetAttr[name="v"](%self.72)
    %11 : Tensor = prim::GetAttr[name="k"](%self.72)
    %12 : Tensor = prim::GetAttr[name="q"](%self.72)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.6/__module.layer.6.rel_attn
    %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.6/__module.layer.6.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.6/__module.layer.6.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.6/__module.layer.6.rel_attn
    %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.8, %9), scope: __module.layer.6/__module.layer.6.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %8, %31), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.6/__module.layer.6.rel_attn
    %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %7, %36), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.6/__module.layer.6.rel_attn
    %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.7, %50), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.8 : Long() = prim::NumToTensor(%51), scope: __module.layer.6/__module.layer.6.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.8), scope: __module.layer.6/__module.layer.6.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.25, %54), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.6/__module.layer.6.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.6/__module.layer.6.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.6/__module.layer.6.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.25, %59), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.6/__module.layer.6.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.6/__module.layer.6.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.6/__module.layer.6.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.25, %64), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.6/__module.layer.6.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.6/__module.layer.6.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.6/__module.layer.6.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.25, %69), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.6/__module.layer.6.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.6/__module.layer.6.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.6/__module.layer.6.rel_attn
    %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %73), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %75, %76, %77, %78), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.6/__module.layer.6.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.6/__module.layer.6.rel_attn
    %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %99), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %106, %105), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %108), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.6/__module.layer.6.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %119, %120), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.6/__module.layer.6.rel_attn
    %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %122, %123), scope: __module.layer.6/__module.layer.6.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.58)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.6/__module.layer.6.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.6/__module.layer.6.rel_attn
    %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.59)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.60)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.8)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.73 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.58 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %2, %3), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.75 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.60 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.75)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.75)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %5, %3, %2, %6, %7), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.7)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.78 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.62 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.dropout # torch/nn/functional.py:973:0
    %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %2, %3), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.dropout # torch/nn/functional.py:973:0
    return (%input.63)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.77 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.77)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.77)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %2, %6), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.61)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.79 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.79)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.79)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %2, %6), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.64)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.81 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.65 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.81)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.81)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %5, %3, %2, %6, %7), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.8)

XLNetLayer._actual_script_module
  graph(%self.82 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.82)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.82)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.87 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.87)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.87)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.87)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.87)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.7/__module.layer.7.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.71)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.7/__module.layer.7.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.74)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.83 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.83)
    %5 : Tensor = prim::GetAttr[name="o"](%self.83)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.83)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.83)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.83)
    %9 : Tensor = prim::GetAttr[name="r"](%self.83)
    %10 : Tensor = prim::GetAttr[name="v"](%self.83)
    %11 : Tensor = prim::GetAttr[name="k"](%self.83)
    %12 : Tensor = prim::GetAttr[name="q"](%self.83)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.7/__module.layer.7.rel_attn
    %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.7/__module.layer.7.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.7/__module.layer.7.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.7/__module.layer.7.rel_attn
    %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.9, %9), scope: __module.layer.7/__module.layer.7.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %8, %31), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.7/__module.layer.7.rel_attn
    %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %7, %36), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.7/__module.layer.7.rel_attn
    %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.8, %50), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.9 : Long() = prim::NumToTensor(%51), scope: __module.layer.7/__module.layer.7.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.9), scope: __module.layer.7/__module.layer.7.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.29, %54), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.7/__module.layer.7.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.7/__module.layer.7.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.7/__module.layer.7.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.29, %59), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.7/__module.layer.7.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.7/__module.layer.7.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.7/__module.layer.7.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.29, %64), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.7/__module.layer.7.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.7/__module.layer.7.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.7/__module.layer.7.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.29, %69), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.7/__module.layer.7.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.7/__module.layer.7.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.7/__module.layer.7.rel_attn
    %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %73), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %75, %76, %77, %78), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.7/__module.layer.7.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.7/__module.layer.7.rel_attn
    %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %99), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %106, %105), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %108), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.7/__module.layer.7.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %119, %120), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.7/__module.layer.7.rel_attn
    %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %122, %123), scope: __module.layer.7/__module.layer.7.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.67)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.7/__module.layer.7.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.7/__module.layer.7.rel_attn
    %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.68)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.69)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.9)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.84 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.67 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %2, %3), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.86 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.69 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.86)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.86)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %5, %3, %2, %6, %7), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.8)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.89 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.71 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.dropout # torch/nn/functional.py:973:0
    %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %2, %3), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.dropout # torch/nn/functional.py:973:0
    return (%input.72)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.88 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.88)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.88)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %2, %6), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.70)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.90 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.90)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.90)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %2, %6), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.73)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.92 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.74 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.92)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.92)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %5, %3, %2, %6, %7), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.9)

XLNetLayer._actual_script_module
  graph(%self.93 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.93)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.93)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.98 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.98)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.98)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.98)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.98)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.8/__module.layer.8.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.80)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.8/__module.layer.8.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.83)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.94 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.94)
    %5 : Tensor = prim::GetAttr[name="o"](%self.94)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.94)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.94)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.94)
    %9 : Tensor = prim::GetAttr[name="r"](%self.94)
    %10 : Tensor = prim::GetAttr[name="v"](%self.94)
    %11 : Tensor = prim::GetAttr[name="k"](%self.94)
    %12 : Tensor = prim::GetAttr[name="q"](%self.94)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.8/__module.layer.8.rel_attn
    %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.8/__module.layer.8.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.8/__module.layer.8.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.8/__module.layer.8.rel_attn
    %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.10, %9), scope: __module.layer.8/__module.layer.8.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %8, %31), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.8/__module.layer.8.rel_attn
    %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %7, %36), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.8/__module.layer.8.rel_attn
    %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.9, %50), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.10 : Long() = prim::NumToTensor(%51), scope: __module.layer.8/__module.layer.8.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.10), scope: __module.layer.8/__module.layer.8.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.33, %54), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.8/__module.layer.8.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.8/__module.layer.8.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.8/__module.layer.8.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.33, %59), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.8/__module.layer.8.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.8/__module.layer.8.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.8/__module.layer.8.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.33, %64), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.8/__module.layer.8.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.8/__module.layer.8.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.8/__module.layer.8.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.33, %69), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.8/__module.layer.8.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.8/__module.layer.8.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.8/__module.layer.8.rel_attn
    %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %73), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %75, %76, %77, %78), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.8/__module.layer.8.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.8/__module.layer.8.rel_attn
    %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %99), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %106, %105), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %108), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.8/__module.layer.8.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %119, %120), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.8/__module.layer.8.rel_attn
    %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %122, %123), scope: __module.layer.8/__module.layer.8.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.76)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.8/__module.layer.8.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.8/__module.layer.8.rel_attn
    %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.77)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.78)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.10)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.95 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.76 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %2, %3), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.97 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.78 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.97)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.97)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %5, %3, %2, %6, %7), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.9)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.100 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.80 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.dropout # torch/nn/functional.py:973:0
    %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %2, %3), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.dropout # torch/nn/functional.py:973:0
    return (%input.81)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.99 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.99)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.99)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %2, %6), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.79)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.101 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.101)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.101)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %2, %6), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.82)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.103 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.83 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.103)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.103)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %5, %3, %2, %6, %7), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.10)

XLNetLayer._actual_script_module
  graph(%self.104 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.104)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.104)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.109 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.109)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.109)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.109)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.109)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.9/__module.layer.9.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.89)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.9/__module.layer.9.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.92)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.105 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.105)
    %5 : Tensor = prim::GetAttr[name="o"](%self.105)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.105)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.105)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.105)
    %9 : Tensor = prim::GetAttr[name="r"](%self.105)
    %10 : Tensor = prim::GetAttr[name="v"](%self.105)
    %11 : Tensor = prim::GetAttr[name="k"](%self.105)
    %12 : Tensor = prim::GetAttr[name="q"](%self.105)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.9/__module.layer.9.rel_attn
    %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.9/__module.layer.9.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.9/__module.layer.9.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.9/__module.layer.9.rel_attn
    %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.11, %9), scope: __module.layer.9/__module.layer.9.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %8, %31), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.9/__module.layer.9.rel_attn
    %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %7, %36), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.9/__module.layer.9.rel_attn
    %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.10, %50), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.11 : Long() = prim::NumToTensor(%51), scope: __module.layer.9/__module.layer.9.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.11), scope: __module.layer.9/__module.layer.9.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.37, %54), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.9/__module.layer.9.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.9/__module.layer.9.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.9/__module.layer.9.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.37, %59), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.9/__module.layer.9.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.9/__module.layer.9.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.9/__module.layer.9.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.37, %64), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.9/__module.layer.9.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.9/__module.layer.9.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.9/__module.layer.9.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.37, %69), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.9/__module.layer.9.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.9/__module.layer.9.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.9/__module.layer.9.rel_attn
    %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %73), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %75, %76, %77, %78), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.9/__module.layer.9.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.9/__module.layer.9.rel_attn
    %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %99), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %106, %105), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %108), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.9/__module.layer.9.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %119, %120), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.9/__module.layer.9.rel_attn
    %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %122, %123), scope: __module.layer.9/__module.layer.9.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.85)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.9/__module.layer.9.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.9/__module.layer.9.rel_attn
    %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.86)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.87)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.11)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.106 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.85 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %2, %3), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.108 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.87 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.108)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.108)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %5, %3, %2, %6, %7), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.10)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.111 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.89 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.dropout # torch/nn/functional.py:973:0
    %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %2, %3), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.dropout # torch/nn/functional.py:973:0
    return (%input.90)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.110 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.110)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.110)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %2, %6), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.88)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.112 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.112)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.112)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %2, %6), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.91)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.114 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.92 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.114)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.114)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %5, %3, %2, %6, %7), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.11)

XLNetLayer._actual_script_module
  graph(%self.115 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.115)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.115)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.120 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.120)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.120)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.120)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.120)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.10/__module.layer.10.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.98)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.10/__module.layer.10.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.101)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.116 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.116)
    %5 : Tensor = prim::GetAttr[name="o"](%self.116)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.116)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.116)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.116)
    %9 : Tensor = prim::GetAttr[name="r"](%self.116)
    %10 : Tensor = prim::GetAttr[name="v"](%self.116)
    %11 : Tensor = prim::GetAttr[name="k"](%self.116)
    %12 : Tensor = prim::GetAttr[name="q"](%self.116)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.10/__module.layer.10.rel_attn
    %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.10/__module.layer.10.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.10/__module.layer.10.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.10/__module.layer.10.rel_attn
    %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.12, %9), scope: __module.layer.10/__module.layer.10.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %8, %31), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.10/__module.layer.10.rel_attn
    %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %7, %36), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.10/__module.layer.10.rel_attn
    %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.11, %50), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.12 : Long() = prim::NumToTensor(%51), scope: __module.layer.10/__module.layer.10.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.12), scope: __module.layer.10/__module.layer.10.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.41, %54), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.10/__module.layer.10.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.10/__module.layer.10.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.10/__module.layer.10.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.41, %59), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.10/__module.layer.10.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.10/__module.layer.10.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.10/__module.layer.10.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.41, %64), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.10/__module.layer.10.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.10/__module.layer.10.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.10/__module.layer.10.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.41, %69), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.10/__module.layer.10.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.10/__module.layer.10.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.10/__module.layer.10.rel_attn
    %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %73), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %75, %76, %77, %78), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.10/__module.layer.10.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.10/__module.layer.10.rel_attn
    %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %99), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %106, %105), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %108), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.10/__module.layer.10.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %119, %120), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.10/__module.layer.10.rel_attn
    %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %122, %123), scope: __module.layer.10/__module.layer.10.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.94)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.10/__module.layer.10.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.10/__module.layer.10.rel_attn
    %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.95)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.96)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.12)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.117 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.94 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %2, %3), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.119 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.96 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.119)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.119)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %5, %3, %2, %6, %7), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.11)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.122 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.98 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.dropout # torch/nn/functional.py:973:0
    %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %2, %3), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.dropout # torch/nn/functional.py:973:0
    return (%input.99)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.121 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.121)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.121)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %2, %6), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.97)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.123 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.123)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.123)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %2, %6), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.100)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.125 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.101 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.125)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.125)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %5, %3, %2, %6, %7), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.12)

XLNetLayer._actual_script_module
  graph(%self.126 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.126)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.126)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.131 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.131)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.131)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.131)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.131)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.11/__module.layer.11.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.107)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.11/__module.layer.11.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.110)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.127 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.127)
    %5 : Tensor = prim::GetAttr[name="o"](%self.127)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.127)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.127)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.127)
    %9 : Tensor = prim::GetAttr[name="r"](%self.127)
    %10 : Tensor = prim::GetAttr[name="v"](%self.127)
    %11 : Tensor = prim::GetAttr[name="k"](%self.127)
    %12 : Tensor = prim::GetAttr[name="q"](%self.127)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.11/__module.layer.11.rel_attn
    %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.11/__module.layer.11.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.11/__module.layer.11.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.11/__module.layer.11.rel_attn
    %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.13, %9), scope: __module.layer.11/__module.layer.11.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %8, %31), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.11/__module.layer.11.rel_attn
    %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %7, %36), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.11/__module.layer.11.rel_attn
    %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.12, %50), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.13 : Long() = prim::NumToTensor(%51), scope: __module.layer.11/__module.layer.11.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.13), scope: __module.layer.11/__module.layer.11.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.45, %54), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.11/__module.layer.11.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.11/__module.layer.11.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.11/__module.layer.11.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.45, %59), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.11/__module.layer.11.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.11/__module.layer.11.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.11/__module.layer.11.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.45, %64), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.11/__module.layer.11.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.11/__module.layer.11.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.11/__module.layer.11.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.45, %69), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.11/__module.layer.11.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.11/__module.layer.11.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.11/__module.layer.11.rel_attn
    %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %73), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %75, %76, %77, %78), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.11/__module.layer.11.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.11/__module.layer.11.rel_attn
    %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %99), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %106, %105), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %108), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.11/__module.layer.11.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %119, %120), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.11/__module.layer.11.rel_attn
    %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %122, %123), scope: __module.layer.11/__module.layer.11.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.103)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.11/__module.layer.11.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.11/__module.layer.11.rel_attn
    %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.104)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.105)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.13)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.128 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.103 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %2, %3), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.130 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.105 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.130)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.130)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %5, %3, %2, %6, %7), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.12)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.133 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.107 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.dropout # torch/nn/functional.py:973:0
    %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %2, %3), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.dropout # torch/nn/functional.py:973:0
    return (%input.108)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.132 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.132)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.132)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %2, %6), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.106)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.134 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.134)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.134)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %2, %6), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.109)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.136 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.110 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.136)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.136)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %5, %3, %2, %6, %7), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.13)

XLNetLayer._actual_script_module
  graph(%self.137 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.137)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.137)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.142 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.142)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.142)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.142)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.142)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.12/__module.layer.12.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.116)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.12/__module.layer.12.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.119)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.138 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.138)
    %5 : Tensor = prim::GetAttr[name="o"](%self.138)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.138)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.138)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.138)
    %9 : Tensor = prim::GetAttr[name="r"](%self.138)
    %10 : Tensor = prim::GetAttr[name="v"](%self.138)
    %11 : Tensor = prim::GetAttr[name="k"](%self.138)
    %12 : Tensor = prim::GetAttr[name="q"](%self.138)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.12/__module.layer.12.rel_attn
    %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.12/__module.layer.12.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.12/__module.layer.12.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.12/__module.layer.12.rel_attn
    %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.14, %9), scope: __module.layer.12/__module.layer.12.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %8, %31), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.12/__module.layer.12.rel_attn
    %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %7, %36), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.12/__module.layer.12.rel_attn
    %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.13, %50), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.14 : Long() = prim::NumToTensor(%51), scope: __module.layer.12/__module.layer.12.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.14), scope: __module.layer.12/__module.layer.12.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.49, %54), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.12/__module.layer.12.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.12/__module.layer.12.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.12/__module.layer.12.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.49, %59), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.12/__module.layer.12.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.12/__module.layer.12.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.12/__module.layer.12.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.49, %64), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.12/__module.layer.12.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.12/__module.layer.12.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.12/__module.layer.12.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.49, %69), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.12/__module.layer.12.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.12/__module.layer.12.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.12/__module.layer.12.rel_attn
    %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %73), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %75, %76, %77, %78), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.12/__module.layer.12.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.12/__module.layer.12.rel_attn
    %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %99), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %106, %105), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %108), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.12/__module.layer.12.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %119, %120), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.12/__module.layer.12.rel_attn
    %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %122, %123), scope: __module.layer.12/__module.layer.12.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.112)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.12/__module.layer.12.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.12/__module.layer.12.rel_attn
    %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.113)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.114)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.14)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.139 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.112 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %2, %3), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.141 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.114 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.141)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.141)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %5, %3, %2, %6, %7), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.13)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.144 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.116 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.dropout # torch/nn/functional.py:973:0
    %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %2, %3), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.dropout # torch/nn/functional.py:973:0
    return (%input.117)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.143 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.143)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.143)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %2, %6), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.115)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.145 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.145)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.145)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %2, %6), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.118)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.147 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.119 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.147)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.147)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %5, %3, %2, %6, %7), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.14)

XLNetLayer._actual_script_module
  graph(%self.148 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.148)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.148)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.153 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.153)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.153)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.153)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.153)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.13/__module.layer.13.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.125)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.13/__module.layer.13.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.128)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.149 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.149)
    %5 : Tensor = prim::GetAttr[name="o"](%self.149)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.149)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.149)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.149)
    %9 : Tensor = prim::GetAttr[name="r"](%self.149)
    %10 : Tensor = prim::GetAttr[name="v"](%self.149)
    %11 : Tensor = prim::GetAttr[name="k"](%self.149)
    %12 : Tensor = prim::GetAttr[name="q"](%self.149)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.13/__module.layer.13.rel_attn
    %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.13/__module.layer.13.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.13/__module.layer.13.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.13/__module.layer.13.rel_attn
    %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.15, %9), scope: __module.layer.13/__module.layer.13.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %8, %31), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.13/__module.layer.13.rel_attn
    %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %7, %36), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.13/__module.layer.13.rel_attn
    %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.14, %50), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.15 : Long() = prim::NumToTensor(%51), scope: __module.layer.13/__module.layer.13.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.15), scope: __module.layer.13/__module.layer.13.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.53, %54), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.13/__module.layer.13.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.13/__module.layer.13.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.13/__module.layer.13.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.53, %59), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.13/__module.layer.13.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.13/__module.layer.13.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.13/__module.layer.13.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.53, %64), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.13/__module.layer.13.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.13/__module.layer.13.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.13/__module.layer.13.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.53, %69), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.13/__module.layer.13.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.13/__module.layer.13.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.13/__module.layer.13.rel_attn
    %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %73), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %75, %76, %77, %78), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.13/__module.layer.13.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.13/__module.layer.13.rel_attn
    %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %99), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %106, %105), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %108), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.13/__module.layer.13.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %119, %120), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.13/__module.layer.13.rel_attn
    %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %122, %123), scope: __module.layer.13/__module.layer.13.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.121)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.13/__module.layer.13.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.13/__module.layer.13.rel_attn
    %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.122)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.123)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.15)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.150 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.121 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %2, %3), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.152 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.123 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.152)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.152)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %5, %3, %2, %6, %7), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.14)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.155 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.125 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.dropout # torch/nn/functional.py:973:0
    %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %2, %3), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.dropout # torch/nn/functional.py:973:0
    return (%input.126)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.154 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.154)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.154)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %2, %6), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.124)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.156 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.156)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.156)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %2, %6), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.127)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.158 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.128 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.158)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.158)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %5, %3, %2, %6, %7), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.15)

XLNetLayer._actual_script_module
  graph(%self.159 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.159)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.159)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.164 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.164)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.164)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.164)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.164)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.14/__module.layer.14.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.134)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.14/__module.layer.14.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.137)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.160 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.160)
    %5 : Tensor = prim::GetAttr[name="o"](%self.160)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.160)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.160)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.160)
    %9 : Tensor = prim::GetAttr[name="r"](%self.160)
    %10 : Tensor = prim::GetAttr[name="v"](%self.160)
    %11 : Tensor = prim::GetAttr[name="k"](%self.160)
    %12 : Tensor = prim::GetAttr[name="q"](%self.160)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.14/__module.layer.14.rel_attn
    %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.14/__module.layer.14.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.14/__module.layer.14.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.14/__module.layer.14.rel_attn
    %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.16, %9), scope: __module.layer.14/__module.layer.14.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %8, %31), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.14/__module.layer.14.rel_attn
    %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %7, %36), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.14/__module.layer.14.rel_attn
    %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.15, %50), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.16 : Long() = prim::NumToTensor(%51), scope: __module.layer.14/__module.layer.14.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.16), scope: __module.layer.14/__module.layer.14.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.57, %54), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.14/__module.layer.14.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.14/__module.layer.14.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.14/__module.layer.14.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.57, %59), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.14/__module.layer.14.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.14/__module.layer.14.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.14/__module.layer.14.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.57, %64), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.14/__module.layer.14.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.14/__module.layer.14.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.14/__module.layer.14.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.57, %69), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.14/__module.layer.14.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.14/__module.layer.14.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.14/__module.layer.14.rel_attn
    %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %73), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %75, %76, %77, %78), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.14/__module.layer.14.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.14/__module.layer.14.rel_attn
    %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %99), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %106, %105), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %108), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.14/__module.layer.14.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %119, %120), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.14/__module.layer.14.rel_attn
    %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %122, %123), scope: __module.layer.14/__module.layer.14.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.130)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.14/__module.layer.14.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.14/__module.layer.14.rel_attn
    %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.131)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.132)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.16)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.161 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.130 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %2, %3), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.163 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.132 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.163)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.163)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %5, %3, %2, %6, %7), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.15)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.166 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.134 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.dropout # torch/nn/functional.py:973:0
    %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %2, %3), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.dropout # torch/nn/functional.py:973:0
    return (%input.135)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.165 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.165)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.165)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %2, %6), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.133)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.167 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.167)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.167)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %2, %6), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.136)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.169 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.137 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.169)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.169)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %5, %3, %2, %6, %7), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.16)

XLNetLayer._actual_script_module
  graph(%self.170 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.170)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.170)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.175 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.175)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.175)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.175)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.175)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.15/__module.layer.15.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.143)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.15/__module.layer.15.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.146)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.171 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.171)
    %5 : Tensor = prim::GetAttr[name="o"](%self.171)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.171)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.171)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.171)
    %9 : Tensor = prim::GetAttr[name="r"](%self.171)
    %10 : Tensor = prim::GetAttr[name="v"](%self.171)
    %11 : Tensor = prim::GetAttr[name="k"](%self.171)
    %12 : Tensor = prim::GetAttr[name="q"](%self.171)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.15/__module.layer.15.rel_attn
    %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.15/__module.layer.15.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.15/__module.layer.15.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.15/__module.layer.15.rel_attn
    %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.17, %9), scope: __module.layer.15/__module.layer.15.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %8, %31), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.15/__module.layer.15.rel_attn
    %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %7, %36), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.15/__module.layer.15.rel_attn
    %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.16, %50), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.17 : Long() = prim::NumToTensor(%51), scope: __module.layer.15/__module.layer.15.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.17), scope: __module.layer.15/__module.layer.15.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.61, %54), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.15/__module.layer.15.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.15/__module.layer.15.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.15/__module.layer.15.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.61, %59), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.15/__module.layer.15.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.15/__module.layer.15.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.15/__module.layer.15.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.61, %64), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.15/__module.layer.15.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.15/__module.layer.15.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.15/__module.layer.15.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.61, %69), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.15/__module.layer.15.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.15/__module.layer.15.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.15/__module.layer.15.rel_attn
    %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %73), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %75, %76, %77, %78), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.15/__module.layer.15.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.15/__module.layer.15.rel_attn
    %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %99), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %106, %105), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %108), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.15/__module.layer.15.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %119, %120), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.15/__module.layer.15.rel_attn
    %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %122, %123), scope: __module.layer.15/__module.layer.15.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.139)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.15/__module.layer.15.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.15/__module.layer.15.rel_attn
    %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.140)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.141)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.17)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.172 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.139 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %2, %3), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.174 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.141 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.174)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.174)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %5, %3, %2, %6, %7), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.16)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.177 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.143 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.dropout # torch/nn/functional.py:973:0
    %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %2, %3), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.dropout # torch/nn/functional.py:973:0
    return (%input.144)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.176 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.176)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.176)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %2, %6), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.142)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.178 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.178)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.178)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %2, %6), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.145)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.180 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.146 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.180)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.180)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %5, %3, %2, %6, %7), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.17)

XLNetLayer._actual_script_module
  graph(%self.181 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.181)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.181)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.186 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.186)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.186)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.186)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.186)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.16/__module.layer.16.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.152)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.16/__module.layer.16.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.155)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.182 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.182)
    %5 : Tensor = prim::GetAttr[name="o"](%self.182)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.182)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.182)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.182)
    %9 : Tensor = prim::GetAttr[name="r"](%self.182)
    %10 : Tensor = prim::GetAttr[name="v"](%self.182)
    %11 : Tensor = prim::GetAttr[name="k"](%self.182)
    %12 : Tensor = prim::GetAttr[name="q"](%self.182)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.16/__module.layer.16.rel_attn
    %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.16/__module.layer.16.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.16/__module.layer.16.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.16/__module.layer.16.rel_attn
    %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.18, %9), scope: __module.layer.16/__module.layer.16.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %8, %31), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.16/__module.layer.16.rel_attn
    %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %7, %36), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.16/__module.layer.16.rel_attn
    %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.17, %50), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.18 : Long() = prim::NumToTensor(%51), scope: __module.layer.16/__module.layer.16.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.18), scope: __module.layer.16/__module.layer.16.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.65, %54), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.16/__module.layer.16.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.16/__module.layer.16.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.16/__module.layer.16.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.65, %59), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.16/__module.layer.16.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.16/__module.layer.16.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.16/__module.layer.16.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.65, %64), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.16/__module.layer.16.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.16/__module.layer.16.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.16/__module.layer.16.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.65, %69), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.16/__module.layer.16.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.16/__module.layer.16.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.16/__module.layer.16.rel_attn
    %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %73), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %75, %76, %77, %78), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.16/__module.layer.16.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.16/__module.layer.16.rel_attn
    %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %99), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %106, %105), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %108), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.16/__module.layer.16.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %119, %120), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.16/__module.layer.16.rel_attn
    %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %122, %123), scope: __module.layer.16/__module.layer.16.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.148)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.16/__module.layer.16.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.16/__module.layer.16.rel_attn
    %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.149)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.150)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.18)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.183 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.148 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %2, %3), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.185 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.150 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.185)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.185)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %5, %3, %2, %6, %7), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.17)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.188 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.152 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.dropout # torch/nn/functional.py:973:0
    %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %2, %3), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.dropout # torch/nn/functional.py:973:0
    return (%input.153)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.187 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.187)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.187)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %2, %6), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.151)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.189 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.189)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.189)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %2, %6), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.154)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.191 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.155 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.191)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.191)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %5, %3, %2, %6, %7), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.18)

XLNetLayer._actual_script_module
  graph(%self.192 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.192)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.192)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.197 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.197)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.197)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.197)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.197)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.17/__module.layer.17.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.161)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.17/__module.layer.17.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.164)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.193 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.193)
    %5 : Tensor = prim::GetAttr[name="o"](%self.193)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.193)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.193)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.193)
    %9 : Tensor = prim::GetAttr[name="r"](%self.193)
    %10 : Tensor = prim::GetAttr[name="v"](%self.193)
    %11 : Tensor = prim::GetAttr[name="k"](%self.193)
    %12 : Tensor = prim::GetAttr[name="q"](%self.193)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.17/__module.layer.17.rel_attn
    %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.17/__module.layer.17.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.17/__module.layer.17.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.17/__module.layer.17.rel_attn
    %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.19, %9), scope: __module.layer.17/__module.layer.17.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %8, %31), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.17/__module.layer.17.rel_attn
    %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %7, %36), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.17/__module.layer.17.rel_attn
    %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.18, %50), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.19 : Long() = prim::NumToTensor(%51), scope: __module.layer.17/__module.layer.17.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.19), scope: __module.layer.17/__module.layer.17.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.69, %54), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.17/__module.layer.17.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.17/__module.layer.17.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.17/__module.layer.17.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.69, %59), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.17/__module.layer.17.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.17/__module.layer.17.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.17/__module.layer.17.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.69, %64), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.17/__module.layer.17.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.17/__module.layer.17.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.17/__module.layer.17.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.69, %69), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.17/__module.layer.17.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.17/__module.layer.17.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.17/__module.layer.17.rel_attn
    %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %73), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %75, %76, %77, %78), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.17/__module.layer.17.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.17/__module.layer.17.rel_attn
    %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %99), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %106, %105), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %108), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.17/__module.layer.17.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %119, %120), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.17/__module.layer.17.rel_attn
    %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %122, %123), scope: __module.layer.17/__module.layer.17.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.157)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.17/__module.layer.17.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.17/__module.layer.17.rel_attn
    %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.158)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.159)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.19)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.194 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.157 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %2, %3), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.196 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.159 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.196)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.196)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %5, %3, %2, %6, %7), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.18)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.199 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.161 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.dropout # torch/nn/functional.py:973:0
    %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %2, %3), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.dropout # torch/nn/functional.py:973:0
    return (%input.162)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.198 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.198)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.198)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %2, %6), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.160)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.200 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.200)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.200)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %2, %6), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.163)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.202 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.164 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.202)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.202)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %5, %3, %2, %6, %7), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.19)

XLNetLayer._actual_script_module
  graph(%self.203 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.203)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.203)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.208 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.208)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.208)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.208)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.208)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.18/__module.layer.18.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.170)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.18/__module.layer.18.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.173)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.204 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.204)
    %5 : Tensor = prim::GetAttr[name="o"](%self.204)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.204)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.204)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.204)
    %9 : Tensor = prim::GetAttr[name="r"](%self.204)
    %10 : Tensor = prim::GetAttr[name="v"](%self.204)
    %11 : Tensor = prim::GetAttr[name="k"](%self.204)
    %12 : Tensor = prim::GetAttr[name="q"](%self.204)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.18/__module.layer.18.rel_attn
    %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.18/__module.layer.18.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.18/__module.layer.18.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.18/__module.layer.18.rel_attn
    %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.20, %9), scope: __module.layer.18/__module.layer.18.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %8, %31), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.18/__module.layer.18.rel_attn
    %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %7, %36), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.18/__module.layer.18.rel_attn
    %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.19, %50), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.20 : Long() = prim::NumToTensor(%51), scope: __module.layer.18/__module.layer.18.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.20), scope: __module.layer.18/__module.layer.18.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.73, %54), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.18/__module.layer.18.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.18/__module.layer.18.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.18/__module.layer.18.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.73, %59), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.18/__module.layer.18.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.18/__module.layer.18.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.18/__module.layer.18.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.73, %64), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.18/__module.layer.18.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.18/__module.layer.18.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.18/__module.layer.18.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.73, %69), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.18/__module.layer.18.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.18/__module.layer.18.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.18/__module.layer.18.rel_attn
    %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %73), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %75, %76, %77, %78), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.18/__module.layer.18.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.18/__module.layer.18.rel_attn
    %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %99), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %106, %105), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %108), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.18/__module.layer.18.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %119, %120), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.18/__module.layer.18.rel_attn
    %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %122, %123), scope: __module.layer.18/__module.layer.18.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.166)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.18/__module.layer.18.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.18/__module.layer.18.rel_attn
    %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.167)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.168)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.20)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.205 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.166 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %2, %3), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.207 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.168 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.207)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.207)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %5, %3, %2, %6, %7), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.19)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.210 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.170 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.dropout # torch/nn/functional.py:973:0
    %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %2, %3), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.dropout # torch/nn/functional.py:973:0
    return (%input.171)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.209 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.209)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.209)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %2, %6), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.169)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.211 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.211)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.211)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %2, %6), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.172)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.213 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.173 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.213)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.213)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %5, %3, %2, %6, %7), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.20)

XLNetLayer._actual_script_module
  graph(%self.214 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.214)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.214)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.219 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.219)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.219)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.219)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.219)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.19/__module.layer.19.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.179)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.19/__module.layer.19.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.182)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.215 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.215)
    %5 : Tensor = prim::GetAttr[name="o"](%self.215)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.215)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.215)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.215)
    %9 : Tensor = prim::GetAttr[name="r"](%self.215)
    %10 : Tensor = prim::GetAttr[name="v"](%self.215)
    %11 : Tensor = prim::GetAttr[name="k"](%self.215)
    %12 : Tensor = prim::GetAttr[name="q"](%self.215)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.19/__module.layer.19.rel_attn
    %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.19/__module.layer.19.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.19/__module.layer.19.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.19/__module.layer.19.rel_attn
    %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.21, %9), scope: __module.layer.19/__module.layer.19.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %8, %31), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.19/__module.layer.19.rel_attn
    %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %7, %36), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.19/__module.layer.19.rel_attn
    %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.20, %50), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.21 : Long() = prim::NumToTensor(%51), scope: __module.layer.19/__module.layer.19.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.21), scope: __module.layer.19/__module.layer.19.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.77, %54), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.19/__module.layer.19.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.19/__module.layer.19.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.19/__module.layer.19.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.77, %59), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.19/__module.layer.19.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.19/__module.layer.19.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.19/__module.layer.19.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.77, %64), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.19/__module.layer.19.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.19/__module.layer.19.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.19/__module.layer.19.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.77, %69), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.19/__module.layer.19.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.19/__module.layer.19.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.19/__module.layer.19.rel_attn
    %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %73), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %75, %76, %77, %78), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.19/__module.layer.19.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.19/__module.layer.19.rel_attn
    %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %99), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %106, %105), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %108), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.19/__module.layer.19.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %119, %120), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.19/__module.layer.19.rel_attn
    %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %122, %123), scope: __module.layer.19/__module.layer.19.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.175)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.19/__module.layer.19.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.19/__module.layer.19.rel_attn
    %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.176)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.177)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.21)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.216 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.175 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %2, %3), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.218 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.177 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.218)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.218)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %5, %3, %2, %6, %7), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.20)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.221 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.179 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.dropout # torch/nn/functional.py:973:0
    %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %2, %3), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.dropout # torch/nn/functional.py:973:0
    return (%input.180)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.220 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.220)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.220)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %2, %6), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.178)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.222 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.222)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.222)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %2, %6), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.181)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.224 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.182 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.224)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.224)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %5, %3, %2, %6, %7), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.21)

XLNetLayer._actual_script_module
  graph(%self.225 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.225)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.225)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.230 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.230)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.230)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.230)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.230)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.20/__module.layer.20.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.188)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.20/__module.layer.20.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.191)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.226 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.226)
    %5 : Tensor = prim::GetAttr[name="o"](%self.226)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.226)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.226)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.226)
    %9 : Tensor = prim::GetAttr[name="r"](%self.226)
    %10 : Tensor = prim::GetAttr[name="v"](%self.226)
    %11 : Tensor = prim::GetAttr[name="k"](%self.226)
    %12 : Tensor = prim::GetAttr[name="q"](%self.226)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.20/__module.layer.20.rel_attn
    %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.20/__module.layer.20.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.20/__module.layer.20.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.20/__module.layer.20.rel_attn
    %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.22, %9), scope: __module.layer.20/__module.layer.20.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %8, %31), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.20/__module.layer.20.rel_attn
    %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %7, %36), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.20/__module.layer.20.rel_attn
    %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.21, %50), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.22 : Long() = prim::NumToTensor(%51), scope: __module.layer.20/__module.layer.20.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.22), scope: __module.layer.20/__module.layer.20.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.81, %54), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.20/__module.layer.20.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.20/__module.layer.20.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.20/__module.layer.20.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.81, %59), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.20/__module.layer.20.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.20/__module.layer.20.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.20/__module.layer.20.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.81, %64), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.20/__module.layer.20.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.20/__module.layer.20.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.20/__module.layer.20.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.81, %69), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.20/__module.layer.20.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.20/__module.layer.20.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.20/__module.layer.20.rel_attn
    %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %73), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %75, %76, %77, %78), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.20/__module.layer.20.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.20/__module.layer.20.rel_attn
    %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %99), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %106, %105), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %108), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.20/__module.layer.20.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %119, %120), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.20/__module.layer.20.rel_attn
    %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %122, %123), scope: __module.layer.20/__module.layer.20.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.184)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.20/__module.layer.20.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.20/__module.layer.20.rel_attn
    %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.185)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.186)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.22)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.227 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.184 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %2, %3), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.229 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.186 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.229)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.229)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %5, %3, %2, %6, %7), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.21)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.232 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.188 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.dropout # torch/nn/functional.py:973:0
    %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %2, %3), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.dropout # torch/nn/functional.py:973:0
    return (%input.189)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.231 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.231)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.231)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2, %6), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.187)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.233 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.233)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.233)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2, %6), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.190)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.235 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.191 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.235)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.235)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %5, %3, %2, %6, %7), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.22)

XLNetLayer._actual_script_module
  graph(%self.236 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.236)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.236)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.241 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.241)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.241)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.241)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.241)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.21/__module.layer.21.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.197)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.21/__module.layer.21.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.200)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.237 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.237)
    %5 : Tensor = prim::GetAttr[name="o"](%self.237)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.237)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.237)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.237)
    %9 : Tensor = prim::GetAttr[name="r"](%self.237)
    %10 : Tensor = prim::GetAttr[name="v"](%self.237)
    %11 : Tensor = prim::GetAttr[name="k"](%self.237)
    %12 : Tensor = prim::GetAttr[name="q"](%self.237)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.21/__module.layer.21.rel_attn
    %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.21/__module.layer.21.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.21/__module.layer.21.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.21/__module.layer.21.rel_attn
    %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r.23, %9), scope: __module.layer.21/__module.layer.21.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %8, %31), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.21/__module.layer.21.rel_attn
    %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %7, %36), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.21/__module.layer.21.rel_attn
    %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.22, %50), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.23 : Long() = prim::NumToTensor(%51), scope: __module.layer.21/__module.layer.21.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.23), scope: __module.layer.21/__module.layer.21.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.85, %54), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.21/__module.layer.21.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.21/__module.layer.21.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.21/__module.layer.21.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.85, %59), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.21/__module.layer.21.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.21/__module.layer.21.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.21/__module.layer.21.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.85, %64), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.21/__module.layer.21.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.21/__module.layer.21.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.21/__module.layer.21.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.85, %69), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.21/__module.layer.21.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.21/__module.layer.21.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.21/__module.layer.21.rel_attn
    %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %73), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %75, %76, %77, %78), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.21/__module.layer.21.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.21/__module.layer.21.rel_attn
    %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %99), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %106, %105), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %108), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.21/__module.layer.21.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %119, %120), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.21/__module.layer.21.rel_attn
    %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %122, %123), scope: __module.layer.21/__module.layer.21.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.193)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.21/__module.layer.21.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.21/__module.layer.21.rel_attn
    %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.194)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.195)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r.23)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.238 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.193 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %2, %3), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.240 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.195 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.240)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.240)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %5, %3, %2, %6, %7), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.22)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.243 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.197 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.dropout # torch/nn/functional.py:973:0
    %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %2, %3), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.dropout # torch/nn/functional.py:973:0
    return (%input.198)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.242 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.242)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.242)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2, %6), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.196)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.244 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.244)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.244)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2, %6), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.199)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.246 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.200 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.246)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.246)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %5, %3, %2, %6, %7), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out.23)

XLNetLayer._actual_script_module
  graph(%self.247 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.247)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.247)
    %32 : (Tensor, Tensor) = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %7 : Float(13:17408, 17:1024, 1024:1), %8 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%32)
    %33 : Tensor = prim::CallMethod[name="forward"](%1, %7)
    %31 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%33, %8)
    return (%31)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.252 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.252)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.252)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.252)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.252)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.22/__module.layer.22.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.206)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.22/__module.layer.22.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.209)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.248 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.248)
    %5 : Tensor = prim::GetAttr[name="o"](%self.248)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.248)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.248)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.248)
    %9 : Tensor = prim::GetAttr[name="r"](%self.248)
    %10 : Tensor = prim::GetAttr[name="v"](%self.248)
    %11 : Tensor = prim::GetAttr[name="k"](%self.248)
    %12 : Tensor = prim::GetAttr[name="q"](%self.248)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.22/__module.layer.22.rel_attn
    %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.22/__module.layer.22.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.22/__module.layer.22.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.22/__module.layer.22.rel_attn
    %r : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%r, %9), scope: __module.layer.22/__module.layer.22.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %8, %31), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.22/__module.layer.22.rel_attn
    %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %7, %36), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.22/__module.layer.22.rel_attn
    %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac.23, %50), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen.24 : Long() = prim::NumToTensor(%51), scope: __module.layer.22/__module.layer.22.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen.24), scope: __module.layer.22/__module.layer.22.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.89, %54), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.22/__module.layer.22.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.22/__module.layer.22.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.22/__module.layer.22.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.89, %59), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.22/__module.layer.22.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.22/__module.layer.22.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.22/__module.layer.22.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.89, %64), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.22/__module.layer.22.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.22/__module.layer.22.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.22/__module.layer.22.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.89, %69), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.22/__module.layer.22.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.22/__module.layer.22.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.22/__module.layer.22.rel_attn
    %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %73), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %75, %76, %77, %78), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.22/__module.layer.22.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.22/__module.layer.22.rel_attn
    %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %99), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %106, %105), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %108), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.22/__module.layer.22.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %119, %120), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.22/__module.layer.22.rel_attn
    %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %122, %123), scope: __module.layer.22/__module.layer.22.rel_attn # torch/nn/functional.py:1498:0
    %137 : Tensor = prim::CallMethod[name="forward"](%6, %input.202)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%137, %21), scope: __module.layer.22/__module.layer.22.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.22/__module.layer.22.rel_attn
    %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
    %138 : Tensor = prim::CallMethod[name="forward1"](%6, %input.203)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%138, %1, %133), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
    %139 : Tensor = prim::CallMethod[name="forward"](%4, %input.204)
    %136 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%139, %r)
    return (%136)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.249 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.202 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %2, %3), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.251 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.204 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.251)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.251)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %5, %3, %2, %6, %7), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor.23)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.254 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.206 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.dropout # torch/nn/functional.py:973:0
    %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %2, %3), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.dropout # torch/nn/functional.py:973:0
    return (%input.207)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.253 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.253)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.253)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2, %6), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.205)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.255 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.255)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.255)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2, %6), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.208)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.257 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.209 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.257)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.257)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %5, %3, %2, %6, %7), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%curr_out)

XLNetLayer._actual_script_module
  graph(%self.258 : __torch__.transformers.modeling_xlnet.XLNetLayer,
        %4 : Float(13:17408, 17:1024, 1024:1),
        %5 : Float(26:1024, 17:0, 1024:1),
        %6 : Float(13:221, 13:17, 17:1, 1:1)):
    %1 : __torch__.transformers.modeling_xlnet.XLNetFeedForward = prim::GetAttr[name="ff"](%self.258)
    %2 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%self.258)
    %29 : Tensor = prim::CallMethod[name="forward"](%2, %4, %5, %6)
    %30 : Tensor = prim::CallMethod[name="forward"](%1, %29)
    return (%30)

XLNetLayer.ff
XLNetFeedForward._actual_script_module
  graph(%self.263 : __torch__.transformers.modeling_xlnet.XLNetFeedForward,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.263)
    %3 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_2"](%self.263)
    %4 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.263)
    %5 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name="layer_1"](%self.263)
    %14 : Tensor = prim::CallMethod[name="forward"](%5, %1)
    %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%14), scope: __module.layer.23/__module.layer.23.ff # torch/nn/functional.py:1369:0
    %15 : Tensor = prim::CallMethod[name="forward"](%4, %input.215)
    %16 : Tensor = prim::CallMethod[name="forward"](%3, %15)
    %17 : Tensor = prim::CallMethod[name="forward1"](%4, %16)
    %11 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%17, %1, %11), scope: __module.layer.23/__module.layer.23.ff # transformers/modeling_xlnet.py:489:0
    %18 : Tensor = prim::CallMethod[name="forward"](%2, %input.218)
    return (%18)

XLNetLayer.rel_attn
XLNetRelativeAttention._actual_script_module
  graph(%self.259 : __torch__.transformers.modeling_xlnet.XLNetRelativeAttention,
        %1 : Float(13:17408, 17:1024, 1024:1),
        %2 : Float(26:1024, 17:0, 1024:1),
        %3 : Float(13:221, 13:17, 17:1, 1:1)):
    %4 : __torch__.torch.nn.modules.normalization.LayerNorm = prim::GetAttr[name="layer_norm"](%self.259)
    %5 : Tensor = prim::GetAttr[name="o"](%self.259)
    %6 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self.259)
    %7 : Tensor = prim::GetAttr[name="r_r_bias"](%self.259)
    %8 : Tensor = prim::GetAttr[name="r_w_bias"](%self.259)
    %9 : Tensor = prim::GetAttr[name="r"](%self.259)
    %10 : Tensor = prim::GetAttr[name="v"](%self.259)
    %11 : Tensor = prim::GetAttr[name="k"](%self.259)
    %12 : Tensor = prim::GetAttr[name="q"](%self.259)
    %13 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %14 : Tensor[] = prim::ListConstruct(%1, %12), scope: __module.layer.23/__module.layer.23.rel_attn
    %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%13, %14), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %16 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %17 : Tensor[] = prim::ListConstruct(%1, %11), scope: __module.layer.23/__module.layer.23.rel_attn
    %18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%16, %17), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %19 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %20 : Tensor[] = prim::ListConstruct(%1, %10), scope: __module.layer.23/__module.layer.23.rel_attn
    %21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%19, %20), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %22 : Device = prim::Constant[value="cpu"](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %23 : int = prim::Constant[value=6](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %24 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %25 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %26 : None = prim::Constant(), scope: __module.layer.23/__module.layer.23.rel_attn
    %27 : Float(26:1024, 17:0, 1024:1) = aten::to(%2, %22, %23, %24, %25, %26), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
    %28 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %29 : Tensor[] = prim::ListConstruct(%27, %9), scope: __module.layer.23/__module.layer.23.rel_attn
    %30 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%28, %29), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %31 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %32 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %8, %31), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
    %33 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %34 : Tensor[] = prim::ListConstruct(%32, %18), scope: __module.layer.23/__module.layer.23.rel_attn
    %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%33, %34), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %36 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %37 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %7, %36), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
    %38 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %39 : Tensor[] = prim::ListConstruct(%37, %30), scope: __module.layer.23/__module.layer.23.rel_attn
    %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%38, %39), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %50 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %51 : int = aten::size(%ac, %50), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
    %klen : Long() = prim::NumToTensor(%51), scope: __module.layer.23/__module.layer.23.rel_attn
    %53 : Scalar = aten::ScalarImplicit(%klen), scope: __module.layer.23/__module.layer.23.rel_attn
    %54 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %55 : int = aten::size(%x.93, %54), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %56 : Long() = prim::NumToTensor(%55), scope: __module.layer.23/__module.layer.23.rel_attn
    %57 : int = aten::Int(%56), scope: __module.layer.23/__module.layer.23.rel_attn
    %58 : int = aten::Int(%56), scope: __module.layer.23/__module.layer.23.rel_attn
    %59 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %60 : int = aten::size(%x.93, %59), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %61 : Long() = prim::NumToTensor(%60), scope: __module.layer.23/__module.layer.23.rel_attn
    %62 : int = aten::Int(%61), scope: __module.layer.23/__module.layer.23.rel_attn
    %63 : int = aten::Int(%61), scope: __module.layer.23/__module.layer.23.rel_attn
    %64 : int = prim::Constant[value=2](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %65 : int = aten::size(%x.93, %64), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %66 : Long() = prim::NumToTensor(%65), scope: __module.layer.23/__module.layer.23.rel_attn
    %67 : int = aten::Int(%66), scope: __module.layer.23/__module.layer.23.rel_attn
    %68 : int = aten::Int(%66), scope: __module.layer.23/__module.layer.23.rel_attn
    %69 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %70 : int = aten::size(%x.93, %69), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
    %71 : Long() = prim::NumToTensor(%70), scope: __module.layer.23/__module.layer.23.rel_attn
    %72 : int = aten::Int(%71), scope: __module.layer.23/__module.layer.23.rel_attn
    %73 : int[] = prim::ListConstruct(%58, %63, %72, %68), scope: __module.layer.23/__module.layer.23.rel_attn
    %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %73), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
    %75 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %76 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %77 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %78 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %79 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %75, %76, %77, %78), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %80 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %81 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %82 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %83 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %84 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%79, %80, %81, %82, %83), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %85 : int = prim::Constant[value=2](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %86 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %87 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %88 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %89 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%84, %85, %86, %87, %88), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %90 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %91 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %92 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %93 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%89, %90, %91, %92, %93), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
    %95 : Long() = prim::Constant[value={1}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %96 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %97 : Long() = aten::sub(%71, %95, %96), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %98 : int = aten::Int(%97), scope: __module.layer.23/__module.layer.23.rel_attn
    %99 : int[] = prim::ListConstruct(%57, %62, %67, %98), scope: __module.layer.23/__module.layer.23.rel_attn
    %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %99), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
    %101 : int = prim::Constant[value=4](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %102 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %103 : Device = prim::Constant[value="cpu"](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %104 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %105 : Long(13:1) = aten::arange(%53, %101, %102, %103, %104), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %106 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %106, %105), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
    %108 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %109 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %108), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %110 : Long() = prim::Constant[value={0}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %111 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%109, %110, %111), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %113 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%112, %113), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
    %115 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %116 : Tensor[] = prim::ListConstruct(%3), scope: __module.layer.23/__module.layer.23.rel_attn
    %117 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%115, %116), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %118 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %119 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%117, %118), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %120 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %119, %120), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
    %122 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %123 : None = prim::Constant(), scope: __module.layer.23/__module.layer.23.rel_attn
    %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %122, %123), scope: __module.layer.23/__module.layer.23.rel_attn # torch/nn/functional.py:1498:0
    %136 : Tensor = prim::CallMethod[name="forward"](%6, %input.211)
    %126 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %127 : Tensor[] = prim::ListConstruct(%136, %21), scope: __module.layer.23/__module.layer.23.rel_attn
    %128 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%126, %127), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %129 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %130 : Tensor[] = prim::ListConstruct(%128, %5), scope: __module.layer.23/__module.layer.23.rel_attn
    %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%129, %130), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
    %137 : Tensor = prim::CallMethod[name="forward1"](%6, %input.212)
    %133 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%137, %1, %133), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
    %138 : Tensor = prim::CallMethod[name="forward"](%4, %input.213)
    return (%138)

XLNetRelativeAttention.dropout
Dropout._actual_script_module
  graph(%self.260 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.211 : Float(17:2704, 16:169, 13:13, 13:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    %4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %2, %3), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
    return (%4)

XLNetRelativeAttention.layer_norm
LayerNorm._actual_script_module
  graph(%self.262 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.213 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.262)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.262)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %5, %3, %2, %6, %7), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
    return (%input_tensor)

XLNetFeedForward.dropout
Dropout._actual_script_module
  graph(%self.265 : __torch__.torch.nn.modules.dropout.Dropout,
        %input.215 : Float(13:69632, 17:4096, 4096:1)):
    %2 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %3 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.dropout # torch/nn/functional.py:973:0
    %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %2, %3), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.dropout # torch/nn/functional.py:973:0
    return (%input.216)

XLNetFeedForward.layer_1
Linear._actual_script_module
  graph(%self.264 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.264)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.264)
    %4 : Float(1024:1, 4096:1024) = aten::t(%3), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1, %4), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %2, %6), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
    return (%input.214)

XLNetFeedForward.layer_2
Linear._actual_script_module
  graph(%self.266 : __torch__.torch.nn.modules.linear.Linear,
        %1 : Float(13:69632, 17:4096, 4096:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.266)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.266)
    %4 : Float(4096:1, 1024:4096) = aten::t(%3), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%1, %4), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
    %6 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %2, %6), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
    return (%input.217)

XLNetFeedForward.layer_norm
LayerNorm._actual_script_module
  graph(%self.268 : __torch__.torch.nn.modules.normalization.LayerNorm,
        %input.218 : Float(13:17408, 17:1024, 1024:1)):
    %2 : Tensor = prim::GetAttr[name="bias"](%self.268)
    %3 : Tensor = prim::GetAttr[name="weight"](%self.268)
    %4 : int = prim::Constant[value=1024](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %5 : int[] = prim::ListConstruct(%4), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm
    %6 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %7 : bool = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    %input : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %5, %3, %2, %6, %7), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
    return (%input)

