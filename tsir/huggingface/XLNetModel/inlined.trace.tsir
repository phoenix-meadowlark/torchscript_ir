graph(%self.1 : __torch__.transformers.modeling_xlnet.___torch_mangle_43456.XLNetModel,
      %input_ids.1 : Long(17:13, 13:1),
      %attention_mask : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %4 : __torch__.transformers.modeling_xlnet.___torch_mangle_43453.XLNetLayer = prim::GetAttr[name="23"](%3)
  %5 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %6 : __torch__.transformers.modeling_xlnet.___torch_mangle_43443.XLNetLayer = prim::GetAttr[name="22"](%5)
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %8 : __torch__.transformers.modeling_xlnet.___torch_mangle_43433.XLNetLayer = prim::GetAttr[name="21"](%7)
  %9 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %10 : __torch__.transformers.modeling_xlnet.___torch_mangle_43423.XLNetLayer = prim::GetAttr[name="20"](%9)
  %11 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %12 : __torch__.transformers.modeling_xlnet.___torch_mangle_43413.XLNetLayer = prim::GetAttr[name="19"](%11)
  %13 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %14 : __torch__.transformers.modeling_xlnet.___torch_mangle_43403.XLNetLayer = prim::GetAttr[name="18"](%13)
  %15 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %16 : __torch__.transformers.modeling_xlnet.___torch_mangle_43393.XLNetLayer = prim::GetAttr[name="17"](%15)
  %17 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %18 : __torch__.transformers.modeling_xlnet.___torch_mangle_43383.XLNetLayer = prim::GetAttr[name="16"](%17)
  %19 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %20 : __torch__.transformers.modeling_xlnet.___torch_mangle_43373.XLNetLayer = prim::GetAttr[name="15"](%19)
  %21 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %22 : __torch__.transformers.modeling_xlnet.___torch_mangle_43363.XLNetLayer = prim::GetAttr[name="14"](%21)
  %23 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %24 : __torch__.transformers.modeling_xlnet.___torch_mangle_43353.XLNetLayer = prim::GetAttr[name="13"](%23)
  %25 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %26 : __torch__.transformers.modeling_xlnet.___torch_mangle_43343.XLNetLayer = prim::GetAttr[name="12"](%25)
  %27 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %28 : __torch__.transformers.modeling_xlnet.___torch_mangle_43333.XLNetLayer = prim::GetAttr[name="11"](%27)
  %29 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %30 : __torch__.transformers.modeling_xlnet.___torch_mangle_43323.XLNetLayer = prim::GetAttr[name="10"](%29)
  %31 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %32 : __torch__.transformers.modeling_xlnet.___torch_mangle_43313.XLNetLayer = prim::GetAttr[name="9"](%31)
  %33 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %34 : __torch__.transformers.modeling_xlnet.___torch_mangle_43303.XLNetLayer = prim::GetAttr[name="8"](%33)
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %36 : __torch__.transformers.modeling_xlnet.___torch_mangle_43293.XLNetLayer = prim::GetAttr[name="7"](%35)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %38 : __torch__.transformers.modeling_xlnet.___torch_mangle_43283.XLNetLayer = prim::GetAttr[name="6"](%37)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %40 : __torch__.transformers.modeling_xlnet.___torch_mangle_43273.XLNetLayer = prim::GetAttr[name="5"](%39)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %42 : __torch__.transformers.modeling_xlnet.___torch_mangle_43263.XLNetLayer = prim::GetAttr[name="4"](%41)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %44 : __torch__.transformers.modeling_xlnet.___torch_mangle_43253.XLNetLayer = prim::GetAttr[name="3"](%43)
  %45 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %46 : __torch__.transformers.modeling_xlnet.___torch_mangle_43243.XLNetLayer = prim::GetAttr[name="2"](%45)
  %47 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %48 : __torch__.transformers.modeling_xlnet.___torch_mangle_43233.XLNetLayer = prim::GetAttr[name="1"](%47)
  %49 : __torch__.torch.nn.modules.container.___torch_mangle_43454.ModuleList = prim::GetAttr[name="layer"](%self.1)
  %50 : __torch__.transformers.modeling_xlnet.___torch_mangle_43223.XLNetLayer = prim::GetAttr[name="0"](%49)
  %51 : __torch__.torch.nn.modules.dropout.___torch_mangle_43455.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %52 : __torch__.torch.nn.modules.sparse.___torch_mangle_43213.Embedding = prim::GetAttr[name="word_embedding"](%self.1)
  %53 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1101:0
  %54 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1101:0
  %55 : Long(13:1, 17:13) = aten::transpose(%input_ids.1, %53, %54) # transformers/modeling_xlnet.py:1101:0
  %56 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1101:0
  %input_ids : Long(13:17, 17:1) = aten::contiguous(%55, %56) # transformers/modeling_xlnet.py:1101:0
  %58 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1102:0
  %59 : int = aten::size(%input_ids, %58) # transformers/modeling_xlnet.py:1102:0
  %qlen : Long() = prim::NumToTensor(%59)
  %61 : int = aten::Int(%qlen)
  %62 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1102:0
  %63 : int = aten::size(%input_ids, %62) # transformers/modeling_xlnet.py:1102:0
  %bsz : Long() = prim::NumToTensor(%63)
  %65 : int = aten::Int(%bsz)
  %66 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1111:0
  %67 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1111:0
  %68 : Long(13:1, 17:13) = aten::transpose(%attention_mask, %66, %67) # transformers/modeling_xlnet.py:1111:0
  %69 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1111:0
  %70 : Long(13:17, 17:1) = aten::contiguous(%68, %69) # transformers/modeling_xlnet.py:1111:0
  %71 : Long() = prim::Constant[value={0}]() # transformers/modeling_xlnet.py:1116:0
  %72 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1116:0
  %klen.1 : Long() = aten::add(%qlen, %71, %72) # transformers/modeling_xlnet.py:1116:0
  %74 : Scalar = aten::ScalarImplicit(%klen.1)
  %75 : float = prim::Constant[value=1.]() # torch/tensor.py:396:0
  %76 : int = prim::Constant[value=1]() # torch/tensor.py:396:0
  %input_mask : Float(13:17, 17:1) = aten::rsub(%70, %75, %76) # torch/tensor.py:396:0
  %78 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1139:0
  %data_mask : Float(1:221, 13:17, 17:1) = aten::unsqueeze(%input_mask, %78) # transformers/modeling_xlnet.py:1139:0
  %80 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
  %81 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
  %82 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
  %83 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
  %84 : Float(1:221, 13:17, 17:1) = aten::slice(%data_mask, %80, %81, %82, %83) # transformers/modeling_xlnet.py:1151:0
  %85 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
  %86 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
  %87 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
  %88 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
  %89 : Float(1:221, 13:17, 17:1) = aten::slice(%84, %85, %86, %87, %88) # transformers/modeling_xlnet.py:1151:0
  %90 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1151:0
  %91 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1151:0
  %92 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1151:0
  %93 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1151:0
  %94 : Float(1:221, 13:17, 17:1) = aten::slice(%89, %90, %91, %92, %93) # transformers/modeling_xlnet.py:1151:0
  %95 : int = prim::Constant[value=3]() # transformers/modeling_xlnet.py:1151:0
  %96 : Float(1:221, 13:17, 17:1, 1:1) = aten::unsqueeze(%94, %95) # transformers/modeling_xlnet.py:1151:0
  %97 : int = prim::Constant[value=0]() # torch/tensor.py:22:0
  %98 : Bool(1:221, 13:17, 17:1, 1:1) = aten::gt(%96, %97) # torch/tensor.py:22:0
  %99 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1156:0
  %100 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1156:0
  %101 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1156:0
  %102 : None = prim::Constant()
  %attn_mask : Float(1:221, 13:17, 17:1, 1:1) = aten::to(%98, %99, %100, %101, %102) # transformers/modeling_xlnet.py:1156:0
  %104 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1159:0
  %105 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
  %106 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1159:0
  %107 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
  %108 : Float(13:13, 13:1) = aten::eye(%61, %104, %105, %106, %107) # transformers/modeling_xlnet.py:1159:0
  %109 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1159:0
  %110 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1159:0
  %111 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
  %112 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1159:0
  %113 : None = prim::Constant()
  %114 : Float(13:13, 13:1) = aten::to(%108, %109, %110, %111, %112, %113) # transformers/modeling_xlnet.py:1159:0
  %non_tgt_mask : Float(13:13, 13:1) = aten::neg(%114) # transformers/modeling_xlnet.py:1159:0
  %116 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
  %117 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
  %118 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1162:0
  %119 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
  %120 : Float(13:13, 13:1) = aten::slice(%non_tgt_mask, %116, %117, %118, %119) # transformers/modeling_xlnet.py:1162:0
  %121 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
  %122 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
  %123 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1162:0
  %124 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
  %125 : Float(13:13, 13:1) = aten::slice(%120, %121, %122, %123, %124) # transformers/modeling_xlnet.py:1162:0
  %126 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1162:0
  %127 : Float(13:13, 13:1, 1:1) = aten::unsqueeze(%125, %126) # transformers/modeling_xlnet.py:1162:0
  %128 : int = prim::Constant[value=3]() # transformers/modeling_xlnet.py:1162:0
  %129 : Float(13:13, 13:1, 1:1, 1:1) = aten::unsqueeze(%127, %128) # transformers/modeling_xlnet.py:1162:0
  %130 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1162:0
  %131 : Float(13:221, 13:17, 17:1, 1:1) = aten::add(%attn_mask, %129, %130) # transformers/modeling_xlnet.py:1162:0
  %132 : int = prim::Constant[value=0]() # torch/tensor.py:22:0
  %133 : Bool(13:221, 13:17, 17:1, 1:1) = aten::gt(%131, %132) # torch/tensor.py:22:0
  %134 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1162:0
  %135 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1162:0
  %136 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
  %137 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1162:0
  %138 : None = prim::Constant()
  %139 : Float(13:221, 13:17, 17:1, 1:1) = aten::to(%133, %134, %135, %136, %137, %138) # transformers/modeling_xlnet.py:1162:0
  %423 : bool = prim::Constant[value=0](), scope: __module.word_embedding # torch/nn/functional.py:1814:0
  %424 : int = prim::Constant[value=-1](), scope: __module.word_embedding # torch/nn/functional.py:1814:0
  %425 : Tensor = prim::GetAttr[name="weight"](%52)
  %input.1 : Float(13:17408, 17:1024, 1024:1) = aten::embedding(%425, %input_ids, %424, %423, %423), scope: __module.word_embedding # torch/nn/functional.py:1814:0
  %427 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %428 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %curr_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.1, %428, %427), scope: __module.dropout # torch/nn/functional.py:973:0
  %142 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
  %143 : int = prim::Constant[value=1024]() # transformers/modeling_xlnet.py:1028:0
  %144 : float = prim::Constant[value=2.]() # transformers/modeling_xlnet.py:1028:0
  %145 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1028:0
  %146 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
  %147 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1028:0
  %148 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1028:0
  %freq_seq : Float(512:1) = aten::arange(%142, %143, %144, %145, %146, %147, %148) # transformers/modeling_xlnet.py:1028:0
  %150 : Long() = prim::Constant[value={1024}]() # transformers/modeling_xlnet.py:1029:0
  %151 : Float(512:1) = aten::div(%freq_seq, %150) # transformers/modeling_xlnet.py:1029:0
  %152 : int = prim::Constant[value=10000]() # transformers/modeling_xlnet.py:1029:0
  %153 : Float(512:1) = aten::pow(%152, %151) # transformers/modeling_xlnet.py:1029:0
  %154 : Float(512:1) = aten::reciprocal(%153) # torch/tensor.py:400:0
  %155 : Long() = prim::Constant[value={1}]() # torch/tensor.py:400:0
  %156 : Float(512:1) = aten::mul(%154, %155) # torch/tensor.py:400:0
  %end : Long() = aten::neg(%qlen) # transformers/modeling_xlnet.py:1033:0
  %158 : Scalar = aten::ScalarImplicit(%end)
  %159 : float = prim::Constant[value=-1.]() # transformers/modeling_xlnet.py:1057:0
  %160 : None = prim::Constant()
  %161 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1057:0
  %162 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1057:0
  %163 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1057:0
  %164 : Float(26:1) = aten::arange(%74, %158, %159, %160, %161, %162, %163) # transformers/modeling_xlnet.py:1057:0
  %165 : str = prim::Constant[value="i,d->id"]() # torch/functional.py:327:0
  %166 : Tensor[] = prim::ListConstruct(%164, %156)
  %sinusoid_inp : Float(26:512, 512:1) = aten::einsum(%165, %166) # torch/functional.py:327:0
  %168 : Float(26:512, 512:1) = aten::sin(%sinusoid_inp) # transformers/modeling_xlnet.py:1018:0
  %169 : Float(26:512, 512:1) = aten::cos(%sinusoid_inp) # transformers/modeling_xlnet.py:1018:0
  %170 : Tensor[] = prim::ListConstruct(%168, %169)
  %171 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1018:0
  %pos_emb.1 : Float(26:1024, 1024:1) = aten::cat(%170, %171) # transformers/modeling_xlnet.py:1018:0
  %173 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
  %174 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
  %175 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1019:0
  %176 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
  %177 : Float(26:1024, 1024:1) = aten::slice(%pos_emb.1, %173, %174, %175, %176) # transformers/modeling_xlnet.py:1019:0
  %178 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
  %179 : Float(26:1024, 1:1024, 1024:1) = aten::unsqueeze(%177, %178) # transformers/modeling_xlnet.py:1019:0
  %180 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1019:0
  %181 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1019:0
  %182 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1019:0
  %183 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1019:0
  %pos_emb.2 : Float(26:1024, 1:1024, 1024:1) = aten::slice(%179, %180, %181, %182, %183) # transformers/modeling_xlnet.py:1019:0
  %185 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1022:0
  %186 : int = prim::Constant[value=-1]() # transformers/modeling_xlnet.py:1022:0
  %187 : int[] = prim::ListConstruct(%185, %65, %186)
  %188 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1022:0
  %pos_emb : Float(26:1024, 17:0, 1024:1) = aten::expand(%pos_emb.2, %187, %188) # transformers/modeling_xlnet.py:1022:0
  %190 : int = prim::Constant[value=6]() # transformers/modeling_xlnet.py:1062:0
  %191 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
  %192 : Device = prim::Constant[value="cpu"]() # transformers/modeling_xlnet.py:1062:0
  %193 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
  %194 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
  %195 : bool = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1062:0
  %196 : None = prim::Constant()
  %input.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%pos_emb, %190, %191, %192, %193, %194, %195, %196) # transformers/modeling_xlnet.py:1062:0
  %430 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %431 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %r.1 : Float(26:1024, 17:0, 1024:1) = aten::dropout(%input.2, %431, %430), scope: __module.dropout # torch/nn/functional.py:973:0
  %199 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %200 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %201 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %202 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.1 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%curr_out.1, %199, %200, %201, %202) # transformers/modeling_xlnet.py:1009:0
  %204 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.1) # transformers/modeling_xlnet.py:1013:0
  %433 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %434 : Device = prim::Constant[value="cpu"](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %435 : int = prim::Constant[value=6](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %436 : bool = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %437 : None = prim::Constant(), scope: __module.layer.0/__module.layer.0.rel_attn
  %438 : int = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %439 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %440 : int = prim::Constant[value=3](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %441 : int = prim::Constant[value=0](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %442 : int = prim::Constant[value=2](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %443 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %444 : Long() = prim::Constant[value={1}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %445 : int = prim::Constant[value=4](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %446 : Long() = prim::Constant[value={0}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %447 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %448 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %449 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %450 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %451 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %452 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %453 : bool = prim::Constant[value=1](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %454 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %455 : int = prim::Constant[value=1024](), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %456 : __torch__.transformers.modeling_xlnet.___torch_mangle_43221.XLNetFeedForward = prim::GetAttr[name="ff"](%50)
  %457 : __torch__.transformers.modeling_xlnet.___torch_mangle_43216.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%50)
  %458 : __torch__.torch.nn.modules.normalization.___torch_mangle_43214.LayerNorm = prim::GetAttr[name="layer_norm"](%457)
  %459 : Tensor = prim::GetAttr[name="o"](%457)
  %460 : Tensor = prim::GetAttr[name="r_r_bias"](%457)
  %461 : Tensor = prim::GetAttr[name="r_w_bias"](%457)
  %462 : Tensor = prim::GetAttr[name="r"](%457)
  %463 : Tensor = prim::GetAttr[name="v"](%457)
  %464 : Tensor = prim::GetAttr[name="k"](%457)
  %465 : Tensor = prim::GetAttr[name="q"](%457)
  %466 : Tensor[] = prim::ListConstruct(%curr_out.1, %465), scope: __module.layer.0/__module.layer.0.rel_attn
  %q_head.1 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%433, %466), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %468 : Tensor[] = prim::ListConstruct(%curr_out.1, %464), scope: __module.layer.0/__module.layer.0.rel_attn
  %469 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%433, %468), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %470 : Tensor[] = prim::ListConstruct(%curr_out.1, %463), scope: __module.layer.0/__module.layer.0.rel_attn
  %471 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%433, %470), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %r.2 : Float(26:1024, 17:0, 1024:1) = aten::to(%r.1, %434, %435, %436, %436, %437), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:443:0
  %473 : Tensor[] = prim::ListConstruct(%r.2, %462), scope: __module.layer.0/__module.layer.0.rel_attn
  %474 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%433, %473), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %475 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %461, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:284:0
  %476 : Tensor[] = prim::ListConstruct(%475, %469), scope: __module.layer.0/__module.layer.0.rel_attn
  %ac.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%439, %476), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %478 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.1, %460, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:287:0
  %479 : Tensor[] = prim::ListConstruct(%478, %474), scope: __module.layer.0/__module.layer.0.rel_attn
  %x.1 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%439, %479), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %481 : int = aten::size(%ac.1, %440), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:288:0
  %482 : int = aten::size(%x.1, %441), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %483 : int = aten::size(%x.1, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %484 : int = aten::size(%x.1, %442), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %485 : int = aten::size(%x.1, %440), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:257:0
  %486 : Long() = prim::NumToTensor(%485), scope: __module.layer.0/__module.layer.0.rel_attn
  %487 : int[] = prim::ListConstruct(%482, %483, %485, %484), scope: __module.layer.0/__module.layer.0.rel_attn
  %x.2 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.1, %487), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:259:0
  %489 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.2, %441, %441, %443, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %490 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%489, %438, %441, %443, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %491 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%490, %442, %438, %443, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.3 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%491, %440, %441, %443, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:260:0
  %493 : Long() = aten::sub(%486, %444, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %494 : int = aten::Int(%493), scope: __module.layer.0/__module.layer.0.rel_attn
  %495 : int[] = prim::ListConstruct(%482, %483, %484, %494), scope: __module.layer.0/__module.layer.0.rel_attn
  %x.4 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.3, %495), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:261:0
  %497 : Long(13:1) = aten::arange(%481, %445, %441, %434, %436), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.4, %440, %497), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:265:0
  %499 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.1, %bd.1, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %500 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%499, %446, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.1 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%500, %447), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:298:0
  %502 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.0/__module.layer.0.rel_attn
  %503 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%448, %502), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %504 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%503, %449), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.1, %504, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.3, %440, %437), scope: __module.layer.0/__module.layer.0.rel_attn # torch/nn/functional.py:1498:0
  %507 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.4, %450, %436), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %508 : Tensor[] = prim::ListConstruct(%507, %471), scope: __module.layer.0/__module.layer.0.rel_attn
  %509 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%451, %508), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %510 : Tensor[] = prim::ListConstruct(%509, %459), scope: __module.layer.0/__module.layer.0.rel_attn
  %input.5 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%452, %510), scope: __module.layer.0/__module.layer.0.rel_attn # torch/functional.py:327:0
  %attn_out.1 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.5, %450, %436), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.6 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.1, %curr_out.1, %438), scope: __module.layer.0/__module.layer.0.rel_attn # transformers/modeling_xlnet.py:329:0
  %514 : Tensor = prim::GetAttr[name="bias"](%458)
  %515 : Tensor = prim::GetAttr[name="weight"](%458)
  %516 : int[] = prim::ListConstruct(%455), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm
  %input_tensor.1 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.6, %516, %515, %514, %454, %453), scope: __module.layer.0/__module.layer.0.rel_attn/__module.layer.0.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %518 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.1, %r.2)
  %519 : Float(13:17408, 17:1024, 1024:1), %520 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%518)
  %521 : __torch__.torch.nn.modules.normalization.___torch_mangle_43217.LayerNorm = prim::GetAttr[name="layer_norm"](%456)
  %522 : __torch__.torch.nn.modules.linear.___torch_mangle_43219.Linear = prim::GetAttr[name="layer_2"](%456)
  %523 : __torch__.torch.nn.modules.linear.___torch_mangle_43218.Linear = prim::GetAttr[name="layer_1"](%456)
  %524 : Tensor = prim::GetAttr[name="bias"](%523)
  %525 : Tensor = prim::GetAttr[name="weight"](%523)
  %526 : Float(1024:1, 4096:1024) = aten::t(%525), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.1 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%519, %526), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.7 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.1, %524, %438), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.8 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.7), scope: __module.layer.0/__module.layer.0.ff # torch/nn/functional.py:1369:0
  %input.9 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.8, %450, %436), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %531 : Tensor = prim::GetAttr[name="bias"](%522)
  %532 : Tensor = prim::GetAttr[name="weight"](%522)
  %533 : Float(4096:1, 1024:4096) = aten::t(%532), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.2 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.9, %533), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.10 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.2, %531, %438), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.10, %450, %436), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.dropout # torch/nn/functional.py:973:0
  %input.11 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.3, %519, %438), scope: __module.layer.0/__module.layer.0.ff # transformers/modeling_xlnet.py:489:0
  %538 : Tensor = prim::GetAttr[name="bias"](%521)
  %539 : Tensor = prim::GetAttr[name="weight"](%521)
  %540 : int[] = prim::ListConstruct(%455), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm
  %curr_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.11, %540, %539, %538, %454, %453), scope: __module.layer.0/__module.layer.0.ff/__module.layer.0.ff.layer_norm # torch/nn/functional.py:2048:0
  %542 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.2, %520)
  %206 : Float(13:17408, 17:1024, 1024:1), %207 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%542)
  %208 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %209 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %210 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %211 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.2 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%206, %208, %209, %210, %211) # transformers/modeling_xlnet.py:1009:0
  %213 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.2) # transformers/modeling_xlnet.py:1013:0
  %543 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %544 : Device = prim::Constant[value="cpu"](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %545 : int = prim::Constant[value=6](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %546 : bool = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %547 : None = prim::Constant(), scope: __module.layer.1/__module.layer.1.rel_attn
  %548 : int = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %549 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %550 : int = prim::Constant[value=3](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %551 : int = prim::Constant[value=0](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %552 : int = prim::Constant[value=2](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %553 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %554 : Long() = prim::Constant[value={1}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %555 : int = prim::Constant[value=4](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %556 : Long() = prim::Constant[value={0}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %557 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %558 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %559 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %560 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %561 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %562 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %563 : bool = prim::Constant[value=1](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %564 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %565 : int = prim::Constant[value=1024](), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %566 : __torch__.transformers.modeling_xlnet.___torch_mangle_43231.XLNetFeedForward = prim::GetAttr[name="ff"](%48)
  %567 : __torch__.transformers.modeling_xlnet.___torch_mangle_43226.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%48)
  %568 : __torch__.torch.nn.modules.normalization.___torch_mangle_43224.LayerNorm = prim::GetAttr[name="layer_norm"](%567)
  %569 : Tensor = prim::GetAttr[name="o"](%567)
  %570 : Tensor = prim::GetAttr[name="r_r_bias"](%567)
  %571 : Tensor = prim::GetAttr[name="r_w_bias"](%567)
  %572 : Tensor = prim::GetAttr[name="r"](%567)
  %573 : Tensor = prim::GetAttr[name="v"](%567)
  %574 : Tensor = prim::GetAttr[name="k"](%567)
  %575 : Tensor = prim::GetAttr[name="q"](%567)
  %576 : Tensor[] = prim::ListConstruct(%206, %575), scope: __module.layer.1/__module.layer.1.rel_attn
  %q_head.2 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%543, %576), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %578 : Tensor[] = prim::ListConstruct(%206, %574), scope: __module.layer.1/__module.layer.1.rel_attn
  %579 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%543, %578), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %580 : Tensor[] = prim::ListConstruct(%206, %573), scope: __module.layer.1/__module.layer.1.rel_attn
  %581 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%543, %580), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %r.3 : Float(26:1024, 17:0, 1024:1) = aten::to(%207, %544, %545, %546, %546, %547), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:443:0
  %583 : Tensor[] = prim::ListConstruct(%r.3, %572), scope: __module.layer.1/__module.layer.1.rel_attn
  %584 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%543, %583), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %585 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %571, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:284:0
  %586 : Tensor[] = prim::ListConstruct(%585, %579), scope: __module.layer.1/__module.layer.1.rel_attn
  %ac.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%549, %586), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %588 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.2, %570, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:287:0
  %589 : Tensor[] = prim::ListConstruct(%588, %584), scope: __module.layer.1/__module.layer.1.rel_attn
  %x.5 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%549, %589), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %591 : int = aten::size(%ac.2, %550), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:288:0
  %592 : int = aten::size(%x.5, %551), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %593 : int = aten::size(%x.5, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %594 : int = aten::size(%x.5, %552), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %595 : int = aten::size(%x.5, %550), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:257:0
  %596 : Long() = prim::NumToTensor(%595), scope: __module.layer.1/__module.layer.1.rel_attn
  %597 : int[] = prim::ListConstruct(%592, %593, %595, %594), scope: __module.layer.1/__module.layer.1.rel_attn
  %x.6 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.5, %597), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:259:0
  %599 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.6, %551, %551, %553, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %600 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%599, %548, %551, %553, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %601 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%600, %552, %548, %553, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.7 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%601, %550, %551, %553, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:260:0
  %603 : Long() = aten::sub(%596, %554, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %604 : int = aten::Int(%603), scope: __module.layer.1/__module.layer.1.rel_attn
  %605 : int[] = prim::ListConstruct(%592, %593, %594, %604), scope: __module.layer.1/__module.layer.1.rel_attn
  %x.8 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.7, %605), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:261:0
  %607 : Long(13:1) = aten::arange(%591, %555, %551, %544, %546), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.8, %550, %607), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:265:0
  %609 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.2, %bd.2, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %610 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%609, %556, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.2 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%610, %557), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:298:0
  %612 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.1/__module.layer.1.rel_attn
  %613 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%558, %612), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %614 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%613, %559), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.2, %614, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.12, %550, %547), scope: __module.layer.1/__module.layer.1.rel_attn # torch/nn/functional.py:1498:0
  %617 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.13, %560, %546), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %618 : Tensor[] = prim::ListConstruct(%617, %581), scope: __module.layer.1/__module.layer.1.rel_attn
  %619 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%561, %618), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %620 : Tensor[] = prim::ListConstruct(%619, %569), scope: __module.layer.1/__module.layer.1.rel_attn
  %input.14 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%562, %620), scope: __module.layer.1/__module.layer.1.rel_attn # torch/functional.py:327:0
  %attn_out.2 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.14, %560, %546), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.15 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.2, %206, %548), scope: __module.layer.1/__module.layer.1.rel_attn # transformers/modeling_xlnet.py:329:0
  %624 : Tensor = prim::GetAttr[name="bias"](%568)
  %625 : Tensor = prim::GetAttr[name="weight"](%568)
  %626 : int[] = prim::ListConstruct(%565), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm
  %input_tensor.2 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.15, %626, %625, %624, %564, %563), scope: __module.layer.1/__module.layer.1.rel_attn/__module.layer.1.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %628 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.2, %r.3)
  %629 : Float(13:17408, 17:1024, 1024:1), %630 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%628)
  %631 : __torch__.torch.nn.modules.normalization.___torch_mangle_43227.LayerNorm = prim::GetAttr[name="layer_norm"](%566)
  %632 : __torch__.torch.nn.modules.linear.___torch_mangle_43229.Linear = prim::GetAttr[name="layer_2"](%566)
  %633 : __torch__.torch.nn.modules.linear.___torch_mangle_43228.Linear = prim::GetAttr[name="layer_1"](%566)
  %634 : Tensor = prim::GetAttr[name="bias"](%633)
  %635 : Tensor = prim::GetAttr[name="weight"](%633)
  %636 : Float(1024:1, 4096:1024) = aten::t(%635), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.4 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%629, %636), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.16 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.4, %634, %548), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.17 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.16), scope: __module.layer.1/__module.layer.1.ff # torch/nn/functional.py:1369:0
  %input.18 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.17, %560, %546), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %641 : Tensor = prim::GetAttr[name="bias"](%632)
  %642 : Tensor = prim::GetAttr[name="weight"](%632)
  %643 : Float(4096:1, 1024:4096) = aten::t(%642), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.5 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.18, %643), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.19 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.5, %641, %548), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.19, %560, %546), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.6, %629, %548), scope: __module.layer.1/__module.layer.1.ff # transformers/modeling_xlnet.py:489:0
  %648 : Tensor = prim::GetAttr[name="bias"](%631)
  %649 : Tensor = prim::GetAttr[name="weight"](%631)
  %650 : int[] = prim::ListConstruct(%565), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm
  %curr_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.20, %650, %649, %648, %564, %563), scope: __module.layer.1/__module.layer.1.ff/__module.layer.1.ff.layer_norm # torch/nn/functional.py:2048:0
  %652 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.3, %630)
  %215 : Float(13:17408, 17:1024, 1024:1), %216 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%652)
  %217 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %218 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %219 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %220 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.3 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%215, %217, %218, %219, %220) # transformers/modeling_xlnet.py:1009:0
  %222 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.3) # transformers/modeling_xlnet.py:1013:0
  %653 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %654 : Device = prim::Constant[value="cpu"](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %655 : int = prim::Constant[value=6](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %656 : bool = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %657 : None = prim::Constant(), scope: __module.layer.2/__module.layer.2.rel_attn
  %658 : int = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %659 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %660 : int = prim::Constant[value=3](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %661 : int = prim::Constant[value=0](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %662 : int = prim::Constant[value=2](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %663 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %664 : Long() = prim::Constant[value={1}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %665 : int = prim::Constant[value=4](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %666 : Long() = prim::Constant[value={0}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %667 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %668 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %669 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %670 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %671 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %672 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %673 : bool = prim::Constant[value=1](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %674 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %675 : int = prim::Constant[value=1024](), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %676 : __torch__.transformers.modeling_xlnet.___torch_mangle_43241.XLNetFeedForward = prim::GetAttr[name="ff"](%46)
  %677 : __torch__.transformers.modeling_xlnet.___torch_mangle_43236.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%46)
  %678 : __torch__.torch.nn.modules.normalization.___torch_mangle_43234.LayerNorm = prim::GetAttr[name="layer_norm"](%677)
  %679 : Tensor = prim::GetAttr[name="o"](%677)
  %680 : Tensor = prim::GetAttr[name="r_r_bias"](%677)
  %681 : Tensor = prim::GetAttr[name="r_w_bias"](%677)
  %682 : Tensor = prim::GetAttr[name="r"](%677)
  %683 : Tensor = prim::GetAttr[name="v"](%677)
  %684 : Tensor = prim::GetAttr[name="k"](%677)
  %685 : Tensor = prim::GetAttr[name="q"](%677)
  %686 : Tensor[] = prim::ListConstruct(%215, %685), scope: __module.layer.2/__module.layer.2.rel_attn
  %q_head.3 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%653, %686), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %688 : Tensor[] = prim::ListConstruct(%215, %684), scope: __module.layer.2/__module.layer.2.rel_attn
  %689 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%653, %688), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %690 : Tensor[] = prim::ListConstruct(%215, %683), scope: __module.layer.2/__module.layer.2.rel_attn
  %691 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%653, %690), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %r.4 : Float(26:1024, 17:0, 1024:1) = aten::to(%216, %654, %655, %656, %656, %657), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:443:0
  %693 : Tensor[] = prim::ListConstruct(%r.4, %682), scope: __module.layer.2/__module.layer.2.rel_attn
  %694 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%653, %693), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %695 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %681, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:284:0
  %696 : Tensor[] = prim::ListConstruct(%695, %689), scope: __module.layer.2/__module.layer.2.rel_attn
  %ac.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%659, %696), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %698 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.3, %680, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:287:0
  %699 : Tensor[] = prim::ListConstruct(%698, %694), scope: __module.layer.2/__module.layer.2.rel_attn
  %x.9 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%659, %699), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %701 : int = aten::size(%ac.3, %660), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:288:0
  %702 : int = aten::size(%x.9, %661), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %703 : int = aten::size(%x.9, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %704 : int = aten::size(%x.9, %662), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %705 : int = aten::size(%x.9, %660), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:257:0
  %706 : Long() = prim::NumToTensor(%705), scope: __module.layer.2/__module.layer.2.rel_attn
  %707 : int[] = prim::ListConstruct(%702, %703, %705, %704), scope: __module.layer.2/__module.layer.2.rel_attn
  %x.10 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.9, %707), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:259:0
  %709 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.10, %661, %661, %663, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %710 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%709, %658, %661, %663, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %711 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%710, %662, %658, %663, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.11 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%711, %660, %661, %663, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:260:0
  %713 : Long() = aten::sub(%706, %664, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %714 : int = aten::Int(%713), scope: __module.layer.2/__module.layer.2.rel_attn
  %715 : int[] = prim::ListConstruct(%702, %703, %704, %714), scope: __module.layer.2/__module.layer.2.rel_attn
  %x.12 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.11, %715), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:261:0
  %717 : Long(13:1) = aten::arange(%701, %665, %661, %654, %656), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.12, %660, %717), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:265:0
  %719 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.3, %bd.3, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %720 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%719, %666, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.3 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%720, %667), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:298:0
  %722 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.2/__module.layer.2.rel_attn
  %723 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%668, %722), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %724 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%723, %669), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.3, %724, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.21, %660, %657), scope: __module.layer.2/__module.layer.2.rel_attn # torch/nn/functional.py:1498:0
  %727 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.22, %670, %656), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %728 : Tensor[] = prim::ListConstruct(%727, %691), scope: __module.layer.2/__module.layer.2.rel_attn
  %729 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%671, %728), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %730 : Tensor[] = prim::ListConstruct(%729, %679), scope: __module.layer.2/__module.layer.2.rel_attn
  %input.23 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%672, %730), scope: __module.layer.2/__module.layer.2.rel_attn # torch/functional.py:327:0
  %attn_out.3 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.23, %670, %656), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.3, %215, %658), scope: __module.layer.2/__module.layer.2.rel_attn # transformers/modeling_xlnet.py:329:0
  %734 : Tensor = prim::GetAttr[name="bias"](%678)
  %735 : Tensor = prim::GetAttr[name="weight"](%678)
  %736 : int[] = prim::ListConstruct(%675), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm
  %input_tensor.3 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.24, %736, %735, %734, %674, %673), scope: __module.layer.2/__module.layer.2.rel_attn/__module.layer.2.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %738 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.3, %r.4)
  %739 : Float(13:17408, 17:1024, 1024:1), %740 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%738)
  %741 : __torch__.torch.nn.modules.normalization.___torch_mangle_43237.LayerNorm = prim::GetAttr[name="layer_norm"](%676)
  %742 : __torch__.torch.nn.modules.linear.___torch_mangle_43239.Linear = prim::GetAttr[name="layer_2"](%676)
  %743 : __torch__.torch.nn.modules.linear.___torch_mangle_43238.Linear = prim::GetAttr[name="layer_1"](%676)
  %744 : Tensor = prim::GetAttr[name="bias"](%743)
  %745 : Tensor = prim::GetAttr[name="weight"](%743)
  %746 : Float(1024:1, 4096:1024) = aten::t(%745), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.7 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%739, %746), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.25 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.7, %744, %658), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.26 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.25), scope: __module.layer.2/__module.layer.2.ff # torch/nn/functional.py:1369:0
  %input.27 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.26, %670, %656), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %751 : Tensor = prim::GetAttr[name="bias"](%742)
  %752 : Tensor = prim::GetAttr[name="weight"](%742)
  %753 : Float(4096:1, 1024:4096) = aten::t(%752), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.8 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.27, %753), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.28 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.8, %751, %658), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.28, %670, %656), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.dropout # torch/nn/functional.py:973:0
  %input.29 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.9, %739, %658), scope: __module.layer.2/__module.layer.2.ff # transformers/modeling_xlnet.py:489:0
  %758 : Tensor = prim::GetAttr[name="bias"](%741)
  %759 : Tensor = prim::GetAttr[name="weight"](%741)
  %760 : int[] = prim::ListConstruct(%675), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm
  %curr_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.29, %760, %759, %758, %674, %673), scope: __module.layer.2/__module.layer.2.ff/__module.layer.2.ff.layer_norm # torch/nn/functional.py:2048:0
  %762 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.4, %740)
  %224 : Float(13:17408, 17:1024, 1024:1), %225 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%762)
  %226 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %227 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %228 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %229 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.4 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%224, %226, %227, %228, %229) # transformers/modeling_xlnet.py:1009:0
  %231 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.4) # transformers/modeling_xlnet.py:1013:0
  %763 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %764 : Device = prim::Constant[value="cpu"](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %765 : int = prim::Constant[value=6](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %766 : bool = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %767 : None = prim::Constant(), scope: __module.layer.3/__module.layer.3.rel_attn
  %768 : int = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %769 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %770 : int = prim::Constant[value=3](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %771 : int = prim::Constant[value=0](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %772 : int = prim::Constant[value=2](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %773 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %774 : Long() = prim::Constant[value={1}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %775 : int = prim::Constant[value=4](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %776 : Long() = prim::Constant[value={0}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %777 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %778 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %779 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %780 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %781 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %782 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %783 : bool = prim::Constant[value=1](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %784 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %785 : int = prim::Constant[value=1024](), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %786 : __torch__.transformers.modeling_xlnet.___torch_mangle_43251.XLNetFeedForward = prim::GetAttr[name="ff"](%44)
  %787 : __torch__.transformers.modeling_xlnet.___torch_mangle_43246.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%44)
  %788 : __torch__.torch.nn.modules.normalization.___torch_mangle_43244.LayerNorm = prim::GetAttr[name="layer_norm"](%787)
  %789 : Tensor = prim::GetAttr[name="o"](%787)
  %790 : Tensor = prim::GetAttr[name="r_r_bias"](%787)
  %791 : Tensor = prim::GetAttr[name="r_w_bias"](%787)
  %792 : Tensor = prim::GetAttr[name="r"](%787)
  %793 : Tensor = prim::GetAttr[name="v"](%787)
  %794 : Tensor = prim::GetAttr[name="k"](%787)
  %795 : Tensor = prim::GetAttr[name="q"](%787)
  %796 : Tensor[] = prim::ListConstruct(%224, %795), scope: __module.layer.3/__module.layer.3.rel_attn
  %q_head.4 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%763, %796), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %798 : Tensor[] = prim::ListConstruct(%224, %794), scope: __module.layer.3/__module.layer.3.rel_attn
  %799 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%763, %798), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %800 : Tensor[] = prim::ListConstruct(%224, %793), scope: __module.layer.3/__module.layer.3.rel_attn
  %801 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%763, %800), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %r.5 : Float(26:1024, 17:0, 1024:1) = aten::to(%225, %764, %765, %766, %766, %767), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:443:0
  %803 : Tensor[] = prim::ListConstruct(%r.5, %792), scope: __module.layer.3/__module.layer.3.rel_attn
  %804 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%763, %803), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %805 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %791, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:284:0
  %806 : Tensor[] = prim::ListConstruct(%805, %799), scope: __module.layer.3/__module.layer.3.rel_attn
  %ac.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%769, %806), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %808 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.4, %790, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:287:0
  %809 : Tensor[] = prim::ListConstruct(%808, %804), scope: __module.layer.3/__module.layer.3.rel_attn
  %x.13 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%769, %809), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %811 : int = aten::size(%ac.4, %770), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:288:0
  %812 : int = aten::size(%x.13, %771), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %813 : int = aten::size(%x.13, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %814 : int = aten::size(%x.13, %772), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %815 : int = aten::size(%x.13, %770), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:257:0
  %816 : Long() = prim::NumToTensor(%815), scope: __module.layer.3/__module.layer.3.rel_attn
  %817 : int[] = prim::ListConstruct(%812, %813, %815, %814), scope: __module.layer.3/__module.layer.3.rel_attn
  %x.14 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.13, %817), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:259:0
  %819 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.14, %771, %771, %773, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %820 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%819, %768, %771, %773, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %821 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%820, %772, %768, %773, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.15 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%821, %770, %771, %773, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:260:0
  %823 : Long() = aten::sub(%816, %774, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %824 : int = aten::Int(%823), scope: __module.layer.3/__module.layer.3.rel_attn
  %825 : int[] = prim::ListConstruct(%812, %813, %814, %824), scope: __module.layer.3/__module.layer.3.rel_attn
  %x.16 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.15, %825), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:261:0
  %827 : Long(13:1) = aten::arange(%811, %775, %771, %764, %766), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.16, %770, %827), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:265:0
  %829 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.4, %bd.4, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %830 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%829, %776, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.4 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%830, %777), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:298:0
  %832 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.3/__module.layer.3.rel_attn
  %833 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%778, %832), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %834 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%833, %779), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.30 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.4, %834, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.31 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.30, %770, %767), scope: __module.layer.3/__module.layer.3.rel_attn # torch/nn/functional.py:1498:0
  %837 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.31, %780, %766), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %838 : Tensor[] = prim::ListConstruct(%837, %801), scope: __module.layer.3/__module.layer.3.rel_attn
  %839 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%781, %838), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %840 : Tensor[] = prim::ListConstruct(%839, %789), scope: __module.layer.3/__module.layer.3.rel_attn
  %input.32 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%782, %840), scope: __module.layer.3/__module.layer.3.rel_attn # torch/functional.py:327:0
  %attn_out.4 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.32, %780, %766), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.33 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.4, %224, %768), scope: __module.layer.3/__module.layer.3.rel_attn # transformers/modeling_xlnet.py:329:0
  %844 : Tensor = prim::GetAttr[name="bias"](%788)
  %845 : Tensor = prim::GetAttr[name="weight"](%788)
  %846 : int[] = prim::ListConstruct(%785), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm
  %input_tensor.4 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.33, %846, %845, %844, %784, %783), scope: __module.layer.3/__module.layer.3.rel_attn/__module.layer.3.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %848 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.4, %r.5)
  %849 : Float(13:17408, 17:1024, 1024:1), %850 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%848)
  %851 : __torch__.torch.nn.modules.normalization.___torch_mangle_43247.LayerNorm = prim::GetAttr[name="layer_norm"](%786)
  %852 : __torch__.torch.nn.modules.linear.___torch_mangle_43249.Linear = prim::GetAttr[name="layer_2"](%786)
  %853 : __torch__.torch.nn.modules.linear.___torch_mangle_43248.Linear = prim::GetAttr[name="layer_1"](%786)
  %854 : Tensor = prim::GetAttr[name="bias"](%853)
  %855 : Tensor = prim::GetAttr[name="weight"](%853)
  %856 : Float(1024:1, 4096:1024) = aten::t(%855), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.10 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%849, %856), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.34 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.10, %854, %768), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.35 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.34), scope: __module.layer.3/__module.layer.3.ff # torch/nn/functional.py:1369:0
  %input.36 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.35, %780, %766), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %861 : Tensor = prim::GetAttr[name="bias"](%852)
  %862 : Tensor = prim::GetAttr[name="weight"](%852)
  %863 : Float(4096:1, 1024:4096) = aten::t(%862), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.11 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.36, %863), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.37 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.11, %861, %768), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.37, %780, %766), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.dropout # torch/nn/functional.py:973:0
  %input.38 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.12, %849, %768), scope: __module.layer.3/__module.layer.3.ff # transformers/modeling_xlnet.py:489:0
  %868 : Tensor = prim::GetAttr[name="bias"](%851)
  %869 : Tensor = prim::GetAttr[name="weight"](%851)
  %870 : int[] = prim::ListConstruct(%785), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm
  %curr_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.38, %870, %869, %868, %784, %783), scope: __module.layer.3/__module.layer.3.ff/__module.layer.3.ff.layer_norm # torch/nn/functional.py:2048:0
  %872 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.5, %850)
  %233 : Float(13:17408, 17:1024, 1024:1), %234 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%872)
  %235 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %236 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %237 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %238 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.5 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%233, %235, %236, %237, %238) # transformers/modeling_xlnet.py:1009:0
  %240 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.5) # transformers/modeling_xlnet.py:1013:0
  %873 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %874 : Device = prim::Constant[value="cpu"](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %875 : int = prim::Constant[value=6](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %876 : bool = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %877 : None = prim::Constant(), scope: __module.layer.4/__module.layer.4.rel_attn
  %878 : int = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %879 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %880 : int = prim::Constant[value=3](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %881 : int = prim::Constant[value=0](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %882 : int = prim::Constant[value=2](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %883 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %884 : Long() = prim::Constant[value={1}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %885 : int = prim::Constant[value=4](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %886 : Long() = prim::Constant[value={0}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %887 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %888 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %889 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %890 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %891 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %892 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %893 : bool = prim::Constant[value=1](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %894 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %895 : int = prim::Constant[value=1024](), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %896 : __torch__.transformers.modeling_xlnet.___torch_mangle_43261.XLNetFeedForward = prim::GetAttr[name="ff"](%42)
  %897 : __torch__.transformers.modeling_xlnet.___torch_mangle_43256.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%42)
  %898 : __torch__.torch.nn.modules.normalization.___torch_mangle_43254.LayerNorm = prim::GetAttr[name="layer_norm"](%897)
  %899 : Tensor = prim::GetAttr[name="o"](%897)
  %900 : Tensor = prim::GetAttr[name="r_r_bias"](%897)
  %901 : Tensor = prim::GetAttr[name="r_w_bias"](%897)
  %902 : Tensor = prim::GetAttr[name="r"](%897)
  %903 : Tensor = prim::GetAttr[name="v"](%897)
  %904 : Tensor = prim::GetAttr[name="k"](%897)
  %905 : Tensor = prim::GetAttr[name="q"](%897)
  %906 : Tensor[] = prim::ListConstruct(%233, %905), scope: __module.layer.4/__module.layer.4.rel_attn
  %q_head.5 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%873, %906), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %908 : Tensor[] = prim::ListConstruct(%233, %904), scope: __module.layer.4/__module.layer.4.rel_attn
  %909 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%873, %908), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %910 : Tensor[] = prim::ListConstruct(%233, %903), scope: __module.layer.4/__module.layer.4.rel_attn
  %911 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%873, %910), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %r.6 : Float(26:1024, 17:0, 1024:1) = aten::to(%234, %874, %875, %876, %876, %877), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:443:0
  %913 : Tensor[] = prim::ListConstruct(%r.6, %902), scope: __module.layer.4/__module.layer.4.rel_attn
  %914 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%873, %913), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %915 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %901, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:284:0
  %916 : Tensor[] = prim::ListConstruct(%915, %909), scope: __module.layer.4/__module.layer.4.rel_attn
  %ac.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%879, %916), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %918 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.5, %900, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:287:0
  %919 : Tensor[] = prim::ListConstruct(%918, %914), scope: __module.layer.4/__module.layer.4.rel_attn
  %x.17 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%879, %919), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %921 : int = aten::size(%ac.5, %880), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:288:0
  %922 : int = aten::size(%x.17, %881), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %923 : int = aten::size(%x.17, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %924 : int = aten::size(%x.17, %882), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %925 : int = aten::size(%x.17, %880), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:257:0
  %926 : Long() = prim::NumToTensor(%925), scope: __module.layer.4/__module.layer.4.rel_attn
  %927 : int[] = prim::ListConstruct(%922, %923, %925, %924), scope: __module.layer.4/__module.layer.4.rel_attn
  %x.18 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.17, %927), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:259:0
  %929 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.18, %881, %881, %883, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %930 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%929, %878, %881, %883, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %931 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%930, %882, %878, %883, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.19 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%931, %880, %881, %883, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:260:0
  %933 : Long() = aten::sub(%926, %884, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %934 : int = aten::Int(%933), scope: __module.layer.4/__module.layer.4.rel_attn
  %935 : int[] = prim::ListConstruct(%922, %923, %924, %934), scope: __module.layer.4/__module.layer.4.rel_attn
  %x.20 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.19, %935), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:261:0
  %937 : Long(13:1) = aten::arange(%921, %885, %881, %874, %876), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.20, %880, %937), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:265:0
  %939 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.5, %bd.5, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %940 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%939, %886, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.5 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%940, %887), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:298:0
  %942 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.4/__module.layer.4.rel_attn
  %943 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%888, %942), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %944 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%943, %889), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.39 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.5, %944, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.40 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.39, %880, %877), scope: __module.layer.4/__module.layer.4.rel_attn # torch/nn/functional.py:1498:0
  %947 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.40, %890, %876), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %948 : Tensor[] = prim::ListConstruct(%947, %911), scope: __module.layer.4/__module.layer.4.rel_attn
  %949 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%891, %948), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %950 : Tensor[] = prim::ListConstruct(%949, %899), scope: __module.layer.4/__module.layer.4.rel_attn
  %input.41 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%892, %950), scope: __module.layer.4/__module.layer.4.rel_attn # torch/functional.py:327:0
  %attn_out.5 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.41, %890, %876), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.42 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.5, %233, %878), scope: __module.layer.4/__module.layer.4.rel_attn # transformers/modeling_xlnet.py:329:0
  %954 : Tensor = prim::GetAttr[name="bias"](%898)
  %955 : Tensor = prim::GetAttr[name="weight"](%898)
  %956 : int[] = prim::ListConstruct(%895), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm
  %input_tensor.5 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.42, %956, %955, %954, %894, %893), scope: __module.layer.4/__module.layer.4.rel_attn/__module.layer.4.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %958 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.5, %r.6)
  %959 : Float(13:17408, 17:1024, 1024:1), %960 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%958)
  %961 : __torch__.torch.nn.modules.normalization.___torch_mangle_43257.LayerNorm = prim::GetAttr[name="layer_norm"](%896)
  %962 : __torch__.torch.nn.modules.linear.___torch_mangle_43259.Linear = prim::GetAttr[name="layer_2"](%896)
  %963 : __torch__.torch.nn.modules.linear.___torch_mangle_43258.Linear = prim::GetAttr[name="layer_1"](%896)
  %964 : Tensor = prim::GetAttr[name="bias"](%963)
  %965 : Tensor = prim::GetAttr[name="weight"](%963)
  %966 : Float(1024:1, 4096:1024) = aten::t(%965), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.13 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%959, %966), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.43 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.13, %964, %878), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.44 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.43), scope: __module.layer.4/__module.layer.4.ff # torch/nn/functional.py:1369:0
  %input.45 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.44, %890, %876), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %971 : Tensor = prim::GetAttr[name="bias"](%962)
  %972 : Tensor = prim::GetAttr[name="weight"](%962)
  %973 : Float(4096:1, 1024:4096) = aten::t(%972), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.14 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.45, %973), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.46 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.14, %971, %878), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.46, %890, %876), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.dropout # torch/nn/functional.py:973:0
  %input.47 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.15, %959, %878), scope: __module.layer.4/__module.layer.4.ff # transformers/modeling_xlnet.py:489:0
  %978 : Tensor = prim::GetAttr[name="bias"](%961)
  %979 : Tensor = prim::GetAttr[name="weight"](%961)
  %980 : int[] = prim::ListConstruct(%895), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm
  %curr_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.47, %980, %979, %978, %894, %893), scope: __module.layer.4/__module.layer.4.ff/__module.layer.4.ff.layer_norm # torch/nn/functional.py:2048:0
  %982 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.6, %960)
  %242 : Float(13:17408, 17:1024, 1024:1), %243 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%982)
  %244 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %245 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %246 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %247 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.6 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%242, %244, %245, %246, %247) # transformers/modeling_xlnet.py:1009:0
  %249 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.6) # transformers/modeling_xlnet.py:1013:0
  %983 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %984 : Device = prim::Constant[value="cpu"](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %985 : int = prim::Constant[value=6](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %986 : bool = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %987 : None = prim::Constant(), scope: __module.layer.5/__module.layer.5.rel_attn
  %988 : int = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %989 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %990 : int = prim::Constant[value=3](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %991 : int = prim::Constant[value=0](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %992 : int = prim::Constant[value=2](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %993 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %994 : Long() = prim::Constant[value={1}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %995 : int = prim::Constant[value=4](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %996 : Long() = prim::Constant[value={0}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %997 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %998 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %999 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %1000 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %1001 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1002 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1003 : bool = prim::Constant[value=1](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1004 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1005 : int = prim::Constant[value=1024](), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1006 : __torch__.transformers.modeling_xlnet.___torch_mangle_43271.XLNetFeedForward = prim::GetAttr[name="ff"](%40)
  %1007 : __torch__.transformers.modeling_xlnet.___torch_mangle_43266.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%40)
  %1008 : __torch__.torch.nn.modules.normalization.___torch_mangle_43264.LayerNorm = prim::GetAttr[name="layer_norm"](%1007)
  %1009 : Tensor = prim::GetAttr[name="o"](%1007)
  %1010 : Tensor = prim::GetAttr[name="r_r_bias"](%1007)
  %1011 : Tensor = prim::GetAttr[name="r_w_bias"](%1007)
  %1012 : Tensor = prim::GetAttr[name="r"](%1007)
  %1013 : Tensor = prim::GetAttr[name="v"](%1007)
  %1014 : Tensor = prim::GetAttr[name="k"](%1007)
  %1015 : Tensor = prim::GetAttr[name="q"](%1007)
  %1016 : Tensor[] = prim::ListConstruct(%242, %1015), scope: __module.layer.5/__module.layer.5.rel_attn
  %q_head.6 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%983, %1016), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1018 : Tensor[] = prim::ListConstruct(%242, %1014), scope: __module.layer.5/__module.layer.5.rel_attn
  %1019 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%983, %1018), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1020 : Tensor[] = prim::ListConstruct(%242, %1013), scope: __module.layer.5/__module.layer.5.rel_attn
  %1021 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%983, %1020), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %r.7 : Float(26:1024, 17:0, 1024:1) = aten::to(%243, %984, %985, %986, %986, %987), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:443:0
  %1023 : Tensor[] = prim::ListConstruct(%r.7, %1012), scope: __module.layer.5/__module.layer.5.rel_attn
  %1024 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%983, %1023), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1025 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %1011, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:284:0
  %1026 : Tensor[] = prim::ListConstruct(%1025, %1019), scope: __module.layer.5/__module.layer.5.rel_attn
  %ac.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%989, %1026), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1028 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.6, %1010, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:287:0
  %1029 : Tensor[] = prim::ListConstruct(%1028, %1024), scope: __module.layer.5/__module.layer.5.rel_attn
  %x.21 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%989, %1029), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1031 : int = aten::size(%ac.6, %990), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:288:0
  %1032 : int = aten::size(%x.21, %991), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %1033 : int = aten::size(%x.21, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %1034 : int = aten::size(%x.21, %992), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %1035 : int = aten::size(%x.21, %990), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:257:0
  %1036 : Long() = prim::NumToTensor(%1035), scope: __module.layer.5/__module.layer.5.rel_attn
  %1037 : int[] = prim::ListConstruct(%1032, %1033, %1035, %1034), scope: __module.layer.5/__module.layer.5.rel_attn
  %x.22 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.21, %1037), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:259:0
  %1039 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.22, %991, %991, %993, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %1040 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1039, %988, %991, %993, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %1041 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1040, %992, %988, %993, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.23 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1041, %990, %991, %993, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:260:0
  %1043 : Long() = aten::sub(%1036, %994, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %1044 : int = aten::Int(%1043), scope: __module.layer.5/__module.layer.5.rel_attn
  %1045 : int[] = prim::ListConstruct(%1032, %1033, %1034, %1044), scope: __module.layer.5/__module.layer.5.rel_attn
  %x.24 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.23, %1045), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:261:0
  %1047 : Long(13:1) = aten::arange(%1031, %995, %991, %984, %986), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.24, %990, %1047), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:265:0
  %1049 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.6, %bd.6, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %1050 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1049, %996, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.6 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1050, %997), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:298:0
  %1052 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.5/__module.layer.5.rel_attn
  %1053 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%998, %1052), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1054 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1053, %999), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.48 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.6, %1054, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.49 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.48, %990, %987), scope: __module.layer.5/__module.layer.5.rel_attn # torch/nn/functional.py:1498:0
  %1057 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.49, %1000, %986), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %1058 : Tensor[] = prim::ListConstruct(%1057, %1021), scope: __module.layer.5/__module.layer.5.rel_attn
  %1059 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1001, %1058), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %1060 : Tensor[] = prim::ListConstruct(%1059, %1009), scope: __module.layer.5/__module.layer.5.rel_attn
  %input.50 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1002, %1060), scope: __module.layer.5/__module.layer.5.rel_attn # torch/functional.py:327:0
  %attn_out.6 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.50, %1000, %986), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.51 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.6, %242, %988), scope: __module.layer.5/__module.layer.5.rel_attn # transformers/modeling_xlnet.py:329:0
  %1064 : Tensor = prim::GetAttr[name="bias"](%1008)
  %1065 : Tensor = prim::GetAttr[name="weight"](%1008)
  %1066 : int[] = prim::ListConstruct(%1005), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm
  %input_tensor.6 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.51, %1066, %1065, %1064, %1004, %1003), scope: __module.layer.5/__module.layer.5.rel_attn/__module.layer.5.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1068 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.6, %r.7)
  %1069 : Float(13:17408, 17:1024, 1024:1), %1070 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1068)
  %1071 : __torch__.torch.nn.modules.normalization.___torch_mangle_43267.LayerNorm = prim::GetAttr[name="layer_norm"](%1006)
  %1072 : __torch__.torch.nn.modules.linear.___torch_mangle_43269.Linear = prim::GetAttr[name="layer_2"](%1006)
  %1073 : __torch__.torch.nn.modules.linear.___torch_mangle_43268.Linear = prim::GetAttr[name="layer_1"](%1006)
  %1074 : Tensor = prim::GetAttr[name="bias"](%1073)
  %1075 : Tensor = prim::GetAttr[name="weight"](%1073)
  %1076 : Float(1024:1, 4096:1024) = aten::t(%1075), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.16 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1069, %1076), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.52 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.16, %1074, %988), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.53 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.52), scope: __module.layer.5/__module.layer.5.ff # torch/nn/functional.py:1369:0
  %input.54 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.53, %1000, %986), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %1081 : Tensor = prim::GetAttr[name="bias"](%1072)
  %1082 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1083 : Float(4096:1, 1024:4096) = aten::t(%1082), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.17 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.54, %1083), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.55 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.17, %1081, %988), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.55, %1000, %986), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.dropout # torch/nn/functional.py:973:0
  %input.56 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.18, %1069, %988), scope: __module.layer.5/__module.layer.5.ff # transformers/modeling_xlnet.py:489:0
  %1088 : Tensor = prim::GetAttr[name="bias"](%1071)
  %1089 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1090 : int[] = prim::ListConstruct(%1005), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm
  %curr_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.56, %1090, %1089, %1088, %1004, %1003), scope: __module.layer.5/__module.layer.5.ff/__module.layer.5.ff.layer_norm # torch/nn/functional.py:2048:0
  %1092 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.7, %1070)
  %251 : Float(13:17408, 17:1024, 1024:1), %252 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1092)
  %253 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %254 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %255 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %256 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.7 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%251, %253, %254, %255, %256) # transformers/modeling_xlnet.py:1009:0
  %258 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.7) # transformers/modeling_xlnet.py:1013:0
  %1093 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1094 : Device = prim::Constant[value="cpu"](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %1095 : int = prim::Constant[value=6](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %1096 : bool = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %1097 : None = prim::Constant(), scope: __module.layer.6/__module.layer.6.rel_attn
  %1098 : int = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %1099 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1100 : int = prim::Constant[value=3](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %1101 : int = prim::Constant[value=0](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1102 : int = prim::Constant[value=2](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1103 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %1104 : Long() = prim::Constant[value={1}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %1105 : int = prim::Constant[value=4](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %1106 : Long() = prim::Constant[value={0}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %1107 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %1108 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1109 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %1110 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %1111 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1112 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1113 : bool = prim::Constant[value=1](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1114 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1115 : int = prim::Constant[value=1024](), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1116 : __torch__.transformers.modeling_xlnet.___torch_mangle_43281.XLNetFeedForward = prim::GetAttr[name="ff"](%38)
  %1117 : __torch__.transformers.modeling_xlnet.___torch_mangle_43276.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%38)
  %1118 : __torch__.torch.nn.modules.normalization.___torch_mangle_43274.LayerNorm = prim::GetAttr[name="layer_norm"](%1117)
  %1119 : Tensor = prim::GetAttr[name="o"](%1117)
  %1120 : Tensor = prim::GetAttr[name="r_r_bias"](%1117)
  %1121 : Tensor = prim::GetAttr[name="r_w_bias"](%1117)
  %1122 : Tensor = prim::GetAttr[name="r"](%1117)
  %1123 : Tensor = prim::GetAttr[name="v"](%1117)
  %1124 : Tensor = prim::GetAttr[name="k"](%1117)
  %1125 : Tensor = prim::GetAttr[name="q"](%1117)
  %1126 : Tensor[] = prim::ListConstruct(%251, %1125), scope: __module.layer.6/__module.layer.6.rel_attn
  %q_head.7 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1093, %1126), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1128 : Tensor[] = prim::ListConstruct(%251, %1124), scope: __module.layer.6/__module.layer.6.rel_attn
  %1129 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1093, %1128), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1130 : Tensor[] = prim::ListConstruct(%251, %1123), scope: __module.layer.6/__module.layer.6.rel_attn
  %1131 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1093, %1130), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %r.8 : Float(26:1024, 17:0, 1024:1) = aten::to(%252, %1094, %1095, %1096, %1096, %1097), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:443:0
  %1133 : Tensor[] = prim::ListConstruct(%r.8, %1122), scope: __module.layer.6/__module.layer.6.rel_attn
  %1134 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1093, %1133), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1135 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %1121, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:284:0
  %1136 : Tensor[] = prim::ListConstruct(%1135, %1129), scope: __module.layer.6/__module.layer.6.rel_attn
  %ac.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1099, %1136), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1138 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.7, %1120, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:287:0
  %1139 : Tensor[] = prim::ListConstruct(%1138, %1134), scope: __module.layer.6/__module.layer.6.rel_attn
  %x.25 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1099, %1139), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1141 : int = aten::size(%ac.7, %1100), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:288:0
  %1142 : int = aten::size(%x.25, %1101), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1143 : int = aten::size(%x.25, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1144 : int = aten::size(%x.25, %1102), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1145 : int = aten::size(%x.25, %1100), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:257:0
  %1146 : Long() = prim::NumToTensor(%1145), scope: __module.layer.6/__module.layer.6.rel_attn
  %1147 : int[] = prim::ListConstruct(%1142, %1143, %1145, %1144), scope: __module.layer.6/__module.layer.6.rel_attn
  %x.26 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.25, %1147), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:259:0
  %1149 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.26, %1101, %1101, %1103, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %1150 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1149, %1098, %1101, %1103, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %1151 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1150, %1102, %1098, %1103, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.27 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1151, %1100, %1101, %1103, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:260:0
  %1153 : Long() = aten::sub(%1146, %1104, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %1154 : int = aten::Int(%1153), scope: __module.layer.6/__module.layer.6.rel_attn
  %1155 : int[] = prim::ListConstruct(%1142, %1143, %1144, %1154), scope: __module.layer.6/__module.layer.6.rel_attn
  %x.28 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.27, %1155), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:261:0
  %1157 : Long(13:1) = aten::arange(%1141, %1105, %1101, %1094, %1096), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.28, %1100, %1157), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:265:0
  %1159 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.7, %bd.7, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %1160 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1159, %1106, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.7 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1160, %1107), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:298:0
  %1162 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.6/__module.layer.6.rel_attn
  %1163 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1108, %1162), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1164 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1163, %1109), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.57 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.7, %1164, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.58 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.57, %1100, %1097), scope: __module.layer.6/__module.layer.6.rel_attn # torch/nn/functional.py:1498:0
  %1167 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.58, %1110, %1096), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %1168 : Tensor[] = prim::ListConstruct(%1167, %1131), scope: __module.layer.6/__module.layer.6.rel_attn
  %1169 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1111, %1168), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %1170 : Tensor[] = prim::ListConstruct(%1169, %1119), scope: __module.layer.6/__module.layer.6.rel_attn
  %input.59 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1112, %1170), scope: __module.layer.6/__module.layer.6.rel_attn # torch/functional.py:327:0
  %attn_out.7 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.59, %1110, %1096), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.7, %251, %1098), scope: __module.layer.6/__module.layer.6.rel_attn # transformers/modeling_xlnet.py:329:0
  %1174 : Tensor = prim::GetAttr[name="bias"](%1118)
  %1175 : Tensor = prim::GetAttr[name="weight"](%1118)
  %1176 : int[] = prim::ListConstruct(%1115), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm
  %input_tensor.7 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.60, %1176, %1175, %1174, %1114, %1113), scope: __module.layer.6/__module.layer.6.rel_attn/__module.layer.6.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1178 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.7, %r.8)
  %1179 : Float(13:17408, 17:1024, 1024:1), %1180 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1178)
  %1181 : __torch__.torch.nn.modules.normalization.___torch_mangle_43277.LayerNorm = prim::GetAttr[name="layer_norm"](%1116)
  %1182 : __torch__.torch.nn.modules.linear.___torch_mangle_43279.Linear = prim::GetAttr[name="layer_2"](%1116)
  %1183 : __torch__.torch.nn.modules.linear.___torch_mangle_43278.Linear = prim::GetAttr[name="layer_1"](%1116)
  %1184 : Tensor = prim::GetAttr[name="bias"](%1183)
  %1185 : Tensor = prim::GetAttr[name="weight"](%1183)
  %1186 : Float(1024:1, 4096:1024) = aten::t(%1185), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.19 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1179, %1186), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.61 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.19, %1184, %1098), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.62 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.61), scope: __module.layer.6/__module.layer.6.ff # torch/nn/functional.py:1369:0
  %input.63 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.62, %1110, %1096), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %1191 : Tensor = prim::GetAttr[name="bias"](%1182)
  %1192 : Tensor = prim::GetAttr[name="weight"](%1182)
  %1193 : Float(4096:1, 1024:4096) = aten::t(%1192), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.20 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.63, %1193), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.64 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.20, %1191, %1098), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.64, %1110, %1096), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.dropout # torch/nn/functional.py:973:0
  %input.65 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.21, %1179, %1098), scope: __module.layer.6/__module.layer.6.ff # transformers/modeling_xlnet.py:489:0
  %1198 : Tensor = prim::GetAttr[name="bias"](%1181)
  %1199 : Tensor = prim::GetAttr[name="weight"](%1181)
  %1200 : int[] = prim::ListConstruct(%1115), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm
  %curr_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.65, %1200, %1199, %1198, %1114, %1113), scope: __module.layer.6/__module.layer.6.ff/__module.layer.6.ff.layer_norm # torch/nn/functional.py:2048:0
  %1202 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.8, %1180)
  %260 : Float(13:17408, 17:1024, 1024:1), %261 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1202)
  %262 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %263 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %264 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %265 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.8 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%260, %262, %263, %264, %265) # transformers/modeling_xlnet.py:1009:0
  %267 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.8) # transformers/modeling_xlnet.py:1013:0
  %1203 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1204 : Device = prim::Constant[value="cpu"](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %1205 : int = prim::Constant[value=6](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %1206 : bool = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %1207 : None = prim::Constant(), scope: __module.layer.7/__module.layer.7.rel_attn
  %1208 : int = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %1209 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1210 : int = prim::Constant[value=3](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %1211 : int = prim::Constant[value=0](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1212 : int = prim::Constant[value=2](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1213 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %1214 : Long() = prim::Constant[value={1}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %1215 : int = prim::Constant[value=4](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %1216 : Long() = prim::Constant[value={0}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %1217 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %1218 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1219 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %1220 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %1221 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1222 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1223 : bool = prim::Constant[value=1](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1224 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1225 : int = prim::Constant[value=1024](), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1226 : __torch__.transformers.modeling_xlnet.___torch_mangle_43291.XLNetFeedForward = prim::GetAttr[name="ff"](%36)
  %1227 : __torch__.transformers.modeling_xlnet.___torch_mangle_43286.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%36)
  %1228 : __torch__.torch.nn.modules.normalization.___torch_mangle_43284.LayerNorm = prim::GetAttr[name="layer_norm"](%1227)
  %1229 : Tensor = prim::GetAttr[name="o"](%1227)
  %1230 : Tensor = prim::GetAttr[name="r_r_bias"](%1227)
  %1231 : Tensor = prim::GetAttr[name="r_w_bias"](%1227)
  %1232 : Tensor = prim::GetAttr[name="r"](%1227)
  %1233 : Tensor = prim::GetAttr[name="v"](%1227)
  %1234 : Tensor = prim::GetAttr[name="k"](%1227)
  %1235 : Tensor = prim::GetAttr[name="q"](%1227)
  %1236 : Tensor[] = prim::ListConstruct(%260, %1235), scope: __module.layer.7/__module.layer.7.rel_attn
  %q_head.8 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1203, %1236), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1238 : Tensor[] = prim::ListConstruct(%260, %1234), scope: __module.layer.7/__module.layer.7.rel_attn
  %1239 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1203, %1238), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1240 : Tensor[] = prim::ListConstruct(%260, %1233), scope: __module.layer.7/__module.layer.7.rel_attn
  %1241 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1203, %1240), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %r.9 : Float(26:1024, 17:0, 1024:1) = aten::to(%261, %1204, %1205, %1206, %1206, %1207), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:443:0
  %1243 : Tensor[] = prim::ListConstruct(%r.9, %1232), scope: __module.layer.7/__module.layer.7.rel_attn
  %1244 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1203, %1243), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1245 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %1231, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:284:0
  %1246 : Tensor[] = prim::ListConstruct(%1245, %1239), scope: __module.layer.7/__module.layer.7.rel_attn
  %ac.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1209, %1246), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1248 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.8, %1230, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:287:0
  %1249 : Tensor[] = prim::ListConstruct(%1248, %1244), scope: __module.layer.7/__module.layer.7.rel_attn
  %x.29 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1209, %1249), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1251 : int = aten::size(%ac.8, %1210), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:288:0
  %1252 : int = aten::size(%x.29, %1211), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1253 : int = aten::size(%x.29, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1254 : int = aten::size(%x.29, %1212), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1255 : int = aten::size(%x.29, %1210), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:257:0
  %1256 : Long() = prim::NumToTensor(%1255), scope: __module.layer.7/__module.layer.7.rel_attn
  %1257 : int[] = prim::ListConstruct(%1252, %1253, %1255, %1254), scope: __module.layer.7/__module.layer.7.rel_attn
  %x.30 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.29, %1257), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:259:0
  %1259 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.30, %1211, %1211, %1213, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %1260 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1259, %1208, %1211, %1213, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %1261 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1260, %1212, %1208, %1213, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.31 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1261, %1210, %1211, %1213, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:260:0
  %1263 : Long() = aten::sub(%1256, %1214, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %1264 : int = aten::Int(%1263), scope: __module.layer.7/__module.layer.7.rel_attn
  %1265 : int[] = prim::ListConstruct(%1252, %1253, %1254, %1264), scope: __module.layer.7/__module.layer.7.rel_attn
  %x.32 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.31, %1265), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:261:0
  %1267 : Long(13:1) = aten::arange(%1251, %1215, %1211, %1204, %1206), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.32, %1210, %1267), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:265:0
  %1269 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.8, %bd.8, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %1270 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1269, %1216, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.8 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1270, %1217), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:298:0
  %1272 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.7/__module.layer.7.rel_attn
  %1273 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1218, %1272), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1274 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1273, %1219), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.66 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.8, %1274, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.67 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.66, %1210, %1207), scope: __module.layer.7/__module.layer.7.rel_attn # torch/nn/functional.py:1498:0
  %1277 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.67, %1220, %1206), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %1278 : Tensor[] = prim::ListConstruct(%1277, %1241), scope: __module.layer.7/__module.layer.7.rel_attn
  %1279 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1221, %1278), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %1280 : Tensor[] = prim::ListConstruct(%1279, %1229), scope: __module.layer.7/__module.layer.7.rel_attn
  %input.68 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1222, %1280), scope: __module.layer.7/__module.layer.7.rel_attn # torch/functional.py:327:0
  %attn_out.8 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.68, %1220, %1206), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.69 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.8, %260, %1208), scope: __module.layer.7/__module.layer.7.rel_attn # transformers/modeling_xlnet.py:329:0
  %1284 : Tensor = prim::GetAttr[name="bias"](%1228)
  %1285 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1286 : int[] = prim::ListConstruct(%1225), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm
  %input_tensor.8 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.69, %1286, %1285, %1284, %1224, %1223), scope: __module.layer.7/__module.layer.7.rel_attn/__module.layer.7.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1288 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.8, %r.9)
  %1289 : Float(13:17408, 17:1024, 1024:1), %1290 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1288)
  %1291 : __torch__.torch.nn.modules.normalization.___torch_mangle_43287.LayerNorm = prim::GetAttr[name="layer_norm"](%1226)
  %1292 : __torch__.torch.nn.modules.linear.___torch_mangle_43289.Linear = prim::GetAttr[name="layer_2"](%1226)
  %1293 : __torch__.torch.nn.modules.linear.___torch_mangle_43288.Linear = prim::GetAttr[name="layer_1"](%1226)
  %1294 : Tensor = prim::GetAttr[name="bias"](%1293)
  %1295 : Tensor = prim::GetAttr[name="weight"](%1293)
  %1296 : Float(1024:1, 4096:1024) = aten::t(%1295), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.22 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1289, %1296), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.70 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.22, %1294, %1208), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.71 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.70), scope: __module.layer.7/__module.layer.7.ff # torch/nn/functional.py:1369:0
  %input.72 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.71, %1220, %1206), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %1301 : Tensor = prim::GetAttr[name="bias"](%1292)
  %1302 : Tensor = prim::GetAttr[name="weight"](%1292)
  %1303 : Float(4096:1, 1024:4096) = aten::t(%1302), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.23 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.72, %1303), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.73 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.23, %1301, %1208), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.24 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.73, %1220, %1206), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.24, %1289, %1208), scope: __module.layer.7/__module.layer.7.ff # transformers/modeling_xlnet.py:489:0
  %1308 : Tensor = prim::GetAttr[name="bias"](%1291)
  %1309 : Tensor = prim::GetAttr[name="weight"](%1291)
  %1310 : int[] = prim::ListConstruct(%1225), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm
  %curr_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.74, %1310, %1309, %1308, %1224, %1223), scope: __module.layer.7/__module.layer.7.ff/__module.layer.7.ff.layer_norm # torch/nn/functional.py:2048:0
  %1312 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.9, %1290)
  %269 : Float(13:17408, 17:1024, 1024:1), %270 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1312)
  %271 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %272 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %273 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %274 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.9 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%269, %271, %272, %273, %274) # transformers/modeling_xlnet.py:1009:0
  %276 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.9) # transformers/modeling_xlnet.py:1013:0
  %1313 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1314 : Device = prim::Constant[value="cpu"](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %1315 : int = prim::Constant[value=6](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %1316 : bool = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %1317 : None = prim::Constant(), scope: __module.layer.8/__module.layer.8.rel_attn
  %1318 : int = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %1319 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1320 : int = prim::Constant[value=3](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %1321 : int = prim::Constant[value=0](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1322 : int = prim::Constant[value=2](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1323 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %1324 : Long() = prim::Constant[value={1}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %1325 : int = prim::Constant[value=4](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %1326 : Long() = prim::Constant[value={0}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1327 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1328 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1329 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %1330 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %1331 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1332 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1333 : bool = prim::Constant[value=1](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1334 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1335 : int = prim::Constant[value=1024](), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1336 : __torch__.transformers.modeling_xlnet.___torch_mangle_43301.XLNetFeedForward = prim::GetAttr[name="ff"](%34)
  %1337 : __torch__.transformers.modeling_xlnet.___torch_mangle_43296.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%34)
  %1338 : __torch__.torch.nn.modules.normalization.___torch_mangle_43294.LayerNorm = prim::GetAttr[name="layer_norm"](%1337)
  %1339 : Tensor = prim::GetAttr[name="o"](%1337)
  %1340 : Tensor = prim::GetAttr[name="r_r_bias"](%1337)
  %1341 : Tensor = prim::GetAttr[name="r_w_bias"](%1337)
  %1342 : Tensor = prim::GetAttr[name="r"](%1337)
  %1343 : Tensor = prim::GetAttr[name="v"](%1337)
  %1344 : Tensor = prim::GetAttr[name="k"](%1337)
  %1345 : Tensor = prim::GetAttr[name="q"](%1337)
  %1346 : Tensor[] = prim::ListConstruct(%269, %1345), scope: __module.layer.8/__module.layer.8.rel_attn
  %q_head.9 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1313, %1346), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1348 : Tensor[] = prim::ListConstruct(%269, %1344), scope: __module.layer.8/__module.layer.8.rel_attn
  %1349 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1313, %1348), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1350 : Tensor[] = prim::ListConstruct(%269, %1343), scope: __module.layer.8/__module.layer.8.rel_attn
  %1351 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1313, %1350), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %r.10 : Float(26:1024, 17:0, 1024:1) = aten::to(%270, %1314, %1315, %1316, %1316, %1317), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:443:0
  %1353 : Tensor[] = prim::ListConstruct(%r.10, %1342), scope: __module.layer.8/__module.layer.8.rel_attn
  %1354 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1313, %1353), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1355 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %1341, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:284:0
  %1356 : Tensor[] = prim::ListConstruct(%1355, %1349), scope: __module.layer.8/__module.layer.8.rel_attn
  %ac.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1319, %1356), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1358 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.9, %1340, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:287:0
  %1359 : Tensor[] = prim::ListConstruct(%1358, %1354), scope: __module.layer.8/__module.layer.8.rel_attn
  %x.33 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1319, %1359), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1361 : int = aten::size(%ac.9, %1320), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:288:0
  %1362 : int = aten::size(%x.33, %1321), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1363 : int = aten::size(%x.33, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1364 : int = aten::size(%x.33, %1322), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1365 : int = aten::size(%x.33, %1320), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:257:0
  %1366 : Long() = prim::NumToTensor(%1365), scope: __module.layer.8/__module.layer.8.rel_attn
  %1367 : int[] = prim::ListConstruct(%1362, %1363, %1365, %1364), scope: __module.layer.8/__module.layer.8.rel_attn
  %x.34 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.33, %1367), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:259:0
  %1369 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.34, %1321, %1321, %1323, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %1370 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1369, %1318, %1321, %1323, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %1371 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1370, %1322, %1318, %1323, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.35 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1371, %1320, %1321, %1323, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:260:0
  %1373 : Long() = aten::sub(%1366, %1324, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %1374 : int = aten::Int(%1373), scope: __module.layer.8/__module.layer.8.rel_attn
  %1375 : int[] = prim::ListConstruct(%1362, %1363, %1364, %1374), scope: __module.layer.8/__module.layer.8.rel_attn
  %x.36 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.35, %1375), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:261:0
  %1377 : Long(13:1) = aten::arange(%1361, %1325, %1321, %1314, %1316), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.36, %1320, %1377), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:265:0
  %1379 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.9, %bd.9, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1380 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1379, %1326, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.9 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1380, %1327), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:298:0
  %1382 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.8/__module.layer.8.rel_attn
  %1383 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1328, %1382), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1384 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1383, %1329), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.75 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.9, %1384, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.76 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.75, %1320, %1317), scope: __module.layer.8/__module.layer.8.rel_attn # torch/nn/functional.py:1498:0
  %1387 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.76, %1330, %1316), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %1388 : Tensor[] = prim::ListConstruct(%1387, %1351), scope: __module.layer.8/__module.layer.8.rel_attn
  %1389 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1331, %1388), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %1390 : Tensor[] = prim::ListConstruct(%1389, %1339), scope: __module.layer.8/__module.layer.8.rel_attn
  %input.77 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1332, %1390), scope: __module.layer.8/__module.layer.8.rel_attn # torch/functional.py:327:0
  %attn_out.9 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.77, %1330, %1316), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.78 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.9, %269, %1318), scope: __module.layer.8/__module.layer.8.rel_attn # transformers/modeling_xlnet.py:329:0
  %1394 : Tensor = prim::GetAttr[name="bias"](%1338)
  %1395 : Tensor = prim::GetAttr[name="weight"](%1338)
  %1396 : int[] = prim::ListConstruct(%1335), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm
  %input_tensor.9 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.78, %1396, %1395, %1394, %1334, %1333), scope: __module.layer.8/__module.layer.8.rel_attn/__module.layer.8.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1398 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.9, %r.10)
  %1399 : Float(13:17408, 17:1024, 1024:1), %1400 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1398)
  %1401 : __torch__.torch.nn.modules.normalization.___torch_mangle_43297.LayerNorm = prim::GetAttr[name="layer_norm"](%1336)
  %1402 : __torch__.torch.nn.modules.linear.___torch_mangle_43299.Linear = prim::GetAttr[name="layer_2"](%1336)
  %1403 : __torch__.torch.nn.modules.linear.___torch_mangle_43298.Linear = prim::GetAttr[name="layer_1"](%1336)
  %1404 : Tensor = prim::GetAttr[name="bias"](%1403)
  %1405 : Tensor = prim::GetAttr[name="weight"](%1403)
  %1406 : Float(1024:1, 4096:1024) = aten::t(%1405), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.25 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1399, %1406), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.79 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.25, %1404, %1318), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.80 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.79), scope: __module.layer.8/__module.layer.8.ff # torch/nn/functional.py:1369:0
  %input.81 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.80, %1330, %1316), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %1411 : Tensor = prim::GetAttr[name="bias"](%1402)
  %1412 : Tensor = prim::GetAttr[name="weight"](%1402)
  %1413 : Float(4096:1, 1024:4096) = aten::t(%1412), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.26 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.81, %1413), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.82 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.26, %1411, %1318), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.27 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.82, %1330, %1316), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.dropout # torch/nn/functional.py:973:0
  %input.83 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.27, %1399, %1318), scope: __module.layer.8/__module.layer.8.ff # transformers/modeling_xlnet.py:489:0
  %1418 : Tensor = prim::GetAttr[name="bias"](%1401)
  %1419 : Tensor = prim::GetAttr[name="weight"](%1401)
  %1420 : int[] = prim::ListConstruct(%1335), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm
  %curr_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.83, %1420, %1419, %1418, %1334, %1333), scope: __module.layer.8/__module.layer.8.ff/__module.layer.8.ff.layer_norm # torch/nn/functional.py:2048:0
  %1422 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.10, %1400)
  %278 : Float(13:17408, 17:1024, 1024:1), %279 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1422)
  %280 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %281 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %282 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %283 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.10 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%278, %280, %281, %282, %283) # transformers/modeling_xlnet.py:1009:0
  %285 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.10) # transformers/modeling_xlnet.py:1013:0
  %1423 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1424 : Device = prim::Constant[value="cpu"](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1425 : int = prim::Constant[value=6](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1426 : bool = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1427 : None = prim::Constant(), scope: __module.layer.9/__module.layer.9.rel_attn
  %1428 : int = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1429 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1430 : int = prim::Constant[value=3](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1431 : int = prim::Constant[value=0](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1432 : int = prim::Constant[value=2](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1433 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1434 : Long() = prim::Constant[value={1}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1435 : int = prim::Constant[value=4](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1436 : Long() = prim::Constant[value={0}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1437 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1438 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1439 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %1440 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1441 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1442 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1443 : bool = prim::Constant[value=1](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1444 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1445 : int = prim::Constant[value=1024](), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1446 : __torch__.transformers.modeling_xlnet.___torch_mangle_43311.XLNetFeedForward = prim::GetAttr[name="ff"](%32)
  %1447 : __torch__.transformers.modeling_xlnet.___torch_mangle_43306.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%32)
  %1448 : __torch__.torch.nn.modules.normalization.___torch_mangle_43304.LayerNorm = prim::GetAttr[name="layer_norm"](%1447)
  %1449 : Tensor = prim::GetAttr[name="o"](%1447)
  %1450 : Tensor = prim::GetAttr[name="r_r_bias"](%1447)
  %1451 : Tensor = prim::GetAttr[name="r_w_bias"](%1447)
  %1452 : Tensor = prim::GetAttr[name="r"](%1447)
  %1453 : Tensor = prim::GetAttr[name="v"](%1447)
  %1454 : Tensor = prim::GetAttr[name="k"](%1447)
  %1455 : Tensor = prim::GetAttr[name="q"](%1447)
  %1456 : Tensor[] = prim::ListConstruct(%278, %1455), scope: __module.layer.9/__module.layer.9.rel_attn
  %q_head.10 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1423, %1456), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1458 : Tensor[] = prim::ListConstruct(%278, %1454), scope: __module.layer.9/__module.layer.9.rel_attn
  %1459 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1423, %1458), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1460 : Tensor[] = prim::ListConstruct(%278, %1453), scope: __module.layer.9/__module.layer.9.rel_attn
  %1461 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1423, %1460), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %r.11 : Float(26:1024, 17:0, 1024:1) = aten::to(%279, %1424, %1425, %1426, %1426, %1427), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:443:0
  %1463 : Tensor[] = prim::ListConstruct(%r.11, %1452), scope: __module.layer.9/__module.layer.9.rel_attn
  %1464 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1423, %1463), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1465 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %1451, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:284:0
  %1466 : Tensor[] = prim::ListConstruct(%1465, %1459), scope: __module.layer.9/__module.layer.9.rel_attn
  %ac.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1429, %1466), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1468 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.10, %1450, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:287:0
  %1469 : Tensor[] = prim::ListConstruct(%1468, %1464), scope: __module.layer.9/__module.layer.9.rel_attn
  %x.37 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1429, %1469), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1471 : int = aten::size(%ac.10, %1430), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:288:0
  %1472 : int = aten::size(%x.37, %1431), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1473 : int = aten::size(%x.37, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1474 : int = aten::size(%x.37, %1432), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1475 : int = aten::size(%x.37, %1430), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:257:0
  %1476 : Long() = prim::NumToTensor(%1475), scope: __module.layer.9/__module.layer.9.rel_attn
  %1477 : int[] = prim::ListConstruct(%1472, %1473, %1475, %1474), scope: __module.layer.9/__module.layer.9.rel_attn
  %x.38 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.37, %1477), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:259:0
  %1479 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.38, %1431, %1431, %1433, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1480 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1479, %1428, %1431, %1433, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1481 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1480, %1432, %1428, %1433, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.39 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1481, %1430, %1431, %1433, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:260:0
  %1483 : Long() = aten::sub(%1476, %1434, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1484 : int = aten::Int(%1483), scope: __module.layer.9/__module.layer.9.rel_attn
  %1485 : int[] = prim::ListConstruct(%1472, %1473, %1474, %1484), scope: __module.layer.9/__module.layer.9.rel_attn
  %x.40 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.39, %1485), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:261:0
  %1487 : Long(13:1) = aten::arange(%1471, %1435, %1431, %1424, %1426), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.40, %1430, %1487), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:265:0
  %1489 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.10, %bd.10, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1490 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1489, %1436, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.10 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1490, %1437), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:298:0
  %1492 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.9/__module.layer.9.rel_attn
  %1493 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1438, %1492), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1494 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1493, %1439), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.84 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.10, %1494, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.85 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.84, %1430, %1427), scope: __module.layer.9/__module.layer.9.rel_attn # torch/nn/functional.py:1498:0
  %1497 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.85, %1440, %1426), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %1498 : Tensor[] = prim::ListConstruct(%1497, %1461), scope: __module.layer.9/__module.layer.9.rel_attn
  %1499 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1441, %1498), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %1500 : Tensor[] = prim::ListConstruct(%1499, %1449), scope: __module.layer.9/__module.layer.9.rel_attn
  %input.86 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1442, %1500), scope: __module.layer.9/__module.layer.9.rel_attn # torch/functional.py:327:0
  %attn_out.10 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.86, %1440, %1426), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.87 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.10, %278, %1428), scope: __module.layer.9/__module.layer.9.rel_attn # transformers/modeling_xlnet.py:329:0
  %1504 : Tensor = prim::GetAttr[name="bias"](%1448)
  %1505 : Tensor = prim::GetAttr[name="weight"](%1448)
  %1506 : int[] = prim::ListConstruct(%1445), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm
  %input_tensor.10 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.87, %1506, %1505, %1504, %1444, %1443), scope: __module.layer.9/__module.layer.9.rel_attn/__module.layer.9.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1508 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.10, %r.11)
  %1509 : Float(13:17408, 17:1024, 1024:1), %1510 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1508)
  %1511 : __torch__.torch.nn.modules.normalization.___torch_mangle_43307.LayerNorm = prim::GetAttr[name="layer_norm"](%1446)
  %1512 : __torch__.torch.nn.modules.linear.___torch_mangle_43309.Linear = prim::GetAttr[name="layer_2"](%1446)
  %1513 : __torch__.torch.nn.modules.linear.___torch_mangle_43308.Linear = prim::GetAttr[name="layer_1"](%1446)
  %1514 : Tensor = prim::GetAttr[name="bias"](%1513)
  %1515 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1516 : Float(1024:1, 4096:1024) = aten::t(%1515), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.28 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1509, %1516), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.88 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.28, %1514, %1428), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.89 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.88), scope: __module.layer.9/__module.layer.9.ff # torch/nn/functional.py:1369:0
  %input.90 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.89, %1440, %1426), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %1521 : Tensor = prim::GetAttr[name="bias"](%1512)
  %1522 : Tensor = prim::GetAttr[name="weight"](%1512)
  %1523 : Float(4096:1, 1024:4096) = aten::t(%1522), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.29 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.90, %1523), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.91 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.29, %1521, %1428), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.30 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.91, %1440, %1426), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.dropout # torch/nn/functional.py:973:0
  %input.92 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.30, %1509, %1428), scope: __module.layer.9/__module.layer.9.ff # transformers/modeling_xlnet.py:489:0
  %1528 : Tensor = prim::GetAttr[name="bias"](%1511)
  %1529 : Tensor = prim::GetAttr[name="weight"](%1511)
  %1530 : int[] = prim::ListConstruct(%1445), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm
  %curr_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.92, %1530, %1529, %1528, %1444, %1443), scope: __module.layer.9/__module.layer.9.ff/__module.layer.9.ff.layer_norm # torch/nn/functional.py:2048:0
  %1532 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.11, %1510)
  %287 : Float(13:17408, 17:1024, 1024:1), %288 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1532)
  %289 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %290 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %291 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %292 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.11 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%287, %289, %290, %291, %292) # transformers/modeling_xlnet.py:1009:0
  %294 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.11) # transformers/modeling_xlnet.py:1013:0
  %1533 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1534 : Device = prim::Constant[value="cpu"](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1535 : int = prim::Constant[value=6](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1536 : bool = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1537 : None = prim::Constant(), scope: __module.layer.10/__module.layer.10.rel_attn
  %1538 : int = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1539 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1540 : int = prim::Constant[value=3](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1541 : int = prim::Constant[value=0](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1542 : int = prim::Constant[value=2](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1543 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1544 : Long() = prim::Constant[value={1}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1545 : int = prim::Constant[value=4](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1546 : Long() = prim::Constant[value={0}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1547 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1548 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1549 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %1550 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1551 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1552 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1553 : bool = prim::Constant[value=1](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1554 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1555 : int = prim::Constant[value=1024](), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1556 : __torch__.transformers.modeling_xlnet.___torch_mangle_43321.XLNetFeedForward = prim::GetAttr[name="ff"](%30)
  %1557 : __torch__.transformers.modeling_xlnet.___torch_mangle_43316.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%30)
  %1558 : __torch__.torch.nn.modules.normalization.___torch_mangle_43314.LayerNorm = prim::GetAttr[name="layer_norm"](%1557)
  %1559 : Tensor = prim::GetAttr[name="o"](%1557)
  %1560 : Tensor = prim::GetAttr[name="r_r_bias"](%1557)
  %1561 : Tensor = prim::GetAttr[name="r_w_bias"](%1557)
  %1562 : Tensor = prim::GetAttr[name="r"](%1557)
  %1563 : Tensor = prim::GetAttr[name="v"](%1557)
  %1564 : Tensor = prim::GetAttr[name="k"](%1557)
  %1565 : Tensor = prim::GetAttr[name="q"](%1557)
  %1566 : Tensor[] = prim::ListConstruct(%287, %1565), scope: __module.layer.10/__module.layer.10.rel_attn
  %q_head.11 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1533, %1566), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1568 : Tensor[] = prim::ListConstruct(%287, %1564), scope: __module.layer.10/__module.layer.10.rel_attn
  %1569 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1533, %1568), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1570 : Tensor[] = prim::ListConstruct(%287, %1563), scope: __module.layer.10/__module.layer.10.rel_attn
  %1571 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1533, %1570), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %r.12 : Float(26:1024, 17:0, 1024:1) = aten::to(%288, %1534, %1535, %1536, %1536, %1537), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:443:0
  %1573 : Tensor[] = prim::ListConstruct(%r.12, %1562), scope: __module.layer.10/__module.layer.10.rel_attn
  %1574 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1533, %1573), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1575 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1561, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:284:0
  %1576 : Tensor[] = prim::ListConstruct(%1575, %1569), scope: __module.layer.10/__module.layer.10.rel_attn
  %ac.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1539, %1576), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1578 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.11, %1560, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:287:0
  %1579 : Tensor[] = prim::ListConstruct(%1578, %1574), scope: __module.layer.10/__module.layer.10.rel_attn
  %x.41 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1539, %1579), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1581 : int = aten::size(%ac.11, %1540), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:288:0
  %1582 : int = aten::size(%x.41, %1541), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1583 : int = aten::size(%x.41, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1584 : int = aten::size(%x.41, %1542), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1585 : int = aten::size(%x.41, %1540), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:257:0
  %1586 : Long() = prim::NumToTensor(%1585), scope: __module.layer.10/__module.layer.10.rel_attn
  %1587 : int[] = prim::ListConstruct(%1582, %1583, %1585, %1584), scope: __module.layer.10/__module.layer.10.rel_attn
  %x.42 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.41, %1587), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:259:0
  %1589 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.42, %1541, %1541, %1543, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1590 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1589, %1538, %1541, %1543, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1591 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1590, %1542, %1538, %1543, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.43 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1591, %1540, %1541, %1543, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:260:0
  %1593 : Long() = aten::sub(%1586, %1544, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1594 : int = aten::Int(%1593), scope: __module.layer.10/__module.layer.10.rel_attn
  %1595 : int[] = prim::ListConstruct(%1582, %1583, %1584, %1594), scope: __module.layer.10/__module.layer.10.rel_attn
  %x.44 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.43, %1595), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:261:0
  %1597 : Long(13:1) = aten::arange(%1581, %1545, %1541, %1534, %1536), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.44, %1540, %1597), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:265:0
  %1599 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.11, %bd.11, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1600 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1599, %1546, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.11 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1600, %1547), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:298:0
  %1602 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.10/__module.layer.10.rel_attn
  %1603 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1548, %1602), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1604 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1603, %1549), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.93 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.11, %1604, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.94 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.93, %1540, %1537), scope: __module.layer.10/__module.layer.10.rel_attn # torch/nn/functional.py:1498:0
  %1607 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.94, %1550, %1536), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %1608 : Tensor[] = prim::ListConstruct(%1607, %1571), scope: __module.layer.10/__module.layer.10.rel_attn
  %1609 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1551, %1608), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %1610 : Tensor[] = prim::ListConstruct(%1609, %1559), scope: __module.layer.10/__module.layer.10.rel_attn
  %input.95 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1552, %1610), scope: __module.layer.10/__module.layer.10.rel_attn # torch/functional.py:327:0
  %attn_out.11 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.95, %1550, %1536), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.96 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.11, %287, %1538), scope: __module.layer.10/__module.layer.10.rel_attn # transformers/modeling_xlnet.py:329:0
  %1614 : Tensor = prim::GetAttr[name="bias"](%1558)
  %1615 : Tensor = prim::GetAttr[name="weight"](%1558)
  %1616 : int[] = prim::ListConstruct(%1555), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm
  %input_tensor.11 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.96, %1616, %1615, %1614, %1554, %1553), scope: __module.layer.10/__module.layer.10.rel_attn/__module.layer.10.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1618 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.11, %r.12)
  %1619 : Float(13:17408, 17:1024, 1024:1), %1620 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1618)
  %1621 : __torch__.torch.nn.modules.normalization.___torch_mangle_43317.LayerNorm = prim::GetAttr[name="layer_norm"](%1556)
  %1622 : __torch__.torch.nn.modules.linear.___torch_mangle_43319.Linear = prim::GetAttr[name="layer_2"](%1556)
  %1623 : __torch__.torch.nn.modules.linear.___torch_mangle_43318.Linear = prim::GetAttr[name="layer_1"](%1556)
  %1624 : Tensor = prim::GetAttr[name="bias"](%1623)
  %1625 : Tensor = prim::GetAttr[name="weight"](%1623)
  %1626 : Float(1024:1, 4096:1024) = aten::t(%1625), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.31 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1619, %1626), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.97 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.31, %1624, %1538), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.98 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.97), scope: __module.layer.10/__module.layer.10.ff # torch/nn/functional.py:1369:0
  %input.99 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.98, %1550, %1536), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %1631 : Tensor = prim::GetAttr[name="bias"](%1622)
  %1632 : Tensor = prim::GetAttr[name="weight"](%1622)
  %1633 : Float(4096:1, 1024:4096) = aten::t(%1632), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.32 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.99, %1633), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.100 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.32, %1631, %1538), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.33 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.100, %1550, %1536), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.dropout # torch/nn/functional.py:973:0
  %input.101 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.33, %1619, %1538), scope: __module.layer.10/__module.layer.10.ff # transformers/modeling_xlnet.py:489:0
  %1638 : Tensor = prim::GetAttr[name="bias"](%1621)
  %1639 : Tensor = prim::GetAttr[name="weight"](%1621)
  %1640 : int[] = prim::ListConstruct(%1555), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm
  %curr_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.101, %1640, %1639, %1638, %1554, %1553), scope: __module.layer.10/__module.layer.10.ff/__module.layer.10.ff.layer_norm # torch/nn/functional.py:2048:0
  %1642 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.12, %1620)
  %296 : Float(13:17408, 17:1024, 1024:1), %297 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1642)
  %298 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %299 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %300 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %301 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.12 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%296, %298, %299, %300, %301) # transformers/modeling_xlnet.py:1009:0
  %303 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.12) # transformers/modeling_xlnet.py:1013:0
  %1643 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1644 : Device = prim::Constant[value="cpu"](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1645 : int = prim::Constant[value=6](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1646 : bool = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1647 : None = prim::Constant(), scope: __module.layer.11/__module.layer.11.rel_attn
  %1648 : int = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1649 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1650 : int = prim::Constant[value=3](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1651 : int = prim::Constant[value=0](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1652 : int = prim::Constant[value=2](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1653 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1654 : Long() = prim::Constant[value={1}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1655 : int = prim::Constant[value=4](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1656 : Long() = prim::Constant[value={0}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1657 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1658 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1659 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %1660 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1661 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1662 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1663 : bool = prim::Constant[value=1](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1664 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1665 : int = prim::Constant[value=1024](), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1666 : __torch__.transformers.modeling_xlnet.___torch_mangle_43331.XLNetFeedForward = prim::GetAttr[name="ff"](%28)
  %1667 : __torch__.transformers.modeling_xlnet.___torch_mangle_43326.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%28)
  %1668 : __torch__.torch.nn.modules.normalization.___torch_mangle_43324.LayerNorm = prim::GetAttr[name="layer_norm"](%1667)
  %1669 : Tensor = prim::GetAttr[name="o"](%1667)
  %1670 : Tensor = prim::GetAttr[name="r_r_bias"](%1667)
  %1671 : Tensor = prim::GetAttr[name="r_w_bias"](%1667)
  %1672 : Tensor = prim::GetAttr[name="r"](%1667)
  %1673 : Tensor = prim::GetAttr[name="v"](%1667)
  %1674 : Tensor = prim::GetAttr[name="k"](%1667)
  %1675 : Tensor = prim::GetAttr[name="q"](%1667)
  %1676 : Tensor[] = prim::ListConstruct(%296, %1675), scope: __module.layer.11/__module.layer.11.rel_attn
  %q_head.12 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1643, %1676), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1678 : Tensor[] = prim::ListConstruct(%296, %1674), scope: __module.layer.11/__module.layer.11.rel_attn
  %1679 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1643, %1678), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1680 : Tensor[] = prim::ListConstruct(%296, %1673), scope: __module.layer.11/__module.layer.11.rel_attn
  %1681 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1643, %1680), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %r.13 : Float(26:1024, 17:0, 1024:1) = aten::to(%297, %1644, %1645, %1646, %1646, %1647), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:443:0
  %1683 : Tensor[] = prim::ListConstruct(%r.13, %1672), scope: __module.layer.11/__module.layer.11.rel_attn
  %1684 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1643, %1683), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1685 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1671, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:284:0
  %1686 : Tensor[] = prim::ListConstruct(%1685, %1679), scope: __module.layer.11/__module.layer.11.rel_attn
  %ac.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1649, %1686), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1688 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.12, %1670, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:287:0
  %1689 : Tensor[] = prim::ListConstruct(%1688, %1684), scope: __module.layer.11/__module.layer.11.rel_attn
  %x.45 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1649, %1689), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1691 : int = aten::size(%ac.12, %1650), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:288:0
  %1692 : int = aten::size(%x.45, %1651), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1693 : int = aten::size(%x.45, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1694 : int = aten::size(%x.45, %1652), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1695 : int = aten::size(%x.45, %1650), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:257:0
  %1696 : Long() = prim::NumToTensor(%1695), scope: __module.layer.11/__module.layer.11.rel_attn
  %1697 : int[] = prim::ListConstruct(%1692, %1693, %1695, %1694), scope: __module.layer.11/__module.layer.11.rel_attn
  %x.46 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.45, %1697), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:259:0
  %1699 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.46, %1651, %1651, %1653, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1700 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1699, %1648, %1651, %1653, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1701 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1700, %1652, %1648, %1653, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.47 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1701, %1650, %1651, %1653, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:260:0
  %1703 : Long() = aten::sub(%1696, %1654, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1704 : int = aten::Int(%1703), scope: __module.layer.11/__module.layer.11.rel_attn
  %1705 : int[] = prim::ListConstruct(%1692, %1693, %1694, %1704), scope: __module.layer.11/__module.layer.11.rel_attn
  %x.48 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.47, %1705), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:261:0
  %1707 : Long(13:1) = aten::arange(%1691, %1655, %1651, %1644, %1646), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.48, %1650, %1707), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:265:0
  %1709 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.12, %bd.12, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1710 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1709, %1656, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.12 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1710, %1657), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:298:0
  %1712 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.11/__module.layer.11.rel_attn
  %1713 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1658, %1712), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1714 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1713, %1659), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.102 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.12, %1714, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.103 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.102, %1650, %1647), scope: __module.layer.11/__module.layer.11.rel_attn # torch/nn/functional.py:1498:0
  %1717 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.103, %1660, %1646), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %1718 : Tensor[] = prim::ListConstruct(%1717, %1681), scope: __module.layer.11/__module.layer.11.rel_attn
  %1719 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1661, %1718), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %1720 : Tensor[] = prim::ListConstruct(%1719, %1669), scope: __module.layer.11/__module.layer.11.rel_attn
  %input.104 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1662, %1720), scope: __module.layer.11/__module.layer.11.rel_attn # torch/functional.py:327:0
  %attn_out.12 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.104, %1660, %1646), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.105 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.12, %296, %1648), scope: __module.layer.11/__module.layer.11.rel_attn # transformers/modeling_xlnet.py:329:0
  %1724 : Tensor = prim::GetAttr[name="bias"](%1668)
  %1725 : Tensor = prim::GetAttr[name="weight"](%1668)
  %1726 : int[] = prim::ListConstruct(%1665), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm
  %input_tensor.12 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.105, %1726, %1725, %1724, %1664, %1663), scope: __module.layer.11/__module.layer.11.rel_attn/__module.layer.11.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1728 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.12, %r.13)
  %1729 : Float(13:17408, 17:1024, 1024:1), %1730 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1728)
  %1731 : __torch__.torch.nn.modules.normalization.___torch_mangle_43327.LayerNorm = prim::GetAttr[name="layer_norm"](%1666)
  %1732 : __torch__.torch.nn.modules.linear.___torch_mangle_43329.Linear = prim::GetAttr[name="layer_2"](%1666)
  %1733 : __torch__.torch.nn.modules.linear.___torch_mangle_43328.Linear = prim::GetAttr[name="layer_1"](%1666)
  %1734 : Tensor = prim::GetAttr[name="bias"](%1733)
  %1735 : Tensor = prim::GetAttr[name="weight"](%1733)
  %1736 : Float(1024:1, 4096:1024) = aten::t(%1735), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.34 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1729, %1736), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.106 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.34, %1734, %1648), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.107 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.106), scope: __module.layer.11/__module.layer.11.ff # torch/nn/functional.py:1369:0
  %input.108 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.107, %1660, %1646), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %1741 : Tensor = prim::GetAttr[name="bias"](%1732)
  %1742 : Tensor = prim::GetAttr[name="weight"](%1732)
  %1743 : Float(4096:1, 1024:4096) = aten::t(%1742), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.35 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.108, %1743), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.109 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.35, %1741, %1648), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.36 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.109, %1660, %1646), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.36, %1729, %1648), scope: __module.layer.11/__module.layer.11.ff # transformers/modeling_xlnet.py:489:0
  %1748 : Tensor = prim::GetAttr[name="bias"](%1731)
  %1749 : Tensor = prim::GetAttr[name="weight"](%1731)
  %1750 : int[] = prim::ListConstruct(%1665), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm
  %curr_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.110, %1750, %1749, %1748, %1664, %1663), scope: __module.layer.11/__module.layer.11.ff/__module.layer.11.ff.layer_norm # torch/nn/functional.py:2048:0
  %1752 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.13, %1730)
  %305 : Float(13:17408, 17:1024, 1024:1), %306 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1752)
  %307 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %308 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %309 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %310 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.13 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%305, %307, %308, %309, %310) # transformers/modeling_xlnet.py:1009:0
  %312 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.13) # transformers/modeling_xlnet.py:1013:0
  %1753 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1754 : Device = prim::Constant[value="cpu"](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1755 : int = prim::Constant[value=6](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1756 : bool = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1757 : None = prim::Constant(), scope: __module.layer.12/__module.layer.12.rel_attn
  %1758 : int = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1759 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1760 : int = prim::Constant[value=3](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1761 : int = prim::Constant[value=0](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1762 : int = prim::Constant[value=2](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1763 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1764 : Long() = prim::Constant[value={1}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1765 : int = prim::Constant[value=4](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1766 : Long() = prim::Constant[value={0}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1767 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1768 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1769 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %1770 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1771 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1772 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1773 : bool = prim::Constant[value=1](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1774 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1775 : int = prim::Constant[value=1024](), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1776 : __torch__.transformers.modeling_xlnet.___torch_mangle_43341.XLNetFeedForward = prim::GetAttr[name="ff"](%26)
  %1777 : __torch__.transformers.modeling_xlnet.___torch_mangle_43336.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%26)
  %1778 : __torch__.torch.nn.modules.normalization.___torch_mangle_43334.LayerNorm = prim::GetAttr[name="layer_norm"](%1777)
  %1779 : Tensor = prim::GetAttr[name="o"](%1777)
  %1780 : Tensor = prim::GetAttr[name="r_r_bias"](%1777)
  %1781 : Tensor = prim::GetAttr[name="r_w_bias"](%1777)
  %1782 : Tensor = prim::GetAttr[name="r"](%1777)
  %1783 : Tensor = prim::GetAttr[name="v"](%1777)
  %1784 : Tensor = prim::GetAttr[name="k"](%1777)
  %1785 : Tensor = prim::GetAttr[name="q"](%1777)
  %1786 : Tensor[] = prim::ListConstruct(%305, %1785), scope: __module.layer.12/__module.layer.12.rel_attn
  %q_head.13 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1753, %1786), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1788 : Tensor[] = prim::ListConstruct(%305, %1784), scope: __module.layer.12/__module.layer.12.rel_attn
  %1789 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1753, %1788), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1790 : Tensor[] = prim::ListConstruct(%305, %1783), scope: __module.layer.12/__module.layer.12.rel_attn
  %1791 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1753, %1790), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %r.14 : Float(26:1024, 17:0, 1024:1) = aten::to(%306, %1754, %1755, %1756, %1756, %1757), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:443:0
  %1793 : Tensor[] = prim::ListConstruct(%r.14, %1782), scope: __module.layer.12/__module.layer.12.rel_attn
  %1794 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1753, %1793), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1795 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1781, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:284:0
  %1796 : Tensor[] = prim::ListConstruct(%1795, %1789), scope: __module.layer.12/__module.layer.12.rel_attn
  %ac.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1759, %1796), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1798 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.13, %1780, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:287:0
  %1799 : Tensor[] = prim::ListConstruct(%1798, %1794), scope: __module.layer.12/__module.layer.12.rel_attn
  %x.49 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1759, %1799), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1801 : int = aten::size(%ac.13, %1760), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:288:0
  %1802 : int = aten::size(%x.49, %1761), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1803 : int = aten::size(%x.49, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1804 : int = aten::size(%x.49, %1762), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1805 : int = aten::size(%x.49, %1760), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:257:0
  %1806 : Long() = prim::NumToTensor(%1805), scope: __module.layer.12/__module.layer.12.rel_attn
  %1807 : int[] = prim::ListConstruct(%1802, %1803, %1805, %1804), scope: __module.layer.12/__module.layer.12.rel_attn
  %x.50 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.49, %1807), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:259:0
  %1809 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.50, %1761, %1761, %1763, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1810 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1809, %1758, %1761, %1763, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1811 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1810, %1762, %1758, %1763, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.51 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1811, %1760, %1761, %1763, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:260:0
  %1813 : Long() = aten::sub(%1806, %1764, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1814 : int = aten::Int(%1813), scope: __module.layer.12/__module.layer.12.rel_attn
  %1815 : int[] = prim::ListConstruct(%1802, %1803, %1804, %1814), scope: __module.layer.12/__module.layer.12.rel_attn
  %x.52 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.51, %1815), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:261:0
  %1817 : Long(13:1) = aten::arange(%1801, %1765, %1761, %1754, %1756), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.52, %1760, %1817), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:265:0
  %1819 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.13, %bd.13, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1820 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1819, %1766, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.13 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1820, %1767), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:298:0
  %1822 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.12/__module.layer.12.rel_attn
  %1823 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1768, %1822), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1824 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1823, %1769), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.111 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.13, %1824, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.112 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.111, %1760, %1757), scope: __module.layer.12/__module.layer.12.rel_attn # torch/nn/functional.py:1498:0
  %1827 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.112, %1770, %1756), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %1828 : Tensor[] = prim::ListConstruct(%1827, %1791), scope: __module.layer.12/__module.layer.12.rel_attn
  %1829 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1771, %1828), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %1830 : Tensor[] = prim::ListConstruct(%1829, %1779), scope: __module.layer.12/__module.layer.12.rel_attn
  %input.113 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1772, %1830), scope: __module.layer.12/__module.layer.12.rel_attn # torch/functional.py:327:0
  %attn_out.13 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.113, %1770, %1756), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.13, %305, %1758), scope: __module.layer.12/__module.layer.12.rel_attn # transformers/modeling_xlnet.py:329:0
  %1834 : Tensor = prim::GetAttr[name="bias"](%1778)
  %1835 : Tensor = prim::GetAttr[name="weight"](%1778)
  %1836 : int[] = prim::ListConstruct(%1775), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm
  %input_tensor.13 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.114, %1836, %1835, %1834, %1774, %1773), scope: __module.layer.12/__module.layer.12.rel_attn/__module.layer.12.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1838 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.13, %r.14)
  %1839 : Float(13:17408, 17:1024, 1024:1), %1840 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1838)
  %1841 : __torch__.torch.nn.modules.normalization.___torch_mangle_43337.LayerNorm = prim::GetAttr[name="layer_norm"](%1776)
  %1842 : __torch__.torch.nn.modules.linear.___torch_mangle_43339.Linear = prim::GetAttr[name="layer_2"](%1776)
  %1843 : __torch__.torch.nn.modules.linear.___torch_mangle_43338.Linear = prim::GetAttr[name="layer_1"](%1776)
  %1844 : Tensor = prim::GetAttr[name="bias"](%1843)
  %1845 : Tensor = prim::GetAttr[name="weight"](%1843)
  %1846 : Float(1024:1, 4096:1024) = aten::t(%1845), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.37 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1839, %1846), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.115 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.37, %1844, %1758), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.116 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.115), scope: __module.layer.12/__module.layer.12.ff # torch/nn/functional.py:1369:0
  %input.117 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.116, %1770, %1756), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %1851 : Tensor = prim::GetAttr[name="bias"](%1842)
  %1852 : Tensor = prim::GetAttr[name="weight"](%1842)
  %1853 : Float(4096:1, 1024:4096) = aten::t(%1852), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.38 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.117, %1853), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.118 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.38, %1851, %1758), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.39 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.118, %1770, %1756), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.dropout # torch/nn/functional.py:973:0
  %input.119 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.39, %1839, %1758), scope: __module.layer.12/__module.layer.12.ff # transformers/modeling_xlnet.py:489:0
  %1858 : Tensor = prim::GetAttr[name="bias"](%1841)
  %1859 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1860 : int[] = prim::ListConstruct(%1775), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm
  %curr_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.119, %1860, %1859, %1858, %1774, %1773), scope: __module.layer.12/__module.layer.12.ff/__module.layer.12.ff.layer_norm # torch/nn/functional.py:2048:0
  %1862 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.14, %1840)
  %314 : Float(13:17408, 17:1024, 1024:1), %315 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1862)
  %316 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %317 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %318 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %319 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.14 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%314, %316, %317, %318, %319) # transformers/modeling_xlnet.py:1009:0
  %321 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.14) # transformers/modeling_xlnet.py:1013:0
  %1863 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1864 : Device = prim::Constant[value="cpu"](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1865 : int = prim::Constant[value=6](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1866 : bool = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1867 : None = prim::Constant(), scope: __module.layer.13/__module.layer.13.rel_attn
  %1868 : int = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1869 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1870 : int = prim::Constant[value=3](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1871 : int = prim::Constant[value=0](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1872 : int = prim::Constant[value=2](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1873 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1874 : Long() = prim::Constant[value={1}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1875 : int = prim::Constant[value=4](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1876 : Long() = prim::Constant[value={0}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1877 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1878 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1879 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %1880 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1881 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1882 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1883 : bool = prim::Constant[value=1](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1884 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1885 : int = prim::Constant[value=1024](), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1886 : __torch__.transformers.modeling_xlnet.___torch_mangle_43351.XLNetFeedForward = prim::GetAttr[name="ff"](%24)
  %1887 : __torch__.transformers.modeling_xlnet.___torch_mangle_43346.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%24)
  %1888 : __torch__.torch.nn.modules.normalization.___torch_mangle_43344.LayerNorm = prim::GetAttr[name="layer_norm"](%1887)
  %1889 : Tensor = prim::GetAttr[name="o"](%1887)
  %1890 : Tensor = prim::GetAttr[name="r_r_bias"](%1887)
  %1891 : Tensor = prim::GetAttr[name="r_w_bias"](%1887)
  %1892 : Tensor = prim::GetAttr[name="r"](%1887)
  %1893 : Tensor = prim::GetAttr[name="v"](%1887)
  %1894 : Tensor = prim::GetAttr[name="k"](%1887)
  %1895 : Tensor = prim::GetAttr[name="q"](%1887)
  %1896 : Tensor[] = prim::ListConstruct(%314, %1895), scope: __module.layer.13/__module.layer.13.rel_attn
  %q_head.14 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1863, %1896), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1898 : Tensor[] = prim::ListConstruct(%314, %1894), scope: __module.layer.13/__module.layer.13.rel_attn
  %1899 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1863, %1898), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1900 : Tensor[] = prim::ListConstruct(%314, %1893), scope: __module.layer.13/__module.layer.13.rel_attn
  %1901 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1863, %1900), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %r.15 : Float(26:1024, 17:0, 1024:1) = aten::to(%315, %1864, %1865, %1866, %1866, %1867), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:443:0
  %1903 : Tensor[] = prim::ListConstruct(%r.15, %1892), scope: __module.layer.13/__module.layer.13.rel_attn
  %1904 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1863, %1903), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1905 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1891, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:284:0
  %1906 : Tensor[] = prim::ListConstruct(%1905, %1899), scope: __module.layer.13/__module.layer.13.rel_attn
  %ac.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1869, %1906), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1908 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.14, %1890, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:287:0
  %1909 : Tensor[] = prim::ListConstruct(%1908, %1904), scope: __module.layer.13/__module.layer.13.rel_attn
  %x.53 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1869, %1909), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1911 : int = aten::size(%ac.14, %1870), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:288:0
  %1912 : int = aten::size(%x.53, %1871), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1913 : int = aten::size(%x.53, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1914 : int = aten::size(%x.53, %1872), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1915 : int = aten::size(%x.53, %1870), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:257:0
  %1916 : Long() = prim::NumToTensor(%1915), scope: __module.layer.13/__module.layer.13.rel_attn
  %1917 : int[] = prim::ListConstruct(%1912, %1913, %1915, %1914), scope: __module.layer.13/__module.layer.13.rel_attn
  %x.54 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.53, %1917), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:259:0
  %1919 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.54, %1871, %1871, %1873, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1920 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%1919, %1868, %1871, %1873, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1921 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1920, %1872, %1868, %1873, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.55 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%1921, %1870, %1871, %1873, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:260:0
  %1923 : Long() = aten::sub(%1916, %1874, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1924 : int = aten::Int(%1923), scope: __module.layer.13/__module.layer.13.rel_attn
  %1925 : int[] = prim::ListConstruct(%1912, %1913, %1914, %1924), scope: __module.layer.13/__module.layer.13.rel_attn
  %x.56 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.55, %1925), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:261:0
  %1927 : Long(13:1) = aten::arange(%1911, %1875, %1871, %1864, %1866), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.56, %1870, %1927), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:265:0
  %1929 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.14, %bd.14, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1930 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%1929, %1876, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.14 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%1930, %1877), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:298:0
  %1932 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.13/__module.layer.13.rel_attn
  %1933 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1878, %1932), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1934 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%1933, %1879), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.120 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.14, %1934, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.121 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.120, %1870, %1867), scope: __module.layer.13/__module.layer.13.rel_attn # torch/nn/functional.py:1498:0
  %1937 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.121, %1880, %1866), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %1938 : Tensor[] = prim::ListConstruct(%1937, %1901), scope: __module.layer.13/__module.layer.13.rel_attn
  %1939 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1881, %1938), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %1940 : Tensor[] = prim::ListConstruct(%1939, %1889), scope: __module.layer.13/__module.layer.13.rel_attn
  %input.122 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1882, %1940), scope: __module.layer.13/__module.layer.13.rel_attn # torch/functional.py:327:0
  %attn_out.14 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.122, %1880, %1866), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.123 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.14, %314, %1868), scope: __module.layer.13/__module.layer.13.rel_attn # transformers/modeling_xlnet.py:329:0
  %1944 : Tensor = prim::GetAttr[name="bias"](%1888)
  %1945 : Tensor = prim::GetAttr[name="weight"](%1888)
  %1946 : int[] = prim::ListConstruct(%1885), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm
  %input_tensor.14 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.123, %1946, %1945, %1944, %1884, %1883), scope: __module.layer.13/__module.layer.13.rel_attn/__module.layer.13.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1948 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.14, %r.15)
  %1949 : Float(13:17408, 17:1024, 1024:1), %1950 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1948)
  %1951 : __torch__.torch.nn.modules.normalization.___torch_mangle_43347.LayerNorm = prim::GetAttr[name="layer_norm"](%1886)
  %1952 : __torch__.torch.nn.modules.linear.___torch_mangle_43349.Linear = prim::GetAttr[name="layer_2"](%1886)
  %1953 : __torch__.torch.nn.modules.linear.___torch_mangle_43348.Linear = prim::GetAttr[name="layer_1"](%1886)
  %1954 : Tensor = prim::GetAttr[name="bias"](%1953)
  %1955 : Tensor = prim::GetAttr[name="weight"](%1953)
  %1956 : Float(1024:1, 4096:1024) = aten::t(%1955), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.40 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%1949, %1956), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.124 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.40, %1954, %1868), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.125 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.124), scope: __module.layer.13/__module.layer.13.ff # torch/nn/functional.py:1369:0
  %input.126 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.125, %1880, %1866), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %1961 : Tensor = prim::GetAttr[name="bias"](%1952)
  %1962 : Tensor = prim::GetAttr[name="weight"](%1952)
  %1963 : Float(4096:1, 1024:4096) = aten::t(%1962), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.41 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.126, %1963), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.127 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.41, %1961, %1868), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.42 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.127, %1880, %1866), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.dropout # torch/nn/functional.py:973:0
  %input.128 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.42, %1949, %1868), scope: __module.layer.13/__module.layer.13.ff # transformers/modeling_xlnet.py:489:0
  %1968 : Tensor = prim::GetAttr[name="bias"](%1951)
  %1969 : Tensor = prim::GetAttr[name="weight"](%1951)
  %1970 : int[] = prim::ListConstruct(%1885), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm
  %curr_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.128, %1970, %1969, %1968, %1884, %1883), scope: __module.layer.13/__module.layer.13.ff/__module.layer.13.ff.layer_norm # torch/nn/functional.py:2048:0
  %1972 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.15, %1950)
  %323 : Float(13:17408, 17:1024, 1024:1), %324 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%1972)
  %325 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %326 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %327 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %328 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.15 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%323, %325, %326, %327, %328) # transformers/modeling_xlnet.py:1009:0
  %330 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.15) # transformers/modeling_xlnet.py:1013:0
  %1973 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %1974 : Device = prim::Constant[value="cpu"](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1975 : int = prim::Constant[value=6](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1976 : bool = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %1977 : None = prim::Constant(), scope: __module.layer.14/__module.layer.14.rel_attn
  %1978 : int = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %1979 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %1980 : int = prim::Constant[value=3](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %1981 : int = prim::Constant[value=0](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1982 : int = prim::Constant[value=2](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %1983 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %1984 : Long() = prim::Constant[value={1}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %1985 : int = prim::Constant[value=4](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %1986 : Long() = prim::Constant[value={0}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1987 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %1988 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %1989 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %1990 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %1991 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %1992 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %1993 : bool = prim::Constant[value=1](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1994 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1995 : int = prim::Constant[value=1024](), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %1996 : __torch__.transformers.modeling_xlnet.___torch_mangle_43361.XLNetFeedForward = prim::GetAttr[name="ff"](%22)
  %1997 : __torch__.transformers.modeling_xlnet.___torch_mangle_43356.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%22)
  %1998 : __torch__.torch.nn.modules.normalization.___torch_mangle_43354.LayerNorm = prim::GetAttr[name="layer_norm"](%1997)
  %1999 : Tensor = prim::GetAttr[name="o"](%1997)
  %2000 : Tensor = prim::GetAttr[name="r_r_bias"](%1997)
  %2001 : Tensor = prim::GetAttr[name="r_w_bias"](%1997)
  %2002 : Tensor = prim::GetAttr[name="r"](%1997)
  %2003 : Tensor = prim::GetAttr[name="v"](%1997)
  %2004 : Tensor = prim::GetAttr[name="k"](%1997)
  %2005 : Tensor = prim::GetAttr[name="q"](%1997)
  %2006 : Tensor[] = prim::ListConstruct(%323, %2005), scope: __module.layer.14/__module.layer.14.rel_attn
  %q_head.15 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1973, %2006), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2008 : Tensor[] = prim::ListConstruct(%323, %2004), scope: __module.layer.14/__module.layer.14.rel_attn
  %2009 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1973, %2008), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2010 : Tensor[] = prim::ListConstruct(%323, %2003), scope: __module.layer.14/__module.layer.14.rel_attn
  %2011 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1973, %2010), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %r.16 : Float(26:1024, 17:0, 1024:1) = aten::to(%324, %1974, %1975, %1976, %1976, %1977), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:443:0
  %2013 : Tensor[] = prim::ListConstruct(%r.16, %2002), scope: __module.layer.14/__module.layer.14.rel_attn
  %2014 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%1973, %2013), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2015 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %2001, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:284:0
  %2016 : Tensor[] = prim::ListConstruct(%2015, %2009), scope: __module.layer.14/__module.layer.14.rel_attn
  %ac.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%1979, %2016), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2018 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.15, %2000, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:287:0
  %2019 : Tensor[] = prim::ListConstruct(%2018, %2014), scope: __module.layer.14/__module.layer.14.rel_attn
  %x.57 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%1979, %2019), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2021 : int = aten::size(%ac.15, %1980), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:288:0
  %2022 : int = aten::size(%x.57, %1981), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %2023 : int = aten::size(%x.57, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %2024 : int = aten::size(%x.57, %1982), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %2025 : int = aten::size(%x.57, %1980), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:257:0
  %2026 : Long() = prim::NumToTensor(%2025), scope: __module.layer.14/__module.layer.14.rel_attn
  %2027 : int[] = prim::ListConstruct(%2022, %2023, %2025, %2024), scope: __module.layer.14/__module.layer.14.rel_attn
  %x.58 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.57, %2027), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:259:0
  %2029 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.58, %1981, %1981, %1983, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %2030 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2029, %1978, %1981, %1983, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %2031 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2030, %1982, %1978, %1983, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.59 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2031, %1980, %1981, %1983, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:260:0
  %2033 : Long() = aten::sub(%2026, %1984, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %2034 : int = aten::Int(%2033), scope: __module.layer.14/__module.layer.14.rel_attn
  %2035 : int[] = prim::ListConstruct(%2022, %2023, %2024, %2034), scope: __module.layer.14/__module.layer.14.rel_attn
  %x.60 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.59, %2035), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:261:0
  %2037 : Long(13:1) = aten::arange(%2021, %1985, %1981, %1974, %1976), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.60, %1980, %2037), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:265:0
  %2039 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.15, %bd.15, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %2040 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2039, %1986, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.15 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2040, %1987), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:298:0
  %2042 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.14/__module.layer.14.rel_attn
  %2043 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%1988, %2042), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2044 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2043, %1989), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.129 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.15, %2044, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.130 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.129, %1980, %1977), scope: __module.layer.14/__module.layer.14.rel_attn # torch/nn/functional.py:1498:0
  %2047 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.130, %1990, %1976), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %2048 : Tensor[] = prim::ListConstruct(%2047, %2011), scope: __module.layer.14/__module.layer.14.rel_attn
  %2049 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%1991, %2048), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %2050 : Tensor[] = prim::ListConstruct(%2049, %1999), scope: __module.layer.14/__module.layer.14.rel_attn
  %input.131 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%1992, %2050), scope: __module.layer.14/__module.layer.14.rel_attn # torch/functional.py:327:0
  %attn_out.15 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.131, %1990, %1976), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.132 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.15, %323, %1978), scope: __module.layer.14/__module.layer.14.rel_attn # transformers/modeling_xlnet.py:329:0
  %2054 : Tensor = prim::GetAttr[name="bias"](%1998)
  %2055 : Tensor = prim::GetAttr[name="weight"](%1998)
  %2056 : int[] = prim::ListConstruct(%1995), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm
  %input_tensor.15 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.132, %2056, %2055, %2054, %1994, %1993), scope: __module.layer.14/__module.layer.14.rel_attn/__module.layer.14.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2058 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.15, %r.16)
  %2059 : Float(13:17408, 17:1024, 1024:1), %2060 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2058)
  %2061 : __torch__.torch.nn.modules.normalization.___torch_mangle_43357.LayerNorm = prim::GetAttr[name="layer_norm"](%1996)
  %2062 : __torch__.torch.nn.modules.linear.___torch_mangle_43359.Linear = prim::GetAttr[name="layer_2"](%1996)
  %2063 : __torch__.torch.nn.modules.linear.___torch_mangle_43358.Linear = prim::GetAttr[name="layer_1"](%1996)
  %2064 : Tensor = prim::GetAttr[name="bias"](%2063)
  %2065 : Tensor = prim::GetAttr[name="weight"](%2063)
  %2066 : Float(1024:1, 4096:1024) = aten::t(%2065), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.43 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2059, %2066), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.133 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.43, %2064, %1978), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.134 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.133), scope: __module.layer.14/__module.layer.14.ff # torch/nn/functional.py:1369:0
  %input.135 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.134, %1990, %1976), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %2071 : Tensor = prim::GetAttr[name="bias"](%2062)
  %2072 : Tensor = prim::GetAttr[name="weight"](%2062)
  %2073 : Float(4096:1, 1024:4096) = aten::t(%2072), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.44 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.135, %2073), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.136 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.44, %2071, %1978), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.45 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.136, %1990, %1976), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.dropout # torch/nn/functional.py:973:0
  %input.137 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.45, %2059, %1978), scope: __module.layer.14/__module.layer.14.ff # transformers/modeling_xlnet.py:489:0
  %2078 : Tensor = prim::GetAttr[name="bias"](%2061)
  %2079 : Tensor = prim::GetAttr[name="weight"](%2061)
  %2080 : int[] = prim::ListConstruct(%1995), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm
  %curr_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.137, %2080, %2079, %2078, %1994, %1993), scope: __module.layer.14/__module.layer.14.ff/__module.layer.14.ff.layer_norm # torch/nn/functional.py:2048:0
  %2082 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.16, %2060)
  %332 : Float(13:17408, 17:1024, 1024:1), %333 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2082)
  %334 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %335 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %336 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %337 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.16 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%332, %334, %335, %336, %337) # transformers/modeling_xlnet.py:1009:0
  %339 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.16) # transformers/modeling_xlnet.py:1013:0
  %2083 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2084 : Device = prim::Constant[value="cpu"](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %2085 : int = prim::Constant[value=6](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %2086 : bool = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %2087 : None = prim::Constant(), scope: __module.layer.15/__module.layer.15.rel_attn
  %2088 : int = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %2089 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2090 : int = prim::Constant[value=3](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %2091 : int = prim::Constant[value=0](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2092 : int = prim::Constant[value=2](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2093 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %2094 : Long() = prim::Constant[value={1}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %2095 : int = prim::Constant[value=4](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %2096 : Long() = prim::Constant[value={0}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %2097 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %2098 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2099 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %2100 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %2101 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2102 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2103 : bool = prim::Constant[value=1](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2104 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2105 : int = prim::Constant[value=1024](), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2106 : __torch__.transformers.modeling_xlnet.___torch_mangle_43371.XLNetFeedForward = prim::GetAttr[name="ff"](%20)
  %2107 : __torch__.transformers.modeling_xlnet.___torch_mangle_43366.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%20)
  %2108 : __torch__.torch.nn.modules.normalization.___torch_mangle_43364.LayerNorm = prim::GetAttr[name="layer_norm"](%2107)
  %2109 : Tensor = prim::GetAttr[name="o"](%2107)
  %2110 : Tensor = prim::GetAttr[name="r_r_bias"](%2107)
  %2111 : Tensor = prim::GetAttr[name="r_w_bias"](%2107)
  %2112 : Tensor = prim::GetAttr[name="r"](%2107)
  %2113 : Tensor = prim::GetAttr[name="v"](%2107)
  %2114 : Tensor = prim::GetAttr[name="k"](%2107)
  %2115 : Tensor = prim::GetAttr[name="q"](%2107)
  %2116 : Tensor[] = prim::ListConstruct(%332, %2115), scope: __module.layer.15/__module.layer.15.rel_attn
  %q_head.16 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2083, %2116), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2118 : Tensor[] = prim::ListConstruct(%332, %2114), scope: __module.layer.15/__module.layer.15.rel_attn
  %2119 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2083, %2118), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2120 : Tensor[] = prim::ListConstruct(%332, %2113), scope: __module.layer.15/__module.layer.15.rel_attn
  %2121 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2083, %2120), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %r.17 : Float(26:1024, 17:0, 1024:1) = aten::to(%333, %2084, %2085, %2086, %2086, %2087), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:443:0
  %2123 : Tensor[] = prim::ListConstruct(%r.17, %2112), scope: __module.layer.15/__module.layer.15.rel_attn
  %2124 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2083, %2123), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2125 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %2111, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:284:0
  %2126 : Tensor[] = prim::ListConstruct(%2125, %2119), scope: __module.layer.15/__module.layer.15.rel_attn
  %ac.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2089, %2126), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2128 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.16, %2110, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:287:0
  %2129 : Tensor[] = prim::ListConstruct(%2128, %2124), scope: __module.layer.15/__module.layer.15.rel_attn
  %x.61 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2089, %2129), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2131 : int = aten::size(%ac.16, %2090), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:288:0
  %2132 : int = aten::size(%x.61, %2091), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2133 : int = aten::size(%x.61, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2134 : int = aten::size(%x.61, %2092), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2135 : int = aten::size(%x.61, %2090), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:257:0
  %2136 : Long() = prim::NumToTensor(%2135), scope: __module.layer.15/__module.layer.15.rel_attn
  %2137 : int[] = prim::ListConstruct(%2132, %2133, %2135, %2134), scope: __module.layer.15/__module.layer.15.rel_attn
  %x.62 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.61, %2137), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:259:0
  %2139 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.62, %2091, %2091, %2093, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %2140 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2139, %2088, %2091, %2093, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %2141 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2140, %2092, %2088, %2093, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.63 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2141, %2090, %2091, %2093, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:260:0
  %2143 : Long() = aten::sub(%2136, %2094, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %2144 : int = aten::Int(%2143), scope: __module.layer.15/__module.layer.15.rel_attn
  %2145 : int[] = prim::ListConstruct(%2132, %2133, %2134, %2144), scope: __module.layer.15/__module.layer.15.rel_attn
  %x.64 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.63, %2145), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:261:0
  %2147 : Long(13:1) = aten::arange(%2131, %2095, %2091, %2084, %2086), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.64, %2090, %2147), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:265:0
  %2149 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.16, %bd.16, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %2150 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2149, %2096, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.16 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2150, %2097), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:298:0
  %2152 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.15/__module.layer.15.rel_attn
  %2153 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2098, %2152), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2154 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2153, %2099), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.138 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.16, %2154, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.139 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.138, %2090, %2087), scope: __module.layer.15/__module.layer.15.rel_attn # torch/nn/functional.py:1498:0
  %2157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.139, %2100, %2086), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %2158 : Tensor[] = prim::ListConstruct(%2157, %2121), scope: __module.layer.15/__module.layer.15.rel_attn
  %2159 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2101, %2158), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %2160 : Tensor[] = prim::ListConstruct(%2159, %2109), scope: __module.layer.15/__module.layer.15.rel_attn
  %input.140 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2102, %2160), scope: __module.layer.15/__module.layer.15.rel_attn # torch/functional.py:327:0
  %attn_out.16 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.140, %2100, %2086), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.141 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.16, %332, %2088), scope: __module.layer.15/__module.layer.15.rel_attn # transformers/modeling_xlnet.py:329:0
  %2164 : Tensor = prim::GetAttr[name="bias"](%2108)
  %2165 : Tensor = prim::GetAttr[name="weight"](%2108)
  %2166 : int[] = prim::ListConstruct(%2105), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm
  %input_tensor.16 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.141, %2166, %2165, %2164, %2104, %2103), scope: __module.layer.15/__module.layer.15.rel_attn/__module.layer.15.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2168 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.16, %r.17)
  %2169 : Float(13:17408, 17:1024, 1024:1), %2170 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2168)
  %2171 : __torch__.torch.nn.modules.normalization.___torch_mangle_43367.LayerNorm = prim::GetAttr[name="layer_norm"](%2106)
  %2172 : __torch__.torch.nn.modules.linear.___torch_mangle_43369.Linear = prim::GetAttr[name="layer_2"](%2106)
  %2173 : __torch__.torch.nn.modules.linear.___torch_mangle_43368.Linear = prim::GetAttr[name="layer_1"](%2106)
  %2174 : Tensor = prim::GetAttr[name="bias"](%2173)
  %2175 : Tensor = prim::GetAttr[name="weight"](%2173)
  %2176 : Float(1024:1, 4096:1024) = aten::t(%2175), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.46 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2169, %2176), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.142 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.46, %2174, %2088), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.143 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.142), scope: __module.layer.15/__module.layer.15.ff # torch/nn/functional.py:1369:0
  %input.144 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.143, %2100, %2086), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %2181 : Tensor = prim::GetAttr[name="bias"](%2172)
  %2182 : Tensor = prim::GetAttr[name="weight"](%2172)
  %2183 : Float(4096:1, 1024:4096) = aten::t(%2182), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.47 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.144, %2183), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.145 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.47, %2181, %2088), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.48 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.145, %2100, %2086), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.dropout # torch/nn/functional.py:973:0
  %input.146 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.48, %2169, %2088), scope: __module.layer.15/__module.layer.15.ff # transformers/modeling_xlnet.py:489:0
  %2188 : Tensor = prim::GetAttr[name="bias"](%2171)
  %2189 : Tensor = prim::GetAttr[name="weight"](%2171)
  %2190 : int[] = prim::ListConstruct(%2105), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm
  %curr_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.146, %2190, %2189, %2188, %2104, %2103), scope: __module.layer.15/__module.layer.15.ff/__module.layer.15.ff.layer_norm # torch/nn/functional.py:2048:0
  %2192 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.17, %2170)
  %341 : Float(13:17408, 17:1024, 1024:1), %342 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2192)
  %343 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %344 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %345 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %346 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.17 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%341, %343, %344, %345, %346) # transformers/modeling_xlnet.py:1009:0
  %348 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.17) # transformers/modeling_xlnet.py:1013:0
  %2193 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2194 : Device = prim::Constant[value="cpu"](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %2195 : int = prim::Constant[value=6](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %2196 : bool = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %2197 : None = prim::Constant(), scope: __module.layer.16/__module.layer.16.rel_attn
  %2198 : int = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %2199 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2200 : int = prim::Constant[value=3](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %2201 : int = prim::Constant[value=0](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2202 : int = prim::Constant[value=2](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2203 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %2204 : Long() = prim::Constant[value={1}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %2205 : int = prim::Constant[value=4](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %2206 : Long() = prim::Constant[value={0}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %2207 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %2208 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2209 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %2210 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %2211 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2212 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2213 : bool = prim::Constant[value=1](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2214 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2215 : int = prim::Constant[value=1024](), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2216 : __torch__.transformers.modeling_xlnet.___torch_mangle_43381.XLNetFeedForward = prim::GetAttr[name="ff"](%18)
  %2217 : __torch__.transformers.modeling_xlnet.___torch_mangle_43376.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%18)
  %2218 : __torch__.torch.nn.modules.normalization.___torch_mangle_43374.LayerNorm = prim::GetAttr[name="layer_norm"](%2217)
  %2219 : Tensor = prim::GetAttr[name="o"](%2217)
  %2220 : Tensor = prim::GetAttr[name="r_r_bias"](%2217)
  %2221 : Tensor = prim::GetAttr[name="r_w_bias"](%2217)
  %2222 : Tensor = prim::GetAttr[name="r"](%2217)
  %2223 : Tensor = prim::GetAttr[name="v"](%2217)
  %2224 : Tensor = prim::GetAttr[name="k"](%2217)
  %2225 : Tensor = prim::GetAttr[name="q"](%2217)
  %2226 : Tensor[] = prim::ListConstruct(%341, %2225), scope: __module.layer.16/__module.layer.16.rel_attn
  %q_head.17 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2193, %2226), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2228 : Tensor[] = prim::ListConstruct(%341, %2224), scope: __module.layer.16/__module.layer.16.rel_attn
  %2229 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2193, %2228), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2230 : Tensor[] = prim::ListConstruct(%341, %2223), scope: __module.layer.16/__module.layer.16.rel_attn
  %2231 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2193, %2230), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %r.18 : Float(26:1024, 17:0, 1024:1) = aten::to(%342, %2194, %2195, %2196, %2196, %2197), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:443:0
  %2233 : Tensor[] = prim::ListConstruct(%r.18, %2222), scope: __module.layer.16/__module.layer.16.rel_attn
  %2234 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2193, %2233), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2235 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %2221, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:284:0
  %2236 : Tensor[] = prim::ListConstruct(%2235, %2229), scope: __module.layer.16/__module.layer.16.rel_attn
  %ac.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2199, %2236), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2238 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.17, %2220, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:287:0
  %2239 : Tensor[] = prim::ListConstruct(%2238, %2234), scope: __module.layer.16/__module.layer.16.rel_attn
  %x.65 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2199, %2239), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2241 : int = aten::size(%ac.17, %2200), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:288:0
  %2242 : int = aten::size(%x.65, %2201), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2243 : int = aten::size(%x.65, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2244 : int = aten::size(%x.65, %2202), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2245 : int = aten::size(%x.65, %2200), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:257:0
  %2246 : Long() = prim::NumToTensor(%2245), scope: __module.layer.16/__module.layer.16.rel_attn
  %2247 : int[] = prim::ListConstruct(%2242, %2243, %2245, %2244), scope: __module.layer.16/__module.layer.16.rel_attn
  %x.66 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.65, %2247), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:259:0
  %2249 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.66, %2201, %2201, %2203, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %2250 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2249, %2198, %2201, %2203, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %2251 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2250, %2202, %2198, %2203, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.67 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2251, %2200, %2201, %2203, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:260:0
  %2253 : Long() = aten::sub(%2246, %2204, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %2254 : int = aten::Int(%2253), scope: __module.layer.16/__module.layer.16.rel_attn
  %2255 : int[] = prim::ListConstruct(%2242, %2243, %2244, %2254), scope: __module.layer.16/__module.layer.16.rel_attn
  %x.68 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.67, %2255), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:261:0
  %2257 : Long(13:1) = aten::arange(%2241, %2205, %2201, %2194, %2196), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.68, %2200, %2257), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:265:0
  %2259 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.17, %bd.17, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %2260 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2259, %2206, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.17 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2260, %2207), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:298:0
  %2262 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.16/__module.layer.16.rel_attn
  %2263 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2208, %2262), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2264 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2263, %2209), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.147 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.17, %2264, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.148 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.147, %2200, %2197), scope: __module.layer.16/__module.layer.16.rel_attn # torch/nn/functional.py:1498:0
  %2267 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.148, %2210, %2196), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %2268 : Tensor[] = prim::ListConstruct(%2267, %2231), scope: __module.layer.16/__module.layer.16.rel_attn
  %2269 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2211, %2268), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %2270 : Tensor[] = prim::ListConstruct(%2269, %2219), scope: __module.layer.16/__module.layer.16.rel_attn
  %input.149 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2212, %2270), scope: __module.layer.16/__module.layer.16.rel_attn # torch/functional.py:327:0
  %attn_out.17 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.149, %2210, %2196), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.150 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.17, %341, %2198), scope: __module.layer.16/__module.layer.16.rel_attn # transformers/modeling_xlnet.py:329:0
  %2274 : Tensor = prim::GetAttr[name="bias"](%2218)
  %2275 : Tensor = prim::GetAttr[name="weight"](%2218)
  %2276 : int[] = prim::ListConstruct(%2215), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm
  %input_tensor.17 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.150, %2276, %2275, %2274, %2214, %2213), scope: __module.layer.16/__module.layer.16.rel_attn/__module.layer.16.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2278 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.17, %r.18)
  %2279 : Float(13:17408, 17:1024, 1024:1), %2280 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2278)
  %2281 : __torch__.torch.nn.modules.normalization.___torch_mangle_43377.LayerNorm = prim::GetAttr[name="layer_norm"](%2216)
  %2282 : __torch__.torch.nn.modules.linear.___torch_mangle_43379.Linear = prim::GetAttr[name="layer_2"](%2216)
  %2283 : __torch__.torch.nn.modules.linear.___torch_mangle_43378.Linear = prim::GetAttr[name="layer_1"](%2216)
  %2284 : Tensor = prim::GetAttr[name="bias"](%2283)
  %2285 : Tensor = prim::GetAttr[name="weight"](%2283)
  %2286 : Float(1024:1, 4096:1024) = aten::t(%2285), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.49 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2279, %2286), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.151 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.49, %2284, %2198), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.152 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.151), scope: __module.layer.16/__module.layer.16.ff # torch/nn/functional.py:1369:0
  %input.153 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.152, %2210, %2196), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %2291 : Tensor = prim::GetAttr[name="bias"](%2282)
  %2292 : Tensor = prim::GetAttr[name="weight"](%2282)
  %2293 : Float(4096:1, 1024:4096) = aten::t(%2292), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.50 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.153, %2293), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.154 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.50, %2291, %2198), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.51 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.154, %2210, %2196), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.dropout # torch/nn/functional.py:973:0
  %input.155 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.51, %2279, %2198), scope: __module.layer.16/__module.layer.16.ff # transformers/modeling_xlnet.py:489:0
  %2298 : Tensor = prim::GetAttr[name="bias"](%2281)
  %2299 : Tensor = prim::GetAttr[name="weight"](%2281)
  %2300 : int[] = prim::ListConstruct(%2215), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm
  %curr_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.155, %2300, %2299, %2298, %2214, %2213), scope: __module.layer.16/__module.layer.16.ff/__module.layer.16.ff.layer_norm # torch/nn/functional.py:2048:0
  %2302 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.18, %2280)
  %350 : Float(13:17408, 17:1024, 1024:1), %351 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2302)
  %352 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %353 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %354 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %355 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.18 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%350, %352, %353, %354, %355) # transformers/modeling_xlnet.py:1009:0
  %357 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.18) # transformers/modeling_xlnet.py:1013:0
  %2303 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2304 : Device = prim::Constant[value="cpu"](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %2305 : int = prim::Constant[value=6](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %2306 : bool = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %2307 : None = prim::Constant(), scope: __module.layer.17/__module.layer.17.rel_attn
  %2308 : int = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %2309 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2310 : int = prim::Constant[value=3](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %2311 : int = prim::Constant[value=0](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2312 : int = prim::Constant[value=2](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2313 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %2314 : Long() = prim::Constant[value={1}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %2315 : int = prim::Constant[value=4](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %2316 : Long() = prim::Constant[value={0}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %2317 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %2318 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2319 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %2320 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %2321 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2322 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2323 : bool = prim::Constant[value=1](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2324 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2325 : int = prim::Constant[value=1024](), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2326 : __torch__.transformers.modeling_xlnet.___torch_mangle_43391.XLNetFeedForward = prim::GetAttr[name="ff"](%16)
  %2327 : __torch__.transformers.modeling_xlnet.___torch_mangle_43386.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%16)
  %2328 : __torch__.torch.nn.modules.normalization.___torch_mangle_43384.LayerNorm = prim::GetAttr[name="layer_norm"](%2327)
  %2329 : Tensor = prim::GetAttr[name="o"](%2327)
  %2330 : Tensor = prim::GetAttr[name="r_r_bias"](%2327)
  %2331 : Tensor = prim::GetAttr[name="r_w_bias"](%2327)
  %2332 : Tensor = prim::GetAttr[name="r"](%2327)
  %2333 : Tensor = prim::GetAttr[name="v"](%2327)
  %2334 : Tensor = prim::GetAttr[name="k"](%2327)
  %2335 : Tensor = prim::GetAttr[name="q"](%2327)
  %2336 : Tensor[] = prim::ListConstruct(%350, %2335), scope: __module.layer.17/__module.layer.17.rel_attn
  %q_head.18 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2303, %2336), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2338 : Tensor[] = prim::ListConstruct(%350, %2334), scope: __module.layer.17/__module.layer.17.rel_attn
  %2339 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2303, %2338), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2340 : Tensor[] = prim::ListConstruct(%350, %2333), scope: __module.layer.17/__module.layer.17.rel_attn
  %2341 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2303, %2340), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %r.19 : Float(26:1024, 17:0, 1024:1) = aten::to(%351, %2304, %2305, %2306, %2306, %2307), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:443:0
  %2343 : Tensor[] = prim::ListConstruct(%r.19, %2332), scope: __module.layer.17/__module.layer.17.rel_attn
  %2344 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2303, %2343), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2345 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %2331, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:284:0
  %2346 : Tensor[] = prim::ListConstruct(%2345, %2339), scope: __module.layer.17/__module.layer.17.rel_attn
  %ac.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2309, %2346), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2348 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.18, %2330, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:287:0
  %2349 : Tensor[] = prim::ListConstruct(%2348, %2344), scope: __module.layer.17/__module.layer.17.rel_attn
  %x.69 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2309, %2349), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2351 : int = aten::size(%ac.18, %2310), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:288:0
  %2352 : int = aten::size(%x.69, %2311), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2353 : int = aten::size(%x.69, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2354 : int = aten::size(%x.69, %2312), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2355 : int = aten::size(%x.69, %2310), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:257:0
  %2356 : Long() = prim::NumToTensor(%2355), scope: __module.layer.17/__module.layer.17.rel_attn
  %2357 : int[] = prim::ListConstruct(%2352, %2353, %2355, %2354), scope: __module.layer.17/__module.layer.17.rel_attn
  %x.70 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.69, %2357), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:259:0
  %2359 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.70, %2311, %2311, %2313, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %2360 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2359, %2308, %2311, %2313, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %2361 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2360, %2312, %2308, %2313, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.71 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2361, %2310, %2311, %2313, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:260:0
  %2363 : Long() = aten::sub(%2356, %2314, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %2364 : int = aten::Int(%2363), scope: __module.layer.17/__module.layer.17.rel_attn
  %2365 : int[] = prim::ListConstruct(%2352, %2353, %2354, %2364), scope: __module.layer.17/__module.layer.17.rel_attn
  %x.72 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.71, %2365), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:261:0
  %2367 : Long(13:1) = aten::arange(%2351, %2315, %2311, %2304, %2306), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.72, %2310, %2367), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:265:0
  %2369 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.18, %bd.18, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %2370 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2369, %2316, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.18 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2370, %2317), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:298:0
  %2372 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.17/__module.layer.17.rel_attn
  %2373 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2318, %2372), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2374 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2373, %2319), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.156 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.18, %2374, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.157 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.156, %2310, %2307), scope: __module.layer.17/__module.layer.17.rel_attn # torch/nn/functional.py:1498:0
  %2377 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.157, %2320, %2306), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %2378 : Tensor[] = prim::ListConstruct(%2377, %2341), scope: __module.layer.17/__module.layer.17.rel_attn
  %2379 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2321, %2378), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %2380 : Tensor[] = prim::ListConstruct(%2379, %2329), scope: __module.layer.17/__module.layer.17.rel_attn
  %input.158 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2322, %2380), scope: __module.layer.17/__module.layer.17.rel_attn # torch/functional.py:327:0
  %attn_out.18 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.158, %2320, %2306), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.159 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.18, %350, %2308), scope: __module.layer.17/__module.layer.17.rel_attn # transformers/modeling_xlnet.py:329:0
  %2384 : Tensor = prim::GetAttr[name="bias"](%2328)
  %2385 : Tensor = prim::GetAttr[name="weight"](%2328)
  %2386 : int[] = prim::ListConstruct(%2325), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm
  %input_tensor.18 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.159, %2386, %2385, %2384, %2324, %2323), scope: __module.layer.17/__module.layer.17.rel_attn/__module.layer.17.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2388 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.18, %r.19)
  %2389 : Float(13:17408, 17:1024, 1024:1), %2390 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2388)
  %2391 : __torch__.torch.nn.modules.normalization.___torch_mangle_43387.LayerNorm = prim::GetAttr[name="layer_norm"](%2326)
  %2392 : __torch__.torch.nn.modules.linear.___torch_mangle_43389.Linear = prim::GetAttr[name="layer_2"](%2326)
  %2393 : __torch__.torch.nn.modules.linear.___torch_mangle_43388.Linear = prim::GetAttr[name="layer_1"](%2326)
  %2394 : Tensor = prim::GetAttr[name="bias"](%2393)
  %2395 : Tensor = prim::GetAttr[name="weight"](%2393)
  %2396 : Float(1024:1, 4096:1024) = aten::t(%2395), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.52 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2389, %2396), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.160 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.52, %2394, %2308), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.161 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.160), scope: __module.layer.17/__module.layer.17.ff # torch/nn/functional.py:1369:0
  %input.162 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.161, %2320, %2306), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %2401 : Tensor = prim::GetAttr[name="bias"](%2392)
  %2402 : Tensor = prim::GetAttr[name="weight"](%2392)
  %2403 : Float(4096:1, 1024:4096) = aten::t(%2402), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.53 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.162, %2403), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.163 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.53, %2401, %2308), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.54 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.163, %2320, %2306), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.dropout # torch/nn/functional.py:973:0
  %input.164 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.54, %2389, %2308), scope: __module.layer.17/__module.layer.17.ff # transformers/modeling_xlnet.py:489:0
  %2408 : Tensor = prim::GetAttr[name="bias"](%2391)
  %2409 : Tensor = prim::GetAttr[name="weight"](%2391)
  %2410 : int[] = prim::ListConstruct(%2325), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm
  %curr_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.164, %2410, %2409, %2408, %2324, %2323), scope: __module.layer.17/__module.layer.17.ff/__module.layer.17.ff.layer_norm # torch/nn/functional.py:2048:0
  %2412 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.19, %2390)
  %359 : Float(13:17408, 17:1024, 1024:1), %360 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2412)
  %361 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %362 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %363 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %364 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.19 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%359, %361, %362, %363, %364) # transformers/modeling_xlnet.py:1009:0
  %366 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.19) # transformers/modeling_xlnet.py:1013:0
  %2413 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2414 : Device = prim::Constant[value="cpu"](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %2415 : int = prim::Constant[value=6](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %2416 : bool = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %2417 : None = prim::Constant(), scope: __module.layer.18/__module.layer.18.rel_attn
  %2418 : int = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %2419 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2420 : int = prim::Constant[value=3](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %2421 : int = prim::Constant[value=0](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2422 : int = prim::Constant[value=2](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2423 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %2424 : Long() = prim::Constant[value={1}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %2425 : int = prim::Constant[value=4](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %2426 : Long() = prim::Constant[value={0}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %2427 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %2428 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2429 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %2430 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %2431 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2432 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2433 : bool = prim::Constant[value=1](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2434 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2435 : int = prim::Constant[value=1024](), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2436 : __torch__.transformers.modeling_xlnet.___torch_mangle_43401.XLNetFeedForward = prim::GetAttr[name="ff"](%14)
  %2437 : __torch__.transformers.modeling_xlnet.___torch_mangle_43396.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%14)
  %2438 : __torch__.torch.nn.modules.normalization.___torch_mangle_43394.LayerNorm = prim::GetAttr[name="layer_norm"](%2437)
  %2439 : Tensor = prim::GetAttr[name="o"](%2437)
  %2440 : Tensor = prim::GetAttr[name="r_r_bias"](%2437)
  %2441 : Tensor = prim::GetAttr[name="r_w_bias"](%2437)
  %2442 : Tensor = prim::GetAttr[name="r"](%2437)
  %2443 : Tensor = prim::GetAttr[name="v"](%2437)
  %2444 : Tensor = prim::GetAttr[name="k"](%2437)
  %2445 : Tensor = prim::GetAttr[name="q"](%2437)
  %2446 : Tensor[] = prim::ListConstruct(%359, %2445), scope: __module.layer.18/__module.layer.18.rel_attn
  %q_head.19 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2413, %2446), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2448 : Tensor[] = prim::ListConstruct(%359, %2444), scope: __module.layer.18/__module.layer.18.rel_attn
  %2449 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2413, %2448), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2450 : Tensor[] = prim::ListConstruct(%359, %2443), scope: __module.layer.18/__module.layer.18.rel_attn
  %2451 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2413, %2450), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %r.20 : Float(26:1024, 17:0, 1024:1) = aten::to(%360, %2414, %2415, %2416, %2416, %2417), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:443:0
  %2453 : Tensor[] = prim::ListConstruct(%r.20, %2442), scope: __module.layer.18/__module.layer.18.rel_attn
  %2454 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2413, %2453), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2455 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %2441, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:284:0
  %2456 : Tensor[] = prim::ListConstruct(%2455, %2449), scope: __module.layer.18/__module.layer.18.rel_attn
  %ac.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2419, %2456), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2458 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.19, %2440, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:287:0
  %2459 : Tensor[] = prim::ListConstruct(%2458, %2454), scope: __module.layer.18/__module.layer.18.rel_attn
  %x.73 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2419, %2459), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2461 : int = aten::size(%ac.19, %2420), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:288:0
  %2462 : int = aten::size(%x.73, %2421), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2463 : int = aten::size(%x.73, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2464 : int = aten::size(%x.73, %2422), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2465 : int = aten::size(%x.73, %2420), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:257:0
  %2466 : Long() = prim::NumToTensor(%2465), scope: __module.layer.18/__module.layer.18.rel_attn
  %2467 : int[] = prim::ListConstruct(%2462, %2463, %2465, %2464), scope: __module.layer.18/__module.layer.18.rel_attn
  %x.74 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.73, %2467), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:259:0
  %2469 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.74, %2421, %2421, %2423, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %2470 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2469, %2418, %2421, %2423, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %2471 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2470, %2422, %2418, %2423, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.75 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2471, %2420, %2421, %2423, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:260:0
  %2473 : Long() = aten::sub(%2466, %2424, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %2474 : int = aten::Int(%2473), scope: __module.layer.18/__module.layer.18.rel_attn
  %2475 : int[] = prim::ListConstruct(%2462, %2463, %2464, %2474), scope: __module.layer.18/__module.layer.18.rel_attn
  %x.76 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.75, %2475), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:261:0
  %2477 : Long(13:1) = aten::arange(%2461, %2425, %2421, %2414, %2416), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.76, %2420, %2477), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:265:0
  %2479 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.19, %bd.19, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %2480 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2479, %2426, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.19 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2480, %2427), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:298:0
  %2482 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.18/__module.layer.18.rel_attn
  %2483 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2428, %2482), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2484 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2483, %2429), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.165 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.19, %2484, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.166 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.165, %2420, %2417), scope: __module.layer.18/__module.layer.18.rel_attn # torch/nn/functional.py:1498:0
  %2487 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.166, %2430, %2416), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %2488 : Tensor[] = prim::ListConstruct(%2487, %2451), scope: __module.layer.18/__module.layer.18.rel_attn
  %2489 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2431, %2488), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %2490 : Tensor[] = prim::ListConstruct(%2489, %2439), scope: __module.layer.18/__module.layer.18.rel_attn
  %input.167 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2432, %2490), scope: __module.layer.18/__module.layer.18.rel_attn # torch/functional.py:327:0
  %attn_out.19 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.167, %2430, %2416), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.168 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.19, %359, %2418), scope: __module.layer.18/__module.layer.18.rel_attn # transformers/modeling_xlnet.py:329:0
  %2494 : Tensor = prim::GetAttr[name="bias"](%2438)
  %2495 : Tensor = prim::GetAttr[name="weight"](%2438)
  %2496 : int[] = prim::ListConstruct(%2435), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm
  %input_tensor.19 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.168, %2496, %2495, %2494, %2434, %2433), scope: __module.layer.18/__module.layer.18.rel_attn/__module.layer.18.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2498 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.19, %r.20)
  %2499 : Float(13:17408, 17:1024, 1024:1), %2500 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2498)
  %2501 : __torch__.torch.nn.modules.normalization.___torch_mangle_43397.LayerNorm = prim::GetAttr[name="layer_norm"](%2436)
  %2502 : __torch__.torch.nn.modules.linear.___torch_mangle_43399.Linear = prim::GetAttr[name="layer_2"](%2436)
  %2503 : __torch__.torch.nn.modules.linear.___torch_mangle_43398.Linear = prim::GetAttr[name="layer_1"](%2436)
  %2504 : Tensor = prim::GetAttr[name="bias"](%2503)
  %2505 : Tensor = prim::GetAttr[name="weight"](%2503)
  %2506 : Float(1024:1, 4096:1024) = aten::t(%2505), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.55 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2499, %2506), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.169 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.55, %2504, %2418), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.170 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.169), scope: __module.layer.18/__module.layer.18.ff # torch/nn/functional.py:1369:0
  %input.171 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.170, %2430, %2416), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %2511 : Tensor = prim::GetAttr[name="bias"](%2502)
  %2512 : Tensor = prim::GetAttr[name="weight"](%2502)
  %2513 : Float(4096:1, 1024:4096) = aten::t(%2512), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.56 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.171, %2513), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.172 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.56, %2511, %2418), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.57 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.172, %2430, %2416), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.dropout # torch/nn/functional.py:973:0
  %input.173 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.57, %2499, %2418), scope: __module.layer.18/__module.layer.18.ff # transformers/modeling_xlnet.py:489:0
  %2518 : Tensor = prim::GetAttr[name="bias"](%2501)
  %2519 : Tensor = prim::GetAttr[name="weight"](%2501)
  %2520 : int[] = prim::ListConstruct(%2435), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm
  %curr_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.173, %2520, %2519, %2518, %2434, %2433), scope: __module.layer.18/__module.layer.18.ff/__module.layer.18.ff.layer_norm # torch/nn/functional.py:2048:0
  %2522 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.20, %2500)
  %368 : Float(13:17408, 17:1024, 1024:1), %369 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2522)
  %370 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %371 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %372 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %373 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.20 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%368, %370, %371, %372, %373) # transformers/modeling_xlnet.py:1009:0
  %375 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.20) # transformers/modeling_xlnet.py:1013:0
  %2523 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2524 : Device = prim::Constant[value="cpu"](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %2525 : int = prim::Constant[value=6](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %2526 : bool = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %2527 : None = prim::Constant(), scope: __module.layer.19/__module.layer.19.rel_attn
  %2528 : int = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %2529 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2530 : int = prim::Constant[value=3](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %2531 : int = prim::Constant[value=0](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2532 : int = prim::Constant[value=2](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2533 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2534 : Long() = prim::Constant[value={1}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %2535 : int = prim::Constant[value=4](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %2536 : Long() = prim::Constant[value={0}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2537 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2538 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2539 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %2540 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %2541 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2542 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2543 : bool = prim::Constant[value=1](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2544 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2545 : int = prim::Constant[value=1024](), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2546 : __torch__.transformers.modeling_xlnet.___torch_mangle_43411.XLNetFeedForward = prim::GetAttr[name="ff"](%12)
  %2547 : __torch__.transformers.modeling_xlnet.___torch_mangle_43406.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%12)
  %2548 : __torch__.torch.nn.modules.normalization.___torch_mangle_43404.LayerNorm = prim::GetAttr[name="layer_norm"](%2547)
  %2549 : Tensor = prim::GetAttr[name="o"](%2547)
  %2550 : Tensor = prim::GetAttr[name="r_r_bias"](%2547)
  %2551 : Tensor = prim::GetAttr[name="r_w_bias"](%2547)
  %2552 : Tensor = prim::GetAttr[name="r"](%2547)
  %2553 : Tensor = prim::GetAttr[name="v"](%2547)
  %2554 : Tensor = prim::GetAttr[name="k"](%2547)
  %2555 : Tensor = prim::GetAttr[name="q"](%2547)
  %2556 : Tensor[] = prim::ListConstruct(%368, %2555), scope: __module.layer.19/__module.layer.19.rel_attn
  %q_head.20 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2523, %2556), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2558 : Tensor[] = prim::ListConstruct(%368, %2554), scope: __module.layer.19/__module.layer.19.rel_attn
  %2559 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2523, %2558), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2560 : Tensor[] = prim::ListConstruct(%368, %2553), scope: __module.layer.19/__module.layer.19.rel_attn
  %2561 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2523, %2560), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %r.21 : Float(26:1024, 17:0, 1024:1) = aten::to(%369, %2524, %2525, %2526, %2526, %2527), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:443:0
  %2563 : Tensor[] = prim::ListConstruct(%r.21, %2552), scope: __module.layer.19/__module.layer.19.rel_attn
  %2564 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2523, %2563), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2565 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %2551, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:284:0
  %2566 : Tensor[] = prim::ListConstruct(%2565, %2559), scope: __module.layer.19/__module.layer.19.rel_attn
  %ac.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2529, %2566), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2568 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.20, %2550, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:287:0
  %2569 : Tensor[] = prim::ListConstruct(%2568, %2564), scope: __module.layer.19/__module.layer.19.rel_attn
  %x.77 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2529, %2569), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2571 : int = aten::size(%ac.20, %2530), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:288:0
  %2572 : int = aten::size(%x.77, %2531), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2573 : int = aten::size(%x.77, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2574 : int = aten::size(%x.77, %2532), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2575 : int = aten::size(%x.77, %2530), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:257:0
  %2576 : Long() = prim::NumToTensor(%2575), scope: __module.layer.19/__module.layer.19.rel_attn
  %2577 : int[] = prim::ListConstruct(%2572, %2573, %2575, %2574), scope: __module.layer.19/__module.layer.19.rel_attn
  %x.78 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.77, %2577), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:259:0
  %2579 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.78, %2531, %2531, %2533, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2580 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2579, %2528, %2531, %2533, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2581 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2580, %2532, %2528, %2533, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.79 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2581, %2530, %2531, %2533, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:260:0
  %2583 : Long() = aten::sub(%2576, %2534, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %2584 : int = aten::Int(%2583), scope: __module.layer.19/__module.layer.19.rel_attn
  %2585 : int[] = prim::ListConstruct(%2572, %2573, %2574, %2584), scope: __module.layer.19/__module.layer.19.rel_attn
  %x.80 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.79, %2585), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:261:0
  %2587 : Long(13:1) = aten::arange(%2571, %2535, %2531, %2524, %2526), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.80, %2530, %2587), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:265:0
  %2589 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.20, %bd.20, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2590 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2589, %2536, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.20 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2590, %2537), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:298:0
  %2592 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.19/__module.layer.19.rel_attn
  %2593 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2538, %2592), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2594 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2593, %2539), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.174 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.20, %2594, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.175 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.174, %2530, %2527), scope: __module.layer.19/__module.layer.19.rel_attn # torch/nn/functional.py:1498:0
  %2597 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.175, %2540, %2526), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %2598 : Tensor[] = prim::ListConstruct(%2597, %2561), scope: __module.layer.19/__module.layer.19.rel_attn
  %2599 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2541, %2598), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %2600 : Tensor[] = prim::ListConstruct(%2599, %2549), scope: __module.layer.19/__module.layer.19.rel_attn
  %input.176 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2542, %2600), scope: __module.layer.19/__module.layer.19.rel_attn # torch/functional.py:327:0
  %attn_out.20 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.176, %2540, %2526), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.177 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.20, %368, %2528), scope: __module.layer.19/__module.layer.19.rel_attn # transformers/modeling_xlnet.py:329:0
  %2604 : Tensor = prim::GetAttr[name="bias"](%2548)
  %2605 : Tensor = prim::GetAttr[name="weight"](%2548)
  %2606 : int[] = prim::ListConstruct(%2545), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm
  %input_tensor.20 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.177, %2606, %2605, %2604, %2544, %2543), scope: __module.layer.19/__module.layer.19.rel_attn/__module.layer.19.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2608 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.20, %r.21)
  %2609 : Float(13:17408, 17:1024, 1024:1), %2610 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2608)
  %2611 : __torch__.torch.nn.modules.normalization.___torch_mangle_43407.LayerNorm = prim::GetAttr[name="layer_norm"](%2546)
  %2612 : __torch__.torch.nn.modules.linear.___torch_mangle_43409.Linear = prim::GetAttr[name="layer_2"](%2546)
  %2613 : __torch__.torch.nn.modules.linear.___torch_mangle_43408.Linear = prim::GetAttr[name="layer_1"](%2546)
  %2614 : Tensor = prim::GetAttr[name="bias"](%2613)
  %2615 : Tensor = prim::GetAttr[name="weight"](%2613)
  %2616 : Float(1024:1, 4096:1024) = aten::t(%2615), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.58 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2609, %2616), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.178 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.58, %2614, %2528), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.179 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.178), scope: __module.layer.19/__module.layer.19.ff # torch/nn/functional.py:1369:0
  %input.180 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.179, %2540, %2526), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %2621 : Tensor = prim::GetAttr[name="bias"](%2612)
  %2622 : Tensor = prim::GetAttr[name="weight"](%2612)
  %2623 : Float(4096:1, 1024:4096) = aten::t(%2622), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.59 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.180, %2623), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.181 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.59, %2621, %2528), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.60 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.181, %2540, %2526), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.dropout # torch/nn/functional.py:973:0
  %input.182 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.60, %2609, %2528), scope: __module.layer.19/__module.layer.19.ff # transformers/modeling_xlnet.py:489:0
  %2628 : Tensor = prim::GetAttr[name="bias"](%2611)
  %2629 : Tensor = prim::GetAttr[name="weight"](%2611)
  %2630 : int[] = prim::ListConstruct(%2545), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm
  %curr_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.182, %2630, %2629, %2628, %2544, %2543), scope: __module.layer.19/__module.layer.19.ff/__module.layer.19.ff.layer_norm # torch/nn/functional.py:2048:0
  %2632 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.21, %2610)
  %377 : Float(13:17408, 17:1024, 1024:1), %378 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2632)
  %379 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %380 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %381 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %382 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.21 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%377, %379, %380, %381, %382) # transformers/modeling_xlnet.py:1009:0
  %384 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.21) # transformers/modeling_xlnet.py:1013:0
  %2633 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2634 : Device = prim::Constant[value="cpu"](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2635 : int = prim::Constant[value=6](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2636 : bool = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2637 : None = prim::Constant(), scope: __module.layer.20/__module.layer.20.rel_attn
  %2638 : int = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2639 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2640 : int = prim::Constant[value=3](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2641 : int = prim::Constant[value=0](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2642 : int = prim::Constant[value=2](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2643 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2644 : Long() = prim::Constant[value={1}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2645 : int = prim::Constant[value=4](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2646 : Long() = prim::Constant[value={0}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2647 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2648 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2649 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %2650 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2651 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2652 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2653 : bool = prim::Constant[value=1](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2654 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2655 : int = prim::Constant[value=1024](), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2656 : __torch__.transformers.modeling_xlnet.___torch_mangle_43421.XLNetFeedForward = prim::GetAttr[name="ff"](%10)
  %2657 : __torch__.transformers.modeling_xlnet.___torch_mangle_43416.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%10)
  %2658 : __torch__.torch.nn.modules.normalization.___torch_mangle_43414.LayerNorm = prim::GetAttr[name="layer_norm"](%2657)
  %2659 : Tensor = prim::GetAttr[name="o"](%2657)
  %2660 : Tensor = prim::GetAttr[name="r_r_bias"](%2657)
  %2661 : Tensor = prim::GetAttr[name="r_w_bias"](%2657)
  %2662 : Tensor = prim::GetAttr[name="r"](%2657)
  %2663 : Tensor = prim::GetAttr[name="v"](%2657)
  %2664 : Tensor = prim::GetAttr[name="k"](%2657)
  %2665 : Tensor = prim::GetAttr[name="q"](%2657)
  %2666 : Tensor[] = prim::ListConstruct(%377, %2665), scope: __module.layer.20/__module.layer.20.rel_attn
  %q_head.21 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2633, %2666), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2668 : Tensor[] = prim::ListConstruct(%377, %2664), scope: __module.layer.20/__module.layer.20.rel_attn
  %2669 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2633, %2668), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2670 : Tensor[] = prim::ListConstruct(%377, %2663), scope: __module.layer.20/__module.layer.20.rel_attn
  %2671 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2633, %2670), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %r.22 : Float(26:1024, 17:0, 1024:1) = aten::to(%378, %2634, %2635, %2636, %2636, %2637), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:443:0
  %2673 : Tensor[] = prim::ListConstruct(%r.22, %2662), scope: __module.layer.20/__module.layer.20.rel_attn
  %2674 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2633, %2673), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2675 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %2661, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:284:0
  %2676 : Tensor[] = prim::ListConstruct(%2675, %2669), scope: __module.layer.20/__module.layer.20.rel_attn
  %ac.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2639, %2676), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2678 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.21, %2660, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:287:0
  %2679 : Tensor[] = prim::ListConstruct(%2678, %2674), scope: __module.layer.20/__module.layer.20.rel_attn
  %x.81 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2639, %2679), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2681 : int = aten::size(%ac.21, %2640), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:288:0
  %2682 : int = aten::size(%x.81, %2641), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2683 : int = aten::size(%x.81, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2684 : int = aten::size(%x.81, %2642), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2685 : int = aten::size(%x.81, %2640), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:257:0
  %2686 : Long() = prim::NumToTensor(%2685), scope: __module.layer.20/__module.layer.20.rel_attn
  %2687 : int[] = prim::ListConstruct(%2682, %2683, %2685, %2684), scope: __module.layer.20/__module.layer.20.rel_attn
  %x.82 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.81, %2687), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:259:0
  %2689 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.82, %2641, %2641, %2643, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2690 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2689, %2638, %2641, %2643, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2691 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2690, %2642, %2638, %2643, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.83 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2691, %2640, %2641, %2643, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:260:0
  %2693 : Long() = aten::sub(%2686, %2644, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2694 : int = aten::Int(%2693), scope: __module.layer.20/__module.layer.20.rel_attn
  %2695 : int[] = prim::ListConstruct(%2682, %2683, %2684, %2694), scope: __module.layer.20/__module.layer.20.rel_attn
  %x.84 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.83, %2695), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:261:0
  %2697 : Long(13:1) = aten::arange(%2681, %2645, %2641, %2634, %2636), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.84, %2640, %2697), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:265:0
  %2699 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.21, %bd.21, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2700 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2699, %2646, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.21 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2700, %2647), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:298:0
  %2702 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.20/__module.layer.20.rel_attn
  %2703 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2648, %2702), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2704 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2703, %2649), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.183 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.21, %2704, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.184 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.183, %2640, %2637), scope: __module.layer.20/__module.layer.20.rel_attn # torch/nn/functional.py:1498:0
  %2707 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.184, %2650, %2636), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %2708 : Tensor[] = prim::ListConstruct(%2707, %2671), scope: __module.layer.20/__module.layer.20.rel_attn
  %2709 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2651, %2708), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %2710 : Tensor[] = prim::ListConstruct(%2709, %2659), scope: __module.layer.20/__module.layer.20.rel_attn
  %input.185 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2652, %2710), scope: __module.layer.20/__module.layer.20.rel_attn # torch/functional.py:327:0
  %attn_out.21 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.185, %2650, %2636), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.186 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.21, %377, %2638), scope: __module.layer.20/__module.layer.20.rel_attn # transformers/modeling_xlnet.py:329:0
  %2714 : Tensor = prim::GetAttr[name="bias"](%2658)
  %2715 : Tensor = prim::GetAttr[name="weight"](%2658)
  %2716 : int[] = prim::ListConstruct(%2655), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm
  %input_tensor.21 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.186, %2716, %2715, %2714, %2654, %2653), scope: __module.layer.20/__module.layer.20.rel_attn/__module.layer.20.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2718 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.21, %r.22)
  %2719 : Float(13:17408, 17:1024, 1024:1), %2720 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2718)
  %2721 : __torch__.torch.nn.modules.normalization.___torch_mangle_43417.LayerNorm = prim::GetAttr[name="layer_norm"](%2656)
  %2722 : __torch__.torch.nn.modules.linear.___torch_mangle_43419.Linear = prim::GetAttr[name="layer_2"](%2656)
  %2723 : __torch__.torch.nn.modules.linear.___torch_mangle_43418.Linear = prim::GetAttr[name="layer_1"](%2656)
  %2724 : Tensor = prim::GetAttr[name="bias"](%2723)
  %2725 : Tensor = prim::GetAttr[name="weight"](%2723)
  %2726 : Float(1024:1, 4096:1024) = aten::t(%2725), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.61 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2719, %2726), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.187 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.61, %2724, %2638), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.188 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.187), scope: __module.layer.20/__module.layer.20.ff # torch/nn/functional.py:1369:0
  %input.189 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.188, %2650, %2636), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %2731 : Tensor = prim::GetAttr[name="bias"](%2722)
  %2732 : Tensor = prim::GetAttr[name="weight"](%2722)
  %2733 : Float(4096:1, 1024:4096) = aten::t(%2732), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.62 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.189, %2733), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.190 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.62, %2731, %2638), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.63 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.190, %2650, %2636), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.dropout # torch/nn/functional.py:973:0
  %input.191 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.63, %2719, %2638), scope: __module.layer.20/__module.layer.20.ff # transformers/modeling_xlnet.py:489:0
  %2738 : Tensor = prim::GetAttr[name="bias"](%2721)
  %2739 : Tensor = prim::GetAttr[name="weight"](%2721)
  %2740 : int[] = prim::ListConstruct(%2655), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm
  %curr_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.191, %2740, %2739, %2738, %2654, %2653), scope: __module.layer.20/__module.layer.20.ff/__module.layer.20.ff.layer_norm # torch/nn/functional.py:2048:0
  %2742 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.22, %2720)
  %386 : Float(13:17408, 17:1024, 1024:1), %387 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2742)
  %388 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %389 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %390 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %391 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.22 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%386, %388, %389, %390, %391) # transformers/modeling_xlnet.py:1009:0
  %393 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.22) # transformers/modeling_xlnet.py:1013:0
  %2743 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2744 : Device = prim::Constant[value="cpu"](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2745 : int = prim::Constant[value=6](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2746 : bool = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2747 : None = prim::Constant(), scope: __module.layer.21/__module.layer.21.rel_attn
  %2748 : int = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2749 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2750 : int = prim::Constant[value=3](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2751 : int = prim::Constant[value=0](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2752 : int = prim::Constant[value=2](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2753 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2754 : Long() = prim::Constant[value={1}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2755 : int = prim::Constant[value=4](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2756 : Long() = prim::Constant[value={0}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2757 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2758 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2759 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %2760 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2761 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2762 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2763 : bool = prim::Constant[value=1](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2764 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2765 : int = prim::Constant[value=1024](), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2766 : __torch__.transformers.modeling_xlnet.___torch_mangle_43431.XLNetFeedForward = prim::GetAttr[name="ff"](%8)
  %2767 : __torch__.transformers.modeling_xlnet.___torch_mangle_43426.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%8)
  %2768 : __torch__.torch.nn.modules.normalization.___torch_mangle_43424.LayerNorm = prim::GetAttr[name="layer_norm"](%2767)
  %2769 : Tensor = prim::GetAttr[name="o"](%2767)
  %2770 : Tensor = prim::GetAttr[name="r_r_bias"](%2767)
  %2771 : Tensor = prim::GetAttr[name="r_w_bias"](%2767)
  %2772 : Tensor = prim::GetAttr[name="r"](%2767)
  %2773 : Tensor = prim::GetAttr[name="v"](%2767)
  %2774 : Tensor = prim::GetAttr[name="k"](%2767)
  %2775 : Tensor = prim::GetAttr[name="q"](%2767)
  %2776 : Tensor[] = prim::ListConstruct(%386, %2775), scope: __module.layer.21/__module.layer.21.rel_attn
  %q_head.22 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2743, %2776), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2778 : Tensor[] = prim::ListConstruct(%386, %2774), scope: __module.layer.21/__module.layer.21.rel_attn
  %2779 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2743, %2778), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2780 : Tensor[] = prim::ListConstruct(%386, %2773), scope: __module.layer.21/__module.layer.21.rel_attn
  %2781 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2743, %2780), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %r.23 : Float(26:1024, 17:0, 1024:1) = aten::to(%387, %2744, %2745, %2746, %2746, %2747), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:443:0
  %2783 : Tensor[] = prim::ListConstruct(%r.23, %2772), scope: __module.layer.21/__module.layer.21.rel_attn
  %2784 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2743, %2783), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2785 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2771, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:284:0
  %2786 : Tensor[] = prim::ListConstruct(%2785, %2779), scope: __module.layer.21/__module.layer.21.rel_attn
  %ac.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2749, %2786), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2788 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.22, %2770, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:287:0
  %2789 : Tensor[] = prim::ListConstruct(%2788, %2784), scope: __module.layer.21/__module.layer.21.rel_attn
  %x.85 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2749, %2789), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2791 : int = aten::size(%ac.22, %2750), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:288:0
  %2792 : int = aten::size(%x.85, %2751), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2793 : int = aten::size(%x.85, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2794 : int = aten::size(%x.85, %2752), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2795 : int = aten::size(%x.85, %2750), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:257:0
  %2796 : Long() = prim::NumToTensor(%2795), scope: __module.layer.21/__module.layer.21.rel_attn
  %2797 : int[] = prim::ListConstruct(%2792, %2793, %2795, %2794), scope: __module.layer.21/__module.layer.21.rel_attn
  %x.86 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.85, %2797), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:259:0
  %2799 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.86, %2751, %2751, %2753, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2800 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2799, %2748, %2751, %2753, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2801 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2800, %2752, %2748, %2753, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.87 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2801, %2750, %2751, %2753, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:260:0
  %2803 : Long() = aten::sub(%2796, %2754, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2804 : int = aten::Int(%2803), scope: __module.layer.21/__module.layer.21.rel_attn
  %2805 : int[] = prim::ListConstruct(%2792, %2793, %2794, %2804), scope: __module.layer.21/__module.layer.21.rel_attn
  %x.88 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.87, %2805), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:261:0
  %2807 : Long(13:1) = aten::arange(%2791, %2755, %2751, %2744, %2746), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.88, %2750, %2807), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:265:0
  %2809 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.22, %bd.22, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2810 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2809, %2756, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.22 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2810, %2757), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:298:0
  %2812 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.21/__module.layer.21.rel_attn
  %2813 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2758, %2812), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2814 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2813, %2759), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.192 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.22, %2814, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.193 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.192, %2750, %2747), scope: __module.layer.21/__module.layer.21.rel_attn # torch/nn/functional.py:1498:0
  %2817 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.193, %2760, %2746), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %2818 : Tensor[] = prim::ListConstruct(%2817, %2781), scope: __module.layer.21/__module.layer.21.rel_attn
  %2819 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2761, %2818), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %2820 : Tensor[] = prim::ListConstruct(%2819, %2769), scope: __module.layer.21/__module.layer.21.rel_attn
  %input.194 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2762, %2820), scope: __module.layer.21/__module.layer.21.rel_attn # torch/functional.py:327:0
  %attn_out.22 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.194, %2760, %2746), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.195 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.22, %386, %2748), scope: __module.layer.21/__module.layer.21.rel_attn # transformers/modeling_xlnet.py:329:0
  %2824 : Tensor = prim::GetAttr[name="bias"](%2768)
  %2825 : Tensor = prim::GetAttr[name="weight"](%2768)
  %2826 : int[] = prim::ListConstruct(%2765), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm
  %input_tensor.22 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.195, %2826, %2825, %2824, %2764, %2763), scope: __module.layer.21/__module.layer.21.rel_attn/__module.layer.21.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2828 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.22, %r.23)
  %2829 : Float(13:17408, 17:1024, 1024:1), %2830 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2828)
  %2831 : __torch__.torch.nn.modules.normalization.___torch_mangle_43427.LayerNorm = prim::GetAttr[name="layer_norm"](%2766)
  %2832 : __torch__.torch.nn.modules.linear.___torch_mangle_43429.Linear = prim::GetAttr[name="layer_2"](%2766)
  %2833 : __torch__.torch.nn.modules.linear.___torch_mangle_43428.Linear = prim::GetAttr[name="layer_1"](%2766)
  %2834 : Tensor = prim::GetAttr[name="bias"](%2833)
  %2835 : Tensor = prim::GetAttr[name="weight"](%2833)
  %2836 : Float(1024:1, 4096:1024) = aten::t(%2835), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.64 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2829, %2836), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.196 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.64, %2834, %2748), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.197 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.196), scope: __module.layer.21/__module.layer.21.ff # torch/nn/functional.py:1369:0
  %input.198 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.197, %2760, %2746), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %2841 : Tensor = prim::GetAttr[name="bias"](%2832)
  %2842 : Tensor = prim::GetAttr[name="weight"](%2832)
  %2843 : Float(4096:1, 1024:4096) = aten::t(%2842), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.65 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.198, %2843), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.199 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.65, %2841, %2748), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.66 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.199, %2760, %2746), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.dropout # torch/nn/functional.py:973:0
  %input.200 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.66, %2829, %2748), scope: __module.layer.21/__module.layer.21.ff # transformers/modeling_xlnet.py:489:0
  %2848 : Tensor = prim::GetAttr[name="bias"](%2831)
  %2849 : Tensor = prim::GetAttr[name="weight"](%2831)
  %2850 : int[] = prim::ListConstruct(%2765), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm
  %curr_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.200, %2850, %2849, %2848, %2764, %2763), scope: __module.layer.21/__module.layer.21.ff/__module.layer.21.ff.layer_norm # torch/nn/functional.py:2048:0
  %2852 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out.23, %2830)
  %395 : Float(13:17408, 17:1024, 1024:1), %396 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2852)
  %397 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %398 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %399 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %400 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem.23 : Float(13:17408, 17:1024, 1024:1) = aten::slice(%395, %397, %398, %399, %400) # transformers/modeling_xlnet.py:1009:0
  %402 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem.23) # transformers/modeling_xlnet.py:1013:0
  %2853 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2854 : Device = prim::Constant[value="cpu"](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2855 : int = prim::Constant[value=6](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2856 : bool = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2857 : None = prim::Constant(), scope: __module.layer.22/__module.layer.22.rel_attn
  %2858 : int = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2859 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2860 : int = prim::Constant[value=3](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2861 : int = prim::Constant[value=0](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2862 : int = prim::Constant[value=2](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2863 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2864 : Long() = prim::Constant[value={1}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2865 : int = prim::Constant[value=4](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2866 : Long() = prim::Constant[value={0}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2867 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2868 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2869 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %2870 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2871 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2872 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2873 : bool = prim::Constant[value=1](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2874 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2875 : int = prim::Constant[value=1024](), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2876 : __torch__.transformers.modeling_xlnet.___torch_mangle_43441.XLNetFeedForward = prim::GetAttr[name="ff"](%6)
  %2877 : __torch__.transformers.modeling_xlnet.___torch_mangle_43436.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%6)
  %2878 : __torch__.torch.nn.modules.normalization.___torch_mangle_43434.LayerNorm = prim::GetAttr[name="layer_norm"](%2877)
  %2879 : Tensor = prim::GetAttr[name="o"](%2877)
  %2880 : Tensor = prim::GetAttr[name="r_r_bias"](%2877)
  %2881 : Tensor = prim::GetAttr[name="r_w_bias"](%2877)
  %2882 : Tensor = prim::GetAttr[name="r"](%2877)
  %2883 : Tensor = prim::GetAttr[name="v"](%2877)
  %2884 : Tensor = prim::GetAttr[name="k"](%2877)
  %2885 : Tensor = prim::GetAttr[name="q"](%2877)
  %2886 : Tensor[] = prim::ListConstruct(%395, %2885), scope: __module.layer.22/__module.layer.22.rel_attn
  %q_head.23 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2853, %2886), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2888 : Tensor[] = prim::ListConstruct(%395, %2884), scope: __module.layer.22/__module.layer.22.rel_attn
  %2889 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2853, %2888), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2890 : Tensor[] = prim::ListConstruct(%395, %2883), scope: __module.layer.22/__module.layer.22.rel_attn
  %2891 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2853, %2890), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %r : Float(26:1024, 17:0, 1024:1) = aten::to(%396, %2854, %2855, %2856, %2856, %2857), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:443:0
  %2893 : Tensor[] = prim::ListConstruct(%r, %2882), scope: __module.layer.22/__module.layer.22.rel_attn
  %2894 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2853, %2893), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2895 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2881, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:284:0
  %2896 : Tensor[] = prim::ListConstruct(%2895, %2889), scope: __module.layer.22/__module.layer.22.rel_attn
  %ac.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2859, %2896), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2898 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head.23, %2880, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:287:0
  %2899 : Tensor[] = prim::ListConstruct(%2898, %2894), scope: __module.layer.22/__module.layer.22.rel_attn
  %x.89 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2859, %2899), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2901 : int = aten::size(%ac.23, %2860), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:288:0
  %2902 : int = aten::size(%x.89, %2861), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2903 : int = aten::size(%x.89, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2904 : int = aten::size(%x.89, %2862), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2905 : int = aten::size(%x.89, %2860), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:257:0
  %2906 : Long() = prim::NumToTensor(%2905), scope: __module.layer.22/__module.layer.22.rel_attn
  %2907 : int[] = prim::ListConstruct(%2902, %2903, %2905, %2904), scope: __module.layer.22/__module.layer.22.rel_attn
  %x.90 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.89, %2907), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:259:0
  %2909 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.90, %2861, %2861, %2863, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2910 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%2909, %2858, %2861, %2863, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2911 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2910, %2862, %2858, %2863, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.91 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%2911, %2860, %2861, %2863, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:260:0
  %2913 : Long() = aten::sub(%2906, %2864, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2914 : int = aten::Int(%2913), scope: __module.layer.22/__module.layer.22.rel_attn
  %2915 : int[] = prim::ListConstruct(%2902, %2903, %2904, %2914), scope: __module.layer.22/__module.layer.22.rel_attn
  %x.92 : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.91, %2915), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:261:0
  %2917 : Long(13:1) = aten::arange(%2901, %2865, %2861, %2854, %2856), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x.92, %2860, %2917), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:265:0
  %2919 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac.23, %bd.23, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2920 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%2919, %2866, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score.23 : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%2920, %2867), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:298:0
  %2922 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.22/__module.layer.22.rel_attn
  %2923 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2868, %2922), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2924 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%2923, %2869), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.201 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score.23, %2924, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.202 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.201, %2860, %2857), scope: __module.layer.22/__module.layer.22.rel_attn # torch/nn/functional.py:1498:0
  %2927 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.202, %2870, %2856), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %2928 : Tensor[] = prim::ListConstruct(%2927, %2891), scope: __module.layer.22/__module.layer.22.rel_attn
  %2929 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2871, %2928), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %2930 : Tensor[] = prim::ListConstruct(%2929, %2879), scope: __module.layer.22/__module.layer.22.rel_attn
  %input.203 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2872, %2930), scope: __module.layer.22/__module.layer.22.rel_attn # torch/functional.py:327:0
  %attn_out.23 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.203, %2870, %2856), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.204 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out.23, %395, %2858), scope: __module.layer.22/__module.layer.22.rel_attn # transformers/modeling_xlnet.py:329:0
  %2934 : Tensor = prim::GetAttr[name="bias"](%2878)
  %2935 : Tensor = prim::GetAttr[name="weight"](%2878)
  %2936 : int[] = prim::ListConstruct(%2875), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm
  %input_tensor.23 : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.204, %2936, %2935, %2934, %2874, %2873), scope: __module.layer.22/__module.layer.22.rel_attn/__module.layer.22.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2938 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%input_tensor.23, %r)
  %2939 : Float(13:17408, 17:1024, 1024:1), %2940 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2938)
  %2941 : __torch__.torch.nn.modules.normalization.___torch_mangle_43437.LayerNorm = prim::GetAttr[name="layer_norm"](%2876)
  %2942 : __torch__.torch.nn.modules.linear.___torch_mangle_43439.Linear = prim::GetAttr[name="layer_2"](%2876)
  %2943 : __torch__.torch.nn.modules.linear.___torch_mangle_43438.Linear = prim::GetAttr[name="layer_1"](%2876)
  %2944 : Tensor = prim::GetAttr[name="bias"](%2943)
  %2945 : Tensor = prim::GetAttr[name="weight"](%2943)
  %2946 : Float(1024:1, 4096:1024) = aten::t(%2945), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.67 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%2939, %2946), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.205 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.67, %2944, %2858), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.206 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.205), scope: __module.layer.22/__module.layer.22.ff # torch/nn/functional.py:1369:0
  %input.207 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.206, %2870, %2856), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %2951 : Tensor = prim::GetAttr[name="bias"](%2942)
  %2952 : Tensor = prim::GetAttr[name="weight"](%2942)
  %2953 : Float(4096:1, 1024:4096) = aten::t(%2952), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.68 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.207, %2953), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.208 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.68, %2951, %2858), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_2 # torch/nn/functional.py:1678:0
  %output.69 : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.208, %2870, %2856), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.dropout # torch/nn/functional.py:973:0
  %input.209 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output.69, %2939, %2858), scope: __module.layer.22/__module.layer.22.ff # transformers/modeling_xlnet.py:489:0
  %2958 : Tensor = prim::GetAttr[name="bias"](%2941)
  %2959 : Tensor = prim::GetAttr[name="weight"](%2941)
  %2960 : int[] = prim::ListConstruct(%2875), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm
  %curr_out : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.209, %2960, %2959, %2958, %2874, %2873), scope: __module.layer.22/__module.layer.22.ff/__module.layer.22.ff.layer_norm # torch/nn/functional.py:2048:0
  %2962 : (Float(13:17408, 17:1024, 1024:1), Float(26:1024, 17:0, 1024:1)) = prim::TupleConstruct(%curr_out, %2940)
  %404 : Float(13:17408, 17:1024, 1024:1), %405 : Float(26:1024, 17:0, 1024:1) = prim::TupleUnpack(%2962)
  %406 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %407 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1009:0
  %408 : int = prim::Constant[value=9223372036854775807]() # transformers/modeling_xlnet.py:1009:0
  %409 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1009:0
  %new_mem : Float(13:17408, 17:1024, 1024:1) = aten::slice(%404, %406, %407, %408, %409) # transformers/modeling_xlnet.py:1009:0
  %411 : Float(13:17408, 17:1024, 1024:1) = aten::detach(%new_mem) # transformers/modeling_xlnet.py:1013:0
  %2963 : str = prim::Constant[value="ibh,hnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2964 : Device = prim::Constant[value="cpu"](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2965 : int = prim::Constant[value=6](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2966 : bool = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %2967 : None = prim::Constant(), scope: __module.layer.23/__module.layer.23.rel_attn
  %2968 : int = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %2969 : str = prim::Constant[value="ibnd,jbnd->bnij"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2970 : int = prim::Constant[value=3](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %2971 : int = prim::Constant[value=0](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2972 : int = prim::Constant[value=2](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %2973 : int = prim::Constant[value=9223372036854775807](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %2974 : Long() = prim::Constant[value={1}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %2975 : int = prim::Constant[value=4](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %2976 : Long() = prim::Constant[value={0}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2977 : Double() = prim::Constant[value={0.125}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %2978 : str = prim::Constant[value="ijbn->bnij"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2979 : Double() = prim::Constant[value={1e+30}](), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %2980 : float = prim::Constant[value=0.10000000000000001](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %2981 : str = prim::Constant[value="bnij,jbnd->ibnd"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2982 : str = prim::Constant[value="ibnd,hnd->ibh"](), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2983 : bool = prim::Constant[value=1](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2984 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2985 : int = prim::Constant[value=1024](), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %2986 : __torch__.transformers.modeling_xlnet.___torch_mangle_43451.XLNetFeedForward = prim::GetAttr[name="ff"](%4)
  %2987 : __torch__.transformers.modeling_xlnet.___torch_mangle_43446.XLNetRelativeAttention = prim::GetAttr[name="rel_attn"](%4)
  %2988 : __torch__.torch.nn.modules.normalization.___torch_mangle_43444.LayerNorm = prim::GetAttr[name="layer_norm"](%2987)
  %2989 : Tensor = prim::GetAttr[name="o"](%2987)
  %2990 : Tensor = prim::GetAttr[name="r_r_bias"](%2987)
  %2991 : Tensor = prim::GetAttr[name="r_w_bias"](%2987)
  %2992 : Tensor = prim::GetAttr[name="r"](%2987)
  %2993 : Tensor = prim::GetAttr[name="v"](%2987)
  %2994 : Tensor = prim::GetAttr[name="k"](%2987)
  %2995 : Tensor = prim::GetAttr[name="q"](%2987)
  %2996 : Tensor[] = prim::ListConstruct(%404, %2995), scope: __module.layer.23/__module.layer.23.rel_attn
  %q_head : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2963, %2996), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %2998 : Tensor[] = prim::ListConstruct(%404, %2994), scope: __module.layer.23/__module.layer.23.rel_attn
  %2999 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2963, %2998), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3000 : Tensor[] = prim::ListConstruct(%404, %2993), scope: __module.layer.23/__module.layer.23.rel_attn
  %3001 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2963, %3000), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3002 : Float(26:1024, 17:0, 1024:1) = aten::to(%405, %2964, %2965, %2966, %2966, %2967), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:443:0
  %3003 : Tensor[] = prim::ListConstruct(%3002, %2992), scope: __module.layer.23/__module.layer.23.rel_attn
  %3004 : Float(26:17408, 17:1024, 16:64, 64:1) = aten::einsum(%2963, %3003), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3005 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2991, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:284:0
  %3006 : Tensor[] = prim::ListConstruct(%3005, %2999), scope: __module.layer.23/__module.layer.23.rel_attn
  %ac : Float(17:2704, 16:169, 13:13, 13:1) = aten::einsum(%2969, %3006), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3008 : Float(13:17408, 17:1024, 16:64, 64:1) = aten::add(%q_head, %2990, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:287:0
  %3009 : Tensor[] = prim::ListConstruct(%3008, %3004), scope: __module.layer.23/__module.layer.23.rel_attn
  %x.93 : Float(17:5408, 16:338, 13:26, 26:1) = aten::einsum(%2969, %3009), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3011 : int = aten::size(%ac, %2970), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:288:0
  %3012 : int = aten::size(%x.93, %2971), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %3013 : int = aten::size(%x.93, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %3014 : int = aten::size(%x.93, %2972), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %3015 : int = aten::size(%x.93, %2970), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:257:0
  %3016 : Long() = prim::NumToTensor(%3015), scope: __module.layer.23/__module.layer.23.rel_attn
  %3017 : int[] = prim::ListConstruct(%3012, %3013, %3015, %3014), scope: __module.layer.23/__module.layer.23.rel_attn
  %x.94 : Float(17:5408, 16:338, 26:13, 13:1) = aten::reshape(%x.93, %3017), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:259:0
  %3019 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%x.94, %2971, %2971, %2973, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %3020 : Float(17:5408, 16:338, 26:13, 13:1) = aten::slice(%3019, %2968, %2971, %2973, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %3021 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%3020, %2972, %2968, %2973, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %x.95 : Float(17:5408, 16:338, 25:13, 13:1) = aten::slice(%3021, %2970, %2971, %2973, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:260:0
  %3023 : Long() = aten::sub(%3016, %2974, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %3024 : int = aten::Int(%3023), scope: __module.layer.23/__module.layer.23.rel_attn
  %3025 : int[] = prim::ListConstruct(%3012, %3013, %3014, %3024), scope: __module.layer.23/__module.layer.23.rel_attn
  %x : Float(17:5408, 16:338, 13:25, 25:1) = aten::reshape(%x.95, %3025), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:261:0
  %3027 : Long(13:1) = aten::arange(%3011, %2975, %2971, %2964, %2966), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %bd : Float(17:2704, 16:169, 13:13, 13:1) = aten::index_select(%x, %2970, %3027), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:265:0
  %3029 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%ac, %bd, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %3030 : Float(17:2704, 16:169, 13:13, 13:1) = aten::add(%3029, %2976, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %attn_score : Float(17:2704, 16:169, 13:13, 13:1) = aten::mul(%3030, %2977), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:298:0
  %3032 : Tensor[] = prim::ListConstruct(%139), scope: __module.layer.23/__module.layer.23.rel_attn
  %3033 : Float(17:1, 1:2873, 13:221, 13:17) = aten::einsum(%2978, %3032), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3034 : Float(17:1, 1:221, 13:221, 13:17) = aten::mul(%3033, %2979), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.210 : Float(17:2704, 16:169, 13:13, 13:1) = aten::sub(%attn_score, %3034, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:304:0
  %input.211 : Float(17:2704, 16:169, 13:13, 13:1) = aten::softmax(%input.210, %2970, %2967), scope: __module.layer.23/__module.layer.23.rel_attn # torch/nn/functional.py:1498:0
  %3037 : Float(17:2704, 16:169, 13:13, 13:1) = aten::dropout(%input.211, %2980, %2966), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %3038 : Tensor[] = prim::ListConstruct(%3037, %3001), scope: __module.layer.23/__module.layer.23.rel_attn
  %3039 : Float(13:64, 17:13312, 16:832, 64:1) = aten::einsum(%2981, %3038), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %3040 : Tensor[] = prim::ListConstruct(%3039, %2989), scope: __module.layer.23/__module.layer.23.rel_attn
  %input.212 : Float(13:17408, 17:1024, 1024:1) = aten::einsum(%2982, %3040), scope: __module.layer.23/__module.layer.23.rel_attn # torch/functional.py:327:0
  %attn_out : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.212, %2980, %2966), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.dropout # torch/nn/functional.py:973:0
  %input.213 : Float(13:17408, 17:1024, 1024:1) = aten::add(%attn_out, %404, %2968), scope: __module.layer.23/__module.layer.23.rel_attn # transformers/modeling_xlnet.py:329:0
  %3044 : Tensor = prim::GetAttr[name="bias"](%2988)
  %3045 : Tensor = prim::GetAttr[name="weight"](%2988)
  %3046 : int[] = prim::ListConstruct(%2985), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm
  %input_tensor : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.213, %3046, %3045, %3044, %2984, %2983), scope: __module.layer.23/__module.layer.23.rel_attn/__module.layer.23.rel_attn.layer_norm # torch/nn/functional.py:2048:0
  %3048 : __torch__.torch.nn.modules.normalization.___torch_mangle_43447.LayerNorm = prim::GetAttr[name="layer_norm"](%2986)
  %3049 : __torch__.torch.nn.modules.linear.___torch_mangle_43449.Linear = prim::GetAttr[name="layer_2"](%2986)
  %3050 : __torch__.torch.nn.modules.linear.___torch_mangle_43448.Linear = prim::GetAttr[name="layer_1"](%2986)
  %3051 : Tensor = prim::GetAttr[name="bias"](%3050)
  %3052 : Tensor = prim::GetAttr[name="weight"](%3050)
  %3053 : Float(1024:1, 4096:1024) = aten::t(%3052), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %output.70 : Float(13:69632, 17:4096, 4096:1) = aten::matmul(%input_tensor, %3053), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1676:0
  %input.214 : Float(13:69632, 17:4096, 4096:1) = aten::add_(%output.70, %3051, %2968), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_1 # torch/nn/functional.py:1678:0
  %input.215 : Float(13:69632, 17:4096, 4096:1) = aten::gelu(%input.214), scope: __module.layer.23/__module.layer.23.ff # torch/nn/functional.py:1369:0
  %input.216 : Float(13:69632, 17:4096, 4096:1) = aten::dropout(%input.215, %2980, %2966), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %3058 : Tensor = prim::GetAttr[name="bias"](%3049)
  %3059 : Tensor = prim::GetAttr[name="weight"](%3049)
  %3060 : Float(4096:1, 1024:4096) = aten::t(%3059), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %output.71 : Float(13:17408, 17:1024, 1024:1) = aten::matmul(%input.216, %3060), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1676:0
  %input.217 : Float(13:17408, 17:1024, 1024:1) = aten::add_(%output.71, %3058, %2968), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_2 # torch/nn/functional.py:1678:0
  %output : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input.217, %2980, %2966), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.dropout # torch/nn/functional.py:973:0
  %input.218 : Float(13:17408, 17:1024, 1024:1) = aten::add(%output, %input_tensor, %2968), scope: __module.layer.23/__module.layer.23.ff # transformers/modeling_xlnet.py:489:0
  %3065 : Tensor = prim::GetAttr[name="bias"](%3048)
  %3066 : Tensor = prim::GetAttr[name="weight"](%3048)
  %3067 : int[] = prim::ListConstruct(%2985), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm
  %input : Float(13:17408, 17:1024, 1024:1) = aten::layer_norm(%input.218, %3067, %3066, %3065, %2984, %2983), scope: __module.layer.23/__module.layer.23.ff/__module.layer.23.ff.layer_norm # torch/nn/functional.py:2048:0
  %3069 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %3070 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %output_h : Float(13:17408, 17:1024, 1024:1) = aten::dropout(%input, %3070, %3069), scope: __module.dropout # torch/nn/functional.py:973:0
  %414 : int = prim::Constant[value=1]() # transformers/modeling_xlnet.py:1253:0
  %415 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1253:0
  %416 : int = prim::Constant[value=2]() # transformers/modeling_xlnet.py:1253:0
  %417 : int[] = prim::ListConstruct(%414, %415, %416)
  %418 : Float(17:1024, 13:17408, 1024:1) = aten::permute(%output_h, %417) # transformers/modeling_xlnet.py:1253:0
  %419 : int = prim::Constant[value=0]() # transformers/modeling_xlnet.py:1253:0
  %420 : Float(17:13312, 13:1024, 1024:1) = aten::contiguous(%418, %419) # transformers/modeling_xlnet.py:1253:0
  %421 : (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1)) = prim::TupleConstruct(%204, %213, %222, %231, %240, %249, %258, %267, %276, %285, %294, %303, %312, %321, %330, %339, %348, %357, %366, %375, %384, %393, %402, %411)
  %422 : (Float(17:13312, 13:1024, 1024:1), (Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1), Float(13:17408, 17:1024, 1024:1))) = prim::TupleConstruct(%420, %421)
  return (%422)
