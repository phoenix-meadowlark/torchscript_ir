graph(%self.1 : __torch__.transformers.modeling_albert.AlbertForTokenClassification,
      %input_ids : Long(17:13, 13:1),
      %attention_mask.1 : Long(17:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_311.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_310.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_albert.___torch_mangle_309.AlbertModel = prim::GetAttr[name="albert"](%self.1)
  %10 : str = prim::Constant[value="bfnd,ndh->bfh"](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %11 : int = prim::Constant[value=4096](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %12 : Double() = prim::Constant[value={8}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %13 : int = prim::Constant[value=-2](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %14 : int = prim::Constant[value=3](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %15 : int = prim::Constant[value=64](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %16 : Double() = prim::Constant[value={0.5}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %17 : float = prim::Constant[value=3.](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %18 : Double() = prim::Constant[value={0.044715}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %19 : Double() = prim::Constant[value={0.797885}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %20 : Double() = prim::Constant[value={1}](), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %21 : int = prim::Constant[value=9223372036854775807](), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %22 : int = prim::Constant[value=-1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %23 : bool = prim::Constant[value=1](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %24 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %25 : int = prim::Constant[value=128](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %26 : float = prim::Constant[value=0.](), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %27 : Double() = prim::Constant[value={-10000}](), scope: __module.albert # transformers/modeling_albert.py:678:0
  %28 : float = prim::Constant[value=1.](), scope: __module.albert # torch/tensor.py:396:0
  %29 : None = prim::Constant(), scope: __module.albert
  %30 : int = prim::Constant[value=6](), scope: __module.albert # transformers/modeling_albert.py:677:0
  %31 : int = prim::Constant[value=2](), scope: __module.albert # transformers/modeling_albert.py:676:0
  %32 : bool = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %33 : Device = prim::Constant[value="cpu"](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %34 : int = prim::Constant[value=4](), scope: __module.albert # transformers/modeling_albert.py:674:0
  %35 : int = prim::Constant[value=1](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %36 : int = prim::Constant[value=0](), scope: __module.albert # transformers/modeling_albert.py:663:0
  %37 : __torch__.transformers.modeling_albert.___torch_mangle_308.AlbertTransformer = prim::GetAttr[name="encoder"](%5)
  %38 : __torch__.transformers.modeling_albert.___torch_mangle_290.AlbertEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %39 : int = aten::size(%input_ids, %36), scope: __module.albert # transformers/modeling_albert.py:663:0
  %40 : int = aten::size(%input_ids, %35), scope: __module.albert # transformers/modeling_albert.py:663:0
  %41 : int[] = prim::ListConstruct(%39, %40), scope: __module.albert
  %input.2 : Long(17:13, 13:1) = aten::zeros(%41, %34, %36, %33, %32), scope: __module.albert # transformers/modeling_albert.py:674:0
  %43 : Long(17:13, 1:13, 13:1) = aten::unsqueeze(%attention_mask.1, %35), scope: __module.albert # transformers/modeling_albert.py:676:0
  %extended_attention_mask : Long(17:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%43, %31), scope: __module.albert # transformers/modeling_albert.py:676:0
  %45 : Float(17:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %30, %32, %32, %29), scope: __module.albert # transformers/modeling_albert.py:677:0
  %46 : Float(17:13, 1:13, 1:13, 13:1) = aten::rsub(%45, %28, %35), scope: __module.albert # torch/tensor.py:396:0
  %attention_mask : Float(17:13, 1:13, 1:13, 13:1) = aten::mul(%46, %27), scope: __module.albert # transformers/modeling_albert.py:678:0
  %48 : __torch__.torch.nn.modules.normalization.___torch_mangle_288.LayerNorm = prim::GetAttr[name="LayerNorm"](%38)
  %49 : __torch__.torch.nn.modules.sparse.___torch_mangle_287.Embedding = prim::GetAttr[name="token_type_embeddings"](%38)
  %50 : __torch__.torch.nn.modules.sparse.___torch_mangle_286.Embedding = prim::GetAttr[name="position_embeddings"](%38)
  %51 : __torch__.torch.nn.modules.sparse.___torch_mangle_285.Embedding = prim::GetAttr[name="word_embeddings"](%38)
  %52 : Tensor = prim::GetAttr[name="position_ids"](%38)
  %53 : int = aten::size(%input_ids, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:222:0
  %54 : Long(1:512, 512:1) = aten::slice(%52, %36, %36, %21, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%54, %35, %36, %53, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:229:0
  %56 : Tensor = prim::GetAttr[name="weight"](%51)
  %inputs_embeds : Float(17:1664, 13:128, 128:1) = aten::embedding(%56, %input_ids, %36, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %58 : Tensor = prim::GetAttr[name="weight"](%50)
  %position_embeddings : Float(1:1664, 13:128, 128:1) = aten::embedding(%58, %input.1, %22, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %60 : Tensor = prim::GetAttr[name="weight"](%49)
  %token_type_embeddings : Float(17:1664, 13:128, 128:1) = aten::embedding(%60, %input.2, %22, %32, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %62 : Float(17:1664, 13:128, 128:1) = aten::add(%inputs_embeds, %position_embeddings, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %input.3 : Float(17:1664, 13:128, 128:1) = aten::add(%62, %token_type_embeddings, %35), scope: __module.albert/__module.albert.embeddings # transformers/modeling_albert.py:239:0
  %64 : Tensor = prim::GetAttr[name="bias"](%48)
  %65 : Tensor = prim::GetAttr[name="weight"](%48)
  %66 : int[] = prim::ListConstruct(%25), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm
  %input.4 : Float(17:1664, 13:128, 128:1) = aten::layer_norm(%input.3, %66, %65, %64, %24, %23), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(17:1664, 13:128, 128:1) = aten::dropout(%input.4, %26, %32), scope: __module.albert/__module.albert.embeddings/__module.albert.embeddings.dropout # torch/nn/functional.py:973:0
  %69 : __torch__.torch.nn.modules.container.___torch_mangle_307.ModuleList = prim::GetAttr[name="albert_layer_groups"](%37)
  %70 : __torch__.transformers.modeling_albert.___torch_mangle_306.AlbertLayerGroup = prim::GetAttr[name="0"](%69)
  %71 : __torch__.torch.nn.modules.linear.___torch_mangle_291.Linear = prim::GetAttr[name="embedding_hidden_mapping_in"](%37)
  %72 : Tensor = prim::GetAttr[name="bias"](%71)
  %73 : Tensor = prim::GetAttr[name="weight"](%71)
  %74 : Float(128:1, 4096:128) = aten::t(%73), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %output.1 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.5, %74), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1676:0
  %input.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.1, %72, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.embedding_hidden_mapping_in # torch/nn/functional.py:1678:0
  %77 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %78 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%77)
  %79 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%78)
  %80 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%78)
  %81 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%78)
  %82 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%78)
  %83 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%82)
  %84 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%82)
  %85 : Tensor = prim::GetAttr[name="bias"](%84)
  %86 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%82)
  %87 : Tensor = prim::GetAttr[name="weight"](%86)
  %88 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%82)
  %89 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%82)
  %90 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%82)
  %91 : Tensor = prim::GetAttr[name="bias"](%90)
  %92 : Tensor = prim::GetAttr[name="weight"](%90)
  %93 : Float(4096:1, 4096:4096) = aten::t(%92), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.2 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %93), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.2, %91, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %96 : Tensor = prim::GetAttr[name="bias"](%89)
  %97 : Tensor = prim::GetAttr[name="weight"](%89)
  %98 : Float(4096:1, 4096:4096) = aten::t(%97), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.3 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %98), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.3, %96, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %101 : Tensor = prim::GetAttr[name="bias"](%88)
  %102 : Tensor = prim::GetAttr[name="weight"](%88)
  %103 : Float(4096:1, 4096:4096) = aten::t(%102), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.4 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.6, %103), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.4, %101, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %106 : int = aten::size(%x.1, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %107 : int = aten::size(%x.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %108 : int[] = prim::ListConstruct(%106, %107, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.2 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.1, %108), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %110 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.2, %110), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %112 : int = aten::size(%x.3, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %113 : int = aten::size(%x.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %114 : int[] = prim::ListConstruct(%112, %113, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.4 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.3, %114), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %116 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.4, %116), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %118 : int = aten::size(%x.5, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %119 : int = aten::size(%x.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %120 : int[] = prim::ListConstruct(%118, %119, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.6 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.5, %120), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %122 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.1 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.6, %122), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %124 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.1, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %124), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.1, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.7, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.8, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %131 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %132 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.1, %131), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %133 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%132, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %134 : Float(4096:1, 4096:4096) = aten::t(%87), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %135 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %136 : Float(64:64, 64:1, 4096:4096) = aten::view(%134, %135), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %137 : Float(64:64, 64:1, 4096:4096) = aten::to(%136, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.1 : Float(4096:1) = aten::to(%85, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %139 : Tensor[] = prim::ListConstruct(%133, %137), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %140 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %139), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.9 : Float(17:53248, 13:4096, 4096:1) = aten::add(%140, %b.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.1 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.9, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.10 : Float(17:53248, 13:4096, 4096:1) = aten::add(%input.6, %projected_context_layer.1, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %144 : Tensor = prim::GetAttr[name="bias"](%83)
  %145 : Tensor = prim::GetAttr[name="weight"](%83)
  %146 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.1 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.10, %146, %145, %144, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %148 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.1, %b.1)
  %149 : Float(17:53248, 13:4096, 4096:1), %150 : Float(4096:1) = prim::TupleUnpack(%148)
  %151 : Tensor = prim::GetAttr[name="bias"](%81)
  %152 : Tensor = prim::GetAttr[name="weight"](%81)
  %153 : Float(4096:1, 16384:4096) = aten::t(%152), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.5 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%149, %153), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.7 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.5, %151, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %156 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.7, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %157 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.7, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %158 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%157, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %159 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.7, %158, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %160 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%159, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %161 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%160), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %162 : Float(17:212992, 13:16384, 16384:1) = aten::add(%161, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.11 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%156, %162), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %164 : Tensor = prim::GetAttr[name="bias"](%80)
  %165 : Tensor = prim::GetAttr[name="weight"](%80)
  %166 : Float(16384:1, 4096:16384) = aten::t(%165), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.6 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.11, %166), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.1 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.6, %164, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.12 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.1, %149, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %170 : Tensor = prim::GetAttr[name="bias"](%79)
  %171 : Tensor = prim::GetAttr[name="weight"](%79)
  %172 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.13 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.12, %172, %171, %170, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %174 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.13, %150)
  %175 : Float(17:53248, 13:4096, 4096:1), %176 : Float(4096:1) = prim::TupleUnpack(%174)
  %177 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%175, %176)
  %178 : Float(17:53248, 13:4096, 4096:1), %179 : Float(4096:1) = prim::TupleUnpack(%177)
  %180 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %181 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%180)
  %182 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%181)
  %183 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%181)
  %184 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%181)
  %185 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%181)
  %186 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%185)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%185)
  %188 : Tensor = prim::GetAttr[name="weight"](%187)
  %189 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%185)
  %190 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%185)
  %191 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%185)
  %192 : Tensor = prim::GetAttr[name="bias"](%191)
  %193 : Tensor = prim::GetAttr[name="weight"](%191)
  %194 : Float(4096:1, 4096:4096) = aten::t(%193), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.7 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%178, %194), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.7, %192, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %197 : Tensor = prim::GetAttr[name="bias"](%190)
  %198 : Tensor = prim::GetAttr[name="weight"](%190)
  %199 : Float(4096:1, 4096:4096) = aten::t(%198), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.8 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%178, %199), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.8, %197, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %202 : Tensor = prim::GetAttr[name="bias"](%189)
  %203 : Tensor = prim::GetAttr[name="weight"](%189)
  %204 : Float(4096:1, 4096:4096) = aten::t(%203), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.9 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%178, %204), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.12 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.9, %202, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %207 : int = aten::size(%x.8, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %208 : int = aten::size(%x.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %209 : int[] = prim::ListConstruct(%207, %208, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.9 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.8, %209), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %211 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.9, %211), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %213 : int = aten::size(%x.10, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %214 : int = aten::size(%x.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %215 : int[] = prim::ListConstruct(%213, %214, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.11 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.10, %215), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %217 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.11, %217), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %219 : int = aten::size(%x.12, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %220 : int = aten::size(%x.12, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %221 : int[] = prim::ListConstruct(%219, %220, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.13 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.12, %221), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %223 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.2 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.13, %223), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %225 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.2, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %225), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.3, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.14, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.15, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.2 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %232 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %233 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.2, %232), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %234 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%233, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %235 : Float(4096:1, 4096:4096) = aten::t(%188), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %236 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %237 : Float(64:64, 64:1, 4096:4096) = aten::view(%235, %236), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %238 : Float(64:64, 64:1, 4096:4096) = aten::to(%237, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.2 : Float(4096:1) = aten::to(%179, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %240 : Tensor[] = prim::ListConstruct(%234, %238), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %241 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %240), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.16 : Float(17:53248, 13:4096, 4096:1) = aten::add(%241, %b.2, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.2 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.16, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.17 : Float(17:53248, 13:4096, 4096:1) = aten::add(%178, %projected_context_layer.2, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %245 : Tensor = prim::GetAttr[name="bias"](%186)
  %246 : Tensor = prim::GetAttr[name="weight"](%186)
  %247 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.2 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.17, %247, %246, %245, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %249 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.2, %b.2)
  %250 : Float(17:53248, 13:4096, 4096:1), %251 : Float(4096:1) = prim::TupleUnpack(%249)
  %252 : Tensor = prim::GetAttr[name="bias"](%184)
  %253 : Tensor = prim::GetAttr[name="weight"](%184)
  %254 : Float(4096:1, 16384:4096) = aten::t(%253), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.10 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%250, %254), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.14 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.10, %252, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %257 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.14, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %258 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.14, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %259 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%258, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %260 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.14, %259, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %261 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%260, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %262 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%261), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %263 : Float(17:212992, 13:16384, 16384:1) = aten::add(%262, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.18 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%257, %263), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %265 : Tensor = prim::GetAttr[name="bias"](%183)
  %266 : Tensor = prim::GetAttr[name="weight"](%183)
  %267 : Float(16384:1, 4096:16384) = aten::t(%266), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.11 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.18, %267), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.2 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.11, %265, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.19 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.2, %250, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %271 : Tensor = prim::GetAttr[name="bias"](%182)
  %272 : Tensor = prim::GetAttr[name="weight"](%182)
  %273 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.20 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.19, %273, %272, %271, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %275 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.20, %251)
  %276 : Float(17:53248, 13:4096, 4096:1), %277 : Float(4096:1) = prim::TupleUnpack(%275)
  %278 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%276, %277)
  %279 : Float(17:53248, 13:4096, 4096:1), %280 : Float(4096:1) = prim::TupleUnpack(%278)
  %281 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %282 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%281)
  %283 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%282)
  %284 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%282)
  %285 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%282)
  %286 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%282)
  %287 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%286)
  %288 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%286)
  %289 : Tensor = prim::GetAttr[name="weight"](%288)
  %290 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%286)
  %291 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%286)
  %292 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%286)
  %293 : Tensor = prim::GetAttr[name="bias"](%292)
  %294 : Tensor = prim::GetAttr[name="weight"](%292)
  %295 : Float(4096:1, 4096:4096) = aten::t(%294), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.12 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%279, %295), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.15 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.12, %293, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %298 : Tensor = prim::GetAttr[name="bias"](%291)
  %299 : Tensor = prim::GetAttr[name="weight"](%291)
  %300 : Float(4096:1, 4096:4096) = aten::t(%299), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.13 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%279, %300), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.17 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.13, %298, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %303 : Tensor = prim::GetAttr[name="bias"](%290)
  %304 : Tensor = prim::GetAttr[name="weight"](%290)
  %305 : Float(4096:1, 4096:4096) = aten::t(%304), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.14 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%279, %305), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.19 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.14, %303, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %308 : int = aten::size(%x.15, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %309 : int = aten::size(%x.15, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %310 : int[] = prim::ListConstruct(%308, %309, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.16 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.15, %310), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %312 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.16, %312), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %314 : int = aten::size(%x.17, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %315 : int = aten::size(%x.17, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %316 : int[] = prim::ListConstruct(%314, %315, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.18 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.17, %316), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %318 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.18, %318), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %320 : int = aten::size(%x.19, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %321 : int = aten::size(%x.19, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %322 : int[] = prim::ListConstruct(%320, %321, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.20 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.19, %322), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %324 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.3 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.20, %324), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %326 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.3, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %326), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.5, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.21, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.22, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %333 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %334 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.3, %333), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %335 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%334, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %336 : Float(4096:1, 4096:4096) = aten::t(%289), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %337 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %338 : Float(64:64, 64:1, 4096:4096) = aten::view(%336, %337), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %339 : Float(64:64, 64:1, 4096:4096) = aten::to(%338, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.3 : Float(4096:1) = aten::to(%280, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %341 : Tensor[] = prim::ListConstruct(%335, %339), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %342 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %341), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.23 : Float(17:53248, 13:4096, 4096:1) = aten::add(%342, %b.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.3 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.23, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.24 : Float(17:53248, 13:4096, 4096:1) = aten::add(%279, %projected_context_layer.3, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %346 : Tensor = prim::GetAttr[name="bias"](%287)
  %347 : Tensor = prim::GetAttr[name="weight"](%287)
  %348 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.3 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.24, %348, %347, %346, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %350 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.3, %b.3)
  %351 : Float(17:53248, 13:4096, 4096:1), %352 : Float(4096:1) = prim::TupleUnpack(%350)
  %353 : Tensor = prim::GetAttr[name="bias"](%285)
  %354 : Tensor = prim::GetAttr[name="weight"](%285)
  %355 : Float(4096:1, 16384:4096) = aten::t(%354), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.15 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%351, %355), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.21 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.15, %353, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %358 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.21, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %359 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.21, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %360 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%359, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %361 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.21, %360, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %362 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%361, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %363 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%362), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %364 : Float(17:212992, 13:16384, 16384:1) = aten::add(%363, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.25 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%358, %364), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %366 : Tensor = prim::GetAttr[name="bias"](%284)
  %367 : Tensor = prim::GetAttr[name="weight"](%284)
  %368 : Float(16384:1, 4096:16384) = aten::t(%367), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.16 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.25, %368), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.3 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.16, %366, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.26 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.3, %351, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %372 : Tensor = prim::GetAttr[name="bias"](%283)
  %373 : Tensor = prim::GetAttr[name="weight"](%283)
  %374 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.27 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.26, %374, %373, %372, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %376 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.27, %352)
  %377 : Float(17:53248, 13:4096, 4096:1), %378 : Float(4096:1) = prim::TupleUnpack(%376)
  %379 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%377, %378)
  %380 : Float(17:53248, 13:4096, 4096:1), %381 : Float(4096:1) = prim::TupleUnpack(%379)
  %382 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %383 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%382)
  %384 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%383)
  %385 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%383)
  %386 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%383)
  %387 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%383)
  %388 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%387)
  %389 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%387)
  %390 : Tensor = prim::GetAttr[name="weight"](%389)
  %391 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%387)
  %392 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%387)
  %393 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%387)
  %394 : Tensor = prim::GetAttr[name="bias"](%393)
  %395 : Tensor = prim::GetAttr[name="weight"](%393)
  %396 : Float(4096:1, 4096:4096) = aten::t(%395), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.17 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%380, %396), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.22 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.17, %394, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %399 : Tensor = prim::GetAttr[name="bias"](%392)
  %400 : Tensor = prim::GetAttr[name="weight"](%392)
  %401 : Float(4096:1, 4096:4096) = aten::t(%400), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.18 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%380, %401), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.24 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.18, %399, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %404 : Tensor = prim::GetAttr[name="bias"](%391)
  %405 : Tensor = prim::GetAttr[name="weight"](%391)
  %406 : Float(4096:1, 4096:4096) = aten::t(%405), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.19 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%380, %406), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.26 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.19, %404, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %409 : int = aten::size(%x.22, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %410 : int = aten::size(%x.22, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %411 : int[] = prim::ListConstruct(%409, %410, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.23 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.22, %411), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %413 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.23, %413), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %415 : int = aten::size(%x.24, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %416 : int = aten::size(%x.24, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %417 : int[] = prim::ListConstruct(%415, %416, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.25 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.24, %417), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %419 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.25, %419), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %421 : int = aten::size(%x.26, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %422 : int = aten::size(%x.26, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %423 : int[] = prim::ListConstruct(%421, %422, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.27 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.26, %423), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %425 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.4 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.27, %425), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %427 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.4, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %427), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.7, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.28 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.29 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.28, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.29, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.4 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %434 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %435 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.4, %434), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %436 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%435, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %437 : Float(4096:1, 4096:4096) = aten::t(%390), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %438 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %439 : Float(64:64, 64:1, 4096:4096) = aten::view(%437, %438), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %440 : Float(64:64, 64:1, 4096:4096) = aten::to(%439, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.4 : Float(4096:1) = aten::to(%381, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %442 : Tensor[] = prim::ListConstruct(%436, %440), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %443 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %442), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.30 : Float(17:53248, 13:4096, 4096:1) = aten::add(%443, %b.4, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.4 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.30, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.31 : Float(17:53248, 13:4096, 4096:1) = aten::add(%380, %projected_context_layer.4, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %447 : Tensor = prim::GetAttr[name="bias"](%388)
  %448 : Tensor = prim::GetAttr[name="weight"](%388)
  %449 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.4 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.31, %449, %448, %447, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %451 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.4, %b.4)
  %452 : Float(17:53248, 13:4096, 4096:1), %453 : Float(4096:1) = prim::TupleUnpack(%451)
  %454 : Tensor = prim::GetAttr[name="bias"](%386)
  %455 : Tensor = prim::GetAttr[name="weight"](%386)
  %456 : Float(4096:1, 16384:4096) = aten::t(%455), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.20 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%452, %456), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.28 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.20, %454, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %459 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.28, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %460 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.28, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %461 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%460, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %462 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.28, %461, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %463 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%462, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %464 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%463), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %465 : Float(17:212992, 13:16384, 16384:1) = aten::add(%464, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.32 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%459, %465), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %467 : Tensor = prim::GetAttr[name="bias"](%385)
  %468 : Tensor = prim::GetAttr[name="weight"](%385)
  %469 : Float(16384:1, 4096:16384) = aten::t(%468), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.21 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.32, %469), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.4 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.21, %467, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.33 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.4, %452, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %473 : Tensor = prim::GetAttr[name="bias"](%384)
  %474 : Tensor = prim::GetAttr[name="weight"](%384)
  %475 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.34 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.33, %475, %474, %473, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %477 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.34, %453)
  %478 : Float(17:53248, 13:4096, 4096:1), %479 : Float(4096:1) = prim::TupleUnpack(%477)
  %480 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%478, %479)
  %481 : Float(17:53248, 13:4096, 4096:1), %482 : Float(4096:1) = prim::TupleUnpack(%480)
  %483 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %484 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%483)
  %485 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%484)
  %486 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%484)
  %487 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%484)
  %488 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%484)
  %489 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%488)
  %490 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%488)
  %491 : Tensor = prim::GetAttr[name="weight"](%490)
  %492 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%488)
  %493 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%488)
  %494 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%488)
  %495 : Tensor = prim::GetAttr[name="bias"](%494)
  %496 : Tensor = prim::GetAttr[name="weight"](%494)
  %497 : Float(4096:1, 4096:4096) = aten::t(%496), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.22 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%481, %497), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.29 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.22, %495, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %500 : Tensor = prim::GetAttr[name="bias"](%493)
  %501 : Tensor = prim::GetAttr[name="weight"](%493)
  %502 : Float(4096:1, 4096:4096) = aten::t(%501), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.23 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%481, %502), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.31 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.23, %500, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %505 : Tensor = prim::GetAttr[name="bias"](%492)
  %506 : Tensor = prim::GetAttr[name="weight"](%492)
  %507 : Float(4096:1, 4096:4096) = aten::t(%506), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.24 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%481, %507), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.33 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.24, %505, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %510 : int = aten::size(%x.29, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %511 : int = aten::size(%x.29, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %512 : int[] = prim::ListConstruct(%510, %511, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.30 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.29, %512), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %514 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.30, %514), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %516 : int = aten::size(%x.31, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %517 : int = aten::size(%x.31, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %518 : int[] = prim::ListConstruct(%516, %517, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.32 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.31, %518), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %520 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.32, %520), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %522 : int = aten::size(%x.33, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %523 : int = aten::size(%x.33, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %524 : int[] = prim::ListConstruct(%522, %523, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.34 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.33, %524), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %526 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.5 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.34, %526), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %528 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.5, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %528), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.9, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.35 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.36 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.35, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.36, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %535 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %536 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.5, %535), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %537 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%536, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %538 : Float(4096:1, 4096:4096) = aten::t(%491), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %539 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %540 : Float(64:64, 64:1, 4096:4096) = aten::view(%538, %539), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %541 : Float(64:64, 64:1, 4096:4096) = aten::to(%540, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.5 : Float(4096:1) = aten::to(%482, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %543 : Tensor[] = prim::ListConstruct(%537, %541), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %544 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %543), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.37 : Float(17:53248, 13:4096, 4096:1) = aten::add(%544, %b.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.5 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.37, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.38 : Float(17:53248, 13:4096, 4096:1) = aten::add(%481, %projected_context_layer.5, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %548 : Tensor = prim::GetAttr[name="bias"](%489)
  %549 : Tensor = prim::GetAttr[name="weight"](%489)
  %550 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.5 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.38, %550, %549, %548, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %552 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.5, %b.5)
  %553 : Float(17:53248, 13:4096, 4096:1), %554 : Float(4096:1) = prim::TupleUnpack(%552)
  %555 : Tensor = prim::GetAttr[name="bias"](%487)
  %556 : Tensor = prim::GetAttr[name="weight"](%487)
  %557 : Float(4096:1, 16384:4096) = aten::t(%556), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.25 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%553, %557), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.35 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.25, %555, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %560 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.35, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %561 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.35, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %562 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%561, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %563 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.35, %562, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %564 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%563, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %565 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%564), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %566 : Float(17:212992, 13:16384, 16384:1) = aten::add(%565, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.39 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%560, %566), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %568 : Tensor = prim::GetAttr[name="bias"](%486)
  %569 : Tensor = prim::GetAttr[name="weight"](%486)
  %570 : Float(16384:1, 4096:16384) = aten::t(%569), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.26 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.39, %570), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.5 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.26, %568, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.40 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.5, %553, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %574 : Tensor = prim::GetAttr[name="bias"](%485)
  %575 : Tensor = prim::GetAttr[name="weight"](%485)
  %576 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.41 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.40, %576, %575, %574, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %578 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.41, %554)
  %579 : Float(17:53248, 13:4096, 4096:1), %580 : Float(4096:1) = prim::TupleUnpack(%578)
  %581 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%579, %580)
  %582 : Float(17:53248, 13:4096, 4096:1), %583 : Float(4096:1) = prim::TupleUnpack(%581)
  %584 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %585 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%584)
  %586 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%585)
  %587 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%585)
  %588 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%585)
  %589 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%585)
  %590 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%589)
  %591 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%589)
  %592 : Tensor = prim::GetAttr[name="weight"](%591)
  %593 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%589)
  %594 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%589)
  %595 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%589)
  %596 : Tensor = prim::GetAttr[name="bias"](%595)
  %597 : Tensor = prim::GetAttr[name="weight"](%595)
  %598 : Float(4096:1, 4096:4096) = aten::t(%597), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.27 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%582, %598), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.36 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.27, %596, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %601 : Tensor = prim::GetAttr[name="bias"](%594)
  %602 : Tensor = prim::GetAttr[name="weight"](%594)
  %603 : Float(4096:1, 4096:4096) = aten::t(%602), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.28 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%582, %603), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.38 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.28, %601, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %606 : Tensor = prim::GetAttr[name="bias"](%593)
  %607 : Tensor = prim::GetAttr[name="weight"](%593)
  %608 : Float(4096:1, 4096:4096) = aten::t(%607), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.29 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%582, %608), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.40 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.29, %606, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %611 : int = aten::size(%x.36, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %612 : int = aten::size(%x.36, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %613 : int[] = prim::ListConstruct(%611, %612, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.37 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.36, %613), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %615 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.37, %615), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %617 : int = aten::size(%x.38, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %618 : int = aten::size(%x.38, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %619 : int[] = prim::ListConstruct(%617, %618, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.39 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.38, %619), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %621 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.39, %621), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %623 : int = aten::size(%x.40, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %624 : int = aten::size(%x.40, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %625 : int[] = prim::ListConstruct(%623, %624, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.41 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.40, %625), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %627 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.6 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.41, %627), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %629 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.6, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %629), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.12 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.11, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.42 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.43 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.42, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.43, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.6 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %636 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %637 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.6, %636), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %638 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%637, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %639 : Float(4096:1, 4096:4096) = aten::t(%592), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %640 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %641 : Float(64:64, 64:1, 4096:4096) = aten::view(%639, %640), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %642 : Float(64:64, 64:1, 4096:4096) = aten::to(%641, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.6 : Float(4096:1) = aten::to(%583, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %644 : Tensor[] = prim::ListConstruct(%638, %642), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %645 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %644), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.44 : Float(17:53248, 13:4096, 4096:1) = aten::add(%645, %b.6, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.6 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.44, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.45 : Float(17:53248, 13:4096, 4096:1) = aten::add(%582, %projected_context_layer.6, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %649 : Tensor = prim::GetAttr[name="bias"](%590)
  %650 : Tensor = prim::GetAttr[name="weight"](%590)
  %651 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.6 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.45, %651, %650, %649, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %653 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.6, %b.6)
  %654 : Float(17:53248, 13:4096, 4096:1), %655 : Float(4096:1) = prim::TupleUnpack(%653)
  %656 : Tensor = prim::GetAttr[name="bias"](%588)
  %657 : Tensor = prim::GetAttr[name="weight"](%588)
  %658 : Float(4096:1, 16384:4096) = aten::t(%657), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.30 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%654, %658), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.42 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.30, %656, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %661 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.42, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %662 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.42, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %663 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%662, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %664 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.42, %663, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %665 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%664, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %666 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%665), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %667 : Float(17:212992, 13:16384, 16384:1) = aten::add(%666, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.46 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%661, %667), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %669 : Tensor = prim::GetAttr[name="bias"](%587)
  %670 : Tensor = prim::GetAttr[name="weight"](%587)
  %671 : Float(16384:1, 4096:16384) = aten::t(%670), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.31 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.46, %671), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.6 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.31, %669, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.47 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.6, %654, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %675 : Tensor = prim::GetAttr[name="bias"](%586)
  %676 : Tensor = prim::GetAttr[name="weight"](%586)
  %677 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.48 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.47, %677, %676, %675, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %679 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.48, %655)
  %680 : Float(17:53248, 13:4096, 4096:1), %681 : Float(4096:1) = prim::TupleUnpack(%679)
  %682 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%680, %681)
  %683 : Float(17:53248, 13:4096, 4096:1), %684 : Float(4096:1) = prim::TupleUnpack(%682)
  %685 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %686 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%685)
  %687 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%686)
  %688 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%686)
  %689 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%686)
  %690 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%686)
  %691 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%690)
  %692 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%690)
  %693 : Tensor = prim::GetAttr[name="weight"](%692)
  %694 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%690)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%690)
  %696 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%690)
  %697 : Tensor = prim::GetAttr[name="bias"](%696)
  %698 : Tensor = prim::GetAttr[name="weight"](%696)
  %699 : Float(4096:1, 4096:4096) = aten::t(%698), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.32 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%683, %699), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.43 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.32, %697, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %702 : Tensor = prim::GetAttr[name="bias"](%695)
  %703 : Tensor = prim::GetAttr[name="weight"](%695)
  %704 : Float(4096:1, 4096:4096) = aten::t(%703), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.33 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%683, %704), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.45 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.33, %702, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %707 : Tensor = prim::GetAttr[name="bias"](%694)
  %708 : Tensor = prim::GetAttr[name="weight"](%694)
  %709 : Float(4096:1, 4096:4096) = aten::t(%708), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.34 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%683, %709), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.47 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.34, %707, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %712 : int = aten::size(%x.43, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %713 : int = aten::size(%x.43, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %714 : int[] = prim::ListConstruct(%712, %713, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.44 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.43, %714), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %716 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.44, %716), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %718 : int = aten::size(%x.45, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %719 : int = aten::size(%x.45, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %720 : int[] = prim::ListConstruct(%718, %719, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.46 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.45, %720), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %722 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.46, %722), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %724 : int = aten::size(%x.47, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %725 : int = aten::size(%x.47, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %726 : int[] = prim::ListConstruct(%724, %725, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.48 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.47, %726), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %728 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.7 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.48, %728), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %730 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.7, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.13 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %730), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.14 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.13, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.49 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.50 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.49, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.50, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %737 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %738 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.7, %737), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %739 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%738, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %740 : Float(4096:1, 4096:4096) = aten::t(%693), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %741 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %742 : Float(64:64, 64:1, 4096:4096) = aten::view(%740, %741), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %743 : Float(64:64, 64:1, 4096:4096) = aten::to(%742, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.7 : Float(4096:1) = aten::to(%684, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %745 : Tensor[] = prim::ListConstruct(%739, %743), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %746 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %745), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.51 : Float(17:53248, 13:4096, 4096:1) = aten::add(%746, %b.7, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.7 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.51, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.52 : Float(17:53248, 13:4096, 4096:1) = aten::add(%683, %projected_context_layer.7, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %750 : Tensor = prim::GetAttr[name="bias"](%691)
  %751 : Tensor = prim::GetAttr[name="weight"](%691)
  %752 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.7 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.52, %752, %751, %750, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %754 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.7, %b.7)
  %755 : Float(17:53248, 13:4096, 4096:1), %756 : Float(4096:1) = prim::TupleUnpack(%754)
  %757 : Tensor = prim::GetAttr[name="bias"](%689)
  %758 : Tensor = prim::GetAttr[name="weight"](%689)
  %759 : Float(4096:1, 16384:4096) = aten::t(%758), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.35 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%755, %759), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.49 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.35, %757, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %762 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.49, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %763 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.49, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %764 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%763, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %765 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.49, %764, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %766 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%765, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %767 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%766), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %768 : Float(17:212992, 13:16384, 16384:1) = aten::add(%767, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.53 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%762, %768), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %770 : Tensor = prim::GetAttr[name="bias"](%688)
  %771 : Tensor = prim::GetAttr[name="weight"](%688)
  %772 : Float(16384:1, 4096:16384) = aten::t(%771), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.36 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.53, %772), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.7 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.36, %770, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.54 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.7, %755, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %776 : Tensor = prim::GetAttr[name="bias"](%687)
  %777 : Tensor = prim::GetAttr[name="weight"](%687)
  %778 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.55 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.54, %778, %777, %776, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %780 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.55, %756)
  %781 : Float(17:53248, 13:4096, 4096:1), %782 : Float(4096:1) = prim::TupleUnpack(%780)
  %783 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%781, %782)
  %784 : Float(17:53248, 13:4096, 4096:1), %785 : Float(4096:1) = prim::TupleUnpack(%783)
  %786 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %787 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%786)
  %788 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%787)
  %789 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%787)
  %790 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%787)
  %791 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%787)
  %792 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%791)
  %793 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%791)
  %794 : Tensor = prim::GetAttr[name="weight"](%793)
  %795 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%791)
  %796 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%791)
  %797 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%791)
  %798 : Tensor = prim::GetAttr[name="bias"](%797)
  %799 : Tensor = prim::GetAttr[name="weight"](%797)
  %800 : Float(4096:1, 4096:4096) = aten::t(%799), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.37 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%784, %800), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.50 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.37, %798, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %803 : Tensor = prim::GetAttr[name="bias"](%796)
  %804 : Tensor = prim::GetAttr[name="weight"](%796)
  %805 : Float(4096:1, 4096:4096) = aten::t(%804), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.38 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%784, %805), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.52 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.38, %803, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %808 : Tensor = prim::GetAttr[name="bias"](%795)
  %809 : Tensor = prim::GetAttr[name="weight"](%795)
  %810 : Float(4096:1, 4096:4096) = aten::t(%809), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.39 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%784, %810), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.54 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.39, %808, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %813 : int = aten::size(%x.50, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %814 : int = aten::size(%x.50, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %815 : int[] = prim::ListConstruct(%813, %814, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.51 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.50, %815), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %817 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.51, %817), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %819 : int = aten::size(%x.52, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %820 : int = aten::size(%x.52, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %821 : int[] = prim::ListConstruct(%819, %820, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.53 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.52, %821), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %823 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.53, %823), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %825 : int = aten::size(%x.54, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %826 : int = aten::size(%x.54, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %827 : int[] = prim::ListConstruct(%825, %826, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.55 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.54, %827), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %829 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.8 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.55, %829), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %831 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.8, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.15 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %831), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.16 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.15, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.56 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.57 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.56, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.57, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.8 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %838 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %839 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.8, %838), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %840 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%839, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %841 : Float(4096:1, 4096:4096) = aten::t(%794), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %842 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %843 : Float(64:64, 64:1, 4096:4096) = aten::view(%841, %842), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %844 : Float(64:64, 64:1, 4096:4096) = aten::to(%843, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.8 : Float(4096:1) = aten::to(%785, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %846 : Tensor[] = prim::ListConstruct(%840, %844), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %847 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %846), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.58 : Float(17:53248, 13:4096, 4096:1) = aten::add(%847, %b.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.8 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.58, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.59 : Float(17:53248, 13:4096, 4096:1) = aten::add(%784, %projected_context_layer.8, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %851 : Tensor = prim::GetAttr[name="bias"](%792)
  %852 : Tensor = prim::GetAttr[name="weight"](%792)
  %853 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.8 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.59, %853, %852, %851, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %855 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.8, %b.8)
  %856 : Float(17:53248, 13:4096, 4096:1), %857 : Float(4096:1) = prim::TupleUnpack(%855)
  %858 : Tensor = prim::GetAttr[name="bias"](%790)
  %859 : Tensor = prim::GetAttr[name="weight"](%790)
  %860 : Float(4096:1, 16384:4096) = aten::t(%859), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.40 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%856, %860), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.56 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.40, %858, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %863 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.56, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %864 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.56, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %865 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%864, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %866 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.56, %865, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %867 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%866, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %868 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%867), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %869 : Float(17:212992, 13:16384, 16384:1) = aten::add(%868, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.60 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%863, %869), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %871 : Tensor = prim::GetAttr[name="bias"](%789)
  %872 : Tensor = prim::GetAttr[name="weight"](%789)
  %873 : Float(16384:1, 4096:16384) = aten::t(%872), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.41 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.60, %873), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.8 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.41, %871, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.61 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.8, %856, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %877 : Tensor = prim::GetAttr[name="bias"](%788)
  %878 : Tensor = prim::GetAttr[name="weight"](%788)
  %879 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.62 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.61, %879, %878, %877, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %881 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.62, %857)
  %882 : Float(17:53248, 13:4096, 4096:1), %883 : Float(4096:1) = prim::TupleUnpack(%881)
  %884 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%882, %883)
  %885 : Float(17:53248, 13:4096, 4096:1), %886 : Float(4096:1) = prim::TupleUnpack(%884)
  %887 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %888 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%887)
  %889 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%888)
  %890 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%888)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%888)
  %892 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%888)
  %893 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%892)
  %894 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%892)
  %895 : Tensor = prim::GetAttr[name="weight"](%894)
  %896 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%892)
  %897 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%892)
  %898 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%892)
  %899 : Tensor = prim::GetAttr[name="bias"](%898)
  %900 : Tensor = prim::GetAttr[name="weight"](%898)
  %901 : Float(4096:1, 4096:4096) = aten::t(%900), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.42 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%885, %901), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.57 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.42, %899, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %904 : Tensor = prim::GetAttr[name="bias"](%897)
  %905 : Tensor = prim::GetAttr[name="weight"](%897)
  %906 : Float(4096:1, 4096:4096) = aten::t(%905), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.43 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%885, %906), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.59 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.43, %904, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %909 : Tensor = prim::GetAttr[name="bias"](%896)
  %910 : Tensor = prim::GetAttr[name="weight"](%896)
  %911 : Float(4096:1, 4096:4096) = aten::t(%910), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.44 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%885, %911), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.61 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.44, %909, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %914 : int = aten::size(%x.57, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %915 : int = aten::size(%x.57, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %916 : int[] = prim::ListConstruct(%914, %915, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.58 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.57, %916), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %918 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.58, %918), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %920 : int = aten::size(%x.59, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %921 : int = aten::size(%x.59, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %922 : int[] = prim::ListConstruct(%920, %921, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.60 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.59, %922), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %924 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.60, %924), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %926 : int = aten::size(%x.61, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %927 : int = aten::size(%x.61, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %928 : int[] = prim::ListConstruct(%926, %927, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.62 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.61, %928), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %930 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.9 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.62, %930), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %932 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.9, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.17 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %932), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.18 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.17, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.63 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.64 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.63, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.64, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %939 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %940 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.9, %939), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %941 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%940, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %942 : Float(4096:1, 4096:4096) = aten::t(%895), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %943 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %944 : Float(64:64, 64:1, 4096:4096) = aten::view(%942, %943), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %945 : Float(64:64, 64:1, 4096:4096) = aten::to(%944, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.9 : Float(4096:1) = aten::to(%886, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %947 : Tensor[] = prim::ListConstruct(%941, %945), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %948 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %947), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.65 : Float(17:53248, 13:4096, 4096:1) = aten::add(%948, %b.9, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.9 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.65, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.66 : Float(17:53248, 13:4096, 4096:1) = aten::add(%885, %projected_context_layer.9, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %952 : Tensor = prim::GetAttr[name="bias"](%893)
  %953 : Tensor = prim::GetAttr[name="weight"](%893)
  %954 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.9 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.66, %954, %953, %952, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %956 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.9, %b.9)
  %957 : Float(17:53248, 13:4096, 4096:1), %958 : Float(4096:1) = prim::TupleUnpack(%956)
  %959 : Tensor = prim::GetAttr[name="bias"](%891)
  %960 : Tensor = prim::GetAttr[name="weight"](%891)
  %961 : Float(4096:1, 16384:4096) = aten::t(%960), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.45 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%957, %961), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.63 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.45, %959, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %964 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.63, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %965 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.63, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %966 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%965, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %967 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.63, %966, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %968 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%967, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %969 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%968), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %970 : Float(17:212992, 13:16384, 16384:1) = aten::add(%969, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.67 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%964, %970), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %972 : Tensor = prim::GetAttr[name="bias"](%890)
  %973 : Tensor = prim::GetAttr[name="weight"](%890)
  %974 : Float(16384:1, 4096:16384) = aten::t(%973), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.46 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.67, %974), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.9 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.46, %972, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.68 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.9, %957, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %978 : Tensor = prim::GetAttr[name="bias"](%889)
  %979 : Tensor = prim::GetAttr[name="weight"](%889)
  %980 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.69 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.68, %980, %979, %978, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %982 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.69, %958)
  %983 : Float(17:53248, 13:4096, 4096:1), %984 : Float(4096:1) = prim::TupleUnpack(%982)
  %985 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%983, %984)
  %986 : Float(17:53248, 13:4096, 4096:1), %987 : Float(4096:1) = prim::TupleUnpack(%985)
  %988 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %989 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%988)
  %990 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%989)
  %991 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%989)
  %992 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%989)
  %993 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%989)
  %994 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%993)
  %995 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%993)
  %996 : Tensor = prim::GetAttr[name="weight"](%995)
  %997 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%993)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%993)
  %999 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%993)
  %1000 : Tensor = prim::GetAttr[name="bias"](%999)
  %1001 : Tensor = prim::GetAttr[name="weight"](%999)
  %1002 : Float(4096:1, 4096:4096) = aten::t(%1001), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.47 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%986, %1002), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.64 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.47, %1000, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1005 : Tensor = prim::GetAttr[name="bias"](%998)
  %1006 : Tensor = prim::GetAttr[name="weight"](%998)
  %1007 : Float(4096:1, 4096:4096) = aten::t(%1006), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.48 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%986, %1007), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.66 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.48, %1005, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1010 : Tensor = prim::GetAttr[name="bias"](%997)
  %1011 : Tensor = prim::GetAttr[name="weight"](%997)
  %1012 : Float(4096:1, 4096:4096) = aten::t(%1011), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.49 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%986, %1012), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.68 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.49, %1010, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1015 : int = aten::size(%x.64, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1016 : int = aten::size(%x.64, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1017 : int[] = prim::ListConstruct(%1015, %1016, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.65 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.64, %1017), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1019 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.65, %1019), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1021 : int = aten::size(%x.66, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1022 : int = aten::size(%x.66, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1023 : int[] = prim::ListConstruct(%1021, %1022, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.67 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.66, %1023), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1025 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.67, %1025), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1027 : int = aten::size(%x.68, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1028 : int = aten::size(%x.68, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1029 : int[] = prim::ListConstruct(%1027, %1028, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.69 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.68, %1029), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1031 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.10 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.69, %1031), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1033 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.10, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.19 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %1033), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.20 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.19, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.70 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.71 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.70, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.71, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.10 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1040 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1041 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.10, %1040), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1042 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1041, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1043 : Float(4096:1, 4096:4096) = aten::t(%996), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1044 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1045 : Float(64:64, 64:1, 4096:4096) = aten::view(%1043, %1044), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1046 : Float(64:64, 64:1, 4096:4096) = aten::to(%1045, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.10 : Float(4096:1) = aten::to(%987, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1048 : Tensor[] = prim::ListConstruct(%1042, %1046), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1049 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1048), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.72 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1049, %b.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.10 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.72, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.73 : Float(17:53248, 13:4096, 4096:1) = aten::add(%986, %projected_context_layer.10, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1053 : Tensor = prim::GetAttr[name="bias"](%994)
  %1054 : Tensor = prim::GetAttr[name="weight"](%994)
  %1055 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.10 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.73, %1055, %1054, %1053, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1057 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.10, %b.10)
  %1058 : Float(17:53248, 13:4096, 4096:1), %1059 : Float(4096:1) = prim::TupleUnpack(%1057)
  %1060 : Tensor = prim::GetAttr[name="bias"](%992)
  %1061 : Tensor = prim::GetAttr[name="weight"](%992)
  %1062 : Float(4096:1, 16384:4096) = aten::t(%1061), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.50 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1058, %1062), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.70 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.50, %1060, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1065 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.70, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1066 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.70, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1067 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1066, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1068 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.70, %1067, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1069 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1068, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1070 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1069), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1071 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1070, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.74 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1065, %1071), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1073 : Tensor = prim::GetAttr[name="bias"](%991)
  %1074 : Tensor = prim::GetAttr[name="weight"](%991)
  %1075 : Float(16384:1, 4096:16384) = aten::t(%1074), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.51 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.74, %1075), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.10 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.51, %1073, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.75 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.10, %1058, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1079 : Tensor = prim::GetAttr[name="bias"](%990)
  %1080 : Tensor = prim::GetAttr[name="weight"](%990)
  %1081 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.76 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.75, %1081, %1080, %1079, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1083 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.76, %1059)
  %1084 : Float(17:53248, 13:4096, 4096:1), %1085 : Float(4096:1) = prim::TupleUnpack(%1083)
  %1086 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1084, %1085)
  %1087 : Float(17:53248, 13:4096, 4096:1), %1088 : Float(4096:1) = prim::TupleUnpack(%1086)
  %1089 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %1090 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%1089)
  %1091 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1090)
  %1092 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%1090)
  %1093 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%1090)
  %1094 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%1090)
  %1095 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%1094)
  %1096 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%1094)
  %1097 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1098 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%1094)
  %1099 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%1094)
  %1100 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%1094)
  %1101 : Tensor = prim::GetAttr[name="bias"](%1100)
  %1102 : Tensor = prim::GetAttr[name="weight"](%1100)
  %1103 : Float(4096:1, 4096:4096) = aten::t(%1102), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.52 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1087, %1103), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.71 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.52, %1101, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1106 : Tensor = prim::GetAttr[name="bias"](%1099)
  %1107 : Tensor = prim::GetAttr[name="weight"](%1099)
  %1108 : Float(4096:1, 4096:4096) = aten::t(%1107), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.53 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1087, %1108), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.73 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.53, %1106, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1111 : Tensor = prim::GetAttr[name="bias"](%1098)
  %1112 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1113 : Float(4096:1, 4096:4096) = aten::t(%1112), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.54 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1087, %1113), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.75 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.54, %1111, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1116 : int = aten::size(%x.71, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1117 : int = aten::size(%x.71, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1118 : int[] = prim::ListConstruct(%1116, %1117, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.72 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.71, %1118), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1120 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.72, %1120), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1122 : int = aten::size(%x.73, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1123 : int = aten::size(%x.73, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1124 : int[] = prim::ListConstruct(%1122, %1123, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.74 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.73, %1124), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1126 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.74, %1126), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1128 : int = aten::size(%x.75, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1129 : int = aten::size(%x.75, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1130 : int[] = prim::ListConstruct(%1128, %1129, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.76 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.75, %1130), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1132 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer.11 : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.76, %1132), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1134 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer.11, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.21 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1134), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.22 : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.21, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.77 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.78 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.77, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.78, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1141 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1142 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer.11, %1141), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1143 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1142, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1144 : Float(4096:1, 4096:4096) = aten::t(%1097), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1145 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1146 : Float(64:64, 64:1, 4096:4096) = aten::view(%1144, %1145), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1147 : Float(64:64, 64:1, 4096:4096) = aten::to(%1146, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b.11 : Float(4096:1) = aten::to(%1088, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1149 : Tensor[] = prim::ListConstruct(%1143, %1147), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1150 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1149), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.79 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1150, %b.11, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer.11 : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.79, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.80 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1087, %projected_context_layer.11, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1154 : Tensor = prim::GetAttr[name="bias"](%1095)
  %1155 : Tensor = prim::GetAttr[name="weight"](%1095)
  %1156 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor.11 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.80, %1156, %1155, %1154, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1158 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input_tensor.11, %b.11)
  %1159 : Float(17:53248, 13:4096, 4096:1), %1160 : Float(4096:1) = prim::TupleUnpack(%1158)
  %1161 : Tensor = prim::GetAttr[name="bias"](%1093)
  %1162 : Tensor = prim::GetAttr[name="weight"](%1093)
  %1163 : Float(4096:1, 16384:4096) = aten::t(%1162), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.55 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%1159, %1163), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x.77 : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.55, %1161, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1166 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x.77, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1167 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x.77, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1168 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1167, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1169 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x.77, %1168, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1170 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1169, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1171 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1170), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1172 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1171, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.81 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1166, %1172), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1174 : Tensor = prim::GetAttr[name="bias"](%1092)
  %1175 : Tensor = prim::GetAttr[name="weight"](%1092)
  %1176 : Float(16384:1, 4096:16384) = aten::t(%1175), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.56 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.81, %1176), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output.11 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.56, %1174, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.82 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output.11, %1159, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1180 : Tensor = prim::GetAttr[name="bias"](%1091)
  %1181 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1182 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.83 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.82, %1182, %1181, %1180, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1184 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%input.83, %1160)
  %1185 : Float(17:53248, 13:4096, 4096:1), %1186 : Float(4096:1) = prim::TupleUnpack(%1184)
  %1187 : (Float(17:53248, 13:4096, 4096:1), Float(4096:1)) = prim::TupleConstruct(%1185, %1186)
  %1188 : Float(17:53248, 13:4096, 4096:1), %1189 : Float(4096:1) = prim::TupleUnpack(%1187)
  %1190 : __torch__.torch.nn.modules.container.___torch_mangle_305.ModuleList = prim::GetAttr[name="albert_layers"](%70)
  %1191 : __torch__.transformers.modeling_albert.___torch_mangle_304.AlbertLayer = prim::GetAttr[name="0"](%1190)
  %1192 : __torch__.torch.nn.modules.normalization.___torch_mangle_292.LayerNorm = prim::GetAttr[name="full_layer_layer_norm"](%1191)
  %1193 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name="ffn_output"](%1191)
  %1194 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name="ffn"](%1191)
  %1195 : __torch__.transformers.modeling_albert.___torch_mangle_300.AlbertAttention = prim::GetAttr[name="attention"](%1191)
  %1196 : __torch__.torch.nn.modules.normalization.___torch_mangle_299.LayerNorm = prim::GetAttr[name="LayerNorm"](%1195)
  %1197 : __torch__.torch.nn.modules.linear.___torch_mangle_298.Linear = prim::GetAttr[name="dense"](%1195)
  %1198 : Tensor = prim::GetAttr[name="weight"](%1197)
  %1199 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name="value"](%1195)
  %1200 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name="key"](%1195)
  %1201 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name="query"](%1195)
  %1202 : Tensor = prim::GetAttr[name="bias"](%1201)
  %1203 : Tensor = prim::GetAttr[name="weight"](%1201)
  %1204 : Float(4096:1, 4096:4096) = aten::t(%1203), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %output.57 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1188, %1204), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1676:0
  %x.78 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.57, %1202, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query # torch/nn/functional.py:1678:0
  %1207 : Tensor = prim::GetAttr[name="bias"](%1200)
  %1208 : Tensor = prim::GetAttr[name="weight"](%1200)
  %1209 : Float(4096:1, 4096:4096) = aten::t(%1208), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %output.58 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1188, %1209), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1676:0
  %x.80 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.58, %1207, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key # torch/nn/functional.py:1678:0
  %1212 : Tensor = prim::GetAttr[name="bias"](%1199)
  %1213 : Tensor = prim::GetAttr[name="weight"](%1199)
  %1214 : Float(4096:1, 4096:4096) = aten::t(%1213), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %output.59 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%1188, %1214), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1676:0
  %x.82 : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.59, %1212, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value # torch/nn/functional.py:1678:0
  %1217 : int = aten::size(%x.78, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1218 : int = aten::size(%x.78, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1219 : int[] = prim::ListConstruct(%1217, %1218, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.79 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.78, %1219), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1221 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %query_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.79, %1221), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1223 : int = aten::size(%x.80, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1224 : int = aten::size(%x.80, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1225 : int[] = prim::ListConstruct(%1223, %1224, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.81 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.80, %1225), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1227 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %key_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.81, %1227), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1229 : int = aten::size(%x.82, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1230 : int = aten::size(%x.82, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:271:0
  %1231 : int[] = prim::ListConstruct(%1229, %1230, %15, %15), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %x.83 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::view(%x.82, %1231), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:272:0
  %1233 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %value_layer : Float(17:53248, 64:64, 13:4096, 64:1) = aten::permute(%x.83, %1233), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:273:0
  %1235 : Float(17:53248, 64:64, 64:1, 13:4096) = aten::transpose(%key_layer, %22, %13), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores.23 : Float(17:10816, 64:169, 13:13, 13:1) = aten::matmul(%query_layer, %1235), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:303:0
  %attention_scores : Float(17:10816, 64:169, 13:13, 13:1) = aten::div(%attention_scores.23, %12), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:304:0
  %input.84 : Float(17:10816, 64:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:307:0
  %input.85 : Float(17:10816, 64:169, 13:13, 13:1) = aten::softmax(%input.84, %22, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/nn/functional.py:1498:0
  %attention_probs : Float(17:10816, 64:169, 13:13, 13:1) = aten::dropout(%input.85, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.attention_dropout # torch/nn/functional.py:973:0
  %context_layer : Float(17:53248, 64:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:320:0
  %1242 : int[] = prim::ListConstruct(%36, %31, %35, %14), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1243 : Float(17:53248, 13:64, 64:832, 64:1) = aten::permute(%context_layer, %1242), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1244 : Float(17:53248, 13:4096, 64:64, 64:1) = aten::contiguous(%1243, %36), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:322:0
  %1245 : Float(4096:1, 4096:4096) = aten::t(%1198), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1246 : int[] = prim::ListConstruct(%15, %15, %11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1247 : Float(64:64, 64:1, 4096:4096) = aten::view(%1245, %1246), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %1248 : Float(64:64, 64:1, 4096:4096) = aten::to(%1247, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:326:0
  %b : Float(4096:1) = aten::to(%1189, %30, %32, %32, %29), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:330:0
  %1250 : Tensor[] = prim::ListConstruct(%1244, %1248), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention
  %1251 : Float(17:53248, 13:4096, 4096:1) = aten::einsum(%10, %1250), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # torch/functional.py:327:0
  %input.86 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1251, %b, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:332:0
  %projected_context_layer : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.86, %26, %32), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.output_dropout # torch/nn/functional.py:973:0
  %input.87 : Float(17:53248, 13:4096, 4096:1) = aten::add(%1188, %projected_context_layer, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention # transformers/modeling_albert.py:334:0
  %1255 : Tensor = prim::GetAttr[name="bias"](%1196)
  %1256 : Tensor = prim::GetAttr[name="weight"](%1196)
  %1257 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm
  %input_tensor : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.87, %1257, %1256, %1255, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm # torch/nn/functional.py:2048:0
  %1259 : Tensor = prim::GetAttr[name="bias"](%1194)
  %1260 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1261 : Float(4096:1, 16384:4096) = aten::t(%1260), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %output.60 : Float(17:212992, 13:16384, 16384:1) = aten::matmul(%input_tensor, %1261), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1676:0
  %x : Float(17:212992, 13:16384, 16384:1) = aten::add_(%output.60, %1259, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn # torch/nn/functional.py:1678:0
  %1264 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%x, %16), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1265 : Float(17:212992, 13:16384, 16384:1) = aten::pow(%x, %17), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1266 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1265, %18), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1267 : Float(17:212992, 13:16384, 16384:1) = aten::add(%x, %1266, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1268 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1267, %19), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1269 : Float(17:212992, 13:16384, 16384:1) = aten::tanh(%1268), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1270 : Float(17:212992, 13:16384, 16384:1) = aten::add(%1269, %20, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %input.88 : Float(17:212992, 13:16384, 16384:1) = aten::mul(%1264, %1270), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/activations.py:30:0
  %1272 : Tensor = prim::GetAttr[name="bias"](%1193)
  %1273 : Tensor = prim::GetAttr[name="weight"](%1193)
  %1274 : Float(16384:1, 4096:16384) = aten::t(%1273), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %output.61 : Float(17:53248, 13:4096, 4096:1) = aten::matmul(%input.88, %1274), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1676:0
  %ffn_output : Float(17:53248, 13:4096, 4096:1) = aten::add_(%output.61, %1272, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output # torch/nn/functional.py:1678:0
  %input.89 : Float(17:53248, 13:4096, 4096:1) = aten::add(%ffn_output, %input_tensor, %35), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0 # transformers/modeling_albert.py:363:0
  %1278 : Tensor = prim::GetAttr[name="bias"](%1192)
  %1279 : Tensor = prim::GetAttr[name="weight"](%1192)
  %1280 : int[] = prim::ListConstruct(%11), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm
  %input.90 : Float(17:53248, 13:4096, 4096:1) = aten::layer_norm(%input.89, %1280, %1279, %1278, %24, %23), scope: __module.albert/__module.albert.encoder/__module.albert.encoder.albert_layer_groups.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0/__module.albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm # torch/nn/functional.py:2048:0
  %1282 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1283 : float = prim::Constant[value=0.](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(17:53248, 13:4096, 4096:1) = aten::dropout(%input.90, %1283, %1282), scope: __module.dropout # torch/nn/functional.py:973:0
  %1285 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1678:0
  %1286 : Tensor = prim::GetAttr[name="bias"](%3)
  %1287 : Tensor = prim::GetAttr[name="weight"](%3)
  %1288 : Float(4096:1, 2:4096) = aten::t(%1287), scope: __module.classifier # torch/nn/functional.py:1676:0
  %output : Float(17:26, 13:2, 2:1) = aten::matmul(%input, %1288), scope: __module.classifier # torch/nn/functional.py:1676:0
  %1290 : Float(17:26, 13:2, 2:1) = aten::add_(%output, %1286, %1285), scope: __module.classifier # torch/nn/functional.py:1678:0
  %9 : (Float(17:26, 13:2, 2:1)) = prim::TupleConstruct(%1290)
  return (%9)
