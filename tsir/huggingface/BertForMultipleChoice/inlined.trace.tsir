graph(%self.1 : __torch__.transformers.modeling_bert.BertForMultipleChoice,
      %input_ids.1 : Long(17:91, 7:13, 13:1),
      %attention_mask.1 : Long(17:91, 7:13, 13:1)):
  %3 : __torch__.torch.nn.modules.linear.___torch_mangle_3305.Linear = prim::GetAttr[name="classifier"](%self.1)
  %4 : __torch__.torch.nn.modules.dropout.___torch_mangle_3304.Dropout = prim::GetAttr[name="dropout"](%self.1)
  %5 : __torch__.transformers.modeling_bert.___torch_mangle_3303.BertModel = prim::GetAttr[name="bert"](%self.1)
  %6 : int = prim::Constant[value=1]() # transformers/modeling_bert.py:1416:0
  %7 : int = aten::size(%input_ids.1, %6) # transformers/modeling_bert.py:1416:0
  %num_choices : Long() = prim::NumToTensor(%7)
  %9 : int = aten::Int(%num_choices)
  %10 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1418:0
  %11 : int = aten::size(%input_ids.1, %10) # transformers/modeling_bert.py:1418:0
  %12 : Long() = prim::NumToTensor(%11)
  %13 : int = aten::Int(%12)
  %14 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1418:0
  %15 : int[] = prim::ListConstruct(%14, %13)
  %input_ids : Long(119:13, 13:1) = aten::view(%input_ids.1, %15) # transformers/modeling_bert.py:1418:0
  %17 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1419:0
  %18 : int = aten::size(%attention_mask.1, %17) # transformers/modeling_bert.py:1419:0
  %19 : Long() = prim::NumToTensor(%18)
  %20 : int = aten::Int(%19)
  %21 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1419:0
  %22 : int[] = prim::ListConstruct(%21, %20)
  %attention_mask.2 : Long(119:13, 13:1) = aten::view(%attention_mask.1, %22) # transformers/modeling_bert.py:1419:0
  %31 : Double() = prim::Constant[value={8}](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %32 : int = prim::Constant[value=-2](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %33 : int = prim::Constant[value=64](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %34 : int = prim::Constant[value=12](), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %35 : int = prim::Constant[value=-1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %36 : bool = prim::Constant[value=1](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %37 : float = prim::Constant[value=9.9999999999999998e-13](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %38 : int = prim::Constant[value=768](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %39 : float = prim::Constant[value=0.10000000000000001](), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %40 : Double() = prim::Constant[value={-10000}](), scope: __module.bert # transformers/modeling_utils.py:258:0
  %41 : float = prim::Constant[value=1.](), scope: __module.bert # torch/tensor.py:396:0
  %42 : None = prim::Constant(), scope: __module.bert
  %43 : int = prim::Constant[value=6](), scope: __module.bert # transformers/modeling_utils.py:257:0
  %44 : int = prim::Constant[value=3](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %45 : int = prim::Constant[value=2](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %46 : int = prim::Constant[value=9223372036854775807](), scope: __module.bert # transformers/modeling_utils.py:244:0
  %47 : bool = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %48 : Device = prim::Constant[value="cpu"](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %49 : int = prim::Constant[value=4](), scope: __module.bert # transformers/modeling_bert.py:806:0
  %50 : int = prim::Constant[value=1](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %51 : int = prim::Constant[value=0](), scope: __module.bert # transformers/modeling_bert.py:795:0
  %52 : __torch__.transformers.modeling_bert.BertPooler = prim::GetAttr[name="pooler"](%5)
  %53 : __torch__.transformers.modeling_bert.___torch_mangle_3300.BertEncoder = prim::GetAttr[name="encoder"](%5)
  %54 : __torch__.transformers.modeling_bert.___torch_mangle_3094.BertEmbeddings = prim::GetAttr[name="embeddings"](%5)
  %55 : int = aten::size(%input_ids, %51), scope: __module.bert # transformers/modeling_bert.py:795:0
  %56 : int = aten::size(%input_ids, %50), scope: __module.bert # transformers/modeling_bert.py:795:0
  %57 : int[] = prim::ListConstruct(%55, %56), scope: __module.bert
  %input.2 : Long(119:13, 13:1) = aten::zeros(%57, %49, %51, %48, %47), scope: __module.bert # transformers/modeling_bert.py:806:0
  %59 : Long(119:13, 13:1) = aten::slice(%attention_mask.2, %51, %51, %46, %50), scope: __module.bert # transformers/modeling_utils.py:244:0
  %60 : Long(119:13, 1:13, 13:1) = aten::unsqueeze(%59, %50), scope: __module.bert # transformers/modeling_utils.py:244:0
  %61 : Long(119:13, 1:13, 1:13, 13:1) = aten::unsqueeze(%60, %45), scope: __module.bert # transformers/modeling_utils.py:244:0
  %extended_attention_mask : Long(119:13, 1:13, 1:13, 13:1) = aten::slice(%61, %44, %51, %46, %50), scope: __module.bert # transformers/modeling_utils.py:244:0
  %63 : Float(119:13, 1:13, 1:13, 13:1) = aten::to(%extended_attention_mask, %43, %47, %47, %42), scope: __module.bert # transformers/modeling_utils.py:257:0
  %64 : Float(119:13, 1:13, 1:13, 13:1) = aten::rsub(%63, %41, %50), scope: __module.bert # torch/tensor.py:396:0
  %attention_mask : Float(119:13, 1:13, 1:13, 13:1) = aten::mul(%64, %40), scope: __module.bert # transformers/modeling_utils.py:258:0
  %66 : __torch__.torch.nn.modules.normalization.___torch_mangle_3092.LayerNorm = prim::GetAttr[name="LayerNorm"](%54)
  %67 : __torch__.torch.nn.modules.sparse.___torch_mangle_3091.Embedding = prim::GetAttr[name="token_type_embeddings"](%54)
  %68 : __torch__.torch.nn.modules.sparse.___torch_mangle_3090.Embedding = prim::GetAttr[name="position_embeddings"](%54)
  %69 : __torch__.torch.nn.modules.sparse.___torch_mangle_3089.Embedding = prim::GetAttr[name="word_embeddings"](%54)
  %70 : Tensor = prim::GetAttr[name="position_ids"](%54)
  %71 : int = aten::size(%input_ids, %50), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:184:0
  %72 : Long(1:512, 512:1) = aten::slice(%70, %51, %51, %46, %50), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %input.1 : Long(1:512, 13:1) = aten::slice(%72, %50, %51, %71, %50), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:191:0
  %74 : Tensor = prim::GetAttr[name="weight"](%69)
  %inputs_embeds : Float(119:9984, 13:768, 768:1) = aten::embedding(%74, %input_ids, %51, %47, %47), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.word_embeddings # torch/nn/functional.py:1814:0
  %76 : Tensor = prim::GetAttr[name="weight"](%68)
  %position_embeddings : Float(1:9984, 13:768, 768:1) = aten::embedding(%76, %input.1, %35, %47, %47), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.position_embeddings # torch/nn/functional.py:1814:0
  %78 : Tensor = prim::GetAttr[name="weight"](%67)
  %token_type_embeddings : Float(119:9984, 13:768, 768:1) = aten::embedding(%78, %input.2, %35, %47, %47), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.token_type_embeddings # torch/nn/functional.py:1814:0
  %80 : Float(119:9984, 13:768, 768:1) = aten::add(%inputs_embeds, %position_embeddings, %50), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %input.3 : Float(119:9984, 13:768, 768:1) = aten::add(%80, %token_type_embeddings, %50), scope: __module.bert/__module.bert.embeddings # transformers/modeling_bert.py:201:0
  %82 : Tensor = prim::GetAttr[name="bias"](%66)
  %83 : Tensor = prim::GetAttr[name="weight"](%66)
  %84 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm
  %input.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.3, %84, %83, %82, %37, %36), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.LayerNorm # torch/nn/functional.py:2048:0
  %input.5 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.4, %39, %47), scope: __module.bert/__module.bert.embeddings/__module.bert.embeddings.dropout # torch/nn/functional.py:973:0
  %87 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %88 : __torch__.transformers.modeling_bert.___torch_mangle_3298.BertLayer = prim::GetAttr[name="11"](%87)
  %89 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %90 : __torch__.transformers.modeling_bert.___torch_mangle_3281.BertLayer = prim::GetAttr[name="10"](%89)
  %91 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %92 : __torch__.transformers.modeling_bert.___torch_mangle_3264.BertLayer = prim::GetAttr[name="9"](%91)
  %93 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %94 : __torch__.transformers.modeling_bert.___torch_mangle_3247.BertLayer = prim::GetAttr[name="8"](%93)
  %95 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %96 : __torch__.transformers.modeling_bert.___torch_mangle_3230.BertLayer = prim::GetAttr[name="7"](%95)
  %97 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %98 : __torch__.transformers.modeling_bert.___torch_mangle_3213.BertLayer = prim::GetAttr[name="6"](%97)
  %99 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %100 : __torch__.transformers.modeling_bert.___torch_mangle_3196.BertLayer = prim::GetAttr[name="5"](%99)
  %101 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %102 : __torch__.transformers.modeling_bert.___torch_mangle_3179.BertLayer = prim::GetAttr[name="4"](%101)
  %103 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %104 : __torch__.transformers.modeling_bert.___torch_mangle_3162.BertLayer = prim::GetAttr[name="3"](%103)
  %105 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %106 : __torch__.transformers.modeling_bert.___torch_mangle_3145.BertLayer = prim::GetAttr[name="2"](%105)
  %107 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %108 : __torch__.transformers.modeling_bert.___torch_mangle_3128.BertLayer = prim::GetAttr[name="1"](%107)
  %109 : __torch__.torch.nn.modules.container.___torch_mangle_3299.ModuleList = prim::GetAttr[name="layer"](%53)
  %110 : __torch__.transformers.modeling_bert.___torch_mangle_3111.BertLayer = prim::GetAttr[name="0"](%109)
  %111 : __torch__.transformers.modeling_bert.___torch_mangle_3110.BertOutput = prim::GetAttr[name="output"](%110)
  %112 : __torch__.transformers.modeling_bert.___torch_mangle_3106.BertIntermediate = prim::GetAttr[name="intermediate"](%110)
  %113 : __torch__.transformers.modeling_bert.___torch_mangle_3104.BertAttention = prim::GetAttr[name="attention"](%110)
  %114 : __torch__.transformers.modeling_bert.___torch_mangle_3103.BertSelfOutput = prim::GetAttr[name="output"](%113)
  %115 : __torch__.transformers.modeling_bert.___torch_mangle_3099.BertSelfAttention = prim::GetAttr[name="self"](%113)
  %116 : __torch__.torch.nn.modules.linear.___torch_mangle_3097.Linear = prim::GetAttr[name="value"](%115)
  %117 : __torch__.torch.nn.modules.linear.___torch_mangle_3096.Linear = prim::GetAttr[name="key"](%115)
  %118 : __torch__.torch.nn.modules.linear.___torch_mangle_3095.Linear = prim::GetAttr[name="query"](%115)
  %119 : Tensor = prim::GetAttr[name="bias"](%118)
  %120 : Tensor = prim::GetAttr[name="weight"](%118)
  %121 : Float(768:1, 768:768) = aten::t(%120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %output.1 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1676:0
  %x.1 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.1, %119, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.query # torch/nn/functional.py:1678:0
  %124 : Tensor = prim::GetAttr[name="bias"](%117)
  %125 : Tensor = prim::GetAttr[name="weight"](%117)
  %126 : Float(768:1, 768:768) = aten::t(%125), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %output.2 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %126), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1676:0
  %x.3 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.2, %124, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.key # torch/nn/functional.py:1678:0
  %129 : Tensor = prim::GetAttr[name="bias"](%116)
  %130 : Tensor = prim::GetAttr[name="weight"](%116)
  %131 : Float(768:1, 768:768) = aten::t(%130), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %output.3 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.5, %131), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1676:0
  %x.5 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.3, %129, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.value # torch/nn/functional.py:1678:0
  %134 : int = aten::size(%x.1, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %135 : int = aten::size(%x.1, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %136 : int[] = prim::ListConstruct(%134, %135, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.2 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.1, %136), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %138 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %query_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.2, %138), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %140 : int = aten::size(%x.3, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %141 : int = aten::size(%x.3, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %142 : int[] = prim::ListConstruct(%140, %141, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.4 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.3, %142), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %144 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %key_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.4, %144), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %146 : int = aten::size(%x.5, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %147 : int = aten::size(%x.5, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:227:0
  %148 : int[] = prim::ListConstruct(%146, %147, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %x.6 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.5, %148), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:228:0
  %150 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %value_layer.1 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.6, %150), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:229:0
  %152 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.1, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.1, %152), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.1, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:259:0
  %input.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.2, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:262:0
  %input.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.6, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.1 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.7, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self/__module.bert.encoder.layer.0.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.1 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.1, %value_layer.1), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:275:0
  %159 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %160 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.1, %159), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.2 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%160, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:277:0
  %162 : int = aten::size(%context_layer.2, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %163 : int = aten::size(%context_layer.2, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:278:0
  %164 : int[] = prim::ListConstruct(%162, %163, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self
  %input.8 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.2, %164), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.self # transformers/modeling_bert.py:279:0
  %166 : __torch__.torch.nn.modules.normalization.___torch_mangle_3101.LayerNorm = prim::GetAttr[name="LayerNorm"](%114)
  %167 : __torch__.torch.nn.modules.linear.___torch_mangle_3100.Linear = prim::GetAttr[name="dense"](%114)
  %168 : Tensor = prim::GetAttr[name="bias"](%167)
  %169 : Tensor = prim::GetAttr[name="weight"](%167)
  %170 : Float(768:1, 768:768) = aten::t(%169), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %output.4 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.8, %170), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1676:0
  %input.9 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.4, %168, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.1 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.9, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.dropout # torch/nn/functional.py:973:0
  %input.10 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.1, %input.5, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output # transformers/modeling_bert.py:295:0
  %175 : Tensor = prim::GetAttr[name="bias"](%166)
  %176 : Tensor = prim::GetAttr[name="weight"](%166)
  %177 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm
  %input_tensor.1 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.10, %177, %176, %175, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.attention/__module.bert.encoder.layer.0.attention.output/__module.bert.encoder.layer.0.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %179 : __torch__.torch.nn.modules.linear.___torch_mangle_3105.Linear = prim::GetAttr[name="dense"](%112)
  %180 : Tensor = prim::GetAttr[name="bias"](%179)
  %181 : Tensor = prim::GetAttr[name="weight"](%179)
  %182 : Float(768:1, 3072:768) = aten::t(%181), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %output.5 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.1, %182), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1676:0
  %input.11 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.5, %180, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate/__module.bert.encoder.layer.0.intermediate.dense # torch/nn/functional.py:1678:0
  %input.12 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.intermediate # torch/nn/functional.py:1369:0
  %186 : __torch__.torch.nn.modules.normalization.___torch_mangle_3108.LayerNorm = prim::GetAttr[name="LayerNorm"](%111)
  %187 : __torch__.torch.nn.modules.linear.___torch_mangle_3107.Linear = prim::GetAttr[name="dense"](%111)
  %188 : Tensor = prim::GetAttr[name="bias"](%187)
  %189 : Tensor = prim::GetAttr[name="weight"](%187)
  %190 : Float(3072:1, 768:3072) = aten::t(%189), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %output.6 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.12, %190), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1676:0
  %input.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.6, %188, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.2 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.13, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.dropout # torch/nn/functional.py:973:0
  %input.14 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.2, %input_tensor.1, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output # transformers/modeling_bert.py:371:0
  %195 : Tensor = prim::GetAttr[name="bias"](%186)
  %196 : Tensor = prim::GetAttr[name="weight"](%186)
  %197 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm
  %input.15 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.14, %197, %196, %195, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.0/__module.bert.encoder.layer.0.output/__module.bert.encoder.layer.0.output.LayerNorm # torch/nn/functional.py:2048:0
  %199 : __torch__.transformers.modeling_bert.___torch_mangle_3127.BertOutput = prim::GetAttr[name="output"](%108)
  %200 : __torch__.transformers.modeling_bert.___torch_mangle_3123.BertIntermediate = prim::GetAttr[name="intermediate"](%108)
  %201 : __torch__.transformers.modeling_bert.___torch_mangle_3121.BertAttention = prim::GetAttr[name="attention"](%108)
  %202 : __torch__.transformers.modeling_bert.___torch_mangle_3120.BertSelfOutput = prim::GetAttr[name="output"](%201)
  %203 : __torch__.transformers.modeling_bert.___torch_mangle_3116.BertSelfAttention = prim::GetAttr[name="self"](%201)
  %204 : __torch__.torch.nn.modules.linear.___torch_mangle_3114.Linear = prim::GetAttr[name="value"](%203)
  %205 : __torch__.torch.nn.modules.linear.___torch_mangle_3113.Linear = prim::GetAttr[name="key"](%203)
  %206 : __torch__.torch.nn.modules.linear.___torch_mangle_3112.Linear = prim::GetAttr[name="query"](%203)
  %207 : Tensor = prim::GetAttr[name="bias"](%206)
  %208 : Tensor = prim::GetAttr[name="weight"](%206)
  %209 : Float(768:1, 768:768) = aten::t(%208), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %output.7 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %209), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1676:0
  %x.7 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.7, %207, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.query # torch/nn/functional.py:1678:0
  %212 : Tensor = prim::GetAttr[name="bias"](%205)
  %213 : Tensor = prim::GetAttr[name="weight"](%205)
  %214 : Float(768:1, 768:768) = aten::t(%213), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %output.8 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %214), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1676:0
  %x.9 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.8, %212, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.key # torch/nn/functional.py:1678:0
  %217 : Tensor = prim::GetAttr[name="bias"](%204)
  %218 : Tensor = prim::GetAttr[name="weight"](%204)
  %219 : Float(768:1, 768:768) = aten::t(%218), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %output.9 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.15, %219), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1676:0
  %x.11 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.9, %217, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.value # torch/nn/functional.py:1678:0
  %222 : int = aten::size(%x.7, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %223 : int = aten::size(%x.7, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %224 : int[] = prim::ListConstruct(%222, %223, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.8 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.7, %224), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %226 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %query_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.8, %226), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %228 : int = aten::size(%x.9, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %229 : int = aten::size(%x.9, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %230 : int[] = prim::ListConstruct(%228, %229, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.10 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.9, %230), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %232 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %key_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.10, %232), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %234 : int = aten::size(%x.11, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %235 : int = aten::size(%x.11, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:227:0
  %236 : int[] = prim::ListConstruct(%234, %235, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %x.12 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.11, %236), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:228:0
  %238 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %value_layer.2 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.12, %238), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:229:0
  %240 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.2, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.2, %240), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.3, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:259:0
  %input.16 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.4, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:262:0
  %input.17 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.16, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.2 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.17, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self/__module.bert.encoder.layer.1.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.3 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.2, %value_layer.2), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:275:0
  %247 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %248 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.3, %247), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.4 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%248, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:277:0
  %250 : int = aten::size(%context_layer.4, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %251 : int = aten::size(%context_layer.4, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:278:0
  %252 : int[] = prim::ListConstruct(%250, %251, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self
  %input.18 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.4, %252), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.self # transformers/modeling_bert.py:279:0
  %254 : __torch__.torch.nn.modules.normalization.___torch_mangle_3118.LayerNorm = prim::GetAttr[name="LayerNorm"](%202)
  %255 : __torch__.torch.nn.modules.linear.___torch_mangle_3117.Linear = prim::GetAttr[name="dense"](%202)
  %256 : Tensor = prim::GetAttr[name="bias"](%255)
  %257 : Tensor = prim::GetAttr[name="weight"](%255)
  %258 : Float(768:1, 768:768) = aten::t(%257), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %output.10 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.18, %258), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1676:0
  %input.19 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.10, %256, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.3 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.19, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.dropout # torch/nn/functional.py:973:0
  %input.20 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.3, %input.15, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output # transformers/modeling_bert.py:295:0
  %263 : Tensor = prim::GetAttr[name="bias"](%254)
  %264 : Tensor = prim::GetAttr[name="weight"](%254)
  %265 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm
  %input_tensor.2 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.20, %265, %264, %263, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.attention/__module.bert.encoder.layer.1.attention.output/__module.bert.encoder.layer.1.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %267 : __torch__.torch.nn.modules.linear.___torch_mangle_3122.Linear = prim::GetAttr[name="dense"](%200)
  %268 : Tensor = prim::GetAttr[name="bias"](%267)
  %269 : Tensor = prim::GetAttr[name="weight"](%267)
  %270 : Float(768:1, 3072:768) = aten::t(%269), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %output.11 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.2, %270), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1676:0
  %input.21 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.11, %268, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate/__module.bert.encoder.layer.1.intermediate.dense # torch/nn/functional.py:1678:0
  %input.22 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.21), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.intermediate # torch/nn/functional.py:1369:0
  %274 : __torch__.torch.nn.modules.normalization.___torch_mangle_3125.LayerNorm = prim::GetAttr[name="LayerNorm"](%199)
  %275 : __torch__.torch.nn.modules.linear.___torch_mangle_3124.Linear = prim::GetAttr[name="dense"](%199)
  %276 : Tensor = prim::GetAttr[name="bias"](%275)
  %277 : Tensor = prim::GetAttr[name="weight"](%275)
  %278 : Float(3072:1, 768:3072) = aten::t(%277), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %output.12 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.22, %278), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1676:0
  %input.23 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.12, %276, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.4 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.23, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.dropout # torch/nn/functional.py:973:0
  %input.24 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.4, %input_tensor.2, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output # transformers/modeling_bert.py:371:0
  %283 : Tensor = prim::GetAttr[name="bias"](%274)
  %284 : Tensor = prim::GetAttr[name="weight"](%274)
  %285 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm
  %input.25 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.24, %285, %284, %283, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.1/__module.bert.encoder.layer.1.output/__module.bert.encoder.layer.1.output.LayerNorm # torch/nn/functional.py:2048:0
  %287 : __torch__.transformers.modeling_bert.___torch_mangle_3144.BertOutput = prim::GetAttr[name="output"](%106)
  %288 : __torch__.transformers.modeling_bert.___torch_mangle_3140.BertIntermediate = prim::GetAttr[name="intermediate"](%106)
  %289 : __torch__.transformers.modeling_bert.___torch_mangle_3138.BertAttention = prim::GetAttr[name="attention"](%106)
  %290 : __torch__.transformers.modeling_bert.___torch_mangle_3137.BertSelfOutput = prim::GetAttr[name="output"](%289)
  %291 : __torch__.transformers.modeling_bert.___torch_mangle_3133.BertSelfAttention = prim::GetAttr[name="self"](%289)
  %292 : __torch__.torch.nn.modules.linear.___torch_mangle_3131.Linear = prim::GetAttr[name="value"](%291)
  %293 : __torch__.torch.nn.modules.linear.___torch_mangle_3130.Linear = prim::GetAttr[name="key"](%291)
  %294 : __torch__.torch.nn.modules.linear.___torch_mangle_3129.Linear = prim::GetAttr[name="query"](%291)
  %295 : Tensor = prim::GetAttr[name="bias"](%294)
  %296 : Tensor = prim::GetAttr[name="weight"](%294)
  %297 : Float(768:1, 768:768) = aten::t(%296), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %output.13 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %297), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1676:0
  %x.13 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.13, %295, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.query # torch/nn/functional.py:1678:0
  %300 : Tensor = prim::GetAttr[name="bias"](%293)
  %301 : Tensor = prim::GetAttr[name="weight"](%293)
  %302 : Float(768:1, 768:768) = aten::t(%301), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %output.14 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %302), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1676:0
  %x.15 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.14, %300, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.key # torch/nn/functional.py:1678:0
  %305 : Tensor = prim::GetAttr[name="bias"](%292)
  %306 : Tensor = prim::GetAttr[name="weight"](%292)
  %307 : Float(768:1, 768:768) = aten::t(%306), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %output.15 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.25, %307), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1676:0
  %x.17 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.15, %305, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.value # torch/nn/functional.py:1678:0
  %310 : int = aten::size(%x.13, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %311 : int = aten::size(%x.13, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %312 : int[] = prim::ListConstruct(%310, %311, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.14 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.13, %312), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %314 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %query_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.14, %314), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %316 : int = aten::size(%x.15, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %317 : int = aten::size(%x.15, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %318 : int[] = prim::ListConstruct(%316, %317, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.16 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.15, %318), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %320 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %key_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.16, %320), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %322 : int = aten::size(%x.17, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %323 : int = aten::size(%x.17, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:227:0
  %324 : int[] = prim::ListConstruct(%322, %323, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %x.18 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.17, %324), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:228:0
  %326 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %value_layer.3 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.18, %326), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:229:0
  %328 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.3, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.3, %328), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.5, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:259:0
  %input.26 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.6, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:262:0
  %input.27 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.26, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.3 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.27, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self/__module.bert.encoder.layer.2.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.5 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.3, %value_layer.3), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:275:0
  %335 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %336 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.5, %335), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.6 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%336, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:277:0
  %338 : int = aten::size(%context_layer.6, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %339 : int = aten::size(%context_layer.6, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:278:0
  %340 : int[] = prim::ListConstruct(%338, %339, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self
  %input.28 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.6, %340), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.self # transformers/modeling_bert.py:279:0
  %342 : __torch__.torch.nn.modules.normalization.___torch_mangle_3135.LayerNorm = prim::GetAttr[name="LayerNorm"](%290)
  %343 : __torch__.torch.nn.modules.linear.___torch_mangle_3134.Linear = prim::GetAttr[name="dense"](%290)
  %344 : Tensor = prim::GetAttr[name="bias"](%343)
  %345 : Tensor = prim::GetAttr[name="weight"](%343)
  %346 : Float(768:1, 768:768) = aten::t(%345), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %output.16 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.28, %346), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1676:0
  %input.29 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.16, %344, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.5 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.29, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.dropout # torch/nn/functional.py:973:0
  %input.30 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.5, %input.25, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output # transformers/modeling_bert.py:295:0
  %351 : Tensor = prim::GetAttr[name="bias"](%342)
  %352 : Tensor = prim::GetAttr[name="weight"](%342)
  %353 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm
  %input_tensor.3 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.30, %353, %352, %351, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.attention/__module.bert.encoder.layer.2.attention.output/__module.bert.encoder.layer.2.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %355 : __torch__.torch.nn.modules.linear.___torch_mangle_3139.Linear = prim::GetAttr[name="dense"](%288)
  %356 : Tensor = prim::GetAttr[name="bias"](%355)
  %357 : Tensor = prim::GetAttr[name="weight"](%355)
  %358 : Float(768:1, 3072:768) = aten::t(%357), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %output.17 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.3, %358), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1676:0
  %input.31 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.17, %356, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate/__module.bert.encoder.layer.2.intermediate.dense # torch/nn/functional.py:1678:0
  %input.32 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.intermediate # torch/nn/functional.py:1369:0
  %362 : __torch__.torch.nn.modules.normalization.___torch_mangle_3142.LayerNorm = prim::GetAttr[name="LayerNorm"](%287)
  %363 : __torch__.torch.nn.modules.linear.___torch_mangle_3141.Linear = prim::GetAttr[name="dense"](%287)
  %364 : Tensor = prim::GetAttr[name="bias"](%363)
  %365 : Tensor = prim::GetAttr[name="weight"](%363)
  %366 : Float(3072:1, 768:3072) = aten::t(%365), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %output.18 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.32, %366), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1676:0
  %input.33 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.18, %364, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.6 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.33, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.dropout # torch/nn/functional.py:973:0
  %input.34 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.6, %input_tensor.3, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output # transformers/modeling_bert.py:371:0
  %371 : Tensor = prim::GetAttr[name="bias"](%362)
  %372 : Tensor = prim::GetAttr[name="weight"](%362)
  %373 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm
  %input.35 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.34, %373, %372, %371, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.2/__module.bert.encoder.layer.2.output/__module.bert.encoder.layer.2.output.LayerNorm # torch/nn/functional.py:2048:0
  %375 : __torch__.transformers.modeling_bert.___torch_mangle_3161.BertOutput = prim::GetAttr[name="output"](%104)
  %376 : __torch__.transformers.modeling_bert.___torch_mangle_3157.BertIntermediate = prim::GetAttr[name="intermediate"](%104)
  %377 : __torch__.transformers.modeling_bert.___torch_mangle_3155.BertAttention = prim::GetAttr[name="attention"](%104)
  %378 : __torch__.transformers.modeling_bert.___torch_mangle_3154.BertSelfOutput = prim::GetAttr[name="output"](%377)
  %379 : __torch__.transformers.modeling_bert.___torch_mangle_3150.BertSelfAttention = prim::GetAttr[name="self"](%377)
  %380 : __torch__.torch.nn.modules.linear.___torch_mangle_3148.Linear = prim::GetAttr[name="value"](%379)
  %381 : __torch__.torch.nn.modules.linear.___torch_mangle_3147.Linear = prim::GetAttr[name="key"](%379)
  %382 : __torch__.torch.nn.modules.linear.___torch_mangle_3146.Linear = prim::GetAttr[name="query"](%379)
  %383 : Tensor = prim::GetAttr[name="bias"](%382)
  %384 : Tensor = prim::GetAttr[name="weight"](%382)
  %385 : Float(768:1, 768:768) = aten::t(%384), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %output.19 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %385), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1676:0
  %x.19 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.19, %383, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.query # torch/nn/functional.py:1678:0
  %388 : Tensor = prim::GetAttr[name="bias"](%381)
  %389 : Tensor = prim::GetAttr[name="weight"](%381)
  %390 : Float(768:1, 768:768) = aten::t(%389), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %output.20 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %390), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1676:0
  %x.21 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.20, %388, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.key # torch/nn/functional.py:1678:0
  %393 : Tensor = prim::GetAttr[name="bias"](%380)
  %394 : Tensor = prim::GetAttr[name="weight"](%380)
  %395 : Float(768:1, 768:768) = aten::t(%394), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %output.21 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.35, %395), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1676:0
  %x.23 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.21, %393, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.value # torch/nn/functional.py:1678:0
  %398 : int = aten::size(%x.19, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %399 : int = aten::size(%x.19, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %400 : int[] = prim::ListConstruct(%398, %399, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.20 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.19, %400), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %402 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %query_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.20, %402), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %404 : int = aten::size(%x.21, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %405 : int = aten::size(%x.21, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %406 : int[] = prim::ListConstruct(%404, %405, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.22 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.21, %406), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %408 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %key_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.22, %408), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %410 : int = aten::size(%x.23, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %411 : int = aten::size(%x.23, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:227:0
  %412 : int[] = prim::ListConstruct(%410, %411, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %x.24 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.23, %412), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:228:0
  %414 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %value_layer.4 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.24, %414), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:229:0
  %416 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.4, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.4, %416), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.7, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:259:0
  %input.36 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.8, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:262:0
  %input.37 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.36, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.4 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.37, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self/__module.bert.encoder.layer.3.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.7 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.4, %value_layer.4), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:275:0
  %423 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %424 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.7, %423), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.8 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%424, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:277:0
  %426 : int = aten::size(%context_layer.8, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %427 : int = aten::size(%context_layer.8, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:278:0
  %428 : int[] = prim::ListConstruct(%426, %427, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self
  %input.38 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.8, %428), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.self # transformers/modeling_bert.py:279:0
  %430 : __torch__.torch.nn.modules.normalization.___torch_mangle_3152.LayerNorm = prim::GetAttr[name="LayerNorm"](%378)
  %431 : __torch__.torch.nn.modules.linear.___torch_mangle_3151.Linear = prim::GetAttr[name="dense"](%378)
  %432 : Tensor = prim::GetAttr[name="bias"](%431)
  %433 : Tensor = prim::GetAttr[name="weight"](%431)
  %434 : Float(768:1, 768:768) = aten::t(%433), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %output.22 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.38, %434), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1676:0
  %input.39 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.22, %432, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.7 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.39, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.dropout # torch/nn/functional.py:973:0
  %input.40 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.7, %input.35, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output # transformers/modeling_bert.py:295:0
  %439 : Tensor = prim::GetAttr[name="bias"](%430)
  %440 : Tensor = prim::GetAttr[name="weight"](%430)
  %441 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm
  %input_tensor.4 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.40, %441, %440, %439, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.attention/__module.bert.encoder.layer.3.attention.output/__module.bert.encoder.layer.3.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %443 : __torch__.torch.nn.modules.linear.___torch_mangle_3156.Linear = prim::GetAttr[name="dense"](%376)
  %444 : Tensor = prim::GetAttr[name="bias"](%443)
  %445 : Tensor = prim::GetAttr[name="weight"](%443)
  %446 : Float(768:1, 3072:768) = aten::t(%445), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %output.23 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.4, %446), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1676:0
  %input.41 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.23, %444, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate/__module.bert.encoder.layer.3.intermediate.dense # torch/nn/functional.py:1678:0
  %input.42 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.41), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.intermediate # torch/nn/functional.py:1369:0
  %450 : __torch__.torch.nn.modules.normalization.___torch_mangle_3159.LayerNorm = prim::GetAttr[name="LayerNorm"](%375)
  %451 : __torch__.torch.nn.modules.linear.___torch_mangle_3158.Linear = prim::GetAttr[name="dense"](%375)
  %452 : Tensor = prim::GetAttr[name="bias"](%451)
  %453 : Tensor = prim::GetAttr[name="weight"](%451)
  %454 : Float(3072:1, 768:3072) = aten::t(%453), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %output.24 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.42, %454), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1676:0
  %input.43 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.24, %452, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.8 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.43, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.dropout # torch/nn/functional.py:973:0
  %input.44 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.8, %input_tensor.4, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output # transformers/modeling_bert.py:371:0
  %459 : Tensor = prim::GetAttr[name="bias"](%450)
  %460 : Tensor = prim::GetAttr[name="weight"](%450)
  %461 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm
  %input.45 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.44, %461, %460, %459, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.3/__module.bert.encoder.layer.3.output/__module.bert.encoder.layer.3.output.LayerNorm # torch/nn/functional.py:2048:0
  %463 : __torch__.transformers.modeling_bert.___torch_mangle_3178.BertOutput = prim::GetAttr[name="output"](%102)
  %464 : __torch__.transformers.modeling_bert.___torch_mangle_3174.BertIntermediate = prim::GetAttr[name="intermediate"](%102)
  %465 : __torch__.transformers.modeling_bert.___torch_mangle_3172.BertAttention = prim::GetAttr[name="attention"](%102)
  %466 : __torch__.transformers.modeling_bert.___torch_mangle_3171.BertSelfOutput = prim::GetAttr[name="output"](%465)
  %467 : __torch__.transformers.modeling_bert.___torch_mangle_3167.BertSelfAttention = prim::GetAttr[name="self"](%465)
  %468 : __torch__.torch.nn.modules.linear.___torch_mangle_3165.Linear = prim::GetAttr[name="value"](%467)
  %469 : __torch__.torch.nn.modules.linear.___torch_mangle_3164.Linear = prim::GetAttr[name="key"](%467)
  %470 : __torch__.torch.nn.modules.linear.___torch_mangle_3163.Linear = prim::GetAttr[name="query"](%467)
  %471 : Tensor = prim::GetAttr[name="bias"](%470)
  %472 : Tensor = prim::GetAttr[name="weight"](%470)
  %473 : Float(768:1, 768:768) = aten::t(%472), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %output.25 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %473), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1676:0
  %x.25 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.25, %471, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.query # torch/nn/functional.py:1678:0
  %476 : Tensor = prim::GetAttr[name="bias"](%469)
  %477 : Tensor = prim::GetAttr[name="weight"](%469)
  %478 : Float(768:1, 768:768) = aten::t(%477), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %output.26 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %478), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1676:0
  %x.27 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.26, %476, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.key # torch/nn/functional.py:1678:0
  %481 : Tensor = prim::GetAttr[name="bias"](%468)
  %482 : Tensor = prim::GetAttr[name="weight"](%468)
  %483 : Float(768:1, 768:768) = aten::t(%482), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %output.27 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.45, %483), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1676:0
  %x.29 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.27, %481, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.value # torch/nn/functional.py:1678:0
  %486 : int = aten::size(%x.25, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %487 : int = aten::size(%x.25, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %488 : int[] = prim::ListConstruct(%486, %487, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.26 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.25, %488), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %490 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %query_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.26, %490), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %492 : int = aten::size(%x.27, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %493 : int = aten::size(%x.27, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %494 : int[] = prim::ListConstruct(%492, %493, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.28 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.27, %494), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %496 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %key_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.28, %496), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %498 : int = aten::size(%x.29, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %499 : int = aten::size(%x.29, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:227:0
  %500 : int[] = prim::ListConstruct(%498, %499, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %x.30 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.29, %500), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:228:0
  %502 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %value_layer.5 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.30, %502), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:229:0
  %504 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.5, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.9 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.5, %504), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.10 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.9, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:259:0
  %input.46 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.10, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:262:0
  %input.47 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.46, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.5 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.47, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self/__module.bert.encoder.layer.4.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.9 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.5, %value_layer.5), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:275:0
  %511 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %512 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.9, %511), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.10 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%512, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:277:0
  %514 : int = aten::size(%context_layer.10, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %515 : int = aten::size(%context_layer.10, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:278:0
  %516 : int[] = prim::ListConstruct(%514, %515, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self
  %input.48 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.10, %516), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.self # transformers/modeling_bert.py:279:0
  %518 : __torch__.torch.nn.modules.normalization.___torch_mangle_3169.LayerNorm = prim::GetAttr[name="LayerNorm"](%466)
  %519 : __torch__.torch.nn.modules.linear.___torch_mangle_3168.Linear = prim::GetAttr[name="dense"](%466)
  %520 : Tensor = prim::GetAttr[name="bias"](%519)
  %521 : Tensor = prim::GetAttr[name="weight"](%519)
  %522 : Float(768:1, 768:768) = aten::t(%521), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %output.28 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.48, %522), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1676:0
  %input.49 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.28, %520, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.9 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.49, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.dropout # torch/nn/functional.py:973:0
  %input.50 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.9, %input.45, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output # transformers/modeling_bert.py:295:0
  %527 : Tensor = prim::GetAttr[name="bias"](%518)
  %528 : Tensor = prim::GetAttr[name="weight"](%518)
  %529 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm
  %input_tensor.5 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.50, %529, %528, %527, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.attention/__module.bert.encoder.layer.4.attention.output/__module.bert.encoder.layer.4.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %531 : __torch__.torch.nn.modules.linear.___torch_mangle_3173.Linear = prim::GetAttr[name="dense"](%464)
  %532 : Tensor = prim::GetAttr[name="bias"](%531)
  %533 : Tensor = prim::GetAttr[name="weight"](%531)
  %534 : Float(768:1, 3072:768) = aten::t(%533), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %output.29 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.5, %534), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1676:0
  %input.51 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.29, %532, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate/__module.bert.encoder.layer.4.intermediate.dense # torch/nn/functional.py:1678:0
  %input.52 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.intermediate # torch/nn/functional.py:1369:0
  %538 : __torch__.torch.nn.modules.normalization.___torch_mangle_3176.LayerNorm = prim::GetAttr[name="LayerNorm"](%463)
  %539 : __torch__.torch.nn.modules.linear.___torch_mangle_3175.Linear = prim::GetAttr[name="dense"](%463)
  %540 : Tensor = prim::GetAttr[name="bias"](%539)
  %541 : Tensor = prim::GetAttr[name="weight"](%539)
  %542 : Float(3072:1, 768:3072) = aten::t(%541), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %output.30 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.52, %542), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1676:0
  %input.53 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.30, %540, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.10 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.53, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.dropout # torch/nn/functional.py:973:0
  %input.54 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.10, %input_tensor.5, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output # transformers/modeling_bert.py:371:0
  %547 : Tensor = prim::GetAttr[name="bias"](%538)
  %548 : Tensor = prim::GetAttr[name="weight"](%538)
  %549 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm
  %input.55 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.54, %549, %548, %547, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.4/__module.bert.encoder.layer.4.output/__module.bert.encoder.layer.4.output.LayerNorm # torch/nn/functional.py:2048:0
  %551 : __torch__.transformers.modeling_bert.___torch_mangle_3195.BertOutput = prim::GetAttr[name="output"](%100)
  %552 : __torch__.transformers.modeling_bert.___torch_mangle_3191.BertIntermediate = prim::GetAttr[name="intermediate"](%100)
  %553 : __torch__.transformers.modeling_bert.___torch_mangle_3189.BertAttention = prim::GetAttr[name="attention"](%100)
  %554 : __torch__.transformers.modeling_bert.___torch_mangle_3188.BertSelfOutput = prim::GetAttr[name="output"](%553)
  %555 : __torch__.transformers.modeling_bert.___torch_mangle_3184.BertSelfAttention = prim::GetAttr[name="self"](%553)
  %556 : __torch__.torch.nn.modules.linear.___torch_mangle_3182.Linear = prim::GetAttr[name="value"](%555)
  %557 : __torch__.torch.nn.modules.linear.___torch_mangle_3181.Linear = prim::GetAttr[name="key"](%555)
  %558 : __torch__.torch.nn.modules.linear.___torch_mangle_3180.Linear = prim::GetAttr[name="query"](%555)
  %559 : Tensor = prim::GetAttr[name="bias"](%558)
  %560 : Tensor = prim::GetAttr[name="weight"](%558)
  %561 : Float(768:1, 768:768) = aten::t(%560), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %output.31 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %561), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1676:0
  %x.31 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.31, %559, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.query # torch/nn/functional.py:1678:0
  %564 : Tensor = prim::GetAttr[name="bias"](%557)
  %565 : Tensor = prim::GetAttr[name="weight"](%557)
  %566 : Float(768:1, 768:768) = aten::t(%565), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %output.32 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %566), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1676:0
  %x.33 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.32, %564, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.key # torch/nn/functional.py:1678:0
  %569 : Tensor = prim::GetAttr[name="bias"](%556)
  %570 : Tensor = prim::GetAttr[name="weight"](%556)
  %571 : Float(768:1, 768:768) = aten::t(%570), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %output.33 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.55, %571), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1676:0
  %x.35 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.33, %569, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.value # torch/nn/functional.py:1678:0
  %574 : int = aten::size(%x.31, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %575 : int = aten::size(%x.31, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %576 : int[] = prim::ListConstruct(%574, %575, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.32 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.31, %576), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %578 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %query_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.32, %578), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %580 : int = aten::size(%x.33, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %581 : int = aten::size(%x.33, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %582 : int[] = prim::ListConstruct(%580, %581, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.34 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.33, %582), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %584 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %key_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.34, %584), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %586 : int = aten::size(%x.35, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %587 : int = aten::size(%x.35, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:227:0
  %588 : int[] = prim::ListConstruct(%586, %587, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %x.36 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.35, %588), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:228:0
  %590 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %value_layer.6 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.36, %590), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:229:0
  %592 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.6, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.11 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.6, %592), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.12 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.11, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:259:0
  %input.56 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.12, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:262:0
  %input.57 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.56, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.6 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.57, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self/__module.bert.encoder.layer.5.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.11 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.6, %value_layer.6), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:275:0
  %599 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %600 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.11, %599), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.12 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%600, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:277:0
  %602 : int = aten::size(%context_layer.12, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %603 : int = aten::size(%context_layer.12, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:278:0
  %604 : int[] = prim::ListConstruct(%602, %603, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self
  %input.58 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.12, %604), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.self # transformers/modeling_bert.py:279:0
  %606 : __torch__.torch.nn.modules.normalization.___torch_mangle_3186.LayerNorm = prim::GetAttr[name="LayerNorm"](%554)
  %607 : __torch__.torch.nn.modules.linear.___torch_mangle_3185.Linear = prim::GetAttr[name="dense"](%554)
  %608 : Tensor = prim::GetAttr[name="bias"](%607)
  %609 : Tensor = prim::GetAttr[name="weight"](%607)
  %610 : Float(768:1, 768:768) = aten::t(%609), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %output.34 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.58, %610), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1676:0
  %input.59 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.34, %608, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.11 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.59, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.dropout # torch/nn/functional.py:973:0
  %input.60 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.11, %input.55, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output # transformers/modeling_bert.py:295:0
  %615 : Tensor = prim::GetAttr[name="bias"](%606)
  %616 : Tensor = prim::GetAttr[name="weight"](%606)
  %617 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm
  %input_tensor.6 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.60, %617, %616, %615, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.attention/__module.bert.encoder.layer.5.attention.output/__module.bert.encoder.layer.5.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %619 : __torch__.torch.nn.modules.linear.___torch_mangle_3190.Linear = prim::GetAttr[name="dense"](%552)
  %620 : Tensor = prim::GetAttr[name="bias"](%619)
  %621 : Tensor = prim::GetAttr[name="weight"](%619)
  %622 : Float(768:1, 3072:768) = aten::t(%621), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %output.35 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.6, %622), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1676:0
  %input.61 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.35, %620, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate/__module.bert.encoder.layer.5.intermediate.dense # torch/nn/functional.py:1678:0
  %input.62 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.61), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.intermediate # torch/nn/functional.py:1369:0
  %626 : __torch__.torch.nn.modules.normalization.___torch_mangle_3193.LayerNorm = prim::GetAttr[name="LayerNorm"](%551)
  %627 : __torch__.torch.nn.modules.linear.___torch_mangle_3192.Linear = prim::GetAttr[name="dense"](%551)
  %628 : Tensor = prim::GetAttr[name="bias"](%627)
  %629 : Tensor = prim::GetAttr[name="weight"](%627)
  %630 : Float(3072:1, 768:3072) = aten::t(%629), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %output.36 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.62, %630), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1676:0
  %input.63 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.36, %628, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.12 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.63, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.dropout # torch/nn/functional.py:973:0
  %input.64 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.12, %input_tensor.6, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output # transformers/modeling_bert.py:371:0
  %635 : Tensor = prim::GetAttr[name="bias"](%626)
  %636 : Tensor = prim::GetAttr[name="weight"](%626)
  %637 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm
  %input.65 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.64, %637, %636, %635, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.5/__module.bert.encoder.layer.5.output/__module.bert.encoder.layer.5.output.LayerNorm # torch/nn/functional.py:2048:0
  %639 : __torch__.transformers.modeling_bert.___torch_mangle_3212.BertOutput = prim::GetAttr[name="output"](%98)
  %640 : __torch__.transformers.modeling_bert.___torch_mangle_3208.BertIntermediate = prim::GetAttr[name="intermediate"](%98)
  %641 : __torch__.transformers.modeling_bert.___torch_mangle_3206.BertAttention = prim::GetAttr[name="attention"](%98)
  %642 : __torch__.transformers.modeling_bert.___torch_mangle_3205.BertSelfOutput = prim::GetAttr[name="output"](%641)
  %643 : __torch__.transformers.modeling_bert.___torch_mangle_3201.BertSelfAttention = prim::GetAttr[name="self"](%641)
  %644 : __torch__.torch.nn.modules.linear.___torch_mangle_3199.Linear = prim::GetAttr[name="value"](%643)
  %645 : __torch__.torch.nn.modules.linear.___torch_mangle_3198.Linear = prim::GetAttr[name="key"](%643)
  %646 : __torch__.torch.nn.modules.linear.___torch_mangle_3197.Linear = prim::GetAttr[name="query"](%643)
  %647 : Tensor = prim::GetAttr[name="bias"](%646)
  %648 : Tensor = prim::GetAttr[name="weight"](%646)
  %649 : Float(768:1, 768:768) = aten::t(%648), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %output.37 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %649), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1676:0
  %x.37 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.37, %647, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.query # torch/nn/functional.py:1678:0
  %652 : Tensor = prim::GetAttr[name="bias"](%645)
  %653 : Tensor = prim::GetAttr[name="weight"](%645)
  %654 : Float(768:1, 768:768) = aten::t(%653), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %output.38 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %654), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1676:0
  %x.39 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.38, %652, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.key # torch/nn/functional.py:1678:0
  %657 : Tensor = prim::GetAttr[name="bias"](%644)
  %658 : Tensor = prim::GetAttr[name="weight"](%644)
  %659 : Float(768:1, 768:768) = aten::t(%658), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %output.39 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.65, %659), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1676:0
  %x.41 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.39, %657, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.value # torch/nn/functional.py:1678:0
  %662 : int = aten::size(%x.37, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %663 : int = aten::size(%x.37, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %664 : int[] = prim::ListConstruct(%662, %663, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.38 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.37, %664), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %666 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %query_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.38, %666), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %668 : int = aten::size(%x.39, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %669 : int = aten::size(%x.39, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %670 : int[] = prim::ListConstruct(%668, %669, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.40 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.39, %670), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %672 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %key_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.40, %672), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %674 : int = aten::size(%x.41, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %675 : int = aten::size(%x.41, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:227:0
  %676 : int[] = prim::ListConstruct(%674, %675, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %x.42 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.41, %676), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:228:0
  %678 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %value_layer.7 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.42, %678), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:229:0
  %680 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.7, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.13 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.7, %680), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.14 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.13, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:259:0
  %input.66 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.14, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:262:0
  %input.67 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.66, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.7 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.67, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self/__module.bert.encoder.layer.6.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.13 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.7, %value_layer.7), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:275:0
  %687 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %688 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.13, %687), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.14 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%688, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:277:0
  %690 : int = aten::size(%context_layer.14, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %691 : int = aten::size(%context_layer.14, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:278:0
  %692 : int[] = prim::ListConstruct(%690, %691, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self
  %input.68 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.14, %692), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.self # transformers/modeling_bert.py:279:0
  %694 : __torch__.torch.nn.modules.normalization.___torch_mangle_3203.LayerNorm = prim::GetAttr[name="LayerNorm"](%642)
  %695 : __torch__.torch.nn.modules.linear.___torch_mangle_3202.Linear = prim::GetAttr[name="dense"](%642)
  %696 : Tensor = prim::GetAttr[name="bias"](%695)
  %697 : Tensor = prim::GetAttr[name="weight"](%695)
  %698 : Float(768:1, 768:768) = aten::t(%697), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %output.40 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.68, %698), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1676:0
  %input.69 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.40, %696, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.13 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.69, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.dropout # torch/nn/functional.py:973:0
  %input.70 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.13, %input.65, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output # transformers/modeling_bert.py:295:0
  %703 : Tensor = prim::GetAttr[name="bias"](%694)
  %704 : Tensor = prim::GetAttr[name="weight"](%694)
  %705 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm
  %input_tensor.7 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.70, %705, %704, %703, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.attention/__module.bert.encoder.layer.6.attention.output/__module.bert.encoder.layer.6.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %707 : __torch__.torch.nn.modules.linear.___torch_mangle_3207.Linear = prim::GetAttr[name="dense"](%640)
  %708 : Tensor = prim::GetAttr[name="bias"](%707)
  %709 : Tensor = prim::GetAttr[name="weight"](%707)
  %710 : Float(768:1, 3072:768) = aten::t(%709), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %output.41 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.7, %710), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1676:0
  %input.71 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.41, %708, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate/__module.bert.encoder.layer.6.intermediate.dense # torch/nn/functional.py:1678:0
  %input.72 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.71), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.intermediate # torch/nn/functional.py:1369:0
  %714 : __torch__.torch.nn.modules.normalization.___torch_mangle_3210.LayerNorm = prim::GetAttr[name="LayerNorm"](%639)
  %715 : __torch__.torch.nn.modules.linear.___torch_mangle_3209.Linear = prim::GetAttr[name="dense"](%639)
  %716 : Tensor = prim::GetAttr[name="bias"](%715)
  %717 : Tensor = prim::GetAttr[name="weight"](%715)
  %718 : Float(3072:1, 768:3072) = aten::t(%717), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %output.42 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.72, %718), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1676:0
  %input.73 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.42, %716, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.14 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.73, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.dropout # torch/nn/functional.py:973:0
  %input.74 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.14, %input_tensor.7, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output # transformers/modeling_bert.py:371:0
  %723 : Tensor = prim::GetAttr[name="bias"](%714)
  %724 : Tensor = prim::GetAttr[name="weight"](%714)
  %725 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm
  %input.75 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.74, %725, %724, %723, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.6/__module.bert.encoder.layer.6.output/__module.bert.encoder.layer.6.output.LayerNorm # torch/nn/functional.py:2048:0
  %727 : __torch__.transformers.modeling_bert.___torch_mangle_3229.BertOutput = prim::GetAttr[name="output"](%96)
  %728 : __torch__.transformers.modeling_bert.___torch_mangle_3225.BertIntermediate = prim::GetAttr[name="intermediate"](%96)
  %729 : __torch__.transformers.modeling_bert.___torch_mangle_3223.BertAttention = prim::GetAttr[name="attention"](%96)
  %730 : __torch__.transformers.modeling_bert.___torch_mangle_3222.BertSelfOutput = prim::GetAttr[name="output"](%729)
  %731 : __torch__.transformers.modeling_bert.___torch_mangle_3218.BertSelfAttention = prim::GetAttr[name="self"](%729)
  %732 : __torch__.torch.nn.modules.linear.___torch_mangle_3216.Linear = prim::GetAttr[name="value"](%731)
  %733 : __torch__.torch.nn.modules.linear.___torch_mangle_3215.Linear = prim::GetAttr[name="key"](%731)
  %734 : __torch__.torch.nn.modules.linear.___torch_mangle_3214.Linear = prim::GetAttr[name="query"](%731)
  %735 : Tensor = prim::GetAttr[name="bias"](%734)
  %736 : Tensor = prim::GetAttr[name="weight"](%734)
  %737 : Float(768:1, 768:768) = aten::t(%736), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %output.43 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %737), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1676:0
  %x.43 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.43, %735, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.query # torch/nn/functional.py:1678:0
  %740 : Tensor = prim::GetAttr[name="bias"](%733)
  %741 : Tensor = prim::GetAttr[name="weight"](%733)
  %742 : Float(768:1, 768:768) = aten::t(%741), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %output.44 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %742), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1676:0
  %x.45 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.44, %740, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.key # torch/nn/functional.py:1678:0
  %745 : Tensor = prim::GetAttr[name="bias"](%732)
  %746 : Tensor = prim::GetAttr[name="weight"](%732)
  %747 : Float(768:1, 768:768) = aten::t(%746), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %output.45 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.75, %747), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1676:0
  %x.47 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.45, %745, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.value # torch/nn/functional.py:1678:0
  %750 : int = aten::size(%x.43, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %751 : int = aten::size(%x.43, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %752 : int[] = prim::ListConstruct(%750, %751, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.44 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.43, %752), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %754 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %query_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.44, %754), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %756 : int = aten::size(%x.45, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %757 : int = aten::size(%x.45, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %758 : int[] = prim::ListConstruct(%756, %757, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.46 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.45, %758), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %760 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %key_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.46, %760), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %762 : int = aten::size(%x.47, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %763 : int = aten::size(%x.47, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:227:0
  %764 : int[] = prim::ListConstruct(%762, %763, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %x.48 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.47, %764), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:228:0
  %766 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %value_layer.8 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.48, %766), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:229:0
  %768 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.8, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.15 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.8, %768), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.16 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.15, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:259:0
  %input.76 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.16, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:262:0
  %input.77 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.76, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.8 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.77, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self/__module.bert.encoder.layer.7.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.15 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.8, %value_layer.8), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:275:0
  %775 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %776 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.15, %775), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.16 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%776, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:277:0
  %778 : int = aten::size(%context_layer.16, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %779 : int = aten::size(%context_layer.16, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:278:0
  %780 : int[] = prim::ListConstruct(%778, %779, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self
  %input.78 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.16, %780), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.self # transformers/modeling_bert.py:279:0
  %782 : __torch__.torch.nn.modules.normalization.___torch_mangle_3220.LayerNorm = prim::GetAttr[name="LayerNorm"](%730)
  %783 : __torch__.torch.nn.modules.linear.___torch_mangle_3219.Linear = prim::GetAttr[name="dense"](%730)
  %784 : Tensor = prim::GetAttr[name="bias"](%783)
  %785 : Tensor = prim::GetAttr[name="weight"](%783)
  %786 : Float(768:1, 768:768) = aten::t(%785), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %output.46 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.78, %786), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1676:0
  %input.79 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.46, %784, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.15 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.79, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.dropout # torch/nn/functional.py:973:0
  %input.80 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.15, %input.75, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output # transformers/modeling_bert.py:295:0
  %791 : Tensor = prim::GetAttr[name="bias"](%782)
  %792 : Tensor = prim::GetAttr[name="weight"](%782)
  %793 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm
  %input_tensor.8 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.80, %793, %792, %791, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.attention/__module.bert.encoder.layer.7.attention.output/__module.bert.encoder.layer.7.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %795 : __torch__.torch.nn.modules.linear.___torch_mangle_3224.Linear = prim::GetAttr[name="dense"](%728)
  %796 : Tensor = prim::GetAttr[name="bias"](%795)
  %797 : Tensor = prim::GetAttr[name="weight"](%795)
  %798 : Float(768:1, 3072:768) = aten::t(%797), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %output.47 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.8, %798), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1676:0
  %input.81 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.47, %796, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate/__module.bert.encoder.layer.7.intermediate.dense # torch/nn/functional.py:1678:0
  %input.82 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.81), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.intermediate # torch/nn/functional.py:1369:0
  %802 : __torch__.torch.nn.modules.normalization.___torch_mangle_3227.LayerNorm = prim::GetAttr[name="LayerNorm"](%727)
  %803 : __torch__.torch.nn.modules.linear.___torch_mangle_3226.Linear = prim::GetAttr[name="dense"](%727)
  %804 : Tensor = prim::GetAttr[name="bias"](%803)
  %805 : Tensor = prim::GetAttr[name="weight"](%803)
  %806 : Float(3072:1, 768:3072) = aten::t(%805), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %output.48 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.82, %806), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1676:0
  %input.83 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.48, %804, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.16 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.83, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.dropout # torch/nn/functional.py:973:0
  %input.84 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.16, %input_tensor.8, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output # transformers/modeling_bert.py:371:0
  %811 : Tensor = prim::GetAttr[name="bias"](%802)
  %812 : Tensor = prim::GetAttr[name="weight"](%802)
  %813 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm
  %input.85 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.84, %813, %812, %811, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.7/__module.bert.encoder.layer.7.output/__module.bert.encoder.layer.7.output.LayerNorm # torch/nn/functional.py:2048:0
  %815 : __torch__.transformers.modeling_bert.___torch_mangle_3246.BertOutput = prim::GetAttr[name="output"](%94)
  %816 : __torch__.transformers.modeling_bert.___torch_mangle_3242.BertIntermediate = prim::GetAttr[name="intermediate"](%94)
  %817 : __torch__.transformers.modeling_bert.___torch_mangle_3240.BertAttention = prim::GetAttr[name="attention"](%94)
  %818 : __torch__.transformers.modeling_bert.___torch_mangle_3239.BertSelfOutput = prim::GetAttr[name="output"](%817)
  %819 : __torch__.transformers.modeling_bert.___torch_mangle_3235.BertSelfAttention = prim::GetAttr[name="self"](%817)
  %820 : __torch__.torch.nn.modules.linear.___torch_mangle_3233.Linear = prim::GetAttr[name="value"](%819)
  %821 : __torch__.torch.nn.modules.linear.___torch_mangle_3232.Linear = prim::GetAttr[name="key"](%819)
  %822 : __torch__.torch.nn.modules.linear.___torch_mangle_3231.Linear = prim::GetAttr[name="query"](%819)
  %823 : Tensor = prim::GetAttr[name="bias"](%822)
  %824 : Tensor = prim::GetAttr[name="weight"](%822)
  %825 : Float(768:1, 768:768) = aten::t(%824), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %output.49 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %825), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1676:0
  %x.49 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.49, %823, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.query # torch/nn/functional.py:1678:0
  %828 : Tensor = prim::GetAttr[name="bias"](%821)
  %829 : Tensor = prim::GetAttr[name="weight"](%821)
  %830 : Float(768:1, 768:768) = aten::t(%829), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %output.50 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %830), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1676:0
  %x.51 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.50, %828, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.key # torch/nn/functional.py:1678:0
  %833 : Tensor = prim::GetAttr[name="bias"](%820)
  %834 : Tensor = prim::GetAttr[name="weight"](%820)
  %835 : Float(768:1, 768:768) = aten::t(%834), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %output.51 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.85, %835), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1676:0
  %x.53 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.51, %833, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.value # torch/nn/functional.py:1678:0
  %838 : int = aten::size(%x.49, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %839 : int = aten::size(%x.49, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %840 : int[] = prim::ListConstruct(%838, %839, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.50 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.49, %840), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %842 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %query_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.50, %842), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %844 : int = aten::size(%x.51, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %845 : int = aten::size(%x.51, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %846 : int[] = prim::ListConstruct(%844, %845, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.52 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.51, %846), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %848 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %key_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.52, %848), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %850 : int = aten::size(%x.53, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %851 : int = aten::size(%x.53, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:227:0
  %852 : int[] = prim::ListConstruct(%850, %851, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %x.54 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.53, %852), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:228:0
  %854 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %value_layer.9 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.54, %854), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:229:0
  %856 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.9, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.17 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.9, %856), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.18 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.17, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:259:0
  %input.86 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.18, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:262:0
  %input.87 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.86, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.9 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.87, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self/__module.bert.encoder.layer.8.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.17 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.9, %value_layer.9), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:275:0
  %863 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %864 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.17, %863), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.18 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%864, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:277:0
  %866 : int = aten::size(%context_layer.18, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %867 : int = aten::size(%context_layer.18, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:278:0
  %868 : int[] = prim::ListConstruct(%866, %867, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self
  %input.88 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.18, %868), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.self # transformers/modeling_bert.py:279:0
  %870 : __torch__.torch.nn.modules.normalization.___torch_mangle_3237.LayerNorm = prim::GetAttr[name="LayerNorm"](%818)
  %871 : __torch__.torch.nn.modules.linear.___torch_mangle_3236.Linear = prim::GetAttr[name="dense"](%818)
  %872 : Tensor = prim::GetAttr[name="bias"](%871)
  %873 : Tensor = prim::GetAttr[name="weight"](%871)
  %874 : Float(768:1, 768:768) = aten::t(%873), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %output.52 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.88, %874), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1676:0
  %input.89 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.52, %872, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.17 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.89, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.dropout # torch/nn/functional.py:973:0
  %input.90 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.17, %input.85, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output # transformers/modeling_bert.py:295:0
  %879 : Tensor = prim::GetAttr[name="bias"](%870)
  %880 : Tensor = prim::GetAttr[name="weight"](%870)
  %881 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm
  %input_tensor.9 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.90, %881, %880, %879, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.attention/__module.bert.encoder.layer.8.attention.output/__module.bert.encoder.layer.8.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %883 : __torch__.torch.nn.modules.linear.___torch_mangle_3241.Linear = prim::GetAttr[name="dense"](%816)
  %884 : Tensor = prim::GetAttr[name="bias"](%883)
  %885 : Tensor = prim::GetAttr[name="weight"](%883)
  %886 : Float(768:1, 3072:768) = aten::t(%885), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %output.53 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.9, %886), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1676:0
  %input.91 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.53, %884, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate/__module.bert.encoder.layer.8.intermediate.dense # torch/nn/functional.py:1678:0
  %input.92 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.91), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.intermediate # torch/nn/functional.py:1369:0
  %890 : __torch__.torch.nn.modules.normalization.___torch_mangle_3244.LayerNorm = prim::GetAttr[name="LayerNorm"](%815)
  %891 : __torch__.torch.nn.modules.linear.___torch_mangle_3243.Linear = prim::GetAttr[name="dense"](%815)
  %892 : Tensor = prim::GetAttr[name="bias"](%891)
  %893 : Tensor = prim::GetAttr[name="weight"](%891)
  %894 : Float(3072:1, 768:3072) = aten::t(%893), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %output.54 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.92, %894), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1676:0
  %input.93 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.54, %892, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.18 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.93, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.dropout # torch/nn/functional.py:973:0
  %input.94 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.18, %input_tensor.9, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output # transformers/modeling_bert.py:371:0
  %899 : Tensor = prim::GetAttr[name="bias"](%890)
  %900 : Tensor = prim::GetAttr[name="weight"](%890)
  %901 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm
  %input.95 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.94, %901, %900, %899, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.8/__module.bert.encoder.layer.8.output/__module.bert.encoder.layer.8.output.LayerNorm # torch/nn/functional.py:2048:0
  %903 : __torch__.transformers.modeling_bert.___torch_mangle_3263.BertOutput = prim::GetAttr[name="output"](%92)
  %904 : __torch__.transformers.modeling_bert.___torch_mangle_3259.BertIntermediate = prim::GetAttr[name="intermediate"](%92)
  %905 : __torch__.transformers.modeling_bert.___torch_mangle_3257.BertAttention = prim::GetAttr[name="attention"](%92)
  %906 : __torch__.transformers.modeling_bert.___torch_mangle_3256.BertSelfOutput = prim::GetAttr[name="output"](%905)
  %907 : __torch__.transformers.modeling_bert.___torch_mangle_3252.BertSelfAttention = prim::GetAttr[name="self"](%905)
  %908 : __torch__.torch.nn.modules.linear.___torch_mangle_3250.Linear = prim::GetAttr[name="value"](%907)
  %909 : __torch__.torch.nn.modules.linear.___torch_mangle_3249.Linear = prim::GetAttr[name="key"](%907)
  %910 : __torch__.torch.nn.modules.linear.___torch_mangle_3248.Linear = prim::GetAttr[name="query"](%907)
  %911 : Tensor = prim::GetAttr[name="bias"](%910)
  %912 : Tensor = prim::GetAttr[name="weight"](%910)
  %913 : Float(768:1, 768:768) = aten::t(%912), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %output.55 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %913), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1676:0
  %x.55 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.55, %911, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.query # torch/nn/functional.py:1678:0
  %916 : Tensor = prim::GetAttr[name="bias"](%909)
  %917 : Tensor = prim::GetAttr[name="weight"](%909)
  %918 : Float(768:1, 768:768) = aten::t(%917), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %output.56 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %918), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1676:0
  %x.57 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.56, %916, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.key # torch/nn/functional.py:1678:0
  %921 : Tensor = prim::GetAttr[name="bias"](%908)
  %922 : Tensor = prim::GetAttr[name="weight"](%908)
  %923 : Float(768:1, 768:768) = aten::t(%922), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %output.57 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.95, %923), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1676:0
  %x.59 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.57, %921, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.value # torch/nn/functional.py:1678:0
  %926 : int = aten::size(%x.55, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %927 : int = aten::size(%x.55, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %928 : int[] = prim::ListConstruct(%926, %927, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.56 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.55, %928), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %930 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %query_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.56, %930), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %932 : int = aten::size(%x.57, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %933 : int = aten::size(%x.57, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %934 : int[] = prim::ListConstruct(%932, %933, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.58 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.57, %934), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %936 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %key_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.58, %936), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %938 : int = aten::size(%x.59, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %939 : int = aten::size(%x.59, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:227:0
  %940 : int[] = prim::ListConstruct(%938, %939, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %x.60 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.59, %940), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:228:0
  %942 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %value_layer.10 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.60, %942), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:229:0
  %944 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.10, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.19 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.10, %944), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.20 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.19, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:259:0
  %input.96 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.20, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:262:0
  %input.97 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.96, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.10 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.97, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self/__module.bert.encoder.layer.9.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.19 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.10, %value_layer.10), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:275:0
  %951 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %952 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.19, %951), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.20 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%952, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:277:0
  %954 : int = aten::size(%context_layer.20, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %955 : int = aten::size(%context_layer.20, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:278:0
  %956 : int[] = prim::ListConstruct(%954, %955, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self
  %input.98 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.20, %956), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.self # transformers/modeling_bert.py:279:0
  %958 : __torch__.torch.nn.modules.normalization.___torch_mangle_3254.LayerNorm = prim::GetAttr[name="LayerNorm"](%906)
  %959 : __torch__.torch.nn.modules.linear.___torch_mangle_3253.Linear = prim::GetAttr[name="dense"](%906)
  %960 : Tensor = prim::GetAttr[name="bias"](%959)
  %961 : Tensor = prim::GetAttr[name="weight"](%959)
  %962 : Float(768:1, 768:768) = aten::t(%961), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %output.58 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.98, %962), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1676:0
  %input.99 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.58, %960, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.19 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.99, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.dropout # torch/nn/functional.py:973:0
  %input.100 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.19, %input.95, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output # transformers/modeling_bert.py:295:0
  %967 : Tensor = prim::GetAttr[name="bias"](%958)
  %968 : Tensor = prim::GetAttr[name="weight"](%958)
  %969 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm
  %input_tensor.10 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.100, %969, %968, %967, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.attention/__module.bert.encoder.layer.9.attention.output/__module.bert.encoder.layer.9.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %971 : __torch__.torch.nn.modules.linear.___torch_mangle_3258.Linear = prim::GetAttr[name="dense"](%904)
  %972 : Tensor = prim::GetAttr[name="bias"](%971)
  %973 : Tensor = prim::GetAttr[name="weight"](%971)
  %974 : Float(768:1, 3072:768) = aten::t(%973), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %output.59 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.10, %974), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1676:0
  %input.101 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.59, %972, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate/__module.bert.encoder.layer.9.intermediate.dense # torch/nn/functional.py:1678:0
  %input.102 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.101), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.intermediate # torch/nn/functional.py:1369:0
  %978 : __torch__.torch.nn.modules.normalization.___torch_mangle_3261.LayerNorm = prim::GetAttr[name="LayerNorm"](%903)
  %979 : __torch__.torch.nn.modules.linear.___torch_mangle_3260.Linear = prim::GetAttr[name="dense"](%903)
  %980 : Tensor = prim::GetAttr[name="bias"](%979)
  %981 : Tensor = prim::GetAttr[name="weight"](%979)
  %982 : Float(3072:1, 768:3072) = aten::t(%981), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %output.60 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.102, %982), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1676:0
  %input.103 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.60, %980, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.20 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.103, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.dropout # torch/nn/functional.py:973:0
  %input.104 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.20, %input_tensor.10, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output # transformers/modeling_bert.py:371:0
  %987 : Tensor = prim::GetAttr[name="bias"](%978)
  %988 : Tensor = prim::GetAttr[name="weight"](%978)
  %989 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm
  %input.105 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.104, %989, %988, %987, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.9/__module.bert.encoder.layer.9.output/__module.bert.encoder.layer.9.output.LayerNorm # torch/nn/functional.py:2048:0
  %991 : __torch__.transformers.modeling_bert.___torch_mangle_3280.BertOutput = prim::GetAttr[name="output"](%90)
  %992 : __torch__.transformers.modeling_bert.___torch_mangle_3276.BertIntermediate = prim::GetAttr[name="intermediate"](%90)
  %993 : __torch__.transformers.modeling_bert.___torch_mangle_3274.BertAttention = prim::GetAttr[name="attention"](%90)
  %994 : __torch__.transformers.modeling_bert.___torch_mangle_3273.BertSelfOutput = prim::GetAttr[name="output"](%993)
  %995 : __torch__.transformers.modeling_bert.___torch_mangle_3269.BertSelfAttention = prim::GetAttr[name="self"](%993)
  %996 : __torch__.torch.nn.modules.linear.___torch_mangle_3267.Linear = prim::GetAttr[name="value"](%995)
  %997 : __torch__.torch.nn.modules.linear.___torch_mangle_3266.Linear = prim::GetAttr[name="key"](%995)
  %998 : __torch__.torch.nn.modules.linear.___torch_mangle_3265.Linear = prim::GetAttr[name="query"](%995)
  %999 : Tensor = prim::GetAttr[name="bias"](%998)
  %1000 : Tensor = prim::GetAttr[name="weight"](%998)
  %1001 : Float(768:1, 768:768) = aten::t(%1000), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %output.61 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1001), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1676:0
  %x.61 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.61, %999, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.query # torch/nn/functional.py:1678:0
  %1004 : Tensor = prim::GetAttr[name="bias"](%997)
  %1005 : Tensor = prim::GetAttr[name="weight"](%997)
  %1006 : Float(768:1, 768:768) = aten::t(%1005), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %output.62 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1006), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1676:0
  %x.63 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.62, %1004, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.key # torch/nn/functional.py:1678:0
  %1009 : Tensor = prim::GetAttr[name="bias"](%996)
  %1010 : Tensor = prim::GetAttr[name="weight"](%996)
  %1011 : Float(768:1, 768:768) = aten::t(%1010), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %output.63 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.105, %1011), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1676:0
  %x.65 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.63, %1009, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.value # torch/nn/functional.py:1678:0
  %1014 : int = aten::size(%x.61, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1015 : int = aten::size(%x.61, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1016 : int[] = prim::ListConstruct(%1014, %1015, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.62 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.61, %1016), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1018 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %query_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.62, %1018), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1020 : int = aten::size(%x.63, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1021 : int = aten::size(%x.63, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1022 : int[] = prim::ListConstruct(%1020, %1021, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.64 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.63, %1022), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1024 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %key_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.64, %1024), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1026 : int = aten::size(%x.65, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1027 : int = aten::size(%x.65, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:227:0
  %1028 : int[] = prim::ListConstruct(%1026, %1027, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %x.66 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.65, %1028), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:228:0
  %1030 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %value_layer.11 : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.66, %1030), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:229:0
  %1032 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer.11, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.21 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer.11, %1032), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.22 : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.21, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:259:0
  %input.106 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores.22, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:262:0
  %input.107 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.106, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # torch/nn/functional.py:1498:0
  %attention_probs.11 : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.107, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self/__module.bert.encoder.layer.10.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.21 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs.11, %value_layer.11), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:275:0
  %1039 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %1040 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.21, %1039), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %context_layer.22 : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1040, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:277:0
  %1042 : int = aten::size(%context_layer.22, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1043 : int = aten::size(%context_layer.22, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:278:0
  %1044 : int[] = prim::ListConstruct(%1042, %1043, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self
  %input.108 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer.22, %1044), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.self # transformers/modeling_bert.py:279:0
  %1046 : __torch__.torch.nn.modules.normalization.___torch_mangle_3271.LayerNorm = prim::GetAttr[name="LayerNorm"](%994)
  %1047 : __torch__.torch.nn.modules.linear.___torch_mangle_3270.Linear = prim::GetAttr[name="dense"](%994)
  %1048 : Tensor = prim::GetAttr[name="bias"](%1047)
  %1049 : Tensor = prim::GetAttr[name="weight"](%1047)
  %1050 : Float(768:1, 768:768) = aten::t(%1049), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %output.64 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.108, %1050), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1676:0
  %input.109 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.64, %1048, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.21 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.109, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.dropout # torch/nn/functional.py:973:0
  %input.110 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.21, %input.105, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output # transformers/modeling_bert.py:295:0
  %1055 : Tensor = prim::GetAttr[name="bias"](%1046)
  %1056 : Tensor = prim::GetAttr[name="weight"](%1046)
  %1057 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm
  %input_tensor.11 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.110, %1057, %1056, %1055, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.attention/__module.bert.encoder.layer.10.attention.output/__module.bert.encoder.layer.10.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1059 : __torch__.torch.nn.modules.linear.___torch_mangle_3275.Linear = prim::GetAttr[name="dense"](%992)
  %1060 : Tensor = prim::GetAttr[name="bias"](%1059)
  %1061 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1062 : Float(768:1, 3072:768) = aten::t(%1061), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %output.65 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor.11, %1062), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1676:0
  %input.111 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.65, %1060, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate/__module.bert.encoder.layer.10.intermediate.dense # torch/nn/functional.py:1678:0
  %input.112 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.111), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.intermediate # torch/nn/functional.py:1369:0
  %1066 : __torch__.torch.nn.modules.normalization.___torch_mangle_3278.LayerNorm = prim::GetAttr[name="LayerNorm"](%991)
  %1067 : __torch__.torch.nn.modules.linear.___torch_mangle_3277.Linear = prim::GetAttr[name="dense"](%991)
  %1068 : Tensor = prim::GetAttr[name="bias"](%1067)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1067)
  %1070 : Float(3072:1, 768:3072) = aten::t(%1069), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %output.66 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.112, %1070), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1676:0
  %input.113 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.66, %1068, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.22 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.113, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.dropout # torch/nn/functional.py:973:0
  %input.114 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.22, %input_tensor.11, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output # transformers/modeling_bert.py:371:0
  %1075 : Tensor = prim::GetAttr[name="bias"](%1066)
  %1076 : Tensor = prim::GetAttr[name="weight"](%1066)
  %1077 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm
  %input.115 : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.114, %1077, %1076, %1075, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.10/__module.bert.encoder.layer.10.output/__module.bert.encoder.layer.10.output.LayerNorm # torch/nn/functional.py:2048:0
  %1079 : __torch__.transformers.modeling_bert.___torch_mangle_3297.BertOutput = prim::GetAttr[name="output"](%88)
  %1080 : __torch__.transformers.modeling_bert.___torch_mangle_3293.BertIntermediate = prim::GetAttr[name="intermediate"](%88)
  %1081 : __torch__.transformers.modeling_bert.___torch_mangle_3291.BertAttention = prim::GetAttr[name="attention"](%88)
  %1082 : __torch__.transformers.modeling_bert.___torch_mangle_3290.BertSelfOutput = prim::GetAttr[name="output"](%1081)
  %1083 : __torch__.transformers.modeling_bert.___torch_mangle_3286.BertSelfAttention = prim::GetAttr[name="self"](%1081)
  %1084 : __torch__.torch.nn.modules.linear.___torch_mangle_3284.Linear = prim::GetAttr[name="value"](%1083)
  %1085 : __torch__.torch.nn.modules.linear.___torch_mangle_3283.Linear = prim::GetAttr[name="key"](%1083)
  %1086 : __torch__.torch.nn.modules.linear.___torch_mangle_3282.Linear = prim::GetAttr[name="query"](%1083)
  %1087 : Tensor = prim::GetAttr[name="bias"](%1086)
  %1088 : Tensor = prim::GetAttr[name="weight"](%1086)
  %1089 : Float(768:1, 768:768) = aten::t(%1088), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %output.67 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1089), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1676:0
  %x.67 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.67, %1087, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.query # torch/nn/functional.py:1678:0
  %1092 : Tensor = prim::GetAttr[name="bias"](%1085)
  %1093 : Tensor = prim::GetAttr[name="weight"](%1085)
  %1094 : Float(768:1, 768:768) = aten::t(%1093), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %output.68 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1094), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1676:0
  %x.69 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.68, %1092, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.key # torch/nn/functional.py:1678:0
  %1097 : Tensor = prim::GetAttr[name="bias"](%1084)
  %1098 : Tensor = prim::GetAttr[name="weight"](%1084)
  %1099 : Float(768:1, 768:768) = aten::t(%1098), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %output.69 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.115, %1099), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1676:0
  %x.71 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.69, %1097, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.value # torch/nn/functional.py:1678:0
  %1102 : int = aten::size(%x.67, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1103 : int = aten::size(%x.67, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1104 : int[] = prim::ListConstruct(%1102, %1103, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.68 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.67, %1104), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1106 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %query_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.68, %1106), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1108 : int = aten::size(%x.69, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1109 : int = aten::size(%x.69, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1110 : int[] = prim::ListConstruct(%1108, %1109, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x.70 : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.69, %1110), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1112 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %key_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x.70, %1112), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1114 : int = aten::size(%x.71, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1115 : int = aten::size(%x.71, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:227:0
  %1116 : int[] = prim::ListConstruct(%1114, %1115, %34, %33), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %x : Float(119:9984, 13:768, 12:64, 64:1) = aten::view(%x.71, %1116), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:228:0
  %1118 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %value_layer : Float(119:9984, 12:64, 13:768, 64:1) = aten::permute(%x, %1118), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:229:0
  %1120 : Float(119:9984, 12:64, 64:1, 13:768) = aten::transpose(%key_layer, %35, %32), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores.23 : Float(119:2028, 12:169, 13:13, 13:1) = aten::matmul(%query_layer, %1120), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:258:0
  %attention_scores : Float(119:2028, 12:169, 13:13, 13:1) = aten::div(%attention_scores.23, %31), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:259:0
  %input.116 : Float(119:2028, 12:169, 13:13, 13:1) = aten::add(%attention_scores, %attention_mask, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:262:0
  %input.117 : Float(119:2028, 12:169, 13:13, 13:1) = aten::softmax(%input.116, %35, %42), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # torch/nn/functional.py:1498:0
  %attention_probs : Float(119:2028, 12:169, 13:13, 13:1) = aten::dropout(%input.117, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self/__module.bert.encoder.layer.11.attention.self.dropout # torch/nn/functional.py:973:0
  %context_layer.23 : Float(119:9984, 12:832, 13:64, 64:1) = aten::matmul(%attention_probs, %value_layer), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:275:0
  %1127 : int[] = prim::ListConstruct(%51, %45, %50, %44), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %1128 : Float(119:9984, 13:64, 12:832, 64:1) = aten::permute(%context_layer.23, %1127), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %context_layer : Float(119:9984, 13:768, 12:64, 64:1) = aten::contiguous(%1128, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:277:0
  %1130 : int = aten::size(%context_layer, %51), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1131 : int = aten::size(%context_layer, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:278:0
  %1132 : int[] = prim::ListConstruct(%1130, %1131, %38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self
  %input.118 : Float(119:9984, 13:768, 768:1) = aten::view(%context_layer, %1132), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.self # transformers/modeling_bert.py:279:0
  %1134 : __torch__.torch.nn.modules.normalization.___torch_mangle_3288.LayerNorm = prim::GetAttr[name="LayerNorm"](%1082)
  %1135 : __torch__.torch.nn.modules.linear.___torch_mangle_3287.Linear = prim::GetAttr[name="dense"](%1082)
  %1136 : Tensor = prim::GetAttr[name="bias"](%1135)
  %1137 : Tensor = prim::GetAttr[name="weight"](%1135)
  %1138 : Float(768:1, 768:768) = aten::t(%1137), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %output.70 : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.118, %1138), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1676:0
  %input.119 : Float(119:9984, 13:768, 768:1) = aten::add_(%output.70, %1136, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.23 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.119, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.dropout # torch/nn/functional.py:973:0
  %input.120 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.23, %input.115, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output # transformers/modeling_bert.py:295:0
  %1143 : Tensor = prim::GetAttr[name="bias"](%1134)
  %1144 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1145 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm
  %input_tensor : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.120, %1145, %1144, %1143, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.attention/__module.bert.encoder.layer.11.attention.output/__module.bert.encoder.layer.11.attention.output.LayerNorm # torch/nn/functional.py:2048:0
  %1147 : __torch__.torch.nn.modules.linear.___torch_mangle_3292.Linear = prim::GetAttr[name="dense"](%1080)
  %1148 : Tensor = prim::GetAttr[name="bias"](%1147)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1147)
  %1150 : Float(768:1, 3072:768) = aten::t(%1149), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %output.71 : Float(119:39936, 13:3072, 3072:1) = aten::matmul(%input_tensor, %1150), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1676:0
  %input.121 : Float(119:39936, 13:3072, 3072:1) = aten::add_(%output.71, %1148, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate/__module.bert.encoder.layer.11.intermediate.dense # torch/nn/functional.py:1678:0
  %input.122 : Float(119:39936, 13:3072, 3072:1) = aten::gelu(%input.121), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.intermediate # torch/nn/functional.py:1369:0
  %1154 : __torch__.torch.nn.modules.normalization.___torch_mangle_3295.LayerNorm = prim::GetAttr[name="LayerNorm"](%1079)
  %1155 : __torch__.torch.nn.modules.linear.___torch_mangle_3294.Linear = prim::GetAttr[name="dense"](%1079)
  %1156 : Tensor = prim::GetAttr[name="bias"](%1155)
  %1157 : Tensor = prim::GetAttr[name="weight"](%1155)
  %1158 : Float(3072:1, 768:3072) = aten::t(%1157), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %output : Float(119:9984, 13:768, 768:1) = aten::matmul(%input.122, %1158), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1676:0
  %input.123 : Float(119:9984, 13:768, 768:1) = aten::add_(%output, %1156, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dense # torch/nn/functional.py:1678:0
  %hidden_states.24 : Float(119:9984, 13:768, 768:1) = aten::dropout(%input.123, %39, %47), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.dropout # torch/nn/functional.py:973:0
  %input.124 : Float(119:9984, 13:768, 768:1) = aten::add(%hidden_states.24, %input_tensor, %50), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output # transformers/modeling_bert.py:371:0
  %1163 : Tensor = prim::GetAttr[name="bias"](%1154)
  %1164 : Tensor = prim::GetAttr[name="weight"](%1154)
  %1165 : int[] = prim::ListConstruct(%38), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm
  %hidden_states : Float(119:9984, 13:768, 768:1) = aten::layer_norm(%input.124, %1165, %1164, %1163, %37, %36), scope: __module.bert/__module.bert.encoder/__module.bert.encoder.layer.11/__module.bert.encoder.layer.11.output/__module.bert.encoder.layer.11.output.LayerNorm # torch/nn/functional.py:2048:0
  %1167 : __torch__.torch.nn.modules.linear.___torch_mangle_3301.Linear = prim::GetAttr[name="dense"](%52)
  %1168 : Float(119:9984, 13:768, 768:1) = aten::slice(%hidden_states, %51, %51, %46, %50), scope: __module.bert/__module.bert.pooler # transformers/modeling_bert.py:507:0
  %input.125 : Float(119:9984, 768:1) = aten::select(%1168, %50, %51), scope: __module.bert/__module.bert.pooler # transformers/modeling_bert.py:507:0
  %1170 : Tensor = prim::GetAttr[name="bias"](%1167)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1167)
  %1172 : Float(768:1, 768:768) = aten::t(%1171), scope: __module.bert/__module.bert.pooler/__module.bert.pooler.dense # torch/nn/functional.py:1674:0
  %input.126 : Float(119:768, 768:1) = aten::addmm(%1170, %input.125, %1172, %50, %50), scope: __module.bert/__module.bert.pooler/__module.bert.pooler.dense # torch/nn/functional.py:1674:0
  %input.127 : Float(119:768, 768:1) = aten::tanh(%input.126), scope: __module.bert/__module.bert.pooler/__module.bert.pooler.activation # torch/nn/modules/activation.py:350:0
  %1175 : bool = prim::Constant[value=0](), scope: __module.dropout # torch/nn/functional.py:973:0
  %1176 : float = prim::Constant[value=0.10000000000000001](), scope: __module.dropout # torch/nn/functional.py:973:0
  %input : Float(119:768, 768:1) = aten::dropout(%input.127, %1176, %1175), scope: __module.dropout # torch/nn/functional.py:973:0
  %1178 : int = prim::Constant[value=1](), scope: __module.classifier # torch/nn/functional.py:1674:0
  %1179 : Tensor = prim::GetAttr[name="bias"](%3)
  %1180 : Tensor = prim::GetAttr[name="weight"](%3)
  %1181 : Float(768:1, 1:768) = aten::t(%1180), scope: __module.classifier # torch/nn/functional.py:1674:0
  %logits : Float(119:1, 1:1) = aten::addmm(%1179, %input, %1181, %1178, %1178), scope: __module.classifier # torch/nn/functional.py:1674:0
  %27 : int = prim::Constant[value=-1]() # transformers/modeling_bert.py:1444:0
  %28 : int[] = prim::ListConstruct(%27, %9)
  %29 : Float(17:7, 7:1) = aten::view(%logits, %28) # transformers/modeling_bert.py:1444:0
  %30 : (Float(17:7, 7:1)) = prim::TupleConstruct(%29)
  return (%30)
