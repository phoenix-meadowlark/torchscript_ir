graph(%self : __torch__.torchvision.models.inception.Inception3,
      %x.1 : Tensor):
  %2 : int = prim::Constant[value=2]()
  %3 : str = prim::Constant[value="Scripted Inception3 always returns Inception3 Tuple"]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:203:30
  %4 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:200:22
  %14 : float = prim::Constant[value=-0.18799999999999994]()
  %15 : float = prim::Constant[value=0.45000000000000001]()
  %16 : float = prim::Constant[value=-0.087999999999999967]()
  %17 : float = prim::Constant[value=0.44800000000000001]()
  %18 : float = prim::Constant[value=-0.030000000000000027]()
  %19 : float = prim::Constant[value=0.45800000000000002]()
  %20 : int = prim::Constant[value=2]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:41
  %21 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:36
  %22 : int = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:41
  %23 : int = prim::Constant[value=9223372036854775807]()
  %24 : bool = prim::GetAttr[name="transform_input"](%self)
  %x.224 : Tensor = prim::If(%24) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:128:8
    block0():
      %26 : Tensor = aten::slice(%x.1, %22, %22, %23, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:36
      %27 : Tensor = aten::select(%26, %21, %22) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:36
      %28 : Tensor = aten::unsqueeze(%27, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:20
      %29 : Tensor = aten::mul(%28, %19) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:20
      %x_ch0.1 : Tensor = aten::add(%29, %18, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:129:20
      %31 : Tensor = aten::slice(%x.1, %22, %22, %23, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:130:36
      %32 : Tensor = aten::select(%31, %21, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:130:36
      %33 : Tensor = aten::unsqueeze(%32, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:130:20
      %34 : Tensor = aten::mul(%33, %17) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:130:20
      %x_ch1.1 : Tensor = aten::add(%34, %16, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:130:20
      %36 : Tensor = aten::slice(%x.1, %22, %22, %23, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:36
      %37 : Tensor = aten::select(%36, %21, %20) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:36
      %38 : Tensor = aten::unsqueeze(%37, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:20
      %39 : Tensor = aten::mul(%38, %15) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:20
      %x_ch2.1 : Tensor = aten::add(%39, %14, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:131:20
      %41 : Tensor[] = prim::ListConstruct(%x_ch0.1, %x_ch1.1, %x_ch2.1)
      %x.222 : Tensor = aten::cat(%41, %21) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:132:16
      -> (%x.222)
    block1():
      -> (%x.1)
  %43 : float = prim::Constant[value=0.5]() # torch/nn/modules/dropout.py:58:32
  %44 : int[] = prim::Constant[value=[0, 0]]()
  %45 : int = prim::Constant[value=5]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:415:40
  %46 : int = prim::Constant[value=3]() # torch/nn/modules/pooling.py:157:35
  %47 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %48 : int = prim::Constant[value=0]() # torch/nn/modules/conv.py:414:34
  %49 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %50 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %51 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %exponential_average_factor.6 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %53 : float = prim::Constant[value=0.001]() # torch/nn/modules/batchnorm.py:136:77
  %54 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:184:29
  %55 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:167:22
  %aux.2 : None = prim::Constant() # torch/hub/pytorch_vision_master/torchvision/models/inception.py:171:18
  %57 : int = prim::Constant[value=-1]()
  %58 : __torch__.torchvision.models.inception.BasicConv2d = prim::GetAttr[name="Conv2d_1a_3x3"](%self)
  %59 : __torch__.torch.nn.modules.conv.___torch_mangle_531.Conv2d = prim::GetAttr[name="conv"](%58)
  %60 : Tensor = prim::GetAttr[name="weight"](%59)
  %61 : Tensor? = prim::GetAttr[name="bias"](%59)
  %62 : int[] = prim::ListConstruct(%47, %47)
  %63 : int[] = prim::ListConstruct(%48, %48)
  %64 : int[] = prim::ListConstruct(%54, %54)
  %x.30 : Tensor = aten::conv2d(%x.224, %60, %61, %62, %63, %64, %54) # torch/nn/modules/conv.py:415:15
  %66 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%58)
  %67 : int = aten::dim(%x.30) # torch/nn/modules/batchnorm.py:276:11
  %68 : bool = aten::ne(%67, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%68) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %69 : bool = prim::GetAttr[name="training"](%66)
   = prim::If(%69) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %70 : Tensor = prim::GetAttr[name="num_batches_tracked"](%66)
      %71 : Tensor = aten::add(%70, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%66, %71)
      -> ()
    block1():
      -> ()
  %72 : bool = prim::GetAttr[name="training"](%66)
  %73 : Tensor = prim::GetAttr[name="running_mean"](%66)
  %74 : Tensor = prim::GetAttr[name="running_var"](%66)
  %75 : Tensor = prim::GetAttr[name="weight"](%66)
  %76 : Tensor = prim::GetAttr[name="bias"](%66)
   = prim::If(%72) # torch/nn/functional.py:2011:4
    block0():
      %77 : int[] = aten::size(%x.30) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%77, %48) # torch/nn/functional.py:1991:17
      %79 : int = aten::len(%77) # torch/nn/functional.py:1992:19
      %80 : int = aten::sub(%79, %47) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%80, %49, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %84 : int = aten::add(%i.39, %47) # torch/nn/functional.py:1993:27
          %85 : int = aten::__getitem__(%77, %84) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %85) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.155)
      %87 : bool = aten::eq(%size_prods.153, %54) # torch/nn/functional.py:1994:7
       = prim::If(%87) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.33 : Tensor = aten::batch_norm(%x.30, %75, %76, %73, %74, %72, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %x.24 : Tensor = aten::relu_(%x.33) # torch/nn/functional.py:1117:17
  %90 : __torch__.torchvision.models.inception.___torch_mangle_533.BasicConv2d = prim::GetAttr[name="Conv2d_2a_3x3"](%self)
  %91 : __torch__.torch.nn.modules.conv.___torch_mangle_532.Conv2d = prim::GetAttr[name="conv"](%90)
  %92 : Tensor = prim::GetAttr[name="weight"](%91)
  %93 : Tensor? = prim::GetAttr[name="bias"](%91)
  %94 : int[] = prim::ListConstruct(%54, %54)
  %95 : int[] = prim::ListConstruct(%48, %48)
  %96 : int[] = prim::ListConstruct(%54, %54)
  %x.37 : Tensor = aten::conv2d(%x.24, %92, %93, %94, %95, %96, %54) # torch/nn/modules/conv.py:415:15
  %98 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%90)
  %99 : int = aten::dim(%x.37) # torch/nn/modules/batchnorm.py:276:11
  %100 : bool = aten::ne(%99, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%100) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %101 : bool = prim::GetAttr[name="training"](%98)
   = prim::If(%101) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %102 : Tensor = prim::GetAttr[name="num_batches_tracked"](%98)
      %103 : Tensor = aten::add(%102, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%98, %103)
      -> ()
    block1():
      -> ()
  %104 : bool = prim::GetAttr[name="training"](%98)
  %105 : Tensor = prim::GetAttr[name="running_mean"](%98)
  %106 : Tensor = prim::GetAttr[name="running_var"](%98)
  %107 : Tensor = prim::GetAttr[name="weight"](%98)
  %108 : Tensor = prim::GetAttr[name="bias"](%98)
   = prim::If(%104) # torch/nn/functional.py:2011:4
    block0():
      %109 : int[] = aten::size(%x.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%109, %48) # torch/nn/functional.py:1991:17
      %111 : int = aten::len(%109) # torch/nn/functional.py:1992:19
      %112 : int = aten::sub(%111, %47) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%112, %49, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %116 : int = aten::add(%i.11, %47) # torch/nn/functional.py:1993:27
          %117 : int = aten::__getitem__(%109, %116) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %117) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.43)
      %119 : bool = aten::eq(%size_prods.41, %54) # torch/nn/functional.py:1994:7
       = prim::If(%119) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.41 : Tensor = aten::batch_norm(%x.37, %107, %108, %105, %106, %104, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %x.26 : Tensor = aten::relu_(%x.41) # torch/nn/functional.py:1117:17
  %122 : __torch__.torchvision.models.inception.___torch_mangle_534.BasicConv2d = prim::GetAttr[name="Conv2d_2b_3x3"](%self)
  %123 : __torch__.torch.nn.modules.conv.___torch_mangle_481.Conv2d = prim::GetAttr[name="conv"](%122)
  %124 : Tensor = prim::GetAttr[name="weight"](%123)
  %125 : Tensor? = prim::GetAttr[name="bias"](%123)
  %126 : int[] = prim::ListConstruct(%54, %54)
  %127 : int[] = prim::ListConstruct(%54, %54)
  %128 : int[] = prim::ListConstruct(%54, %54)
  %x.45 : Tensor = aten::conv2d(%x.26, %124, %125, %126, %127, %128, %54) # torch/nn/modules/conv.py:415:15
  %130 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%122)
  %131 : int = aten::dim(%x.45) # torch/nn/modules/batchnorm.py:276:11
  %132 : bool = aten::ne(%131, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%132) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %133 : bool = prim::GetAttr[name="training"](%130)
   = prim::If(%133) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %134 : Tensor = prim::GetAttr[name="num_batches_tracked"](%130)
      %135 : Tensor = aten::add(%134, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%130, %135)
      -> ()
    block1():
      -> ()
  %136 : bool = prim::GetAttr[name="training"](%130)
  %137 : Tensor = prim::GetAttr[name="running_mean"](%130)
  %138 : Tensor = prim::GetAttr[name="running_var"](%130)
  %139 : Tensor = prim::GetAttr[name="weight"](%130)
  %140 : Tensor = prim::GetAttr[name="bias"](%130)
   = prim::If(%136) # torch/nn/functional.py:2011:4
    block0():
      %141 : int[] = aten::size(%x.45) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%141, %48) # torch/nn/functional.py:1991:17
      %143 : int = aten::len(%141) # torch/nn/functional.py:1992:19
      %144 : int = aten::sub(%143, %47) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%144, %49, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %148 : int = aten::add(%i.37, %47) # torch/nn/functional.py:1993:27
          %149 : int = aten::__getitem__(%141, %148) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %149) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.147)
      %151 : bool = aten::eq(%size_prods.145, %54) # torch/nn/functional.py:1994:7
       = prim::If(%151) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.98 : Tensor = aten::batch_norm(%x.45, %139, %140, %137, %138, %136, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %x.28 : Tensor = aten::relu_(%x.98) # torch/nn/functional.py:1117:17
  %154 : int[] = prim::ListConstruct(%46, %46)
  %155 : int[] = prim::ListConstruct(%47, %47)
  %156 : int[] = prim::ListConstruct(%48, %48)
  %157 : int[] = prim::ListConstruct(%54, %54)
  %x.32 : Tensor = aten::max_pool2d(%x.28, %154, %155, %156, %157, %55) # torch/nn/functional.py:575:11
  %159 : __torch__.torchvision.models.inception.___torch_mangle_537.BasicConv2d = prim::GetAttr[name="Conv2d_3b_1x1"](%self)
  %160 : __torch__.torch.nn.modules.conv.___torch_mangle_535.Conv2d = prim::GetAttr[name="conv"](%159)
  %161 : Tensor = prim::GetAttr[name="weight"](%160)
  %162 : Tensor? = prim::GetAttr[name="bias"](%160)
  %163 : int[] = prim::ListConstruct(%54, %54)
  %164 : int[] = prim::ListConstruct(%48, %48)
  %165 : int[] = prim::ListConstruct(%54, %54)
  %x.100 : Tensor = aten::conv2d(%x.32, %161, %162, %163, %164, %165, %54) # torch/nn/modules/conv.py:415:15
  %167 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_536.BatchNorm2d = prim::GetAttr[name="bn"](%159)
  %168 : int = aten::dim(%x.100) # torch/nn/modules/batchnorm.py:276:11
  %169 : bool = aten::ne(%168, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%169) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %170 : bool = prim::GetAttr[name="training"](%167)
   = prim::If(%170) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %171 : Tensor = prim::GetAttr[name="num_batches_tracked"](%167)
      %172 : Tensor = aten::add(%171, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%167, %172)
      -> ()
    block1():
      -> ()
  %173 : bool = prim::GetAttr[name="training"](%167)
  %174 : Tensor = prim::GetAttr[name="running_mean"](%167)
  %175 : Tensor = prim::GetAttr[name="running_var"](%167)
  %176 : Tensor = prim::GetAttr[name="weight"](%167)
  %177 : Tensor = prim::GetAttr[name="bias"](%167)
   = prim::If(%173) # torch/nn/functional.py:2011:4
    block0():
      %178 : int[] = aten::size(%x.100) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%178, %48) # torch/nn/functional.py:1991:17
      %180 : int = aten::len(%178) # torch/nn/functional.py:1992:19
      %181 : int = aten::sub(%180, %47) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%181, %49, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %185 : int = aten::add(%i.38, %47) # torch/nn/functional.py:1993:27
          %186 : int = aten::__getitem__(%178, %185) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %186) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.151)
      %188 : bool = aten::eq(%size_prods.149, %54) # torch/nn/functional.py:1994:7
       = prim::If(%188) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.102 : Tensor = aten::batch_norm(%x.100, %176, %177, %174, %175, %173, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %x.35 : Tensor = aten::relu_(%x.102) # torch/nn/functional.py:1117:17
  %191 : __torch__.torchvision.models.inception.___torch_mangle_539.BasicConv2d = prim::GetAttr[name="Conv2d_4a_3x3"](%self)
  %192 : __torch__.torch.nn.modules.conv.___torch_mangle_538.Conv2d = prim::GetAttr[name="conv"](%191)
  %193 : Tensor = prim::GetAttr[name="weight"](%192)
  %194 : Tensor? = prim::GetAttr[name="bias"](%192)
  %195 : int[] = prim::ListConstruct(%54, %54)
  %196 : int[] = prim::ListConstruct(%48, %48)
  %197 : int[] = prim::ListConstruct(%54, %54)
  %x.47 : Tensor = aten::conv2d(%x.35, %193, %194, %195, %196, %197, %54) # torch/nn/modules/conv.py:415:15
  %199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%191)
  %200 : int = aten::dim(%x.47) # torch/nn/modules/batchnorm.py:276:11
  %201 : bool = aten::ne(%200, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%201) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %202 : bool = prim::GetAttr[name="training"](%199)
   = prim::If(%202) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %203 : Tensor = prim::GetAttr[name="num_batches_tracked"](%199)
      %204 : Tensor = aten::add(%203, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%199, %204)
      -> ()
    block1():
      -> ()
  %205 : bool = prim::GetAttr[name="training"](%199)
  %206 : Tensor = prim::GetAttr[name="running_mean"](%199)
  %207 : Tensor = prim::GetAttr[name="running_var"](%199)
  %208 : Tensor = prim::GetAttr[name="weight"](%199)
  %209 : Tensor = prim::GetAttr[name="bias"](%199)
   = prim::If(%205) # torch/nn/functional.py:2011:4
    block0():
      %210 : int[] = aten::size(%x.47) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%210, %48) # torch/nn/functional.py:1991:17
      %212 : int = aten::len(%210) # torch/nn/functional.py:1992:19
      %213 : int = aten::sub(%212, %47) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%213, %49, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %217 : int = aten::add(%i.12, %47) # torch/nn/functional.py:1993:27
          %218 : int = aten::__getitem__(%210, %217) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %218) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.47)
      %220 : bool = aten::eq(%size_prods.45, %54) # torch/nn/functional.py:1994:7
       = prim::If(%220) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.48 : Tensor = aten::batch_norm(%x.47, %208, %209, %206, %207, %205, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %x.39 : Tensor = aten::relu_(%x.48) # torch/nn/functional.py:1117:17
  %223 : int[] = prim::ListConstruct(%46, %46)
  %224 : int[] = prim::ListConstruct(%47, %47)
  %225 : int[] = prim::ListConstruct(%48, %48)
  %226 : int[] = prim::ListConstruct(%54, %54)
  %x.43 : Tensor = aten::max_pool2d(%x.39, %223, %224, %225, %226, %55) # torch/nn/functional.py:575:11
  %228 : __torch__.torchvision.models.inception.InceptionA = prim::GetAttr[name="Mixed_5b"](%self)
  %229 : __torch__.torchvision.models.inception.___torch_mangle_540.BasicConv2d = prim::GetAttr[name="branch1x1"](%228)
  %230 : __torch__.torch.nn.modules.conv.___torch_mangle_397.Conv2d = prim::GetAttr[name="conv"](%229)
  %231 : Tensor = prim::GetAttr[name="weight"](%230)
  %232 : Tensor? = prim::GetAttr[name="bias"](%230)
  %233 : int[] = prim::ListConstruct(%54, %54)
  %234 : int[] = prim::ListConstruct(%48, %48)
  %235 : int[] = prim::ListConstruct(%54, %54)
  %x.49 : Tensor = aten::conv2d(%x.43, %231, %232, %233, %234, %235, %54) # torch/nn/modules/conv.py:415:15
  %237 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%229)
  %238 : int = aten::dim(%x.49) # torch/nn/modules/batchnorm.py:276:11
  %239 : bool = aten::ne(%238, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%239) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %240 : bool = prim::GetAttr[name="training"](%237)
   = prim::If(%240) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %241 : Tensor = prim::GetAttr[name="num_batches_tracked"](%237)
      %242 : Tensor = aten::add(%241, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%237, %242)
      -> ()
    block1():
      -> ()
  %243 : bool = prim::GetAttr[name="training"](%237)
  %244 : Tensor = prim::GetAttr[name="running_mean"](%237)
  %245 : Tensor = prim::GetAttr[name="running_var"](%237)
  %246 : Tensor = prim::GetAttr[name="weight"](%237)
  %247 : Tensor = prim::GetAttr[name="bias"](%237)
   = prim::If(%243) # torch/nn/functional.py:2011:4
    block0():
      %248 : int[] = aten::size(%x.49) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%248, %48) # torch/nn/functional.py:1991:17
      %250 : int = aten::len(%248) # torch/nn/functional.py:1992:19
      %251 : int = aten::sub(%250, %47) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%251, %49, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %255 : int = aten::add(%i.13, %47) # torch/nn/functional.py:1993:27
          %256 : int = aten::__getitem__(%248, %255) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %256) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.51)
      %258 : bool = aten::eq(%size_prods.49, %54) # torch/nn/functional.py:1994:7
       = prim::If(%258) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.50 : Tensor = aten::batch_norm(%x.49, %246, %247, %244, %245, %243, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.2 : Tensor = aten::relu_(%x.50) # torch/nn/functional.py:1117:17
  %261 : __torch__.torchvision.models.inception.___torch_mangle_542.BasicConv2d = prim::GetAttr[name="branch5x5_1"](%228)
  %262 : __torch__.torch.nn.modules.conv.___torch_mangle_541.Conv2d = prim::GetAttr[name="conv"](%261)
  %263 : Tensor = prim::GetAttr[name="weight"](%262)
  %264 : Tensor? = prim::GetAttr[name="bias"](%262)
  %265 : int[] = prim::ListConstruct(%54, %54)
  %266 : int[] = prim::ListConstruct(%48, %48)
  %267 : int[] = prim::ListConstruct(%54, %54)
  %x.51 : Tensor = aten::conv2d(%x.43, %263, %264, %265, %266, %267, %54) # torch/nn/modules/conv.py:415:15
  %269 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_439.BatchNorm2d = prim::GetAttr[name="bn"](%261)
  %270 : int = aten::dim(%x.51) # torch/nn/modules/batchnorm.py:276:11
  %271 : bool = aten::ne(%270, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%271) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %272 : bool = prim::GetAttr[name="training"](%269)
   = prim::If(%272) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %273 : Tensor = prim::GetAttr[name="num_batches_tracked"](%269)
      %274 : Tensor = aten::add(%273, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%269, %274)
      -> ()
    block1():
      -> ()
  %275 : bool = prim::GetAttr[name="training"](%269)
  %276 : Tensor = prim::GetAttr[name="running_mean"](%269)
  %277 : Tensor = prim::GetAttr[name="running_var"](%269)
  %278 : Tensor = prim::GetAttr[name="weight"](%269)
  %279 : Tensor = prim::GetAttr[name="bias"](%269)
   = prim::If(%275) # torch/nn/functional.py:2011:4
    block0():
      %280 : int[] = aten::size(%x.51) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%280, %48) # torch/nn/functional.py:1991:17
      %282 : int = aten::len(%280) # torch/nn/functional.py:1992:19
      %283 : int = aten::sub(%282, %47) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%283, %49, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %287 : int = aten::add(%i.14, %47) # torch/nn/functional.py:1993:27
          %288 : int = aten::__getitem__(%280, %287) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %288) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.55)
      %290 : bool = aten::eq(%size_prods.53, %54) # torch/nn/functional.py:1994:7
       = prim::If(%290) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.52 : Tensor = aten::batch_norm(%x.51, %278, %279, %276, %277, %275, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.2 : Tensor = aten::relu_(%x.52) # torch/nn/functional.py:1117:17
  %293 : __torch__.torchvision.models.inception.___torch_mangle_544.BasicConv2d = prim::GetAttr[name="branch5x5_2"](%228)
  %294 : __torch__.torch.nn.modules.conv.___torch_mangle_543.Conv2d = prim::GetAttr[name="conv"](%293)
  %295 : Tensor = prim::GetAttr[name="weight"](%294)
  %296 : Tensor? = prim::GetAttr[name="bias"](%294)
  %297 : int[] = prim::ListConstruct(%54, %54)
  %298 : int[] = prim::ListConstruct(%47, %47)
  %299 : int[] = prim::ListConstruct(%54, %54)
  %x.53 : Tensor = aten::conv2d(%branch5x5.2, %295, %296, %297, %298, %299, %54) # torch/nn/modules/conv.py:415:15
  %301 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%293)
  %302 : int = aten::dim(%x.53) # torch/nn/modules/batchnorm.py:276:11
  %303 : bool = aten::ne(%302, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%303) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %304 : bool = prim::GetAttr[name="training"](%301)
   = prim::If(%304) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %305 : Tensor = prim::GetAttr[name="num_batches_tracked"](%301)
      %306 : Tensor = aten::add(%305, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%301, %306)
      -> ()
    block1():
      -> ()
  %307 : bool = prim::GetAttr[name="training"](%301)
  %308 : Tensor = prim::GetAttr[name="running_mean"](%301)
  %309 : Tensor = prim::GetAttr[name="running_var"](%301)
  %310 : Tensor = prim::GetAttr[name="weight"](%301)
  %311 : Tensor = prim::GetAttr[name="bias"](%301)
   = prim::If(%307) # torch/nn/functional.py:2011:4
    block0():
      %312 : int[] = aten::size(%x.53) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%312, %48) # torch/nn/functional.py:1991:17
      %314 : int = aten::len(%312) # torch/nn/functional.py:1992:19
      %315 : int = aten::sub(%314, %47) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%315, %49, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %319 : int = aten::add(%i.15, %47) # torch/nn/functional.py:1993:27
          %320 : int = aten::__getitem__(%312, %319) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %320) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.59)
      %322 : bool = aten::eq(%size_prods.57, %54) # torch/nn/functional.py:1994:7
       = prim::If(%322) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.54 : Tensor = aten::batch_norm(%x.53, %310, %311, %308, %309, %307, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.4 : Tensor = aten::relu_(%x.54) # torch/nn/functional.py:1117:17
  %325 : __torch__.torchvision.models.inception.___torch_mangle_540.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%228)
  %326 : __torch__.torch.nn.modules.conv.___torch_mangle_397.Conv2d = prim::GetAttr[name="conv"](%325)
  %327 : Tensor = prim::GetAttr[name="weight"](%326)
  %328 : Tensor? = prim::GetAttr[name="bias"](%326)
  %329 : int[] = prim::ListConstruct(%54, %54)
  %330 : int[] = prim::ListConstruct(%48, %48)
  %331 : int[] = prim::ListConstruct(%54, %54)
  %x.55 : Tensor = aten::conv2d(%x.43, %327, %328, %329, %330, %331, %54) # torch/nn/modules/conv.py:415:15
  %333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%325)
  %334 : int = aten::dim(%x.55) # torch/nn/modules/batchnorm.py:276:11
  %335 : bool = aten::ne(%334, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%335) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %336 : bool = prim::GetAttr[name="training"](%333)
   = prim::If(%336) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %337 : Tensor = prim::GetAttr[name="num_batches_tracked"](%333)
      %338 : Tensor = aten::add(%337, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%333, %338)
      -> ()
    block1():
      -> ()
  %339 : bool = prim::GetAttr[name="training"](%333)
  %340 : Tensor = prim::GetAttr[name="running_mean"](%333)
  %341 : Tensor = prim::GetAttr[name="running_var"](%333)
  %342 : Tensor = prim::GetAttr[name="weight"](%333)
  %343 : Tensor = prim::GetAttr[name="bias"](%333)
   = prim::If(%339) # torch/nn/functional.py:2011:4
    block0():
      %344 : int[] = aten::size(%x.55) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%344, %48) # torch/nn/functional.py:1991:17
      %346 : int = aten::len(%344) # torch/nn/functional.py:1992:19
      %347 : int = aten::sub(%346, %47) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%347, %49, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %351 : int = aten::add(%i.16, %47) # torch/nn/functional.py:1993:27
          %352 : int = aten::__getitem__(%344, %351) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %352) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.63)
      %354 : bool = aten::eq(%size_prods.61, %54) # torch/nn/functional.py:1994:7
       = prim::If(%354) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.56 : Tensor = aten::batch_norm(%x.55, %342, %343, %340, %341, %339, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.2 : Tensor = aten::relu_(%x.56) # torch/nn/functional.py:1117:17
  %357 : __torch__.torchvision.models.inception.___torch_mangle_546.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%228)
  %358 : __torch__.torch.nn.modules.conv.___torch_mangle_545.Conv2d = prim::GetAttr[name="conv"](%357)
  %359 : Tensor = prim::GetAttr[name="weight"](%358)
  %360 : Tensor? = prim::GetAttr[name="bias"](%358)
  %361 : int[] = prim::ListConstruct(%54, %54)
  %362 : int[] = prim::ListConstruct(%54, %54)
  %363 : int[] = prim::ListConstruct(%54, %54)
  %x.57 : Tensor = aten::conv2d(%branch3x3dbl.2, %359, %360, %361, %362, %363, %54) # torch/nn/modules/conv.py:415:15
  %365 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%357)
  %366 : int = aten::dim(%x.57) # torch/nn/modules/batchnorm.py:276:11
  %367 : bool = aten::ne(%366, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%367) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %368 : bool = prim::GetAttr[name="training"](%365)
   = prim::If(%368) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %369 : Tensor = prim::GetAttr[name="num_batches_tracked"](%365)
      %370 : Tensor = aten::add(%369, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%365, %370)
      -> ()
    block1():
      -> ()
  %371 : bool = prim::GetAttr[name="training"](%365)
  %372 : Tensor = prim::GetAttr[name="running_mean"](%365)
  %373 : Tensor = prim::GetAttr[name="running_var"](%365)
  %374 : Tensor = prim::GetAttr[name="weight"](%365)
  %375 : Tensor = prim::GetAttr[name="bias"](%365)
   = prim::If(%371) # torch/nn/functional.py:2011:4
    block0():
      %376 : int[] = aten::size(%x.57) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%376, %48) # torch/nn/functional.py:1991:17
      %378 : int = aten::len(%376) # torch/nn/functional.py:1992:19
      %379 : int = aten::sub(%378, %47) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%379, %49, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %383 : int = aten::add(%i.17, %47) # torch/nn/functional.py:1993:27
          %384 : int = aten::__getitem__(%376, %383) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %384) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.67)
      %386 : bool = aten::eq(%size_prods.65, %54) # torch/nn/functional.py:1994:7
       = prim::If(%386) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.58 : Tensor = aten::batch_norm(%x.57, %374, %375, %372, %373, %371, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.4 : Tensor = aten::relu_(%x.58) # torch/nn/functional.py:1117:17
  %389 : __torch__.torchvision.models.inception.___torch_mangle_548.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3"](%228)
  %390 : __torch__.torch.nn.modules.conv.___torch_mangle_547.Conv2d = prim::GetAttr[name="conv"](%389)
  %391 : Tensor = prim::GetAttr[name="weight"](%390)
  %392 : Tensor? = prim::GetAttr[name="bias"](%390)
  %393 : int[] = prim::ListConstruct(%54, %54)
  %394 : int[] = prim::ListConstruct(%54, %54)
  %395 : int[] = prim::ListConstruct(%54, %54)
  %x.59 : Tensor = aten::conv2d(%branch3x3dbl.4, %391, %392, %393, %394, %395, %54) # torch/nn/modules/conv.py:415:15
  %397 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%389)
  %398 : int = aten::dim(%x.59) # torch/nn/modules/batchnorm.py:276:11
  %399 : bool = aten::ne(%398, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%399) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %400 : bool = prim::GetAttr[name="training"](%397)
   = prim::If(%400) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %401 : Tensor = prim::GetAttr[name="num_batches_tracked"](%397)
      %402 : Tensor = aten::add(%401, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%397, %402)
      -> ()
    block1():
      -> ()
  %403 : bool = prim::GetAttr[name="training"](%397)
  %404 : Tensor = prim::GetAttr[name="running_mean"](%397)
  %405 : Tensor = prim::GetAttr[name="running_var"](%397)
  %406 : Tensor = prim::GetAttr[name="weight"](%397)
  %407 : Tensor = prim::GetAttr[name="bias"](%397)
   = prim::If(%403) # torch/nn/functional.py:2011:4
    block0():
      %408 : int[] = aten::size(%x.59) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%408, %48) # torch/nn/functional.py:1991:17
      %410 : int = aten::len(%408) # torch/nn/functional.py:1992:19
      %411 : int = aten::sub(%410, %47) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%411, %49, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %415 : int = aten::add(%i.18, %47) # torch/nn/functional.py:1993:27
          %416 : int = aten::__getitem__(%408, %415) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %416) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.71)
      %418 : bool = aten::eq(%size_prods.69, %54) # torch/nn/functional.py:1994:7
       = prim::If(%418) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.60 : Tensor = aten::batch_norm(%x.59, %406, %407, %404, %405, %403, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.15 : Tensor = aten::relu_(%x.60) # torch/nn/functional.py:1117:17
  %421 : int[] = prim::ListConstruct(%46, %46)
  %422 : int[] = prim::ListConstruct(%54, %54)
  %423 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.2 : Tensor = aten::avg_pool2d(%x.43, %421, %422, %423, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:236:22
  %425 : __torch__.torchvision.models.inception.___torch_mangle_549.BasicConv2d = prim::GetAttr[name="branch_pool"](%228)
  %426 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="conv"](%425)
  %427 : Tensor = prim::GetAttr[name="weight"](%426)
  %428 : Tensor? = prim::GetAttr[name="bias"](%426)
  %429 : int[] = prim::ListConstruct(%54, %54)
  %430 : int[] = prim::ListConstruct(%48, %48)
  %431 : int[] = prim::ListConstruct(%54, %54)
  %x.61 : Tensor = aten::conv2d(%branch_pool.2, %427, %428, %429, %430, %431, %54) # torch/nn/modules/conv.py:415:15
  %433 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%425)
  %434 : int = aten::dim(%x.61) # torch/nn/modules/batchnorm.py:276:11
  %435 : bool = aten::ne(%434, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%435) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %436 : bool = prim::GetAttr[name="training"](%433)
   = prim::If(%436) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %437 : Tensor = prim::GetAttr[name="num_batches_tracked"](%433)
      %438 : Tensor = aten::add(%437, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%433, %438)
      -> ()
    block1():
      -> ()
  %439 : bool = prim::GetAttr[name="training"](%433)
  %440 : Tensor = prim::GetAttr[name="running_mean"](%433)
  %441 : Tensor = prim::GetAttr[name="running_var"](%433)
  %442 : Tensor = prim::GetAttr[name="weight"](%433)
  %443 : Tensor = prim::GetAttr[name="bias"](%433)
   = prim::If(%439) # torch/nn/functional.py:2011:4
    block0():
      %444 : int[] = aten::size(%x.61) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%444, %48) # torch/nn/functional.py:1991:17
      %446 : int = aten::len(%444) # torch/nn/functional.py:1992:19
      %447 : int = aten::sub(%446, %47) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%447, %49, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %451 : int = aten::add(%i.19, %47) # torch/nn/functional.py:1993:27
          %452 : int = aten::__getitem__(%444, %451) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %452) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.75)
      %454 : bool = aten::eq(%size_prods.73, %54) # torch/nn/functional.py:1994:7
       = prim::If(%454) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.62 : Tensor = aten::batch_norm(%x.61, %442, %443, %440, %441, %439, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.4 : Tensor = aten::relu_(%x.62) # torch/nn/functional.py:1117:17
  %outputs.3 : Tensor[] = prim::ListConstruct(%branch1x1.2, %branch5x5.4, %branch3x3dbl.15, %branch_pool.4)
  %x.97 : Tensor = aten::cat(%outputs.3, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:244:15
  %459 : __torch__.torchvision.models.inception.___torch_mangle_553.InceptionA = prim::GetAttr[name="Mixed_5c"](%self)
  %460 : __torch__.torchvision.models.inception.___torch_mangle_550.BasicConv2d = prim::GetAttr[name="branch1x1"](%459)
  %461 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv"](%460)
  %462 : Tensor = prim::GetAttr[name="weight"](%461)
  %463 : Tensor? = prim::GetAttr[name="bias"](%461)
  %464 : int[] = prim::ListConstruct(%54, %54)
  %465 : int[] = prim::ListConstruct(%48, %48)
  %466 : int[] = prim::ListConstruct(%54, %54)
  %x.63 : Tensor = aten::conv2d(%x.97, %462, %463, %464, %465, %466, %54) # torch/nn/modules/conv.py:415:15
  %468 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%460)
  %469 : int = aten::dim(%x.63) # torch/nn/modules/batchnorm.py:276:11
  %470 : bool = aten::ne(%469, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%470) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %471 : bool = prim::GetAttr[name="training"](%468)
   = prim::If(%471) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %472 : Tensor = prim::GetAttr[name="num_batches_tracked"](%468)
      %473 : Tensor = aten::add(%472, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%468, %473)
      -> ()
    block1():
      -> ()
  %474 : bool = prim::GetAttr[name="training"](%468)
  %475 : Tensor = prim::GetAttr[name="running_mean"](%468)
  %476 : Tensor = prim::GetAttr[name="running_var"](%468)
  %477 : Tensor = prim::GetAttr[name="weight"](%468)
  %478 : Tensor = prim::GetAttr[name="bias"](%468)
   = prim::If(%474) # torch/nn/functional.py:2011:4
    block0():
      %479 : int[] = aten::size(%x.63) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%479, %48) # torch/nn/functional.py:1991:17
      %481 : int = aten::len(%479) # torch/nn/functional.py:1992:19
      %482 : int = aten::sub(%481, %47) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%482, %49, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %486 : int = aten::add(%i.20, %47) # torch/nn/functional.py:1993:27
          %487 : int = aten::__getitem__(%479, %486) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %487) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.79)
      %489 : bool = aten::eq(%size_prods.77, %54) # torch/nn/functional.py:1994:7
       = prim::If(%489) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.64 : Tensor = aten::batch_norm(%x.63, %477, %478, %475, %476, %474, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.3 : Tensor = aten::relu_(%x.64) # torch/nn/functional.py:1117:17
  %492 : __torch__.torchvision.models.inception.___torch_mangle_552.BasicConv2d = prim::GetAttr[name="branch5x5_1"](%459)
  %493 : __torch__.torch.nn.modules.conv.___torch_mangle_551.Conv2d = prim::GetAttr[name="conv"](%492)
  %494 : Tensor = prim::GetAttr[name="weight"](%493)
  %495 : Tensor? = prim::GetAttr[name="bias"](%493)
  %496 : int[] = prim::ListConstruct(%54, %54)
  %497 : int[] = prim::ListConstruct(%48, %48)
  %498 : int[] = prim::ListConstruct(%54, %54)
  %x.65 : Tensor = aten::conv2d(%x.97, %494, %495, %496, %497, %498, %54) # torch/nn/modules/conv.py:415:15
  %500 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_439.BatchNorm2d = prim::GetAttr[name="bn"](%492)
  %501 : int = aten::dim(%x.65) # torch/nn/modules/batchnorm.py:276:11
  %502 : bool = aten::ne(%501, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%502) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %503 : bool = prim::GetAttr[name="training"](%500)
   = prim::If(%503) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %504 : Tensor = prim::GetAttr[name="num_batches_tracked"](%500)
      %505 : Tensor = aten::add(%504, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%500, %505)
      -> ()
    block1():
      -> ()
  %506 : bool = prim::GetAttr[name="training"](%500)
  %507 : Tensor = prim::GetAttr[name="running_mean"](%500)
  %508 : Tensor = prim::GetAttr[name="running_var"](%500)
  %509 : Tensor = prim::GetAttr[name="weight"](%500)
  %510 : Tensor = prim::GetAttr[name="bias"](%500)
   = prim::If(%506) # torch/nn/functional.py:2011:4
    block0():
      %511 : int[] = aten::size(%x.65) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%511, %48) # torch/nn/functional.py:1991:17
      %513 : int = aten::len(%511) # torch/nn/functional.py:1992:19
      %514 : int = aten::sub(%513, %47) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%514, %49, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %518 : int = aten::add(%i.21, %47) # torch/nn/functional.py:1993:27
          %519 : int = aten::__getitem__(%511, %518) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %519) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.83)
      %521 : bool = aten::eq(%size_prods.81, %54) # torch/nn/functional.py:1994:7
       = prim::If(%521) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.66 : Tensor = aten::batch_norm(%x.65, %509, %510, %507, %508, %506, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.5 : Tensor = aten::relu_(%x.66) # torch/nn/functional.py:1117:17
  %524 : __torch__.torchvision.models.inception.___torch_mangle_544.BasicConv2d = prim::GetAttr[name="branch5x5_2"](%459)
  %525 : __torch__.torch.nn.modules.conv.___torch_mangle_543.Conv2d = prim::GetAttr[name="conv"](%524)
  %526 : Tensor = prim::GetAttr[name="weight"](%525)
  %527 : Tensor? = prim::GetAttr[name="bias"](%525)
  %528 : int[] = prim::ListConstruct(%54, %54)
  %529 : int[] = prim::ListConstruct(%47, %47)
  %530 : int[] = prim::ListConstruct(%54, %54)
  %x.67 : Tensor = aten::conv2d(%branch5x5.5, %526, %527, %528, %529, %530, %54) # torch/nn/modules/conv.py:415:15
  %532 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%524)
  %533 : int = aten::dim(%x.67) # torch/nn/modules/batchnorm.py:276:11
  %534 : bool = aten::ne(%533, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%534) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %535 : bool = prim::GetAttr[name="training"](%532)
   = prim::If(%535) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %536 : Tensor = prim::GetAttr[name="num_batches_tracked"](%532)
      %537 : Tensor = aten::add(%536, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%532, %537)
      -> ()
    block1():
      -> ()
  %538 : bool = prim::GetAttr[name="training"](%532)
  %539 : Tensor = prim::GetAttr[name="running_mean"](%532)
  %540 : Tensor = prim::GetAttr[name="running_var"](%532)
  %541 : Tensor = prim::GetAttr[name="weight"](%532)
  %542 : Tensor = prim::GetAttr[name="bias"](%532)
   = prim::If(%538) # torch/nn/functional.py:2011:4
    block0():
      %543 : int[] = aten::size(%x.67) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%543, %48) # torch/nn/functional.py:1991:17
      %545 : int = aten::len(%543) # torch/nn/functional.py:1992:19
      %546 : int = aten::sub(%545, %47) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%546, %49, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %550 : int = aten::add(%i.22, %47) # torch/nn/functional.py:1993:27
          %551 : int = aten::__getitem__(%543, %550) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %551) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.87)
      %553 : bool = aten::eq(%size_prods.85, %54) # torch/nn/functional.py:1994:7
       = prim::If(%553) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.68 : Tensor = aten::batch_norm(%x.67, %541, %542, %539, %540, %538, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.6 : Tensor = aten::relu_(%x.68) # torch/nn/functional.py:1117:17
  %556 : __torch__.torchvision.models.inception.___torch_mangle_550.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%459)
  %557 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv"](%556)
  %558 : Tensor = prim::GetAttr[name="weight"](%557)
  %559 : Tensor? = prim::GetAttr[name="bias"](%557)
  %560 : int[] = prim::ListConstruct(%54, %54)
  %561 : int[] = prim::ListConstruct(%48, %48)
  %562 : int[] = prim::ListConstruct(%54, %54)
  %x.69 : Tensor = aten::conv2d(%x.97, %558, %559, %560, %561, %562, %54) # torch/nn/modules/conv.py:415:15
  %564 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%556)
  %565 : int = aten::dim(%x.69) # torch/nn/modules/batchnorm.py:276:11
  %566 : bool = aten::ne(%565, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%566) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %567 : bool = prim::GetAttr[name="training"](%564)
   = prim::If(%567) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %568 : Tensor = prim::GetAttr[name="num_batches_tracked"](%564)
      %569 : Tensor = aten::add(%568, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%564, %569)
      -> ()
    block1():
      -> ()
  %570 : bool = prim::GetAttr[name="training"](%564)
  %571 : Tensor = prim::GetAttr[name="running_mean"](%564)
  %572 : Tensor = prim::GetAttr[name="running_var"](%564)
  %573 : Tensor = prim::GetAttr[name="weight"](%564)
  %574 : Tensor = prim::GetAttr[name="bias"](%564)
   = prim::If(%570) # torch/nn/functional.py:2011:4
    block0():
      %575 : int[] = aten::size(%x.69) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%575, %48) # torch/nn/functional.py:1991:17
      %577 : int = aten::len(%575) # torch/nn/functional.py:1992:19
      %578 : int = aten::sub(%577, %47) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%578, %49, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %582 : int = aten::add(%i.23, %47) # torch/nn/functional.py:1993:27
          %583 : int = aten::__getitem__(%575, %582) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %583) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.91)
      %585 : bool = aten::eq(%size_prods.89, %54) # torch/nn/functional.py:1994:7
       = prim::If(%585) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.70 : Tensor = aten::batch_norm(%x.69, %573, %574, %571, %572, %570, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.7 : Tensor = aten::relu_(%x.70) # torch/nn/functional.py:1117:17
  %588 : __torch__.torchvision.models.inception.___torch_mangle_546.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%459)
  %589 : __torch__.torch.nn.modules.conv.___torch_mangle_545.Conv2d = prim::GetAttr[name="conv"](%588)
  %590 : Tensor = prim::GetAttr[name="weight"](%589)
  %591 : Tensor? = prim::GetAttr[name="bias"](%589)
  %592 : int[] = prim::ListConstruct(%54, %54)
  %593 : int[] = prim::ListConstruct(%54, %54)
  %594 : int[] = prim::ListConstruct(%54, %54)
  %x.71 : Tensor = aten::conv2d(%branch3x3dbl.7, %590, %591, %592, %593, %594, %54) # torch/nn/modules/conv.py:415:15
  %596 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%588)
  %597 : int = aten::dim(%x.71) # torch/nn/modules/batchnorm.py:276:11
  %598 : bool = aten::ne(%597, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%598) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %599 : bool = prim::GetAttr[name="training"](%596)
   = prim::If(%599) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %600 : Tensor = prim::GetAttr[name="num_batches_tracked"](%596)
      %601 : Tensor = aten::add(%600, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%596, %601)
      -> ()
    block1():
      -> ()
  %602 : bool = prim::GetAttr[name="training"](%596)
  %603 : Tensor = prim::GetAttr[name="running_mean"](%596)
  %604 : Tensor = prim::GetAttr[name="running_var"](%596)
  %605 : Tensor = prim::GetAttr[name="weight"](%596)
  %606 : Tensor = prim::GetAttr[name="bias"](%596)
   = prim::If(%602) # torch/nn/functional.py:2011:4
    block0():
      %607 : int[] = aten::size(%x.71) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%607, %48) # torch/nn/functional.py:1991:17
      %609 : int = aten::len(%607) # torch/nn/functional.py:1992:19
      %610 : int = aten::sub(%609, %47) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%610, %49, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %614 : int = aten::add(%i.24, %47) # torch/nn/functional.py:1993:27
          %615 : int = aten::__getitem__(%607, %614) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %615) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.95)
      %617 : bool = aten::eq(%size_prods.93, %54) # torch/nn/functional.py:1994:7
       = prim::If(%617) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.72 : Tensor = aten::batch_norm(%x.71, %605, %606, %603, %604, %602, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.16 : Tensor = aten::relu_(%x.72) # torch/nn/functional.py:1117:17
  %620 : __torch__.torchvision.models.inception.___torch_mangle_548.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3"](%459)
  %621 : __torch__.torch.nn.modules.conv.___torch_mangle_547.Conv2d = prim::GetAttr[name="conv"](%620)
  %622 : Tensor = prim::GetAttr[name="weight"](%621)
  %623 : Tensor? = prim::GetAttr[name="bias"](%621)
  %624 : int[] = prim::ListConstruct(%54, %54)
  %625 : int[] = prim::ListConstruct(%54, %54)
  %626 : int[] = prim::ListConstruct(%54, %54)
  %x.73 : Tensor = aten::conv2d(%branch3x3dbl.16, %622, %623, %624, %625, %626, %54) # torch/nn/modules/conv.py:415:15
  %628 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%620)
  %629 : int = aten::dim(%x.73) # torch/nn/modules/batchnorm.py:276:11
  %630 : bool = aten::ne(%629, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%630) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %631 : bool = prim::GetAttr[name="training"](%628)
   = prim::If(%631) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %632 : Tensor = prim::GetAttr[name="num_batches_tracked"](%628)
      %633 : Tensor = aten::add(%632, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%628, %633)
      -> ()
    block1():
      -> ()
  %634 : bool = prim::GetAttr[name="training"](%628)
  %635 : Tensor = prim::GetAttr[name="running_mean"](%628)
  %636 : Tensor = prim::GetAttr[name="running_var"](%628)
  %637 : Tensor = prim::GetAttr[name="weight"](%628)
  %638 : Tensor = prim::GetAttr[name="bias"](%628)
   = prim::If(%634) # torch/nn/functional.py:2011:4
    block0():
      %639 : int[] = aten::size(%x.73) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%639, %48) # torch/nn/functional.py:1991:17
      %641 : int = aten::len(%639) # torch/nn/functional.py:1992:19
      %642 : int = aten::sub(%641, %47) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%642, %49, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %646 : int = aten::add(%i.25, %47) # torch/nn/functional.py:1993:27
          %647 : int = aten::__getitem__(%639, %646) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %647) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.99)
      %649 : bool = aten::eq(%size_prods.97, %54) # torch/nn/functional.py:1994:7
       = prim::If(%649) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.74 : Tensor = aten::batch_norm(%x.73, %637, %638, %635, %636, %634, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.9 : Tensor = aten::relu_(%x.74) # torch/nn/functional.py:1117:17
  %652 : int[] = prim::ListConstruct(%46, %46)
  %653 : int[] = prim::ListConstruct(%54, %54)
  %654 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.5 : Tensor = aten::avg_pool2d(%x.97, %652, %653, %654, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:236:22
  %656 : __torch__.torchvision.models.inception.___torch_mangle_550.BasicConv2d = prim::GetAttr[name="branch_pool"](%459)
  %657 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv"](%656)
  %658 : Tensor = prim::GetAttr[name="weight"](%657)
  %659 : Tensor? = prim::GetAttr[name="bias"](%657)
  %660 : int[] = prim::ListConstruct(%54, %54)
  %661 : int[] = prim::ListConstruct(%48, %48)
  %662 : int[] = prim::ListConstruct(%54, %54)
  %x.75 : Tensor = aten::conv2d(%branch_pool.5, %658, %659, %660, %661, %662, %54) # torch/nn/modules/conv.py:415:15
  %664 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%656)
  %665 : int = aten::dim(%x.75) # torch/nn/modules/batchnorm.py:276:11
  %666 : bool = aten::ne(%665, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%666) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %667 : bool = prim::GetAttr[name="training"](%664)
   = prim::If(%667) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %668 : Tensor = prim::GetAttr[name="num_batches_tracked"](%664)
      %669 : Tensor = aten::add(%668, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%664, %669)
      -> ()
    block1():
      -> ()
  %670 : bool = prim::GetAttr[name="training"](%664)
  %671 : Tensor = prim::GetAttr[name="running_mean"](%664)
  %672 : Tensor = prim::GetAttr[name="running_var"](%664)
  %673 : Tensor = prim::GetAttr[name="weight"](%664)
  %674 : Tensor = prim::GetAttr[name="bias"](%664)
   = prim::If(%670) # torch/nn/functional.py:2011:4
    block0():
      %675 : int[] = aten::size(%x.75) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%675, %48) # torch/nn/functional.py:1991:17
      %677 : int = aten::len(%675) # torch/nn/functional.py:1992:19
      %678 : int = aten::sub(%677, %47) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%678, %49, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %682 : int = aten::add(%i.26, %47) # torch/nn/functional.py:1993:27
          %683 : int = aten::__getitem__(%675, %682) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %683) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.103)
      %685 : bool = aten::eq(%size_prods.101, %54) # torch/nn/functional.py:1994:7
       = prim::If(%685) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.76 : Tensor = aten::batch_norm(%x.75, %673, %674, %671, %672, %670, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.6 : Tensor = aten::relu_(%x.76) # torch/nn/functional.py:1117:17
  %outputs.4 : Tensor[] = prim::ListConstruct(%branch1x1.3, %branch5x5.6, %branch3x3dbl.9, %branch_pool.6)
  %x.99 : Tensor = aten::cat(%outputs.4, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:244:15
  %690 : __torch__.torchvision.models.inception.___torch_mangle_558.InceptionA = prim::GetAttr[name="Mixed_5d"](%self)
  %691 : __torch__.torchvision.models.inception.___torch_mangle_555.BasicConv2d = prim::GetAttr[name="branch1x1"](%690)
  %692 : __torch__.torch.nn.modules.conv.___torch_mangle_554.Conv2d = prim::GetAttr[name="conv"](%691)
  %693 : Tensor = prim::GetAttr[name="weight"](%692)
  %694 : Tensor? = prim::GetAttr[name="bias"](%692)
  %695 : int[] = prim::ListConstruct(%54, %54)
  %696 : int[] = prim::ListConstruct(%48, %48)
  %697 : int[] = prim::ListConstruct(%54, %54)
  %x.77 : Tensor = aten::conv2d(%x.99, %693, %694, %695, %696, %697, %54) # torch/nn/modules/conv.py:415:15
  %699 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%691)
  %700 : int = aten::dim(%x.77) # torch/nn/modules/batchnorm.py:276:11
  %701 : bool = aten::ne(%700, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%701) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %702 : bool = prim::GetAttr[name="training"](%699)
   = prim::If(%702) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %703 : Tensor = prim::GetAttr[name="num_batches_tracked"](%699)
      %704 : Tensor = aten::add(%703, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%699, %704)
      -> ()
    block1():
      -> ()
  %705 : bool = prim::GetAttr[name="training"](%699)
  %706 : Tensor = prim::GetAttr[name="running_mean"](%699)
  %707 : Tensor = prim::GetAttr[name="running_var"](%699)
  %708 : Tensor = prim::GetAttr[name="weight"](%699)
  %709 : Tensor = prim::GetAttr[name="bias"](%699)
   = prim::If(%705) # torch/nn/functional.py:2011:4
    block0():
      %710 : int[] = aten::size(%x.77) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%710, %48) # torch/nn/functional.py:1991:17
      %712 : int = aten::len(%710) # torch/nn/functional.py:1992:19
      %713 : int = aten::sub(%712, %47) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%713, %49, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %717 : int = aten::add(%i.27, %47) # torch/nn/functional.py:1993:27
          %718 : int = aten::__getitem__(%710, %717) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %718) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.107)
      %720 : bool = aten::eq(%size_prods.105, %54) # torch/nn/functional.py:1994:7
       = prim::If(%720) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.78 : Tensor = aten::batch_norm(%x.77, %708, %709, %706, %707, %705, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.4 : Tensor = aten::relu_(%x.78) # torch/nn/functional.py:1117:17
  %723 : __torch__.torchvision.models.inception.___torch_mangle_557.BasicConv2d = prim::GetAttr[name="branch5x5_1"](%690)
  %724 : __torch__.torch.nn.modules.conv.___torch_mangle_556.Conv2d = prim::GetAttr[name="conv"](%723)
  %725 : Tensor = prim::GetAttr[name="weight"](%724)
  %726 : Tensor? = prim::GetAttr[name="bias"](%724)
  %727 : int[] = prim::ListConstruct(%54, %54)
  %728 : int[] = prim::ListConstruct(%48, %48)
  %729 : int[] = prim::ListConstruct(%54, %54)
  %x.79 : Tensor = aten::conv2d(%x.99, %725, %726, %727, %728, %729, %54) # torch/nn/modules/conv.py:415:15
  %731 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_439.BatchNorm2d = prim::GetAttr[name="bn"](%723)
  %732 : int = aten::dim(%x.79) # torch/nn/modules/batchnorm.py:276:11
  %733 : bool = aten::ne(%732, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%733) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %734 : bool = prim::GetAttr[name="training"](%731)
   = prim::If(%734) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %735 : Tensor = prim::GetAttr[name="num_batches_tracked"](%731)
      %736 : Tensor = aten::add(%735, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%731, %736)
      -> ()
    block1():
      -> ()
  %737 : bool = prim::GetAttr[name="training"](%731)
  %738 : Tensor = prim::GetAttr[name="running_mean"](%731)
  %739 : Tensor = prim::GetAttr[name="running_var"](%731)
  %740 : Tensor = prim::GetAttr[name="weight"](%731)
  %741 : Tensor = prim::GetAttr[name="bias"](%731)
   = prim::If(%737) # torch/nn/functional.py:2011:4
    block0():
      %742 : int[] = aten::size(%x.79) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%742, %48) # torch/nn/functional.py:1991:17
      %744 : int = aten::len(%742) # torch/nn/functional.py:1992:19
      %745 : int = aten::sub(%744, %47) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%745, %49, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %749 : int = aten::add(%i.28, %47) # torch/nn/functional.py:1993:27
          %750 : int = aten::__getitem__(%742, %749) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %750) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.111)
      %752 : bool = aten::eq(%size_prods.109, %54) # torch/nn/functional.py:1994:7
       = prim::If(%752) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.80 : Tensor = aten::batch_norm(%x.79, %740, %741, %738, %739, %737, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.1 : Tensor = aten::relu_(%x.80) # torch/nn/functional.py:1117:17
  %755 : __torch__.torchvision.models.inception.___torch_mangle_544.BasicConv2d = prim::GetAttr[name="branch5x5_2"](%690)
  %756 : __torch__.torch.nn.modules.conv.___torch_mangle_543.Conv2d = prim::GetAttr[name="conv"](%755)
  %757 : Tensor = prim::GetAttr[name="weight"](%756)
  %758 : Tensor? = prim::GetAttr[name="bias"](%756)
  %759 : int[] = prim::ListConstruct(%54, %54)
  %760 : int[] = prim::ListConstruct(%47, %47)
  %761 : int[] = prim::ListConstruct(%54, %54)
  %x.81 : Tensor = aten::conv2d(%branch5x5.1, %757, %758, %759, %760, %761, %54) # torch/nn/modules/conv.py:415:15
  %763 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%755)
  %764 : int = aten::dim(%x.81) # torch/nn/modules/batchnorm.py:276:11
  %765 : bool = aten::ne(%764, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%765) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %766 : bool = prim::GetAttr[name="training"](%763)
   = prim::If(%766) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %767 : Tensor = prim::GetAttr[name="num_batches_tracked"](%763)
      %768 : Tensor = aten::add(%767, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%763, %768)
      -> ()
    block1():
      -> ()
  %769 : bool = prim::GetAttr[name="training"](%763)
  %770 : Tensor = prim::GetAttr[name="running_mean"](%763)
  %771 : Tensor = prim::GetAttr[name="running_var"](%763)
  %772 : Tensor = prim::GetAttr[name="weight"](%763)
  %773 : Tensor = prim::GetAttr[name="bias"](%763)
   = prim::If(%769) # torch/nn/functional.py:2011:4
    block0():
      %774 : int[] = aten::size(%x.81) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%774, %48) # torch/nn/functional.py:1991:17
      %776 : int = aten::len(%774) # torch/nn/functional.py:1992:19
      %777 : int = aten::sub(%776, %47) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%777, %49, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %781 : int = aten::add(%i.29, %47) # torch/nn/functional.py:1993:27
          %782 : int = aten::__getitem__(%774, %781) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %782) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.115)
      %784 : bool = aten::eq(%size_prods.113, %54) # torch/nn/functional.py:1994:7
       = prim::If(%784) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.82 : Tensor = aten::batch_norm(%x.81, %772, %773, %770, %771, %769, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch5x5.3 : Tensor = aten::relu_(%x.82) # torch/nn/functional.py:1117:17
  %787 : __torch__.torchvision.models.inception.___torch_mangle_555.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%690)
  %788 : __torch__.torch.nn.modules.conv.___torch_mangle_554.Conv2d = prim::GetAttr[name="conv"](%787)
  %789 : Tensor = prim::GetAttr[name="weight"](%788)
  %790 : Tensor? = prim::GetAttr[name="bias"](%788)
  %791 : int[] = prim::ListConstruct(%54, %54)
  %792 : int[] = prim::ListConstruct(%48, %48)
  %793 : int[] = prim::ListConstruct(%54, %54)
  %x.91 : Tensor = aten::conv2d(%x.99, %789, %790, %791, %792, %793, %54) # torch/nn/modules/conv.py:415:15
  %795 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%787)
  %796 : int = aten::dim(%x.91) # torch/nn/modules/batchnorm.py:276:11
  %797 : bool = aten::ne(%796, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%797) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %798 : bool = prim::GetAttr[name="training"](%795)
   = prim::If(%798) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %799 : Tensor = prim::GetAttr[name="num_batches_tracked"](%795)
      %800 : Tensor = aten::add(%799, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%795, %800)
      -> ()
    block1():
      -> ()
  %801 : bool = prim::GetAttr[name="training"](%795)
  %802 : Tensor = prim::GetAttr[name="running_mean"](%795)
  %803 : Tensor = prim::GetAttr[name="running_var"](%795)
  %804 : Tensor = prim::GetAttr[name="weight"](%795)
  %805 : Tensor = prim::GetAttr[name="bias"](%795)
   = prim::If(%801) # torch/nn/functional.py:2011:4
    block0():
      %806 : int[] = aten::size(%x.91) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%806, %48) # torch/nn/functional.py:1991:17
      %808 : int = aten::len(%806) # torch/nn/functional.py:1992:19
      %809 : int = aten::sub(%808, %47) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%809, %49, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %813 : int = aten::add(%i.34, %47) # torch/nn/functional.py:1993:27
          %814 : int = aten::__getitem__(%806, %813) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %814) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.135)
      %816 : bool = aten::eq(%size_prods.133, %54) # torch/nn/functional.py:1994:7
       = prim::If(%816) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.92 : Tensor = aten::batch_norm(%x.91, %804, %805, %802, %803, %801, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.10 : Tensor = aten::relu_(%x.92) # torch/nn/functional.py:1117:17
  %819 : __torch__.torchvision.models.inception.___torch_mangle_546.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%690)
  %820 : __torch__.torch.nn.modules.conv.___torch_mangle_545.Conv2d = prim::GetAttr[name="conv"](%819)
  %821 : Tensor = prim::GetAttr[name="weight"](%820)
  %822 : Tensor? = prim::GetAttr[name="bias"](%820)
  %823 : int[] = prim::ListConstruct(%54, %54)
  %824 : int[] = prim::ListConstruct(%54, %54)
  %825 : int[] = prim::ListConstruct(%54, %54)
  %x.93 : Tensor = aten::conv2d(%branch3x3dbl.10, %821, %822, %823, %824, %825, %54) # torch/nn/modules/conv.py:415:15
  %827 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%819)
  %828 : int = aten::dim(%x.93) # torch/nn/modules/batchnorm.py:276:11
  %829 : bool = aten::ne(%828, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%829) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %830 : bool = prim::GetAttr[name="training"](%827)
   = prim::If(%830) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %831 : Tensor = prim::GetAttr[name="num_batches_tracked"](%827)
      %832 : Tensor = aten::add(%831, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%827, %832)
      -> ()
    block1():
      -> ()
  %833 : bool = prim::GetAttr[name="training"](%827)
  %834 : Tensor = prim::GetAttr[name="running_mean"](%827)
  %835 : Tensor = prim::GetAttr[name="running_var"](%827)
  %836 : Tensor = prim::GetAttr[name="weight"](%827)
  %837 : Tensor = prim::GetAttr[name="bias"](%827)
   = prim::If(%833) # torch/nn/functional.py:2011:4
    block0():
      %838 : int[] = aten::size(%x.93) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%838, %48) # torch/nn/functional.py:1991:17
      %840 : int = aten::len(%838) # torch/nn/functional.py:1992:19
      %841 : int = aten::sub(%840, %47) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%841, %49, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %845 : int = aten::add(%i.35, %47) # torch/nn/functional.py:1993:27
          %846 : int = aten::__getitem__(%838, %845) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %846) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.139)
      %848 : bool = aten::eq(%size_prods.137, %54) # torch/nn/functional.py:1994:7
       = prim::If(%848) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.94 : Tensor = aten::batch_norm(%x.93, %836, %837, %834, %835, %833, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.11 : Tensor = aten::relu_(%x.94) # torch/nn/functional.py:1117:17
  %851 : __torch__.torchvision.models.inception.___torch_mangle_548.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3"](%690)
  %852 : __torch__.torch.nn.modules.conv.___torch_mangle_547.Conv2d = prim::GetAttr[name="conv"](%851)
  %853 : Tensor = prim::GetAttr[name="weight"](%852)
  %854 : Tensor? = prim::GetAttr[name="bias"](%852)
  %855 : int[] = prim::ListConstruct(%54, %54)
  %856 : int[] = prim::ListConstruct(%54, %54)
  %857 : int[] = prim::ListConstruct(%54, %54)
  %x.95 : Tensor = aten::conv2d(%branch3x3dbl.11, %853, %854, %855, %856, %857, %54) # torch/nn/modules/conv.py:415:15
  %859 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%851)
  %860 : int = aten::dim(%x.95) # torch/nn/modules/batchnorm.py:276:11
  %861 : bool = aten::ne(%860, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%861) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %862 : bool = prim::GetAttr[name="training"](%859)
   = prim::If(%862) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %863 : Tensor = prim::GetAttr[name="num_batches_tracked"](%859)
      %864 : Tensor = aten::add(%863, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%859, %864)
      -> ()
    block1():
      -> ()
  %865 : bool = prim::GetAttr[name="training"](%859)
  %866 : Tensor = prim::GetAttr[name="running_mean"](%859)
  %867 : Tensor = prim::GetAttr[name="running_var"](%859)
  %868 : Tensor = prim::GetAttr[name="weight"](%859)
  %869 : Tensor = prim::GetAttr[name="bias"](%859)
   = prim::If(%865) # torch/nn/functional.py:2011:4
    block0():
      %870 : int[] = aten::size(%x.95) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%870, %48) # torch/nn/functional.py:1991:17
      %872 : int = aten::len(%870) # torch/nn/functional.py:1992:19
      %873 : int = aten::sub(%872, %47) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%873, %49, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %877 : int = aten::add(%i.36, %47) # torch/nn/functional.py:1993:27
          %878 : int = aten::__getitem__(%870, %877) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %878) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.143)
      %880 : bool = aten::eq(%size_prods.141, %54) # torch/nn/functional.py:1994:7
       = prim::If(%880) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.96 : Tensor = aten::batch_norm(%x.95, %868, %869, %866, %867, %865, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.12 : Tensor = aten::relu_(%x.96) # torch/nn/functional.py:1117:17
  %883 : int[] = prim::ListConstruct(%46, %46)
  %884 : int[] = prim::ListConstruct(%54, %54)
  %885 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.7 : Tensor = aten::avg_pool2d(%x.99, %883, %884, %885, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:236:22
  %887 : __torch__.torchvision.models.inception.___torch_mangle_555.BasicConv2d = prim::GetAttr[name="branch_pool"](%690)
  %888 : __torch__.torch.nn.modules.conv.___torch_mangle_554.Conv2d = prim::GetAttr[name="conv"](%887)
  %889 : Tensor = prim::GetAttr[name="weight"](%888)
  %890 : Tensor? = prim::GetAttr[name="bias"](%888)
  %891 : int[] = prim::ListConstruct(%54, %54)
  %892 : int[] = prim::ListConstruct(%48, %48)
  %893 : int[] = prim::ListConstruct(%54, %54)
  %x.83 : Tensor = aten::conv2d(%branch_pool.7, %889, %890, %891, %892, %893, %54) # torch/nn/modules/conv.py:415:15
  %895 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%887)
  %896 : int = aten::dim(%x.83) # torch/nn/modules/batchnorm.py:276:11
  %897 : bool = aten::ne(%896, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%897) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %898 : bool = prim::GetAttr[name="training"](%895)
   = prim::If(%898) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %899 : Tensor = prim::GetAttr[name="num_batches_tracked"](%895)
      %900 : Tensor = aten::add(%899, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%895, %900)
      -> ()
    block1():
      -> ()
  %901 : bool = prim::GetAttr[name="training"](%895)
  %902 : Tensor = prim::GetAttr[name="running_mean"](%895)
  %903 : Tensor = prim::GetAttr[name="running_var"](%895)
  %904 : Tensor = prim::GetAttr[name="weight"](%895)
  %905 : Tensor = prim::GetAttr[name="bias"](%895)
   = prim::If(%901) # torch/nn/functional.py:2011:4
    block0():
      %906 : int[] = aten::size(%x.83) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%906, %48) # torch/nn/functional.py:1991:17
      %908 : int = aten::len(%906) # torch/nn/functional.py:1992:19
      %909 : int = aten::sub(%908, %47) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%909, %49, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %913 : int = aten::add(%i.30, %47) # torch/nn/functional.py:1993:27
          %914 : int = aten::__getitem__(%906, %913) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %914) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.119)
      %916 : bool = aten::eq(%size_prods.117, %54) # torch/nn/functional.py:1994:7
       = prim::If(%916) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.84 : Tensor = aten::batch_norm(%x.83, %904, %905, %902, %903, %901, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.9 : Tensor = aten::relu_(%x.84) # torch/nn/functional.py:1117:17
  %outputs.5 : Tensor[] = prim::ListConstruct(%branch1x1.4, %branch5x5.3, %branch3x3dbl.12, %branch_pool.9)
  %x.101 : Tensor = aten::cat(%outputs.5, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:244:15
  %921 : __torch__.torchvision.models.inception.InceptionB = prim::GetAttr[name="Mixed_6a"](%self)
  %922 : __torch__.torchvision.models.inception.___torch_mangle_560.BasicConv2d = prim::GetAttr[name="branch3x3"](%921)
  %923 : __torch__.torch.nn.modules.conv.___torch_mangle_559.Conv2d = prim::GetAttr[name="conv"](%922)
  %924 : Tensor = prim::GetAttr[name="weight"](%923)
  %925 : Tensor? = prim::GetAttr[name="bias"](%923)
  %926 : int[] = prim::ListConstruct(%47, %47)
  %927 : int[] = prim::ListConstruct(%48, %48)
  %928 : int[] = prim::ListConstruct(%54, %54)
  %x.85 : Tensor = aten::conv2d(%x.101, %924, %925, %926, %927, %928, %54) # torch/nn/modules/conv.py:415:15
  %930 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%922)
  %931 : int = aten::dim(%x.85) # torch/nn/modules/batchnorm.py:276:11
  %932 : bool = aten::ne(%931, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%932) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %933 : bool = prim::GetAttr[name="training"](%930)
   = prim::If(%933) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %934 : Tensor = prim::GetAttr[name="num_batches_tracked"](%930)
      %935 : Tensor = aten::add(%934, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%930, %935)
      -> ()
    block1():
      -> ()
  %936 : bool = prim::GetAttr[name="training"](%930)
  %937 : Tensor = prim::GetAttr[name="running_mean"](%930)
  %938 : Tensor = prim::GetAttr[name="running_var"](%930)
  %939 : Tensor = prim::GetAttr[name="weight"](%930)
  %940 : Tensor = prim::GetAttr[name="bias"](%930)
   = prim::If(%936) # torch/nn/functional.py:2011:4
    block0():
      %941 : int[] = aten::size(%x.85) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%941, %48) # torch/nn/functional.py:1991:17
      %943 : int = aten::len(%941) # torch/nn/functional.py:1992:19
      %944 : int = aten::sub(%943, %47) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%944, %49, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %948 : int = aten::add(%i.31, %47) # torch/nn/functional.py:1993:27
          %949 : int = aten::__getitem__(%941, %948) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %949) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.123)
      %951 : bool = aten::eq(%size_prods.121, %54) # torch/nn/functional.py:1994:7
       = prim::If(%951) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.86 : Tensor = aten::batch_norm(%x.85, %939, %940, %937, %938, %936, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3.2 : Tensor = aten::relu_(%x.86) # torch/nn/functional.py:1117:17
  %954 : __torch__.torchvision.models.inception.___torch_mangle_555.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%921)
  %955 : __torch__.torch.nn.modules.conv.___torch_mangle_554.Conv2d = prim::GetAttr[name="conv"](%954)
  %956 : Tensor = prim::GetAttr[name="weight"](%955)
  %957 : Tensor? = prim::GetAttr[name="bias"](%955)
  %958 : int[] = prim::ListConstruct(%54, %54)
  %959 : int[] = prim::ListConstruct(%48, %48)
  %960 : int[] = prim::ListConstruct(%54, %54)
  %x.87 : Tensor = aten::conv2d(%x.101, %956, %957, %958, %959, %960, %54) # torch/nn/modules/conv.py:415:15
  %962 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%954)
  %963 : int = aten::dim(%x.87) # torch/nn/modules/batchnorm.py:276:11
  %964 : bool = aten::ne(%963, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%964) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %965 : bool = prim::GetAttr[name="training"](%962)
   = prim::If(%965) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %966 : Tensor = prim::GetAttr[name="num_batches_tracked"](%962)
      %967 : Tensor = aten::add(%966, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%962, %967)
      -> ()
    block1():
      -> ()
  %968 : bool = prim::GetAttr[name="training"](%962)
  %969 : Tensor = prim::GetAttr[name="running_mean"](%962)
  %970 : Tensor = prim::GetAttr[name="running_var"](%962)
  %971 : Tensor = prim::GetAttr[name="weight"](%962)
  %972 : Tensor = prim::GetAttr[name="bias"](%962)
   = prim::If(%968) # torch/nn/functional.py:2011:4
    block0():
      %973 : int[] = aten::size(%x.87) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%973, %48) # torch/nn/functional.py:1991:17
      %975 : int = aten::len(%973) # torch/nn/functional.py:1992:19
      %976 : int = aten::sub(%975, %47) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%976, %49, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %980 : int = aten::add(%i.32, %47) # torch/nn/functional.py:1993:27
          %981 : int = aten::__getitem__(%973, %980) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %981) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.127)
      %983 : bool = aten::eq(%size_prods.125, %54) # torch/nn/functional.py:1994:7
       = prim::If(%983) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.88 : Tensor = aten::batch_norm(%x.87, %971, %972, %969, %970, %968, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.13 : Tensor = aten::relu_(%x.88) # torch/nn/functional.py:1117:17
  %986 : __torch__.torchvision.models.inception.___torch_mangle_546.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%921)
  %987 : __torch__.torch.nn.modules.conv.___torch_mangle_545.Conv2d = prim::GetAttr[name="conv"](%986)
  %988 : Tensor = prim::GetAttr[name="weight"](%987)
  %989 : Tensor? = prim::GetAttr[name="bias"](%987)
  %990 : int[] = prim::ListConstruct(%54, %54)
  %991 : int[] = prim::ListConstruct(%54, %54)
  %992 : int[] = prim::ListConstruct(%54, %54)
  %x.89 : Tensor = aten::conv2d(%branch3x3dbl.13, %988, %989, %990, %991, %992, %54) # torch/nn/modules/conv.py:415:15
  %994 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%986)
  %995 : int = aten::dim(%x.89) # torch/nn/modules/batchnorm.py:276:11
  %996 : bool = aten::ne(%995, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%996) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %997 : bool = prim::GetAttr[name="training"](%994)
   = prim::If(%997) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %998 : Tensor = prim::GetAttr[name="num_batches_tracked"](%994)
      %999 : Tensor = aten::add(%998, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%994, %999)
      -> ()
    block1():
      -> ()
  %1000 : bool = prim::GetAttr[name="training"](%994)
  %1001 : Tensor = prim::GetAttr[name="running_mean"](%994)
  %1002 : Tensor = prim::GetAttr[name="running_var"](%994)
  %1003 : Tensor = prim::GetAttr[name="weight"](%994)
  %1004 : Tensor = prim::GetAttr[name="bias"](%994)
   = prim::If(%1000) # torch/nn/functional.py:2011:4
    block0():
      %1005 : int[] = aten::size(%x.89) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%1005, %48) # torch/nn/functional.py:1991:17
      %1007 : int = aten::len(%1005) # torch/nn/functional.py:1992:19
      %1008 : int = aten::sub(%1007, %47) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%1008, %49, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %1012 : int = aten::add(%i.33, %47) # torch/nn/functional.py:1993:27
          %1013 : int = aten::__getitem__(%1005, %1012) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %1013) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.131)
      %1015 : bool = aten::eq(%size_prods.129, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1015) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.90 : Tensor = aten::batch_norm(%x.89, %1003, %1004, %1001, %1002, %1000, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.14 : Tensor = aten::relu_(%x.90) # torch/nn/functional.py:1117:17
  %1018 : __torch__.torchvision.models.inception.___torch_mangle_562.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3"](%921)
  %1019 : __torch__.torch.nn.modules.conv.___torch_mangle_561.Conv2d = prim::GetAttr[name="conv"](%1018)
  %1020 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1021 : Tensor? = prim::GetAttr[name="bias"](%1019)
  %1022 : int[] = prim::ListConstruct(%47, %47)
  %1023 : int[] = prim::ListConstruct(%48, %48)
  %1024 : int[] = prim::ListConstruct(%54, %54)
  %x.103 : Tensor = aten::conv2d(%branch3x3dbl.14, %1020, %1021, %1022, %1023, %1024, %54) # torch/nn/modules/conv.py:415:15
  %1026 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%1018)
  %1027 : int = aten::dim(%x.103) # torch/nn/modules/batchnorm.py:276:11
  %1028 : bool = aten::ne(%1027, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1028) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1029 : bool = prim::GetAttr[name="training"](%1026)
   = prim::If(%1029) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1030 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1026)
      %1031 : Tensor = aten::add(%1030, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1026, %1031)
      -> ()
    block1():
      -> ()
  %1032 : bool = prim::GetAttr[name="training"](%1026)
  %1033 : Tensor = prim::GetAttr[name="running_mean"](%1026)
  %1034 : Tensor = prim::GetAttr[name="running_var"](%1026)
  %1035 : Tensor = prim::GetAttr[name="weight"](%1026)
  %1036 : Tensor = prim::GetAttr[name="bias"](%1026)
   = prim::If(%1032) # torch/nn/functional.py:2011:4
    block0():
      %1037 : int[] = aten::size(%x.103) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%1037, %48) # torch/nn/functional.py:1991:17
      %1039 : int = aten::len(%1037) # torch/nn/functional.py:1992:19
      %1040 : int = aten::sub(%1039, %47) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%1040, %49, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %1044 : int = aten::add(%i.40, %47) # torch/nn/functional.py:1993:27
          %1045 : int = aten::__getitem__(%1037, %1044) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %1045) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.159)
      %1047 : bool = aten::eq(%size_prods.157, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1047) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.104 : Tensor = aten::batch_norm(%x.103, %1035, %1036, %1033, %1034, %1032, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.5 : Tensor = aten::relu_(%x.104) # torch/nn/functional.py:1117:17
  %1050 : int[] = prim::ListConstruct(%46, %46)
  %1051 : int[] = prim::ListConstruct(%47, %47)
  %1052 : int[] = prim::ListConstruct(%48, %48)
  %1053 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.8 : Tensor = aten::max_pool2d(%x.101, %1050, %1051, %1052, %1053, %55) # torch/nn/functional.py:575:11
  %outputs.6 : Tensor[] = prim::ListConstruct(%branch3x3.2, %branch3x3dbl.5, %branch_pool.8)
  %x.23 : Tensor = aten::cat(%outputs.6, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:273:15
  %1057 : __torch__.torchvision.models.inception.InceptionC = prim::GetAttr[name="Mixed_6b"](%self)
  %1058 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch1x1"](%1057)
  %1059 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%1058)
  %1060 : Tensor = prim::GetAttr[name="weight"](%1059)
  %1061 : Tensor? = prim::GetAttr[name="bias"](%1059)
  %1062 : int[] = prim::ListConstruct(%54, %54)
  %1063 : int[] = prim::ListConstruct(%48, %48)
  %1064 : int[] = prim::ListConstruct(%54, %54)
  %x.105 : Tensor = aten::conv2d(%x.23, %1060, %1061, %1062, %1063, %1064, %54) # torch/nn/modules/conv.py:415:15
  %1066 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1058)
  %1067 : int = aten::dim(%x.105) # torch/nn/modules/batchnorm.py:276:11
  %1068 : bool = aten::ne(%1067, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1068) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1069 : bool = prim::GetAttr[name="training"](%1066)
   = prim::If(%1069) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1070 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1066)
      %1071 : Tensor = aten::add(%1070, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1066, %1071)
      -> ()
    block1():
      -> ()
  %1072 : bool = prim::GetAttr[name="training"](%1066)
  %1073 : Tensor = prim::GetAttr[name="running_mean"](%1066)
  %1074 : Tensor = prim::GetAttr[name="running_var"](%1066)
  %1075 : Tensor = prim::GetAttr[name="weight"](%1066)
  %1076 : Tensor = prim::GetAttr[name="bias"](%1066)
   = prim::If(%1072) # torch/nn/functional.py:2011:4
    block0():
      %1077 : int[] = aten::size(%x.105) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%1077, %48) # torch/nn/functional.py:1991:17
      %1079 : int = aten::len(%1077) # torch/nn/functional.py:1992:19
      %1080 : int = aten::sub(%1079, %47) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%1080, %49, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %1084 : int = aten::add(%i.41, %47) # torch/nn/functional.py:1993:27
          %1085 : int = aten::__getitem__(%1077, %1084) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %1085) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.163)
      %1087 : bool = aten::eq(%size_prods.161, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1087) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.106 : Tensor = aten::batch_norm(%x.105, %1075, %1076, %1073, %1074, %1072, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.5 : Tensor = aten::relu_(%x.106) # torch/nn/functional.py:1117:17
  %1090 : __torch__.torchvision.models.inception.___torch_mangle_564.BasicConv2d = prim::GetAttr[name="branch7x7_1"](%1057)
  %1091 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv"](%1090)
  %1092 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1093 : Tensor? = prim::GetAttr[name="bias"](%1091)
  %1094 : int[] = prim::ListConstruct(%54, %54)
  %1095 : int[] = prim::ListConstruct(%48, %48)
  %1096 : int[] = prim::ListConstruct(%54, %54)
  %x.107 : Tensor = aten::conv2d(%x.23, %1092, %1093, %1094, %1095, %1096, %54) # torch/nn/modules/conv.py:415:15
  %1098 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1090)
  %1099 : int = aten::dim(%x.107) # torch/nn/modules/batchnorm.py:276:11
  %1100 : bool = aten::ne(%1099, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1100) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1101 : bool = prim::GetAttr[name="training"](%1098)
   = prim::If(%1101) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1102 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1098)
      %1103 : Tensor = aten::add(%1102, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1098, %1103)
      -> ()
    block1():
      -> ()
  %1104 : bool = prim::GetAttr[name="training"](%1098)
  %1105 : Tensor = prim::GetAttr[name="running_mean"](%1098)
  %1106 : Tensor = prim::GetAttr[name="running_var"](%1098)
  %1107 : Tensor = prim::GetAttr[name="weight"](%1098)
  %1108 : Tensor = prim::GetAttr[name="bias"](%1098)
   = prim::If(%1104) # torch/nn/functional.py:2011:4
    block0():
      %1109 : int[] = aten::size(%x.107) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%1109, %48) # torch/nn/functional.py:1991:17
      %1111 : int = aten::len(%1109) # torch/nn/functional.py:1992:19
      %1112 : int = aten::sub(%1111, %47) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%1112, %49, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %1116 : int = aten::add(%i.42, %47) # torch/nn/functional.py:1993:27
          %1117 : int = aten::__getitem__(%1109, %1116) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %1117) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.167)
      %1119 : bool = aten::eq(%size_prods.165, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1119) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.108 : Tensor = aten::batch_norm(%x.107, %1107, %1108, %1105, %1106, %1104, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.2 : Tensor = aten::relu_(%x.108) # torch/nn/functional.py:1117:17
  %1122 : __torch__.torchvision.models.inception.___torch_mangle_566.BasicConv2d = prim::GetAttr[name="branch7x7_2"](%1057)
  %1123 : __torch__.torch.nn.modules.conv.___torch_mangle_565.Conv2d = prim::GetAttr[name="conv"](%1122)
  %1124 : Tensor = prim::GetAttr[name="weight"](%1123)
  %1125 : Tensor? = prim::GetAttr[name="bias"](%1123)
  %1126 : int[] = prim::ListConstruct(%54, %54)
  %1127 : int[] = prim::ListConstruct(%48, %46)
  %1128 : int[] = prim::ListConstruct(%54, %54)
  %x.109 : Tensor = aten::conv2d(%branch7x7.2, %1124, %1125, %1126, %1127, %1128, %54) # torch/nn/modules/conv.py:415:15
  %1130 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1122)
  %1131 : int = aten::dim(%x.109) # torch/nn/modules/batchnorm.py:276:11
  %1132 : bool = aten::ne(%1131, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1132) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1133 : bool = prim::GetAttr[name="training"](%1130)
   = prim::If(%1133) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1134 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1130)
      %1135 : Tensor = aten::add(%1134, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1130, %1135)
      -> ()
    block1():
      -> ()
  %1136 : bool = prim::GetAttr[name="training"](%1130)
  %1137 : Tensor = prim::GetAttr[name="running_mean"](%1130)
  %1138 : Tensor = prim::GetAttr[name="running_var"](%1130)
  %1139 : Tensor = prim::GetAttr[name="weight"](%1130)
  %1140 : Tensor = prim::GetAttr[name="bias"](%1130)
   = prim::If(%1136) # torch/nn/functional.py:2011:4
    block0():
      %1141 : int[] = aten::size(%x.109) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%1141, %48) # torch/nn/functional.py:1991:17
      %1143 : int = aten::len(%1141) # torch/nn/functional.py:1992:19
      %1144 : int = aten::sub(%1143, %47) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%1144, %49, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %1148 : int = aten::add(%i.43, %47) # torch/nn/functional.py:1993:27
          %1149 : int = aten::__getitem__(%1141, %1148) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %1149) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.171)
      %1151 : bool = aten::eq(%size_prods.169, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1151) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.110 : Tensor = aten::batch_norm(%x.109, %1139, %1140, %1137, %1138, %1136, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.4 : Tensor = aten::relu_(%x.110) # torch/nn/functional.py:1117:17
  %1154 : __torch__.torchvision.models.inception.___torch_mangle_568.BasicConv2d = prim::GetAttr[name="branch7x7_3"](%1057)
  %1155 : __torch__.torch.nn.modules.conv.___torch_mangle_567.Conv2d = prim::GetAttr[name="conv"](%1154)
  %1156 : Tensor = prim::GetAttr[name="weight"](%1155)
  %1157 : Tensor? = prim::GetAttr[name="bias"](%1155)
  %1158 : int[] = prim::ListConstruct(%54, %54)
  %1159 : int[] = prim::ListConstruct(%46, %48)
  %1160 : int[] = prim::ListConstruct(%54, %54)
  %x.111 : Tensor = aten::conv2d(%branch7x7.4, %1156, %1157, %1158, %1159, %1160, %54) # torch/nn/modules/conv.py:415:15
  %1162 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1154)
  %1163 : int = aten::dim(%x.111) # torch/nn/modules/batchnorm.py:276:11
  %1164 : bool = aten::ne(%1163, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1164) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1165 : bool = prim::GetAttr[name="training"](%1162)
   = prim::If(%1165) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1166 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1162)
      %1167 : Tensor = aten::add(%1166, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1162, %1167)
      -> ()
    block1():
      -> ()
  %1168 : bool = prim::GetAttr[name="training"](%1162)
  %1169 : Tensor = prim::GetAttr[name="running_mean"](%1162)
  %1170 : Tensor = prim::GetAttr[name="running_var"](%1162)
  %1171 : Tensor = prim::GetAttr[name="weight"](%1162)
  %1172 : Tensor = prim::GetAttr[name="bias"](%1162)
   = prim::If(%1168) # torch/nn/functional.py:2011:4
    block0():
      %1173 : int[] = aten::size(%x.111) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%1173, %48) # torch/nn/functional.py:1991:17
      %1175 : int = aten::len(%1173) # torch/nn/functional.py:1992:19
      %1176 : int = aten::sub(%1175, %47) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%1176, %49, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %1180 : int = aten::add(%i.44, %47) # torch/nn/functional.py:1993:27
          %1181 : int = aten::__getitem__(%1173, %1180) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %1181) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.175)
      %1183 : bool = aten::eq(%size_prods.173, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1183) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.112 : Tensor = aten::batch_norm(%x.111, %1171, %1172, %1169, %1170, %1168, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.6 : Tensor = aten::relu_(%x.112) # torch/nn/functional.py:1117:17
  %1186 : __torch__.torchvision.models.inception.___torch_mangle_564.BasicConv2d = prim::GetAttr[name="branch7x7dbl_1"](%1057)
  %1187 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv"](%1186)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1187)
  %1189 : Tensor? = prim::GetAttr[name="bias"](%1187)
  %1190 : int[] = prim::ListConstruct(%54, %54)
  %1191 : int[] = prim::ListConstruct(%48, %48)
  %1192 : int[] = prim::ListConstruct(%54, %54)
  %x.113 : Tensor = aten::conv2d(%x.23, %1188, %1189, %1190, %1191, %1192, %54) # torch/nn/modules/conv.py:415:15
  %1194 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1186)
  %1195 : int = aten::dim(%x.113) # torch/nn/modules/batchnorm.py:276:11
  %1196 : bool = aten::ne(%1195, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1196) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1197 : bool = prim::GetAttr[name="training"](%1194)
   = prim::If(%1197) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1198 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1194)
      %1199 : Tensor = aten::add(%1198, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1194, %1199)
      -> ()
    block1():
      -> ()
  %1200 : bool = prim::GetAttr[name="training"](%1194)
  %1201 : Tensor = prim::GetAttr[name="running_mean"](%1194)
  %1202 : Tensor = prim::GetAttr[name="running_var"](%1194)
  %1203 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1204 : Tensor = prim::GetAttr[name="bias"](%1194)
   = prim::If(%1200) # torch/nn/functional.py:2011:4
    block0():
      %1205 : int[] = aten::size(%x.113) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%1205, %48) # torch/nn/functional.py:1991:17
      %1207 : int = aten::len(%1205) # torch/nn/functional.py:1992:19
      %1208 : int = aten::sub(%1207, %47) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%1208, %49, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %1212 : int = aten::add(%i.45, %47) # torch/nn/functional.py:1993:27
          %1213 : int = aten::__getitem__(%1205, %1212) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %1213) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.179)
      %1215 : bool = aten::eq(%size_prods.177, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1215) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.114 : Tensor = aten::batch_norm(%x.113, %1203, %1204, %1201, %1202, %1200, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.2 : Tensor = aten::relu_(%x.114) # torch/nn/functional.py:1117:17
  %1218 : __torch__.torchvision.models.inception.___torch_mangle_570.BasicConv2d = prim::GetAttr[name="branch7x7dbl_2"](%1057)
  %1219 : __torch__.torch.nn.modules.conv.___torch_mangle_569.Conv2d = prim::GetAttr[name="conv"](%1218)
  %1220 : Tensor = prim::GetAttr[name="weight"](%1219)
  %1221 : Tensor? = prim::GetAttr[name="bias"](%1219)
  %1222 : int[] = prim::ListConstruct(%54, %54)
  %1223 : int[] = prim::ListConstruct(%46, %48)
  %1224 : int[] = prim::ListConstruct(%54, %54)
  %x.115 : Tensor = aten::conv2d(%branch7x7dbl.2, %1220, %1221, %1222, %1223, %1224, %54) # torch/nn/modules/conv.py:415:15
  %1226 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1218)
  %1227 : int = aten::dim(%x.115) # torch/nn/modules/batchnorm.py:276:11
  %1228 : bool = aten::ne(%1227, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1228) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1229 : bool = prim::GetAttr[name="training"](%1226)
   = prim::If(%1229) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1230 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1226)
      %1231 : Tensor = aten::add(%1230, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1226, %1231)
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1226)
  %1233 : Tensor = prim::GetAttr[name="running_mean"](%1226)
  %1234 : Tensor = prim::GetAttr[name="running_var"](%1226)
  %1235 : Tensor = prim::GetAttr[name="weight"](%1226)
  %1236 : Tensor = prim::GetAttr[name="bias"](%1226)
   = prim::If(%1232) # torch/nn/functional.py:2011:4
    block0():
      %1237 : int[] = aten::size(%x.115) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%1237, %48) # torch/nn/functional.py:1991:17
      %1239 : int = aten::len(%1237) # torch/nn/functional.py:1992:19
      %1240 : int = aten::sub(%1239, %47) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%1240, %49, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %1244 : int = aten::add(%i.46, %47) # torch/nn/functional.py:1993:27
          %1245 : int = aten::__getitem__(%1237, %1244) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %1245) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.183)
      %1247 : bool = aten::eq(%size_prods.181, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1247) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.116 : Tensor = aten::batch_norm(%x.115, %1235, %1236, %1233, %1234, %1232, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.4 : Tensor = aten::relu_(%x.116) # torch/nn/functional.py:1117:17
  %1250 : __torch__.torchvision.models.inception.___torch_mangle_566.BasicConv2d = prim::GetAttr[name="branch7x7dbl_3"](%1057)
  %1251 : __torch__.torch.nn.modules.conv.___torch_mangle_565.Conv2d = prim::GetAttr[name="conv"](%1250)
  %1252 : Tensor = prim::GetAttr[name="weight"](%1251)
  %1253 : Tensor? = prim::GetAttr[name="bias"](%1251)
  %1254 : int[] = prim::ListConstruct(%54, %54)
  %1255 : int[] = prim::ListConstruct(%48, %46)
  %1256 : int[] = prim::ListConstruct(%54, %54)
  %x.117 : Tensor = aten::conv2d(%branch7x7dbl.4, %1252, %1253, %1254, %1255, %1256, %54) # torch/nn/modules/conv.py:415:15
  %1258 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1250)
  %1259 : int = aten::dim(%x.117) # torch/nn/modules/batchnorm.py:276:11
  %1260 : bool = aten::ne(%1259, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1260) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1261 : bool = prim::GetAttr[name="training"](%1258)
   = prim::If(%1261) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1262 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1258)
      %1263 : Tensor = aten::add(%1262, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1258, %1263)
      -> ()
    block1():
      -> ()
  %1264 : bool = prim::GetAttr[name="training"](%1258)
  %1265 : Tensor = prim::GetAttr[name="running_mean"](%1258)
  %1266 : Tensor = prim::GetAttr[name="running_var"](%1258)
  %1267 : Tensor = prim::GetAttr[name="weight"](%1258)
  %1268 : Tensor = prim::GetAttr[name="bias"](%1258)
   = prim::If(%1264) # torch/nn/functional.py:2011:4
    block0():
      %1269 : int[] = aten::size(%x.117) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%1269, %48) # torch/nn/functional.py:1991:17
      %1271 : int = aten::len(%1269) # torch/nn/functional.py:1992:19
      %1272 : int = aten::sub(%1271, %47) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%1272, %49, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %1276 : int = aten::add(%i.47, %47) # torch/nn/functional.py:1993:27
          %1277 : int = aten::__getitem__(%1269, %1276) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %1277) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.187)
      %1279 : bool = aten::eq(%size_prods.185, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1279) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.118 : Tensor = aten::batch_norm(%x.117, %1267, %1268, %1265, %1266, %1264, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.6 : Tensor = aten::relu_(%x.118) # torch/nn/functional.py:1117:17
  %1282 : __torch__.torchvision.models.inception.___torch_mangle_570.BasicConv2d = prim::GetAttr[name="branch7x7dbl_4"](%1057)
  %1283 : __torch__.torch.nn.modules.conv.___torch_mangle_569.Conv2d = prim::GetAttr[name="conv"](%1282)
  %1284 : Tensor = prim::GetAttr[name="weight"](%1283)
  %1285 : Tensor? = prim::GetAttr[name="bias"](%1283)
  %1286 : int[] = prim::ListConstruct(%54, %54)
  %1287 : int[] = prim::ListConstruct(%46, %48)
  %1288 : int[] = prim::ListConstruct(%54, %54)
  %x.119 : Tensor = aten::conv2d(%branch7x7dbl.6, %1284, %1285, %1286, %1287, %1288, %54) # torch/nn/modules/conv.py:415:15
  %1290 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1282)
  %1291 : int = aten::dim(%x.119) # torch/nn/modules/batchnorm.py:276:11
  %1292 : bool = aten::ne(%1291, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1292) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1293 : bool = prim::GetAttr[name="training"](%1290)
   = prim::If(%1293) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1294 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1290)
      %1295 : Tensor = aten::add(%1294, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1290, %1295)
      -> ()
    block1():
      -> ()
  %1296 : bool = prim::GetAttr[name="training"](%1290)
  %1297 : Tensor = prim::GetAttr[name="running_mean"](%1290)
  %1298 : Tensor = prim::GetAttr[name="running_var"](%1290)
  %1299 : Tensor = prim::GetAttr[name="weight"](%1290)
  %1300 : Tensor = prim::GetAttr[name="bias"](%1290)
   = prim::If(%1296) # torch/nn/functional.py:2011:4
    block0():
      %1301 : int[] = aten::size(%x.119) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%1301, %48) # torch/nn/functional.py:1991:17
      %1303 : int = aten::len(%1301) # torch/nn/functional.py:1992:19
      %1304 : int = aten::sub(%1303, %47) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%1304, %49, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %1308 : int = aten::add(%i.48, %47) # torch/nn/functional.py:1993:27
          %1309 : int = aten::__getitem__(%1301, %1308) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %1309) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.191)
      %1311 : bool = aten::eq(%size_prods.189, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1311) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.120 : Tensor = aten::batch_norm(%x.119, %1299, %1300, %1297, %1298, %1296, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.8 : Tensor = aten::relu_(%x.120) # torch/nn/functional.py:1117:17
  %1314 : __torch__.torchvision.models.inception.___torch_mangle_572.BasicConv2d = prim::GetAttr[name="branch7x7dbl_5"](%1057)
  %1315 : __torch__.torch.nn.modules.conv.___torch_mangle_571.Conv2d = prim::GetAttr[name="conv"](%1314)
  %1316 : Tensor = prim::GetAttr[name="weight"](%1315)
  %1317 : Tensor? = prim::GetAttr[name="bias"](%1315)
  %1318 : int[] = prim::ListConstruct(%54, %54)
  %1319 : int[] = prim::ListConstruct(%48, %46)
  %1320 : int[] = prim::ListConstruct(%54, %54)
  %x.121 : Tensor = aten::conv2d(%branch7x7dbl.8, %1316, %1317, %1318, %1319, %1320, %54) # torch/nn/modules/conv.py:415:15
  %1322 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1314)
  %1323 : int = aten::dim(%x.121) # torch/nn/modules/batchnorm.py:276:11
  %1324 : bool = aten::ne(%1323, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1324) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1325 : bool = prim::GetAttr[name="training"](%1322)
   = prim::If(%1325) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1326 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1322)
      %1327 : Tensor = aten::add(%1326, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1322, %1327)
      -> ()
    block1():
      -> ()
  %1328 : bool = prim::GetAttr[name="training"](%1322)
  %1329 : Tensor = prim::GetAttr[name="running_mean"](%1322)
  %1330 : Tensor = prim::GetAttr[name="running_var"](%1322)
  %1331 : Tensor = prim::GetAttr[name="weight"](%1322)
  %1332 : Tensor = prim::GetAttr[name="bias"](%1322)
   = prim::If(%1328) # torch/nn/functional.py:2011:4
    block0():
      %1333 : int[] = aten::size(%x.121) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%1333, %48) # torch/nn/functional.py:1991:17
      %1335 : int = aten::len(%1333) # torch/nn/functional.py:1992:19
      %1336 : int = aten::sub(%1335, %47) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%1336, %49, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %1340 : int = aten::add(%i.49, %47) # torch/nn/functional.py:1993:27
          %1341 : int = aten::__getitem__(%1333, %1340) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %1341) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.195)
      %1343 : bool = aten::eq(%size_prods.193, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1343) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.122 : Tensor = aten::batch_norm(%x.121, %1331, %1332, %1329, %1330, %1328, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.10 : Tensor = aten::relu_(%x.122) # torch/nn/functional.py:1117:17
  %1346 : int[] = prim::ListConstruct(%46, %46)
  %1347 : int[] = prim::ListConstruct(%54, %54)
  %1348 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.10 : Tensor = aten::avg_pool2d(%x.23, %1346, %1347, %1348, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:310:22
  %1350 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch_pool"](%1057)
  %1351 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%1350)
  %1352 : Tensor = prim::GetAttr[name="weight"](%1351)
  %1353 : Tensor? = prim::GetAttr[name="bias"](%1351)
  %1354 : int[] = prim::ListConstruct(%54, %54)
  %1355 : int[] = prim::ListConstruct(%48, %48)
  %1356 : int[] = prim::ListConstruct(%54, %54)
  %x.123 : Tensor = aten::conv2d(%branch_pool.10, %1352, %1353, %1354, %1355, %1356, %54) # torch/nn/modules/conv.py:415:15
  %1358 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1350)
  %1359 : int = aten::dim(%x.123) # torch/nn/modules/batchnorm.py:276:11
  %1360 : bool = aten::ne(%1359, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1360) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1361 : bool = prim::GetAttr[name="training"](%1358)
   = prim::If(%1361) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1362 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1358)
      %1363 : Tensor = aten::add(%1362, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1358, %1363)
      -> ()
    block1():
      -> ()
  %1364 : bool = prim::GetAttr[name="training"](%1358)
  %1365 : Tensor = prim::GetAttr[name="running_mean"](%1358)
  %1366 : Tensor = prim::GetAttr[name="running_var"](%1358)
  %1367 : Tensor = prim::GetAttr[name="weight"](%1358)
  %1368 : Tensor = prim::GetAttr[name="bias"](%1358)
   = prim::If(%1364) # torch/nn/functional.py:2011:4
    block0():
      %1369 : int[] = aten::size(%x.123) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%1369, %48) # torch/nn/functional.py:1991:17
      %1371 : int = aten::len(%1369) # torch/nn/functional.py:1992:19
      %1372 : int = aten::sub(%1371, %47) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%1372, %49, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %1376 : int = aten::add(%i.50, %47) # torch/nn/functional.py:1993:27
          %1377 : int = aten::__getitem__(%1369, %1376) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %1377) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.199)
      %1379 : bool = aten::eq(%size_prods.197, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1379) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.124 : Tensor = aten::batch_norm(%x.123, %1367, %1368, %1365, %1366, %1364, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.11 : Tensor = aten::relu_(%x.124) # torch/nn/functional.py:1117:17
  %outputs.7 : Tensor[] = prim::ListConstruct(%branch1x1.5, %branch7x7.6, %branch7x7dbl.10, %branch_pool.11)
  %x.25 : Tensor = aten::cat(%outputs.7, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:318:15
  %1384 : __torch__.torchvision.models.inception.___torch_mangle_583.InceptionC = prim::GetAttr[name="Mixed_6c"](%self)
  %1385 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch1x1"](%1384)
  %1386 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%1385)
  %1387 : Tensor = prim::GetAttr[name="weight"](%1386)
  %1388 : Tensor? = prim::GetAttr[name="bias"](%1386)
  %1389 : int[] = prim::ListConstruct(%54, %54)
  %1390 : int[] = prim::ListConstruct(%48, %48)
  %1391 : int[] = prim::ListConstruct(%54, %54)
  %x.125 : Tensor = aten::conv2d(%x.25, %1387, %1388, %1389, %1390, %1391, %54) # torch/nn/modules/conv.py:415:15
  %1393 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1385)
  %1394 : int = aten::dim(%x.125) # torch/nn/modules/batchnorm.py:276:11
  %1395 : bool = aten::ne(%1394, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1395) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1396 : bool = prim::GetAttr[name="training"](%1393)
   = prim::If(%1396) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1397 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1393)
      %1398 : Tensor = aten::add(%1397, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1393, %1398)
      -> ()
    block1():
      -> ()
  %1399 : bool = prim::GetAttr[name="training"](%1393)
  %1400 : Tensor = prim::GetAttr[name="running_mean"](%1393)
  %1401 : Tensor = prim::GetAttr[name="running_var"](%1393)
  %1402 : Tensor = prim::GetAttr[name="weight"](%1393)
  %1403 : Tensor = prim::GetAttr[name="bias"](%1393)
   = prim::If(%1399) # torch/nn/functional.py:2011:4
    block0():
      %1404 : int[] = aten::size(%x.125) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%1404, %48) # torch/nn/functional.py:1991:17
      %1406 : int = aten::len(%1404) # torch/nn/functional.py:1992:19
      %1407 : int = aten::sub(%1406, %47) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%1407, %49, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %1411 : int = aten::add(%i.51, %47) # torch/nn/functional.py:1993:27
          %1412 : int = aten::__getitem__(%1404, %1411) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %1412) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.203)
      %1414 : bool = aten::eq(%size_prods.201, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1414) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.126 : Tensor = aten::batch_norm(%x.125, %1402, %1403, %1400, %1401, %1399, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.6 : Tensor = aten::relu_(%x.126) # torch/nn/functional.py:1117:17
  %1417 : __torch__.torchvision.models.inception.___torch_mangle_574.BasicConv2d = prim::GetAttr[name="branch7x7_1"](%1384)
  %1418 : __torch__.torch.nn.modules.conv.___torch_mangle_573.Conv2d = prim::GetAttr[name="conv"](%1417)
  %1419 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1420 : Tensor? = prim::GetAttr[name="bias"](%1418)
  %1421 : int[] = prim::ListConstruct(%54, %54)
  %1422 : int[] = prim::ListConstruct(%48, %48)
  %1423 : int[] = prim::ListConstruct(%54, %54)
  %x.127 : Tensor = aten::conv2d(%x.25, %1419, %1420, %1421, %1422, %1423, %54) # torch/nn/modules/conv.py:415:15
  %1425 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1417)
  %1426 : int = aten::dim(%x.127) # torch/nn/modules/batchnorm.py:276:11
  %1427 : bool = aten::ne(%1426, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1427) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1428 : bool = prim::GetAttr[name="training"](%1425)
   = prim::If(%1428) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1429 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1425)
      %1430 : Tensor = aten::add(%1429, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1425, %1430)
      -> ()
    block1():
      -> ()
  %1431 : bool = prim::GetAttr[name="training"](%1425)
  %1432 : Tensor = prim::GetAttr[name="running_mean"](%1425)
  %1433 : Tensor = prim::GetAttr[name="running_var"](%1425)
  %1434 : Tensor = prim::GetAttr[name="weight"](%1425)
  %1435 : Tensor = prim::GetAttr[name="bias"](%1425)
   = prim::If(%1431) # torch/nn/functional.py:2011:4
    block0():
      %1436 : int[] = aten::size(%x.127) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%1436, %48) # torch/nn/functional.py:1991:17
      %1438 : int = aten::len(%1436) # torch/nn/functional.py:1992:19
      %1439 : int = aten::sub(%1438, %47) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%1439, %49, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %1443 : int = aten::add(%i.52, %47) # torch/nn/functional.py:1993:27
          %1444 : int = aten::__getitem__(%1436, %1443) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %1444) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.207)
      %1446 : bool = aten::eq(%size_prods.205, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1446) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.128 : Tensor = aten::batch_norm(%x.127, %1434, %1435, %1432, %1433, %1431, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.7 : Tensor = aten::relu_(%x.128) # torch/nn/functional.py:1117:17
  %1449 : __torch__.torchvision.models.inception.___torch_mangle_576.BasicConv2d = prim::GetAttr[name="branch7x7_2"](%1384)
  %1450 : __torch__.torch.nn.modules.conv.___torch_mangle_575.Conv2d = prim::GetAttr[name="conv"](%1449)
  %1451 : Tensor = prim::GetAttr[name="weight"](%1450)
  %1452 : Tensor? = prim::GetAttr[name="bias"](%1450)
  %1453 : int[] = prim::ListConstruct(%54, %54)
  %1454 : int[] = prim::ListConstruct(%48, %46)
  %1455 : int[] = prim::ListConstruct(%54, %54)
  %x.129 : Tensor = aten::conv2d(%branch7x7.7, %1451, %1452, %1453, %1454, %1455, %54) # torch/nn/modules/conv.py:415:15
  %1457 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1449)
  %1458 : int = aten::dim(%x.129) # torch/nn/modules/batchnorm.py:276:11
  %1459 : bool = aten::ne(%1458, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1459) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1460 : bool = prim::GetAttr[name="training"](%1457)
   = prim::If(%1460) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1461 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1457)
      %1462 : Tensor = aten::add(%1461, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1457, %1462)
      -> ()
    block1():
      -> ()
  %1463 : bool = prim::GetAttr[name="training"](%1457)
  %1464 : Tensor = prim::GetAttr[name="running_mean"](%1457)
  %1465 : Tensor = prim::GetAttr[name="running_var"](%1457)
  %1466 : Tensor = prim::GetAttr[name="weight"](%1457)
  %1467 : Tensor = prim::GetAttr[name="bias"](%1457)
   = prim::If(%1463) # torch/nn/functional.py:2011:4
    block0():
      %1468 : int[] = aten::size(%x.129) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%1468, %48) # torch/nn/functional.py:1991:17
      %1470 : int = aten::len(%1468) # torch/nn/functional.py:1992:19
      %1471 : int = aten::sub(%1470, %47) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%1471, %49, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %1475 : int = aten::add(%i.53, %47) # torch/nn/functional.py:1993:27
          %1476 : int = aten::__getitem__(%1468, %1475) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %1476) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.211)
      %1478 : bool = aten::eq(%size_prods.209, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1478) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.130 : Tensor = aten::batch_norm(%x.129, %1466, %1467, %1464, %1465, %1463, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.8 : Tensor = aten::relu_(%x.130) # torch/nn/functional.py:1117:17
  %1481 : __torch__.torchvision.models.inception.___torch_mangle_578.BasicConv2d = prim::GetAttr[name="branch7x7_3"](%1384)
  %1482 : __torch__.torch.nn.modules.conv.___torch_mangle_577.Conv2d = prim::GetAttr[name="conv"](%1481)
  %1483 : Tensor = prim::GetAttr[name="weight"](%1482)
  %1484 : Tensor? = prim::GetAttr[name="bias"](%1482)
  %1485 : int[] = prim::ListConstruct(%54, %54)
  %1486 : int[] = prim::ListConstruct(%46, %48)
  %1487 : int[] = prim::ListConstruct(%54, %54)
  %x.131 : Tensor = aten::conv2d(%branch7x7.8, %1483, %1484, %1485, %1486, %1487, %54) # torch/nn/modules/conv.py:415:15
  %1489 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1481)
  %1490 : int = aten::dim(%x.131) # torch/nn/modules/batchnorm.py:276:11
  %1491 : bool = aten::ne(%1490, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1491) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1492 : bool = prim::GetAttr[name="training"](%1489)
   = prim::If(%1492) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1493 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1489)
      %1494 : Tensor = aten::add(%1493, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1489, %1494)
      -> ()
    block1():
      -> ()
  %1495 : bool = prim::GetAttr[name="training"](%1489)
  %1496 : Tensor = prim::GetAttr[name="running_mean"](%1489)
  %1497 : Tensor = prim::GetAttr[name="running_var"](%1489)
  %1498 : Tensor = prim::GetAttr[name="weight"](%1489)
  %1499 : Tensor = prim::GetAttr[name="bias"](%1489)
   = prim::If(%1495) # torch/nn/functional.py:2011:4
    block0():
      %1500 : int[] = aten::size(%x.131) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%1500, %48) # torch/nn/functional.py:1991:17
      %1502 : int = aten::len(%1500) # torch/nn/functional.py:1992:19
      %1503 : int = aten::sub(%1502, %47) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%1503, %49, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %1507 : int = aten::add(%i.54, %47) # torch/nn/functional.py:1993:27
          %1508 : int = aten::__getitem__(%1500, %1507) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %1508) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.215)
      %1510 : bool = aten::eq(%size_prods.213, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1510) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.132 : Tensor = aten::batch_norm(%x.131, %1498, %1499, %1496, %1497, %1495, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.9 : Tensor = aten::relu_(%x.132) # torch/nn/functional.py:1117:17
  %1513 : __torch__.torchvision.models.inception.___torch_mangle_574.BasicConv2d = prim::GetAttr[name="branch7x7dbl_1"](%1384)
  %1514 : __torch__.torch.nn.modules.conv.___torch_mangle_573.Conv2d = prim::GetAttr[name="conv"](%1513)
  %1515 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1516 : Tensor? = prim::GetAttr[name="bias"](%1514)
  %1517 : int[] = prim::ListConstruct(%54, %54)
  %1518 : int[] = prim::ListConstruct(%48, %48)
  %1519 : int[] = prim::ListConstruct(%54, %54)
  %x.133 : Tensor = aten::conv2d(%x.25, %1515, %1516, %1517, %1518, %1519, %54) # torch/nn/modules/conv.py:415:15
  %1521 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1513)
  %1522 : int = aten::dim(%x.133) # torch/nn/modules/batchnorm.py:276:11
  %1523 : bool = aten::ne(%1522, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1523) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1524 : bool = prim::GetAttr[name="training"](%1521)
   = prim::If(%1524) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1525 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1521)
      %1526 : Tensor = aten::add(%1525, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1521, %1526)
      -> ()
    block1():
      -> ()
  %1527 : bool = prim::GetAttr[name="training"](%1521)
  %1528 : Tensor = prim::GetAttr[name="running_mean"](%1521)
  %1529 : Tensor = prim::GetAttr[name="running_var"](%1521)
  %1530 : Tensor = prim::GetAttr[name="weight"](%1521)
  %1531 : Tensor = prim::GetAttr[name="bias"](%1521)
   = prim::If(%1527) # torch/nn/functional.py:2011:4
    block0():
      %1532 : int[] = aten::size(%x.133) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%1532, %48) # torch/nn/functional.py:1991:17
      %1534 : int = aten::len(%1532) # torch/nn/functional.py:1992:19
      %1535 : int = aten::sub(%1534, %47) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%1535, %49, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %1539 : int = aten::add(%i.55, %47) # torch/nn/functional.py:1993:27
          %1540 : int = aten::__getitem__(%1532, %1539) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %1540) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.219)
      %1542 : bool = aten::eq(%size_prods.217, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1542) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.134 : Tensor = aten::batch_norm(%x.133, %1530, %1531, %1528, %1529, %1527, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.11 : Tensor = aten::relu_(%x.134) # torch/nn/functional.py:1117:17
  %1545 : __torch__.torchvision.models.inception.___torch_mangle_580.BasicConv2d = prim::GetAttr[name="branch7x7dbl_2"](%1384)
  %1546 : __torch__.torch.nn.modules.conv.___torch_mangle_579.Conv2d = prim::GetAttr[name="conv"](%1545)
  %1547 : Tensor = prim::GetAttr[name="weight"](%1546)
  %1548 : Tensor? = prim::GetAttr[name="bias"](%1546)
  %1549 : int[] = prim::ListConstruct(%54, %54)
  %1550 : int[] = prim::ListConstruct(%46, %48)
  %1551 : int[] = prim::ListConstruct(%54, %54)
  %x.135 : Tensor = aten::conv2d(%branch7x7dbl.11, %1547, %1548, %1549, %1550, %1551, %54) # torch/nn/modules/conv.py:415:15
  %1553 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1545)
  %1554 : int = aten::dim(%x.135) # torch/nn/modules/batchnorm.py:276:11
  %1555 : bool = aten::ne(%1554, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1555) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1556 : bool = prim::GetAttr[name="training"](%1553)
   = prim::If(%1556) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1557 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1553)
      %1558 : Tensor = aten::add(%1557, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1553, %1558)
      -> ()
    block1():
      -> ()
  %1559 : bool = prim::GetAttr[name="training"](%1553)
  %1560 : Tensor = prim::GetAttr[name="running_mean"](%1553)
  %1561 : Tensor = prim::GetAttr[name="running_var"](%1553)
  %1562 : Tensor = prim::GetAttr[name="weight"](%1553)
  %1563 : Tensor = prim::GetAttr[name="bias"](%1553)
   = prim::If(%1559) # torch/nn/functional.py:2011:4
    block0():
      %1564 : int[] = aten::size(%x.135) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%1564, %48) # torch/nn/functional.py:1991:17
      %1566 : int = aten::len(%1564) # torch/nn/functional.py:1992:19
      %1567 : int = aten::sub(%1566, %47) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%1567, %49, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %1571 : int = aten::add(%i.56, %47) # torch/nn/functional.py:1993:27
          %1572 : int = aten::__getitem__(%1564, %1571) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %1572) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.223)
      %1574 : bool = aten::eq(%size_prods.221, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1574) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.136 : Tensor = aten::batch_norm(%x.135, %1562, %1563, %1560, %1561, %1559, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.12 : Tensor = aten::relu_(%x.136) # torch/nn/functional.py:1117:17
  %1577 : __torch__.torchvision.models.inception.___torch_mangle_576.BasicConv2d = prim::GetAttr[name="branch7x7dbl_3"](%1384)
  %1578 : __torch__.torch.nn.modules.conv.___torch_mangle_575.Conv2d = prim::GetAttr[name="conv"](%1577)
  %1579 : Tensor = prim::GetAttr[name="weight"](%1578)
  %1580 : Tensor? = prim::GetAttr[name="bias"](%1578)
  %1581 : int[] = prim::ListConstruct(%54, %54)
  %1582 : int[] = prim::ListConstruct(%48, %46)
  %1583 : int[] = prim::ListConstruct(%54, %54)
  %x.137 : Tensor = aten::conv2d(%branch7x7dbl.12, %1579, %1580, %1581, %1582, %1583, %54) # torch/nn/modules/conv.py:415:15
  %1585 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1577)
  %1586 : int = aten::dim(%x.137) # torch/nn/modules/batchnorm.py:276:11
  %1587 : bool = aten::ne(%1586, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1587) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1588 : bool = prim::GetAttr[name="training"](%1585)
   = prim::If(%1588) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1589 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1585)
      %1590 : Tensor = aten::add(%1589, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1585, %1590)
      -> ()
    block1():
      -> ()
  %1591 : bool = prim::GetAttr[name="training"](%1585)
  %1592 : Tensor = prim::GetAttr[name="running_mean"](%1585)
  %1593 : Tensor = prim::GetAttr[name="running_var"](%1585)
  %1594 : Tensor = prim::GetAttr[name="weight"](%1585)
  %1595 : Tensor = prim::GetAttr[name="bias"](%1585)
   = prim::If(%1591) # torch/nn/functional.py:2011:4
    block0():
      %1596 : int[] = aten::size(%x.137) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%1596, %48) # torch/nn/functional.py:1991:17
      %1598 : int = aten::len(%1596) # torch/nn/functional.py:1992:19
      %1599 : int = aten::sub(%1598, %47) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%1599, %49, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %1603 : int = aten::add(%i.57, %47) # torch/nn/functional.py:1993:27
          %1604 : int = aten::__getitem__(%1596, %1603) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %1604) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.227)
      %1606 : bool = aten::eq(%size_prods.225, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1606) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.138 : Tensor = aten::batch_norm(%x.137, %1594, %1595, %1592, %1593, %1591, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.13 : Tensor = aten::relu_(%x.138) # torch/nn/functional.py:1117:17
  %1609 : __torch__.torchvision.models.inception.___torch_mangle_580.BasicConv2d = prim::GetAttr[name="branch7x7dbl_4"](%1384)
  %1610 : __torch__.torch.nn.modules.conv.___torch_mangle_579.Conv2d = prim::GetAttr[name="conv"](%1609)
  %1611 : Tensor = prim::GetAttr[name="weight"](%1610)
  %1612 : Tensor? = prim::GetAttr[name="bias"](%1610)
  %1613 : int[] = prim::ListConstruct(%54, %54)
  %1614 : int[] = prim::ListConstruct(%46, %48)
  %1615 : int[] = prim::ListConstruct(%54, %54)
  %x.139 : Tensor = aten::conv2d(%branch7x7dbl.13, %1611, %1612, %1613, %1614, %1615, %54) # torch/nn/modules/conv.py:415:15
  %1617 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1609)
  %1618 : int = aten::dim(%x.139) # torch/nn/modules/batchnorm.py:276:11
  %1619 : bool = aten::ne(%1618, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1619) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1620 : bool = prim::GetAttr[name="training"](%1617)
   = prim::If(%1620) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1621 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1617)
      %1622 : Tensor = aten::add(%1621, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1617, %1622)
      -> ()
    block1():
      -> ()
  %1623 : bool = prim::GetAttr[name="training"](%1617)
  %1624 : Tensor = prim::GetAttr[name="running_mean"](%1617)
  %1625 : Tensor = prim::GetAttr[name="running_var"](%1617)
  %1626 : Tensor = prim::GetAttr[name="weight"](%1617)
  %1627 : Tensor = prim::GetAttr[name="bias"](%1617)
   = prim::If(%1623) # torch/nn/functional.py:2011:4
    block0():
      %1628 : int[] = aten::size(%x.139) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%1628, %48) # torch/nn/functional.py:1991:17
      %1630 : int = aten::len(%1628) # torch/nn/functional.py:1992:19
      %1631 : int = aten::sub(%1630, %47) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%1631, %49, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %1635 : int = aten::add(%i.58, %47) # torch/nn/functional.py:1993:27
          %1636 : int = aten::__getitem__(%1628, %1635) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %1636) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.231)
      %1638 : bool = aten::eq(%size_prods.229, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1638) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.140 : Tensor = aten::batch_norm(%x.139, %1626, %1627, %1624, %1625, %1623, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.14 : Tensor = aten::relu_(%x.140) # torch/nn/functional.py:1117:17
  %1641 : __torch__.torchvision.models.inception.___torch_mangle_582.BasicConv2d = prim::GetAttr[name="branch7x7dbl_5"](%1384)
  %1642 : __torch__.torch.nn.modules.conv.___torch_mangle_581.Conv2d = prim::GetAttr[name="conv"](%1641)
  %1643 : Tensor = prim::GetAttr[name="weight"](%1642)
  %1644 : Tensor? = prim::GetAttr[name="bias"](%1642)
  %1645 : int[] = prim::ListConstruct(%54, %54)
  %1646 : int[] = prim::ListConstruct(%48, %46)
  %1647 : int[] = prim::ListConstruct(%54, %54)
  %x.141 : Tensor = aten::conv2d(%branch7x7dbl.14, %1643, %1644, %1645, %1646, %1647, %54) # torch/nn/modules/conv.py:415:15
  %1649 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1641)
  %1650 : int = aten::dim(%x.141) # torch/nn/modules/batchnorm.py:276:11
  %1651 : bool = aten::ne(%1650, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1651) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1652 : bool = prim::GetAttr[name="training"](%1649)
   = prim::If(%1652) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1653 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1649)
      %1654 : Tensor = aten::add(%1653, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1649, %1654)
      -> ()
    block1():
      -> ()
  %1655 : bool = prim::GetAttr[name="training"](%1649)
  %1656 : Tensor = prim::GetAttr[name="running_mean"](%1649)
  %1657 : Tensor = prim::GetAttr[name="running_var"](%1649)
  %1658 : Tensor = prim::GetAttr[name="weight"](%1649)
  %1659 : Tensor = prim::GetAttr[name="bias"](%1649)
   = prim::If(%1655) # torch/nn/functional.py:2011:4
    block0():
      %1660 : int[] = aten::size(%x.141) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%1660, %48) # torch/nn/functional.py:1991:17
      %1662 : int = aten::len(%1660) # torch/nn/functional.py:1992:19
      %1663 : int = aten::sub(%1662, %47) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%1663, %49, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %1667 : int = aten::add(%i.59, %47) # torch/nn/functional.py:1993:27
          %1668 : int = aten::__getitem__(%1660, %1667) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %1668) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.235)
      %1670 : bool = aten::eq(%size_prods.233, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1670) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.142 : Tensor = aten::batch_norm(%x.141, %1658, %1659, %1656, %1657, %1655, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.15 : Tensor = aten::relu_(%x.142) # torch/nn/functional.py:1117:17
  %1673 : int[] = prim::ListConstruct(%46, %46)
  %1674 : int[] = prim::ListConstruct(%54, %54)
  %1675 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.12 : Tensor = aten::avg_pool2d(%x.25, %1673, %1674, %1675, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:310:22
  %1677 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch_pool"](%1384)
  %1678 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%1677)
  %1679 : Tensor = prim::GetAttr[name="weight"](%1678)
  %1680 : Tensor? = prim::GetAttr[name="bias"](%1678)
  %1681 : int[] = prim::ListConstruct(%54, %54)
  %1682 : int[] = prim::ListConstruct(%48, %48)
  %1683 : int[] = prim::ListConstruct(%54, %54)
  %x.143 : Tensor = aten::conv2d(%branch_pool.12, %1679, %1680, %1681, %1682, %1683, %54) # torch/nn/modules/conv.py:415:15
  %1685 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1677)
  %1686 : int = aten::dim(%x.143) # torch/nn/modules/batchnorm.py:276:11
  %1687 : bool = aten::ne(%1686, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1687) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1688 : bool = prim::GetAttr[name="training"](%1685)
   = prim::If(%1688) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1689 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1685)
      %1690 : Tensor = aten::add(%1689, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1685, %1690)
      -> ()
    block1():
      -> ()
  %1691 : bool = prim::GetAttr[name="training"](%1685)
  %1692 : Tensor = prim::GetAttr[name="running_mean"](%1685)
  %1693 : Tensor = prim::GetAttr[name="running_var"](%1685)
  %1694 : Tensor = prim::GetAttr[name="weight"](%1685)
  %1695 : Tensor = prim::GetAttr[name="bias"](%1685)
   = prim::If(%1691) # torch/nn/functional.py:2011:4
    block0():
      %1696 : int[] = aten::size(%x.143) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%1696, %48) # torch/nn/functional.py:1991:17
      %1698 : int = aten::len(%1696) # torch/nn/functional.py:1992:19
      %1699 : int = aten::sub(%1698, %47) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%1699, %49, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %1703 : int = aten::add(%i.60, %47) # torch/nn/functional.py:1993:27
          %1704 : int = aten::__getitem__(%1696, %1703) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %1704) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.239)
      %1706 : bool = aten::eq(%size_prods.237, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1706) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.144 : Tensor = aten::batch_norm(%x.143, %1694, %1695, %1692, %1693, %1691, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.13 : Tensor = aten::relu_(%x.144) # torch/nn/functional.py:1117:17
  %outputs.8 : Tensor[] = prim::ListConstruct(%branch1x1.6, %branch7x7.9, %branch7x7dbl.15, %branch_pool.13)
  %x.27 : Tensor = aten::cat(%outputs.8, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:318:15
  %1711 : __torch__.torchvision.models.inception.___torch_mangle_583.InceptionC = prim::GetAttr[name="Mixed_6d"](%self)
  %1712 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch1x1"](%1711)
  %1713 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%1712)
  %1714 : Tensor = prim::GetAttr[name="weight"](%1713)
  %1715 : Tensor? = prim::GetAttr[name="bias"](%1713)
  %1716 : int[] = prim::ListConstruct(%54, %54)
  %1717 : int[] = prim::ListConstruct(%48, %48)
  %1718 : int[] = prim::ListConstruct(%54, %54)
  %x.145 : Tensor = aten::conv2d(%x.27, %1714, %1715, %1716, %1717, %1718, %54) # torch/nn/modules/conv.py:415:15
  %1720 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1712)
  %1721 : int = aten::dim(%x.145) # torch/nn/modules/batchnorm.py:276:11
  %1722 : bool = aten::ne(%1721, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1722) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1723 : bool = prim::GetAttr[name="training"](%1720)
   = prim::If(%1723) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1724 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1720)
      %1725 : Tensor = aten::add(%1724, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1720, %1725)
      -> ()
    block1():
      -> ()
  %1726 : bool = prim::GetAttr[name="training"](%1720)
  %1727 : Tensor = prim::GetAttr[name="running_mean"](%1720)
  %1728 : Tensor = prim::GetAttr[name="running_var"](%1720)
  %1729 : Tensor = prim::GetAttr[name="weight"](%1720)
  %1730 : Tensor = prim::GetAttr[name="bias"](%1720)
   = prim::If(%1726) # torch/nn/functional.py:2011:4
    block0():
      %1731 : int[] = aten::size(%x.145) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%1731, %48) # torch/nn/functional.py:1991:17
      %1733 : int = aten::len(%1731) # torch/nn/functional.py:1992:19
      %1734 : int = aten::sub(%1733, %47) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%1734, %49, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %1738 : int = aten::add(%i.61, %47) # torch/nn/functional.py:1993:27
          %1739 : int = aten::__getitem__(%1731, %1738) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %1739) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.243)
      %1741 : bool = aten::eq(%size_prods.241, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1741) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.146 : Tensor = aten::batch_norm(%x.145, %1729, %1730, %1727, %1728, %1726, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.7 : Tensor = aten::relu_(%x.146) # torch/nn/functional.py:1117:17
  %1744 : __torch__.torchvision.models.inception.___torch_mangle_574.BasicConv2d = prim::GetAttr[name="branch7x7_1"](%1711)
  %1745 : __torch__.torch.nn.modules.conv.___torch_mangle_573.Conv2d = prim::GetAttr[name="conv"](%1744)
  %1746 : Tensor = prim::GetAttr[name="weight"](%1745)
  %1747 : Tensor? = prim::GetAttr[name="bias"](%1745)
  %1748 : int[] = prim::ListConstruct(%54, %54)
  %1749 : int[] = prim::ListConstruct(%48, %48)
  %1750 : int[] = prim::ListConstruct(%54, %54)
  %x.147 : Tensor = aten::conv2d(%x.27, %1746, %1747, %1748, %1749, %1750, %54) # torch/nn/modules/conv.py:415:15
  %1752 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1744)
  %1753 : int = aten::dim(%x.147) # torch/nn/modules/batchnorm.py:276:11
  %1754 : bool = aten::ne(%1753, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1754) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1755 : bool = prim::GetAttr[name="training"](%1752)
   = prim::If(%1755) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1756 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1752)
      %1757 : Tensor = aten::add(%1756, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1752, %1757)
      -> ()
    block1():
      -> ()
  %1758 : bool = prim::GetAttr[name="training"](%1752)
  %1759 : Tensor = prim::GetAttr[name="running_mean"](%1752)
  %1760 : Tensor = prim::GetAttr[name="running_var"](%1752)
  %1761 : Tensor = prim::GetAttr[name="weight"](%1752)
  %1762 : Tensor = prim::GetAttr[name="bias"](%1752)
   = prim::If(%1758) # torch/nn/functional.py:2011:4
    block0():
      %1763 : int[] = aten::size(%x.147) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%1763, %48) # torch/nn/functional.py:1991:17
      %1765 : int = aten::len(%1763) # torch/nn/functional.py:1992:19
      %1766 : int = aten::sub(%1765, %47) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%1766, %49, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %1770 : int = aten::add(%i.62, %47) # torch/nn/functional.py:1993:27
          %1771 : int = aten::__getitem__(%1763, %1770) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %1771) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.247)
      %1773 : bool = aten::eq(%size_prods.245, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1773) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.148 : Tensor = aten::batch_norm(%x.147, %1761, %1762, %1759, %1760, %1758, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.10 : Tensor = aten::relu_(%x.148) # torch/nn/functional.py:1117:17
  %1776 : __torch__.torchvision.models.inception.___torch_mangle_576.BasicConv2d = prim::GetAttr[name="branch7x7_2"](%1711)
  %1777 : __torch__.torch.nn.modules.conv.___torch_mangle_575.Conv2d = prim::GetAttr[name="conv"](%1776)
  %1778 : Tensor = prim::GetAttr[name="weight"](%1777)
  %1779 : Tensor? = prim::GetAttr[name="bias"](%1777)
  %1780 : int[] = prim::ListConstruct(%54, %54)
  %1781 : int[] = prim::ListConstruct(%48, %46)
  %1782 : int[] = prim::ListConstruct(%54, %54)
  %x.149 : Tensor = aten::conv2d(%branch7x7.10, %1778, %1779, %1780, %1781, %1782, %54) # torch/nn/modules/conv.py:415:15
  %1784 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1776)
  %1785 : int = aten::dim(%x.149) # torch/nn/modules/batchnorm.py:276:11
  %1786 : bool = aten::ne(%1785, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1786) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1787 : bool = prim::GetAttr[name="training"](%1784)
   = prim::If(%1787) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1788 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1784)
      %1789 : Tensor = aten::add(%1788, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1784, %1789)
      -> ()
    block1():
      -> ()
  %1790 : bool = prim::GetAttr[name="training"](%1784)
  %1791 : Tensor = prim::GetAttr[name="running_mean"](%1784)
  %1792 : Tensor = prim::GetAttr[name="running_var"](%1784)
  %1793 : Tensor = prim::GetAttr[name="weight"](%1784)
  %1794 : Tensor = prim::GetAttr[name="bias"](%1784)
   = prim::If(%1790) # torch/nn/functional.py:2011:4
    block0():
      %1795 : int[] = aten::size(%x.149) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%1795, %48) # torch/nn/functional.py:1991:17
      %1797 : int = aten::len(%1795) # torch/nn/functional.py:1992:19
      %1798 : int = aten::sub(%1797, %47) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%1798, %49, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %1802 : int = aten::add(%i.63, %47) # torch/nn/functional.py:1993:27
          %1803 : int = aten::__getitem__(%1795, %1802) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %1803) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.251)
      %1805 : bool = aten::eq(%size_prods.249, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1805) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.150 : Tensor = aten::batch_norm(%x.149, %1793, %1794, %1791, %1792, %1790, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.11 : Tensor = aten::relu_(%x.150) # torch/nn/functional.py:1117:17
  %1808 : __torch__.torchvision.models.inception.___torch_mangle_578.BasicConv2d = prim::GetAttr[name="branch7x7_3"](%1711)
  %1809 : __torch__.torch.nn.modules.conv.___torch_mangle_577.Conv2d = prim::GetAttr[name="conv"](%1808)
  %1810 : Tensor = prim::GetAttr[name="weight"](%1809)
  %1811 : Tensor? = prim::GetAttr[name="bias"](%1809)
  %1812 : int[] = prim::ListConstruct(%54, %54)
  %1813 : int[] = prim::ListConstruct(%46, %48)
  %1814 : int[] = prim::ListConstruct(%54, %54)
  %x.151 : Tensor = aten::conv2d(%branch7x7.11, %1810, %1811, %1812, %1813, %1814, %54) # torch/nn/modules/conv.py:415:15
  %1816 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1808)
  %1817 : int = aten::dim(%x.151) # torch/nn/modules/batchnorm.py:276:11
  %1818 : bool = aten::ne(%1817, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1818) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1819 : bool = prim::GetAttr[name="training"](%1816)
   = prim::If(%1819) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1820 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1816)
      %1821 : Tensor = aten::add(%1820, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1816, %1821)
      -> ()
    block1():
      -> ()
  %1822 : bool = prim::GetAttr[name="training"](%1816)
  %1823 : Tensor = prim::GetAttr[name="running_mean"](%1816)
  %1824 : Tensor = prim::GetAttr[name="running_var"](%1816)
  %1825 : Tensor = prim::GetAttr[name="weight"](%1816)
  %1826 : Tensor = prim::GetAttr[name="bias"](%1816)
   = prim::If(%1822) # torch/nn/functional.py:2011:4
    block0():
      %1827 : int[] = aten::size(%x.151) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%1827, %48) # torch/nn/functional.py:1991:17
      %1829 : int = aten::len(%1827) # torch/nn/functional.py:1992:19
      %1830 : int = aten::sub(%1829, %47) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%1830, %49, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %1834 : int = aten::add(%i.64, %47) # torch/nn/functional.py:1993:27
          %1835 : int = aten::__getitem__(%1827, %1834) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %1835) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.255)
      %1837 : bool = aten::eq(%size_prods.253, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1837) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.152 : Tensor = aten::batch_norm(%x.151, %1825, %1826, %1823, %1824, %1822, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.12 : Tensor = aten::relu_(%x.152) # torch/nn/functional.py:1117:17
  %1840 : __torch__.torchvision.models.inception.___torch_mangle_574.BasicConv2d = prim::GetAttr[name="branch7x7dbl_1"](%1711)
  %1841 : __torch__.torch.nn.modules.conv.___torch_mangle_573.Conv2d = prim::GetAttr[name="conv"](%1840)
  %1842 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1843 : Tensor? = prim::GetAttr[name="bias"](%1841)
  %1844 : int[] = prim::ListConstruct(%54, %54)
  %1845 : int[] = prim::ListConstruct(%48, %48)
  %1846 : int[] = prim::ListConstruct(%54, %54)
  %x.153 : Tensor = aten::conv2d(%x.27, %1842, %1843, %1844, %1845, %1846, %54) # torch/nn/modules/conv.py:415:15
  %1848 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1840)
  %1849 : int = aten::dim(%x.153) # torch/nn/modules/batchnorm.py:276:11
  %1850 : bool = aten::ne(%1849, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1850) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1851 : bool = prim::GetAttr[name="training"](%1848)
   = prim::If(%1851) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1852 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1848)
      %1853 : Tensor = aten::add(%1852, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1848, %1853)
      -> ()
    block1():
      -> ()
  %1854 : bool = prim::GetAttr[name="training"](%1848)
  %1855 : Tensor = prim::GetAttr[name="running_mean"](%1848)
  %1856 : Tensor = prim::GetAttr[name="running_var"](%1848)
  %1857 : Tensor = prim::GetAttr[name="weight"](%1848)
  %1858 : Tensor = prim::GetAttr[name="bias"](%1848)
   = prim::If(%1854) # torch/nn/functional.py:2011:4
    block0():
      %1859 : int[] = aten::size(%x.153) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%1859, %48) # torch/nn/functional.py:1991:17
      %1861 : int = aten::len(%1859) # torch/nn/functional.py:1992:19
      %1862 : int = aten::sub(%1861, %47) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%1862, %49, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %1866 : int = aten::add(%i.65, %47) # torch/nn/functional.py:1993:27
          %1867 : int = aten::__getitem__(%1859, %1866) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %1867) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.259)
      %1869 : bool = aten::eq(%size_prods.257, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1869) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.154 : Tensor = aten::batch_norm(%x.153, %1857, %1858, %1855, %1856, %1854, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.16 : Tensor = aten::relu_(%x.154) # torch/nn/functional.py:1117:17
  %1872 : __torch__.torchvision.models.inception.___torch_mangle_580.BasicConv2d = prim::GetAttr[name="branch7x7dbl_2"](%1711)
  %1873 : __torch__.torch.nn.modules.conv.___torch_mangle_579.Conv2d = prim::GetAttr[name="conv"](%1872)
  %1874 : Tensor = prim::GetAttr[name="weight"](%1873)
  %1875 : Tensor? = prim::GetAttr[name="bias"](%1873)
  %1876 : int[] = prim::ListConstruct(%54, %54)
  %1877 : int[] = prim::ListConstruct(%46, %48)
  %1878 : int[] = prim::ListConstruct(%54, %54)
  %x.155 : Tensor = aten::conv2d(%branch7x7dbl.16, %1874, %1875, %1876, %1877, %1878, %54) # torch/nn/modules/conv.py:415:15
  %1880 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1872)
  %1881 : int = aten::dim(%x.155) # torch/nn/modules/batchnorm.py:276:11
  %1882 : bool = aten::ne(%1881, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1882) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1883 : bool = prim::GetAttr[name="training"](%1880)
   = prim::If(%1883) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1884 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1880)
      %1885 : Tensor = aten::add(%1884, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1880, %1885)
      -> ()
    block1():
      -> ()
  %1886 : bool = prim::GetAttr[name="training"](%1880)
  %1887 : Tensor = prim::GetAttr[name="running_mean"](%1880)
  %1888 : Tensor = prim::GetAttr[name="running_var"](%1880)
  %1889 : Tensor = prim::GetAttr[name="weight"](%1880)
  %1890 : Tensor = prim::GetAttr[name="bias"](%1880)
   = prim::If(%1886) # torch/nn/functional.py:2011:4
    block0():
      %1891 : int[] = aten::size(%x.155) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%1891, %48) # torch/nn/functional.py:1991:17
      %1893 : int = aten::len(%1891) # torch/nn/functional.py:1992:19
      %1894 : int = aten::sub(%1893, %47) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%1894, %49, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %1898 : int = aten::add(%i.66, %47) # torch/nn/functional.py:1993:27
          %1899 : int = aten::__getitem__(%1891, %1898) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %1899) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.263)
      %1901 : bool = aten::eq(%size_prods.261, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1901) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.156 : Tensor = aten::batch_norm(%x.155, %1889, %1890, %1887, %1888, %1886, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.17 : Tensor = aten::relu_(%x.156) # torch/nn/functional.py:1117:17
  %1904 : __torch__.torchvision.models.inception.___torch_mangle_576.BasicConv2d = prim::GetAttr[name="branch7x7dbl_3"](%1711)
  %1905 : __torch__.torch.nn.modules.conv.___torch_mangle_575.Conv2d = prim::GetAttr[name="conv"](%1904)
  %1906 : Tensor = prim::GetAttr[name="weight"](%1905)
  %1907 : Tensor? = prim::GetAttr[name="bias"](%1905)
  %1908 : int[] = prim::ListConstruct(%54, %54)
  %1909 : int[] = prim::ListConstruct(%48, %46)
  %1910 : int[] = prim::ListConstruct(%54, %54)
  %x.157 : Tensor = aten::conv2d(%branch7x7dbl.17, %1906, %1907, %1908, %1909, %1910, %54) # torch/nn/modules/conv.py:415:15
  %1912 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1904)
  %1913 : int = aten::dim(%x.157) # torch/nn/modules/batchnorm.py:276:11
  %1914 : bool = aten::ne(%1913, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1914) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1915 : bool = prim::GetAttr[name="training"](%1912)
   = prim::If(%1915) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1916 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1912)
      %1917 : Tensor = aten::add(%1916, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1912, %1917)
      -> ()
    block1():
      -> ()
  %1918 : bool = prim::GetAttr[name="training"](%1912)
  %1919 : Tensor = prim::GetAttr[name="running_mean"](%1912)
  %1920 : Tensor = prim::GetAttr[name="running_var"](%1912)
  %1921 : Tensor = prim::GetAttr[name="weight"](%1912)
  %1922 : Tensor = prim::GetAttr[name="bias"](%1912)
   = prim::If(%1918) # torch/nn/functional.py:2011:4
    block0():
      %1923 : int[] = aten::size(%x.157) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%1923, %48) # torch/nn/functional.py:1991:17
      %1925 : int = aten::len(%1923) # torch/nn/functional.py:1992:19
      %1926 : int = aten::sub(%1925, %47) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%1926, %49, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %1930 : int = aten::add(%i.67, %47) # torch/nn/functional.py:1993:27
          %1931 : int = aten::__getitem__(%1923, %1930) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %1931) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.267)
      %1933 : bool = aten::eq(%size_prods.265, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1933) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.158 : Tensor = aten::batch_norm(%x.157, %1921, %1922, %1919, %1920, %1918, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.18 : Tensor = aten::relu_(%x.158) # torch/nn/functional.py:1117:17
  %1936 : __torch__.torchvision.models.inception.___torch_mangle_580.BasicConv2d = prim::GetAttr[name="branch7x7dbl_4"](%1711)
  %1937 : __torch__.torch.nn.modules.conv.___torch_mangle_579.Conv2d = prim::GetAttr[name="conv"](%1936)
  %1938 : Tensor = prim::GetAttr[name="weight"](%1937)
  %1939 : Tensor? = prim::GetAttr[name="bias"](%1937)
  %1940 : int[] = prim::ListConstruct(%54, %54)
  %1941 : int[] = prim::ListConstruct(%46, %48)
  %1942 : int[] = prim::ListConstruct(%54, %54)
  %x.159 : Tensor = aten::conv2d(%branch7x7dbl.18, %1938, %1939, %1940, %1941, %1942, %54) # torch/nn/modules/conv.py:415:15
  %1944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1936)
  %1945 : int = aten::dim(%x.159) # torch/nn/modules/batchnorm.py:276:11
  %1946 : bool = aten::ne(%1945, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1946) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1947 : bool = prim::GetAttr[name="training"](%1944)
   = prim::If(%1947) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1948 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1944)
      %1949 : Tensor = aten::add(%1948, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1944, %1949)
      -> ()
    block1():
      -> ()
  %1950 : bool = prim::GetAttr[name="training"](%1944)
  %1951 : Tensor = prim::GetAttr[name="running_mean"](%1944)
  %1952 : Tensor = prim::GetAttr[name="running_var"](%1944)
  %1953 : Tensor = prim::GetAttr[name="weight"](%1944)
  %1954 : Tensor = prim::GetAttr[name="bias"](%1944)
   = prim::If(%1950) # torch/nn/functional.py:2011:4
    block0():
      %1955 : int[] = aten::size(%x.159) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%1955, %48) # torch/nn/functional.py:1991:17
      %1957 : int = aten::len(%1955) # torch/nn/functional.py:1992:19
      %1958 : int = aten::sub(%1957, %47) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%1958, %49, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %1962 : int = aten::add(%i.68, %47) # torch/nn/functional.py:1993:27
          %1963 : int = aten::__getitem__(%1955, %1962) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %1963) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.271)
      %1965 : bool = aten::eq(%size_prods.269, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1965) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.160 : Tensor = aten::batch_norm(%x.159, %1953, %1954, %1951, %1952, %1950, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.19 : Tensor = aten::relu_(%x.160) # torch/nn/functional.py:1117:17
  %1968 : __torch__.torchvision.models.inception.___torch_mangle_582.BasicConv2d = prim::GetAttr[name="branch7x7dbl_5"](%1711)
  %1969 : __torch__.torch.nn.modules.conv.___torch_mangle_581.Conv2d = prim::GetAttr[name="conv"](%1968)
  %1970 : Tensor = prim::GetAttr[name="weight"](%1969)
  %1971 : Tensor? = prim::GetAttr[name="bias"](%1969)
  %1972 : int[] = prim::ListConstruct(%54, %54)
  %1973 : int[] = prim::ListConstruct(%48, %46)
  %1974 : int[] = prim::ListConstruct(%54, %54)
  %x.161 : Tensor = aten::conv2d(%branch7x7dbl.19, %1970, %1971, %1972, %1973, %1974, %54) # torch/nn/modules/conv.py:415:15
  %1976 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1968)
  %1977 : int = aten::dim(%x.161) # torch/nn/modules/batchnorm.py:276:11
  %1978 : bool = aten::ne(%1977, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1978) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1979 : bool = prim::GetAttr[name="training"](%1976)
   = prim::If(%1979) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1980 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1976)
      %1981 : Tensor = aten::add(%1980, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1976, %1981)
      -> ()
    block1():
      -> ()
  %1982 : bool = prim::GetAttr[name="training"](%1976)
  %1983 : Tensor = prim::GetAttr[name="running_mean"](%1976)
  %1984 : Tensor = prim::GetAttr[name="running_var"](%1976)
  %1985 : Tensor = prim::GetAttr[name="weight"](%1976)
  %1986 : Tensor = prim::GetAttr[name="bias"](%1976)
   = prim::If(%1982) # torch/nn/functional.py:2011:4
    block0():
      %1987 : int[] = aten::size(%x.161) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%1987, %48) # torch/nn/functional.py:1991:17
      %1989 : int = aten::len(%1987) # torch/nn/functional.py:1992:19
      %1990 : int = aten::sub(%1989, %47) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%1990, %49, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %1994 : int = aten::add(%i.69, %47) # torch/nn/functional.py:1993:27
          %1995 : int = aten::__getitem__(%1987, %1994) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %1995) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.275)
      %1997 : bool = aten::eq(%size_prods.273, %54) # torch/nn/functional.py:1994:7
       = prim::If(%1997) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.162 : Tensor = aten::batch_norm(%x.161, %1985, %1986, %1983, %1984, %1982, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.20 : Tensor = aten::relu_(%x.162) # torch/nn/functional.py:1117:17
  %2000 : int[] = prim::ListConstruct(%46, %46)
  %2001 : int[] = prim::ListConstruct(%54, %54)
  %2002 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.14 : Tensor = aten::avg_pool2d(%x.27, %2000, %2001, %2002, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:310:22
  %2004 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch_pool"](%1711)
  %2005 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2004)
  %2006 : Tensor = prim::GetAttr[name="weight"](%2005)
  %2007 : Tensor? = prim::GetAttr[name="bias"](%2005)
  %2008 : int[] = prim::ListConstruct(%54, %54)
  %2009 : int[] = prim::ListConstruct(%48, %48)
  %2010 : int[] = prim::ListConstruct(%54, %54)
  %x.163 : Tensor = aten::conv2d(%branch_pool.14, %2006, %2007, %2008, %2009, %2010, %54) # torch/nn/modules/conv.py:415:15
  %2012 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2004)
  %2013 : int = aten::dim(%x.163) # torch/nn/modules/batchnorm.py:276:11
  %2014 : bool = aten::ne(%2013, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2014) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2015 : bool = prim::GetAttr[name="training"](%2012)
   = prim::If(%2015) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2016 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2012)
      %2017 : Tensor = aten::add(%2016, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2012, %2017)
      -> ()
    block1():
      -> ()
  %2018 : bool = prim::GetAttr[name="training"](%2012)
  %2019 : Tensor = prim::GetAttr[name="running_mean"](%2012)
  %2020 : Tensor = prim::GetAttr[name="running_var"](%2012)
  %2021 : Tensor = prim::GetAttr[name="weight"](%2012)
  %2022 : Tensor = prim::GetAttr[name="bias"](%2012)
   = prim::If(%2018) # torch/nn/functional.py:2011:4
    block0():
      %2023 : int[] = aten::size(%x.163) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%2023, %48) # torch/nn/functional.py:1991:17
      %2025 : int = aten::len(%2023) # torch/nn/functional.py:1992:19
      %2026 : int = aten::sub(%2025, %47) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%2026, %49, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %2030 : int = aten::add(%i.70, %47) # torch/nn/functional.py:1993:27
          %2031 : int = aten::__getitem__(%2023, %2030) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %2031) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.279)
      %2033 : bool = aten::eq(%size_prods.277, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2033) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.164 : Tensor = aten::batch_norm(%x.163, %2021, %2022, %2019, %2020, %2018, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.15 : Tensor = aten::relu_(%x.164) # torch/nn/functional.py:1117:17
  %outputs.9 : Tensor[] = prim::ListConstruct(%branch1x1.7, %branch7x7.12, %branch7x7dbl.20, %branch_pool.15)
  %x.29 : Tensor = aten::cat(%outputs.9, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:318:15
  %2038 : __torch__.torchvision.models.inception.___torch_mangle_588.InceptionC = prim::GetAttr[name="Mixed_6e"](%self)
  %2039 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch1x1"](%2038)
  %2040 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2039)
  %2041 : Tensor = prim::GetAttr[name="weight"](%2040)
  %2042 : Tensor? = prim::GetAttr[name="bias"](%2040)
  %2043 : int[] = prim::ListConstruct(%54, %54)
  %2044 : int[] = prim::ListConstruct(%48, %48)
  %2045 : int[] = prim::ListConstruct(%54, %54)
  %x.165 : Tensor = aten::conv2d(%x.29, %2041, %2042, %2043, %2044, %2045, %54) # torch/nn/modules/conv.py:415:15
  %2047 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2039)
  %2048 : int = aten::dim(%x.165) # torch/nn/modules/batchnorm.py:276:11
  %2049 : bool = aten::ne(%2048, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2049) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2050 : bool = prim::GetAttr[name="training"](%2047)
   = prim::If(%2050) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2051 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2047)
      %2052 : Tensor = aten::add(%2051, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2047, %2052)
      -> ()
    block1():
      -> ()
  %2053 : bool = prim::GetAttr[name="training"](%2047)
  %2054 : Tensor = prim::GetAttr[name="running_mean"](%2047)
  %2055 : Tensor = prim::GetAttr[name="running_var"](%2047)
  %2056 : Tensor = prim::GetAttr[name="weight"](%2047)
  %2057 : Tensor = prim::GetAttr[name="bias"](%2047)
   = prim::If(%2053) # torch/nn/functional.py:2011:4
    block0():
      %2058 : int[] = aten::size(%x.165) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%2058, %48) # torch/nn/functional.py:1991:17
      %2060 : int = aten::len(%2058) # torch/nn/functional.py:1992:19
      %2061 : int = aten::sub(%2060, %47) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%2061, %49, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %2065 : int = aten::add(%i.71, %47) # torch/nn/functional.py:1993:27
          %2066 : int = aten::__getitem__(%2058, %2065) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %2066) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.283)
      %2068 : bool = aten::eq(%size_prods.281, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2068) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.168 : Tensor = aten::batch_norm(%x.165, %2056, %2057, %2054, %2055, %2053, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.8 : Tensor = aten::relu_(%x.168) # torch/nn/functional.py:1117:17
  %2071 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch7x7_1"](%2038)
  %2072 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2071)
  %2073 : Tensor = prim::GetAttr[name="weight"](%2072)
  %2074 : Tensor? = prim::GetAttr[name="bias"](%2072)
  %2075 : int[] = prim::ListConstruct(%54, %54)
  %2076 : int[] = prim::ListConstruct(%48, %48)
  %2077 : int[] = prim::ListConstruct(%54, %54)
  %x.171 : Tensor = aten::conv2d(%x.29, %2073, %2074, %2075, %2076, %2077, %54) # torch/nn/modules/conv.py:415:15
  %2079 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2071)
  %2080 : int = aten::dim(%x.171) # torch/nn/modules/batchnorm.py:276:11
  %2081 : bool = aten::ne(%2080, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2081) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2082 : bool = prim::GetAttr[name="training"](%2079)
   = prim::If(%2082) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2083 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2079)
      %2084 : Tensor = aten::add(%2083, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2079, %2084)
      -> ()
    block1():
      -> ()
  %2085 : bool = prim::GetAttr[name="training"](%2079)
  %2086 : Tensor = prim::GetAttr[name="running_mean"](%2079)
  %2087 : Tensor = prim::GetAttr[name="running_var"](%2079)
  %2088 : Tensor = prim::GetAttr[name="weight"](%2079)
  %2089 : Tensor = prim::GetAttr[name="bias"](%2079)
   = prim::If(%2085) # torch/nn/functional.py:2011:4
    block0():
      %2090 : int[] = aten::size(%x.171) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%2090, %48) # torch/nn/functional.py:1991:17
      %2092 : int = aten::len(%2090) # torch/nn/functional.py:1992:19
      %2093 : int = aten::sub(%2092, %47) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%2093, %49, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %2097 : int = aten::add(%i.74, %47) # torch/nn/functional.py:1993:27
          %2098 : int = aten::__getitem__(%2090, %2097) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %2098) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.295)
      %2100 : bool = aten::eq(%size_prods.293, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2100) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.166 : Tensor = aten::batch_norm(%x.171, %2088, %2089, %2086, %2087, %2085, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.1 : Tensor = aten::relu_(%x.166) # torch/nn/functional.py:1117:17
  %2103 : __torch__.torchvision.models.inception.___torch_mangle_585.BasicConv2d = prim::GetAttr[name="branch7x7_2"](%2038)
  %2104 : __torch__.torch.nn.modules.conv.___torch_mangle_584.Conv2d = prim::GetAttr[name="conv"](%2103)
  %2105 : Tensor = prim::GetAttr[name="weight"](%2104)
  %2106 : Tensor? = prim::GetAttr[name="bias"](%2104)
  %2107 : int[] = prim::ListConstruct(%54, %54)
  %2108 : int[] = prim::ListConstruct(%48, %46)
  %2109 : int[] = prim::ListConstruct(%54, %54)
  %x.172 : Tensor = aten::conv2d(%branch7x7.1, %2105, %2106, %2107, %2108, %2109, %54) # torch/nn/modules/conv.py:415:15
  %2111 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2103)
  %2112 : int = aten::dim(%x.172) # torch/nn/modules/batchnorm.py:276:11
  %2113 : bool = aten::ne(%2112, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2113) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2114 : bool = prim::GetAttr[name="training"](%2111)
   = prim::If(%2114) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2115 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2111)
      %2116 : Tensor = aten::add(%2115, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2111, %2116)
      -> ()
    block1():
      -> ()
  %2117 : bool = prim::GetAttr[name="training"](%2111)
  %2118 : Tensor = prim::GetAttr[name="running_mean"](%2111)
  %2119 : Tensor = prim::GetAttr[name="running_var"](%2111)
  %2120 : Tensor = prim::GetAttr[name="weight"](%2111)
  %2121 : Tensor = prim::GetAttr[name="bias"](%2111)
   = prim::If(%2117) # torch/nn/functional.py:2011:4
    block0():
      %2122 : int[] = aten::size(%x.172) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%2122, %48) # torch/nn/functional.py:1991:17
      %2124 : int = aten::len(%2122) # torch/nn/functional.py:1992:19
      %2125 : int = aten::sub(%2124, %47) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%2125, %49, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %2129 : int = aten::add(%i.75, %47) # torch/nn/functional.py:1993:27
          %2130 : int = aten::__getitem__(%2122, %2129) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %2130) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.299)
      %2132 : bool = aten::eq(%size_prods.297, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2132) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.167 : Tensor = aten::batch_norm(%x.172, %2120, %2121, %2118, %2119, %2117, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.3 : Tensor = aten::relu_(%x.167) # torch/nn/functional.py:1117:17
  %2135 : __torch__.torchvision.models.inception.___torch_mangle_587.BasicConv2d = prim::GetAttr[name="branch7x7_3"](%2038)
  %2136 : __torch__.torch.nn.modules.conv.___torch_mangle_586.Conv2d = prim::GetAttr[name="conv"](%2135)
  %2137 : Tensor = prim::GetAttr[name="weight"](%2136)
  %2138 : Tensor? = prim::GetAttr[name="bias"](%2136)
  %2139 : int[] = prim::ListConstruct(%54, %54)
  %2140 : int[] = prim::ListConstruct(%46, %48)
  %2141 : int[] = prim::ListConstruct(%54, %54)
  %x.173 : Tensor = aten::conv2d(%branch7x7.3, %2137, %2138, %2139, %2140, %2141, %54) # torch/nn/modules/conv.py:415:15
  %2143 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2135)
  %2144 : int = aten::dim(%x.173) # torch/nn/modules/batchnorm.py:276:11
  %2145 : bool = aten::ne(%2144, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2145) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2146 : bool = prim::GetAttr[name="training"](%2143)
   = prim::If(%2146) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2147 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2143)
      %2148 : Tensor = aten::add(%2147, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2143, %2148)
      -> ()
    block1():
      -> ()
  %2149 : bool = prim::GetAttr[name="training"](%2143)
  %2150 : Tensor = prim::GetAttr[name="running_mean"](%2143)
  %2151 : Tensor = prim::GetAttr[name="running_var"](%2143)
  %2152 : Tensor = prim::GetAttr[name="weight"](%2143)
  %2153 : Tensor = prim::GetAttr[name="bias"](%2143)
   = prim::If(%2149) # torch/nn/functional.py:2011:4
    block0():
      %2154 : int[] = aten::size(%x.173) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%2154, %48) # torch/nn/functional.py:1991:17
      %2156 : int = aten::len(%2154) # torch/nn/functional.py:1992:19
      %2157 : int = aten::sub(%2156, %47) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%2157, %49, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %2161 : int = aten::add(%i.76, %47) # torch/nn/functional.py:1993:27
          %2162 : int = aten::__getitem__(%2154, %2161) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %2162) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.303)
      %2164 : bool = aten::eq(%size_prods.301, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2164) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.181 : Tensor = aten::batch_norm(%x.173, %2152, %2153, %2150, %2151, %2149, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7.5 : Tensor = aten::relu_(%x.181) # torch/nn/functional.py:1117:17
  %2167 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch7x7dbl_1"](%2038)
  %2168 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2167)
  %2169 : Tensor = prim::GetAttr[name="weight"](%2168)
  %2170 : Tensor? = prim::GetAttr[name="bias"](%2168)
  %2171 : int[] = prim::ListConstruct(%54, %54)
  %2172 : int[] = prim::ListConstruct(%48, %48)
  %2173 : int[] = prim::ListConstruct(%54, %54)
  %x.174 : Tensor = aten::conv2d(%x.29, %2169, %2170, %2171, %2172, %2173, %54) # torch/nn/modules/conv.py:415:15
  %2175 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2167)
  %2176 : int = aten::dim(%x.174) # torch/nn/modules/batchnorm.py:276:11
  %2177 : bool = aten::ne(%2176, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2177) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2178 : bool = prim::GetAttr[name="training"](%2175)
   = prim::If(%2178) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2179 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2175)
      %2180 : Tensor = aten::add(%2179, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2175, %2180)
      -> ()
    block1():
      -> ()
  %2181 : bool = prim::GetAttr[name="training"](%2175)
  %2182 : Tensor = prim::GetAttr[name="running_mean"](%2175)
  %2183 : Tensor = prim::GetAttr[name="running_var"](%2175)
  %2184 : Tensor = prim::GetAttr[name="weight"](%2175)
  %2185 : Tensor = prim::GetAttr[name="bias"](%2175)
   = prim::If(%2181) # torch/nn/functional.py:2011:4
    block0():
      %2186 : int[] = aten::size(%x.174) # torch/nn/functional.py:2012:27
      %size_prods.304 : int = aten::__getitem__(%2186, %48) # torch/nn/functional.py:1991:17
      %2188 : int = aten::len(%2186) # torch/nn/functional.py:1992:19
      %2189 : int = aten::sub(%2188, %47) # torch/nn/functional.py:1992:19
      %size_prods.305 : int = prim::Loop(%2189, %49, %size_prods.304) # torch/nn/functional.py:1992:4
        block0(%i.77 : int, %size_prods.306 : int):
          %2193 : int = aten::add(%i.77, %47) # torch/nn/functional.py:1993:27
          %2194 : int = aten::__getitem__(%2186, %2193) # torch/nn/functional.py:1993:22
          %size_prods.307 : int = aten::mul(%size_prods.306, %2194) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.307)
      %2196 : bool = aten::eq(%size_prods.305, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2196) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.183 : Tensor = aten::batch_norm(%x.174, %2184, %2185, %2182, %2183, %2181, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.1 : Tensor = aten::relu_(%x.183) # torch/nn/functional.py:1117:17
  %2199 : __torch__.torchvision.models.inception.___torch_mangle_587.BasicConv2d = prim::GetAttr[name="branch7x7dbl_2"](%2038)
  %2200 : __torch__.torch.nn.modules.conv.___torch_mangle_586.Conv2d = prim::GetAttr[name="conv"](%2199)
  %2201 : Tensor = prim::GetAttr[name="weight"](%2200)
  %2202 : Tensor? = prim::GetAttr[name="bias"](%2200)
  %2203 : int[] = prim::ListConstruct(%54, %54)
  %2204 : int[] = prim::ListConstruct(%46, %48)
  %2205 : int[] = prim::ListConstruct(%54, %54)
  %x.196 : Tensor = aten::conv2d(%branch7x7dbl.1, %2201, %2202, %2203, %2204, %2205, %54) # torch/nn/modules/conv.py:415:15
  %2207 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2199)
  %2208 : int = aten::dim(%x.196) # torch/nn/modules/batchnorm.py:276:11
  %2209 : bool = aten::ne(%2208, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2209) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2210 : bool = prim::GetAttr[name="training"](%2207)
   = prim::If(%2210) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2211 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2207)
      %2212 : Tensor = aten::add(%2211, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2207, %2212)
      -> ()
    block1():
      -> ()
  %2213 : bool = prim::GetAttr[name="training"](%2207)
  %2214 : Tensor = prim::GetAttr[name="running_mean"](%2207)
  %2215 : Tensor = prim::GetAttr[name="running_var"](%2207)
  %2216 : Tensor = prim::GetAttr[name="weight"](%2207)
  %2217 : Tensor = prim::GetAttr[name="bias"](%2207)
   = prim::If(%2213) # torch/nn/functional.py:2011:4
    block0():
      %2218 : int[] = aten::size(%x.196) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%2218, %48) # torch/nn/functional.py:1991:17
      %2220 : int = aten::len(%2218) # torch/nn/functional.py:1992:19
      %2221 : int = aten::sub(%2220, %47) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%2221, %49, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %2225 : int = aten::add(%i.84, %47) # torch/nn/functional.py:1993:27
          %2226 : int = aten::__getitem__(%2218, %2225) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %2226) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.335)
      %2228 : bool = aten::eq(%size_prods.333, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2228) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.197 : Tensor = aten::batch_norm(%x.196, %2216, %2217, %2214, %2215, %2213, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.3 : Tensor = aten::relu_(%x.197) # torch/nn/functional.py:1117:17
  %2231 : __torch__.torchvision.models.inception.___torch_mangle_585.BasicConv2d = prim::GetAttr[name="branch7x7dbl_3"](%2038)
  %2232 : __torch__.torch.nn.modules.conv.___torch_mangle_584.Conv2d = prim::GetAttr[name="conv"](%2231)
  %2233 : Tensor = prim::GetAttr[name="weight"](%2232)
  %2234 : Tensor? = prim::GetAttr[name="bias"](%2232)
  %2235 : int[] = prim::ListConstruct(%54, %54)
  %2236 : int[] = prim::ListConstruct(%48, %46)
  %2237 : int[] = prim::ListConstruct(%54, %54)
  %x.198 : Tensor = aten::conv2d(%branch7x7dbl.3, %2233, %2234, %2235, %2236, %2237, %54) # torch/nn/modules/conv.py:415:15
  %2239 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2231)
  %2240 : int = aten::dim(%x.198) # torch/nn/modules/batchnorm.py:276:11
  %2241 : bool = aten::ne(%2240, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2241) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2242 : bool = prim::GetAttr[name="training"](%2239)
   = prim::If(%2242) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2243 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2239)
      %2244 : Tensor = aten::add(%2243, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2239, %2244)
      -> ()
    block1():
      -> ()
  %2245 : bool = prim::GetAttr[name="training"](%2239)
  %2246 : Tensor = prim::GetAttr[name="running_mean"](%2239)
  %2247 : Tensor = prim::GetAttr[name="running_var"](%2239)
  %2248 : Tensor = prim::GetAttr[name="weight"](%2239)
  %2249 : Tensor = prim::GetAttr[name="bias"](%2239)
   = prim::If(%2245) # torch/nn/functional.py:2011:4
    block0():
      %2250 : int[] = aten::size(%x.198) # torch/nn/functional.py:2012:27
      %size_prods.336 : int = aten::__getitem__(%2250, %48) # torch/nn/functional.py:1991:17
      %2252 : int = aten::len(%2250) # torch/nn/functional.py:1992:19
      %2253 : int = aten::sub(%2252, %47) # torch/nn/functional.py:1992:19
      %size_prods.337 : int = prim::Loop(%2253, %49, %size_prods.336) # torch/nn/functional.py:1992:4
        block0(%i.85 : int, %size_prods.338 : int):
          %2257 : int = aten::add(%i.85, %47) # torch/nn/functional.py:1993:27
          %2258 : int = aten::__getitem__(%2250, %2257) # torch/nn/functional.py:1993:22
          %size_prods.339 : int = aten::mul(%size_prods.338, %2258) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.339)
      %2260 : bool = aten::eq(%size_prods.337, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2260) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.199 : Tensor = aten::batch_norm(%x.198, %2248, %2249, %2246, %2247, %2245, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.5 : Tensor = aten::relu_(%x.199) # torch/nn/functional.py:1117:17
  %2263 : __torch__.torchvision.models.inception.___torch_mangle_587.BasicConv2d = prim::GetAttr[name="branch7x7dbl_4"](%2038)
  %2264 : __torch__.torch.nn.modules.conv.___torch_mangle_586.Conv2d = prim::GetAttr[name="conv"](%2263)
  %2265 : Tensor = prim::GetAttr[name="weight"](%2264)
  %2266 : Tensor? = prim::GetAttr[name="bias"](%2264)
  %2267 : int[] = prim::ListConstruct(%54, %54)
  %2268 : int[] = prim::ListConstruct(%46, %48)
  %2269 : int[] = prim::ListConstruct(%54, %54)
  %x.200 : Tensor = aten::conv2d(%branch7x7dbl.5, %2265, %2266, %2267, %2268, %2269, %54) # torch/nn/modules/conv.py:415:15
  %2271 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2263)
  %2272 : int = aten::dim(%x.200) # torch/nn/modules/batchnorm.py:276:11
  %2273 : bool = aten::ne(%2272, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2273) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2274 : bool = prim::GetAttr[name="training"](%2271)
   = prim::If(%2274) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2275 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2271)
      %2276 : Tensor = aten::add(%2275, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2271, %2276)
      -> ()
    block1():
      -> ()
  %2277 : bool = prim::GetAttr[name="training"](%2271)
  %2278 : Tensor = prim::GetAttr[name="running_mean"](%2271)
  %2279 : Tensor = prim::GetAttr[name="running_var"](%2271)
  %2280 : Tensor = prim::GetAttr[name="weight"](%2271)
  %2281 : Tensor = prim::GetAttr[name="bias"](%2271)
   = prim::If(%2277) # torch/nn/functional.py:2011:4
    block0():
      %2282 : int[] = aten::size(%x.200) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%2282, %48) # torch/nn/functional.py:1991:17
      %2284 : int = aten::len(%2282) # torch/nn/functional.py:1992:19
      %2285 : int = aten::sub(%2284, %47) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%2285, %49, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %2289 : int = aten::add(%i.86, %47) # torch/nn/functional.py:1993:27
          %2290 : int = aten::__getitem__(%2282, %2289) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %2290) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.343)
      %2292 : bool = aten::eq(%size_prods.341, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2292) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.201 : Tensor = aten::batch_norm(%x.200, %2280, %2281, %2278, %2279, %2277, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.7 : Tensor = aten::relu_(%x.201) # torch/nn/functional.py:1117:17
  %2295 : __torch__.torchvision.models.inception.___torch_mangle_585.BasicConv2d = prim::GetAttr[name="branch7x7dbl_5"](%2038)
  %2296 : __torch__.torch.nn.modules.conv.___torch_mangle_584.Conv2d = prim::GetAttr[name="conv"](%2295)
  %2297 : Tensor = prim::GetAttr[name="weight"](%2296)
  %2298 : Tensor? = prim::GetAttr[name="bias"](%2296)
  %2299 : int[] = prim::ListConstruct(%54, %54)
  %2300 : int[] = prim::ListConstruct(%48, %46)
  %2301 : int[] = prim::ListConstruct(%54, %54)
  %x.21 : Tensor = aten::conv2d(%branch7x7dbl.7, %2297, %2298, %2299, %2300, %2301, %54) # torch/nn/modules/conv.py:415:15
  %2303 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2295)
  %2304 : int = aten::dim(%x.21) # torch/nn/modules/batchnorm.py:276:11
  %2305 : bool = aten::ne(%2304, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2305) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2306 : bool = prim::GetAttr[name="training"](%2303)
   = prim::If(%2306) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2307 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2303)
      %2308 : Tensor = aten::add(%2307, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2303, %2308)
      -> ()
    block1():
      -> ()
  %2309 : bool = prim::GetAttr[name="training"](%2303)
  %2310 : Tensor = prim::GetAttr[name="running_mean"](%2303)
  %2311 : Tensor = prim::GetAttr[name="running_var"](%2303)
  %2312 : Tensor = prim::GetAttr[name="weight"](%2303)
  %2313 : Tensor = prim::GetAttr[name="bias"](%2303)
   = prim::If(%2309) # torch/nn/functional.py:2011:4
    block0():
      %2314 : int[] = aten::size(%x.21) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%2314, %48) # torch/nn/functional.py:1991:17
      %2316 : int = aten::len(%2314) # torch/nn/functional.py:1992:19
      %2317 : int = aten::sub(%2316, %47) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%2317, %49, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %2321 : int = aten::add(%i.10, %47) # torch/nn/functional.py:1993:27
          %2322 : int = aten::__getitem__(%2314, %2321) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %2322) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.39)
      %2324 : bool = aten::eq(%size_prods.37, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2324) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.22 : Tensor = aten::batch_norm(%x.21, %2312, %2313, %2310, %2311, %2309, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7dbl.9 : Tensor = aten::relu_(%x.22) # torch/nn/functional.py:1117:17
  %2327 : int[] = prim::ListConstruct(%46, %46)
  %2328 : int[] = prim::ListConstruct(%54, %54)
  %2329 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.16 : Tensor = aten::avg_pool2d(%x.29, %2327, %2328, %2329, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:310:22
  %2331 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch_pool"](%2038)
  %2332 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2331)
  %2333 : Tensor = prim::GetAttr[name="weight"](%2332)
  %2334 : Tensor? = prim::GetAttr[name="bias"](%2332)
  %2335 : int[] = prim::ListConstruct(%54, %54)
  %2336 : int[] = prim::ListConstruct(%48, %48)
  %2337 : int[] = prim::ListConstruct(%54, %54)
  %x.169 : Tensor = aten::conv2d(%branch_pool.16, %2333, %2334, %2335, %2336, %2337, %54) # torch/nn/modules/conv.py:415:15
  %2339 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2331)
  %2340 : int = aten::dim(%x.169) # torch/nn/modules/batchnorm.py:276:11
  %2341 : bool = aten::ne(%2340, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2341) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2342 : bool = prim::GetAttr[name="training"](%2339)
   = prim::If(%2342) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2343 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2339)
      %2344 : Tensor = aten::add(%2343, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2339, %2344)
      -> ()
    block1():
      -> ()
  %2345 : bool = prim::GetAttr[name="training"](%2339)
  %2346 : Tensor = prim::GetAttr[name="running_mean"](%2339)
  %2347 : Tensor = prim::GetAttr[name="running_var"](%2339)
  %2348 : Tensor = prim::GetAttr[name="weight"](%2339)
  %2349 : Tensor = prim::GetAttr[name="bias"](%2339)
   = prim::If(%2345) # torch/nn/functional.py:2011:4
    block0():
      %2350 : int[] = aten::size(%x.169) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%2350, %48) # torch/nn/functional.py:1991:17
      %2352 : int = aten::len(%2350) # torch/nn/functional.py:1992:19
      %2353 : int = aten::sub(%2352, %47) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%2353, %49, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %2357 : int = aten::add(%i.72, %47) # torch/nn/functional.py:1993:27
          %2358 : int = aten::__getitem__(%2350, %2357) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %2358) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.287)
      %2360 : bool = aten::eq(%size_prods.285, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2360) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.170 : Tensor = aten::batch_norm(%x.169, %2348, %2349, %2346, %2347, %2345, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.18 : Tensor = aten::relu_(%x.170) # torch/nn/functional.py:1117:17
  %outputs.10 : Tensor[] = prim::ListConstruct(%branch1x1.8, %branch7x7.5, %branch7x7dbl.9, %branch_pool.18)
  %x.31 : Tensor = aten::cat(%outputs.10, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:318:15
  %2365 : bool = prim::GetAttr[name="training"](%self)
  %aux_defined.1 : bool = prim::If(%2365) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:167:22
    block0():
      %2367 : bool = prim::GetAttr[name="aux_logits"](%self)
      -> (%2367)
    block1():
      -> (%55)
  %aux : Tensor? = prim::If(%aux_defined.1) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:168:8
    block0():
      %2369 : __torch__.torchvision.models.inception.InceptionAux = prim::GetAttr[name="AuxLogits"](%self)
      %2370 : int[] = prim::ListConstruct(%45, %45)
      %2371 : int[] = prim::ListConstruct(%46, %46)
      %x.182 : Tensor = aten::avg_pool2d(%x.31, %2370, %2371, %44, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:415:12
      %2373 : __torch__.torchvision.models.inception.___torch_mangle_564.BasicConv2d = prim::GetAttr[name="conv0"](%2369)
      %2374 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv"](%2373)
      %2375 : Tensor = prim::GetAttr[name="weight"](%2374)
      %2376 : Tensor? = prim::GetAttr[name="bias"](%2374)
      %2377 : int[] = prim::ListConstruct(%54, %54)
      %2378 : int[] = prim::ListConstruct(%48, %48)
      %2379 : int[] = prim::ListConstruct(%54, %54)
      %x.177 : Tensor = aten::conv2d(%x.182, %2375, %2376, %2377, %2378, %2379, %54) # torch/nn/modules/conv.py:415:15
      %2381 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%2373)
      %2382 : int = aten::dim(%x.177) # torch/nn/modules/batchnorm.py:276:11
      %2383 : bool = aten::ne(%2382, %51) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2383) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2384 : bool = prim::GetAttr[name="training"](%2381)
       = prim::If(%2384) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2385 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2381)
          %2386 : Tensor = aten::add(%2385, %54, %54) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2381, %2386)
          -> ()
        block1():
          -> ()
      %2387 : bool = prim::GetAttr[name="training"](%2381)
      %2388 : Tensor = prim::GetAttr[name="running_mean"](%2381)
      %2389 : Tensor = prim::GetAttr[name="running_var"](%2381)
      %2390 : Tensor = prim::GetAttr[name="weight"](%2381)
      %2391 : Tensor = prim::GetAttr[name="bias"](%2381)
       = prim::If(%2387) # torch/nn/functional.py:2011:4
        block0():
          %2392 : int[] = aten::size(%x.177) # torch/nn/functional.py:2012:27
          %size_prods.288 : int = aten::__getitem__(%2392, %48) # torch/nn/functional.py:1991:17
          %2394 : int = aten::len(%2392) # torch/nn/functional.py:1992:19
          %2395 : int = aten::sub(%2394, %47) # torch/nn/functional.py:1992:19
          %size_prods.289 : int = prim::Loop(%2395, %49, %size_prods.288) # torch/nn/functional.py:1992:4
            block0(%i.73 : int, %size_prods.290 : int):
              %2399 : int = aten::add(%i.73, %47) # torch/nn/functional.py:1993:27
              %2400 : int = aten::__getitem__(%2392, %2399) # torch/nn/functional.py:1993:22
              %size_prods.291 : int = aten::mul(%size_prods.290, %2400) # torch/nn/functional.py:1993:8
              -> (%49, %size_prods.291)
          %2402 : bool = aten::eq(%size_prods.289, %54) # torch/nn/functional.py:1994:7
           = prim::If(%2402) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %x.179 : Tensor = aten::batch_norm(%x.177, %2390, %2391, %2388, %2389, %2387, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
      %x.175 : Tensor = aten::relu_(%x.179) # torch/nn/functional.py:1117:17
      %2405 : __torch__.torchvision.models.inception.___torch_mangle_591.BasicConv2d = prim::GetAttr[name="conv1"](%2369)
      %2406 : __torch__.torch.nn.modules.conv.___torch_mangle_589.Conv2d = prim::GetAttr[name="conv"](%2405)
      %2407 : Tensor = prim::GetAttr[name="weight"](%2406)
      %2408 : Tensor? = prim::GetAttr[name="bias"](%2406)
      %2409 : int[] = prim::ListConstruct(%54, %54)
      %2410 : int[] = prim::ListConstruct(%48, %48)
      %2411 : int[] = prim::ListConstruct(%54, %54)
      %x.184 : Tensor = aten::conv2d(%x.175, %2407, %2408, %2409, %2410, %2411, %54) # torch/nn/modules/conv.py:415:15
      %2413 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_590.BatchNorm2d = prim::GetAttr[name="bn"](%2405)
      %2414 : int = aten::dim(%x.184) # torch/nn/modules/batchnorm.py:276:11
      %2415 : bool = aten::ne(%2414, %51) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2415) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2416 : bool = prim::GetAttr[name="training"](%2413)
       = prim::If(%2416) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2417 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2413)
          %2418 : Tensor = aten::add(%2417, %54, %54) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2413, %2418)
          -> ()
        block1():
          -> ()
      %2419 : bool = prim::GetAttr[name="training"](%2413)
      %2420 : Tensor = prim::GetAttr[name="running_mean"](%2413)
      %2421 : Tensor = prim::GetAttr[name="running_var"](%2413)
      %2422 : Tensor = prim::GetAttr[name="weight"](%2413)
      %2423 : Tensor = prim::GetAttr[name="bias"](%2413)
       = prim::If(%2419) # torch/nn/functional.py:2011:4
        block0():
          %2424 : int[] = aten::size(%x.184) # torch/nn/functional.py:2012:27
          %size_prods.308 : int = aten::__getitem__(%2424, %48) # torch/nn/functional.py:1991:17
          %2426 : int = aten::len(%2424) # torch/nn/functional.py:1992:19
          %2427 : int = aten::sub(%2426, %47) # torch/nn/functional.py:1992:19
          %size_prods.309 : int = prim::Loop(%2427, %49, %size_prods.308) # torch/nn/functional.py:1992:4
            block0(%i.78 : int, %size_prods.310 : int):
              %2431 : int = aten::add(%i.78, %47) # torch/nn/functional.py:1993:27
              %2432 : int = aten::__getitem__(%2424, %2431) # torch/nn/functional.py:1993:22
              %size_prods.311 : int = aten::mul(%size_prods.310, %2432) # torch/nn/functional.py:1993:8
              -> (%49, %size_prods.311)
          %2434 : bool = aten::eq(%size_prods.309, %54) # torch/nn/functional.py:1994:7
           = prim::If(%2434) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %x.185 : Tensor = aten::batch_norm(%x.184, %2422, %2423, %2420, %2421, %2419, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
      %x.176 : Tensor = aten::relu_(%x.185) # torch/nn/functional.py:1117:17
      %2437 : int[] = prim::ListConstruct(%54, %54)
      %2438 : int[] = aten::size(%x.176) # torch/nn/functional.py:925:51
      %2439 : int = aten::len(%2438) # <string>:5:9
      %2440 : bool = aten::gt(%2439, %47) # <string>:5:9
       = prim::If(%2440) # <string>:5:2
        block0():
          -> ()
        block1():
           = prim::RaiseException(%50) # <string>:5:2
          -> ()
      %x.178 : Tensor = aten::adaptive_avg_pool2d(%x.176, %2437) # torch/nn/functional.py:926:11
      %x.180 : Tensor = aten::flatten(%x.178, %54, %57) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:424:12
      %2443 : __torch__.torch.nn.modules.linear.___torch_mangle_592.Linear = prim::GetAttr[name="fc"](%2369)
      %2444 : Tensor = prim::GetAttr[name="weight"](%2443)
      %2445 : Tensor = prim::GetAttr[name="bias"](%2443)
      %2446 : int = aten::dim(%x.180) # torch/nn/functional.py:1672:7
      %2447 : bool = aten::eq(%2446, %47) # torch/nn/functional.py:1672:7
      %aux.1 : Tensor = prim::If(%2447) # torch/nn/functional.py:1672:4
        block0():
          %2449 : Tensor = aten::t(%2444) # torch/nn/functional.py:1674:39
          %ret.2 : Tensor = aten::addmm(%2445, %x.180, %2449, %54, %54) # torch/nn/functional.py:1674:14
          -> (%ret.2)
        block1():
          %2451 : Tensor = aten::t(%2444) # torch/nn/functional.py:1676:30
          %output.2 : Tensor = aten::matmul(%x.180, %2451) # torch/nn/functional.py:1676:17
          %output.4 : Tensor = aten::add_(%output.2, %2445, %54) # torch/nn/functional.py:1678:12
          -> (%output.4)
      -> (%aux.1)
    block1():
      -> (%aux.2)
  %2454 : __torch__.torchvision.models.inception.InceptionD = prim::GetAttr[name="Mixed_7a"](%self)
  %2455 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch3x3_1"](%2454)
  %2456 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2455)
  %2457 : Tensor = prim::GetAttr[name="weight"](%2456)
  %2458 : Tensor? = prim::GetAttr[name="bias"](%2456)
  %2459 : int[] = prim::ListConstruct(%54, %54)
  %2460 : int[] = prim::ListConstruct(%48, %48)
  %2461 : int[] = prim::ListConstruct(%54, %54)
  %x.186 : Tensor = aten::conv2d(%x.31, %2457, %2458, %2459, %2460, %2461, %54) # torch/nn/modules/conv.py:415:15
  %2463 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2455)
  %2464 : int = aten::dim(%x.186) # torch/nn/modules/batchnorm.py:276:11
  %2465 : bool = aten::ne(%2464, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2465) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2466 : bool = prim::GetAttr[name="training"](%2463)
   = prim::If(%2466) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2467 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2463)
      %2468 : Tensor = aten::add(%2467, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2463, %2468)
      -> ()
    block1():
      -> ()
  %2469 : bool = prim::GetAttr[name="training"](%2463)
  %2470 : Tensor = prim::GetAttr[name="running_mean"](%2463)
  %2471 : Tensor = prim::GetAttr[name="running_var"](%2463)
  %2472 : Tensor = prim::GetAttr[name="weight"](%2463)
  %2473 : Tensor = prim::GetAttr[name="bias"](%2463)
   = prim::If(%2469) # torch/nn/functional.py:2011:4
    block0():
      %2474 : int[] = aten::size(%x.186) # torch/nn/functional.py:2012:27
      %size_prods.312 : int = aten::__getitem__(%2474, %48) # torch/nn/functional.py:1991:17
      %2476 : int = aten::len(%2474) # torch/nn/functional.py:1992:19
      %2477 : int = aten::sub(%2476, %47) # torch/nn/functional.py:1992:19
      %size_prods.313 : int = prim::Loop(%2477, %49, %size_prods.312) # torch/nn/functional.py:1992:4
        block0(%i.79 : int, %size_prods.314 : int):
          %2481 : int = aten::add(%i.79, %47) # torch/nn/functional.py:1993:27
          %2482 : int = aten::__getitem__(%2474, %2481) # torch/nn/functional.py:1993:22
          %size_prods.315 : int = aten::mul(%size_prods.314, %2482) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.315)
      %2484 : bool = aten::eq(%size_prods.313, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2484) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.187 : Tensor = aten::batch_norm(%x.186, %2472, %2473, %2470, %2471, %2469, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3.5 : Tensor = aten::relu_(%x.187) # torch/nn/functional.py:1117:17
  %2487 : __torch__.torchvision.models.inception.___torch_mangle_594.BasicConv2d = prim::GetAttr[name="branch3x3_2"](%2454)
  %2488 : __torch__.torch.nn.modules.conv.___torch_mangle_593.Conv2d = prim::GetAttr[name="conv"](%2487)
  %2489 : Tensor = prim::GetAttr[name="weight"](%2488)
  %2490 : Tensor? = prim::GetAttr[name="bias"](%2488)
  %2491 : int[] = prim::ListConstruct(%47, %47)
  %2492 : int[] = prim::ListConstruct(%48, %48)
  %2493 : int[] = prim::ListConstruct(%54, %54)
  %x.188 : Tensor = aten::conv2d(%branch3x3.5, %2489, %2490, %2491, %2492, %2493, %54) # torch/nn/modules/conv.py:415:15
  %2495 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_490.BatchNorm2d = prim::GetAttr[name="bn"](%2487)
  %2496 : int = aten::dim(%x.188) # torch/nn/modules/batchnorm.py:276:11
  %2497 : bool = aten::ne(%2496, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2497) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2498 : bool = prim::GetAttr[name="training"](%2495)
   = prim::If(%2498) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2499 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2495)
      %2500 : Tensor = aten::add(%2499, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2495, %2500)
      -> ()
    block1():
      -> ()
  %2501 : bool = prim::GetAttr[name="training"](%2495)
  %2502 : Tensor = prim::GetAttr[name="running_mean"](%2495)
  %2503 : Tensor = prim::GetAttr[name="running_var"](%2495)
  %2504 : Tensor = prim::GetAttr[name="weight"](%2495)
  %2505 : Tensor = prim::GetAttr[name="bias"](%2495)
   = prim::If(%2501) # torch/nn/functional.py:2011:4
    block0():
      %2506 : int[] = aten::size(%x.188) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%2506, %48) # torch/nn/functional.py:1991:17
      %2508 : int = aten::len(%2506) # torch/nn/functional.py:1992:19
      %2509 : int = aten::sub(%2508, %47) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%2509, %49, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %2513 : int = aten::add(%i.80, %47) # torch/nn/functional.py:1993:27
          %2514 : int = aten::__getitem__(%2506, %2513) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %2514) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.319)
      %2516 : bool = aten::eq(%size_prods.317, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2516) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.189 : Tensor = aten::batch_norm(%x.188, %2504, %2505, %2502, %2503, %2501, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3.3 : Tensor = aten::relu_(%x.189) # torch/nn/functional.py:1117:17
  %2519 : __torch__.torchvision.models.inception.___torch_mangle_563.BasicConv2d = prim::GetAttr[name="branch7x7x3_1"](%2454)
  %2520 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv"](%2519)
  %2521 : Tensor = prim::GetAttr[name="weight"](%2520)
  %2522 : Tensor? = prim::GetAttr[name="bias"](%2520)
  %2523 : int[] = prim::ListConstruct(%54, %54)
  %2524 : int[] = prim::ListConstruct(%48, %48)
  %2525 : int[] = prim::ListConstruct(%54, %54)
  %x.190 : Tensor = aten::conv2d(%x.31, %2521, %2522, %2523, %2524, %2525, %54) # torch/nn/modules/conv.py:415:15
  %2527 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2519)
  %2528 : int = aten::dim(%x.190) # torch/nn/modules/batchnorm.py:276:11
  %2529 : bool = aten::ne(%2528, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2529) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2530 : bool = prim::GetAttr[name="training"](%2527)
   = prim::If(%2530) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2531 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2527)
      %2532 : Tensor = aten::add(%2531, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2527, %2532)
      -> ()
    block1():
      -> ()
  %2533 : bool = prim::GetAttr[name="training"](%2527)
  %2534 : Tensor = prim::GetAttr[name="running_mean"](%2527)
  %2535 : Tensor = prim::GetAttr[name="running_var"](%2527)
  %2536 : Tensor = prim::GetAttr[name="weight"](%2527)
  %2537 : Tensor = prim::GetAttr[name="bias"](%2527)
   = prim::If(%2533) # torch/nn/functional.py:2011:4
    block0():
      %2538 : int[] = aten::size(%x.190) # torch/nn/functional.py:2012:27
      %size_prods.320 : int = aten::__getitem__(%2538, %48) # torch/nn/functional.py:1991:17
      %2540 : int = aten::len(%2538) # torch/nn/functional.py:1992:19
      %2541 : int = aten::sub(%2540, %47) # torch/nn/functional.py:1992:19
      %size_prods.321 : int = prim::Loop(%2541, %49, %size_prods.320) # torch/nn/functional.py:1992:4
        block0(%i.81 : int, %size_prods.322 : int):
          %2545 : int = aten::add(%i.81, %47) # torch/nn/functional.py:1993:27
          %2546 : int = aten::__getitem__(%2538, %2545) # torch/nn/functional.py:1993:22
          %size_prods.323 : int = aten::mul(%size_prods.322, %2546) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.323)
      %2548 : bool = aten::eq(%size_prods.321, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2548) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.191 : Tensor = aten::batch_norm(%x.190, %2536, %2537, %2534, %2535, %2533, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7x3.1 : Tensor = aten::relu_(%x.191) # torch/nn/functional.py:1117:17
  %2551 : __torch__.torchvision.models.inception.___torch_mangle_585.BasicConv2d = prim::GetAttr[name="branch7x7x3_2"](%2454)
  %2552 : __torch__.torch.nn.modules.conv.___torch_mangle_584.Conv2d = prim::GetAttr[name="conv"](%2551)
  %2553 : Tensor = prim::GetAttr[name="weight"](%2552)
  %2554 : Tensor? = prim::GetAttr[name="bias"](%2552)
  %2555 : int[] = prim::ListConstruct(%54, %54)
  %2556 : int[] = prim::ListConstruct(%48, %46)
  %2557 : int[] = prim::ListConstruct(%54, %54)
  %x.192 : Tensor = aten::conv2d(%branch7x7x3.1, %2553, %2554, %2555, %2556, %2557, %54) # torch/nn/modules/conv.py:415:15
  %2559 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2551)
  %2560 : int = aten::dim(%x.192) # torch/nn/modules/batchnorm.py:276:11
  %2561 : bool = aten::ne(%2560, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2561) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2562 : bool = prim::GetAttr[name="training"](%2559)
   = prim::If(%2562) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2563 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2559)
      %2564 : Tensor = aten::add(%2563, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2559, %2564)
      -> ()
    block1():
      -> ()
  %2565 : bool = prim::GetAttr[name="training"](%2559)
  %2566 : Tensor = prim::GetAttr[name="running_mean"](%2559)
  %2567 : Tensor = prim::GetAttr[name="running_var"](%2559)
  %2568 : Tensor = prim::GetAttr[name="weight"](%2559)
  %2569 : Tensor = prim::GetAttr[name="bias"](%2559)
   = prim::If(%2565) # torch/nn/functional.py:2011:4
    block0():
      %2570 : int[] = aten::size(%x.192) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%2570, %48) # torch/nn/functional.py:1991:17
      %2572 : int = aten::len(%2570) # torch/nn/functional.py:1992:19
      %2573 : int = aten::sub(%2572, %47) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%2573, %49, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %2577 : int = aten::add(%i.82, %47) # torch/nn/functional.py:1993:27
          %2578 : int = aten::__getitem__(%2570, %2577) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %2578) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.327)
      %2580 : bool = aten::eq(%size_prods.325, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2580) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.193 : Tensor = aten::batch_norm(%x.192, %2568, %2569, %2566, %2567, %2565, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7x3.3 : Tensor = aten::relu_(%x.193) # torch/nn/functional.py:1117:17
  %2583 : __torch__.torchvision.models.inception.___torch_mangle_587.BasicConv2d = prim::GetAttr[name="branch7x7x3_3"](%2454)
  %2584 : __torch__.torch.nn.modules.conv.___torch_mangle_586.Conv2d = prim::GetAttr[name="conv"](%2583)
  %2585 : Tensor = prim::GetAttr[name="weight"](%2584)
  %2586 : Tensor? = prim::GetAttr[name="bias"](%2584)
  %2587 : int[] = prim::ListConstruct(%54, %54)
  %2588 : int[] = prim::ListConstruct(%46, %48)
  %2589 : int[] = prim::ListConstruct(%54, %54)
  %x.194 : Tensor = aten::conv2d(%branch7x7x3.3, %2585, %2586, %2587, %2588, %2589, %54) # torch/nn/modules/conv.py:415:15
  %2591 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2583)
  %2592 : int = aten::dim(%x.194) # torch/nn/modules/batchnorm.py:276:11
  %2593 : bool = aten::ne(%2592, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2593) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2594 : bool = prim::GetAttr[name="training"](%2591)
   = prim::If(%2594) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2595 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2591)
      %2596 : Tensor = aten::add(%2595, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2591, %2596)
      -> ()
    block1():
      -> ()
  %2597 : bool = prim::GetAttr[name="training"](%2591)
  %2598 : Tensor = prim::GetAttr[name="running_mean"](%2591)
  %2599 : Tensor = prim::GetAttr[name="running_var"](%2591)
  %2600 : Tensor = prim::GetAttr[name="weight"](%2591)
  %2601 : Tensor = prim::GetAttr[name="bias"](%2591)
   = prim::If(%2597) # torch/nn/functional.py:2011:4
    block0():
      %2602 : int[] = aten::size(%x.194) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%2602, %48) # torch/nn/functional.py:1991:17
      %2604 : int = aten::len(%2602) # torch/nn/functional.py:1992:19
      %2605 : int = aten::sub(%2604, %47) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%2605, %49, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %2609 : int = aten::add(%i.83, %47) # torch/nn/functional.py:1993:27
          %2610 : int = aten::__getitem__(%2602, %2609) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %2610) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.331)
      %2612 : bool = aten::eq(%size_prods.329, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2612) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.195 : Tensor = aten::batch_norm(%x.194, %2600, %2601, %2598, %2599, %2597, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7x3.5 : Tensor = aten::relu_(%x.195) # torch/nn/functional.py:1117:17
  %2615 : __torch__.torchvision.models.inception.___torch_mangle_596.BasicConv2d = prim::GetAttr[name="branch7x7x3_4"](%2454)
  %2616 : __torch__.torch.nn.modules.conv.___torch_mangle_595.Conv2d = prim::GetAttr[name="conv"](%2615)
  %2617 : Tensor = prim::GetAttr[name="weight"](%2616)
  %2618 : Tensor? = prim::GetAttr[name="bias"](%2616)
  %2619 : int[] = prim::ListConstruct(%47, %47)
  %2620 : int[] = prim::ListConstruct(%48, %48)
  %2621 : int[] = prim::ListConstruct(%54, %54)
  %x.202 : Tensor = aten::conv2d(%branch7x7x3.5, %2617, %2618, %2619, %2620, %2621, %54) # torch/nn/modules/conv.py:415:15
  %2623 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2615)
  %2624 : int = aten::dim(%x.202) # torch/nn/modules/batchnorm.py:276:11
  %2625 : bool = aten::ne(%2624, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2625) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2626 : bool = prim::GetAttr[name="training"](%2623)
   = prim::If(%2626) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2627 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2623)
      %2628 : Tensor = aten::add(%2627, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2623, %2628)
      -> ()
    block1():
      -> ()
  %2629 : bool = prim::GetAttr[name="training"](%2623)
  %2630 : Tensor = prim::GetAttr[name="running_mean"](%2623)
  %2631 : Tensor = prim::GetAttr[name="running_var"](%2623)
  %2632 : Tensor = prim::GetAttr[name="weight"](%2623)
  %2633 : Tensor = prim::GetAttr[name="bias"](%2623)
   = prim::If(%2629) # torch/nn/functional.py:2011:4
    block0():
      %2634 : int[] = aten::size(%x.202) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%2634, %48) # torch/nn/functional.py:1991:17
      %2636 : int = aten::len(%2634) # torch/nn/functional.py:1992:19
      %2637 : int = aten::sub(%2636, %47) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%2637, %49, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %2641 : int = aten::add(%i.87, %47) # torch/nn/functional.py:1993:27
          %2642 : int = aten::__getitem__(%2634, %2641) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %2642) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.347)
      %2644 : bool = aten::eq(%size_prods.345, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2644) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.203 : Tensor = aten::batch_norm(%x.202, %2632, %2633, %2630, %2631, %2629, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch7x7x3.7 : Tensor = aten::relu_(%x.203) # torch/nn/functional.py:1117:17
  %2647 : int[] = prim::ListConstruct(%46, %46)
  %2648 : int[] = prim::ListConstruct(%47, %47)
  %2649 : int[] = prim::ListConstruct(%48, %48)
  %2650 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.17 : Tensor = aten::max_pool2d(%x.31, %2647, %2648, %2649, %2650, %55) # torch/nn/functional.py:575:11
  %outputs.11 : Tensor[] = prim::ListConstruct(%branch3x3.3, %branch7x7x3.7, %branch_pool.17)
  %x.34 : Tensor = aten::cat(%outputs.11, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:350:15
  %2654 : __torch__.torchvision.models.inception.InceptionE = prim::GetAttr[name="Mixed_7b"](%self)
  %2655 : __torch__.torchvision.models.inception.___torch_mangle_598.BasicConv2d = prim::GetAttr[name="branch1x1"](%2654)
  %2656 : __torch__.torch.nn.modules.conv.___torch_mangle_597.Conv2d = prim::GetAttr[name="conv"](%2655)
  %2657 : Tensor = prim::GetAttr[name="weight"](%2656)
  %2658 : Tensor? = prim::GetAttr[name="bias"](%2656)
  %2659 : int[] = prim::ListConstruct(%54, %54)
  %2660 : int[] = prim::ListConstruct(%48, %48)
  %2661 : int[] = prim::ListConstruct(%54, %54)
  %x.204 : Tensor = aten::conv2d(%x.34, %2657, %2658, %2659, %2660, %2661, %54) # torch/nn/modules/conv.py:415:15
  %2663 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_490.BatchNorm2d = prim::GetAttr[name="bn"](%2655)
  %2664 : int = aten::dim(%x.204) # torch/nn/modules/batchnorm.py:276:11
  %2665 : bool = aten::ne(%2664, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2665) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2666 : bool = prim::GetAttr[name="training"](%2663)
   = prim::If(%2666) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2667 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2663)
      %2668 : Tensor = aten::add(%2667, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2663, %2668)
      -> ()
    block1():
      -> ()
  %2669 : bool = prim::GetAttr[name="training"](%2663)
  %2670 : Tensor = prim::GetAttr[name="running_mean"](%2663)
  %2671 : Tensor = prim::GetAttr[name="running_var"](%2663)
  %2672 : Tensor = prim::GetAttr[name="weight"](%2663)
  %2673 : Tensor = prim::GetAttr[name="bias"](%2663)
   = prim::If(%2669) # torch/nn/functional.py:2011:4
    block0():
      %2674 : int[] = aten::size(%x.204) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%2674, %48) # torch/nn/functional.py:1991:17
      %2676 : int = aten::len(%2674) # torch/nn/functional.py:1992:19
      %2677 : int = aten::sub(%2676, %47) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%2677, %49, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %2681 : int = aten::add(%i.88, %47) # torch/nn/functional.py:1993:27
          %2682 : int = aten::__getitem__(%2674, %2681) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %2682) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.351)
      %2684 : bool = aten::eq(%size_prods.349, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2684) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.205 : Tensor = aten::batch_norm(%x.204, %2672, %2673, %2670, %2671, %2669, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.9 : Tensor = aten::relu_(%x.205) # torch/nn/functional.py:1117:17
  %2687 : __torch__.torchvision.models.inception.___torch_mangle_600.BasicConv2d = prim::GetAttr[name="branch3x3_1"](%2654)
  %2688 : __torch__.torch.nn.modules.conv.___torch_mangle_599.Conv2d = prim::GetAttr[name="conv"](%2687)
  %2689 : Tensor = prim::GetAttr[name="weight"](%2688)
  %2690 : Tensor? = prim::GetAttr[name="bias"](%2688)
  %2691 : int[] = prim::ListConstruct(%54, %54)
  %2692 : int[] = prim::ListConstruct(%48, %48)
  %2693 : int[] = prim::ListConstruct(%54, %54)
  %x.206 : Tensor = aten::conv2d(%x.34, %2689, %2690, %2691, %2692, %2693, %54) # torch/nn/modules/conv.py:415:15
  %2695 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2687)
  %2696 : int = aten::dim(%x.206) # torch/nn/modules/batchnorm.py:276:11
  %2697 : bool = aten::ne(%2696, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2697) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2698 : bool = prim::GetAttr[name="training"](%2695)
   = prim::If(%2698) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2699 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2695)
      %2700 : Tensor = aten::add(%2699, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2695, %2700)
      -> ()
    block1():
      -> ()
  %2701 : bool = prim::GetAttr[name="training"](%2695)
  %2702 : Tensor = prim::GetAttr[name="running_mean"](%2695)
  %2703 : Tensor = prim::GetAttr[name="running_var"](%2695)
  %2704 : Tensor = prim::GetAttr[name="weight"](%2695)
  %2705 : Tensor = prim::GetAttr[name="bias"](%2695)
   = prim::If(%2701) # torch/nn/functional.py:2011:4
    block0():
      %2706 : int[] = aten::size(%x.206) # torch/nn/functional.py:2012:27
      %size_prods.352 : int = aten::__getitem__(%2706, %48) # torch/nn/functional.py:1991:17
      %2708 : int = aten::len(%2706) # torch/nn/functional.py:1992:19
      %2709 : int = aten::sub(%2708, %47) # torch/nn/functional.py:1992:19
      %size_prods.353 : int = prim::Loop(%2709, %49, %size_prods.352) # torch/nn/functional.py:1992:4
        block0(%i.89 : int, %size_prods.354 : int):
          %2713 : int = aten::add(%i.89, %47) # torch/nn/functional.py:1993:27
          %2714 : int = aten::__getitem__(%2706, %2713) # torch/nn/functional.py:1993:22
          %size_prods.355 : int = aten::mul(%size_prods.354, %2714) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.355)
      %2716 : bool = aten::eq(%size_prods.353, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2716) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.207 : Tensor = aten::batch_norm(%x.206, %2704, %2705, %2702, %2703, %2701, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3.7 : Tensor = aten::relu_(%x.207) # torch/nn/functional.py:1117:17
  %2719 : __torch__.torchvision.models.inception.___torch_mangle_602.BasicConv2d = prim::GetAttr[name="branch3x3_2a"](%2654)
  %2720 : __torch__.torch.nn.modules.conv.___torch_mangle_601.Conv2d = prim::GetAttr[name="conv"](%2719)
  %2721 : Tensor = prim::GetAttr[name="weight"](%2720)
  %2722 : Tensor? = prim::GetAttr[name="bias"](%2720)
  %2723 : int[] = prim::ListConstruct(%54, %54)
  %2724 : int[] = prim::ListConstruct(%48, %54)
  %2725 : int[] = prim::ListConstruct(%54, %54)
  %x.208 : Tensor = aten::conv2d(%branch3x3.7, %2721, %2722, %2723, %2724, %2725, %54) # torch/nn/modules/conv.py:415:15
  %2727 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2719)
  %2728 : int = aten::dim(%x.208) # torch/nn/modules/batchnorm.py:276:11
  %2729 : bool = aten::ne(%2728, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2729) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2730 : bool = prim::GetAttr[name="training"](%2727)
   = prim::If(%2730) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2731 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2727)
      %2732 : Tensor = aten::add(%2731, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2727, %2732)
      -> ()
    block1():
      -> ()
  %2733 : bool = prim::GetAttr[name="training"](%2727)
  %2734 : Tensor = prim::GetAttr[name="running_mean"](%2727)
  %2735 : Tensor = prim::GetAttr[name="running_var"](%2727)
  %2736 : Tensor = prim::GetAttr[name="weight"](%2727)
  %2737 : Tensor = prim::GetAttr[name="bias"](%2727)
   = prim::If(%2733) # torch/nn/functional.py:2011:4
    block0():
      %2738 : int[] = aten::size(%x.208) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%2738, %48) # torch/nn/functional.py:1991:17
      %2740 : int = aten::len(%2738) # torch/nn/functional.py:1992:19
      %2741 : int = aten::sub(%2740, %47) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%2741, %49, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %2745 : int = aten::add(%i.90, %47) # torch/nn/functional.py:1993:27
          %2746 : int = aten::__getitem__(%2738, %2745) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %2746) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.359)
      %2748 : bool = aten::eq(%size_prods.357, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2748) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.209 : Tensor = aten::batch_norm(%x.208, %2736, %2737, %2734, %2735, %2733, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.6 : Tensor = aten::relu_(%x.209) # torch/nn/functional.py:1117:17
  %2751 : __torch__.torchvision.models.inception.___torch_mangle_604.BasicConv2d = prim::GetAttr[name="branch3x3_2b"](%2654)
  %2752 : __torch__.torch.nn.modules.conv.___torch_mangle_603.Conv2d = prim::GetAttr[name="conv"](%2751)
  %2753 : Tensor = prim::GetAttr[name="weight"](%2752)
  %2754 : Tensor? = prim::GetAttr[name="bias"](%2752)
  %2755 : int[] = prim::ListConstruct(%54, %54)
  %2756 : int[] = prim::ListConstruct(%54, %48)
  %2757 : int[] = prim::ListConstruct(%54, %54)
  %x.210 : Tensor = aten::conv2d(%branch3x3.7, %2753, %2754, %2755, %2756, %2757, %54) # torch/nn/modules/conv.py:415:15
  %2759 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2751)
  %2760 : int = aten::dim(%x.210) # torch/nn/modules/batchnorm.py:276:11
  %2761 : bool = aten::ne(%2760, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2761) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2762 : bool = prim::GetAttr[name="training"](%2759)
   = prim::If(%2762) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2763 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2759)
      %2764 : Tensor = aten::add(%2763, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2759, %2764)
      -> ()
    block1():
      -> ()
  %2765 : bool = prim::GetAttr[name="training"](%2759)
  %2766 : Tensor = prim::GetAttr[name="running_mean"](%2759)
  %2767 : Tensor = prim::GetAttr[name="running_var"](%2759)
  %2768 : Tensor = prim::GetAttr[name="weight"](%2759)
  %2769 : Tensor = prim::GetAttr[name="bias"](%2759)
   = prim::If(%2765) # torch/nn/functional.py:2011:4
    block0():
      %2770 : int[] = aten::size(%x.210) # torch/nn/functional.py:2012:27
      %size_prods.360 : int = aten::__getitem__(%2770, %48) # torch/nn/functional.py:1991:17
      %2772 : int = aten::len(%2770) # torch/nn/functional.py:1992:19
      %2773 : int = aten::sub(%2772, %47) # torch/nn/functional.py:1992:19
      %size_prods.361 : int = prim::Loop(%2773, %49, %size_prods.360) # torch/nn/functional.py:1992:4
        block0(%i.91 : int, %size_prods.362 : int):
          %2777 : int = aten::add(%i.91, %47) # torch/nn/functional.py:1993:27
          %2778 : int = aten::__getitem__(%2770, %2777) # torch/nn/functional.py:1993:22
          %size_prods.363 : int = aten::mul(%size_prods.362, %2778) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.363)
      %2780 : bool = aten::eq(%size_prods.361, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2780) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.211 : Tensor = aten::batch_norm(%x.210, %2768, %2769, %2766, %2767, %2765, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.7 : Tensor = aten::relu_(%x.211) # torch/nn/functional.py:1117:17
  %branch3x3.8 : Tensor[] = prim::ListConstruct(%result.6, %result.7)
  %branch3x3.9 : Tensor = aten::cat(%branch3x3.8, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:380:20
  %2785 : __torch__.torchvision.models.inception.___torch_mangle_607.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%2654)
  %2786 : __torch__.torch.nn.modules.conv.___torch_mangle_605.Conv2d = prim::GetAttr[name="conv"](%2785)
  %2787 : Tensor = prim::GetAttr[name="weight"](%2786)
  %2788 : Tensor? = prim::GetAttr[name="bias"](%2786)
  %2789 : int[] = prim::ListConstruct(%54, %54)
  %2790 : int[] = prim::ListConstruct(%48, %48)
  %2791 : int[] = prim::ListConstruct(%54, %54)
  %x.212 : Tensor = aten::conv2d(%x.34, %2787, %2788, %2789, %2790, %2791, %54) # torch/nn/modules/conv.py:415:15
  %2793 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_606.BatchNorm2d = prim::GetAttr[name="bn"](%2785)
  %2794 : int = aten::dim(%x.212) # torch/nn/modules/batchnorm.py:276:11
  %2795 : bool = aten::ne(%2794, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2795) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2796 : bool = prim::GetAttr[name="training"](%2793)
   = prim::If(%2796) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2797 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2793)
      %2798 : Tensor = aten::add(%2797, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2793, %2798)
      -> ()
    block1():
      -> ()
  %2799 : bool = prim::GetAttr[name="training"](%2793)
  %2800 : Tensor = prim::GetAttr[name="running_mean"](%2793)
  %2801 : Tensor = prim::GetAttr[name="running_var"](%2793)
  %2802 : Tensor = prim::GetAttr[name="weight"](%2793)
  %2803 : Tensor = prim::GetAttr[name="bias"](%2793)
   = prim::If(%2799) # torch/nn/functional.py:2011:4
    block0():
      %2804 : int[] = aten::size(%x.212) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%2804, %48) # torch/nn/functional.py:1991:17
      %2806 : int = aten::len(%2804) # torch/nn/functional.py:1992:19
      %2807 : int = aten::sub(%2806, %47) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%2807, %49, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %2811 : int = aten::add(%i.92, %47) # torch/nn/functional.py:1993:27
          %2812 : int = aten::__getitem__(%2804, %2811) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %2812) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.367)
      %2814 : bool = aten::eq(%size_prods.365, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2814) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.213 : Tensor = aten::batch_norm(%x.212, %2802, %2803, %2800, %2801, %2799, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.17 : Tensor = aten::relu_(%x.213) # torch/nn/functional.py:1117:17
  %2817 : __torch__.torchvision.models.inception.___torch_mangle_609.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%2654)
  %2818 : __torch__.torch.nn.modules.conv.___torch_mangle_608.Conv2d = prim::GetAttr[name="conv"](%2817)
  %2819 : Tensor = prim::GetAttr[name="weight"](%2818)
  %2820 : Tensor? = prim::GetAttr[name="bias"](%2818)
  %2821 : int[] = prim::ListConstruct(%54, %54)
  %2822 : int[] = prim::ListConstruct(%54, %54)
  %2823 : int[] = prim::ListConstruct(%54, %54)
  %x.214 : Tensor = aten::conv2d(%branch3x3dbl.17, %2819, %2820, %2821, %2822, %2823, %54) # torch/nn/modules/conv.py:415:15
  %2825 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2817)
  %2826 : int = aten::dim(%x.214) # torch/nn/modules/batchnorm.py:276:11
  %2827 : bool = aten::ne(%2826, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2827) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2828 : bool = prim::GetAttr[name="training"](%2825)
   = prim::If(%2828) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2829 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2825)
      %2830 : Tensor = aten::add(%2829, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2825, %2830)
      -> ()
    block1():
      -> ()
  %2831 : bool = prim::GetAttr[name="training"](%2825)
  %2832 : Tensor = prim::GetAttr[name="running_mean"](%2825)
  %2833 : Tensor = prim::GetAttr[name="running_var"](%2825)
  %2834 : Tensor = prim::GetAttr[name="weight"](%2825)
  %2835 : Tensor = prim::GetAttr[name="bias"](%2825)
   = prim::If(%2831) # torch/nn/functional.py:2011:4
    block0():
      %2836 : int[] = aten::size(%x.214) # torch/nn/functional.py:2012:27
      %size_prods.368 : int = aten::__getitem__(%2836, %48) # torch/nn/functional.py:1991:17
      %2838 : int = aten::len(%2836) # torch/nn/functional.py:1992:19
      %2839 : int = aten::sub(%2838, %47) # torch/nn/functional.py:1992:19
      %size_prods.369 : int = prim::Loop(%2839, %49, %size_prods.368) # torch/nn/functional.py:1992:4
        block0(%i.93 : int, %size_prods.370 : int):
          %2843 : int = aten::add(%i.93, %47) # torch/nn/functional.py:1993:27
          %2844 : int = aten::__getitem__(%2836, %2843) # torch/nn/functional.py:1993:22
          %size_prods.371 : int = aten::mul(%size_prods.370, %2844) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.371)
      %2846 : bool = aten::eq(%size_prods.369, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2846) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.215 : Tensor = aten::batch_norm(%x.214, %2834, %2835, %2832, %2833, %2831, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.18 : Tensor = aten::relu_(%x.215) # torch/nn/functional.py:1117:17
  %2849 : __torch__.torchvision.models.inception.___torch_mangle_602.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3a"](%2654)
  %2850 : __torch__.torch.nn.modules.conv.___torch_mangle_601.Conv2d = prim::GetAttr[name="conv"](%2849)
  %2851 : Tensor = prim::GetAttr[name="weight"](%2850)
  %2852 : Tensor? = prim::GetAttr[name="bias"](%2850)
  %2853 : int[] = prim::ListConstruct(%54, %54)
  %2854 : int[] = prim::ListConstruct(%48, %54)
  %2855 : int[] = prim::ListConstruct(%54, %54)
  %x.216 : Tensor = aten::conv2d(%branch3x3dbl.18, %2851, %2852, %2853, %2854, %2855, %54) # torch/nn/modules/conv.py:415:15
  %2857 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2849)
  %2858 : int = aten::dim(%x.216) # torch/nn/modules/batchnorm.py:276:11
  %2859 : bool = aten::ne(%2858, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2859) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2860 : bool = prim::GetAttr[name="training"](%2857)
   = prim::If(%2860) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2861 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2857)
      %2862 : Tensor = aten::add(%2861, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2857, %2862)
      -> ()
    block1():
      -> ()
  %2863 : bool = prim::GetAttr[name="training"](%2857)
  %2864 : Tensor = prim::GetAttr[name="running_mean"](%2857)
  %2865 : Tensor = prim::GetAttr[name="running_var"](%2857)
  %2866 : Tensor = prim::GetAttr[name="weight"](%2857)
  %2867 : Tensor = prim::GetAttr[name="bias"](%2857)
   = prim::If(%2863) # torch/nn/functional.py:2011:4
    block0():
      %2868 : int[] = aten::size(%x.216) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%2868, %48) # torch/nn/functional.py:1991:17
      %2870 : int = aten::len(%2868) # torch/nn/functional.py:1992:19
      %2871 : int = aten::sub(%2870, %47) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%2871, %49, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %2875 : int = aten::add(%i.94, %47) # torch/nn/functional.py:1993:27
          %2876 : int = aten::__getitem__(%2868, %2875) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %2876) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.375)
      %2878 : bool = aten::eq(%size_prods.373, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2878) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.217 : Tensor = aten::batch_norm(%x.216, %2866, %2867, %2864, %2865, %2863, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.8 : Tensor = aten::relu_(%x.217) # torch/nn/functional.py:1117:17
  %2881 : __torch__.torchvision.models.inception.___torch_mangle_604.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3b"](%2654)
  %2882 : __torch__.torch.nn.modules.conv.___torch_mangle_603.Conv2d = prim::GetAttr[name="conv"](%2881)
  %2883 : Tensor = prim::GetAttr[name="weight"](%2882)
  %2884 : Tensor? = prim::GetAttr[name="bias"](%2882)
  %2885 : int[] = prim::ListConstruct(%54, %54)
  %2886 : int[] = prim::ListConstruct(%54, %48)
  %2887 : int[] = prim::ListConstruct(%54, %54)
  %x.218 : Tensor = aten::conv2d(%branch3x3dbl.18, %2883, %2884, %2885, %2886, %2887, %54) # torch/nn/modules/conv.py:415:15
  %2889 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2881)
  %2890 : int = aten::dim(%x.218) # torch/nn/modules/batchnorm.py:276:11
  %2891 : bool = aten::ne(%2890, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2891) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2892 : bool = prim::GetAttr[name="training"](%2889)
   = prim::If(%2892) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2893 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2889)
      %2894 : Tensor = aten::add(%2893, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2889, %2894)
      -> ()
    block1():
      -> ()
  %2895 : bool = prim::GetAttr[name="training"](%2889)
  %2896 : Tensor = prim::GetAttr[name="running_mean"](%2889)
  %2897 : Tensor = prim::GetAttr[name="running_var"](%2889)
  %2898 : Tensor = prim::GetAttr[name="weight"](%2889)
  %2899 : Tensor = prim::GetAttr[name="bias"](%2889)
   = prim::If(%2895) # torch/nn/functional.py:2011:4
    block0():
      %2900 : int[] = aten::size(%x.218) # torch/nn/functional.py:2012:27
      %size_prods.376 : int = aten::__getitem__(%2900, %48) # torch/nn/functional.py:1991:17
      %2902 : int = aten::len(%2900) # torch/nn/functional.py:1992:19
      %2903 : int = aten::sub(%2902, %47) # torch/nn/functional.py:1992:19
      %size_prods.377 : int = prim::Loop(%2903, %49, %size_prods.376) # torch/nn/functional.py:1992:4
        block0(%i.95 : int, %size_prods.378 : int):
          %2907 : int = aten::add(%i.95, %47) # torch/nn/functional.py:1993:27
          %2908 : int = aten::__getitem__(%2900, %2907) # torch/nn/functional.py:1993:22
          %size_prods.379 : int = aten::mul(%size_prods.378, %2908) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.379)
      %2910 : bool = aten::eq(%size_prods.377, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2910) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.219 : Tensor = aten::batch_norm(%x.218, %2898, %2899, %2896, %2897, %2895, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.9 : Tensor = aten::relu_(%x.219) # torch/nn/functional.py:1117:17
  %branch3x3dbl.19 : Tensor[] = prim::ListConstruct(%result.8, %result.9)
  %branch3x3dbl.20 : Tensor = aten::cat(%branch3x3dbl.19, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:388:23
  %2915 : int[] = prim::ListConstruct(%46, %46)
  %2916 : int[] = prim::ListConstruct(%54, %54)
  %2917 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.19 : Tensor = aten::avg_pool2d(%x.34, %2915, %2916, %2917, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:390:22
  %2919 : __torch__.torchvision.models.inception.___torch_mangle_611.BasicConv2d = prim::GetAttr[name="branch_pool"](%2654)
  %2920 : __torch__.torch.nn.modules.conv.___torch_mangle_610.Conv2d = prim::GetAttr[name="conv"](%2919)
  %2921 : Tensor = prim::GetAttr[name="weight"](%2920)
  %2922 : Tensor? = prim::GetAttr[name="bias"](%2920)
  %2923 : int[] = prim::ListConstruct(%54, %54)
  %2924 : int[] = prim::ListConstruct(%48, %48)
  %2925 : int[] = prim::ListConstruct(%54, %54)
  %x.220 : Tensor = aten::conv2d(%branch_pool.19, %2921, %2922, %2923, %2924, %2925, %54) # torch/nn/modules/conv.py:415:15
  %2927 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%2919)
  %2928 : int = aten::dim(%x.220) # torch/nn/modules/batchnorm.py:276:11
  %2929 : bool = aten::ne(%2928, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2929) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2930 : bool = prim::GetAttr[name="training"](%2927)
   = prim::If(%2930) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2931 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2927)
      %2932 : Tensor = aten::add(%2931, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2927, %2932)
      -> ()
    block1():
      -> ()
  %2933 : bool = prim::GetAttr[name="training"](%2927)
  %2934 : Tensor = prim::GetAttr[name="running_mean"](%2927)
  %2935 : Tensor = prim::GetAttr[name="running_var"](%2927)
  %2936 : Tensor = prim::GetAttr[name="weight"](%2927)
  %2937 : Tensor = prim::GetAttr[name="bias"](%2927)
   = prim::If(%2933) # torch/nn/functional.py:2011:4
    block0():
      %2938 : int[] = aten::size(%x.220) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%2938, %48) # torch/nn/functional.py:1991:17
      %2940 : int = aten::len(%2938) # torch/nn/functional.py:1992:19
      %2941 : int = aten::sub(%2940, %47) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%2941, %49, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %2945 : int = aten::add(%i.96, %47) # torch/nn/functional.py:1993:27
          %2946 : int = aten::__getitem__(%2938, %2945) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %2946) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.383)
      %2948 : bool = aten::eq(%size_prods.381, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2948) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.221 : Tensor = aten::batch_norm(%x.220, %2936, %2937, %2934, %2935, %2933, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.20 : Tensor = aten::relu_(%x.221) # torch/nn/functional.py:1117:17
  %outputs.12 : Tensor[] = prim::ListConstruct(%branch1x1.9, %branch3x3.9, %branch3x3dbl.20, %branch_pool.20)
  %x.36 : Tensor = aten::cat(%outputs.12, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:398:15
  %2953 : __torch__.torchvision.models.inception.___torch_mangle_620.InceptionE = prim::GetAttr[name="Mixed_7c"](%self)
  %2954 : __torch__.torchvision.models.inception.___torch_mangle_613.BasicConv2d = prim::GetAttr[name="branch1x1"](%2953)
  %2955 : __torch__.torch.nn.modules.conv.___torch_mangle_612.Conv2d = prim::GetAttr[name="conv"](%2954)
  %2956 : Tensor = prim::GetAttr[name="weight"](%2955)
  %2957 : Tensor? = prim::GetAttr[name="bias"](%2955)
  %2958 : int[] = prim::ListConstruct(%54, %54)
  %2959 : int[] = prim::ListConstruct(%48, %48)
  %2960 : int[] = prim::ListConstruct(%54, %54)
  %x.4 : Tensor = aten::conv2d(%x.36, %2956, %2957, %2958, %2959, %2960, %54) # torch/nn/modules/conv.py:415:15
  %2962 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_490.BatchNorm2d = prim::GetAttr[name="bn"](%2954)
  %2963 : int = aten::dim(%x.4) # torch/nn/modules/batchnorm.py:276:11
  %2964 : bool = aten::ne(%2963, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2964) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2965 : bool = prim::GetAttr[name="training"](%2962)
   = prim::If(%2965) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2966 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2962)
      %2967 : Tensor = aten::add(%2966, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2962, %2967)
      -> ()
    block1():
      -> ()
  %2968 : bool = prim::GetAttr[name="training"](%2962)
  %2969 : Tensor = prim::GetAttr[name="running_mean"](%2962)
  %2970 : Tensor = prim::GetAttr[name="running_var"](%2962)
  %2971 : Tensor = prim::GetAttr[name="weight"](%2962)
  %2972 : Tensor = prim::GetAttr[name="bias"](%2962)
   = prim::If(%2968) # torch/nn/functional.py:2011:4
    block0():
      %2973 : int[] = aten::size(%x.4) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%2973, %48) # torch/nn/functional.py:1991:17
      %2975 : int = aten::len(%2973) # torch/nn/functional.py:1992:19
      %2976 : int = aten::sub(%2975, %47) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%2976, %49, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.7 : int):
          %2980 : int = aten::add(%i.2, %47) # torch/nn/functional.py:1993:27
          %2981 : int = aten::__getitem__(%2973, %2980) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %2981) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.5)
      %2983 : bool = aten::eq(%size_prods.4, %54) # torch/nn/functional.py:1994:7
       = prim::If(%2983) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.6 : Tensor = aten::batch_norm(%x.4, %2971, %2972, %2969, %2970, %2968, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch1x1.1 : Tensor = aten::relu_(%x.6) # torch/nn/functional.py:1117:17
  %2986 : __torch__.torchvision.models.inception.___torch_mangle_615.BasicConv2d = prim::GetAttr[name="branch3x3_1"](%2953)
  %2987 : __torch__.torch.nn.modules.conv.___torch_mangle_614.Conv2d = prim::GetAttr[name="conv"](%2986)
  %2988 : Tensor = prim::GetAttr[name="weight"](%2987)
  %2989 : Tensor? = prim::GetAttr[name="bias"](%2987)
  %2990 : int[] = prim::ListConstruct(%54, %54)
  %2991 : int[] = prim::ListConstruct(%48, %48)
  %2992 : int[] = prim::ListConstruct(%54, %54)
  %x.7 : Tensor = aten::conv2d(%x.36, %2988, %2989, %2990, %2991, %2992, %54) # torch/nn/modules/conv.py:415:15
  %2994 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%2986)
  %2995 : int = aten::dim(%x.7) # torch/nn/modules/batchnorm.py:276:11
  %2996 : bool = aten::ne(%2995, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2996) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2997 : bool = prim::GetAttr[name="training"](%2994)
   = prim::If(%2997) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2998 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2994)
      %2999 : Tensor = aten::add(%2998, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2994, %2999)
      -> ()
    block1():
      -> ()
  %3000 : bool = prim::GetAttr[name="training"](%2994)
  %3001 : Tensor = prim::GetAttr[name="running_mean"](%2994)
  %3002 : Tensor = prim::GetAttr[name="running_var"](%2994)
  %3003 : Tensor = prim::GetAttr[name="weight"](%2994)
  %3004 : Tensor = prim::GetAttr[name="bias"](%2994)
   = prim::If(%3000) # torch/nn/functional.py:2011:4
    block0():
      %3005 : int[] = aten::size(%x.7) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%3005, %48) # torch/nn/functional.py:1991:17
      %3007 : int = aten::len(%3005) # torch/nn/functional.py:1992:19
      %3008 : int = aten::sub(%3007, %47) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%3008, %49, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %3012 : int = aten::add(%i.3, %47) # torch/nn/functional.py:1993:27
          %3013 : int = aten::__getitem__(%3005, %3012) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %3013) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.11)
      %3015 : bool = aten::eq(%size_prods.9, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3015) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.8 : Tensor = aten::batch_norm(%x.7, %3003, %3004, %3001, %3002, %3000, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3.1 : Tensor = aten::relu_(%x.8) # torch/nn/functional.py:1117:17
  %3018 : __torch__.torchvision.models.inception.___torch_mangle_602.BasicConv2d = prim::GetAttr[name="branch3x3_2a"](%2953)
  %3019 : __torch__.torch.nn.modules.conv.___torch_mangle_601.Conv2d = prim::GetAttr[name="conv"](%3018)
  %3020 : Tensor = prim::GetAttr[name="weight"](%3019)
  %3021 : Tensor? = prim::GetAttr[name="bias"](%3019)
  %3022 : int[] = prim::ListConstruct(%54, %54)
  %3023 : int[] = prim::ListConstruct(%48, %54)
  %3024 : int[] = prim::ListConstruct(%54, %54)
  %x.9 : Tensor = aten::conv2d(%branch3x3.1, %3020, %3021, %3022, %3023, %3024, %54) # torch/nn/modules/conv.py:415:15
  %3026 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%3018)
  %3027 : int = aten::dim(%x.9) # torch/nn/modules/batchnorm.py:276:11
  %3028 : bool = aten::ne(%3027, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3028) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3029 : bool = prim::GetAttr[name="training"](%3026)
   = prim::If(%3029) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3030 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3026)
      %3031 : Tensor = aten::add(%3030, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3026, %3031)
      -> ()
    block1():
      -> ()
  %3032 : bool = prim::GetAttr[name="training"](%3026)
  %3033 : Tensor = prim::GetAttr[name="running_mean"](%3026)
  %3034 : Tensor = prim::GetAttr[name="running_var"](%3026)
  %3035 : Tensor = prim::GetAttr[name="weight"](%3026)
  %3036 : Tensor = prim::GetAttr[name="bias"](%3026)
   = prim::If(%3032) # torch/nn/functional.py:2011:4
    block0():
      %3037 : int[] = aten::size(%x.9) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%3037, %48) # torch/nn/functional.py:1991:17
      %3039 : int = aten::len(%3037) # torch/nn/functional.py:1992:19
      %3040 : int = aten::sub(%3039, %47) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%3040, %49, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %3044 : int = aten::add(%i.4, %47) # torch/nn/functional.py:1993:27
          %3045 : int = aten::__getitem__(%3037, %3044) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %3045) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.15)
      %3047 : bool = aten::eq(%size_prods.13, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3047) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.10 : Tensor = aten::batch_norm(%x.9, %3035, %3036, %3033, %3034, %3032, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.2 : Tensor = aten::relu_(%x.10) # torch/nn/functional.py:1117:17
  %3050 : __torch__.torchvision.models.inception.___torch_mangle_604.BasicConv2d = prim::GetAttr[name="branch3x3_2b"](%2953)
  %3051 : __torch__.torch.nn.modules.conv.___torch_mangle_603.Conv2d = prim::GetAttr[name="conv"](%3050)
  %3052 : Tensor = prim::GetAttr[name="weight"](%3051)
  %3053 : Tensor? = prim::GetAttr[name="bias"](%3051)
  %3054 : int[] = prim::ListConstruct(%54, %54)
  %3055 : int[] = prim::ListConstruct(%54, %48)
  %3056 : int[] = prim::ListConstruct(%54, %54)
  %x.11 : Tensor = aten::conv2d(%branch3x3.1, %3052, %3053, %3054, %3055, %3056, %54) # torch/nn/modules/conv.py:415:15
  %3058 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%3050)
  %3059 : int = aten::dim(%x.11) # torch/nn/modules/batchnorm.py:276:11
  %3060 : bool = aten::ne(%3059, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3060) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3061 : bool = prim::GetAttr[name="training"](%3058)
   = prim::If(%3061) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3062 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3058)
      %3063 : Tensor = aten::add(%3062, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3058, %3063)
      -> ()
    block1():
      -> ()
  %3064 : bool = prim::GetAttr[name="training"](%3058)
  %3065 : Tensor = prim::GetAttr[name="running_mean"](%3058)
  %3066 : Tensor = prim::GetAttr[name="running_var"](%3058)
  %3067 : Tensor = prim::GetAttr[name="weight"](%3058)
  %3068 : Tensor = prim::GetAttr[name="bias"](%3058)
   = prim::If(%3064) # torch/nn/functional.py:2011:4
    block0():
      %3069 : int[] = aten::size(%x.11) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%3069, %48) # torch/nn/functional.py:1991:17
      %3071 : int = aten::len(%3069) # torch/nn/functional.py:1992:19
      %3072 : int = aten::sub(%3071, %47) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%3072, %49, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.18 : int):
          %3076 : int = aten::add(%i.5, %47) # torch/nn/functional.py:1993:27
          %3077 : int = aten::__getitem__(%3069, %3076) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %3077) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.19)
      %3079 : bool = aten::eq(%size_prods.17, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3079) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.12 : Tensor = aten::batch_norm(%x.11, %3067, %3068, %3065, %3066, %3064, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.3 : Tensor = aten::relu_(%x.12) # torch/nn/functional.py:1117:17
  %branch3x3.4 : Tensor[] = prim::ListConstruct(%result.2, %result.3)
  %branch3x3.6 : Tensor = aten::cat(%branch3x3.4, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:380:20
  %3084 : __torch__.torchvision.models.inception.___torch_mangle_617.BasicConv2d = prim::GetAttr[name="branch3x3dbl_1"](%2953)
  %3085 : __torch__.torch.nn.modules.conv.___torch_mangle_616.Conv2d = prim::GetAttr[name="conv"](%3084)
  %3086 : Tensor = prim::GetAttr[name="weight"](%3085)
  %3087 : Tensor? = prim::GetAttr[name="bias"](%3085)
  %3088 : int[] = prim::ListConstruct(%54, %54)
  %3089 : int[] = prim::ListConstruct(%48, %48)
  %3090 : int[] = prim::ListConstruct(%54, %54)
  %x.13 : Tensor = aten::conv2d(%x.36, %3086, %3087, %3088, %3089, %3090, %54) # torch/nn/modules/conv.py:415:15
  %3092 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_606.BatchNorm2d = prim::GetAttr[name="bn"](%3084)
  %3093 : int = aten::dim(%x.13) # torch/nn/modules/batchnorm.py:276:11
  %3094 : bool = aten::ne(%3093, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3094) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3095 : bool = prim::GetAttr[name="training"](%3092)
   = prim::If(%3095) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3096 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3092)
      %3097 : Tensor = aten::add(%3096, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3092, %3097)
      -> ()
    block1():
      -> ()
  %3098 : bool = prim::GetAttr[name="training"](%3092)
  %3099 : Tensor = prim::GetAttr[name="running_mean"](%3092)
  %3100 : Tensor = prim::GetAttr[name="running_var"](%3092)
  %3101 : Tensor = prim::GetAttr[name="weight"](%3092)
  %3102 : Tensor = prim::GetAttr[name="bias"](%3092)
   = prim::If(%3098) # torch/nn/functional.py:2011:4
    block0():
      %3103 : int[] = aten::size(%x.13) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%3103, %48) # torch/nn/functional.py:1991:17
      %3105 : int = aten::len(%3103) # torch/nn/functional.py:1992:19
      %3106 : int = aten::sub(%3105, %47) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%3106, %49, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %3110 : int = aten::add(%i.6, %47) # torch/nn/functional.py:1993:27
          %3111 : int = aten::__getitem__(%3103, %3110) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %3111) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.23)
      %3113 : bool = aten::eq(%size_prods.21, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3113) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.14 : Tensor = aten::batch_norm(%x.13, %3101, %3102, %3099, %3100, %3098, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.1 : Tensor = aten::relu_(%x.14) # torch/nn/functional.py:1117:17
  %3116 : __torch__.torchvision.models.inception.___torch_mangle_609.BasicConv2d = prim::GetAttr[name="branch3x3dbl_2"](%2953)
  %3117 : __torch__.torch.nn.modules.conv.___torch_mangle_608.Conv2d = prim::GetAttr[name="conv"](%3116)
  %3118 : Tensor = prim::GetAttr[name="weight"](%3117)
  %3119 : Tensor? = prim::GetAttr[name="bias"](%3117)
  %3120 : int[] = prim::ListConstruct(%54, %54)
  %3121 : int[] = prim::ListConstruct(%54, %54)
  %3122 : int[] = prim::ListConstruct(%54, %54)
  %x.15 : Tensor = aten::conv2d(%branch3x3dbl.1, %3118, %3119, %3120, %3121, %3122, %54) # torch/nn/modules/conv.py:415:15
  %3124 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%3116)
  %3125 : int = aten::dim(%x.15) # torch/nn/modules/batchnorm.py:276:11
  %3126 : bool = aten::ne(%3125, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3126) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3127 : bool = prim::GetAttr[name="training"](%3124)
   = prim::If(%3127) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3128 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3124)
      %3129 : Tensor = aten::add(%3128, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3124, %3129)
      -> ()
    block1():
      -> ()
  %3130 : bool = prim::GetAttr[name="training"](%3124)
  %3131 : Tensor = prim::GetAttr[name="running_mean"](%3124)
  %3132 : Tensor = prim::GetAttr[name="running_var"](%3124)
  %3133 : Tensor = prim::GetAttr[name="weight"](%3124)
  %3134 : Tensor = prim::GetAttr[name="bias"](%3124)
   = prim::If(%3130) # torch/nn/functional.py:2011:4
    block0():
      %3135 : int[] = aten::size(%x.15) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%3135, %48) # torch/nn/functional.py:1991:17
      %3137 : int = aten::len(%3135) # torch/nn/functional.py:1992:19
      %3138 : int = aten::sub(%3137, %47) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%3138, %49, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %3142 : int = aten::add(%i.7, %47) # torch/nn/functional.py:1993:27
          %3143 : int = aten::__getitem__(%3135, %3142) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %3143) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.27)
      %3145 : bool = aten::eq(%size_prods.25, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3145) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.16 : Tensor = aten::batch_norm(%x.15, %3133, %3134, %3131, %3132, %3130, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch3x3dbl.3 : Tensor = aten::relu_(%x.16) # torch/nn/functional.py:1117:17
  %3148 : __torch__.torchvision.models.inception.___torch_mangle_602.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3a"](%2953)
  %3149 : __torch__.torch.nn.modules.conv.___torch_mangle_601.Conv2d = prim::GetAttr[name="conv"](%3148)
  %3150 : Tensor = prim::GetAttr[name="weight"](%3149)
  %3151 : Tensor? = prim::GetAttr[name="bias"](%3149)
  %3152 : int[] = prim::ListConstruct(%54, %54)
  %3153 : int[] = prim::ListConstruct(%48, %54)
  %3154 : int[] = prim::ListConstruct(%54, %54)
  %x.17 : Tensor = aten::conv2d(%branch3x3dbl.3, %3150, %3151, %3152, %3153, %3154, %54) # torch/nn/modules/conv.py:415:15
  %3156 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%3148)
  %3157 : int = aten::dim(%x.17) # torch/nn/modules/batchnorm.py:276:11
  %3158 : bool = aten::ne(%3157, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3158) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3159 : bool = prim::GetAttr[name="training"](%3156)
   = prim::If(%3159) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3160 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3156)
      %3161 : Tensor = aten::add(%3160, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3156, %3161)
      -> ()
    block1():
      -> ()
  %3162 : bool = prim::GetAttr[name="training"](%3156)
  %3163 : Tensor = prim::GetAttr[name="running_mean"](%3156)
  %3164 : Tensor = prim::GetAttr[name="running_var"](%3156)
  %3165 : Tensor = prim::GetAttr[name="weight"](%3156)
  %3166 : Tensor = prim::GetAttr[name="bias"](%3156)
   = prim::If(%3162) # torch/nn/functional.py:2011:4
    block0():
      %3167 : int[] = aten::size(%x.17) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%3167, %48) # torch/nn/functional.py:1991:17
      %3169 : int = aten::len(%3167) # torch/nn/functional.py:1992:19
      %3170 : int = aten::sub(%3169, %47) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%3170, %49, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %3174 : int = aten::add(%i.8, %47) # torch/nn/functional.py:1993:27
          %3175 : int = aten::__getitem__(%3167, %3174) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %3175) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.31)
      %3177 : bool = aten::eq(%size_prods.29, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3177) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.18 : Tensor = aten::batch_norm(%x.17, %3165, %3166, %3163, %3164, %3162, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.4 : Tensor = aten::relu_(%x.18) # torch/nn/functional.py:1117:17
  %3180 : __torch__.torchvision.models.inception.___torch_mangle_604.BasicConv2d = prim::GetAttr[name="branch3x3dbl_3b"](%2953)
  %3181 : __torch__.torch.nn.modules.conv.___torch_mangle_603.Conv2d = prim::GetAttr[name="conv"](%3180)
  %3182 : Tensor = prim::GetAttr[name="weight"](%3181)
  %3183 : Tensor? = prim::GetAttr[name="bias"](%3181)
  %3184 : int[] = prim::ListConstruct(%54, %54)
  %3185 : int[] = prim::ListConstruct(%54, %48)
  %3186 : int[] = prim::ListConstruct(%54, %54)
  %x.19 : Tensor = aten::conv2d(%branch3x3dbl.3, %3182, %3183, %3184, %3185, %3186, %54) # torch/nn/modules/conv.py:415:15
  %3188 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%3180)
  %3189 : int = aten::dim(%x.19) # torch/nn/modules/batchnorm.py:276:11
  %3190 : bool = aten::ne(%3189, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3190) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3191 : bool = prim::GetAttr[name="training"](%3188)
   = prim::If(%3191) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3192 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3188)
      %3193 : Tensor = aten::add(%3192, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3188, %3193)
      -> ()
    block1():
      -> ()
  %3194 : bool = prim::GetAttr[name="training"](%3188)
  %3195 : Tensor = prim::GetAttr[name="running_mean"](%3188)
  %3196 : Tensor = prim::GetAttr[name="running_var"](%3188)
  %3197 : Tensor = prim::GetAttr[name="weight"](%3188)
  %3198 : Tensor = prim::GetAttr[name="bias"](%3188)
   = prim::If(%3194) # torch/nn/functional.py:2011:4
    block0():
      %3199 : int[] = aten::size(%x.19) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%3199, %48) # torch/nn/functional.py:1991:17
      %3201 : int = aten::len(%3199) # torch/nn/functional.py:1992:19
      %3202 : int = aten::sub(%3201, %47) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%3202, %49, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %3206 : int = aten::add(%i.9, %47) # torch/nn/functional.py:1993:27
          %3207 : int = aten::__getitem__(%3199, %3206) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %3207) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.35)
      %3209 : bool = aten::eq(%size_prods.33, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3209) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.20 : Tensor = aten::batch_norm(%x.19, %3197, %3198, %3195, %3196, %3194, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %result.5 : Tensor = aten::relu_(%x.20) # torch/nn/functional.py:1117:17
  %branch3x3dbl.6 : Tensor[] = prim::ListConstruct(%result.4, %result.5)
  %branch3x3dbl.8 : Tensor = aten::cat(%branch3x3dbl.6, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:388:23
  %3214 : int[] = prim::ListConstruct(%46, %46)
  %3215 : int[] = prim::ListConstruct(%54, %54)
  %3216 : int[] = prim::ListConstruct(%54, %54)
  %branch_pool.1 : Tensor = aten::avg_pool2d(%x.36, %3214, %3215, %3216, %55, %49, %aux.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:390:22
  %3218 : __torch__.torchvision.models.inception.___torch_mangle_619.BasicConv2d = prim::GetAttr[name="branch_pool"](%2953)
  %3219 : __torch__.torch.nn.modules.conv.___torch_mangle_618.Conv2d = prim::GetAttr[name="conv"](%3218)
  %3220 : Tensor = prim::GetAttr[name="weight"](%3219)
  %3221 : Tensor? = prim::GetAttr[name="bias"](%3219)
  %3222 : int[] = prim::ListConstruct(%54, %54)
  %3223 : int[] = prim::ListConstruct(%48, %48)
  %3224 : int[] = prim::ListConstruct(%54, %54)
  %x.3 : Tensor = aten::conv2d(%branch_pool.1, %3220, %3221, %3222, %3223, %3224, %54) # torch/nn/modules/conv.py:415:15
  %3226 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%3218)
  %3227 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %3228 : bool = aten::ne(%3227, %51) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3228) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%50) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3229 : bool = prim::GetAttr[name="training"](%3226)
   = prim::If(%3229) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3230 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3226)
      %3231 : Tensor = aten::add(%3230, %54, %54) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3226, %3231)
      -> ()
    block1():
      -> ()
  %3232 : bool = prim::GetAttr[name="training"](%3226)
  %3233 : Tensor = prim::GetAttr[name="running_mean"](%3226)
  %3234 : Tensor = prim::GetAttr[name="running_var"](%3226)
  %3235 : Tensor = prim::GetAttr[name="weight"](%3226)
  %3236 : Tensor = prim::GetAttr[name="bias"](%3226)
   = prim::If(%3232) # torch/nn/functional.py:2011:4
    block0():
      %3237 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3237, %48) # torch/nn/functional.py:1991:17
      %3239 : int = aten::len(%3237) # torch/nn/functional.py:1992:19
      %3240 : int = aten::sub(%3239, %47) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3240, %49, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %3244 : int = aten::add(%i.1, %47) # torch/nn/functional.py:1993:27
          %3245 : int = aten::__getitem__(%3237, %3244) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3245) # torch/nn/functional.py:1993:8
          -> (%49, %size_prods.3)
      %3247 : bool = aten::eq(%size_prods, %54) # torch/nn/functional.py:1994:7
       = prim::If(%3247) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%50) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.5 : Tensor = aten::batch_norm(%x.3, %3235, %3236, %3233, %3234, %3232, %exponential_average_factor.6, %53, %49) # torch/nn/functional.py:2014:11
  %branch_pool.3 : Tensor = aten::relu_(%x.5) # torch/nn/functional.py:1117:17
  %outputs.2 : Tensor[] = prim::ListConstruct(%branch1x1.1, %branch3x3.6, %branch3x3dbl.8, %branch_pool.3)
  %x.38 : Tensor = aten::cat(%outputs.2, %54) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:398:15
  %3252 : int[] = prim::ListConstruct(%54, %54)
  %3253 : int[] = aten::size(%x.38) # torch/nn/functional.py:925:51
  %3254 : int = aten::len(%3253) # <string>:5:9
  %3255 : bool = aten::gt(%3254, %47) # <string>:5:9
   = prim::If(%3255) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%50) # <string>:5:2
      -> ()
  %x.40 : Tensor = aten::adaptive_avg_pool2d(%x.38, %3252) # torch/nn/functional.py:926:11
  %3257 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="dropout"](%self)
  %3258 : bool = prim::GetAttr[name="training"](%3257)
  %x.42 : Tensor = aten::dropout(%x.40, %43, %3258) # torch/nn/functional.py:973:17
  %x.44 : Tensor = aten::flatten(%x.42, %54, %57) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:184:12
  %3261 : __torch__.torch.nn.modules.linear.___torch_mangle_621.Linear = prim::GetAttr[name="fc"](%self)
  %3262 : Tensor = prim::GetAttr[name="weight"](%3261)
  %3263 : Tensor = prim::GetAttr[name="bias"](%3261)
  %3264 : int = aten::dim(%x.44) # torch/nn/functional.py:1672:7
  %3265 : bool = aten::eq(%3264, %47) # torch/nn/functional.py:1672:7
  %x.46 : Tensor = prim::If(%3265) # torch/nn/functional.py:1672:4
    block0():
      %3267 : Tensor = aten::t(%3262) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%3263, %x.44, %3267, %54, %54) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %3269 : Tensor = aten::t(%3262) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%x.44, %3269) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %3263, %54) # torch/nn/functional.py:1678:12
      -> (%output.3)
  %3272 : (Tensor, Tensor?) = prim::TupleConstruct(%x.46, %aux)
  %x.223 : Tensor, %aux.3 : Tensor? = prim::TupleUnpack(%3272)
  %9 : bool = prim::GetAttr[name="training"](%self)
  %aux_defined.2 : bool = prim::If(%9) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:200:22
    block0():
      %11 : bool = prim::GetAttr[name="aux_logits"](%self)
      -> (%11)
    block1():
      -> (%4)
  %12 : bool = aten::__not__(%aux_defined.2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:202:15
   = prim::If(%12) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:202:12
    block0():
       = aten::warn(%3, %2) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:203:16
      -> ()
    block1():
      -> ()
  %13 : NamedTuple(logits : Tensor, aux_logits : Tensor?) = prim::TupleConstruct(%x.223, %aux.3) # torch/hub/pytorch_vision_master/torchvision/models/inception.py:204:19
  return (%13)
