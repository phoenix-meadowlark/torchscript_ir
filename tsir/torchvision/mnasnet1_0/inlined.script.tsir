graph(%self : __torch__.torchvision.models.mnasnet.___torch_mangle_809.MNASNet,
      %x.1 : Tensor):
  %2 : None = prim::Constant()
  %3 : bool = prim::Constant[value=0]()
  %4 : int = prim::Constant[value=2]() # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:136:20
  %5 : int = prim::Constant[value=3]() # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:136:23
  %6 : __torch__.torch.nn.modules.container.___torch_mangle_808.Sequential = prim::GetAttr[name="layers"](%self)
  %12 : int = prim::Constant[value=1152]() # torch/nn/modules/conv.py:414:53
  %13 : int = prim::Constant[value=576]() # torch/nn/modules/conv.py:414:53
  %14 : int = prim::Constant[value=240]() # torch/nn/modules/conv.py:414:53
  %15 : int = prim::Constant[value=480]() # torch/nn/modules/conv.py:414:53
  %16 : int = prim::Constant[value=120]() # torch/nn/modules/conv.py:414:53
  %17 : int = prim::Constant[value=48]() # torch/nn/modules/conv.py:414:53
  %18 : int = prim::Constant[value=72]() # torch/nn/modules/conv.py:414:53
  %19 : int = prim::Constant[value=32]() # torch/nn/modules/conv.py:414:53
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.00029999999999996696]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : __torch__.torch.nn.modules.conv.___torch_mangle_761.Conv2d = prim::GetAttr[name="0"](%6)
  %29 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_709.BatchNorm2d = prim::GetAttr[name="1"](%6)
  %30 : __torch__.torch.nn.modules.conv.___torch_mangle_762.Conv2d = prim::GetAttr[name="3"](%6)
  %31 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_709.BatchNorm2d = prim::GetAttr[name="4"](%6)
  %32 : __torch__.torch.nn.modules.conv.___torch_mangle_763.Conv2d = prim::GetAttr[name="6"](%6)
  %33 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_623.BatchNorm2d = prim::GetAttr[name="7"](%6)
  %34 : __torch__.torch.nn.modules.container.___torch_mangle_706.Sequential = prim::GetAttr[name="8"](%6)
  %35 : __torch__.torch.nn.modules.container.___torch_mangle_773.Sequential = prim::GetAttr[name="9"](%6)
  %36 : __torch__.torch.nn.modules.container.___torch_mangle_785.Sequential = prim::GetAttr[name="10"](%6)
  %37 : __torch__.torch.nn.modules.container.___torch_mangle_791.Sequential = prim::GetAttr[name="11"](%6)
  %38 : __torch__.torch.nn.modules.container.___torch_mangle_800.Sequential = prim::GetAttr[name="12"](%6)
  %39 : __torch__.torch.nn.modules.container.___torch_mangle_806.Sequential = prim::GetAttr[name="13"](%6)
  %40 : __torch__.torch.nn.modules.conv.___torch_mangle_807.Conv2d = prim::GetAttr[name="14"](%6)
  %41 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_693.BatchNorm2d = prim::GetAttr[name="15"](%6)
  %42 : Tensor = prim::GetAttr[name="weight"](%28)
  %43 : Tensor? = prim::GetAttr[name="bias"](%28)
  %44 : int[] = prim::ListConstruct(%26, %26)
  %45 : int[] = prim::ListConstruct(%27, %27)
  %46 : int[] = prim::ListConstruct(%27, %27)
  %input.246 : Tensor = aten::conv2d(%x.1, %42, %43, %44, %45, %46, %27) # torch/nn/modules/conv.py:415:15
  %48 : int = aten::dim(%input.246) # torch/nn/modules/batchnorm.py:276:11
  %49 : bool = aten::ne(%48, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%49) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %50 : bool = prim::GetAttr[name="training"](%29)
   = prim::If(%50) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %51 : Tensor = prim::GetAttr[name="num_batches_tracked"](%29)
      %52 : Tensor = aten::add(%51, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%29, %52)
      -> ()
    block1():
      -> ()
  %53 : bool = prim::GetAttr[name="training"](%29)
  %54 : Tensor = prim::GetAttr[name="running_mean"](%29)
  %55 : Tensor = prim::GetAttr[name="running_var"](%29)
  %56 : Tensor = prim::GetAttr[name="weight"](%29)
  %57 : Tensor = prim::GetAttr[name="bias"](%29)
   = prim::If(%53) # torch/nn/functional.py:2011:4
    block0():
      %58 : int[] = aten::size(%input.246) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%58, %24) # torch/nn/functional.py:1991:17
      %60 : int = aten::len(%58) # torch/nn/functional.py:1992:19
      %61 : int = aten::sub(%60, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%61, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %65 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %66 : int = aten::__getitem__(%58, %65) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %66) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %68 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%68) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.247 : Tensor = aten::batch_norm(%input.246, %56, %57, %54, %55, %53, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.248 : Tensor = aten::relu_(%input.247) # torch/nn/functional.py:1117:17
  %71 : Tensor = prim::GetAttr[name="weight"](%30)
  %72 : Tensor? = prim::GetAttr[name="bias"](%30)
  %73 : int[] = prim::ListConstruct(%27, %27)
  %74 : int[] = prim::ListConstruct(%27, %27)
  %75 : int[] = prim::ListConstruct(%27, %27)
  %input.249 : Tensor = aten::conv2d(%input.248, %71, %72, %73, %74, %75, %19) # torch/nn/modules/conv.py:415:15
  %77 : int = aten::dim(%input.249) # torch/nn/modules/batchnorm.py:276:11
  %78 : bool = aten::ne(%77, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%78) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %79 : bool = prim::GetAttr[name="training"](%31)
   = prim::If(%79) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %80 : Tensor = prim::GetAttr[name="num_batches_tracked"](%31)
      %81 : Tensor = aten::add(%80, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%31, %81)
      -> ()
    block1():
      -> ()
  %82 : bool = prim::GetAttr[name="training"](%31)
  %83 : Tensor = prim::GetAttr[name="running_mean"](%31)
  %84 : Tensor = prim::GetAttr[name="running_var"](%31)
  %85 : Tensor = prim::GetAttr[name="weight"](%31)
  %86 : Tensor = prim::GetAttr[name="bias"](%31)
   = prim::If(%82) # torch/nn/functional.py:2011:4
    block0():
      %87 : int[] = aten::size(%input.249) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%87, %24) # torch/nn/functional.py:1991:17
      %89 : int = aten::len(%87) # torch/nn/functional.py:1992:19
      %90 : int = aten::sub(%89, %26) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%90, %25, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %94 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
          %95 : int = aten::__getitem__(%87, %94) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %95) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.331)
      %97 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
       = prim::If(%97) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.250 : Tensor = aten::batch_norm(%input.249, %85, %86, %83, %84, %82, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.251 : Tensor = aten::relu_(%input.250) # torch/nn/functional.py:1117:17
  %100 : Tensor = prim::GetAttr[name="weight"](%32)
  %101 : Tensor? = prim::GetAttr[name="bias"](%32)
  %102 : int[] = prim::ListConstruct(%27, %27)
  %103 : int[] = prim::ListConstruct(%24, %24)
  %104 : int[] = prim::ListConstruct(%27, %27)
  %input.252 : Tensor = aten::conv2d(%input.251, %100, %101, %102, %103, %104, %27) # torch/nn/modules/conv.py:415:15
  %106 : int = aten::dim(%input.252) # torch/nn/modules/batchnorm.py:276:11
  %107 : bool = aten::ne(%106, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%107) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %108 : bool = prim::GetAttr[name="training"](%33)
   = prim::If(%108) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %109 : Tensor = prim::GetAttr[name="num_batches_tracked"](%33)
      %110 : Tensor = aten::add(%109, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%33, %110)
      -> ()
    block1():
      -> ()
  %111 : bool = prim::GetAttr[name="training"](%33)
  %112 : Tensor = prim::GetAttr[name="running_mean"](%33)
  %113 : Tensor = prim::GetAttr[name="running_var"](%33)
  %114 : Tensor = prim::GetAttr[name="weight"](%33)
  %115 : Tensor = prim::GetAttr[name="bias"](%33)
   = prim::If(%111) # torch/nn/functional.py:2011:4
    block0():
      %116 : int[] = aten::size(%input.252) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%116, %24) # torch/nn/functional.py:1991:17
      %118 : int = aten::len(%116) # torch/nn/functional.py:1992:19
      %119 : int = aten::sub(%118, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%119, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %123 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %124 : int = aten::__getitem__(%116, %123) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %124) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %126 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%126) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.253 : Tensor = aten::batch_norm(%input.252, %114, %115, %112, %113, %111, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %128 : __torch__.torchvision.models.mnasnet.___torch_mangle_702._InvertedResidual = prim::GetAttr[name="0"](%34)
  %129 : __torch__.torchvision.models.mnasnet.___torch_mangle_705._InvertedResidual = prim::GetAttr[name="1"](%34)
  %130 : __torch__.torchvision.models.mnasnet.___torch_mangle_705._InvertedResidual = prim::GetAttr[name="2"](%34)
  %131 : bool = prim::GetAttr[name="apply_residual"](%128)
  %input.71 : Tensor = prim::If(%131) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %133 : __torch__.torch.nn.modules.container.___torch_mangle_701.Sequential = prim::GetAttr[name="layers"](%128)
      %134 : __torch__.torch.nn.modules.conv.___torch_mangle_632.Conv2d = prim::GetAttr[name="0"](%133)
      %135 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_633.BatchNorm2d = prim::GetAttr[name="1"](%133)
      %136 : __torch__.torch.nn.modules.conv.___torch_mangle_700.Conv2d = prim::GetAttr[name="3"](%133)
      %137 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_633.BatchNorm2d = prim::GetAttr[name="4"](%133)
      %138 : __torch__.torch.nn.modules.conv.___torch_mangle_640.Conv2d = prim::GetAttr[name="6"](%133)
      %139 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%133)
      %140 : Tensor = prim::GetAttr[name="weight"](%134)
      %141 : Tensor? = prim::GetAttr[name="bias"](%134)
      %142 : int[] = prim::ListConstruct(%27, %27)
      %143 : int[] = prim::ListConstruct(%24, %24)
      %144 : int[] = prim::ListConstruct(%27, %27)
      %input.72 : Tensor = aten::conv2d(%input.253, %140, %141, %142, %143, %144, %27) # torch/nn/modules/conv.py:415:15
      %146 : int = aten::dim(%input.72) # torch/nn/modules/batchnorm.py:276:11
      %147 : bool = aten::ne(%146, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%147) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %148 : bool = prim::GetAttr[name="training"](%135)
       = prim::If(%148) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %149 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
          %150 : Tensor = aten::add(%149, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%135, %150)
          -> ()
        block1():
          -> ()
      %151 : bool = prim::GetAttr[name="training"](%135)
      %152 : Tensor = prim::GetAttr[name="running_mean"](%135)
      %153 : Tensor = prim::GetAttr[name="running_var"](%135)
      %154 : Tensor = prim::GetAttr[name="weight"](%135)
      %155 : Tensor = prim::GetAttr[name="bias"](%135)
       = prim::If(%151) # torch/nn/functional.py:2011:4
        block0():
          %156 : int[] = aten::size(%input.72) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%156, %24) # torch/nn/functional.py:1991:17
          %158 : int = aten::len(%156) # torch/nn/functional.py:1992:19
          %159 : int = aten::sub(%158, %26) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%159, %25, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %163 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
              %164 : int = aten::__getitem__(%156, %163) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %164) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.339)
          %166 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
           = prim::If(%166) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.73 : Tensor = aten::batch_norm(%input.72, %154, %155, %152, %153, %151, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.74 : Tensor = aten::relu_(%input.73) # torch/nn/functional.py:1117:17
      %169 : Tensor = prim::GetAttr[name="weight"](%136)
      %170 : Tensor? = prim::GetAttr[name="bias"](%136)
      %171 : int[] = prim::ListConstruct(%26, %26)
      %172 : int[] = prim::ListConstruct(%27, %27)
      %173 : int[] = prim::ListConstruct(%27, %27)
      %input.75 : Tensor = aten::conv2d(%input.74, %169, %170, %171, %172, %173, %17) # torch/nn/modules/conv.py:415:15
      %175 : int = aten::dim(%input.75) # torch/nn/modules/batchnorm.py:276:11
      %176 : bool = aten::ne(%175, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%176) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %177 : bool = prim::GetAttr[name="training"](%137)
       = prim::If(%177) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %178 : Tensor = prim::GetAttr[name="num_batches_tracked"](%137)
          %179 : Tensor = aten::add(%178, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%137, %179)
          -> ()
        block1():
          -> ()
      %180 : bool = prim::GetAttr[name="training"](%137)
      %181 : Tensor = prim::GetAttr[name="running_mean"](%137)
      %182 : Tensor = prim::GetAttr[name="running_var"](%137)
      %183 : Tensor = prim::GetAttr[name="weight"](%137)
      %184 : Tensor = prim::GetAttr[name="bias"](%137)
       = prim::If(%180) # torch/nn/functional.py:2011:4
        block0():
          %185 : int[] = aten::size(%input.75) # torch/nn/functional.py:2012:27
          %size_prods.340 : int = aten::__getitem__(%185, %24) # torch/nn/functional.py:1991:17
          %187 : int = aten::len(%185) # torch/nn/functional.py:1992:19
          %188 : int = aten::sub(%187, %26) # torch/nn/functional.py:1992:19
          %size_prods.341 : int = prim::Loop(%188, %25, %size_prods.340) # torch/nn/functional.py:1992:4
            block0(%i.86 : int, %size_prods.342 : int):
              %192 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
              %193 : int = aten::__getitem__(%185, %192) # torch/nn/functional.py:1993:22
              %size_prods.343 : int = aten::mul(%size_prods.342, %193) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.343)
          %195 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
           = prim::If(%195) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.76 : Tensor = aten::batch_norm(%input.75, %183, %184, %181, %182, %180, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.77 : Tensor = aten::relu_(%input.76) # torch/nn/functional.py:1117:17
      %198 : Tensor = prim::GetAttr[name="weight"](%138)
      %199 : Tensor? = prim::GetAttr[name="bias"](%138)
      %200 : int[] = prim::ListConstruct(%27, %27)
      %201 : int[] = prim::ListConstruct(%24, %24)
      %202 : int[] = prim::ListConstruct(%27, %27)
      %input.78 : Tensor = aten::conv2d(%input.77, %198, %199, %200, %201, %202, %27) # torch/nn/modules/conv.py:415:15
      %204 : int = aten::dim(%input.78) # torch/nn/modules/batchnorm.py:276:11
      %205 : bool = aten::ne(%204, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%205) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %206 : bool = prim::GetAttr[name="training"](%139)
       = prim::If(%206) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %207 : Tensor = prim::GetAttr[name="num_batches_tracked"](%139)
          %208 : Tensor = aten::add(%207, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%139, %208)
          -> ()
        block1():
          -> ()
      %209 : bool = prim::GetAttr[name="training"](%139)
      %210 : Tensor = prim::GetAttr[name="running_mean"](%139)
      %211 : Tensor = prim::GetAttr[name="running_var"](%139)
      %212 : Tensor = prim::GetAttr[name="weight"](%139)
      %213 : Tensor = prim::GetAttr[name="bias"](%139)
       = prim::If(%209) # torch/nn/functional.py:2011:4
        block0():
          %214 : int[] = aten::size(%input.78) # torch/nn/functional.py:2012:27
          %size_prods.344 : int = aten::__getitem__(%214, %24) # torch/nn/functional.py:1991:17
          %216 : int = aten::len(%214) # torch/nn/functional.py:1992:19
          %217 : int = aten::sub(%216, %26) # torch/nn/functional.py:1992:19
          %size_prods.345 : int = prim::Loop(%217, %25, %size_prods.344) # torch/nn/functional.py:1992:4
            block0(%i.87 : int, %size_prods.346 : int):
              %221 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
              %222 : int = aten::__getitem__(%214, %221) # torch/nn/functional.py:1993:22
              %size_prods.347 : int = aten::mul(%size_prods.346, %222) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.347)
          %224 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
           = prim::If(%224) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.79 : Tensor = aten::batch_norm(%input.78, %212, %213, %210, %211, %209, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %226 : Tensor = aten::add(%input.79, %input.253, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%226)
    block1():
      %227 : __torch__.torch.nn.modules.container.___torch_mangle_701.Sequential = prim::GetAttr[name="layers"](%128)
      %228 : __torch__.torch.nn.modules.conv.___torch_mangle_632.Conv2d = prim::GetAttr[name="0"](%227)
      %229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_633.BatchNorm2d = prim::GetAttr[name="1"](%227)
      %230 : __torch__.torch.nn.modules.conv.___torch_mangle_700.Conv2d = prim::GetAttr[name="3"](%227)
      %231 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_633.BatchNorm2d = prim::GetAttr[name="4"](%227)
      %232 : __torch__.torch.nn.modules.conv.___torch_mangle_640.Conv2d = prim::GetAttr[name="6"](%227)
      %233 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%227)
      %234 : Tensor = prim::GetAttr[name="weight"](%228)
      %235 : Tensor? = prim::GetAttr[name="bias"](%228)
      %236 : int[] = prim::ListConstruct(%27, %27)
      %237 : int[] = prim::ListConstruct(%24, %24)
      %238 : int[] = prim::ListConstruct(%27, %27)
      %input.80 : Tensor = aten::conv2d(%input.253, %234, %235, %236, %237, %238, %27) # torch/nn/modules/conv.py:415:15
      %240 : int = aten::dim(%input.80) # torch/nn/modules/batchnorm.py:276:11
      %241 : bool = aten::ne(%240, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%241) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %242 : bool = prim::GetAttr[name="training"](%229)
       = prim::If(%242) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %243 : Tensor = prim::GetAttr[name="num_batches_tracked"](%229)
          %244 : Tensor = aten::add(%243, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%229, %244)
          -> ()
        block1():
          -> ()
      %245 : bool = prim::GetAttr[name="training"](%229)
      %246 : Tensor = prim::GetAttr[name="running_mean"](%229)
      %247 : Tensor = prim::GetAttr[name="running_var"](%229)
      %248 : Tensor = prim::GetAttr[name="weight"](%229)
      %249 : Tensor = prim::GetAttr[name="bias"](%229)
       = prim::If(%245) # torch/nn/functional.py:2011:4
        block0():
          %250 : int[] = aten::size(%input.80) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%250, %24) # torch/nn/functional.py:1991:17
          %252 : int = aten::len(%250) # torch/nn/functional.py:1992:19
          %253 : int = aten::sub(%252, %26) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%253, %25, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %257 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
              %258 : int = aten::__getitem__(%250, %257) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %258) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.99)
          %260 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
           = prim::If(%260) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.81 : Tensor = aten::batch_norm(%input.80, %248, %249, %246, %247, %245, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.82 : Tensor = aten::relu_(%input.81) # torch/nn/functional.py:1117:17
      %263 : Tensor = prim::GetAttr[name="weight"](%230)
      %264 : Tensor? = prim::GetAttr[name="bias"](%230)
      %265 : int[] = prim::ListConstruct(%26, %26)
      %266 : int[] = prim::ListConstruct(%27, %27)
      %267 : int[] = prim::ListConstruct(%27, %27)
      %input.83 : Tensor = aten::conv2d(%input.82, %263, %264, %265, %266, %267, %17) # torch/nn/modules/conv.py:415:15
      %269 : int = aten::dim(%input.83) # torch/nn/modules/batchnorm.py:276:11
      %270 : bool = aten::ne(%269, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%270) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %271 : bool = prim::GetAttr[name="training"](%231)
       = prim::If(%271) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %272 : Tensor = prim::GetAttr[name="num_batches_tracked"](%231)
          %273 : Tensor = aten::add(%272, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%231, %273)
          -> ()
        block1():
          -> ()
      %274 : bool = prim::GetAttr[name="training"](%231)
      %275 : Tensor = prim::GetAttr[name="running_mean"](%231)
      %276 : Tensor = prim::GetAttr[name="running_var"](%231)
      %277 : Tensor = prim::GetAttr[name="weight"](%231)
      %278 : Tensor = prim::GetAttr[name="bias"](%231)
       = prim::If(%274) # torch/nn/functional.py:2011:4
        block0():
          %279 : int[] = aten::size(%input.83) # torch/nn/functional.py:2012:27
          %size_prods.100 : int = aten::__getitem__(%279, %24) # torch/nn/functional.py:1991:17
          %281 : int = aten::len(%279) # torch/nn/functional.py:1992:19
          %282 : int = aten::sub(%281, %26) # torch/nn/functional.py:1992:19
          %size_prods.101 : int = prim::Loop(%282, %25, %size_prods.100) # torch/nn/functional.py:1992:4
            block0(%i.26 : int, %size_prods.102 : int):
              %286 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
              %287 : int = aten::__getitem__(%279, %286) # torch/nn/functional.py:1993:22
              %size_prods.103 : int = aten::mul(%size_prods.102, %287) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.103)
          %289 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
           = prim::If(%289) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.84 : Tensor = aten::batch_norm(%input.83, %277, %278, %275, %276, %274, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.85 : Tensor = aten::relu_(%input.84) # torch/nn/functional.py:1117:17
      %292 : Tensor = prim::GetAttr[name="weight"](%232)
      %293 : Tensor? = prim::GetAttr[name="bias"](%232)
      %294 : int[] = prim::ListConstruct(%27, %27)
      %295 : int[] = prim::ListConstruct(%24, %24)
      %296 : int[] = prim::ListConstruct(%27, %27)
      %input.86 : Tensor = aten::conv2d(%input.85, %292, %293, %294, %295, %296, %27) # torch/nn/modules/conv.py:415:15
      %298 : int = aten::dim(%input.86) # torch/nn/modules/batchnorm.py:276:11
      %299 : bool = aten::ne(%298, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%299) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %300 : bool = prim::GetAttr[name="training"](%233)
       = prim::If(%300) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %301 : Tensor = prim::GetAttr[name="num_batches_tracked"](%233)
          %302 : Tensor = aten::add(%301, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%233, %302)
          -> ()
        block1():
          -> ()
      %303 : bool = prim::GetAttr[name="training"](%233)
      %304 : Tensor = prim::GetAttr[name="running_mean"](%233)
      %305 : Tensor = prim::GetAttr[name="running_var"](%233)
      %306 : Tensor = prim::GetAttr[name="weight"](%233)
      %307 : Tensor = prim::GetAttr[name="bias"](%233)
       = prim::If(%303) # torch/nn/functional.py:2011:4
        block0():
          %308 : int[] = aten::size(%input.86) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%308, %24) # torch/nn/functional.py:1991:17
          %310 : int = aten::len(%308) # torch/nn/functional.py:1992:19
          %311 : int = aten::sub(%310, %26) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%311, %25, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %315 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
              %316 : int = aten::__getitem__(%308, %315) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %316) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.107)
          %318 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
           = prim::If(%318) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.87 : Tensor = aten::batch_norm(%input.86, %306, %307, %304, %305, %303, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.87)
  %320 : bool = prim::GetAttr[name="apply_residual"](%129)
  %input.88 : Tensor = prim::If(%320) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %322 : __torch__.torch.nn.modules.container.___torch_mangle_704.Sequential = prim::GetAttr[name="layers"](%129)
      %323 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%322)
      %324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%322)
      %325 : __torch__.torch.nn.modules.conv.___torch_mangle_703.Conv2d = prim::GetAttr[name="3"](%322)
      %326 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%322)
      %327 : __torch__.torch.nn.modules.conv.___torch_mangle_646.Conv2d = prim::GetAttr[name="6"](%322)
      %328 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%322)
      %329 : Tensor = prim::GetAttr[name="weight"](%323)
      %330 : Tensor? = prim::GetAttr[name="bias"](%323)
      %331 : int[] = prim::ListConstruct(%27, %27)
      %332 : int[] = prim::ListConstruct(%24, %24)
      %333 : int[] = prim::ListConstruct(%27, %27)
      %input.89 : Tensor = aten::conv2d(%input.71, %329, %330, %331, %332, %333, %27) # torch/nn/modules/conv.py:415:15
      %335 : int = aten::dim(%input.89) # torch/nn/modules/batchnorm.py:276:11
      %336 : bool = aten::ne(%335, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%336) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %337 : bool = prim::GetAttr[name="training"](%324)
       = prim::If(%337) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %338 : Tensor = prim::GetAttr[name="num_batches_tracked"](%324)
          %339 : Tensor = aten::add(%338, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%324, %339)
          -> ()
        block1():
          -> ()
      %340 : bool = prim::GetAttr[name="training"](%324)
      %341 : Tensor = prim::GetAttr[name="running_mean"](%324)
      %342 : Tensor = prim::GetAttr[name="running_var"](%324)
      %343 : Tensor = prim::GetAttr[name="weight"](%324)
      %344 : Tensor = prim::GetAttr[name="bias"](%324)
       = prim::If(%340) # torch/nn/functional.py:2011:4
        block0():
          %345 : int[] = aten::size(%input.89) # torch/nn/functional.py:2012:27
          %size_prods.108 : int = aten::__getitem__(%345, %24) # torch/nn/functional.py:1991:17
          %347 : int = aten::len(%345) # torch/nn/functional.py:1992:19
          %348 : int = aten::sub(%347, %26) # torch/nn/functional.py:1992:19
          %size_prods.109 : int = prim::Loop(%348, %25, %size_prods.108) # torch/nn/functional.py:1992:4
            block0(%i.28 : int, %size_prods.110 : int):
              %352 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
              %353 : int = aten::__getitem__(%345, %352) # torch/nn/functional.py:1993:22
              %size_prods.111 : int = aten::mul(%size_prods.110, %353) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.111)
          %355 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
           = prim::If(%355) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.90 : Tensor = aten::batch_norm(%input.89, %343, %344, %341, %342, %340, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.91 : Tensor = aten::relu_(%input.90) # torch/nn/functional.py:1117:17
      %358 : Tensor = prim::GetAttr[name="weight"](%325)
      %359 : Tensor? = prim::GetAttr[name="bias"](%325)
      %360 : int[] = prim::ListConstruct(%27, %27)
      %361 : int[] = prim::ListConstruct(%27, %27)
      %362 : int[] = prim::ListConstruct(%27, %27)
      %input.92 : Tensor = aten::conv2d(%input.91, %358, %359, %360, %361, %362, %18) # torch/nn/modules/conv.py:415:15
      %364 : int = aten::dim(%input.92) # torch/nn/modules/batchnorm.py:276:11
      %365 : bool = aten::ne(%364, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%365) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %366 : bool = prim::GetAttr[name="training"](%326)
       = prim::If(%366) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %367 : Tensor = prim::GetAttr[name="num_batches_tracked"](%326)
          %368 : Tensor = aten::add(%367, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%326, %368)
          -> ()
        block1():
          -> ()
      %369 : bool = prim::GetAttr[name="training"](%326)
      %370 : Tensor = prim::GetAttr[name="running_mean"](%326)
      %371 : Tensor = prim::GetAttr[name="running_var"](%326)
      %372 : Tensor = prim::GetAttr[name="weight"](%326)
      %373 : Tensor = prim::GetAttr[name="bias"](%326)
       = prim::If(%369) # torch/nn/functional.py:2011:4
        block0():
          %374 : int[] = aten::size(%input.92) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%374, %24) # torch/nn/functional.py:1991:17
          %376 : int = aten::len(%374) # torch/nn/functional.py:1992:19
          %377 : int = aten::sub(%376, %26) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%377, %25, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %381 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
              %382 : int = aten::__getitem__(%374, %381) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %382) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.115)
          %384 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
           = prim::If(%384) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.93 : Tensor = aten::batch_norm(%input.92, %372, %373, %370, %371, %369, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.94 : Tensor = aten::relu_(%input.93) # torch/nn/functional.py:1117:17
      %387 : Tensor = prim::GetAttr[name="weight"](%327)
      %388 : Tensor? = prim::GetAttr[name="bias"](%327)
      %389 : int[] = prim::ListConstruct(%27, %27)
      %390 : int[] = prim::ListConstruct(%24, %24)
      %391 : int[] = prim::ListConstruct(%27, %27)
      %input.95 : Tensor = aten::conv2d(%input.94, %387, %388, %389, %390, %391, %27) # torch/nn/modules/conv.py:415:15
      %393 : int = aten::dim(%input.95) # torch/nn/modules/batchnorm.py:276:11
      %394 : bool = aten::ne(%393, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%394) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %395 : bool = prim::GetAttr[name="training"](%328)
       = prim::If(%395) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %396 : Tensor = prim::GetAttr[name="num_batches_tracked"](%328)
          %397 : Tensor = aten::add(%396, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%328, %397)
          -> ()
        block1():
          -> ()
      %398 : bool = prim::GetAttr[name="training"](%328)
      %399 : Tensor = prim::GetAttr[name="running_mean"](%328)
      %400 : Tensor = prim::GetAttr[name="running_var"](%328)
      %401 : Tensor = prim::GetAttr[name="weight"](%328)
      %402 : Tensor = prim::GetAttr[name="bias"](%328)
       = prim::If(%398) # torch/nn/functional.py:2011:4
        block0():
          %403 : int[] = aten::size(%input.95) # torch/nn/functional.py:2012:27
          %size_prods.116 : int = aten::__getitem__(%403, %24) # torch/nn/functional.py:1991:17
          %405 : int = aten::len(%403) # torch/nn/functional.py:1992:19
          %406 : int = aten::sub(%405, %26) # torch/nn/functional.py:1992:19
          %size_prods.117 : int = prim::Loop(%406, %25, %size_prods.116) # torch/nn/functional.py:1992:4
            block0(%i.30 : int, %size_prods.118 : int):
              %410 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
              %411 : int = aten::__getitem__(%403, %410) # torch/nn/functional.py:1993:22
              %size_prods.119 : int = aten::mul(%size_prods.118, %411) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.119)
          %413 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
           = prim::If(%413) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.96 : Tensor = aten::batch_norm(%input.95, %401, %402, %399, %400, %398, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %415 : Tensor = aten::add(%input.96, %input.71, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%415)
    block1():
      %416 : __torch__.torch.nn.modules.container.___torch_mangle_704.Sequential = prim::GetAttr[name="layers"](%129)
      %417 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%416)
      %418 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%416)
      %419 : __torch__.torch.nn.modules.conv.___torch_mangle_703.Conv2d = prim::GetAttr[name="3"](%416)
      %420 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%416)
      %421 : __torch__.torch.nn.modules.conv.___torch_mangle_646.Conv2d = prim::GetAttr[name="6"](%416)
      %422 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%416)
      %423 : Tensor = prim::GetAttr[name="weight"](%417)
      %424 : Tensor? = prim::GetAttr[name="bias"](%417)
      %425 : int[] = prim::ListConstruct(%27, %27)
      %426 : int[] = prim::ListConstruct(%24, %24)
      %427 : int[] = prim::ListConstruct(%27, %27)
      %input.97 : Tensor = aten::conv2d(%input.71, %423, %424, %425, %426, %427, %27) # torch/nn/modules/conv.py:415:15
      %429 : int = aten::dim(%input.97) # torch/nn/modules/batchnorm.py:276:11
      %430 : bool = aten::ne(%429, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%430) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %431 : bool = prim::GetAttr[name="training"](%418)
       = prim::If(%431) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %432 : Tensor = prim::GetAttr[name="num_batches_tracked"](%418)
          %433 : Tensor = aten::add(%432, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%418, %433)
          -> ()
        block1():
          -> ()
      %434 : bool = prim::GetAttr[name="training"](%418)
      %435 : Tensor = prim::GetAttr[name="running_mean"](%418)
      %436 : Tensor = prim::GetAttr[name="running_var"](%418)
      %437 : Tensor = prim::GetAttr[name="weight"](%418)
      %438 : Tensor = prim::GetAttr[name="bias"](%418)
       = prim::If(%434) # torch/nn/functional.py:2011:4
        block0():
          %439 : int[] = aten::size(%input.97) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%439, %24) # torch/nn/functional.py:1991:17
          %441 : int = aten::len(%439) # torch/nn/functional.py:1992:19
          %442 : int = aten::sub(%441, %26) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%442, %25, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %446 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
              %447 : int = aten::__getitem__(%439, %446) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %447) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.123)
          %449 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
           = prim::If(%449) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.98 : Tensor = aten::batch_norm(%input.97, %437, %438, %435, %436, %434, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.99 : Tensor = aten::relu_(%input.98) # torch/nn/functional.py:1117:17
      %452 : Tensor = prim::GetAttr[name="weight"](%419)
      %453 : Tensor? = prim::GetAttr[name="bias"](%419)
      %454 : int[] = prim::ListConstruct(%27, %27)
      %455 : int[] = prim::ListConstruct(%27, %27)
      %456 : int[] = prim::ListConstruct(%27, %27)
      %input.100 : Tensor = aten::conv2d(%input.99, %452, %453, %454, %455, %456, %18) # torch/nn/modules/conv.py:415:15
      %458 : int = aten::dim(%input.100) # torch/nn/modules/batchnorm.py:276:11
      %459 : bool = aten::ne(%458, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%459) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %460 : bool = prim::GetAttr[name="training"](%420)
       = prim::If(%460) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %461 : Tensor = prim::GetAttr[name="num_batches_tracked"](%420)
          %462 : Tensor = aten::add(%461, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%420, %462)
          -> ()
        block1():
          -> ()
      %463 : bool = prim::GetAttr[name="training"](%420)
      %464 : Tensor = prim::GetAttr[name="running_mean"](%420)
      %465 : Tensor = prim::GetAttr[name="running_var"](%420)
      %466 : Tensor = prim::GetAttr[name="weight"](%420)
      %467 : Tensor = prim::GetAttr[name="bias"](%420)
       = prim::If(%463) # torch/nn/functional.py:2011:4
        block0():
          %468 : int[] = aten::size(%input.100) # torch/nn/functional.py:2012:27
          %size_prods.124 : int = aten::__getitem__(%468, %24) # torch/nn/functional.py:1991:17
          %470 : int = aten::len(%468) # torch/nn/functional.py:1992:19
          %471 : int = aten::sub(%470, %26) # torch/nn/functional.py:1992:19
          %size_prods.125 : int = prim::Loop(%471, %25, %size_prods.124) # torch/nn/functional.py:1992:4
            block0(%i.32 : int, %size_prods.126 : int):
              %475 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
              %476 : int = aten::__getitem__(%468, %475) # torch/nn/functional.py:1993:22
              %size_prods.127 : int = aten::mul(%size_prods.126, %476) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.127)
          %478 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
           = prim::If(%478) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.101 : Tensor = aten::batch_norm(%input.100, %466, %467, %464, %465, %463, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.102 : Tensor = aten::relu_(%input.101) # torch/nn/functional.py:1117:17
      %481 : Tensor = prim::GetAttr[name="weight"](%421)
      %482 : Tensor? = prim::GetAttr[name="bias"](%421)
      %483 : int[] = prim::ListConstruct(%27, %27)
      %484 : int[] = prim::ListConstruct(%24, %24)
      %485 : int[] = prim::ListConstruct(%27, %27)
      %input.103 : Tensor = aten::conv2d(%input.102, %481, %482, %483, %484, %485, %27) # torch/nn/modules/conv.py:415:15
      %487 : int = aten::dim(%input.103) # torch/nn/modules/batchnorm.py:276:11
      %488 : bool = aten::ne(%487, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%488) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %489 : bool = prim::GetAttr[name="training"](%422)
       = prim::If(%489) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %490 : Tensor = prim::GetAttr[name="num_batches_tracked"](%422)
          %491 : Tensor = aten::add(%490, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%422, %491)
          -> ()
        block1():
          -> ()
      %492 : bool = prim::GetAttr[name="training"](%422)
      %493 : Tensor = prim::GetAttr[name="running_mean"](%422)
      %494 : Tensor = prim::GetAttr[name="running_var"](%422)
      %495 : Tensor = prim::GetAttr[name="weight"](%422)
      %496 : Tensor = prim::GetAttr[name="bias"](%422)
       = prim::If(%492) # torch/nn/functional.py:2011:4
        block0():
          %497 : int[] = aten::size(%input.103) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%497, %24) # torch/nn/functional.py:1991:17
          %499 : int = aten::len(%497) # torch/nn/functional.py:1992:19
          %500 : int = aten::sub(%499, %26) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%500, %25, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %504 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
              %505 : int = aten::__getitem__(%497, %504) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %505) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.131)
          %507 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
           = prim::If(%507) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.104 : Tensor = aten::batch_norm(%input.103, %495, %496, %493, %494, %492, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.104)
  %509 : bool = prim::GetAttr[name="apply_residual"](%130)
  %input.245 : Tensor = prim::If(%509) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %511 : __torch__.torch.nn.modules.container.___torch_mangle_704.Sequential = prim::GetAttr[name="layers"](%130)
      %512 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%511)
      %513 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%511)
      %514 : __torch__.torch.nn.modules.conv.___torch_mangle_703.Conv2d = prim::GetAttr[name="3"](%511)
      %515 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%511)
      %516 : __torch__.torch.nn.modules.conv.___torch_mangle_646.Conv2d = prim::GetAttr[name="6"](%511)
      %517 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%511)
      %518 : Tensor = prim::GetAttr[name="weight"](%512)
      %519 : Tensor? = prim::GetAttr[name="bias"](%512)
      %520 : int[] = prim::ListConstruct(%27, %27)
      %521 : int[] = prim::ListConstruct(%24, %24)
      %522 : int[] = prim::ListConstruct(%27, %27)
      %input.105 : Tensor = aten::conv2d(%input.88, %518, %519, %520, %521, %522, %27) # torch/nn/modules/conv.py:415:15
      %524 : int = aten::dim(%input.105) # torch/nn/modules/batchnorm.py:276:11
      %525 : bool = aten::ne(%524, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%525) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %526 : bool = prim::GetAttr[name="training"](%513)
       = prim::If(%526) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %527 : Tensor = prim::GetAttr[name="num_batches_tracked"](%513)
          %528 : Tensor = aten::add(%527, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%513, %528)
          -> ()
        block1():
          -> ()
      %529 : bool = prim::GetAttr[name="training"](%513)
      %530 : Tensor = prim::GetAttr[name="running_mean"](%513)
      %531 : Tensor = prim::GetAttr[name="running_var"](%513)
      %532 : Tensor = prim::GetAttr[name="weight"](%513)
      %533 : Tensor = prim::GetAttr[name="bias"](%513)
       = prim::If(%529) # torch/nn/functional.py:2011:4
        block0():
          %534 : int[] = aten::size(%input.105) # torch/nn/functional.py:2012:27
          %size_prods.132 : int = aten::__getitem__(%534, %24) # torch/nn/functional.py:1991:17
          %536 : int = aten::len(%534) # torch/nn/functional.py:1992:19
          %537 : int = aten::sub(%536, %26) # torch/nn/functional.py:1992:19
          %size_prods.133 : int = prim::Loop(%537, %25, %size_prods.132) # torch/nn/functional.py:1992:4
            block0(%i.34 : int, %size_prods.134 : int):
              %541 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
              %542 : int = aten::__getitem__(%534, %541) # torch/nn/functional.py:1993:22
              %size_prods.135 : int = aten::mul(%size_prods.134, %542) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.135)
          %544 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
           = prim::If(%544) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.106 : Tensor = aten::batch_norm(%input.105, %532, %533, %530, %531, %529, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.107 : Tensor = aten::relu_(%input.106) # torch/nn/functional.py:1117:17
      %547 : Tensor = prim::GetAttr[name="weight"](%514)
      %548 : Tensor? = prim::GetAttr[name="bias"](%514)
      %549 : int[] = prim::ListConstruct(%27, %27)
      %550 : int[] = prim::ListConstruct(%27, %27)
      %551 : int[] = prim::ListConstruct(%27, %27)
      %input.108 : Tensor = aten::conv2d(%input.107, %547, %548, %549, %550, %551, %18) # torch/nn/modules/conv.py:415:15
      %553 : int = aten::dim(%input.108) # torch/nn/modules/batchnorm.py:276:11
      %554 : bool = aten::ne(%553, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%554) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %555 : bool = prim::GetAttr[name="training"](%515)
       = prim::If(%555) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %556 : Tensor = prim::GetAttr[name="num_batches_tracked"](%515)
          %557 : Tensor = aten::add(%556, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%515, %557)
          -> ()
        block1():
          -> ()
      %558 : bool = prim::GetAttr[name="training"](%515)
      %559 : Tensor = prim::GetAttr[name="running_mean"](%515)
      %560 : Tensor = prim::GetAttr[name="running_var"](%515)
      %561 : Tensor = prim::GetAttr[name="weight"](%515)
      %562 : Tensor = prim::GetAttr[name="bias"](%515)
       = prim::If(%558) # torch/nn/functional.py:2011:4
        block0():
          %563 : int[] = aten::size(%input.108) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%563, %24) # torch/nn/functional.py:1991:17
          %565 : int = aten::len(%563) # torch/nn/functional.py:1992:19
          %566 : int = aten::sub(%565, %26) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%566, %25, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %570 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
              %571 : int = aten::__getitem__(%563, %570) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %571) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.139)
          %573 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
           = prim::If(%573) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.109 : Tensor = aten::batch_norm(%input.108, %561, %562, %559, %560, %558, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.110 : Tensor = aten::relu_(%input.109) # torch/nn/functional.py:1117:17
      %576 : Tensor = prim::GetAttr[name="weight"](%516)
      %577 : Tensor? = prim::GetAttr[name="bias"](%516)
      %578 : int[] = prim::ListConstruct(%27, %27)
      %579 : int[] = prim::ListConstruct(%24, %24)
      %580 : int[] = prim::ListConstruct(%27, %27)
      %input.111 : Tensor = aten::conv2d(%input.110, %576, %577, %578, %579, %580, %27) # torch/nn/modules/conv.py:415:15
      %582 : int = aten::dim(%input.111) # torch/nn/modules/batchnorm.py:276:11
      %583 : bool = aten::ne(%582, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%583) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %584 : bool = prim::GetAttr[name="training"](%517)
       = prim::If(%584) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %585 : Tensor = prim::GetAttr[name="num_batches_tracked"](%517)
          %586 : Tensor = aten::add(%585, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%517, %586)
          -> ()
        block1():
          -> ()
      %587 : bool = prim::GetAttr[name="training"](%517)
      %588 : Tensor = prim::GetAttr[name="running_mean"](%517)
      %589 : Tensor = prim::GetAttr[name="running_var"](%517)
      %590 : Tensor = prim::GetAttr[name="weight"](%517)
      %591 : Tensor = prim::GetAttr[name="bias"](%517)
       = prim::If(%587) # torch/nn/functional.py:2011:4
        block0():
          %592 : int[] = aten::size(%input.111) # torch/nn/functional.py:2012:27
          %size_prods.140 : int = aten::__getitem__(%592, %24) # torch/nn/functional.py:1991:17
          %594 : int = aten::len(%592) # torch/nn/functional.py:1992:19
          %595 : int = aten::sub(%594, %26) # torch/nn/functional.py:1992:19
          %size_prods.141 : int = prim::Loop(%595, %25, %size_prods.140) # torch/nn/functional.py:1992:4
            block0(%i.36 : int, %size_prods.142 : int):
              %599 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
              %600 : int = aten::__getitem__(%592, %599) # torch/nn/functional.py:1993:22
              %size_prods.143 : int = aten::mul(%size_prods.142, %600) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.143)
          %602 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
           = prim::If(%602) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.112 : Tensor = aten::batch_norm(%input.111, %590, %591, %588, %589, %587, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %604 : Tensor = aten::add(%input.112, %input.88, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%604)
    block1():
      %605 : __torch__.torch.nn.modules.container.___torch_mangle_704.Sequential = prim::GetAttr[name="layers"](%130)
      %606 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%605)
      %607 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%605)
      %608 : __torch__.torch.nn.modules.conv.___torch_mangle_703.Conv2d = prim::GetAttr[name="3"](%605)
      %609 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%605)
      %610 : __torch__.torch.nn.modules.conv.___torch_mangle_646.Conv2d = prim::GetAttr[name="6"](%605)
      %611 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_628.BatchNorm2d = prim::GetAttr[name="7"](%605)
      %612 : Tensor = prim::GetAttr[name="weight"](%606)
      %613 : Tensor? = prim::GetAttr[name="bias"](%606)
      %614 : int[] = prim::ListConstruct(%27, %27)
      %615 : int[] = prim::ListConstruct(%24, %24)
      %616 : int[] = prim::ListConstruct(%27, %27)
      %input.113 : Tensor = aten::conv2d(%input.88, %612, %613, %614, %615, %616, %27) # torch/nn/modules/conv.py:415:15
      %618 : int = aten::dim(%input.113) # torch/nn/modules/batchnorm.py:276:11
      %619 : bool = aten::ne(%618, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%619) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %620 : bool = prim::GetAttr[name="training"](%607)
       = prim::If(%620) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %621 : Tensor = prim::GetAttr[name="num_batches_tracked"](%607)
          %622 : Tensor = aten::add(%621, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%607, %622)
          -> ()
        block1():
          -> ()
      %623 : bool = prim::GetAttr[name="training"](%607)
      %624 : Tensor = prim::GetAttr[name="running_mean"](%607)
      %625 : Tensor = prim::GetAttr[name="running_var"](%607)
      %626 : Tensor = prim::GetAttr[name="weight"](%607)
      %627 : Tensor = prim::GetAttr[name="bias"](%607)
       = prim::If(%623) # torch/nn/functional.py:2011:4
        block0():
          %628 : int[] = aten::size(%input.113) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%628, %24) # torch/nn/functional.py:1991:17
          %630 : int = aten::len(%628) # torch/nn/functional.py:1992:19
          %631 : int = aten::sub(%630, %26) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%631, %25, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %635 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
              %636 : int = aten::__getitem__(%628, %635) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %636) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.147)
          %638 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
           = prim::If(%638) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.114 : Tensor = aten::batch_norm(%input.113, %626, %627, %624, %625, %623, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.115 : Tensor = aten::relu_(%input.114) # torch/nn/functional.py:1117:17
      %641 : Tensor = prim::GetAttr[name="weight"](%608)
      %642 : Tensor? = prim::GetAttr[name="bias"](%608)
      %643 : int[] = prim::ListConstruct(%27, %27)
      %644 : int[] = prim::ListConstruct(%27, %27)
      %645 : int[] = prim::ListConstruct(%27, %27)
      %input.116 : Tensor = aten::conv2d(%input.115, %641, %642, %643, %644, %645, %18) # torch/nn/modules/conv.py:415:15
      %647 : int = aten::dim(%input.116) # torch/nn/modules/batchnorm.py:276:11
      %648 : bool = aten::ne(%647, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%648) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %649 : bool = prim::GetAttr[name="training"](%609)
       = prim::If(%649) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %650 : Tensor = prim::GetAttr[name="num_batches_tracked"](%609)
          %651 : Tensor = aten::add(%650, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%609, %651)
          -> ()
        block1():
          -> ()
      %652 : bool = prim::GetAttr[name="training"](%609)
      %653 : Tensor = prim::GetAttr[name="running_mean"](%609)
      %654 : Tensor = prim::GetAttr[name="running_var"](%609)
      %655 : Tensor = prim::GetAttr[name="weight"](%609)
      %656 : Tensor = prim::GetAttr[name="bias"](%609)
       = prim::If(%652) # torch/nn/functional.py:2011:4
        block0():
          %657 : int[] = aten::size(%input.116) # torch/nn/functional.py:2012:27
          %size_prods.148 : int = aten::__getitem__(%657, %24) # torch/nn/functional.py:1991:17
          %659 : int = aten::len(%657) # torch/nn/functional.py:1992:19
          %660 : int = aten::sub(%659, %26) # torch/nn/functional.py:1992:19
          %size_prods.149 : int = prim::Loop(%660, %25, %size_prods.148) # torch/nn/functional.py:1992:4
            block0(%i.38 : int, %size_prods.150 : int):
              %664 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
              %665 : int = aten::__getitem__(%657, %664) # torch/nn/functional.py:1993:22
              %size_prods.151 : int = aten::mul(%size_prods.150, %665) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.151)
          %667 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
           = prim::If(%667) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.117 : Tensor = aten::batch_norm(%input.116, %655, %656, %653, %654, %652, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.118 : Tensor = aten::relu_(%input.117) # torch/nn/functional.py:1117:17
      %670 : Tensor = prim::GetAttr[name="weight"](%610)
      %671 : Tensor? = prim::GetAttr[name="bias"](%610)
      %672 : int[] = prim::ListConstruct(%27, %27)
      %673 : int[] = prim::ListConstruct(%24, %24)
      %674 : int[] = prim::ListConstruct(%27, %27)
      %input.119 : Tensor = aten::conv2d(%input.118, %670, %671, %672, %673, %674, %27) # torch/nn/modules/conv.py:415:15
      %676 : int = aten::dim(%input.119) # torch/nn/modules/batchnorm.py:276:11
      %677 : bool = aten::ne(%676, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%677) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %678 : bool = prim::GetAttr[name="training"](%611)
       = prim::If(%678) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %679 : Tensor = prim::GetAttr[name="num_batches_tracked"](%611)
          %680 : Tensor = aten::add(%679, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%611, %680)
          -> ()
        block1():
          -> ()
      %681 : bool = prim::GetAttr[name="training"](%611)
      %682 : Tensor = prim::GetAttr[name="running_mean"](%611)
      %683 : Tensor = prim::GetAttr[name="running_var"](%611)
      %684 : Tensor = prim::GetAttr[name="weight"](%611)
      %685 : Tensor = prim::GetAttr[name="bias"](%611)
       = prim::If(%681) # torch/nn/functional.py:2011:4
        block0():
          %686 : int[] = aten::size(%input.119) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%686, %24) # torch/nn/functional.py:1991:17
          %688 : int = aten::len(%686) # torch/nn/functional.py:1992:19
          %689 : int = aten::sub(%688, %26) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%689, %25, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %693 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
              %694 : int = aten::__getitem__(%686, %693) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %694) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.155)
          %696 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
           = prim::If(%696) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.120 : Tensor = aten::batch_norm(%input.119, %684, %685, %682, %683, %681, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.120)
  %698 : __torch__.torchvision.models.mnasnet.___torch_mangle_766._InvertedResidual = prim::GetAttr[name="0"](%35)
  %699 : __torch__.torchvision.models.mnasnet.___torch_mangle_772._InvertedResidual = prim::GetAttr[name="1"](%35)
  %700 : __torch__.torchvision.models.mnasnet.___torch_mangle_772._InvertedResidual = prim::GetAttr[name="2"](%35)
  %701 : bool = prim::GetAttr[name="apply_residual"](%698)
  %input.121 : Tensor = prim::If(%701) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %703 : __torch__.torch.nn.modules.container.___torch_mangle_765.Sequential = prim::GetAttr[name="layers"](%698)
      %704 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%703)
      %705 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%703)
      %706 : __torch__.torch.nn.modules.conv.___torch_mangle_707.Conv2d = prim::GetAttr[name="3"](%703)
      %707 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%703)
      %708 : __torch__.torch.nn.modules.conv.___torch_mangle_764.Conv2d = prim::GetAttr[name="6"](%703)
      %709 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%703)
      %710 : Tensor = prim::GetAttr[name="weight"](%704)
      %711 : Tensor? = prim::GetAttr[name="bias"](%704)
      %712 : int[] = prim::ListConstruct(%27, %27)
      %713 : int[] = prim::ListConstruct(%24, %24)
      %714 : int[] = prim::ListConstruct(%27, %27)
      %input.122 : Tensor = aten::conv2d(%input.245, %710, %711, %712, %713, %714, %27) # torch/nn/modules/conv.py:415:15
      %716 : int = aten::dim(%input.122) # torch/nn/modules/batchnorm.py:276:11
      %717 : bool = aten::ne(%716, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%717) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %718 : bool = prim::GetAttr[name="training"](%705)
       = prim::If(%718) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %719 : Tensor = prim::GetAttr[name="num_batches_tracked"](%705)
          %720 : Tensor = aten::add(%719, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%705, %720)
          -> ()
        block1():
          -> ()
      %721 : bool = prim::GetAttr[name="training"](%705)
      %722 : Tensor = prim::GetAttr[name="running_mean"](%705)
      %723 : Tensor = prim::GetAttr[name="running_var"](%705)
      %724 : Tensor = prim::GetAttr[name="weight"](%705)
      %725 : Tensor = prim::GetAttr[name="bias"](%705)
       = prim::If(%721) # torch/nn/functional.py:2011:4
        block0():
          %726 : int[] = aten::size(%input.122) # torch/nn/functional.py:2012:27
          %size_prods.156 : int = aten::__getitem__(%726, %24) # torch/nn/functional.py:1991:17
          %728 : int = aten::len(%726) # torch/nn/functional.py:1992:19
          %729 : int = aten::sub(%728, %26) # torch/nn/functional.py:1992:19
          %size_prods.157 : int = prim::Loop(%729, %25, %size_prods.156) # torch/nn/functional.py:1992:4
            block0(%i.40 : int, %size_prods.158 : int):
              %733 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
              %734 : int = aten::__getitem__(%726, %733) # torch/nn/functional.py:1993:22
              %size_prods.159 : int = aten::mul(%size_prods.158, %734) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.159)
          %736 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
           = prim::If(%736) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.123 : Tensor = aten::batch_norm(%input.122, %724, %725, %722, %723, %721, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.124 : Tensor = aten::relu_(%input.123) # torch/nn/functional.py:1117:17
      %739 : Tensor = prim::GetAttr[name="weight"](%706)
      %740 : Tensor? = prim::GetAttr[name="bias"](%706)
      %741 : int[] = prim::ListConstruct(%26, %26)
      %742 : int[] = prim::ListConstruct(%26, %26)
      %743 : int[] = prim::ListConstruct(%27, %27)
      %input.125 : Tensor = aten::conv2d(%input.124, %739, %740, %741, %742, %743, %18) # torch/nn/modules/conv.py:415:15
      %745 : int = aten::dim(%input.125) # torch/nn/modules/batchnorm.py:276:11
      %746 : bool = aten::ne(%745, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%746) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %747 : bool = prim::GetAttr[name="training"](%707)
       = prim::If(%747) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %748 : Tensor = prim::GetAttr[name="num_batches_tracked"](%707)
          %749 : Tensor = aten::add(%748, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%707, %749)
          -> ()
        block1():
          -> ()
      %750 : bool = prim::GetAttr[name="training"](%707)
      %751 : Tensor = prim::GetAttr[name="running_mean"](%707)
      %752 : Tensor = prim::GetAttr[name="running_var"](%707)
      %753 : Tensor = prim::GetAttr[name="weight"](%707)
      %754 : Tensor = prim::GetAttr[name="bias"](%707)
       = prim::If(%750) # torch/nn/functional.py:2011:4
        block0():
          %755 : int[] = aten::size(%input.125) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%755, %24) # torch/nn/functional.py:1991:17
          %757 : int = aten::len(%755) # torch/nn/functional.py:1992:19
          %758 : int = aten::sub(%757, %26) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%758, %25, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %762 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
              %763 : int = aten::__getitem__(%755, %762) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %763) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.163)
          %765 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
           = prim::If(%765) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.126 : Tensor = aten::batch_norm(%input.125, %753, %754, %751, %752, %750, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.127 : Tensor = aten::relu_(%input.126) # torch/nn/functional.py:1117:17
      %768 : Tensor = prim::GetAttr[name="weight"](%708)
      %769 : Tensor? = prim::GetAttr[name="bias"](%708)
      %770 : int[] = prim::ListConstruct(%27, %27)
      %771 : int[] = prim::ListConstruct(%24, %24)
      %772 : int[] = prim::ListConstruct(%27, %27)
      %input.128 : Tensor = aten::conv2d(%input.127, %768, %769, %770, %771, %772, %27) # torch/nn/modules/conv.py:415:15
      %774 : int = aten::dim(%input.128) # torch/nn/modules/batchnorm.py:276:11
      %775 : bool = aten::ne(%774, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%775) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %776 : bool = prim::GetAttr[name="training"](%709)
       = prim::If(%776) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %777 : Tensor = prim::GetAttr[name="num_batches_tracked"](%709)
          %778 : Tensor = aten::add(%777, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%709, %778)
          -> ()
        block1():
          -> ()
      %779 : bool = prim::GetAttr[name="training"](%709)
      %780 : Tensor = prim::GetAttr[name="running_mean"](%709)
      %781 : Tensor = prim::GetAttr[name="running_var"](%709)
      %782 : Tensor = prim::GetAttr[name="weight"](%709)
      %783 : Tensor = prim::GetAttr[name="bias"](%709)
       = prim::If(%779) # torch/nn/functional.py:2011:4
        block0():
          %784 : int[] = aten::size(%input.128) # torch/nn/functional.py:2012:27
          %size_prods.164 : int = aten::__getitem__(%784, %24) # torch/nn/functional.py:1991:17
          %786 : int = aten::len(%784) # torch/nn/functional.py:1992:19
          %787 : int = aten::sub(%786, %26) # torch/nn/functional.py:1992:19
          %size_prods.165 : int = prim::Loop(%787, %25, %size_prods.164) # torch/nn/functional.py:1992:4
            block0(%i.42 : int, %size_prods.166 : int):
              %791 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
              %792 : int = aten::__getitem__(%784, %791) # torch/nn/functional.py:1993:22
              %size_prods.167 : int = aten::mul(%size_prods.166, %792) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.167)
          %794 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
           = prim::If(%794) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.129 : Tensor = aten::batch_norm(%input.128, %782, %783, %780, %781, %779, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %796 : Tensor = aten::add(%input.129, %input.245, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%796)
    block1():
      %797 : __torch__.torch.nn.modules.container.___torch_mangle_765.Sequential = prim::GetAttr[name="layers"](%698)
      %798 : __torch__.torch.nn.modules.conv.___torch_mangle_643.Conv2d = prim::GetAttr[name="0"](%797)
      %799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="1"](%797)
      %800 : __torch__.torch.nn.modules.conv.___torch_mangle_707.Conv2d = prim::GetAttr[name="3"](%797)
      %801 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_644.BatchNorm2d = prim::GetAttr[name="4"](%797)
      %802 : __torch__.torch.nn.modules.conv.___torch_mangle_764.Conv2d = prim::GetAttr[name="6"](%797)
      %803 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%797)
      %804 : Tensor = prim::GetAttr[name="weight"](%798)
      %805 : Tensor? = prim::GetAttr[name="bias"](%798)
      %806 : int[] = prim::ListConstruct(%27, %27)
      %807 : int[] = prim::ListConstruct(%24, %24)
      %808 : int[] = prim::ListConstruct(%27, %27)
      %input.130 : Tensor = aten::conv2d(%input.245, %804, %805, %806, %807, %808, %27) # torch/nn/modules/conv.py:415:15
      %810 : int = aten::dim(%input.130) # torch/nn/modules/batchnorm.py:276:11
      %811 : bool = aten::ne(%810, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%811) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %812 : bool = prim::GetAttr[name="training"](%799)
       = prim::If(%812) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %813 : Tensor = prim::GetAttr[name="num_batches_tracked"](%799)
          %814 : Tensor = aten::add(%813, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%799, %814)
          -> ()
        block1():
          -> ()
      %815 : bool = prim::GetAttr[name="training"](%799)
      %816 : Tensor = prim::GetAttr[name="running_mean"](%799)
      %817 : Tensor = prim::GetAttr[name="running_var"](%799)
      %818 : Tensor = prim::GetAttr[name="weight"](%799)
      %819 : Tensor = prim::GetAttr[name="bias"](%799)
       = prim::If(%815) # torch/nn/functional.py:2011:4
        block0():
          %820 : int[] = aten::size(%input.130) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%820, %24) # torch/nn/functional.py:1991:17
          %822 : int = aten::len(%820) # torch/nn/functional.py:1992:19
          %823 : int = aten::sub(%822, %26) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%823, %25, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %827 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
              %828 : int = aten::__getitem__(%820, %827) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %828) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.171)
          %830 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
           = prim::If(%830) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.131 : Tensor = aten::batch_norm(%input.130, %818, %819, %816, %817, %815, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.132 : Tensor = aten::relu_(%input.131) # torch/nn/functional.py:1117:17
      %833 : Tensor = prim::GetAttr[name="weight"](%800)
      %834 : Tensor? = prim::GetAttr[name="bias"](%800)
      %835 : int[] = prim::ListConstruct(%26, %26)
      %836 : int[] = prim::ListConstruct(%26, %26)
      %837 : int[] = prim::ListConstruct(%27, %27)
      %input.133 : Tensor = aten::conv2d(%input.132, %833, %834, %835, %836, %837, %18) # torch/nn/modules/conv.py:415:15
      %839 : int = aten::dim(%input.133) # torch/nn/modules/batchnorm.py:276:11
      %840 : bool = aten::ne(%839, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%840) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %841 : bool = prim::GetAttr[name="training"](%801)
       = prim::If(%841) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %842 : Tensor = prim::GetAttr[name="num_batches_tracked"](%801)
          %843 : Tensor = aten::add(%842, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%801, %843)
          -> ()
        block1():
          -> ()
      %844 : bool = prim::GetAttr[name="training"](%801)
      %845 : Tensor = prim::GetAttr[name="running_mean"](%801)
      %846 : Tensor = prim::GetAttr[name="running_var"](%801)
      %847 : Tensor = prim::GetAttr[name="weight"](%801)
      %848 : Tensor = prim::GetAttr[name="bias"](%801)
       = prim::If(%844) # torch/nn/functional.py:2011:4
        block0():
          %849 : int[] = aten::size(%input.133) # torch/nn/functional.py:2012:27
          %size_prods.172 : int = aten::__getitem__(%849, %24) # torch/nn/functional.py:1991:17
          %851 : int = aten::len(%849) # torch/nn/functional.py:1992:19
          %852 : int = aten::sub(%851, %26) # torch/nn/functional.py:1992:19
          %size_prods.173 : int = prim::Loop(%852, %25, %size_prods.172) # torch/nn/functional.py:1992:4
            block0(%i.44 : int, %size_prods.174 : int):
              %856 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
              %857 : int = aten::__getitem__(%849, %856) # torch/nn/functional.py:1993:22
              %size_prods.175 : int = aten::mul(%size_prods.174, %857) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.175)
          %859 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
           = prim::If(%859) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.134 : Tensor = aten::batch_norm(%input.133, %847, %848, %845, %846, %844, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.135 : Tensor = aten::relu_(%input.134) # torch/nn/functional.py:1117:17
      %862 : Tensor = prim::GetAttr[name="weight"](%802)
      %863 : Tensor? = prim::GetAttr[name="bias"](%802)
      %864 : int[] = prim::ListConstruct(%27, %27)
      %865 : int[] = prim::ListConstruct(%24, %24)
      %866 : int[] = prim::ListConstruct(%27, %27)
      %input.136 : Tensor = aten::conv2d(%input.135, %862, %863, %864, %865, %866, %27) # torch/nn/modules/conv.py:415:15
      %868 : int = aten::dim(%input.136) # torch/nn/modules/batchnorm.py:276:11
      %869 : bool = aten::ne(%868, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%869) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %870 : bool = prim::GetAttr[name="training"](%803)
       = prim::If(%870) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %871 : Tensor = prim::GetAttr[name="num_batches_tracked"](%803)
          %872 : Tensor = aten::add(%871, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%803, %872)
          -> ()
        block1():
          -> ()
      %873 : bool = prim::GetAttr[name="training"](%803)
      %874 : Tensor = prim::GetAttr[name="running_mean"](%803)
      %875 : Tensor = prim::GetAttr[name="running_var"](%803)
      %876 : Tensor = prim::GetAttr[name="weight"](%803)
      %877 : Tensor = prim::GetAttr[name="bias"](%803)
       = prim::If(%873) # torch/nn/functional.py:2011:4
        block0():
          %878 : int[] = aten::size(%input.136) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%878, %24) # torch/nn/functional.py:1991:17
          %880 : int = aten::len(%878) # torch/nn/functional.py:1992:19
          %881 : int = aten::sub(%880, %26) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%881, %25, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %885 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
              %886 : int = aten::__getitem__(%878, %885) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %886) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.179)
          %888 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
           = prim::If(%888) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.137 : Tensor = aten::batch_norm(%input.136, %876, %877, %874, %875, %873, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.137)
  %890 : bool = prim::GetAttr[name="apply_residual"](%699)
  %input.138 : Tensor = prim::If(%890) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %892 : __torch__.torch.nn.modules.container.___torch_mangle_771.Sequential = prim::GetAttr[name="layers"](%699)
      %893 : __torch__.torch.nn.modules.conv.___torch_mangle_767.Conv2d = prim::GetAttr[name="0"](%892)
      %894 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="1"](%892)
      %895 : __torch__.torch.nn.modules.conv.___torch_mangle_769.Conv2d = prim::GetAttr[name="3"](%892)
      %896 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="4"](%892)
      %897 : __torch__.torch.nn.modules.conv.___torch_mangle_770.Conv2d = prim::GetAttr[name="6"](%892)
      %898 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%892)
      %899 : Tensor = prim::GetAttr[name="weight"](%893)
      %900 : Tensor? = prim::GetAttr[name="bias"](%893)
      %901 : int[] = prim::ListConstruct(%27, %27)
      %902 : int[] = prim::ListConstruct(%24, %24)
      %903 : int[] = prim::ListConstruct(%27, %27)
      %input.139 : Tensor = aten::conv2d(%input.121, %899, %900, %901, %902, %903, %27) # torch/nn/modules/conv.py:415:15
      %905 : int = aten::dim(%input.139) # torch/nn/modules/batchnorm.py:276:11
      %906 : bool = aten::ne(%905, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%906) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %907 : bool = prim::GetAttr[name="training"](%894)
       = prim::If(%907) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %908 : Tensor = prim::GetAttr[name="num_batches_tracked"](%894)
          %909 : Tensor = aten::add(%908, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%894, %909)
          -> ()
        block1():
          -> ()
      %910 : bool = prim::GetAttr[name="training"](%894)
      %911 : Tensor = prim::GetAttr[name="running_mean"](%894)
      %912 : Tensor = prim::GetAttr[name="running_var"](%894)
      %913 : Tensor = prim::GetAttr[name="weight"](%894)
      %914 : Tensor = prim::GetAttr[name="bias"](%894)
       = prim::If(%910) # torch/nn/functional.py:2011:4
        block0():
          %915 : int[] = aten::size(%input.139) # torch/nn/functional.py:2012:27
          %size_prods.180 : int = aten::__getitem__(%915, %24) # torch/nn/functional.py:1991:17
          %917 : int = aten::len(%915) # torch/nn/functional.py:1992:19
          %918 : int = aten::sub(%917, %26) # torch/nn/functional.py:1992:19
          %size_prods.181 : int = prim::Loop(%918, %25, %size_prods.180) # torch/nn/functional.py:1992:4
            block0(%i.46 : int, %size_prods.182 : int):
              %922 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
              %923 : int = aten::__getitem__(%915, %922) # torch/nn/functional.py:1993:22
              %size_prods.183 : int = aten::mul(%size_prods.182, %923) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.183)
          %925 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
           = prim::If(%925) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.140 : Tensor = aten::batch_norm(%input.139, %913, %914, %911, %912, %910, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.141 : Tensor = aten::relu_(%input.140) # torch/nn/functional.py:1117:17
      %928 : Tensor = prim::GetAttr[name="weight"](%895)
      %929 : Tensor? = prim::GetAttr[name="bias"](%895)
      %930 : int[] = prim::ListConstruct(%27, %27)
      %931 : int[] = prim::ListConstruct(%26, %26)
      %932 : int[] = prim::ListConstruct(%27, %27)
      %input.142 : Tensor = aten::conv2d(%input.141, %928, %929, %930, %931, %932, %16) # torch/nn/modules/conv.py:415:15
      %934 : int = aten::dim(%input.142) # torch/nn/modules/batchnorm.py:276:11
      %935 : bool = aten::ne(%934, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%935) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %936 : bool = prim::GetAttr[name="training"](%896)
       = prim::If(%936) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %937 : Tensor = prim::GetAttr[name="num_batches_tracked"](%896)
          %938 : Tensor = aten::add(%937, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%896, %938)
          -> ()
        block1():
          -> ()
      %939 : bool = prim::GetAttr[name="training"](%896)
      %940 : Tensor = prim::GetAttr[name="running_mean"](%896)
      %941 : Tensor = prim::GetAttr[name="running_var"](%896)
      %942 : Tensor = prim::GetAttr[name="weight"](%896)
      %943 : Tensor = prim::GetAttr[name="bias"](%896)
       = prim::If(%939) # torch/nn/functional.py:2011:4
        block0():
          %944 : int[] = aten::size(%input.142) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%944, %24) # torch/nn/functional.py:1991:17
          %946 : int = aten::len(%944) # torch/nn/functional.py:1992:19
          %947 : int = aten::sub(%946, %26) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%947, %25, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %951 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
              %952 : int = aten::__getitem__(%944, %951) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %952) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.187)
          %954 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
           = prim::If(%954) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.143 : Tensor = aten::batch_norm(%input.142, %942, %943, %940, %941, %939, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.144 : Tensor = aten::relu_(%input.143) # torch/nn/functional.py:1117:17
      %957 : Tensor = prim::GetAttr[name="weight"](%897)
      %958 : Tensor? = prim::GetAttr[name="bias"](%897)
      %959 : int[] = prim::ListConstruct(%27, %27)
      %960 : int[] = prim::ListConstruct(%24, %24)
      %961 : int[] = prim::ListConstruct(%27, %27)
      %input.145 : Tensor = aten::conv2d(%input.144, %957, %958, %959, %960, %961, %27) # torch/nn/modules/conv.py:415:15
      %963 : int = aten::dim(%input.145) # torch/nn/modules/batchnorm.py:276:11
      %964 : bool = aten::ne(%963, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%964) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %965 : bool = prim::GetAttr[name="training"](%898)
       = prim::If(%965) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %966 : Tensor = prim::GetAttr[name="num_batches_tracked"](%898)
          %967 : Tensor = aten::add(%966, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%898, %967)
          -> ()
        block1():
          -> ()
      %968 : bool = prim::GetAttr[name="training"](%898)
      %969 : Tensor = prim::GetAttr[name="running_mean"](%898)
      %970 : Tensor = prim::GetAttr[name="running_var"](%898)
      %971 : Tensor = prim::GetAttr[name="weight"](%898)
      %972 : Tensor = prim::GetAttr[name="bias"](%898)
       = prim::If(%968) # torch/nn/functional.py:2011:4
        block0():
          %973 : int[] = aten::size(%input.145) # torch/nn/functional.py:2012:27
          %size_prods.188 : int = aten::__getitem__(%973, %24) # torch/nn/functional.py:1991:17
          %975 : int = aten::len(%973) # torch/nn/functional.py:1992:19
          %976 : int = aten::sub(%975, %26) # torch/nn/functional.py:1992:19
          %size_prods.189 : int = prim::Loop(%976, %25, %size_prods.188) # torch/nn/functional.py:1992:4
            block0(%i.48 : int, %size_prods.190 : int):
              %980 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
              %981 : int = aten::__getitem__(%973, %980) # torch/nn/functional.py:1993:22
              %size_prods.191 : int = aten::mul(%size_prods.190, %981) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.191)
          %983 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
           = prim::If(%983) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.146 : Tensor = aten::batch_norm(%input.145, %971, %972, %969, %970, %968, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %985 : Tensor = aten::add(%input.146, %input.121, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%985)
    block1():
      %986 : __torch__.torch.nn.modules.container.___torch_mangle_771.Sequential = prim::GetAttr[name="layers"](%699)
      %987 : __torch__.torch.nn.modules.conv.___torch_mangle_767.Conv2d = prim::GetAttr[name="0"](%986)
      %988 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="1"](%986)
      %989 : __torch__.torch.nn.modules.conv.___torch_mangle_769.Conv2d = prim::GetAttr[name="3"](%986)
      %990 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="4"](%986)
      %991 : __torch__.torch.nn.modules.conv.___torch_mangle_770.Conv2d = prim::GetAttr[name="6"](%986)
      %992 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%986)
      %993 : Tensor = prim::GetAttr[name="weight"](%987)
      %994 : Tensor? = prim::GetAttr[name="bias"](%987)
      %995 : int[] = prim::ListConstruct(%27, %27)
      %996 : int[] = prim::ListConstruct(%24, %24)
      %997 : int[] = prim::ListConstruct(%27, %27)
      %input.147 : Tensor = aten::conv2d(%input.121, %993, %994, %995, %996, %997, %27) # torch/nn/modules/conv.py:415:15
      %999 : int = aten::dim(%input.147) # torch/nn/modules/batchnorm.py:276:11
      %1000 : bool = aten::ne(%999, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1000) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1001 : bool = prim::GetAttr[name="training"](%988)
       = prim::If(%1001) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1002 : Tensor = prim::GetAttr[name="num_batches_tracked"](%988)
          %1003 : Tensor = aten::add(%1002, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%988, %1003)
          -> ()
        block1():
          -> ()
      %1004 : bool = prim::GetAttr[name="training"](%988)
      %1005 : Tensor = prim::GetAttr[name="running_mean"](%988)
      %1006 : Tensor = prim::GetAttr[name="running_var"](%988)
      %1007 : Tensor = prim::GetAttr[name="weight"](%988)
      %1008 : Tensor = prim::GetAttr[name="bias"](%988)
       = prim::If(%1004) # torch/nn/functional.py:2011:4
        block0():
          %1009 : int[] = aten::size(%input.147) # torch/nn/functional.py:2012:27
          %size_prods.192 : int = aten::__getitem__(%1009, %24) # torch/nn/functional.py:1991:17
          %1011 : int = aten::len(%1009) # torch/nn/functional.py:1992:19
          %1012 : int = aten::sub(%1011, %26) # torch/nn/functional.py:1992:19
          %size_prods.193 : int = prim::Loop(%1012, %25, %size_prods.192) # torch/nn/functional.py:1992:4
            block0(%i.49 : int, %size_prods.194 : int):
              %1016 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
              %1017 : int = aten::__getitem__(%1009, %1016) # torch/nn/functional.py:1993:22
              %size_prods.195 : int = aten::mul(%size_prods.194, %1017) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.195)
          %1019 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1019) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.148 : Tensor = aten::batch_norm(%input.147, %1007, %1008, %1005, %1006, %1004, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.149 : Tensor = aten::relu_(%input.148) # torch/nn/functional.py:1117:17
      %1022 : Tensor = prim::GetAttr[name="weight"](%989)
      %1023 : Tensor? = prim::GetAttr[name="bias"](%989)
      %1024 : int[] = prim::ListConstruct(%27, %27)
      %1025 : int[] = prim::ListConstruct(%26, %26)
      %1026 : int[] = prim::ListConstruct(%27, %27)
      %input.150 : Tensor = aten::conv2d(%input.149, %1022, %1023, %1024, %1025, %1026, %16) # torch/nn/modules/conv.py:415:15
      %1028 : int = aten::dim(%input.150) # torch/nn/modules/batchnorm.py:276:11
      %1029 : bool = aten::ne(%1028, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1029) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1030 : bool = prim::GetAttr[name="training"](%990)
       = prim::If(%1030) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1031 : Tensor = prim::GetAttr[name="num_batches_tracked"](%990)
          %1032 : Tensor = aten::add(%1031, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%990, %1032)
          -> ()
        block1():
          -> ()
      %1033 : bool = prim::GetAttr[name="training"](%990)
      %1034 : Tensor = prim::GetAttr[name="running_mean"](%990)
      %1035 : Tensor = prim::GetAttr[name="running_var"](%990)
      %1036 : Tensor = prim::GetAttr[name="weight"](%990)
      %1037 : Tensor = prim::GetAttr[name="bias"](%990)
       = prim::If(%1033) # torch/nn/functional.py:2011:4
        block0():
          %1038 : int[] = aten::size(%input.150) # torch/nn/functional.py:2012:27
          %size_prods.196 : int = aten::__getitem__(%1038, %24) # torch/nn/functional.py:1991:17
          %1040 : int = aten::len(%1038) # torch/nn/functional.py:1992:19
          %1041 : int = aten::sub(%1040, %26) # torch/nn/functional.py:1992:19
          %size_prods.197 : int = prim::Loop(%1041, %25, %size_prods.196) # torch/nn/functional.py:1992:4
            block0(%i.50 : int, %size_prods.198 : int):
              %1045 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
              %1046 : int = aten::__getitem__(%1038, %1045) # torch/nn/functional.py:1993:22
              %size_prods.199 : int = aten::mul(%size_prods.198, %1046) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.199)
          %1048 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1048) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.151 : Tensor = aten::batch_norm(%input.150, %1036, %1037, %1034, %1035, %1033, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.152 : Tensor = aten::relu_(%input.151) # torch/nn/functional.py:1117:17
      %1051 : Tensor = prim::GetAttr[name="weight"](%991)
      %1052 : Tensor? = prim::GetAttr[name="bias"](%991)
      %1053 : int[] = prim::ListConstruct(%27, %27)
      %1054 : int[] = prim::ListConstruct(%24, %24)
      %1055 : int[] = prim::ListConstruct(%27, %27)
      %input.153 : Tensor = aten::conv2d(%input.152, %1051, %1052, %1053, %1054, %1055, %27) # torch/nn/modules/conv.py:415:15
      %1057 : int = aten::dim(%input.153) # torch/nn/modules/batchnorm.py:276:11
      %1058 : bool = aten::ne(%1057, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1058) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1059 : bool = prim::GetAttr[name="training"](%992)
       = prim::If(%1059) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1060 : Tensor = prim::GetAttr[name="num_batches_tracked"](%992)
          %1061 : Tensor = aten::add(%1060, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%992, %1061)
          -> ()
        block1():
          -> ()
      %1062 : bool = prim::GetAttr[name="training"](%992)
      %1063 : Tensor = prim::GetAttr[name="running_mean"](%992)
      %1064 : Tensor = prim::GetAttr[name="running_var"](%992)
      %1065 : Tensor = prim::GetAttr[name="weight"](%992)
      %1066 : Tensor = prim::GetAttr[name="bias"](%992)
       = prim::If(%1062) # torch/nn/functional.py:2011:4
        block0():
          %1067 : int[] = aten::size(%input.153) # torch/nn/functional.py:2012:27
          %size_prods.200 : int = aten::__getitem__(%1067, %24) # torch/nn/functional.py:1991:17
          %1069 : int = aten::len(%1067) # torch/nn/functional.py:1992:19
          %1070 : int = aten::sub(%1069, %26) # torch/nn/functional.py:1992:19
          %size_prods.201 : int = prim::Loop(%1070, %25, %size_prods.200) # torch/nn/functional.py:1992:4
            block0(%i.51 : int, %size_prods.202 : int):
              %1074 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
              %1075 : int = aten::__getitem__(%1067, %1074) # torch/nn/functional.py:1993:22
              %size_prods.203 : int = aten::mul(%size_prods.202, %1075) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.203)
          %1077 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1077) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.154 : Tensor = aten::batch_norm(%input.153, %1065, %1066, %1063, %1064, %1062, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.154)
  %1079 : bool = prim::GetAttr[name="apply_residual"](%700)
  %input.244 : Tensor = prim::If(%1079) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %1081 : __torch__.torch.nn.modules.container.___torch_mangle_771.Sequential = prim::GetAttr[name="layers"](%700)
      %1082 : __torch__.torch.nn.modules.conv.___torch_mangle_767.Conv2d = prim::GetAttr[name="0"](%1081)
      %1083 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="1"](%1081)
      %1084 : __torch__.torch.nn.modules.conv.___torch_mangle_769.Conv2d = prim::GetAttr[name="3"](%1081)
      %1085 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="4"](%1081)
      %1086 : __torch__.torch.nn.modules.conv.___torch_mangle_770.Conv2d = prim::GetAttr[name="6"](%1081)
      %1087 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%1081)
      %1088 : Tensor = prim::GetAttr[name="weight"](%1082)
      %1089 : Tensor? = prim::GetAttr[name="bias"](%1082)
      %1090 : int[] = prim::ListConstruct(%27, %27)
      %1091 : int[] = prim::ListConstruct(%24, %24)
      %1092 : int[] = prim::ListConstruct(%27, %27)
      %input.155 : Tensor = aten::conv2d(%input.138, %1088, %1089, %1090, %1091, %1092, %27) # torch/nn/modules/conv.py:415:15
      %1094 : int = aten::dim(%input.155) # torch/nn/modules/batchnorm.py:276:11
      %1095 : bool = aten::ne(%1094, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1095) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1096 : bool = prim::GetAttr[name="training"](%1083)
       = prim::If(%1096) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1097 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1083)
          %1098 : Tensor = aten::add(%1097, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1083, %1098)
          -> ()
        block1():
          -> ()
      %1099 : bool = prim::GetAttr[name="training"](%1083)
      %1100 : Tensor = prim::GetAttr[name="running_mean"](%1083)
      %1101 : Tensor = prim::GetAttr[name="running_var"](%1083)
      %1102 : Tensor = prim::GetAttr[name="weight"](%1083)
      %1103 : Tensor = prim::GetAttr[name="bias"](%1083)
       = prim::If(%1099) # torch/nn/functional.py:2011:4
        block0():
          %1104 : int[] = aten::size(%input.155) # torch/nn/functional.py:2012:27
          %size_prods.204 : int = aten::__getitem__(%1104, %24) # torch/nn/functional.py:1991:17
          %1106 : int = aten::len(%1104) # torch/nn/functional.py:1992:19
          %1107 : int = aten::sub(%1106, %26) # torch/nn/functional.py:1992:19
          %size_prods.205 : int = prim::Loop(%1107, %25, %size_prods.204) # torch/nn/functional.py:1992:4
            block0(%i.52 : int, %size_prods.206 : int):
              %1111 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
              %1112 : int = aten::__getitem__(%1104, %1111) # torch/nn/functional.py:1993:22
              %size_prods.207 : int = aten::mul(%size_prods.206, %1112) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.207)
          %1114 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1114) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.156 : Tensor = aten::batch_norm(%input.155, %1102, %1103, %1100, %1101, %1099, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.157 : Tensor = aten::relu_(%input.156) # torch/nn/functional.py:1117:17
      %1117 : Tensor = prim::GetAttr[name="weight"](%1084)
      %1118 : Tensor? = prim::GetAttr[name="bias"](%1084)
      %1119 : int[] = prim::ListConstruct(%27, %27)
      %1120 : int[] = prim::ListConstruct(%26, %26)
      %1121 : int[] = prim::ListConstruct(%27, %27)
      %input.158 : Tensor = aten::conv2d(%input.157, %1117, %1118, %1119, %1120, %1121, %16) # torch/nn/modules/conv.py:415:15
      %1123 : int = aten::dim(%input.158) # torch/nn/modules/batchnorm.py:276:11
      %1124 : bool = aten::ne(%1123, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1124) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1125 : bool = prim::GetAttr[name="training"](%1085)
       = prim::If(%1125) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1126 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1085)
          %1127 : Tensor = aten::add(%1126, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1085, %1127)
          -> ()
        block1():
          -> ()
      %1128 : bool = prim::GetAttr[name="training"](%1085)
      %1129 : Tensor = prim::GetAttr[name="running_mean"](%1085)
      %1130 : Tensor = prim::GetAttr[name="running_var"](%1085)
      %1131 : Tensor = prim::GetAttr[name="weight"](%1085)
      %1132 : Tensor = prim::GetAttr[name="bias"](%1085)
       = prim::If(%1128) # torch/nn/functional.py:2011:4
        block0():
          %1133 : int[] = aten::size(%input.158) # torch/nn/functional.py:2012:27
          %size_prods.208 : int = aten::__getitem__(%1133, %24) # torch/nn/functional.py:1991:17
          %1135 : int = aten::len(%1133) # torch/nn/functional.py:1992:19
          %1136 : int = aten::sub(%1135, %26) # torch/nn/functional.py:1992:19
          %size_prods.209 : int = prim::Loop(%1136, %25, %size_prods.208) # torch/nn/functional.py:1992:4
            block0(%i.53 : int, %size_prods.210 : int):
              %1140 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
              %1141 : int = aten::__getitem__(%1133, %1140) # torch/nn/functional.py:1993:22
              %size_prods.211 : int = aten::mul(%size_prods.210, %1141) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.211)
          %1143 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1143) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.159 : Tensor = aten::batch_norm(%input.158, %1131, %1132, %1129, %1130, %1128, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.160 : Tensor = aten::relu_(%input.159) # torch/nn/functional.py:1117:17
      %1146 : Tensor = prim::GetAttr[name="weight"](%1086)
      %1147 : Tensor? = prim::GetAttr[name="bias"](%1086)
      %1148 : int[] = prim::ListConstruct(%27, %27)
      %1149 : int[] = prim::ListConstruct(%24, %24)
      %1150 : int[] = prim::ListConstruct(%27, %27)
      %input.161 : Tensor = aten::conv2d(%input.160, %1146, %1147, %1148, %1149, %1150, %27) # torch/nn/modules/conv.py:415:15
      %1152 : int = aten::dim(%input.161) # torch/nn/modules/batchnorm.py:276:11
      %1153 : bool = aten::ne(%1152, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1153) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1154 : bool = prim::GetAttr[name="training"](%1087)
       = prim::If(%1154) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1155 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1087)
          %1156 : Tensor = aten::add(%1155, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1087, %1156)
          -> ()
        block1():
          -> ()
      %1157 : bool = prim::GetAttr[name="training"](%1087)
      %1158 : Tensor = prim::GetAttr[name="running_mean"](%1087)
      %1159 : Tensor = prim::GetAttr[name="running_var"](%1087)
      %1160 : Tensor = prim::GetAttr[name="weight"](%1087)
      %1161 : Tensor = prim::GetAttr[name="bias"](%1087)
       = prim::If(%1157) # torch/nn/functional.py:2011:4
        block0():
          %1162 : int[] = aten::size(%input.161) # torch/nn/functional.py:2012:27
          %size_prods.212 : int = aten::__getitem__(%1162, %24) # torch/nn/functional.py:1991:17
          %1164 : int = aten::len(%1162) # torch/nn/functional.py:1992:19
          %1165 : int = aten::sub(%1164, %26) # torch/nn/functional.py:1992:19
          %size_prods.213 : int = prim::Loop(%1165, %25, %size_prods.212) # torch/nn/functional.py:1992:4
            block0(%i.54 : int, %size_prods.214 : int):
              %1169 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
              %1170 : int = aten::__getitem__(%1162, %1169) # torch/nn/functional.py:1993:22
              %size_prods.215 : int = aten::mul(%size_prods.214, %1170) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.215)
          %1172 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1172) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.162 : Tensor = aten::batch_norm(%input.161, %1160, %1161, %1158, %1159, %1157, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %1174 : Tensor = aten::add(%input.162, %input.138, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%1174)
    block1():
      %1175 : __torch__.torch.nn.modules.container.___torch_mangle_771.Sequential = prim::GetAttr[name="layers"](%700)
      %1176 : __torch__.torch.nn.modules.conv.___torch_mangle_767.Conv2d = prim::GetAttr[name="0"](%1175)
      %1177 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="1"](%1175)
      %1178 : __torch__.torch.nn.modules.conv.___torch_mangle_769.Conv2d = prim::GetAttr[name="3"](%1175)
      %1179 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_768.BatchNorm2d = prim::GetAttr[name="4"](%1175)
      %1180 : __torch__.torch.nn.modules.conv.___torch_mangle_770.Conv2d = prim::GetAttr[name="6"](%1175)
      %1181 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_654.BatchNorm2d = prim::GetAttr[name="7"](%1175)
      %1182 : Tensor = prim::GetAttr[name="weight"](%1176)
      %1183 : Tensor? = prim::GetAttr[name="bias"](%1176)
      %1184 : int[] = prim::ListConstruct(%27, %27)
      %1185 : int[] = prim::ListConstruct(%24, %24)
      %1186 : int[] = prim::ListConstruct(%27, %27)
      %input.163 : Tensor = aten::conv2d(%input.138, %1182, %1183, %1184, %1185, %1186, %27) # torch/nn/modules/conv.py:415:15
      %1188 : int = aten::dim(%input.163) # torch/nn/modules/batchnorm.py:276:11
      %1189 : bool = aten::ne(%1188, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1189) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1190 : bool = prim::GetAttr[name="training"](%1177)
       = prim::If(%1190) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1191 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1177)
          %1192 : Tensor = aten::add(%1191, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1177, %1192)
          -> ()
        block1():
          -> ()
      %1193 : bool = prim::GetAttr[name="training"](%1177)
      %1194 : Tensor = prim::GetAttr[name="running_mean"](%1177)
      %1195 : Tensor = prim::GetAttr[name="running_var"](%1177)
      %1196 : Tensor = prim::GetAttr[name="weight"](%1177)
      %1197 : Tensor = prim::GetAttr[name="bias"](%1177)
       = prim::If(%1193) # torch/nn/functional.py:2011:4
        block0():
          %1198 : int[] = aten::size(%input.163) # torch/nn/functional.py:2012:27
          %size_prods.216 : int = aten::__getitem__(%1198, %24) # torch/nn/functional.py:1991:17
          %1200 : int = aten::len(%1198) # torch/nn/functional.py:1992:19
          %1201 : int = aten::sub(%1200, %26) # torch/nn/functional.py:1992:19
          %size_prods.217 : int = prim::Loop(%1201, %25, %size_prods.216) # torch/nn/functional.py:1992:4
            block0(%i.55 : int, %size_prods.218 : int):
              %1205 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
              %1206 : int = aten::__getitem__(%1198, %1205) # torch/nn/functional.py:1993:22
              %size_prods.219 : int = aten::mul(%size_prods.218, %1206) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.219)
          %1208 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1208) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.164 : Tensor = aten::batch_norm(%input.163, %1196, %1197, %1194, %1195, %1193, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.165 : Tensor = aten::relu_(%input.164) # torch/nn/functional.py:1117:17
      %1211 : Tensor = prim::GetAttr[name="weight"](%1178)
      %1212 : Tensor? = prim::GetAttr[name="bias"](%1178)
      %1213 : int[] = prim::ListConstruct(%27, %27)
      %1214 : int[] = prim::ListConstruct(%26, %26)
      %1215 : int[] = prim::ListConstruct(%27, %27)
      %input.166 : Tensor = aten::conv2d(%input.165, %1211, %1212, %1213, %1214, %1215, %16) # torch/nn/modules/conv.py:415:15
      %1217 : int = aten::dim(%input.166) # torch/nn/modules/batchnorm.py:276:11
      %1218 : bool = aten::ne(%1217, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1218) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1219 : bool = prim::GetAttr[name="training"](%1179)
       = prim::If(%1219) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1220 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1179)
          %1221 : Tensor = aten::add(%1220, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1179, %1221)
          -> ()
        block1():
          -> ()
      %1222 : bool = prim::GetAttr[name="training"](%1179)
      %1223 : Tensor = prim::GetAttr[name="running_mean"](%1179)
      %1224 : Tensor = prim::GetAttr[name="running_var"](%1179)
      %1225 : Tensor = prim::GetAttr[name="weight"](%1179)
      %1226 : Tensor = prim::GetAttr[name="bias"](%1179)
       = prim::If(%1222) # torch/nn/functional.py:2011:4
        block0():
          %1227 : int[] = aten::size(%input.166) # torch/nn/functional.py:2012:27
          %size_prods.220 : int = aten::__getitem__(%1227, %24) # torch/nn/functional.py:1991:17
          %1229 : int = aten::len(%1227) # torch/nn/functional.py:1992:19
          %1230 : int = aten::sub(%1229, %26) # torch/nn/functional.py:1992:19
          %size_prods.221 : int = prim::Loop(%1230, %25, %size_prods.220) # torch/nn/functional.py:1992:4
            block0(%i.56 : int, %size_prods.222 : int):
              %1234 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
              %1235 : int = aten::__getitem__(%1227, %1234) # torch/nn/functional.py:1993:22
              %size_prods.223 : int = aten::mul(%size_prods.222, %1235) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.223)
          %1237 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1237) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.167 : Tensor = aten::batch_norm(%input.166, %1225, %1226, %1223, %1224, %1222, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.168 : Tensor = aten::relu_(%input.167) # torch/nn/functional.py:1117:17
      %1240 : Tensor = prim::GetAttr[name="weight"](%1180)
      %1241 : Tensor? = prim::GetAttr[name="bias"](%1180)
      %1242 : int[] = prim::ListConstruct(%27, %27)
      %1243 : int[] = prim::ListConstruct(%24, %24)
      %1244 : int[] = prim::ListConstruct(%27, %27)
      %input.169 : Tensor = aten::conv2d(%input.168, %1240, %1241, %1242, %1243, %1244, %27) # torch/nn/modules/conv.py:415:15
      %1246 : int = aten::dim(%input.169) # torch/nn/modules/batchnorm.py:276:11
      %1247 : bool = aten::ne(%1246, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1247) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1248 : bool = prim::GetAttr[name="training"](%1181)
       = prim::If(%1248) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1249 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1181)
          %1250 : Tensor = aten::add(%1249, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1181, %1250)
          -> ()
        block1():
          -> ()
      %1251 : bool = prim::GetAttr[name="training"](%1181)
      %1252 : Tensor = prim::GetAttr[name="running_mean"](%1181)
      %1253 : Tensor = prim::GetAttr[name="running_var"](%1181)
      %1254 : Tensor = prim::GetAttr[name="weight"](%1181)
      %1255 : Tensor = prim::GetAttr[name="bias"](%1181)
       = prim::If(%1251) # torch/nn/functional.py:2011:4
        block0():
          %1256 : int[] = aten::size(%input.169) # torch/nn/functional.py:2012:27
          %size_prods.224 : int = aten::__getitem__(%1256, %24) # torch/nn/functional.py:1991:17
          %1258 : int = aten::len(%1256) # torch/nn/functional.py:1992:19
          %1259 : int = aten::sub(%1258, %26) # torch/nn/functional.py:1992:19
          %size_prods.225 : int = prim::Loop(%1259, %25, %size_prods.224) # torch/nn/functional.py:1992:4
            block0(%i.57 : int, %size_prods.226 : int):
              %1263 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
              %1264 : int = aten::__getitem__(%1256, %1263) # torch/nn/functional.py:1993:22
              %size_prods.227 : int = aten::mul(%size_prods.226, %1264) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.227)
          %1266 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1266) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.170 : Tensor = aten::batch_norm(%input.169, %1254, %1255, %1252, %1253, %1251, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.170)
  %1268 : __torch__.torchvision.models.mnasnet.___torch_mangle_778._InvertedResidual = prim::GetAttr[name="0"](%36)
  %1269 : __torch__.torchvision.models.mnasnet.___torch_mangle_784._InvertedResidual = prim::GetAttr[name="1"](%36)
  %1270 : __torch__.torchvision.models.mnasnet.___torch_mangle_784._InvertedResidual = prim::GetAttr[name="2"](%36)
  %1271 : bool = prim::GetAttr[name="apply_residual"](%1268)
  %input.187 : Tensor = prim::If(%1271) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %1273 : __torch__.torch.nn.modules.container.___torch_mangle_777.Sequential = prim::GetAttr[name="layers"](%1268)
      %1274 : __torch__.torch.nn.modules.conv.___torch_mangle_657.Conv2d = prim::GetAttr[name="0"](%1273)
      %1275 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_658.BatchNorm2d = prim::GetAttr[name="1"](%1273)
      %1276 : __torch__.torch.nn.modules.conv.___torch_mangle_774.Conv2d = prim::GetAttr[name="3"](%1273)
      %1277 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_658.BatchNorm2d = prim::GetAttr[name="4"](%1273)
      %1278 : __torch__.torch.nn.modules.conv.___torch_mangle_775.Conv2d = prim::GetAttr[name="6"](%1273)
      %1279 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1273)
      %1280 : Tensor = prim::GetAttr[name="weight"](%1274)
      %1281 : Tensor? = prim::GetAttr[name="bias"](%1274)
      %1282 : int[] = prim::ListConstruct(%27, %27)
      %1283 : int[] = prim::ListConstruct(%24, %24)
      %1284 : int[] = prim::ListConstruct(%27, %27)
      %input.172 : Tensor = aten::conv2d(%input.244, %1280, %1281, %1282, %1283, %1284, %27) # torch/nn/modules/conv.py:415:15
      %1286 : int = aten::dim(%input.172) # torch/nn/modules/batchnorm.py:276:11
      %1287 : bool = aten::ne(%1286, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1287) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1288 : bool = prim::GetAttr[name="training"](%1275)
       = prim::If(%1288) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1289 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1275)
          %1290 : Tensor = aten::add(%1289, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1275, %1290)
          -> ()
        block1():
          -> ()
      %1291 : bool = prim::GetAttr[name="training"](%1275)
      %1292 : Tensor = prim::GetAttr[name="running_mean"](%1275)
      %1293 : Tensor = prim::GetAttr[name="running_var"](%1275)
      %1294 : Tensor = prim::GetAttr[name="weight"](%1275)
      %1295 : Tensor = prim::GetAttr[name="bias"](%1275)
       = prim::If(%1291) # torch/nn/functional.py:2011:4
        block0():
          %1296 : int[] = aten::size(%input.172) # torch/nn/functional.py:2012:27
          %size_prods.228 : int = aten::__getitem__(%1296, %24) # torch/nn/functional.py:1991:17
          %1298 : int = aten::len(%1296) # torch/nn/functional.py:1992:19
          %1299 : int = aten::sub(%1298, %26) # torch/nn/functional.py:1992:19
          %size_prods.229 : int = prim::Loop(%1299, %25, %size_prods.228) # torch/nn/functional.py:1992:4
            block0(%i.58 : int, %size_prods.230 : int):
              %1303 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
              %1304 : int = aten::__getitem__(%1296, %1303) # torch/nn/functional.py:1993:22
              %size_prods.231 : int = aten::mul(%size_prods.230, %1304) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.231)
          %1306 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1306) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.173 : Tensor = aten::batch_norm(%input.172, %1294, %1295, %1292, %1293, %1291, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.174 : Tensor = aten::relu_(%input.173) # torch/nn/functional.py:1117:17
      %1309 : Tensor = prim::GetAttr[name="weight"](%1276)
      %1310 : Tensor? = prim::GetAttr[name="bias"](%1276)
      %1311 : int[] = prim::ListConstruct(%26, %26)
      %1312 : int[] = prim::ListConstruct(%26, %26)
      %1313 : int[] = prim::ListConstruct(%27, %27)
      %input.175 : Tensor = aten::conv2d(%input.174, %1309, %1310, %1311, %1312, %1313, %14) # torch/nn/modules/conv.py:415:15
      %1315 : int = aten::dim(%input.175) # torch/nn/modules/batchnorm.py:276:11
      %1316 : bool = aten::ne(%1315, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1316) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1317 : bool = prim::GetAttr[name="training"](%1277)
       = prim::If(%1317) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1318 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1277)
          %1319 : Tensor = aten::add(%1318, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1277, %1319)
          -> ()
        block1():
          -> ()
      %1320 : bool = prim::GetAttr[name="training"](%1277)
      %1321 : Tensor = prim::GetAttr[name="running_mean"](%1277)
      %1322 : Tensor = prim::GetAttr[name="running_var"](%1277)
      %1323 : Tensor = prim::GetAttr[name="weight"](%1277)
      %1324 : Tensor = prim::GetAttr[name="bias"](%1277)
       = prim::If(%1320) # torch/nn/functional.py:2011:4
        block0():
          %1325 : int[] = aten::size(%input.175) # torch/nn/functional.py:2012:27
          %size_prods.232 : int = aten::__getitem__(%1325, %24) # torch/nn/functional.py:1991:17
          %1327 : int = aten::len(%1325) # torch/nn/functional.py:1992:19
          %1328 : int = aten::sub(%1327, %26) # torch/nn/functional.py:1992:19
          %size_prods.233 : int = prim::Loop(%1328, %25, %size_prods.232) # torch/nn/functional.py:1992:4
            block0(%i.59 : int, %size_prods.234 : int):
              %1332 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
              %1333 : int = aten::__getitem__(%1325, %1332) # torch/nn/functional.py:1993:22
              %size_prods.235 : int = aten::mul(%size_prods.234, %1333) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.235)
          %1335 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1335) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.176 : Tensor = aten::batch_norm(%input.175, %1323, %1324, %1321, %1322, %1320, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.177 : Tensor = aten::relu_(%input.176) # torch/nn/functional.py:1117:17
      %1338 : Tensor = prim::GetAttr[name="weight"](%1278)
      %1339 : Tensor? = prim::GetAttr[name="bias"](%1278)
      %1340 : int[] = prim::ListConstruct(%27, %27)
      %1341 : int[] = prim::ListConstruct(%24, %24)
      %1342 : int[] = prim::ListConstruct(%27, %27)
      %input.178 : Tensor = aten::conv2d(%input.177, %1338, %1339, %1340, %1341, %1342, %27) # torch/nn/modules/conv.py:415:15
      %1344 : int = aten::dim(%input.178) # torch/nn/modules/batchnorm.py:276:11
      %1345 : bool = aten::ne(%1344, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1345) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1346 : bool = prim::GetAttr[name="training"](%1279)
       = prim::If(%1346) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1279)
          %1348 : Tensor = aten::add(%1347, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1279, %1348)
          -> ()
        block1():
          -> ()
      %1349 : bool = prim::GetAttr[name="training"](%1279)
      %1350 : Tensor = prim::GetAttr[name="running_mean"](%1279)
      %1351 : Tensor = prim::GetAttr[name="running_var"](%1279)
      %1352 : Tensor = prim::GetAttr[name="weight"](%1279)
      %1353 : Tensor = prim::GetAttr[name="bias"](%1279)
       = prim::If(%1349) # torch/nn/functional.py:2011:4
        block0():
          %1354 : int[] = aten::size(%input.178) # torch/nn/functional.py:2012:27
          %size_prods.236 : int = aten::__getitem__(%1354, %24) # torch/nn/functional.py:1991:17
          %1356 : int = aten::len(%1354) # torch/nn/functional.py:1992:19
          %1357 : int = aten::sub(%1356, %26) # torch/nn/functional.py:1992:19
          %size_prods.237 : int = prim::Loop(%1357, %25, %size_prods.236) # torch/nn/functional.py:1992:4
            block0(%i.60 : int, %size_prods.238 : int):
              %1361 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
              %1362 : int = aten::__getitem__(%1354, %1361) # torch/nn/functional.py:1993:22
              %size_prods.239 : int = aten::mul(%size_prods.238, %1362) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.239)
          %1364 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1364) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.179 : Tensor = aten::batch_norm(%input.178, %1352, %1353, %1350, %1351, %1349, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %1366 : Tensor = aten::add(%input.179, %input.244, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%1366)
    block1():
      %1367 : __torch__.torch.nn.modules.container.___torch_mangle_777.Sequential = prim::GetAttr[name="layers"](%1268)
      %1368 : __torch__.torch.nn.modules.conv.___torch_mangle_657.Conv2d = prim::GetAttr[name="0"](%1367)
      %1369 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_658.BatchNorm2d = prim::GetAttr[name="1"](%1367)
      %1370 : __torch__.torch.nn.modules.conv.___torch_mangle_774.Conv2d = prim::GetAttr[name="3"](%1367)
      %1371 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_658.BatchNorm2d = prim::GetAttr[name="4"](%1367)
      %1372 : __torch__.torch.nn.modules.conv.___torch_mangle_775.Conv2d = prim::GetAttr[name="6"](%1367)
      %1373 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1367)
      %1374 : Tensor = prim::GetAttr[name="weight"](%1368)
      %1375 : Tensor? = prim::GetAttr[name="bias"](%1368)
      %1376 : int[] = prim::ListConstruct(%27, %27)
      %1377 : int[] = prim::ListConstruct(%24, %24)
      %1378 : int[] = prim::ListConstruct(%27, %27)
      %input.180 : Tensor = aten::conv2d(%input.244, %1374, %1375, %1376, %1377, %1378, %27) # torch/nn/modules/conv.py:415:15
      %1380 : int = aten::dim(%input.180) # torch/nn/modules/batchnorm.py:276:11
      %1381 : bool = aten::ne(%1380, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1381) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1382 : bool = prim::GetAttr[name="training"](%1369)
       = prim::If(%1382) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1383 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1369)
          %1384 : Tensor = aten::add(%1383, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1369, %1384)
          -> ()
        block1():
          -> ()
      %1385 : bool = prim::GetAttr[name="training"](%1369)
      %1386 : Tensor = prim::GetAttr[name="running_mean"](%1369)
      %1387 : Tensor = prim::GetAttr[name="running_var"](%1369)
      %1388 : Tensor = prim::GetAttr[name="weight"](%1369)
      %1389 : Tensor = prim::GetAttr[name="bias"](%1369)
       = prim::If(%1385) # torch/nn/functional.py:2011:4
        block0():
          %1390 : int[] = aten::size(%input.180) # torch/nn/functional.py:2012:27
          %size_prods.240 : int = aten::__getitem__(%1390, %24) # torch/nn/functional.py:1991:17
          %1392 : int = aten::len(%1390) # torch/nn/functional.py:1992:19
          %1393 : int = aten::sub(%1392, %26) # torch/nn/functional.py:1992:19
          %size_prods.241 : int = prim::Loop(%1393, %25, %size_prods.240) # torch/nn/functional.py:1992:4
            block0(%i.61 : int, %size_prods.242 : int):
              %1397 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
              %1398 : int = aten::__getitem__(%1390, %1397) # torch/nn/functional.py:1993:22
              %size_prods.243 : int = aten::mul(%size_prods.242, %1398) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.243)
          %1400 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1400) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.181 : Tensor = aten::batch_norm(%input.180, %1388, %1389, %1386, %1387, %1385, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.182 : Tensor = aten::relu_(%input.181) # torch/nn/functional.py:1117:17
      %1403 : Tensor = prim::GetAttr[name="weight"](%1370)
      %1404 : Tensor? = prim::GetAttr[name="bias"](%1370)
      %1405 : int[] = prim::ListConstruct(%26, %26)
      %1406 : int[] = prim::ListConstruct(%26, %26)
      %1407 : int[] = prim::ListConstruct(%27, %27)
      %input.183 : Tensor = aten::conv2d(%input.182, %1403, %1404, %1405, %1406, %1407, %14) # torch/nn/modules/conv.py:415:15
      %1409 : int = aten::dim(%input.183) # torch/nn/modules/batchnorm.py:276:11
      %1410 : bool = aten::ne(%1409, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1410) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1411 : bool = prim::GetAttr[name="training"](%1371)
       = prim::If(%1411) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1412 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1371)
          %1413 : Tensor = aten::add(%1412, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1371, %1413)
          -> ()
        block1():
          -> ()
      %1414 : bool = prim::GetAttr[name="training"](%1371)
      %1415 : Tensor = prim::GetAttr[name="running_mean"](%1371)
      %1416 : Tensor = prim::GetAttr[name="running_var"](%1371)
      %1417 : Tensor = prim::GetAttr[name="weight"](%1371)
      %1418 : Tensor = prim::GetAttr[name="bias"](%1371)
       = prim::If(%1414) # torch/nn/functional.py:2011:4
        block0():
          %1419 : int[] = aten::size(%input.183) # torch/nn/functional.py:2012:27
          %size_prods.244 : int = aten::__getitem__(%1419, %24) # torch/nn/functional.py:1991:17
          %1421 : int = aten::len(%1419) # torch/nn/functional.py:1992:19
          %1422 : int = aten::sub(%1421, %26) # torch/nn/functional.py:1992:19
          %size_prods.245 : int = prim::Loop(%1422, %25, %size_prods.244) # torch/nn/functional.py:1992:4
            block0(%i.62 : int, %size_prods.246 : int):
              %1426 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
              %1427 : int = aten::__getitem__(%1419, %1426) # torch/nn/functional.py:1993:22
              %size_prods.247 : int = aten::mul(%size_prods.246, %1427) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.247)
          %1429 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1429) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.184 : Tensor = aten::batch_norm(%input.183, %1417, %1418, %1415, %1416, %1414, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.185 : Tensor = aten::relu_(%input.184) # torch/nn/functional.py:1117:17
      %1432 : Tensor = prim::GetAttr[name="weight"](%1372)
      %1433 : Tensor? = prim::GetAttr[name="bias"](%1372)
      %1434 : int[] = prim::ListConstruct(%27, %27)
      %1435 : int[] = prim::ListConstruct(%24, %24)
      %1436 : int[] = prim::ListConstruct(%27, %27)
      %input.186 : Tensor = aten::conv2d(%input.185, %1432, %1433, %1434, %1435, %1436, %27) # torch/nn/modules/conv.py:415:15
      %1438 : int = aten::dim(%input.186) # torch/nn/modules/batchnorm.py:276:11
      %1439 : bool = aten::ne(%1438, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1439) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1440 : bool = prim::GetAttr[name="training"](%1373)
       = prim::If(%1440) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1441 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1373)
          %1442 : Tensor = aten::add(%1441, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1373, %1442)
          -> ()
        block1():
          -> ()
      %1443 : bool = prim::GetAttr[name="training"](%1373)
      %1444 : Tensor = prim::GetAttr[name="running_mean"](%1373)
      %1445 : Tensor = prim::GetAttr[name="running_var"](%1373)
      %1446 : Tensor = prim::GetAttr[name="weight"](%1373)
      %1447 : Tensor = prim::GetAttr[name="bias"](%1373)
       = prim::If(%1443) # torch/nn/functional.py:2011:4
        block0():
          %1448 : int[] = aten::size(%input.186) # torch/nn/functional.py:2012:27
          %size_prods.248 : int = aten::__getitem__(%1448, %24) # torch/nn/functional.py:1991:17
          %1450 : int = aten::len(%1448) # torch/nn/functional.py:1992:19
          %1451 : int = aten::sub(%1450, %26) # torch/nn/functional.py:1992:19
          %size_prods.249 : int = prim::Loop(%1451, %25, %size_prods.248) # torch/nn/functional.py:1992:4
            block0(%i.63 : int, %size_prods.250 : int):
              %1455 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
              %1456 : int = aten::__getitem__(%1448, %1455) # torch/nn/functional.py:1993:22
              %size_prods.251 : int = aten::mul(%size_prods.250, %1456) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.251)
          %1458 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1458) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.218 : Tensor = aten::batch_norm(%input.186, %1446, %1447, %1444, %1445, %1443, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.218)
  %1460 : bool = prim::GetAttr[name="apply_residual"](%1269)
  %input.171 : Tensor = prim::If(%1460) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %1462 : __torch__.torch.nn.modules.container.___torch_mangle_783.Sequential = prim::GetAttr[name="layers"](%1269)
      %1463 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1462)
      %1464 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1462)
      %1465 : __torch__.torch.nn.modules.conv.___torch_mangle_781.Conv2d = prim::GetAttr[name="3"](%1462)
      %1466 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1462)
      %1467 : __torch__.torch.nn.modules.conv.___torch_mangle_782.Conv2d = prim::GetAttr[name="6"](%1462)
      %1468 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1462)
      %1469 : Tensor = prim::GetAttr[name="weight"](%1463)
      %1470 : Tensor? = prim::GetAttr[name="bias"](%1463)
      %1471 : int[] = prim::ListConstruct(%27, %27)
      %1472 : int[] = prim::ListConstruct(%24, %24)
      %1473 : int[] = prim::ListConstruct(%27, %27)
      %input.219 : Tensor = aten::conv2d(%input.187, %1469, %1470, %1471, %1472, %1473, %27) # torch/nn/modules/conv.py:415:15
      %1475 : int = aten::dim(%input.219) # torch/nn/modules/batchnorm.py:276:11
      %1476 : bool = aten::ne(%1475, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1476) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1477 : bool = prim::GetAttr[name="training"](%1464)
       = prim::If(%1477) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1478 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1464)
          %1479 : Tensor = aten::add(%1478, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1464, %1479)
          -> ()
        block1():
          -> ()
      %1480 : bool = prim::GetAttr[name="training"](%1464)
      %1481 : Tensor = prim::GetAttr[name="running_mean"](%1464)
      %1482 : Tensor = prim::GetAttr[name="running_var"](%1464)
      %1483 : Tensor = prim::GetAttr[name="weight"](%1464)
      %1484 : Tensor = prim::GetAttr[name="bias"](%1464)
       = prim::If(%1480) # torch/nn/functional.py:2011:4
        block0():
          %1485 : int[] = aten::size(%input.219) # torch/nn/functional.py:2012:27
          %size_prods.300 : int = aten::__getitem__(%1485, %24) # torch/nn/functional.py:1991:17
          %1487 : int = aten::len(%1485) # torch/nn/functional.py:1992:19
          %1488 : int = aten::sub(%1487, %26) # torch/nn/functional.py:1992:19
          %size_prods.301 : int = prim::Loop(%1488, %25, %size_prods.300) # torch/nn/functional.py:1992:4
            block0(%i.76 : int, %size_prods.302 : int):
              %1492 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
              %1493 : int = aten::__getitem__(%1485, %1492) # torch/nn/functional.py:1993:22
              %size_prods.303 : int = aten::mul(%size_prods.302, %1493) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.303)
          %1495 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1495) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.221 : Tensor = aten::batch_norm(%input.219, %1483, %1484, %1481, %1482, %1480, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.222 : Tensor = aten::relu_(%input.221) # torch/nn/functional.py:1117:17
      %1498 : Tensor = prim::GetAttr[name="weight"](%1465)
      %1499 : Tensor? = prim::GetAttr[name="bias"](%1465)
      %1500 : int[] = prim::ListConstruct(%27, %27)
      %1501 : int[] = prim::ListConstruct(%26, %26)
      %1502 : int[] = prim::ListConstruct(%27, %27)
      %input.223 : Tensor = aten::conv2d(%input.222, %1498, %1499, %1500, %1501, %1502, %15) # torch/nn/modules/conv.py:415:15
      %1504 : int = aten::dim(%input.223) # torch/nn/modules/batchnorm.py:276:11
      %1505 : bool = aten::ne(%1504, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1505) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1506 : bool = prim::GetAttr[name="training"](%1466)
       = prim::If(%1506) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1507 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1466)
          %1508 : Tensor = aten::add(%1507, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1466, %1508)
          -> ()
        block1():
          -> ()
      %1509 : bool = prim::GetAttr[name="training"](%1466)
      %1510 : Tensor = prim::GetAttr[name="running_mean"](%1466)
      %1511 : Tensor = prim::GetAttr[name="running_var"](%1466)
      %1512 : Tensor = prim::GetAttr[name="weight"](%1466)
      %1513 : Tensor = prim::GetAttr[name="bias"](%1466)
       = prim::If(%1509) # torch/nn/functional.py:2011:4
        block0():
          %1514 : int[] = aten::size(%input.223) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%1514, %24) # torch/nn/functional.py:1991:17
          %1516 : int = aten::len(%1514) # torch/nn/functional.py:1992:19
          %1517 : int = aten::sub(%1516, %26) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%1517, %25, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %1521 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
              %1522 : int = aten::__getitem__(%1514, %1521) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %1522) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.307)
          %1524 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1524) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.224 : Tensor = aten::batch_norm(%input.223, %1512, %1513, %1510, %1511, %1509, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.225 : Tensor = aten::relu_(%input.224) # torch/nn/functional.py:1117:17
      %1527 : Tensor = prim::GetAttr[name="weight"](%1467)
      %1528 : Tensor? = prim::GetAttr[name="bias"](%1467)
      %1529 : int[] = prim::ListConstruct(%27, %27)
      %1530 : int[] = prim::ListConstruct(%24, %24)
      %1531 : int[] = prim::ListConstruct(%27, %27)
      %input.226 : Tensor = aten::conv2d(%input.225, %1527, %1528, %1529, %1530, %1531, %27) # torch/nn/modules/conv.py:415:15
      %1533 : int = aten::dim(%input.226) # torch/nn/modules/batchnorm.py:276:11
      %1534 : bool = aten::ne(%1533, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1534) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1535 : bool = prim::GetAttr[name="training"](%1468)
       = prim::If(%1535) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1536 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1468)
          %1537 : Tensor = aten::add(%1536, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1468, %1537)
          -> ()
        block1():
          -> ()
      %1538 : bool = prim::GetAttr[name="training"](%1468)
      %1539 : Tensor = prim::GetAttr[name="running_mean"](%1468)
      %1540 : Tensor = prim::GetAttr[name="running_var"](%1468)
      %1541 : Tensor = prim::GetAttr[name="weight"](%1468)
      %1542 : Tensor = prim::GetAttr[name="bias"](%1468)
       = prim::If(%1538) # torch/nn/functional.py:2011:4
        block0():
          %1543 : int[] = aten::size(%input.226) # torch/nn/functional.py:2012:27
          %size_prods.308 : int = aten::__getitem__(%1543, %24) # torch/nn/functional.py:1991:17
          %1545 : int = aten::len(%1543) # torch/nn/functional.py:1992:19
          %1546 : int = aten::sub(%1545, %26) # torch/nn/functional.py:1992:19
          %size_prods.309 : int = prim::Loop(%1546, %25, %size_prods.308) # torch/nn/functional.py:1992:4
            block0(%i.78 : int, %size_prods.310 : int):
              %1550 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
              %1551 : int = aten::__getitem__(%1543, %1550) # torch/nn/functional.py:1993:22
              %size_prods.311 : int = aten::mul(%size_prods.310, %1551) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.311)
          %1553 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1553) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.227 : Tensor = aten::batch_norm(%input.226, %1541, %1542, %1539, %1540, %1538, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %1555 : Tensor = aten::add(%input.227, %input.187, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%1555)
    block1():
      %1556 : __torch__.torch.nn.modules.container.___torch_mangle_783.Sequential = prim::GetAttr[name="layers"](%1269)
      %1557 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1556)
      %1558 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1556)
      %1559 : __torch__.torch.nn.modules.conv.___torch_mangle_781.Conv2d = prim::GetAttr[name="3"](%1556)
      %1560 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1556)
      %1561 : __torch__.torch.nn.modules.conv.___torch_mangle_782.Conv2d = prim::GetAttr[name="6"](%1556)
      %1562 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1556)
      %1563 : Tensor = prim::GetAttr[name="weight"](%1557)
      %1564 : Tensor? = prim::GetAttr[name="bias"](%1557)
      %1565 : int[] = prim::ListConstruct(%27, %27)
      %1566 : int[] = prim::ListConstruct(%24, %24)
      %1567 : int[] = prim::ListConstruct(%27, %27)
      %input.228 : Tensor = aten::conv2d(%input.187, %1563, %1564, %1565, %1566, %1567, %27) # torch/nn/modules/conv.py:415:15
      %1569 : int = aten::dim(%input.228) # torch/nn/modules/batchnorm.py:276:11
      %1570 : bool = aten::ne(%1569, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1570) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1571 : bool = prim::GetAttr[name="training"](%1558)
       = prim::If(%1571) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1572 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1558)
          %1573 : Tensor = aten::add(%1572, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1558, %1573)
          -> ()
        block1():
          -> ()
      %1574 : bool = prim::GetAttr[name="training"](%1558)
      %1575 : Tensor = prim::GetAttr[name="running_mean"](%1558)
      %1576 : Tensor = prim::GetAttr[name="running_var"](%1558)
      %1577 : Tensor = prim::GetAttr[name="weight"](%1558)
      %1578 : Tensor = prim::GetAttr[name="bias"](%1558)
       = prim::If(%1574) # torch/nn/functional.py:2011:4
        block0():
          %1579 : int[] = aten::size(%input.228) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%1579, %24) # torch/nn/functional.py:1991:17
          %1581 : int = aten::len(%1579) # torch/nn/functional.py:1992:19
          %1582 : int = aten::sub(%1581, %26) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%1582, %25, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %1586 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
              %1587 : int = aten::__getitem__(%1579, %1586) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %1587) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.315)
          %1589 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1589) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.229 : Tensor = aten::batch_norm(%input.228, %1577, %1578, %1575, %1576, %1574, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.230 : Tensor = aten::relu_(%input.229) # torch/nn/functional.py:1117:17
      %1592 : Tensor = prim::GetAttr[name="weight"](%1559)
      %1593 : Tensor? = prim::GetAttr[name="bias"](%1559)
      %1594 : int[] = prim::ListConstruct(%27, %27)
      %1595 : int[] = prim::ListConstruct(%26, %26)
      %1596 : int[] = prim::ListConstruct(%27, %27)
      %input.231 : Tensor = aten::conv2d(%input.230, %1592, %1593, %1594, %1595, %1596, %15) # torch/nn/modules/conv.py:415:15
      %1598 : int = aten::dim(%input.231) # torch/nn/modules/batchnorm.py:276:11
      %1599 : bool = aten::ne(%1598, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1599) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1600 : bool = prim::GetAttr[name="training"](%1560)
       = prim::If(%1600) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1601 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1560)
          %1602 : Tensor = aten::add(%1601, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1560, %1602)
          -> ()
        block1():
          -> ()
      %1603 : bool = prim::GetAttr[name="training"](%1560)
      %1604 : Tensor = prim::GetAttr[name="running_mean"](%1560)
      %1605 : Tensor = prim::GetAttr[name="running_var"](%1560)
      %1606 : Tensor = prim::GetAttr[name="weight"](%1560)
      %1607 : Tensor = prim::GetAttr[name="bias"](%1560)
       = prim::If(%1603) # torch/nn/functional.py:2011:4
        block0():
          %1608 : int[] = aten::size(%input.231) # torch/nn/functional.py:2012:27
          %size_prods.316 : int = aten::__getitem__(%1608, %24) # torch/nn/functional.py:1991:17
          %1610 : int = aten::len(%1608) # torch/nn/functional.py:1992:19
          %1611 : int = aten::sub(%1610, %26) # torch/nn/functional.py:1992:19
          %size_prods.317 : int = prim::Loop(%1611, %25, %size_prods.316) # torch/nn/functional.py:1992:4
            block0(%i.80 : int, %size_prods.318 : int):
              %1615 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
              %1616 : int = aten::__getitem__(%1608, %1615) # torch/nn/functional.py:1993:22
              %size_prods.319 : int = aten::mul(%size_prods.318, %1616) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.319)
          %1618 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1618) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.232 : Tensor = aten::batch_norm(%input.231, %1606, %1607, %1604, %1605, %1603, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.233 : Tensor = aten::relu_(%input.232) # torch/nn/functional.py:1117:17
      %1621 : Tensor = prim::GetAttr[name="weight"](%1561)
      %1622 : Tensor? = prim::GetAttr[name="bias"](%1561)
      %1623 : int[] = prim::ListConstruct(%27, %27)
      %1624 : int[] = prim::ListConstruct(%24, %24)
      %1625 : int[] = prim::ListConstruct(%27, %27)
      %input.234 : Tensor = aten::conv2d(%input.233, %1621, %1622, %1623, %1624, %1625, %27) # torch/nn/modules/conv.py:415:15
      %1627 : int = aten::dim(%input.234) # torch/nn/modules/batchnorm.py:276:11
      %1628 : bool = aten::ne(%1627, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1628) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1629 : bool = prim::GetAttr[name="training"](%1562)
       = prim::If(%1629) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1630 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1562)
          %1631 : Tensor = aten::add(%1630, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1562, %1631)
          -> ()
        block1():
          -> ()
      %1632 : bool = prim::GetAttr[name="training"](%1562)
      %1633 : Tensor = prim::GetAttr[name="running_mean"](%1562)
      %1634 : Tensor = prim::GetAttr[name="running_var"](%1562)
      %1635 : Tensor = prim::GetAttr[name="weight"](%1562)
      %1636 : Tensor = prim::GetAttr[name="bias"](%1562)
       = prim::If(%1632) # torch/nn/functional.py:2011:4
        block0():
          %1637 : int[] = aten::size(%input.234) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%1637, %24) # torch/nn/functional.py:1991:17
          %1639 : int = aten::len(%1637) # torch/nn/functional.py:1992:19
          %1640 : int = aten::sub(%1639, %26) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%1640, %25, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %1644 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
              %1645 : int = aten::__getitem__(%1637, %1644) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %1645) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.323)
          %1647 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1647) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.235 : Tensor = aten::batch_norm(%input.234, %1635, %1636, %1633, %1634, %1632, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.235)
  %1649 : bool = prim::GetAttr[name="apply_residual"](%1270)
  %input.243 : Tensor = prim::If(%1649) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %1651 : __torch__.torch.nn.modules.container.___torch_mangle_783.Sequential = prim::GetAttr[name="layers"](%1270)
      %1652 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1651)
      %1653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1651)
      %1654 : __torch__.torch.nn.modules.conv.___torch_mangle_781.Conv2d = prim::GetAttr[name="3"](%1651)
      %1655 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1651)
      %1656 : __torch__.torch.nn.modules.conv.___torch_mangle_782.Conv2d = prim::GetAttr[name="6"](%1651)
      %1657 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1651)
      %1658 : Tensor = prim::GetAttr[name="weight"](%1652)
      %1659 : Tensor? = prim::GetAttr[name="bias"](%1652)
      %1660 : int[] = prim::ListConstruct(%27, %27)
      %1661 : int[] = prim::ListConstruct(%24, %24)
      %1662 : int[] = prim::ListConstruct(%27, %27)
      %input.188 : Tensor = aten::conv2d(%input.171, %1658, %1659, %1660, %1661, %1662, %27) # torch/nn/modules/conv.py:415:15
      %1664 : int = aten::dim(%input.188) # torch/nn/modules/batchnorm.py:276:11
      %1665 : bool = aten::ne(%1664, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1665) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1666 : bool = prim::GetAttr[name="training"](%1653)
       = prim::If(%1666) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1667 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1653)
          %1668 : Tensor = aten::add(%1667, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1653, %1668)
          -> ()
        block1():
          -> ()
      %1669 : bool = prim::GetAttr[name="training"](%1653)
      %1670 : Tensor = prim::GetAttr[name="running_mean"](%1653)
      %1671 : Tensor = prim::GetAttr[name="running_var"](%1653)
      %1672 : Tensor = prim::GetAttr[name="weight"](%1653)
      %1673 : Tensor = prim::GetAttr[name="bias"](%1653)
       = prim::If(%1669) # torch/nn/functional.py:2011:4
        block0():
          %1674 : int[] = aten::size(%input.188) # torch/nn/functional.py:2012:27
          %size_prods.252 : int = aten::__getitem__(%1674, %24) # torch/nn/functional.py:1991:17
          %1676 : int = aten::len(%1674) # torch/nn/functional.py:1992:19
          %1677 : int = aten::sub(%1676, %26) # torch/nn/functional.py:1992:19
          %size_prods.253 : int = prim::Loop(%1677, %25, %size_prods.252) # torch/nn/functional.py:1992:4
            block0(%i.64 : int, %size_prods.254 : int):
              %1681 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
              %1682 : int = aten::__getitem__(%1674, %1681) # torch/nn/functional.py:1993:22
              %size_prods.255 : int = aten::mul(%size_prods.254, %1682) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.255)
          %1684 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1684) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.189 : Tensor = aten::batch_norm(%input.188, %1672, %1673, %1670, %1671, %1669, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.190 : Tensor = aten::relu_(%input.189) # torch/nn/functional.py:1117:17
      %1687 : Tensor = prim::GetAttr[name="weight"](%1654)
      %1688 : Tensor? = prim::GetAttr[name="bias"](%1654)
      %1689 : int[] = prim::ListConstruct(%27, %27)
      %1690 : int[] = prim::ListConstruct(%26, %26)
      %1691 : int[] = prim::ListConstruct(%27, %27)
      %input.191 : Tensor = aten::conv2d(%input.190, %1687, %1688, %1689, %1690, %1691, %15) # torch/nn/modules/conv.py:415:15
      %1693 : int = aten::dim(%input.191) # torch/nn/modules/batchnorm.py:276:11
      %1694 : bool = aten::ne(%1693, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1694) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1695 : bool = prim::GetAttr[name="training"](%1655)
       = prim::If(%1695) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1696 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1655)
          %1697 : Tensor = aten::add(%1696, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1655, %1697)
          -> ()
        block1():
          -> ()
      %1698 : bool = prim::GetAttr[name="training"](%1655)
      %1699 : Tensor = prim::GetAttr[name="running_mean"](%1655)
      %1700 : Tensor = prim::GetAttr[name="running_var"](%1655)
      %1701 : Tensor = prim::GetAttr[name="weight"](%1655)
      %1702 : Tensor = prim::GetAttr[name="bias"](%1655)
       = prim::If(%1698) # torch/nn/functional.py:2011:4
        block0():
          %1703 : int[] = aten::size(%input.191) # torch/nn/functional.py:2012:27
          %size_prods.256 : int = aten::__getitem__(%1703, %24) # torch/nn/functional.py:1991:17
          %1705 : int = aten::len(%1703) # torch/nn/functional.py:1992:19
          %1706 : int = aten::sub(%1705, %26) # torch/nn/functional.py:1992:19
          %size_prods.257 : int = prim::Loop(%1706, %25, %size_prods.256) # torch/nn/functional.py:1992:4
            block0(%i.65 : int, %size_prods.258 : int):
              %1710 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
              %1711 : int = aten::__getitem__(%1703, %1710) # torch/nn/functional.py:1993:22
              %size_prods.259 : int = aten::mul(%size_prods.258, %1711) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.259)
          %1713 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1713) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.192 : Tensor = aten::batch_norm(%input.191, %1701, %1702, %1699, %1700, %1698, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.193 : Tensor = aten::relu_(%input.192) # torch/nn/functional.py:1117:17
      %1716 : Tensor = prim::GetAttr[name="weight"](%1656)
      %1717 : Tensor? = prim::GetAttr[name="bias"](%1656)
      %1718 : int[] = prim::ListConstruct(%27, %27)
      %1719 : int[] = prim::ListConstruct(%24, %24)
      %1720 : int[] = prim::ListConstruct(%27, %27)
      %input.194 : Tensor = aten::conv2d(%input.193, %1716, %1717, %1718, %1719, %1720, %27) # torch/nn/modules/conv.py:415:15
      %1722 : int = aten::dim(%input.194) # torch/nn/modules/batchnorm.py:276:11
      %1723 : bool = aten::ne(%1722, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1723) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1724 : bool = prim::GetAttr[name="training"](%1657)
       = prim::If(%1724) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1725 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1657)
          %1726 : Tensor = aten::add(%1725, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1657, %1726)
          -> ()
        block1():
          -> ()
      %1727 : bool = prim::GetAttr[name="training"](%1657)
      %1728 : Tensor = prim::GetAttr[name="running_mean"](%1657)
      %1729 : Tensor = prim::GetAttr[name="running_var"](%1657)
      %1730 : Tensor = prim::GetAttr[name="weight"](%1657)
      %1731 : Tensor = prim::GetAttr[name="bias"](%1657)
       = prim::If(%1727) # torch/nn/functional.py:2011:4
        block0():
          %1732 : int[] = aten::size(%input.194) # torch/nn/functional.py:2012:27
          %size_prods.260 : int = aten::__getitem__(%1732, %24) # torch/nn/functional.py:1991:17
          %1734 : int = aten::len(%1732) # torch/nn/functional.py:1992:19
          %1735 : int = aten::sub(%1734, %26) # torch/nn/functional.py:1992:19
          %size_prods.261 : int = prim::Loop(%1735, %25, %size_prods.260) # torch/nn/functional.py:1992:4
            block0(%i.66 : int, %size_prods.262 : int):
              %1739 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
              %1740 : int = aten::__getitem__(%1732, %1739) # torch/nn/functional.py:1993:22
              %size_prods.263 : int = aten::mul(%size_prods.262, %1740) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.263)
          %1742 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1742) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.195 : Tensor = aten::batch_norm(%input.194, %1730, %1731, %1728, %1729, %1727, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %1744 : Tensor = aten::add(%input.195, %input.171, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%1744)
    block1():
      %1745 : __torch__.torch.nn.modules.container.___torch_mangle_783.Sequential = prim::GetAttr[name="layers"](%1270)
      %1746 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1745)
      %1747 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1745)
      %1748 : __torch__.torch.nn.modules.conv.___torch_mangle_781.Conv2d = prim::GetAttr[name="3"](%1745)
      %1749 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1745)
      %1750 : __torch__.torch.nn.modules.conv.___torch_mangle_782.Conv2d = prim::GetAttr[name="6"](%1745)
      %1751 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_776.BatchNorm2d = prim::GetAttr[name="7"](%1745)
      %1752 : Tensor = prim::GetAttr[name="weight"](%1746)
      %1753 : Tensor? = prim::GetAttr[name="bias"](%1746)
      %1754 : int[] = prim::ListConstruct(%27, %27)
      %1755 : int[] = prim::ListConstruct(%24, %24)
      %1756 : int[] = prim::ListConstruct(%27, %27)
      %input.196 : Tensor = aten::conv2d(%input.171, %1752, %1753, %1754, %1755, %1756, %27) # torch/nn/modules/conv.py:415:15
      %1758 : int = aten::dim(%input.196) # torch/nn/modules/batchnorm.py:276:11
      %1759 : bool = aten::ne(%1758, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1759) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1760 : bool = prim::GetAttr[name="training"](%1747)
       = prim::If(%1760) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1761 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1747)
          %1762 : Tensor = aten::add(%1761, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1747, %1762)
          -> ()
        block1():
          -> ()
      %1763 : bool = prim::GetAttr[name="training"](%1747)
      %1764 : Tensor = prim::GetAttr[name="running_mean"](%1747)
      %1765 : Tensor = prim::GetAttr[name="running_var"](%1747)
      %1766 : Tensor = prim::GetAttr[name="weight"](%1747)
      %1767 : Tensor = prim::GetAttr[name="bias"](%1747)
       = prim::If(%1763) # torch/nn/functional.py:2011:4
        block0():
          %1768 : int[] = aten::size(%input.196) # torch/nn/functional.py:2012:27
          %size_prods.264 : int = aten::__getitem__(%1768, %24) # torch/nn/functional.py:1991:17
          %1770 : int = aten::len(%1768) # torch/nn/functional.py:1992:19
          %1771 : int = aten::sub(%1770, %26) # torch/nn/functional.py:1992:19
          %size_prods.265 : int = prim::Loop(%1771, %25, %size_prods.264) # torch/nn/functional.py:1992:4
            block0(%i.67 : int, %size_prods.266 : int):
              %1775 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
              %1776 : int = aten::__getitem__(%1768, %1775) # torch/nn/functional.py:1993:22
              %size_prods.267 : int = aten::mul(%size_prods.266, %1776) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.267)
          %1778 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1778) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.197 : Tensor = aten::batch_norm(%input.196, %1766, %1767, %1764, %1765, %1763, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.198 : Tensor = aten::relu_(%input.197) # torch/nn/functional.py:1117:17
      %1781 : Tensor = prim::GetAttr[name="weight"](%1748)
      %1782 : Tensor? = prim::GetAttr[name="bias"](%1748)
      %1783 : int[] = prim::ListConstruct(%27, %27)
      %1784 : int[] = prim::ListConstruct(%26, %26)
      %1785 : int[] = prim::ListConstruct(%27, %27)
      %input.199 : Tensor = aten::conv2d(%input.198, %1781, %1782, %1783, %1784, %1785, %15) # torch/nn/modules/conv.py:415:15
      %1787 : int = aten::dim(%input.199) # torch/nn/modules/batchnorm.py:276:11
      %1788 : bool = aten::ne(%1787, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1788) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1789 : bool = prim::GetAttr[name="training"](%1749)
       = prim::If(%1789) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1790 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1749)
          %1791 : Tensor = aten::add(%1790, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1749, %1791)
          -> ()
        block1():
          -> ()
      %1792 : bool = prim::GetAttr[name="training"](%1749)
      %1793 : Tensor = prim::GetAttr[name="running_mean"](%1749)
      %1794 : Tensor = prim::GetAttr[name="running_var"](%1749)
      %1795 : Tensor = prim::GetAttr[name="weight"](%1749)
      %1796 : Tensor = prim::GetAttr[name="bias"](%1749)
       = prim::If(%1792) # torch/nn/functional.py:2011:4
        block0():
          %1797 : int[] = aten::size(%input.199) # torch/nn/functional.py:2012:27
          %size_prods.268 : int = aten::__getitem__(%1797, %24) # torch/nn/functional.py:1991:17
          %1799 : int = aten::len(%1797) # torch/nn/functional.py:1992:19
          %1800 : int = aten::sub(%1799, %26) # torch/nn/functional.py:1992:19
          %size_prods.269 : int = prim::Loop(%1800, %25, %size_prods.268) # torch/nn/functional.py:1992:4
            block0(%i.68 : int, %size_prods.270 : int):
              %1804 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
              %1805 : int = aten::__getitem__(%1797, %1804) # torch/nn/functional.py:1993:22
              %size_prods.271 : int = aten::mul(%size_prods.270, %1805) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.271)
          %1807 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1807) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.200 : Tensor = aten::batch_norm(%input.199, %1795, %1796, %1793, %1794, %1792, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.201 : Tensor = aten::relu_(%input.200) # torch/nn/functional.py:1117:17
      %1810 : Tensor = prim::GetAttr[name="weight"](%1750)
      %1811 : Tensor? = prim::GetAttr[name="bias"](%1750)
      %1812 : int[] = prim::ListConstruct(%27, %27)
      %1813 : int[] = prim::ListConstruct(%24, %24)
      %1814 : int[] = prim::ListConstruct(%27, %27)
      %input.202 : Tensor = aten::conv2d(%input.201, %1810, %1811, %1812, %1813, %1814, %27) # torch/nn/modules/conv.py:415:15
      %1816 : int = aten::dim(%input.202) # torch/nn/modules/batchnorm.py:276:11
      %1817 : bool = aten::ne(%1816, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1817) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1818 : bool = prim::GetAttr[name="training"](%1751)
       = prim::If(%1818) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1819 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1751)
          %1820 : Tensor = aten::add(%1819, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1751, %1820)
          -> ()
        block1():
          -> ()
      %1821 : bool = prim::GetAttr[name="training"](%1751)
      %1822 : Tensor = prim::GetAttr[name="running_mean"](%1751)
      %1823 : Tensor = prim::GetAttr[name="running_var"](%1751)
      %1824 : Tensor = prim::GetAttr[name="weight"](%1751)
      %1825 : Tensor = prim::GetAttr[name="bias"](%1751)
       = prim::If(%1821) # torch/nn/functional.py:2011:4
        block0():
          %1826 : int[] = aten::size(%input.202) # torch/nn/functional.py:2012:27
          %size_prods.272 : int = aten::__getitem__(%1826, %24) # torch/nn/functional.py:1991:17
          %1828 : int = aten::len(%1826) # torch/nn/functional.py:1992:19
          %1829 : int = aten::sub(%1828, %26) # torch/nn/functional.py:1992:19
          %size_prods.273 : int = prim::Loop(%1829, %25, %size_prods.272) # torch/nn/functional.py:1992:4
            block0(%i.69 : int, %size_prods.274 : int):
              %1833 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
              %1834 : int = aten::__getitem__(%1826, %1833) # torch/nn/functional.py:1993:22
              %size_prods.275 : int = aten::mul(%size_prods.274, %1834) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.275)
          %1836 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1836) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.203 : Tensor = aten::batch_norm(%input.202, %1824, %1825, %1822, %1823, %1821, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.203)
  %1838 : __torch__.torchvision.models.mnasnet.___torch_mangle_788._InvertedResidual = prim::GetAttr[name="0"](%37)
  %1839 : __torch__.torchvision.models.mnasnet.___torch_mangle_790._InvertedResidual = prim::GetAttr[name="1"](%37)
  %1840 : bool = prim::GetAttr[name="apply_residual"](%1838)
  %input.220 : Tensor = prim::If(%1840) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %1842 : __torch__.torch.nn.modules.container.___torch_mangle_787.Sequential = prim::GetAttr[name="layers"](%1838)
      %1843 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1842)
      %1844 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1842)
      %1845 : __torch__.torch.nn.modules.conv.___torch_mangle_786.Conv2d = prim::GetAttr[name="3"](%1842)
      %1846 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1842)
      %1847 : __torch__.torch.nn.modules.conv.___torch_mangle_430.Conv2d = prim::GetAttr[name="6"](%1842)
      %1848 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_676.BatchNorm2d = prim::GetAttr[name="7"](%1842)
      %1849 : Tensor = prim::GetAttr[name="weight"](%1843)
      %1850 : Tensor? = prim::GetAttr[name="bias"](%1843)
      %1851 : int[] = prim::ListConstruct(%27, %27)
      %1852 : int[] = prim::ListConstruct(%24, %24)
      %1853 : int[] = prim::ListConstruct(%27, %27)
      %input.237 : Tensor = aten::conv2d(%input.243, %1849, %1850, %1851, %1852, %1853, %27) # torch/nn/modules/conv.py:415:15
      %1855 : int = aten::dim(%input.237) # torch/nn/modules/batchnorm.py:276:11
      %1856 : bool = aten::ne(%1855, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1856) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1857 : bool = prim::GetAttr[name="training"](%1844)
       = prim::If(%1857) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1858 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1844)
          %1859 : Tensor = aten::add(%1858, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1844, %1859)
          -> ()
        block1():
          -> ()
      %1860 : bool = prim::GetAttr[name="training"](%1844)
      %1861 : Tensor = prim::GetAttr[name="running_mean"](%1844)
      %1862 : Tensor = prim::GetAttr[name="running_var"](%1844)
      %1863 : Tensor = prim::GetAttr[name="weight"](%1844)
      %1864 : Tensor = prim::GetAttr[name="bias"](%1844)
       = prim::If(%1860) # torch/nn/functional.py:2011:4
        block0():
          %1865 : int[] = aten::size(%input.237) # torch/nn/functional.py:2012:27
          %size_prods.276 : int = aten::__getitem__(%1865, %24) # torch/nn/functional.py:1991:17
          %1867 : int = aten::len(%1865) # torch/nn/functional.py:1992:19
          %1868 : int = aten::sub(%1867, %26) # torch/nn/functional.py:1992:19
          %size_prods.277 : int = prim::Loop(%1868, %25, %size_prods.276) # torch/nn/functional.py:1992:4
            block0(%i.70 : int, %size_prods.278 : int):
              %1872 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
              %1873 : int = aten::__getitem__(%1865, %1872) # torch/nn/functional.py:1993:22
              %size_prods.279 : int = aten::mul(%size_prods.278, %1873) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.279)
          %1875 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1875) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.254 : Tensor = aten::batch_norm(%input.237, %1863, %1864, %1861, %1862, %1860, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.204 : Tensor = aten::relu_(%input.254) # torch/nn/functional.py:1117:17
      %1878 : Tensor = prim::GetAttr[name="weight"](%1845)
      %1879 : Tensor? = prim::GetAttr[name="bias"](%1845)
      %1880 : int[] = prim::ListConstruct(%27, %27)
      %1881 : int[] = prim::ListConstruct(%27, %27)
      %1882 : int[] = prim::ListConstruct(%27, %27)
      %input.205 : Tensor = aten::conv2d(%input.204, %1878, %1879, %1880, %1881, %1882, %15) # torch/nn/modules/conv.py:415:15
      %1884 : int = aten::dim(%input.205) # torch/nn/modules/batchnorm.py:276:11
      %1885 : bool = aten::ne(%1884, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1885) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1886 : bool = prim::GetAttr[name="training"](%1846)
       = prim::If(%1886) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1887 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1846)
          %1888 : Tensor = aten::add(%1887, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1846, %1888)
          -> ()
        block1():
          -> ()
      %1889 : bool = prim::GetAttr[name="training"](%1846)
      %1890 : Tensor = prim::GetAttr[name="running_mean"](%1846)
      %1891 : Tensor = prim::GetAttr[name="running_var"](%1846)
      %1892 : Tensor = prim::GetAttr[name="weight"](%1846)
      %1893 : Tensor = prim::GetAttr[name="bias"](%1846)
       = prim::If(%1889) # torch/nn/functional.py:2011:4
        block0():
          %1894 : int[] = aten::size(%input.205) # torch/nn/functional.py:2012:27
          %size_prods.280 : int = aten::__getitem__(%1894, %24) # torch/nn/functional.py:1991:17
          %1896 : int = aten::len(%1894) # torch/nn/functional.py:1992:19
          %1897 : int = aten::sub(%1896, %26) # torch/nn/functional.py:1992:19
          %size_prods.281 : int = prim::Loop(%1897, %25, %size_prods.280) # torch/nn/functional.py:1992:4
            block0(%i.71 : int, %size_prods.282 : int):
              %1901 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
              %1902 : int = aten::__getitem__(%1894, %1901) # torch/nn/functional.py:1993:22
              %size_prods.283 : int = aten::mul(%size_prods.282, %1902) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.283)
          %1904 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1904) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.206 : Tensor = aten::batch_norm(%input.205, %1892, %1893, %1890, %1891, %1889, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.207 : Tensor = aten::relu_(%input.206) # torch/nn/functional.py:1117:17
      %1907 : Tensor = prim::GetAttr[name="weight"](%1847)
      %1908 : Tensor? = prim::GetAttr[name="bias"](%1847)
      %1909 : int[] = prim::ListConstruct(%27, %27)
      %1910 : int[] = prim::ListConstruct(%24, %24)
      %1911 : int[] = prim::ListConstruct(%27, %27)
      %input.208 : Tensor = aten::conv2d(%input.207, %1907, %1908, %1909, %1910, %1911, %27) # torch/nn/modules/conv.py:415:15
      %1913 : int = aten::dim(%input.208) # torch/nn/modules/batchnorm.py:276:11
      %1914 : bool = aten::ne(%1913, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1914) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1915 : bool = prim::GetAttr[name="training"](%1848)
       = prim::If(%1915) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1916 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1848)
          %1917 : Tensor = aten::add(%1916, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1848, %1917)
          -> ()
        block1():
          -> ()
      %1918 : bool = prim::GetAttr[name="training"](%1848)
      %1919 : Tensor = prim::GetAttr[name="running_mean"](%1848)
      %1920 : Tensor = prim::GetAttr[name="running_var"](%1848)
      %1921 : Tensor = prim::GetAttr[name="weight"](%1848)
      %1922 : Tensor = prim::GetAttr[name="bias"](%1848)
       = prim::If(%1918) # torch/nn/functional.py:2011:4
        block0():
          %1923 : int[] = aten::size(%input.208) # torch/nn/functional.py:2012:27
          %size_prods.284 : int = aten::__getitem__(%1923, %24) # torch/nn/functional.py:1991:17
          %1925 : int = aten::len(%1923) # torch/nn/functional.py:1992:19
          %1926 : int = aten::sub(%1925, %26) # torch/nn/functional.py:1992:19
          %size_prods.285 : int = prim::Loop(%1926, %25, %size_prods.284) # torch/nn/functional.py:1992:4
            block0(%i.72 : int, %size_prods.286 : int):
              %1930 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
              %1931 : int = aten::__getitem__(%1923, %1930) # torch/nn/functional.py:1993:22
              %size_prods.287 : int = aten::mul(%size_prods.286, %1931) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.287)
          %1933 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1933) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.209 : Tensor = aten::batch_norm(%input.208, %1921, %1922, %1919, %1920, %1918, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %1935 : Tensor = aten::add(%input.209, %input.243, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%1935)
    block1():
      %1936 : __torch__.torch.nn.modules.container.___torch_mangle_787.Sequential = prim::GetAttr[name="layers"](%1838)
      %1937 : __torch__.torch.nn.modules.conv.___torch_mangle_779.Conv2d = prim::GetAttr[name="0"](%1936)
      %1938 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="1"](%1936)
      %1939 : __torch__.torch.nn.modules.conv.___torch_mangle_786.Conv2d = prim::GetAttr[name="3"](%1936)
      %1940 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_780.BatchNorm2d = prim::GetAttr[name="4"](%1936)
      %1941 : __torch__.torch.nn.modules.conv.___torch_mangle_430.Conv2d = prim::GetAttr[name="6"](%1936)
      %1942 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_676.BatchNorm2d = prim::GetAttr[name="7"](%1936)
      %1943 : Tensor = prim::GetAttr[name="weight"](%1937)
      %1944 : Tensor? = prim::GetAttr[name="bias"](%1937)
      %1945 : int[] = prim::ListConstruct(%27, %27)
      %1946 : int[] = prim::ListConstruct(%24, %24)
      %1947 : int[] = prim::ListConstruct(%27, %27)
      %input.210 : Tensor = aten::conv2d(%input.243, %1943, %1944, %1945, %1946, %1947, %27) # torch/nn/modules/conv.py:415:15
      %1949 : int = aten::dim(%input.210) # torch/nn/modules/batchnorm.py:276:11
      %1950 : bool = aten::ne(%1949, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1950) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1951 : bool = prim::GetAttr[name="training"](%1938)
       = prim::If(%1951) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1952 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1938)
          %1953 : Tensor = aten::add(%1952, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1938, %1953)
          -> ()
        block1():
          -> ()
      %1954 : bool = prim::GetAttr[name="training"](%1938)
      %1955 : Tensor = prim::GetAttr[name="running_mean"](%1938)
      %1956 : Tensor = prim::GetAttr[name="running_var"](%1938)
      %1957 : Tensor = prim::GetAttr[name="weight"](%1938)
      %1958 : Tensor = prim::GetAttr[name="bias"](%1938)
       = prim::If(%1954) # torch/nn/functional.py:2011:4
        block0():
          %1959 : int[] = aten::size(%input.210) # torch/nn/functional.py:2012:27
          %size_prods.288 : int = aten::__getitem__(%1959, %24) # torch/nn/functional.py:1991:17
          %1961 : int = aten::len(%1959) # torch/nn/functional.py:1992:19
          %1962 : int = aten::sub(%1961, %26) # torch/nn/functional.py:1992:19
          %size_prods.289 : int = prim::Loop(%1962, %25, %size_prods.288) # torch/nn/functional.py:1992:4
            block0(%i.73 : int, %size_prods.290 : int):
              %1966 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
              %1967 : int = aten::__getitem__(%1959, %1966) # torch/nn/functional.py:1993:22
              %size_prods.291 : int = aten::mul(%size_prods.290, %1967) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.291)
          %1969 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1969) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.211 : Tensor = aten::batch_norm(%input.210, %1957, %1958, %1955, %1956, %1954, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.212 : Tensor = aten::relu_(%input.211) # torch/nn/functional.py:1117:17
      %1972 : Tensor = prim::GetAttr[name="weight"](%1939)
      %1973 : Tensor? = prim::GetAttr[name="bias"](%1939)
      %1974 : int[] = prim::ListConstruct(%27, %27)
      %1975 : int[] = prim::ListConstruct(%27, %27)
      %1976 : int[] = prim::ListConstruct(%27, %27)
      %input.213 : Tensor = aten::conv2d(%input.212, %1972, %1973, %1974, %1975, %1976, %15) # torch/nn/modules/conv.py:415:15
      %1978 : int = aten::dim(%input.213) # torch/nn/modules/batchnorm.py:276:11
      %1979 : bool = aten::ne(%1978, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1979) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1980 : bool = prim::GetAttr[name="training"](%1940)
       = prim::If(%1980) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1981 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1940)
          %1982 : Tensor = aten::add(%1981, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1940, %1982)
          -> ()
        block1():
          -> ()
      %1983 : bool = prim::GetAttr[name="training"](%1940)
      %1984 : Tensor = prim::GetAttr[name="running_mean"](%1940)
      %1985 : Tensor = prim::GetAttr[name="running_var"](%1940)
      %1986 : Tensor = prim::GetAttr[name="weight"](%1940)
      %1987 : Tensor = prim::GetAttr[name="bias"](%1940)
       = prim::If(%1983) # torch/nn/functional.py:2011:4
        block0():
          %1988 : int[] = aten::size(%input.213) # torch/nn/functional.py:2012:27
          %size_prods.292 : int = aten::__getitem__(%1988, %24) # torch/nn/functional.py:1991:17
          %1990 : int = aten::len(%1988) # torch/nn/functional.py:1992:19
          %1991 : int = aten::sub(%1990, %26) # torch/nn/functional.py:1992:19
          %size_prods.293 : int = prim::Loop(%1991, %25, %size_prods.292) # torch/nn/functional.py:1992:4
            block0(%i.74 : int, %size_prods.294 : int):
              %1995 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
              %1996 : int = aten::__getitem__(%1988, %1995) # torch/nn/functional.py:1993:22
              %size_prods.295 : int = aten::mul(%size_prods.294, %1996) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.295)
          %1998 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1998) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.214 : Tensor = aten::batch_norm(%input.213, %1986, %1987, %1984, %1985, %1983, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.215 : Tensor = aten::relu_(%input.214) # torch/nn/functional.py:1117:17
      %2001 : Tensor = prim::GetAttr[name="weight"](%1941)
      %2002 : Tensor? = prim::GetAttr[name="bias"](%1941)
      %2003 : int[] = prim::ListConstruct(%27, %27)
      %2004 : int[] = prim::ListConstruct(%24, %24)
      %2005 : int[] = prim::ListConstruct(%27, %27)
      %input.216 : Tensor = aten::conv2d(%input.215, %2001, %2002, %2003, %2004, %2005, %27) # torch/nn/modules/conv.py:415:15
      %2007 : int = aten::dim(%input.216) # torch/nn/modules/batchnorm.py:276:11
      %2008 : bool = aten::ne(%2007, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2008) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2009 : bool = prim::GetAttr[name="training"](%1942)
       = prim::If(%2009) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2010 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1942)
          %2011 : Tensor = aten::add(%2010, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1942, %2011)
          -> ()
        block1():
          -> ()
      %2012 : bool = prim::GetAttr[name="training"](%1942)
      %2013 : Tensor = prim::GetAttr[name="running_mean"](%1942)
      %2014 : Tensor = prim::GetAttr[name="running_var"](%1942)
      %2015 : Tensor = prim::GetAttr[name="weight"](%1942)
      %2016 : Tensor = prim::GetAttr[name="bias"](%1942)
       = prim::If(%2012) # torch/nn/functional.py:2011:4
        block0():
          %2017 : int[] = aten::size(%input.216) # torch/nn/functional.py:2012:27
          %size_prods.296 : int = aten::__getitem__(%2017, %24) # torch/nn/functional.py:1991:17
          %2019 : int = aten::len(%2017) # torch/nn/functional.py:1992:19
          %2020 : int = aten::sub(%2019, %26) # torch/nn/functional.py:1992:19
          %size_prods.297 : int = prim::Loop(%2020, %25, %size_prods.296) # torch/nn/functional.py:1992:4
            block0(%i.75 : int, %size_prods.298 : int):
              %2024 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
              %2025 : int = aten::__getitem__(%2017, %2024) # torch/nn/functional.py:1993:22
              %size_prods.299 : int = aten::mul(%size_prods.298, %2025) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.299)
          %2027 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2027) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.217 : Tensor = aten::batch_norm(%input.216, %2015, %2016, %2013, %2014, %2012, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.217)
  %2029 : bool = prim::GetAttr[name="apply_residual"](%1839)
  %input.240 : Tensor = prim::If(%2029) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2031 : __torch__.torch.nn.modules.container.___torch_mangle_789.Sequential = prim::GetAttr[name="layers"](%1839)
      %2032 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2031)
      %2033 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="1"](%2031)
      %2034 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="3"](%2031)
      %2035 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="4"](%2031)
      %2036 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="6"](%2031)
      %2037 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_676.BatchNorm2d = prim::GetAttr[name="7"](%2031)
      %2038 : Tensor = prim::GetAttr[name="weight"](%2032)
      %2039 : Tensor? = prim::GetAttr[name="bias"](%2032)
      %2040 : int[] = prim::ListConstruct(%27, %27)
      %2041 : int[] = prim::ListConstruct(%24, %24)
      %2042 : int[] = prim::ListConstruct(%27, %27)
      %input.255 : Tensor = aten::conv2d(%input.220, %2038, %2039, %2040, %2041, %2042, %27) # torch/nn/modules/conv.py:415:15
      %2044 : int = aten::dim(%input.255) # torch/nn/modules/batchnorm.py:276:11
      %2045 : bool = aten::ne(%2044, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2045) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2046 : bool = prim::GetAttr[name="training"](%2033)
       = prim::If(%2046) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2047 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2033)
          %2048 : Tensor = aten::add(%2047, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2033, %2048)
          -> ()
        block1():
          -> ()
      %2049 : bool = prim::GetAttr[name="training"](%2033)
      %2050 : Tensor = prim::GetAttr[name="running_mean"](%2033)
      %2051 : Tensor = prim::GetAttr[name="running_var"](%2033)
      %2052 : Tensor = prim::GetAttr[name="weight"](%2033)
      %2053 : Tensor = prim::GetAttr[name="bias"](%2033)
       = prim::If(%2049) # torch/nn/functional.py:2011:4
        block0():
          %2054 : int[] = aten::size(%input.255) # torch/nn/functional.py:2012:27
          %size_prods.348 : int = aten::__getitem__(%2054, %24) # torch/nn/functional.py:1991:17
          %2056 : int = aten::len(%2054) # torch/nn/functional.py:1992:19
          %2057 : int = aten::sub(%2056, %26) # torch/nn/functional.py:1992:19
          %size_prods.349 : int = prim::Loop(%2057, %25, %size_prods.348) # torch/nn/functional.py:1992:4
            block0(%i.88 : int, %size_prods.350 : int):
              %2061 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
              %2062 : int = aten::__getitem__(%2054, %2061) # torch/nn/functional.py:1993:22
              %size_prods.351 : int = aten::mul(%size_prods.350, %2062) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.351)
          %2064 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2064) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.256 : Tensor = aten::batch_norm(%input.255, %2052, %2053, %2050, %2051, %2049, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.257 : Tensor = aten::relu_(%input.256) # torch/nn/functional.py:1117:17
      %2067 : Tensor = prim::GetAttr[name="weight"](%2034)
      %2068 : Tensor? = prim::GetAttr[name="bias"](%2034)
      %2069 : int[] = prim::ListConstruct(%27, %27)
      %2070 : int[] = prim::ListConstruct(%27, %27)
      %2071 : int[] = prim::ListConstruct(%27, %27)
      %input.258 : Tensor = aten::conv2d(%input.257, %2067, %2068, %2069, %2070, %2071, %13) # torch/nn/modules/conv.py:415:15
      %2073 : int = aten::dim(%input.258) # torch/nn/modules/batchnorm.py:276:11
      %2074 : bool = aten::ne(%2073, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2074) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2075 : bool = prim::GetAttr[name="training"](%2035)
       = prim::If(%2075) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2076 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2035)
          %2077 : Tensor = aten::add(%2076, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2035, %2077)
          -> ()
        block1():
          -> ()
      %2078 : bool = prim::GetAttr[name="training"](%2035)
      %2079 : Tensor = prim::GetAttr[name="running_mean"](%2035)
      %2080 : Tensor = prim::GetAttr[name="running_var"](%2035)
      %2081 : Tensor = prim::GetAttr[name="weight"](%2035)
      %2082 : Tensor = prim::GetAttr[name="bias"](%2035)
       = prim::If(%2078) # torch/nn/functional.py:2011:4
        block0():
          %2083 : int[] = aten::size(%input.258) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%2083, %24) # torch/nn/functional.py:1991:17
          %2085 : int = aten::len(%2083) # torch/nn/functional.py:1992:19
          %2086 : int = aten::sub(%2085, %26) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%2086, %25, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %2090 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
              %2091 : int = aten::__getitem__(%2083, %2090) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %2091) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.355)
          %2093 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2093) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.259 : Tensor = aten::batch_norm(%input.258, %2081, %2082, %2079, %2080, %2078, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.260 : Tensor = aten::relu_(%input.259) # torch/nn/functional.py:1117:17
      %2096 : Tensor = prim::GetAttr[name="weight"](%2036)
      %2097 : Tensor? = prim::GetAttr[name="bias"](%2036)
      %2098 : int[] = prim::ListConstruct(%27, %27)
      %2099 : int[] = prim::ListConstruct(%24, %24)
      %2100 : int[] = prim::ListConstruct(%27, %27)
      %input.261 : Tensor = aten::conv2d(%input.260, %2096, %2097, %2098, %2099, %2100, %27) # torch/nn/modules/conv.py:415:15
      %2102 : int = aten::dim(%input.261) # torch/nn/modules/batchnorm.py:276:11
      %2103 : bool = aten::ne(%2102, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2103) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2104 : bool = prim::GetAttr[name="training"](%2037)
       = prim::If(%2104) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2105 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2037)
          %2106 : Tensor = aten::add(%2105, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2037, %2106)
          -> ()
        block1():
          -> ()
      %2107 : bool = prim::GetAttr[name="training"](%2037)
      %2108 : Tensor = prim::GetAttr[name="running_mean"](%2037)
      %2109 : Tensor = prim::GetAttr[name="running_var"](%2037)
      %2110 : Tensor = prim::GetAttr[name="weight"](%2037)
      %2111 : Tensor = prim::GetAttr[name="bias"](%2037)
       = prim::If(%2107) # torch/nn/functional.py:2011:4
        block0():
          %2112 : int[] = aten::size(%input.261) # torch/nn/functional.py:2012:27
          %size_prods.356 : int = aten::__getitem__(%2112, %24) # torch/nn/functional.py:1991:17
          %2114 : int = aten::len(%2112) # torch/nn/functional.py:1992:19
          %2115 : int = aten::sub(%2114, %26) # torch/nn/functional.py:1992:19
          %size_prods.357 : int = prim::Loop(%2115, %25, %size_prods.356) # torch/nn/functional.py:1992:4
            block0(%i.90 : int, %size_prods.358 : int):
              %2119 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
              %2120 : int = aten::__getitem__(%2112, %2119) # torch/nn/functional.py:1993:22
              %size_prods.359 : int = aten::mul(%size_prods.358, %2120) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.359)
          %2122 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2122) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.262 : Tensor = aten::batch_norm(%input.261, %2110, %2111, %2108, %2109, %2107, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %2124 : Tensor = aten::add(%input.262, %input.220, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%2124)
    block1():
      %2125 : __torch__.torch.nn.modules.container.___torch_mangle_789.Sequential = prim::GetAttr[name="layers"](%1839)
      %2126 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2125)
      %2127 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="1"](%2125)
      %2128 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="3"](%2125)
      %2129 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="4"](%2125)
      %2130 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="6"](%2125)
      %2131 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_676.BatchNorm2d = prim::GetAttr[name="7"](%2125)
      %2132 : Tensor = prim::GetAttr[name="weight"](%2126)
      %2133 : Tensor? = prim::GetAttr[name="bias"](%2126)
      %2134 : int[] = prim::ListConstruct(%27, %27)
      %2135 : int[] = prim::ListConstruct(%24, %24)
      %2136 : int[] = prim::ListConstruct(%27, %27)
      %input.263 : Tensor = aten::conv2d(%input.220, %2132, %2133, %2134, %2135, %2136, %27) # torch/nn/modules/conv.py:415:15
      %2138 : int = aten::dim(%input.263) # torch/nn/modules/batchnorm.py:276:11
      %2139 : bool = aten::ne(%2138, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2139) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2140 : bool = prim::GetAttr[name="training"](%2127)
       = prim::If(%2140) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2141 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2127)
          %2142 : Tensor = aten::add(%2141, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2127, %2142)
          -> ()
        block1():
          -> ()
      %2143 : bool = prim::GetAttr[name="training"](%2127)
      %2144 : Tensor = prim::GetAttr[name="running_mean"](%2127)
      %2145 : Tensor = prim::GetAttr[name="running_var"](%2127)
      %2146 : Tensor = prim::GetAttr[name="weight"](%2127)
      %2147 : Tensor = prim::GetAttr[name="bias"](%2127)
       = prim::If(%2143) # torch/nn/functional.py:2011:4
        block0():
          %2148 : int[] = aten::size(%input.263) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%2148, %24) # torch/nn/functional.py:1991:17
          %2150 : int = aten::len(%2148) # torch/nn/functional.py:1992:19
          %2151 : int = aten::sub(%2150, %26) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%2151, %25, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %2155 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
              %2156 : int = aten::__getitem__(%2148, %2155) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %2156) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.363)
          %2158 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2158) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.264 : Tensor = aten::batch_norm(%input.263, %2146, %2147, %2144, %2145, %2143, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.265 : Tensor = aten::relu_(%input.264) # torch/nn/functional.py:1117:17
      %2161 : Tensor = prim::GetAttr[name="weight"](%2128)
      %2162 : Tensor? = prim::GetAttr[name="bias"](%2128)
      %2163 : int[] = prim::ListConstruct(%27, %27)
      %2164 : int[] = prim::ListConstruct(%27, %27)
      %2165 : int[] = prim::ListConstruct(%27, %27)
      %input.266 : Tensor = aten::conv2d(%input.265, %2161, %2162, %2163, %2164, %2165, %13) # torch/nn/modules/conv.py:415:15
      %2167 : int = aten::dim(%input.266) # torch/nn/modules/batchnorm.py:276:11
      %2168 : bool = aten::ne(%2167, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2168) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2169 : bool = prim::GetAttr[name="training"](%2129)
       = prim::If(%2169) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2170 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2129)
          %2171 : Tensor = aten::add(%2170, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2129, %2171)
          -> ()
        block1():
          -> ()
      %2172 : bool = prim::GetAttr[name="training"](%2129)
      %2173 : Tensor = prim::GetAttr[name="running_mean"](%2129)
      %2174 : Tensor = prim::GetAttr[name="running_var"](%2129)
      %2175 : Tensor = prim::GetAttr[name="weight"](%2129)
      %2176 : Tensor = prim::GetAttr[name="bias"](%2129)
       = prim::If(%2172) # torch/nn/functional.py:2011:4
        block0():
          %2177 : int[] = aten::size(%input.266) # torch/nn/functional.py:2012:27
          %size_prods.364 : int = aten::__getitem__(%2177, %24) # torch/nn/functional.py:1991:17
          %2179 : int = aten::len(%2177) # torch/nn/functional.py:1992:19
          %2180 : int = aten::sub(%2179, %26) # torch/nn/functional.py:1992:19
          %size_prods.365 : int = prim::Loop(%2180, %25, %size_prods.364) # torch/nn/functional.py:1992:4
            block0(%i.92 : int, %size_prods.366 : int):
              %2184 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
              %2185 : int = aten::__getitem__(%2177, %2184) # torch/nn/functional.py:1993:22
              %size_prods.367 : int = aten::mul(%size_prods.366, %2185) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.367)
          %2187 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2187) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.267 : Tensor = aten::batch_norm(%input.266, %2175, %2176, %2173, %2174, %2172, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.268 : Tensor = aten::relu_(%input.267) # torch/nn/functional.py:1117:17
      %2190 : Tensor = prim::GetAttr[name="weight"](%2130)
      %2191 : Tensor? = prim::GetAttr[name="bias"](%2130)
      %2192 : int[] = prim::ListConstruct(%27, %27)
      %2193 : int[] = prim::ListConstruct(%24, %24)
      %2194 : int[] = prim::ListConstruct(%27, %27)
      %input.269 : Tensor = aten::conv2d(%input.268, %2190, %2191, %2192, %2193, %2194, %27) # torch/nn/modules/conv.py:415:15
      %2196 : int = aten::dim(%input.269) # torch/nn/modules/batchnorm.py:276:11
      %2197 : bool = aten::ne(%2196, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2197) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2198 : bool = prim::GetAttr[name="training"](%2131)
       = prim::If(%2198) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2199 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2131)
          %2200 : Tensor = aten::add(%2199, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2131, %2200)
          -> ()
        block1():
          -> ()
      %2201 : bool = prim::GetAttr[name="training"](%2131)
      %2202 : Tensor = prim::GetAttr[name="running_mean"](%2131)
      %2203 : Tensor = prim::GetAttr[name="running_var"](%2131)
      %2204 : Tensor = prim::GetAttr[name="weight"](%2131)
      %2205 : Tensor = prim::GetAttr[name="bias"](%2131)
       = prim::If(%2201) # torch/nn/functional.py:2011:4
        block0():
          %2206 : int[] = aten::size(%input.269) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%2206, %24) # torch/nn/functional.py:1991:17
          %2208 : int = aten::len(%2206) # torch/nn/functional.py:1992:19
          %2209 : int = aten::sub(%2208, %26) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%2209, %25, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %2213 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
              %2214 : int = aten::__getitem__(%2206, %2213) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %2214) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.371)
          %2216 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2216) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.270 : Tensor = aten::batch_norm(%input.269, %2204, %2205, %2202, %2203, %2201, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.270)
  %2218 : __torch__.torchvision.models.mnasnet.___torch_mangle_794._InvertedResidual = prim::GetAttr[name="0"](%38)
  %2219 : __torch__.torchvision.models.mnasnet.___torch_mangle_799._InvertedResidual = prim::GetAttr[name="1"](%38)
  %2220 : __torch__.torchvision.models.mnasnet.___torch_mangle_799._InvertedResidual = prim::GetAttr[name="2"](%38)
  %2221 : __torch__.torchvision.models.mnasnet.___torch_mangle_799._InvertedResidual = prim::GetAttr[name="3"](%38)
  %2222 : bool = prim::GetAttr[name="apply_residual"](%2218)
  %input.22 : Tensor = prim::If(%2222) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2224 : __torch__.torch.nn.modules.container.___torch_mangle_793.Sequential = prim::GetAttr[name="layers"](%2218)
      %2225 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2224)
      %2226 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="1"](%2224)
      %2227 : __torch__.torch.nn.modules.conv.___torch_mangle_792.Conv2d = prim::GetAttr[name="3"](%2224)
      %2228 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="4"](%2224)
      %2229 : __torch__.torch.nn.modules.conv.___torch_mangle_191.Conv2d = prim::GetAttr[name="6"](%2224)
      %2230 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2224)
      %2231 : Tensor = prim::GetAttr[name="weight"](%2225)
      %2232 : Tensor? = prim::GetAttr[name="bias"](%2225)
      %2233 : int[] = prim::ListConstruct(%27, %27)
      %2234 : int[] = prim::ListConstruct(%24, %24)
      %2235 : int[] = prim::ListConstruct(%27, %27)
      %input.23 : Tensor = aten::conv2d(%input.240, %2231, %2232, %2233, %2234, %2235, %27) # torch/nn/modules/conv.py:415:15
      %2237 : int = aten::dim(%input.23) # torch/nn/modules/batchnorm.py:276:11
      %2238 : bool = aten::ne(%2237, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2238) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2239 : bool = prim::GetAttr[name="training"](%2226)
       = prim::If(%2239) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2240 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2226)
          %2241 : Tensor = aten::add(%2240, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2226, %2241)
          -> ()
        block1():
          -> ()
      %2242 : bool = prim::GetAttr[name="training"](%2226)
      %2243 : Tensor = prim::GetAttr[name="running_mean"](%2226)
      %2244 : Tensor = prim::GetAttr[name="running_var"](%2226)
      %2245 : Tensor = prim::GetAttr[name="weight"](%2226)
      %2246 : Tensor = prim::GetAttr[name="bias"](%2226)
       = prim::If(%2242) # torch/nn/functional.py:2011:4
        block0():
          %2247 : int[] = aten::size(%input.23) # torch/nn/functional.py:2012:27
          %size_prods.24 : int = aten::__getitem__(%2247, %24) # torch/nn/functional.py:1991:17
          %2249 : int = aten::len(%2247) # torch/nn/functional.py:1992:19
          %2250 : int = aten::sub(%2249, %26) # torch/nn/functional.py:1992:19
          %size_prods.25 : int = prim::Loop(%2250, %25, %size_prods.24) # torch/nn/functional.py:1992:4
            block0(%i.7 : int, %size_prods.26 : int):
              %2254 : int = aten::add(%i.7, %26) # torch/nn/functional.py:1993:27
              %2255 : int = aten::__getitem__(%2247, %2254) # torch/nn/functional.py:1993:22
              %size_prods.27 : int = aten::mul(%size_prods.26, %2255) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.27)
          %2257 : bool = aten::eq(%size_prods.25, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2257) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.24 : Tensor = aten::batch_norm(%input.23, %2245, %2246, %2243, %2244, %2242, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.25 : Tensor = aten::relu_(%input.24) # torch/nn/functional.py:1117:17
      %2260 : Tensor = prim::GetAttr[name="weight"](%2227)
      %2261 : Tensor? = prim::GetAttr[name="bias"](%2227)
      %2262 : int[] = prim::ListConstruct(%26, %26)
      %2263 : int[] = prim::ListConstruct(%26, %26)
      %2264 : int[] = prim::ListConstruct(%27, %27)
      %input.26 : Tensor = aten::conv2d(%input.25, %2260, %2261, %2262, %2263, %2264, %13) # torch/nn/modules/conv.py:415:15
      %2266 : int = aten::dim(%input.26) # torch/nn/modules/batchnorm.py:276:11
      %2267 : bool = aten::ne(%2266, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2267) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2268 : bool = prim::GetAttr[name="training"](%2228)
       = prim::If(%2268) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2269 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2228)
          %2270 : Tensor = aten::add(%2269, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2228, %2270)
          -> ()
        block1():
          -> ()
      %2271 : bool = prim::GetAttr[name="training"](%2228)
      %2272 : Tensor = prim::GetAttr[name="running_mean"](%2228)
      %2273 : Tensor = prim::GetAttr[name="running_var"](%2228)
      %2274 : Tensor = prim::GetAttr[name="weight"](%2228)
      %2275 : Tensor = prim::GetAttr[name="bias"](%2228)
       = prim::If(%2271) # torch/nn/functional.py:2011:4
        block0():
          %2276 : int[] = aten::size(%input.26) # torch/nn/functional.py:2012:27
          %size_prods.28 : int = aten::__getitem__(%2276, %24) # torch/nn/functional.py:1991:17
          %2278 : int = aten::len(%2276) # torch/nn/functional.py:1992:19
          %2279 : int = aten::sub(%2278, %26) # torch/nn/functional.py:1992:19
          %size_prods.29 : int = prim::Loop(%2279, %25, %size_prods.28) # torch/nn/functional.py:1992:4
            block0(%i.8 : int, %size_prods.30 : int):
              %2283 : int = aten::add(%i.8, %26) # torch/nn/functional.py:1993:27
              %2284 : int = aten::__getitem__(%2276, %2283) # torch/nn/functional.py:1993:22
              %size_prods.31 : int = aten::mul(%size_prods.30, %2284) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.31)
          %2286 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2286) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.27 : Tensor = aten::batch_norm(%input.26, %2274, %2275, %2272, %2273, %2271, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.28 : Tensor = aten::relu_(%input.27) # torch/nn/functional.py:1117:17
      %2289 : Tensor = prim::GetAttr[name="weight"](%2229)
      %2290 : Tensor? = prim::GetAttr[name="bias"](%2229)
      %2291 : int[] = prim::ListConstruct(%27, %27)
      %2292 : int[] = prim::ListConstruct(%24, %24)
      %2293 : int[] = prim::ListConstruct(%27, %27)
      %input.29 : Tensor = aten::conv2d(%input.28, %2289, %2290, %2291, %2292, %2293, %27) # torch/nn/modules/conv.py:415:15
      %2295 : int = aten::dim(%input.29) # torch/nn/modules/batchnorm.py:276:11
      %2296 : bool = aten::ne(%2295, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2296) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2297 : bool = prim::GetAttr[name="training"](%2230)
       = prim::If(%2297) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2230)
          %2299 : Tensor = aten::add(%2298, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2230, %2299)
          -> ()
        block1():
          -> ()
      %2300 : bool = prim::GetAttr[name="training"](%2230)
      %2301 : Tensor = prim::GetAttr[name="running_mean"](%2230)
      %2302 : Tensor = prim::GetAttr[name="running_var"](%2230)
      %2303 : Tensor = prim::GetAttr[name="weight"](%2230)
      %2304 : Tensor = prim::GetAttr[name="bias"](%2230)
       = prim::If(%2300) # torch/nn/functional.py:2011:4
        block0():
          %2305 : int[] = aten::size(%input.29) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%2305, %24) # torch/nn/functional.py:1991:17
          %2307 : int = aten::len(%2305) # torch/nn/functional.py:1992:19
          %2308 : int = aten::sub(%2307, %26) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%2308, %25, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %2312 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
              %2313 : int = aten::__getitem__(%2305, %2312) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %2313) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.35)
          %2315 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2315) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.30 : Tensor = aten::batch_norm(%input.29, %2303, %2304, %2301, %2302, %2300, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %2317 : Tensor = aten::add(%input.30, %input.240, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%2317)
    block1():
      %2318 : __torch__.torch.nn.modules.container.___torch_mangle_793.Sequential = prim::GetAttr[name="layers"](%2218)
      %2319 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2318)
      %2320 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="1"](%2318)
      %2321 : __torch__.torch.nn.modules.conv.___torch_mangle_792.Conv2d = prim::GetAttr[name="3"](%2318)
      %2322 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_680.BatchNorm2d = prim::GetAttr[name="4"](%2318)
      %2323 : __torch__.torch.nn.modules.conv.___torch_mangle_191.Conv2d = prim::GetAttr[name="6"](%2318)
      %2324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2318)
      %2325 : Tensor = prim::GetAttr[name="weight"](%2319)
      %2326 : Tensor? = prim::GetAttr[name="bias"](%2319)
      %2327 : int[] = prim::ListConstruct(%27, %27)
      %2328 : int[] = prim::ListConstruct(%24, %24)
      %2329 : int[] = prim::ListConstruct(%27, %27)
      %input.31 : Tensor = aten::conv2d(%input.240, %2325, %2326, %2327, %2328, %2329, %27) # torch/nn/modules/conv.py:415:15
      %2331 : int = aten::dim(%input.31) # torch/nn/modules/batchnorm.py:276:11
      %2332 : bool = aten::ne(%2331, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2332) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2333 : bool = prim::GetAttr[name="training"](%2320)
       = prim::If(%2333) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2334 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2320)
          %2335 : Tensor = aten::add(%2334, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2320, %2335)
          -> ()
        block1():
          -> ()
      %2336 : bool = prim::GetAttr[name="training"](%2320)
      %2337 : Tensor = prim::GetAttr[name="running_mean"](%2320)
      %2338 : Tensor = prim::GetAttr[name="running_var"](%2320)
      %2339 : Tensor = prim::GetAttr[name="weight"](%2320)
      %2340 : Tensor = prim::GetAttr[name="bias"](%2320)
       = prim::If(%2336) # torch/nn/functional.py:2011:4
        block0():
          %2341 : int[] = aten::size(%input.31) # torch/nn/functional.py:2012:27
          %size_prods.36 : int = aten::__getitem__(%2341, %24) # torch/nn/functional.py:1991:17
          %2343 : int = aten::len(%2341) # torch/nn/functional.py:1992:19
          %2344 : int = aten::sub(%2343, %26) # torch/nn/functional.py:1992:19
          %size_prods.37 : int = prim::Loop(%2344, %25, %size_prods.36) # torch/nn/functional.py:1992:4
            block0(%i.10 : int, %size_prods.38 : int):
              %2348 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
              %2349 : int = aten::__getitem__(%2341, %2348) # torch/nn/functional.py:1993:22
              %size_prods.39 : int = aten::mul(%size_prods.38, %2349) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.39)
          %2351 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2351) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.32 : Tensor = aten::batch_norm(%input.31, %2339, %2340, %2337, %2338, %2336, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.33 : Tensor = aten::relu_(%input.32) # torch/nn/functional.py:1117:17
      %2354 : Tensor = prim::GetAttr[name="weight"](%2321)
      %2355 : Tensor? = prim::GetAttr[name="bias"](%2321)
      %2356 : int[] = prim::ListConstruct(%26, %26)
      %2357 : int[] = prim::ListConstruct(%26, %26)
      %2358 : int[] = prim::ListConstruct(%27, %27)
      %input.34 : Tensor = aten::conv2d(%input.33, %2354, %2355, %2356, %2357, %2358, %13) # torch/nn/modules/conv.py:415:15
      %2360 : int = aten::dim(%input.34) # torch/nn/modules/batchnorm.py:276:11
      %2361 : bool = aten::ne(%2360, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2361) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2362 : bool = prim::GetAttr[name="training"](%2322)
       = prim::If(%2362) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2363 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2322)
          %2364 : Tensor = aten::add(%2363, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2322, %2364)
          -> ()
        block1():
          -> ()
      %2365 : bool = prim::GetAttr[name="training"](%2322)
      %2366 : Tensor = prim::GetAttr[name="running_mean"](%2322)
      %2367 : Tensor = prim::GetAttr[name="running_var"](%2322)
      %2368 : Tensor = prim::GetAttr[name="weight"](%2322)
      %2369 : Tensor = prim::GetAttr[name="bias"](%2322)
       = prim::If(%2365) # torch/nn/functional.py:2011:4
        block0():
          %2370 : int[] = aten::size(%input.34) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%2370, %24) # torch/nn/functional.py:1991:17
          %2372 : int = aten::len(%2370) # torch/nn/functional.py:1992:19
          %2373 : int = aten::sub(%2372, %26) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%2373, %25, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %2377 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
              %2378 : int = aten::__getitem__(%2370, %2377) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %2378) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.43)
          %2380 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2380) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.35 : Tensor = aten::batch_norm(%input.34, %2368, %2369, %2366, %2367, %2365, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.36 : Tensor = aten::relu_(%input.35) # torch/nn/functional.py:1117:17
      %2383 : Tensor = prim::GetAttr[name="weight"](%2323)
      %2384 : Tensor? = prim::GetAttr[name="bias"](%2323)
      %2385 : int[] = prim::ListConstruct(%27, %27)
      %2386 : int[] = prim::ListConstruct(%24, %24)
      %2387 : int[] = prim::ListConstruct(%27, %27)
      %input.37 : Tensor = aten::conv2d(%input.36, %2383, %2384, %2385, %2386, %2387, %27) # torch/nn/modules/conv.py:415:15
      %2389 : int = aten::dim(%input.37) # torch/nn/modules/batchnorm.py:276:11
      %2390 : bool = aten::ne(%2389, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2390) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2391 : bool = prim::GetAttr[name="training"](%2324)
       = prim::If(%2391) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2324)
          %2393 : Tensor = aten::add(%2392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2324, %2393)
          -> ()
        block1():
          -> ()
      %2394 : bool = prim::GetAttr[name="training"](%2324)
      %2395 : Tensor = prim::GetAttr[name="running_mean"](%2324)
      %2396 : Tensor = prim::GetAttr[name="running_var"](%2324)
      %2397 : Tensor = prim::GetAttr[name="weight"](%2324)
      %2398 : Tensor = prim::GetAttr[name="bias"](%2324)
       = prim::If(%2394) # torch/nn/functional.py:2011:4
        block0():
          %2399 : int[] = aten::size(%input.37) # torch/nn/functional.py:2012:27
          %size_prods.44 : int = aten::__getitem__(%2399, %24) # torch/nn/functional.py:1991:17
          %2401 : int = aten::len(%2399) # torch/nn/functional.py:1992:19
          %2402 : int = aten::sub(%2401, %26) # torch/nn/functional.py:1992:19
          %size_prods.45 : int = prim::Loop(%2402, %25, %size_prods.44) # torch/nn/functional.py:1992:4
            block0(%i.12 : int, %size_prods.46 : int):
              %2406 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
              %2407 : int = aten::__getitem__(%2399, %2406) # torch/nn/functional.py:1993:22
              %size_prods.47 : int = aten::mul(%size_prods.46, %2407) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.47)
          %2409 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2409) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.38 : Tensor = aten::batch_norm(%input.37, %2397, %2398, %2395, %2396, %2394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.38)
  %2411 : bool = prim::GetAttr[name="apply_residual"](%2219)
  %input.20 : Tensor = prim::If(%2411) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2413 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2219)
      %2414 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2413)
      %2415 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2413)
      %2416 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2413)
      %2417 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2413)
      %2418 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2413)
      %2419 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2413)
      %2420 : Tensor = prim::GetAttr[name="weight"](%2414)
      %2421 : Tensor? = prim::GetAttr[name="bias"](%2414)
      %2422 : int[] = prim::ListConstruct(%27, %27)
      %2423 : int[] = prim::ListConstruct(%24, %24)
      %2424 : int[] = prim::ListConstruct(%27, %27)
      %input.39 : Tensor = aten::conv2d(%input.22, %2420, %2421, %2422, %2423, %2424, %27) # torch/nn/modules/conv.py:415:15
      %2426 : int = aten::dim(%input.39) # torch/nn/modules/batchnorm.py:276:11
      %2427 : bool = aten::ne(%2426, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2427) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2428 : bool = prim::GetAttr[name="training"](%2415)
       = prim::If(%2428) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2429 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2415)
          %2430 : Tensor = aten::add(%2429, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2415, %2430)
          -> ()
        block1():
          -> ()
      %2431 : bool = prim::GetAttr[name="training"](%2415)
      %2432 : Tensor = prim::GetAttr[name="running_mean"](%2415)
      %2433 : Tensor = prim::GetAttr[name="running_var"](%2415)
      %2434 : Tensor = prim::GetAttr[name="weight"](%2415)
      %2435 : Tensor = prim::GetAttr[name="bias"](%2415)
       = prim::If(%2431) # torch/nn/functional.py:2011:4
        block0():
          %2436 : int[] = aten::size(%input.39) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%2436, %24) # torch/nn/functional.py:1991:17
          %2438 : int = aten::len(%2436) # torch/nn/functional.py:1992:19
          %2439 : int = aten::sub(%2438, %26) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%2439, %25, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %2443 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
              %2444 : int = aten::__getitem__(%2436, %2443) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %2444) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.51)
          %2446 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2446) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.40 : Tensor = aten::batch_norm(%input.39, %2434, %2435, %2432, %2433, %2431, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.41 : Tensor = aten::relu_(%input.40) # torch/nn/functional.py:1117:17
      %2449 : Tensor = prim::GetAttr[name="weight"](%2416)
      %2450 : Tensor? = prim::GetAttr[name="bias"](%2416)
      %2451 : int[] = prim::ListConstruct(%27, %27)
      %2452 : int[] = prim::ListConstruct(%26, %26)
      %2453 : int[] = prim::ListConstruct(%27, %27)
      %input.42 : Tensor = aten::conv2d(%input.41, %2449, %2450, %2451, %2452, %2453, %12) # torch/nn/modules/conv.py:415:15
      %2455 : int = aten::dim(%input.42) # torch/nn/modules/batchnorm.py:276:11
      %2456 : bool = aten::ne(%2455, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2456) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2457 : bool = prim::GetAttr[name="training"](%2417)
       = prim::If(%2457) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2458 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2417)
          %2459 : Tensor = aten::add(%2458, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2417, %2459)
          -> ()
        block1():
          -> ()
      %2460 : bool = prim::GetAttr[name="training"](%2417)
      %2461 : Tensor = prim::GetAttr[name="running_mean"](%2417)
      %2462 : Tensor = prim::GetAttr[name="running_var"](%2417)
      %2463 : Tensor = prim::GetAttr[name="weight"](%2417)
      %2464 : Tensor = prim::GetAttr[name="bias"](%2417)
       = prim::If(%2460) # torch/nn/functional.py:2011:4
        block0():
          %2465 : int[] = aten::size(%input.42) # torch/nn/functional.py:2012:27
          %size_prods.52 : int = aten::__getitem__(%2465, %24) # torch/nn/functional.py:1991:17
          %2467 : int = aten::len(%2465) # torch/nn/functional.py:1992:19
          %2468 : int = aten::sub(%2467, %26) # torch/nn/functional.py:1992:19
          %size_prods.53 : int = prim::Loop(%2468, %25, %size_prods.52) # torch/nn/functional.py:1992:4
            block0(%i.14 : int, %size_prods.54 : int):
              %2472 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
              %2473 : int = aten::__getitem__(%2465, %2472) # torch/nn/functional.py:1993:22
              %size_prods.55 : int = aten::mul(%size_prods.54, %2473) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.55)
          %2475 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2475) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.43 : Tensor = aten::batch_norm(%input.42, %2463, %2464, %2461, %2462, %2460, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.44 : Tensor = aten::relu_(%input.43) # torch/nn/functional.py:1117:17
      %2478 : Tensor = prim::GetAttr[name="weight"](%2418)
      %2479 : Tensor? = prim::GetAttr[name="bias"](%2418)
      %2480 : int[] = prim::ListConstruct(%27, %27)
      %2481 : int[] = prim::ListConstruct(%24, %24)
      %2482 : int[] = prim::ListConstruct(%27, %27)
      %input.45 : Tensor = aten::conv2d(%input.44, %2478, %2479, %2480, %2481, %2482, %27) # torch/nn/modules/conv.py:415:15
      %2484 : int = aten::dim(%input.45) # torch/nn/modules/batchnorm.py:276:11
      %2485 : bool = aten::ne(%2484, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2485) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2486 : bool = prim::GetAttr[name="training"](%2419)
       = prim::If(%2486) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2419)
          %2488 : Tensor = aten::add(%2487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2419, %2488)
          -> ()
        block1():
          -> ()
      %2489 : bool = prim::GetAttr[name="training"](%2419)
      %2490 : Tensor = prim::GetAttr[name="running_mean"](%2419)
      %2491 : Tensor = prim::GetAttr[name="running_var"](%2419)
      %2492 : Tensor = prim::GetAttr[name="weight"](%2419)
      %2493 : Tensor = prim::GetAttr[name="bias"](%2419)
       = prim::If(%2489) # torch/nn/functional.py:2011:4
        block0():
          %2494 : int[] = aten::size(%input.45) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%2494, %24) # torch/nn/functional.py:1991:17
          %2496 : int = aten::len(%2494) # torch/nn/functional.py:1992:19
          %2497 : int = aten::sub(%2496, %26) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%2497, %25, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %2501 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
              %2502 : int = aten::__getitem__(%2494, %2501) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %2502) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.59)
          %2504 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2504) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.46 : Tensor = aten::batch_norm(%input.45, %2492, %2493, %2490, %2491, %2489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %2506 : Tensor = aten::add(%input.46, %input.22, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%2506)
    block1():
      %2507 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2219)
      %2508 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2507)
      %2509 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2507)
      %2510 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2507)
      %2511 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2507)
      %2512 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2507)
      %2513 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2507)
      %2514 : Tensor = prim::GetAttr[name="weight"](%2508)
      %2515 : Tensor? = prim::GetAttr[name="bias"](%2508)
      %2516 : int[] = prim::ListConstruct(%27, %27)
      %2517 : int[] = prim::ListConstruct(%24, %24)
      %2518 : int[] = prim::ListConstruct(%27, %27)
      %input.47 : Tensor = aten::conv2d(%input.22, %2514, %2515, %2516, %2517, %2518, %27) # torch/nn/modules/conv.py:415:15
      %2520 : int = aten::dim(%input.47) # torch/nn/modules/batchnorm.py:276:11
      %2521 : bool = aten::ne(%2520, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2521) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2522 : bool = prim::GetAttr[name="training"](%2509)
       = prim::If(%2522) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2523 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2509)
          %2524 : Tensor = aten::add(%2523, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2509, %2524)
          -> ()
        block1():
          -> ()
      %2525 : bool = prim::GetAttr[name="training"](%2509)
      %2526 : Tensor = prim::GetAttr[name="running_mean"](%2509)
      %2527 : Tensor = prim::GetAttr[name="running_var"](%2509)
      %2528 : Tensor = prim::GetAttr[name="weight"](%2509)
      %2529 : Tensor = prim::GetAttr[name="bias"](%2509)
       = prim::If(%2525) # torch/nn/functional.py:2011:4
        block0():
          %2530 : int[] = aten::size(%input.47) # torch/nn/functional.py:2012:27
          %size_prods.60 : int = aten::__getitem__(%2530, %24) # torch/nn/functional.py:1991:17
          %2532 : int = aten::len(%2530) # torch/nn/functional.py:1992:19
          %2533 : int = aten::sub(%2532, %26) # torch/nn/functional.py:1992:19
          %size_prods.61 : int = prim::Loop(%2533, %25, %size_prods.60) # torch/nn/functional.py:1992:4
            block0(%i.16 : int, %size_prods.62 : int):
              %2537 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
              %2538 : int = aten::__getitem__(%2530, %2537) # torch/nn/functional.py:1993:22
              %size_prods.63 : int = aten::mul(%size_prods.62, %2538) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.63)
          %2540 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2540) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.48 : Tensor = aten::batch_norm(%input.47, %2528, %2529, %2526, %2527, %2525, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.49 : Tensor = aten::relu_(%input.48) # torch/nn/functional.py:1117:17
      %2543 : Tensor = prim::GetAttr[name="weight"](%2510)
      %2544 : Tensor? = prim::GetAttr[name="bias"](%2510)
      %2545 : int[] = prim::ListConstruct(%27, %27)
      %2546 : int[] = prim::ListConstruct(%26, %26)
      %2547 : int[] = prim::ListConstruct(%27, %27)
      %input.50 : Tensor = aten::conv2d(%input.49, %2543, %2544, %2545, %2546, %2547, %12) # torch/nn/modules/conv.py:415:15
      %2549 : int = aten::dim(%input.50) # torch/nn/modules/batchnorm.py:276:11
      %2550 : bool = aten::ne(%2549, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2550) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2551 : bool = prim::GetAttr[name="training"](%2511)
       = prim::If(%2551) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2552 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2511)
          %2553 : Tensor = aten::add(%2552, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2511, %2553)
          -> ()
        block1():
          -> ()
      %2554 : bool = prim::GetAttr[name="training"](%2511)
      %2555 : Tensor = prim::GetAttr[name="running_mean"](%2511)
      %2556 : Tensor = prim::GetAttr[name="running_var"](%2511)
      %2557 : Tensor = prim::GetAttr[name="weight"](%2511)
      %2558 : Tensor = prim::GetAttr[name="bias"](%2511)
       = prim::If(%2554) # torch/nn/functional.py:2011:4
        block0():
          %2559 : int[] = aten::size(%input.50) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%2559, %24) # torch/nn/functional.py:1991:17
          %2561 : int = aten::len(%2559) # torch/nn/functional.py:1992:19
          %2562 : int = aten::sub(%2561, %26) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%2562, %25, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %2566 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
              %2567 : int = aten::__getitem__(%2559, %2566) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %2567) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.67)
          %2569 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2569) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.51 : Tensor = aten::batch_norm(%input.50, %2557, %2558, %2555, %2556, %2554, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.52 : Tensor = aten::relu_(%input.51) # torch/nn/functional.py:1117:17
      %2572 : Tensor = prim::GetAttr[name="weight"](%2512)
      %2573 : Tensor? = prim::GetAttr[name="bias"](%2512)
      %2574 : int[] = prim::ListConstruct(%27, %27)
      %2575 : int[] = prim::ListConstruct(%24, %24)
      %2576 : int[] = prim::ListConstruct(%27, %27)
      %input.53 : Tensor = aten::conv2d(%input.52, %2572, %2573, %2574, %2575, %2576, %27) # torch/nn/modules/conv.py:415:15
      %2578 : int = aten::dim(%input.53) # torch/nn/modules/batchnorm.py:276:11
      %2579 : bool = aten::ne(%2578, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2579) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2580 : bool = prim::GetAttr[name="training"](%2513)
       = prim::If(%2580) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2581 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2513)
          %2582 : Tensor = aten::add(%2581, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2513, %2582)
          -> ()
        block1():
          -> ()
      %2583 : bool = prim::GetAttr[name="training"](%2513)
      %2584 : Tensor = prim::GetAttr[name="running_mean"](%2513)
      %2585 : Tensor = prim::GetAttr[name="running_var"](%2513)
      %2586 : Tensor = prim::GetAttr[name="weight"](%2513)
      %2587 : Tensor = prim::GetAttr[name="bias"](%2513)
       = prim::If(%2583) # torch/nn/functional.py:2011:4
        block0():
          %2588 : int[] = aten::size(%input.53) # torch/nn/functional.py:2012:27
          %size_prods.68 : int = aten::__getitem__(%2588, %24) # torch/nn/functional.py:1991:17
          %2590 : int = aten::len(%2588) # torch/nn/functional.py:1992:19
          %2591 : int = aten::sub(%2590, %26) # torch/nn/functional.py:1992:19
          %size_prods.69 : int = prim::Loop(%2591, %25, %size_prods.68) # torch/nn/functional.py:1992:4
            block0(%i.18 : int, %size_prods.70 : int):
              %2595 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
              %2596 : int = aten::__getitem__(%2588, %2595) # torch/nn/functional.py:1993:22
              %size_prods.71 : int = aten::mul(%size_prods.70, %2596) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.71)
          %2598 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2598) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.54 : Tensor = aten::batch_norm(%input.53, %2586, %2587, %2584, %2585, %2583, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.54)
  %2600 : bool = prim::GetAttr[name="apply_residual"](%2220)
  %input.21 : Tensor = prim::If(%2600) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2602 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2220)
      %2603 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2602)
      %2604 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2602)
      %2605 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2602)
      %2606 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2602)
      %2607 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2602)
      %2608 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2602)
      %2609 : Tensor = prim::GetAttr[name="weight"](%2603)
      %2610 : Tensor? = prim::GetAttr[name="bias"](%2603)
      %2611 : int[] = prim::ListConstruct(%27, %27)
      %2612 : int[] = prim::ListConstruct(%24, %24)
      %2613 : int[] = prim::ListConstruct(%27, %27)
      %input.55 : Tensor = aten::conv2d(%input.20, %2609, %2610, %2611, %2612, %2613, %27) # torch/nn/modules/conv.py:415:15
      %2615 : int = aten::dim(%input.55) # torch/nn/modules/batchnorm.py:276:11
      %2616 : bool = aten::ne(%2615, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2616) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2617 : bool = prim::GetAttr[name="training"](%2604)
       = prim::If(%2617) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2618 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2604)
          %2619 : Tensor = aten::add(%2618, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2604, %2619)
          -> ()
        block1():
          -> ()
      %2620 : bool = prim::GetAttr[name="training"](%2604)
      %2621 : Tensor = prim::GetAttr[name="running_mean"](%2604)
      %2622 : Tensor = prim::GetAttr[name="running_var"](%2604)
      %2623 : Tensor = prim::GetAttr[name="weight"](%2604)
      %2624 : Tensor = prim::GetAttr[name="bias"](%2604)
       = prim::If(%2620) # torch/nn/functional.py:2011:4
        block0():
          %2625 : int[] = aten::size(%input.55) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%2625, %24) # torch/nn/functional.py:1991:17
          %2627 : int = aten::len(%2625) # torch/nn/functional.py:1992:19
          %2628 : int = aten::sub(%2627, %26) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%2628, %25, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %2632 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
              %2633 : int = aten::__getitem__(%2625, %2632) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %2633) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.75)
          %2635 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2635) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.56 : Tensor = aten::batch_norm(%input.55, %2623, %2624, %2621, %2622, %2620, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.57 : Tensor = aten::relu_(%input.56) # torch/nn/functional.py:1117:17
      %2638 : Tensor = prim::GetAttr[name="weight"](%2605)
      %2639 : Tensor? = prim::GetAttr[name="bias"](%2605)
      %2640 : int[] = prim::ListConstruct(%27, %27)
      %2641 : int[] = prim::ListConstruct(%26, %26)
      %2642 : int[] = prim::ListConstruct(%27, %27)
      %input.58 : Tensor = aten::conv2d(%input.57, %2638, %2639, %2640, %2641, %2642, %12) # torch/nn/modules/conv.py:415:15
      %2644 : int = aten::dim(%input.58) # torch/nn/modules/batchnorm.py:276:11
      %2645 : bool = aten::ne(%2644, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2645) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2646 : bool = prim::GetAttr[name="training"](%2606)
       = prim::If(%2646) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2647 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2606)
          %2648 : Tensor = aten::add(%2647, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2606, %2648)
          -> ()
        block1():
          -> ()
      %2649 : bool = prim::GetAttr[name="training"](%2606)
      %2650 : Tensor = prim::GetAttr[name="running_mean"](%2606)
      %2651 : Tensor = prim::GetAttr[name="running_var"](%2606)
      %2652 : Tensor = prim::GetAttr[name="weight"](%2606)
      %2653 : Tensor = prim::GetAttr[name="bias"](%2606)
       = prim::If(%2649) # torch/nn/functional.py:2011:4
        block0():
          %2654 : int[] = aten::size(%input.58) # torch/nn/functional.py:2012:27
          %size_prods.76 : int = aten::__getitem__(%2654, %24) # torch/nn/functional.py:1991:17
          %2656 : int = aten::len(%2654) # torch/nn/functional.py:1992:19
          %2657 : int = aten::sub(%2656, %26) # torch/nn/functional.py:1992:19
          %size_prods.77 : int = prim::Loop(%2657, %25, %size_prods.76) # torch/nn/functional.py:1992:4
            block0(%i.20 : int, %size_prods.78 : int):
              %2661 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
              %2662 : int = aten::__getitem__(%2654, %2661) # torch/nn/functional.py:1993:22
              %size_prods.79 : int = aten::mul(%size_prods.78, %2662) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.79)
          %2664 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2664) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.59 : Tensor = aten::batch_norm(%input.58, %2652, %2653, %2650, %2651, %2649, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.60 : Tensor = aten::relu_(%input.59) # torch/nn/functional.py:1117:17
      %2667 : Tensor = prim::GetAttr[name="weight"](%2607)
      %2668 : Tensor? = prim::GetAttr[name="bias"](%2607)
      %2669 : int[] = prim::ListConstruct(%27, %27)
      %2670 : int[] = prim::ListConstruct(%24, %24)
      %2671 : int[] = prim::ListConstruct(%27, %27)
      %input.61 : Tensor = aten::conv2d(%input.60, %2667, %2668, %2669, %2670, %2671, %27) # torch/nn/modules/conv.py:415:15
      %2673 : int = aten::dim(%input.61) # torch/nn/modules/batchnorm.py:276:11
      %2674 : bool = aten::ne(%2673, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2674) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2675 : bool = prim::GetAttr[name="training"](%2608)
       = prim::If(%2675) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2676 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2608)
          %2677 : Tensor = aten::add(%2676, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2608, %2677)
          -> ()
        block1():
          -> ()
      %2678 : bool = prim::GetAttr[name="training"](%2608)
      %2679 : Tensor = prim::GetAttr[name="running_mean"](%2608)
      %2680 : Tensor = prim::GetAttr[name="running_var"](%2608)
      %2681 : Tensor = prim::GetAttr[name="weight"](%2608)
      %2682 : Tensor = prim::GetAttr[name="bias"](%2608)
       = prim::If(%2678) # torch/nn/functional.py:2011:4
        block0():
          %2683 : int[] = aten::size(%input.61) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%2683, %24) # torch/nn/functional.py:1991:17
          %2685 : int = aten::len(%2683) # torch/nn/functional.py:1992:19
          %2686 : int = aten::sub(%2685, %26) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%2686, %25, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %2690 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
              %2691 : int = aten::__getitem__(%2683, %2690) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %2691) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.83)
          %2693 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2693) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.62 : Tensor = aten::batch_norm(%input.61, %2681, %2682, %2679, %2680, %2678, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %2695 : Tensor = aten::add(%input.62, %input.20, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%2695)
    block1():
      %2696 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2220)
      %2697 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2696)
      %2698 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2696)
      %2699 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2696)
      %2700 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2696)
      %2701 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2696)
      %2702 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2696)
      %2703 : Tensor = prim::GetAttr[name="weight"](%2697)
      %2704 : Tensor? = prim::GetAttr[name="bias"](%2697)
      %2705 : int[] = prim::ListConstruct(%27, %27)
      %2706 : int[] = prim::ListConstruct(%24, %24)
      %2707 : int[] = prim::ListConstruct(%27, %27)
      %input.63 : Tensor = aten::conv2d(%input.20, %2703, %2704, %2705, %2706, %2707, %27) # torch/nn/modules/conv.py:415:15
      %2709 : int = aten::dim(%input.63) # torch/nn/modules/batchnorm.py:276:11
      %2710 : bool = aten::ne(%2709, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2710) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2711 : bool = prim::GetAttr[name="training"](%2698)
       = prim::If(%2711) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2712 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2698)
          %2713 : Tensor = aten::add(%2712, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2698, %2713)
          -> ()
        block1():
          -> ()
      %2714 : bool = prim::GetAttr[name="training"](%2698)
      %2715 : Tensor = prim::GetAttr[name="running_mean"](%2698)
      %2716 : Tensor = prim::GetAttr[name="running_var"](%2698)
      %2717 : Tensor = prim::GetAttr[name="weight"](%2698)
      %2718 : Tensor = prim::GetAttr[name="bias"](%2698)
       = prim::If(%2714) # torch/nn/functional.py:2011:4
        block0():
          %2719 : int[] = aten::size(%input.63) # torch/nn/functional.py:2012:27
          %size_prods.84 : int = aten::__getitem__(%2719, %24) # torch/nn/functional.py:1991:17
          %2721 : int = aten::len(%2719) # torch/nn/functional.py:1992:19
          %2722 : int = aten::sub(%2721, %26) # torch/nn/functional.py:1992:19
          %size_prods.85 : int = prim::Loop(%2722, %25, %size_prods.84) # torch/nn/functional.py:1992:4
            block0(%i.22 : int, %size_prods.86 : int):
              %2726 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
              %2727 : int = aten::__getitem__(%2719, %2726) # torch/nn/functional.py:1993:22
              %size_prods.87 : int = aten::mul(%size_prods.86, %2727) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.87)
          %2729 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2729) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.64 : Tensor = aten::batch_norm(%input.63, %2717, %2718, %2715, %2716, %2714, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.65 : Tensor = aten::relu_(%input.64) # torch/nn/functional.py:1117:17
      %2732 : Tensor = prim::GetAttr[name="weight"](%2699)
      %2733 : Tensor? = prim::GetAttr[name="bias"](%2699)
      %2734 : int[] = prim::ListConstruct(%27, %27)
      %2735 : int[] = prim::ListConstruct(%26, %26)
      %2736 : int[] = prim::ListConstruct(%27, %27)
      %input.66 : Tensor = aten::conv2d(%input.65, %2732, %2733, %2734, %2735, %2736, %12) # torch/nn/modules/conv.py:415:15
      %2738 : int = aten::dim(%input.66) # torch/nn/modules/batchnorm.py:276:11
      %2739 : bool = aten::ne(%2738, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2739) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2740 : bool = prim::GetAttr[name="training"](%2700)
       = prim::If(%2740) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2741 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2700)
          %2742 : Tensor = aten::add(%2741, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2700, %2742)
          -> ()
        block1():
          -> ()
      %2743 : bool = prim::GetAttr[name="training"](%2700)
      %2744 : Tensor = prim::GetAttr[name="running_mean"](%2700)
      %2745 : Tensor = prim::GetAttr[name="running_var"](%2700)
      %2746 : Tensor = prim::GetAttr[name="weight"](%2700)
      %2747 : Tensor = prim::GetAttr[name="bias"](%2700)
       = prim::If(%2743) # torch/nn/functional.py:2011:4
        block0():
          %2748 : int[] = aten::size(%input.66) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%2748, %24) # torch/nn/functional.py:1991:17
          %2750 : int = aten::len(%2748) # torch/nn/functional.py:1992:19
          %2751 : int = aten::sub(%2750, %26) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%2751, %25, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %2755 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
              %2756 : int = aten::__getitem__(%2748, %2755) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %2756) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.91)
          %2758 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2758) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.67 : Tensor = aten::batch_norm(%input.66, %2746, %2747, %2744, %2745, %2743, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.68 : Tensor = aten::relu_(%input.67) # torch/nn/functional.py:1117:17
      %2761 : Tensor = prim::GetAttr[name="weight"](%2701)
      %2762 : Tensor? = prim::GetAttr[name="bias"](%2701)
      %2763 : int[] = prim::ListConstruct(%27, %27)
      %2764 : int[] = prim::ListConstruct(%24, %24)
      %2765 : int[] = prim::ListConstruct(%27, %27)
      %input.69 : Tensor = aten::conv2d(%input.68, %2761, %2762, %2763, %2764, %2765, %27) # torch/nn/modules/conv.py:415:15
      %2767 : int = aten::dim(%input.69) # torch/nn/modules/batchnorm.py:276:11
      %2768 : bool = aten::ne(%2767, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2768) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2769 : bool = prim::GetAttr[name="training"](%2702)
       = prim::If(%2769) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2770 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2702)
          %2771 : Tensor = aten::add(%2770, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2702, %2771)
          -> ()
        block1():
          -> ()
      %2772 : bool = prim::GetAttr[name="training"](%2702)
      %2773 : Tensor = prim::GetAttr[name="running_mean"](%2702)
      %2774 : Tensor = prim::GetAttr[name="running_var"](%2702)
      %2775 : Tensor = prim::GetAttr[name="weight"](%2702)
      %2776 : Tensor = prim::GetAttr[name="bias"](%2702)
       = prim::If(%2772) # torch/nn/functional.py:2011:4
        block0():
          %2777 : int[] = aten::size(%input.69) # torch/nn/functional.py:2012:27
          %size_prods.92 : int = aten::__getitem__(%2777, %24) # torch/nn/functional.py:1991:17
          %2779 : int = aten::len(%2777) # torch/nn/functional.py:1992:19
          %2780 : int = aten::sub(%2779, %26) # torch/nn/functional.py:1992:19
          %size_prods.93 : int = prim::Loop(%2780, %25, %size_prods.92) # torch/nn/functional.py:1992:4
            block0(%i.24 : int, %size_prods.94 : int):
              %2784 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
              %2785 : int = aten::__getitem__(%2777, %2784) # torch/nn/functional.py:1993:22
              %size_prods.95 : int = aten::mul(%size_prods.94, %2785) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.95)
          %2787 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2787) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.70 : Tensor = aten::batch_norm(%input.69, %2775, %2776, %2773, %2774, %2772, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.70)
  %2789 : bool = prim::GetAttr[name="apply_residual"](%2221)
  %input.238 : Tensor = prim::If(%2789) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2791 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2221)
      %2792 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2791)
      %2793 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2791)
      %2794 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2791)
      %2795 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2791)
      %2796 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2791)
      %2797 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2791)
      %2798 : Tensor = prim::GetAttr[name="weight"](%2792)
      %2799 : Tensor? = prim::GetAttr[name="bias"](%2792)
      %2800 : int[] = prim::ListConstruct(%27, %27)
      %2801 : int[] = prim::ListConstruct(%24, %24)
      %2802 : int[] = prim::ListConstruct(%27, %27)
      %input.271 : Tensor = aten::conv2d(%input.21, %2798, %2799, %2800, %2801, %2802, %27) # torch/nn/modules/conv.py:415:15
      %2804 : int = aten::dim(%input.271) # torch/nn/modules/batchnorm.py:276:11
      %2805 : bool = aten::ne(%2804, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2805) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2806 : bool = prim::GetAttr[name="training"](%2793)
       = prim::If(%2806) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2807 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2793)
          %2808 : Tensor = aten::add(%2807, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2793, %2808)
          -> ()
        block1():
          -> ()
      %2809 : bool = prim::GetAttr[name="training"](%2793)
      %2810 : Tensor = prim::GetAttr[name="running_mean"](%2793)
      %2811 : Tensor = prim::GetAttr[name="running_var"](%2793)
      %2812 : Tensor = prim::GetAttr[name="weight"](%2793)
      %2813 : Tensor = prim::GetAttr[name="bias"](%2793)
       = prim::If(%2809) # torch/nn/functional.py:2011:4
        block0():
          %2814 : int[] = aten::size(%input.271) # torch/nn/functional.py:2012:27
          %size_prods.372 : int = aten::__getitem__(%2814, %24) # torch/nn/functional.py:1991:17
          %2816 : int = aten::len(%2814) # torch/nn/functional.py:1992:19
          %2817 : int = aten::sub(%2816, %26) # torch/nn/functional.py:1992:19
          %size_prods.373 : int = prim::Loop(%2817, %25, %size_prods.372) # torch/nn/functional.py:1992:4
            block0(%i.94 : int, %size_prods.374 : int):
              %2821 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
              %2822 : int = aten::__getitem__(%2814, %2821) # torch/nn/functional.py:1993:22
              %size_prods.375 : int = aten::mul(%size_prods.374, %2822) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.375)
          %2824 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2824) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.272 : Tensor = aten::batch_norm(%input.271, %2812, %2813, %2810, %2811, %2809, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.273 : Tensor = aten::relu_(%input.272) # torch/nn/functional.py:1117:17
      %2827 : Tensor = prim::GetAttr[name="weight"](%2794)
      %2828 : Tensor? = prim::GetAttr[name="bias"](%2794)
      %2829 : int[] = prim::ListConstruct(%27, %27)
      %2830 : int[] = prim::ListConstruct(%26, %26)
      %2831 : int[] = prim::ListConstruct(%27, %27)
      %input.274 : Tensor = aten::conv2d(%input.273, %2827, %2828, %2829, %2830, %2831, %12) # torch/nn/modules/conv.py:415:15
      %2833 : int = aten::dim(%input.274) # torch/nn/modules/batchnorm.py:276:11
      %2834 : bool = aten::ne(%2833, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2834) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2835 : bool = prim::GetAttr[name="training"](%2795)
       = prim::If(%2835) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2836 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2795)
          %2837 : Tensor = aten::add(%2836, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2795, %2837)
          -> ()
        block1():
          -> ()
      %2838 : bool = prim::GetAttr[name="training"](%2795)
      %2839 : Tensor = prim::GetAttr[name="running_mean"](%2795)
      %2840 : Tensor = prim::GetAttr[name="running_var"](%2795)
      %2841 : Tensor = prim::GetAttr[name="weight"](%2795)
      %2842 : Tensor = prim::GetAttr[name="bias"](%2795)
       = prim::If(%2838) # torch/nn/functional.py:2011:4
        block0():
          %2843 : int[] = aten::size(%input.274) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%2843, %24) # torch/nn/functional.py:1991:17
          %2845 : int = aten::len(%2843) # torch/nn/functional.py:1992:19
          %2846 : int = aten::sub(%2845, %26) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%2846, %25, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %2850 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
              %2851 : int = aten::__getitem__(%2843, %2850) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %2851) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.379)
          %2853 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2853) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.275 : Tensor = aten::batch_norm(%input.274, %2841, %2842, %2839, %2840, %2838, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.276 : Tensor = aten::relu_(%input.275) # torch/nn/functional.py:1117:17
      %2856 : Tensor = prim::GetAttr[name="weight"](%2796)
      %2857 : Tensor? = prim::GetAttr[name="bias"](%2796)
      %2858 : int[] = prim::ListConstruct(%27, %27)
      %2859 : int[] = prim::ListConstruct(%24, %24)
      %2860 : int[] = prim::ListConstruct(%27, %27)
      %input.277 : Tensor = aten::conv2d(%input.276, %2856, %2857, %2858, %2859, %2860, %27) # torch/nn/modules/conv.py:415:15
      %2862 : int = aten::dim(%input.277) # torch/nn/modules/batchnorm.py:276:11
      %2863 : bool = aten::ne(%2862, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2863) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2864 : bool = prim::GetAttr[name="training"](%2797)
       = prim::If(%2864) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2865 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2797)
          %2866 : Tensor = aten::add(%2865, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2797, %2866)
          -> ()
        block1():
          -> ()
      %2867 : bool = prim::GetAttr[name="training"](%2797)
      %2868 : Tensor = prim::GetAttr[name="running_mean"](%2797)
      %2869 : Tensor = prim::GetAttr[name="running_var"](%2797)
      %2870 : Tensor = prim::GetAttr[name="weight"](%2797)
      %2871 : Tensor = prim::GetAttr[name="bias"](%2797)
       = prim::If(%2867) # torch/nn/functional.py:2011:4
        block0():
          %2872 : int[] = aten::size(%input.277) # torch/nn/functional.py:2012:27
          %size_prods.380 : int = aten::__getitem__(%2872, %24) # torch/nn/functional.py:1991:17
          %2874 : int = aten::len(%2872) # torch/nn/functional.py:1992:19
          %2875 : int = aten::sub(%2874, %26) # torch/nn/functional.py:1992:19
          %size_prods.381 : int = prim::Loop(%2875, %25, %size_prods.380) # torch/nn/functional.py:1992:4
            block0(%i.96 : int, %size_prods.382 : int):
              %2879 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
              %2880 : int = aten::__getitem__(%2872, %2879) # torch/nn/functional.py:1993:22
              %size_prods.383 : int = aten::mul(%size_prods.382, %2880) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.383)
          %2882 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2882) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.278 : Tensor = aten::batch_norm(%input.277, %2870, %2871, %2868, %2869, %2867, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %2884 : Tensor = aten::add(%input.278, %input.21, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%2884)
    block1():
      %2885 : __torch__.torch.nn.modules.container.___torch_mangle_798.Sequential = prim::GetAttr[name="layers"](%2221)
      %2886 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2885)
      %2887 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2885)
      %2888 : __torch__.torch.nn.modules.conv.___torch_mangle_797.Conv2d = prim::GetAttr[name="3"](%2885)
      %2889 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2885)
      %2890 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="6"](%2885)
      %2891 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_719.BatchNorm2d = prim::GetAttr[name="7"](%2885)
      %2892 : Tensor = prim::GetAttr[name="weight"](%2886)
      %2893 : Tensor? = prim::GetAttr[name="bias"](%2886)
      %2894 : int[] = prim::ListConstruct(%27, %27)
      %2895 : int[] = prim::ListConstruct(%24, %24)
      %2896 : int[] = prim::ListConstruct(%27, %27)
      %input.279 : Tensor = aten::conv2d(%input.21, %2892, %2893, %2894, %2895, %2896, %27) # torch/nn/modules/conv.py:415:15
      %2898 : int = aten::dim(%input.279) # torch/nn/modules/batchnorm.py:276:11
      %2899 : bool = aten::ne(%2898, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2899) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2900 : bool = prim::GetAttr[name="training"](%2887)
       = prim::If(%2900) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2901 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2887)
          %2902 : Tensor = aten::add(%2901, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2887, %2902)
          -> ()
        block1():
          -> ()
      %2903 : bool = prim::GetAttr[name="training"](%2887)
      %2904 : Tensor = prim::GetAttr[name="running_mean"](%2887)
      %2905 : Tensor = prim::GetAttr[name="running_var"](%2887)
      %2906 : Tensor = prim::GetAttr[name="weight"](%2887)
      %2907 : Tensor = prim::GetAttr[name="bias"](%2887)
       = prim::If(%2903) # torch/nn/functional.py:2011:4
        block0():
          %2908 : int[] = aten::size(%input.279) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%2908, %24) # torch/nn/functional.py:1991:17
          %2910 : int = aten::len(%2908) # torch/nn/functional.py:1992:19
          %2911 : int = aten::sub(%2910, %26) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%2911, %25, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %2915 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
              %2916 : int = aten::__getitem__(%2908, %2915) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %2916) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.387)
          %2918 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2918) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.280 : Tensor = aten::batch_norm(%input.279, %2906, %2907, %2904, %2905, %2903, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.281 : Tensor = aten::relu_(%input.280) # torch/nn/functional.py:1117:17
      %2921 : Tensor = prim::GetAttr[name="weight"](%2888)
      %2922 : Tensor? = prim::GetAttr[name="bias"](%2888)
      %2923 : int[] = prim::ListConstruct(%27, %27)
      %2924 : int[] = prim::ListConstruct(%26, %26)
      %2925 : int[] = prim::ListConstruct(%27, %27)
      %input.282 : Tensor = aten::conv2d(%input.281, %2921, %2922, %2923, %2924, %2925, %12) # torch/nn/modules/conv.py:415:15
      %2927 : int = aten::dim(%input.282) # torch/nn/modules/batchnorm.py:276:11
      %2928 : bool = aten::ne(%2927, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2928) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2929 : bool = prim::GetAttr[name="training"](%2889)
       = prim::If(%2929) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2930 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2889)
          %2931 : Tensor = aten::add(%2930, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2889, %2931)
          -> ()
        block1():
          -> ()
      %2932 : bool = prim::GetAttr[name="training"](%2889)
      %2933 : Tensor = prim::GetAttr[name="running_mean"](%2889)
      %2934 : Tensor = prim::GetAttr[name="running_var"](%2889)
      %2935 : Tensor = prim::GetAttr[name="weight"](%2889)
      %2936 : Tensor = prim::GetAttr[name="bias"](%2889)
       = prim::If(%2932) # torch/nn/functional.py:2011:4
        block0():
          %2937 : int[] = aten::size(%input.282) # torch/nn/functional.py:2012:27
          %size_prods.388 : int = aten::__getitem__(%2937, %24) # torch/nn/functional.py:1991:17
          %2939 : int = aten::len(%2937) # torch/nn/functional.py:1992:19
          %2940 : int = aten::sub(%2939, %26) # torch/nn/functional.py:1992:19
          %size_prods.389 : int = prim::Loop(%2940, %25, %size_prods.388) # torch/nn/functional.py:1992:4
            block0(%i.98 : int, %size_prods.390 : int):
              %2944 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
              %2945 : int = aten::__getitem__(%2937, %2944) # torch/nn/functional.py:1993:22
              %size_prods.391 : int = aten::mul(%size_prods.390, %2945) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.391)
          %2947 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2947) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.283 : Tensor = aten::batch_norm(%input.282, %2935, %2936, %2933, %2934, %2932, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.284 : Tensor = aten::relu_(%input.283) # torch/nn/functional.py:1117:17
      %2950 : Tensor = prim::GetAttr[name="weight"](%2890)
      %2951 : Tensor? = prim::GetAttr[name="bias"](%2890)
      %2952 : int[] = prim::ListConstruct(%27, %27)
      %2953 : int[] = prim::ListConstruct(%24, %24)
      %2954 : int[] = prim::ListConstruct(%27, %27)
      %input.285 : Tensor = aten::conv2d(%input.284, %2950, %2951, %2952, %2953, %2954, %27) # torch/nn/modules/conv.py:415:15
      %2956 : int = aten::dim(%input.285) # torch/nn/modules/batchnorm.py:276:11
      %2957 : bool = aten::ne(%2956, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2957) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2958 : bool = prim::GetAttr[name="training"](%2891)
       = prim::If(%2958) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2959 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2891)
          %2960 : Tensor = aten::add(%2959, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2891, %2960)
          -> ()
        block1():
          -> ()
      %2961 : bool = prim::GetAttr[name="training"](%2891)
      %2962 : Tensor = prim::GetAttr[name="running_mean"](%2891)
      %2963 : Tensor = prim::GetAttr[name="running_var"](%2891)
      %2964 : Tensor = prim::GetAttr[name="weight"](%2891)
      %2965 : Tensor = prim::GetAttr[name="bias"](%2891)
       = prim::If(%2961) # torch/nn/functional.py:2011:4
        block0():
          %2966 : int[] = aten::size(%input.285) # torch/nn/functional.py:2012:27
          %size_prods.392 : int = aten::__getitem__(%2966, %24) # torch/nn/functional.py:1991:17
          %2968 : int = aten::len(%2966) # torch/nn/functional.py:1992:19
          %2969 : int = aten::sub(%2968, %26) # torch/nn/functional.py:1992:19
          %size_prods.393 : int = prim::Loop(%2969, %25, %size_prods.392) # torch/nn/functional.py:1992:4
            block0(%i.99 : int, %size_prods.394 : int):
              %2973 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
              %2974 : int = aten::__getitem__(%2966, %2973) # torch/nn/functional.py:1993:22
              %size_prods.395 : int = aten::mul(%size_prods.394, %2974) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.395)
          %2976 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2976) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.286 : Tensor = aten::batch_norm(%input.285, %2964, %2965, %2962, %2963, %2961, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.286)
  %2978 : __torch__.torchvision.models.mnasnet.___torch_mangle_805._InvertedResidual = prim::GetAttr[name="0"](%39)
  %2979 : bool = prim::GetAttr[name="apply_residual"](%2978)
  %input.242 : Tensor = prim::If(%2979) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:47:8
    block0():
      %2981 : __torch__.torch.nn.modules.container.___torch_mangle_804.Sequential = prim::GetAttr[name="layers"](%2978)
      %2982 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%2981)
      %2983 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%2981)
      %2984 : __torch__.torch.nn.modules.conv.___torch_mangle_801.Conv2d = prim::GetAttr[name="3"](%2981)
      %2985 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%2981)
      %2986 : __torch__.torch.nn.modules.conv.___torch_mangle_802.Conv2d = prim::GetAttr[name="6"](%2981)
      %2987 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_803.BatchNorm2d = prim::GetAttr[name="7"](%2981)
      %2988 : Tensor = prim::GetAttr[name="weight"](%2982)
      %2989 : Tensor? = prim::GetAttr[name="bias"](%2982)
      %2990 : int[] = prim::ListConstruct(%27, %27)
      %2991 : int[] = prim::ListConstruct(%24, %24)
      %2992 : int[] = prim::ListConstruct(%27, %27)
      %input.4 : Tensor = aten::conv2d(%input.238, %2988, %2989, %2990, %2991, %2992, %27) # torch/nn/modules/conv.py:415:15
      %2994 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
      %2995 : bool = aten::ne(%2994, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2995) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2996 : bool = prim::GetAttr[name="training"](%2983)
       = prim::If(%2996) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2997 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2983)
          %2998 : Tensor = aten::add(%2997, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2983, %2998)
          -> ()
        block1():
          -> ()
      %2999 : bool = prim::GetAttr[name="training"](%2983)
      %3000 : Tensor = prim::GetAttr[name="running_mean"](%2983)
      %3001 : Tensor = prim::GetAttr[name="running_var"](%2983)
      %3002 : Tensor = prim::GetAttr[name="weight"](%2983)
      %3003 : Tensor = prim::GetAttr[name="bias"](%2983)
       = prim::If(%2999) # torch/nn/functional.py:2011:4
        block0():
          %3004 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
          %size_prods.12 : int = aten::__getitem__(%3004, %24) # torch/nn/functional.py:1991:17
          %3006 : int = aten::len(%3004) # torch/nn/functional.py:1992:19
          %3007 : int = aten::sub(%3006, %26) # torch/nn/functional.py:1992:19
          %size_prods.13 : int = prim::Loop(%3007, %25, %size_prods.12) # torch/nn/functional.py:1992:4
            block0(%i.4 : int, %size_prods.14 : int):
              %3011 : int = aten::add(%i.4, %26) # torch/nn/functional.py:1993:27
              %3012 : int = aten::__getitem__(%3004, %3011) # torch/nn/functional.py:1993:22
              %size_prods.15 : int = aten::mul(%size_prods.14, %3012) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.15)
          %3014 : bool = aten::eq(%size_prods.13, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3014) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.6 : Tensor = aten::batch_norm(%input.4, %3002, %3003, %3000, %3001, %2999, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.8 : Tensor = aten::relu_(%input.6) # torch/nn/functional.py:1117:17
      %3017 : Tensor = prim::GetAttr[name="weight"](%2984)
      %3018 : Tensor? = prim::GetAttr[name="bias"](%2984)
      %3019 : int[] = prim::ListConstruct(%27, %27)
      %3020 : int[] = prim::ListConstruct(%27, %27)
      %3021 : int[] = prim::ListConstruct(%27, %27)
      %input.10 : Tensor = aten::conv2d(%input.8, %3017, %3018, %3019, %3020, %3021, %12) # torch/nn/modules/conv.py:415:15
      %3023 : int = aten::dim(%input.10) # torch/nn/modules/batchnorm.py:276:11
      %3024 : bool = aten::ne(%3023, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3024) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3025 : bool = prim::GetAttr[name="training"](%2985)
       = prim::If(%3025) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3026 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2985)
          %3027 : Tensor = aten::add(%3026, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2985, %3027)
          -> ()
        block1():
          -> ()
      %3028 : bool = prim::GetAttr[name="training"](%2985)
      %3029 : Tensor = prim::GetAttr[name="running_mean"](%2985)
      %3030 : Tensor = prim::GetAttr[name="running_var"](%2985)
      %3031 : Tensor = prim::GetAttr[name="weight"](%2985)
      %3032 : Tensor = prim::GetAttr[name="bias"](%2985)
       = prim::If(%3028) # torch/nn/functional.py:2011:4
        block0():
          %3033 : int[] = aten::size(%input.10) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%3033, %24) # torch/nn/functional.py:1991:17
          %3035 : int = aten::len(%3033) # torch/nn/functional.py:1992:19
          %3036 : int = aten::sub(%3035, %26) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%3036, %25, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %3040 : int = aten::add(%i.5, %26) # torch/nn/functional.py:1993:27
              %3041 : int = aten::__getitem__(%3033, %3040) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %3041) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.19)
          %3043 : bool = aten::eq(%size_prods.17, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3043) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.12 : Tensor = aten::batch_norm(%input.10, %3031, %3032, %3029, %3030, %3028, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.14 : Tensor = aten::relu_(%input.12) # torch/nn/functional.py:1117:17
      %3046 : Tensor = prim::GetAttr[name="weight"](%2986)
      %3047 : Tensor? = prim::GetAttr[name="bias"](%2986)
      %3048 : int[] = prim::ListConstruct(%27, %27)
      %3049 : int[] = prim::ListConstruct(%24, %24)
      %3050 : int[] = prim::ListConstruct(%27, %27)
      %input.16 : Tensor = aten::conv2d(%input.14, %3046, %3047, %3048, %3049, %3050, %27) # torch/nn/modules/conv.py:415:15
      %3052 : int = aten::dim(%input.16) # torch/nn/modules/batchnorm.py:276:11
      %3053 : bool = aten::ne(%3052, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3053) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3054 : bool = prim::GetAttr[name="training"](%2987)
       = prim::If(%3054) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3055 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2987)
          %3056 : Tensor = aten::add(%3055, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2987, %3056)
          -> ()
        block1():
          -> ()
      %3057 : bool = prim::GetAttr[name="training"](%2987)
      %3058 : Tensor = prim::GetAttr[name="running_mean"](%2987)
      %3059 : Tensor = prim::GetAttr[name="running_var"](%2987)
      %3060 : Tensor = prim::GetAttr[name="weight"](%2987)
      %3061 : Tensor = prim::GetAttr[name="bias"](%2987)
       = prim::If(%3057) # torch/nn/functional.py:2011:4
        block0():
          %3062 : int[] = aten::size(%input.16) # torch/nn/functional.py:2012:27
          %size_prods.20 : int = aten::__getitem__(%3062, %24) # torch/nn/functional.py:1991:17
          %3064 : int = aten::len(%3062) # torch/nn/functional.py:1992:19
          %3065 : int = aten::sub(%3064, %26) # torch/nn/functional.py:1992:19
          %size_prods.21 : int = prim::Loop(%3065, %25, %size_prods.20) # torch/nn/functional.py:1992:4
            block0(%i.6 : int, %size_prods.22 : int):
              %3069 : int = aten::add(%i.6, %26) # torch/nn/functional.py:1993:27
              %3070 : int = aten::__getitem__(%3062, %3069) # torch/nn/functional.py:1993:22
              %size_prods.23 : int = aten::mul(%size_prods.22, %3070) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.23)
          %3072 : bool = aten::eq(%size_prods.21, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3072) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.18 : Tensor = aten::batch_norm(%input.16, %3060, %3061, %3058, %3059, %3057, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %3074 : Tensor = aten::add(%input.18, %input.238, %27) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:48:19
      -> (%3074)
    block1():
      %3075 : __torch__.torch.nn.modules.container.___torch_mangle_804.Sequential = prim::GetAttr[name="layers"](%2978)
      %3076 : __torch__.torch.nn.modules.conv.___torch_mangle_795.Conv2d = prim::GetAttr[name="0"](%3075)
      %3077 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="1"](%3075)
      %3078 : __torch__.torch.nn.modules.conv.___torch_mangle_801.Conv2d = prim::GetAttr[name="3"](%3075)
      %3079 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_796.BatchNorm2d = prim::GetAttr[name="4"](%3075)
      %3080 : __torch__.torch.nn.modules.conv.___torch_mangle_802.Conv2d = prim::GetAttr[name="6"](%3075)
      %3081 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_803.BatchNorm2d = prim::GetAttr[name="7"](%3075)
      %3082 : Tensor = prim::GetAttr[name="weight"](%3076)
      %3083 : Tensor? = prim::GetAttr[name="bias"](%3076)
      %3084 : int[] = prim::ListConstruct(%27, %27)
      %3085 : int[] = prim::ListConstruct(%24, %24)
      %3086 : int[] = prim::ListConstruct(%27, %27)
      %input.19 : Tensor = aten::conv2d(%input.238, %3082, %3083, %3084, %3085, %3086, %27) # torch/nn/modules/conv.py:415:15
      %3088 : int = aten::dim(%input.19) # torch/nn/modules/batchnorm.py:276:11
      %3089 : bool = aten::ne(%3088, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3089) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3090 : bool = prim::GetAttr[name="training"](%3077)
       = prim::If(%3090) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3091 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3077)
          %3092 : Tensor = aten::add(%3091, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3077, %3092)
          -> ()
        block1():
          -> ()
      %3093 : bool = prim::GetAttr[name="training"](%3077)
      %3094 : Tensor = prim::GetAttr[name="running_mean"](%3077)
      %3095 : Tensor = prim::GetAttr[name="running_var"](%3077)
      %3096 : Tensor = prim::GetAttr[name="weight"](%3077)
      %3097 : Tensor = prim::GetAttr[name="bias"](%3077)
       = prim::If(%3093) # torch/nn/functional.py:2011:4
        block0():
          %3098 : int[] = aten::size(%input.19) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%3098, %24) # torch/nn/functional.py:1991:17
          %3100 : int = aten::len(%3098) # torch/nn/functional.py:1992:19
          %3101 : int = aten::sub(%3100, %26) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%3101, %25, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %3105 : int = aten::add(%i.2, %26) # torch/nn/functional.py:1993:27
              %3106 : int = aten::__getitem__(%3098, %3105) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %3106) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.5)
          %3108 : bool = aten::eq(%size_prods.4, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3108) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.241 : Tensor = aten::batch_norm(%input.19, %3096, %3097, %3094, %3095, %3093, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.7 : Tensor = aten::relu_(%input.241) # torch/nn/functional.py:1117:17
      %3111 : Tensor = prim::GetAttr[name="weight"](%3078)
      %3112 : Tensor? = prim::GetAttr[name="bias"](%3078)
      %3113 : int[] = prim::ListConstruct(%27, %27)
      %3114 : int[] = prim::ListConstruct(%27, %27)
      %3115 : int[] = prim::ListConstruct(%27, %27)
      %input.9 : Tensor = aten::conv2d(%input.7, %3111, %3112, %3113, %3114, %3115, %12) # torch/nn/modules/conv.py:415:15
      %3117 : int = aten::dim(%input.9) # torch/nn/modules/batchnorm.py:276:11
      %3118 : bool = aten::ne(%3117, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3118) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3119 : bool = prim::GetAttr[name="training"](%3079)
       = prim::If(%3119) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3120 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3079)
          %3121 : Tensor = aten::add(%3120, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3079, %3121)
          -> ()
        block1():
          -> ()
      %3122 : bool = prim::GetAttr[name="training"](%3079)
      %3123 : Tensor = prim::GetAttr[name="running_mean"](%3079)
      %3124 : Tensor = prim::GetAttr[name="running_var"](%3079)
      %3125 : Tensor = prim::GetAttr[name="weight"](%3079)
      %3126 : Tensor = prim::GetAttr[name="bias"](%3079)
       = prim::If(%3122) # torch/nn/functional.py:2011:4
        block0():
          %3127 : int[] = aten::size(%input.9) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%3127, %24) # torch/nn/functional.py:1991:17
          %3129 : int = aten::len(%3127) # torch/nn/functional.py:1992:19
          %3130 : int = aten::sub(%3129, %26) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%3130, %25, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %3134 : int = aten::add(%i.3, %26) # torch/nn/functional.py:1993:27
              %3135 : int = aten::__getitem__(%3127, %3134) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %3135) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.11)
          %3137 : bool = aten::eq(%size_prods.9, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3137) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.11 : Tensor = aten::batch_norm(%input.9, %3125, %3126, %3123, %3124, %3122, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %input.13 : Tensor = aten::relu_(%input.11) # torch/nn/functional.py:1117:17
      %3140 : Tensor = prim::GetAttr[name="weight"](%3080)
      %3141 : Tensor? = prim::GetAttr[name="bias"](%3080)
      %3142 : int[] = prim::ListConstruct(%27, %27)
      %3143 : int[] = prim::ListConstruct(%24, %24)
      %3144 : int[] = prim::ListConstruct(%27, %27)
      %input.15 : Tensor = aten::conv2d(%input.13, %3140, %3141, %3142, %3143, %3144, %27) # torch/nn/modules/conv.py:415:15
      %3146 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
      %3147 : bool = aten::ne(%3146, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3147) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3148 : bool = prim::GetAttr[name="training"](%3081)
       = prim::If(%3148) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3149 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3081)
          %3150 : Tensor = aten::add(%3149, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3081, %3150)
          -> ()
        block1():
          -> ()
      %3151 : bool = prim::GetAttr[name="training"](%3081)
      %3152 : Tensor = prim::GetAttr[name="running_mean"](%3081)
      %3153 : Tensor = prim::GetAttr[name="running_var"](%3081)
      %3154 : Tensor = prim::GetAttr[name="weight"](%3081)
      %3155 : Tensor = prim::GetAttr[name="bias"](%3081)
       = prim::If(%3151) # torch/nn/functional.py:2011:4
        block0():
          %3156 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
          %size_prods.396 : int = aten::__getitem__(%3156, %24) # torch/nn/functional.py:1991:17
          %3158 : int = aten::len(%3156) # torch/nn/functional.py:1992:19
          %3159 : int = aten::sub(%3158, %26) # torch/nn/functional.py:1992:19
          %size_prods.397 : int = prim::Loop(%3159, %25, %size_prods.396) # torch/nn/functional.py:1992:4
            block0(%i.100 : int, %size_prods.398 : int):
              %3163 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
              %3164 : int = aten::__getitem__(%3156, %3163) # torch/nn/functional.py:1993:22
              %size_prods.399 : int = aten::mul(%size_prods.398, %3164) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.399)
          %3166 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3166) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.17 : Tensor = aten::batch_norm(%input.15, %3154, %3155, %3152, %3153, %3151, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      -> (%input.17)
  %3168 : Tensor = prim::GetAttr[name="weight"](%40)
  %3169 : Tensor? = prim::GetAttr[name="bias"](%40)
  %3170 : int[] = prim::ListConstruct(%27, %27)
  %3171 : int[] = prim::ListConstruct(%24, %24)
  %3172 : int[] = prim::ListConstruct(%27, %27)
  %input.236 : Tensor = aten::conv2d(%input.242, %3168, %3169, %3170, %3171, %3172, %27) # torch/nn/modules/conv.py:415:15
  %3174 : int = aten::dim(%input.236) # torch/nn/modules/batchnorm.py:276:11
  %3175 : bool = aten::ne(%3174, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3175) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3176 : bool = prim::GetAttr[name="training"](%41)
   = prim::If(%3176) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3177 : Tensor = prim::GetAttr[name="num_batches_tracked"](%41)
      %3178 : Tensor = aten::add(%3177, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%41, %3178)
      -> ()
    block1():
      -> ()
  %3179 : bool = prim::GetAttr[name="training"](%41)
  %3180 : Tensor = prim::GetAttr[name="running_mean"](%41)
  %3181 : Tensor = prim::GetAttr[name="running_var"](%41)
  %3182 : Tensor = prim::GetAttr[name="weight"](%41)
  %3183 : Tensor = prim::GetAttr[name="bias"](%41)
   = prim::If(%3179) # torch/nn/functional.py:2011:4
    block0():
      %3184 : int[] = aten::size(%input.236) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3184, %24) # torch/nn/functional.py:1991:17
      %3186 : int = aten::len(%3184) # torch/nn/functional.py:1992:19
      %3187 : int = aten::sub(%3186, %26) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3187, %25, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %3191 : int = aten::add(%i.1, %26) # torch/nn/functional.py:1993:27
          %3192 : int = aten::__getitem__(%3184, %3191) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3192) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.3)
      %3194 : bool = aten::eq(%size_prods, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3194) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.239 : Tensor = aten::batch_norm(%input.236, %3182, %3183, %3180, %3181, %3179, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %x.3 : Tensor = aten::relu_(%input.239) # torch/nn/functional.py:1117:17
  %8 : int[] = prim::ListConstruct(%4, %5)
  %x.5 : Tensor = aten::mean(%x.3, %8, %3, %2) # torch/hub/pytorch_vision_master/torchvision/models/mnasnet.py:136:12
  %10 : __torch__.torch.nn.modules.container.___torch_mangle_697.Sequential = prim::GetAttr[name="classifier"](%self)
  %3197 : int = prim::Constant[value=2]() # torch/nn/functional.py:1672:22
  %3198 : int = prim::Constant[value=1]()
  %3199 : float = prim::Constant[value=0.20000000000000001]() # torch/nn/modules/dropout.py:58:32
  %3200 : __torch__.torch.nn.modules.dropout.___torch_mangle_695.Dropout = prim::GetAttr[name="0"](%10)
  %3201 : __torch__.torch.nn.modules.linear.___torch_mangle_696.Linear = prim::GetAttr[name="1"](%10)
  %3202 : bool = prim::GetAttr[name="training"](%3200)
  %input.3 : Tensor = aten::dropout_(%x.5, %3199, %3202) # torch/nn/functional.py:971:12
  %3204 : Tensor = prim::GetAttr[name="weight"](%3201)
  %3205 : Tensor = prim::GetAttr[name="bias"](%3201)
  %3206 : int = aten::dim(%input.3) # torch/nn/functional.py:1672:7
  %3207 : bool = aten::eq(%3206, %3197) # torch/nn/functional.py:1672:7
  %input.5 : Tensor = prim::If(%3207) # torch/nn/functional.py:1672:4
    block0():
      %3209 : Tensor = aten::t(%3204) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%3205, %input.3, %3209, %3198, %3198) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %3211 : Tensor = aten::t(%3204) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%input.3, %3211) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %3205, %3198) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%input.5)
