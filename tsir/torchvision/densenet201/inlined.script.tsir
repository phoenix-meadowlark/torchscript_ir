graph(%self : __torch__.torchvision.models.densenet.___torch_mangle_386.DenseNet,
      %x.1 : Tensor):
  %2 : int = prim::Constant[value=-1]()
  %3 : Function = prim::Constant[name="adaptive_avg_pool2d"]()
  %4 : Function = prim::Constant[name="relu"]()
  %5 : bool = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:193:39
  %6 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:194:42
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_384.Sequential = prim::GetAttr[name="features"](%self)
  %15 : None = prim::Constant() # torch/nn/modules/pooling.py:599:82
  %16 : float = prim::Constant[value=0.]() # torch/nn/functional.py:968:11
  %17 : float = prim::Constant[value=1.]() # torch/nn/functional.py:968:21
  %18 : int = prim::Constant[value=9223372036854775807]()
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %29 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv0"](%7)
  %30 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm0"](%7)
  %31 : __torch__.torchvision.models.densenet._DenseBlock = prim::GetAttr[name="denseblock1"](%7)
  %32 : __torch__.torchvision.models.densenet._Transition = prim::GetAttr[name="transition1"](%7)
  %33 : __torch__.torchvision.models.densenet.___torch_mangle_109._DenseBlock = prim::GetAttr[name="denseblock2"](%7)
  %34 : __torch__.torchvision.models.densenet.___torch_mangle_110._Transition = prim::GetAttr[name="transition2"](%7)
  %35 : __torch__.torchvision.models.densenet.___torch_mangle_369._DenseBlock = prim::GetAttr[name="denseblock3"](%7)
  %36 : __torch__.torchvision.models.densenet.___torch_mangle_372._Transition = prim::GetAttr[name="transition3"](%7)
  %37 : __torch__.torchvision.models.densenet.___torch_mangle_383._DenseBlock = prim::GetAttr[name="denseblock4"](%7)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_273.BatchNorm2d = prim::GetAttr[name="norm5"](%7)
  %39 : Tensor = prim::GetAttr[name="weight"](%29)
  %40 : Tensor? = prim::GetAttr[name="bias"](%29)
  %41 : int[] = prim::ListConstruct(%26, %26)
  %42 : int[] = prim::ListConstruct(%28, %28)
  %43 : int[] = prim::ListConstruct(%27, %27)
  %input.4 : Tensor = aten::conv2d(%x.1, %39, %40, %41, %42, %43, %27) # torch/nn/modules/conv.py:415:15
  %45 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %46 : bool = aten::ne(%45, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%46) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %47 : bool = prim::GetAttr[name="training"](%30)
   = prim::If(%47) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %48 : Tensor = prim::GetAttr[name="num_batches_tracked"](%30)
      %49 : Tensor = aten::add(%48, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%30, %49)
      -> ()
    block1():
      -> ()
  %50 : bool = prim::GetAttr[name="training"](%30)
  %51 : Tensor = prim::GetAttr[name="running_mean"](%30)
  %52 : Tensor = prim::GetAttr[name="running_var"](%30)
  %53 : Tensor = prim::GetAttr[name="weight"](%30)
  %54 : Tensor = prim::GetAttr[name="bias"](%30)
   = prim::If(%50) # torch/nn/functional.py:2011:4
    block0():
      %55 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%55, %24) # torch/nn/functional.py:1991:17
      %57 : int = aten::len(%55) # torch/nn/functional.py:1992:19
      %58 : int = aten::sub(%57, %26) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%58, %25, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %62 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
          %63 : int = aten::__getitem__(%55, %62) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %63) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.395)
      %65 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
       = prim::If(%65) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %53, %54, %51, %52, %50, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu_(%input.6) # torch/nn/functional.py:1117:17
  %68 : int[] = prim::ListConstruct(%28, %28)
  %69 : int[] = prim::ListConstruct(%26, %26)
  %70 : int[] = prim::ListConstruct(%27, %27)
  %71 : int[] = prim::ListConstruct(%27, %27)
  %input.10 : Tensor = aten::max_pool2d(%input.8, %68, %69, %70, %71, %19) # torch/nn/functional.py:575:11
  %features.2 : Tensor[] = prim::ListConstruct(%input.10)
  %74 : __torch__.torchvision.models.densenet._DenseLayer = prim::GetAttr[name="denselayer1"](%31)
  %75 : __torch__.torchvision.models.densenet.___torch_mangle_75._DenseLayer = prim::GetAttr[name="denselayer2"](%31)
  %76 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer3"](%31)
  %77 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer4"](%31)
  %78 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer5"](%31)
  %79 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer6"](%31)
  %80 : Tensor = prim::Uninitialized()
  %81 : bool = prim::GetAttr[name="memory_efficient"](%74)
  %82 : bool = prim::If(%81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %83 : bool = prim::Uninitialized()
      %84 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %85 : bool = aten::gt(%84, %24)
      %86 : bool, %87 : bool, %88 : int = prim::Loop(%18, %85, %19, %83, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%89 : int, %90 : bool, %91 : bool, %92 : int):
          %tensor.51 : Tensor = aten::__getitem__(%features.2, %92) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %94 : bool = prim::requires_grad(%tensor.51)
          %95 : bool, %96 : bool = prim::If(%94) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %83)
          %97 : int = aten::add(%92, %27)
          %98 : bool = aten::lt(%97, %84)
          %99 : bool = aten::__and__(%98, %95)
          -> (%99, %94, %96, %97)
      %100 : bool = prim::If(%86)
        block0():
          -> (%87)
        block1():
          -> (%19)
      -> (%100)
    block1():
      -> (%19)
  %bottleneck_output.100 : Tensor = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%80)
    block1():
      %concated_features.51 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %103 : __torch__.torch.nn.modules.conv.___torch_mangle_71.Conv2d = prim::GetAttr[name="conv1"](%74)
      %104 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm1"](%74)
      %105 : int = aten::dim(%concated_features.51) # torch/nn/modules/batchnorm.py:276:11
      %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %107 : bool = prim::GetAttr[name="training"](%104)
       = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
          %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%104, %109)
          -> ()
        block1():
          -> ()
      %110 : bool = prim::GetAttr[name="training"](%104)
      %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
      %112 : Tensor = prim::GetAttr[name="running_var"](%104)
      %113 : Tensor = prim::GetAttr[name="weight"](%104)
      %114 : Tensor = prim::GetAttr[name="bias"](%104)
       = prim::If(%110) # torch/nn/functional.py:2011:4
        block0():
          %115 : int[] = aten::size(%concated_features.51) # torch/nn/functional.py:2012:27
          %size_prods.400 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
          %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
          %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
          %size_prods.401 : int = prim::Loop(%118, %25, %size_prods.400) # torch/nn/functional.py:1992:4
            block0(%i.101 : int, %size_prods.402 : int):
              %122 : int = aten::add(%i.101, %26) # torch/nn/functional.py:1993:27
              %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
              %size_prods.403 : int = aten::mul(%size_prods.402, %123) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.403)
          %125 : bool = aten::eq(%size_prods.401, %27) # torch/nn/functional.py:1994:7
           = prim::If(%125) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %126 : Tensor = aten::batch_norm(%concated_features.51, %113, %114, %111, %112, %110, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.101 : Tensor = aten::relu_(%126) # torch/nn/functional.py:1117:17
      %128 : Tensor = prim::GetAttr[name="weight"](%103)
      %129 : Tensor? = prim::GetAttr[name="bias"](%103)
      %130 : int[] = prim::ListConstruct(%27, %27)
      %131 : int[] = prim::ListConstruct(%24, %24)
      %132 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.101 : Tensor = aten::conv2d(%result.101, %128, %129, %130, %131, %132, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.101)
  %134 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%74)
  %135 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%74)
  %136 : int = aten::dim(%bottleneck_output.100) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%bottleneck_output.100) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%149, %25, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %153 : int = aten::add(%i.102, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.407)
      %156 : bool = aten::eq(%size_prods.405, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %157 : Tensor = aten::batch_norm(%bottleneck_output.100, %144, %145, %142, %143, %141, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.102 : Tensor = aten::relu_(%157) # torch/nn/functional.py:1117:17
  %159 : Tensor = prim::GetAttr[name="weight"](%134)
  %160 : Tensor? = prim::GetAttr[name="bias"](%134)
  %161 : int[] = prim::ListConstruct(%27, %27)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%27, %27)
  %new_features.129 : Tensor = aten::conv2d(%result.102, %159, %160, %161, %162, %163, %27) # torch/nn/modules/conv.py:415:15
  %165 : float = prim::GetAttr[name="drop_rate"](%74)
  %166 : bool = aten::gt(%165, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.106 : Tensor = prim::If(%166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %168 : float = prim::GetAttr[name="drop_rate"](%74)
      %169 : bool = prim::GetAttr[name="training"](%74)
      %170 : bool = aten::lt(%168, %16) # torch/nn/functional.py:968:7
      %171 : bool = prim::If(%170) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %172 : bool = aten::gt(%168, %17) # torch/nn/functional.py:968:17
          -> (%172)
       = prim::If(%171) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %173 : Tensor = aten::dropout(%new_features.129, %168, %169) # torch/nn/functional.py:973:17
      -> (%173)
    block1():
      -> (%new_features.129)
  %174 : Tensor[] = aten::append(%features.2, %new_features.106) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %175 : Tensor = prim::Uninitialized()
  %176 : bool = prim::GetAttr[name="memory_efficient"](%75)
  %177 : bool = prim::If(%176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %178 : bool = prim::Uninitialized()
      %179 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %180 : bool = aten::gt(%179, %24)
      %181 : bool, %182 : bool, %183 : int = prim::Loop(%18, %180, %19, %178, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%184 : int, %185 : bool, %186 : bool, %187 : int):
          %tensor.64 : Tensor = aten::__getitem__(%features.2, %187) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %189 : bool = prim::requires_grad(%tensor.64)
          %190 : bool, %191 : bool = prim::If(%189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %178)
          %192 : int = aten::add(%187, %27)
          %193 : bool = aten::lt(%192, %179)
          %194 : bool = aten::__and__(%193, %190)
          -> (%194, %189, %191, %192)
      %195 : bool = prim::If(%181)
        block0():
          -> (%182)
        block1():
          -> (%19)
      -> (%195)
    block1():
      -> (%19)
  %bottleneck_output.126 : Tensor = prim::If(%177) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%175)
    block1():
      %concated_features.64 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %198 : __torch__.torch.nn.modules.conv.___torch_mangle_74.Conv2d = prim::GetAttr[name="conv1"](%75)
      %199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="norm1"](%75)
      %200 : int = aten::dim(%concated_features.64) # torch/nn/modules/batchnorm.py:276:11
      %201 : bool = aten::ne(%200, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%201) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %202 : bool = prim::GetAttr[name="training"](%199)
       = prim::If(%202) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %203 : Tensor = prim::GetAttr[name="num_batches_tracked"](%199)
          %204 : Tensor = aten::add(%203, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%199, %204)
          -> ()
        block1():
          -> ()
      %205 : bool = prim::GetAttr[name="training"](%199)
      %206 : Tensor = prim::GetAttr[name="running_mean"](%199)
      %207 : Tensor = prim::GetAttr[name="running_var"](%199)
      %208 : Tensor = prim::GetAttr[name="weight"](%199)
      %209 : Tensor = prim::GetAttr[name="bias"](%199)
       = prim::If(%205) # torch/nn/functional.py:2011:4
        block0():
          %210 : int[] = aten::size(%concated_features.64) # torch/nn/functional.py:2012:27
          %size_prods.408 : int = aten::__getitem__(%210, %24) # torch/nn/functional.py:1991:17
          %212 : int = aten::len(%210) # torch/nn/functional.py:1992:19
          %213 : int = aten::sub(%212, %26) # torch/nn/functional.py:1992:19
          %size_prods.409 : int = prim::Loop(%213, %25, %size_prods.408) # torch/nn/functional.py:1992:4
            block0(%i.103 : int, %size_prods.410 : int):
              %217 : int = aten::add(%i.103, %26) # torch/nn/functional.py:1993:27
              %218 : int = aten::__getitem__(%210, %217) # torch/nn/functional.py:1993:22
              %size_prods.411 : int = aten::mul(%size_prods.410, %218) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.411)
          %220 : bool = aten::eq(%size_prods.409, %27) # torch/nn/functional.py:1994:7
           = prim::If(%220) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %221 : Tensor = aten::batch_norm(%concated_features.64, %208, %209, %206, %207, %205, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.127 : Tensor = aten::relu_(%221) # torch/nn/functional.py:1117:17
      %223 : Tensor = prim::GetAttr[name="weight"](%198)
      %224 : Tensor? = prim::GetAttr[name="bias"](%198)
      %225 : int[] = prim::ListConstruct(%27, %27)
      %226 : int[] = prim::ListConstruct(%24, %24)
      %227 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.127 : Tensor = aten::conv2d(%result.127, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.127)
  %229 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%75)
  %230 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%75)
  %231 : int = aten::dim(%bottleneck_output.126) # torch/nn/modules/batchnorm.py:276:11
  %232 : bool = aten::ne(%231, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%232) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %233 : bool = prim::GetAttr[name="training"](%230)
   = prim::If(%233) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %234 : Tensor = prim::GetAttr[name="num_batches_tracked"](%230)
      %235 : Tensor = aten::add(%234, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%230, %235)
      -> ()
    block1():
      -> ()
  %236 : bool = prim::GetAttr[name="training"](%230)
  %237 : Tensor = prim::GetAttr[name="running_mean"](%230)
  %238 : Tensor = prim::GetAttr[name="running_var"](%230)
  %239 : Tensor = prim::GetAttr[name="weight"](%230)
  %240 : Tensor = prim::GetAttr[name="bias"](%230)
   = prim::If(%236) # torch/nn/functional.py:2011:4
    block0():
      %241 : int[] = aten::size(%bottleneck_output.126) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%241, %24) # torch/nn/functional.py:1991:17
      %243 : int = aten::len(%241) # torch/nn/functional.py:1992:19
      %244 : int = aten::sub(%243, %26) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%244, %25, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %248 : int = aten::add(%i.104, %26) # torch/nn/functional.py:1993:27
          %249 : int = aten::__getitem__(%241, %248) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %249) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.415)
      %251 : bool = aten::eq(%size_prods.413, %27) # torch/nn/functional.py:1994:7
       = prim::If(%251) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %252 : Tensor = aten::batch_norm(%bottleneck_output.126, %239, %240, %237, %238, %236, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.128 : Tensor = aten::relu_(%252) # torch/nn/functional.py:1117:17
  %254 : Tensor = prim::GetAttr[name="weight"](%229)
  %255 : Tensor? = prim::GetAttr[name="bias"](%229)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %new_features.98 : Tensor = aten::conv2d(%result.128, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : float = prim::GetAttr[name="drop_rate"](%75)
  %261 : bool = aten::gt(%260, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.132 : Tensor = prim::If(%261) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %263 : float = prim::GetAttr[name="drop_rate"](%75)
      %264 : bool = prim::GetAttr[name="training"](%75)
      %265 : bool = aten::lt(%263, %16) # torch/nn/functional.py:968:7
      %266 : bool = prim::If(%265) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %267 : bool = aten::gt(%263, %17) # torch/nn/functional.py:968:17
          -> (%267)
       = prim::If(%266) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %268 : Tensor = aten::dropout(%new_features.98, %263, %264) # torch/nn/functional.py:973:17
      -> (%268)
    block1():
      -> (%new_features.98)
  %269 : Tensor[] = aten::append(%features.2, %new_features.132) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %270 : Tensor = prim::Uninitialized()
  %271 : bool = prim::GetAttr[name="memory_efficient"](%76)
  %272 : bool = prim::If(%271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %273 : bool = prim::Uninitialized()
      %274 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %275 : bool = aten::gt(%274, %24)
      %276 : bool, %277 : bool, %278 : int = prim::Loop(%18, %275, %19, %273, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%279 : int, %280 : bool, %281 : bool, %282 : int):
          %tensor.65 : Tensor = aten::__getitem__(%features.2, %282) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %284 : bool = prim::requires_grad(%tensor.65)
          %285 : bool, %286 : bool = prim::If(%284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %273)
          %287 : int = aten::add(%282, %27)
          %288 : bool = aten::lt(%287, %274)
          %289 : bool = aten::__and__(%288, %285)
          -> (%289, %284, %286, %287)
      %290 : bool = prim::If(%276)
        block0():
          -> (%277)
        block1():
          -> (%19)
      -> (%290)
    block1():
      -> (%19)
  %bottleneck_output.128 : Tensor = prim::If(%272) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%270)
    block1():
      %concated_features.65 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %293 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%76)
      %294 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%76)
      %295 : int = aten::dim(%concated_features.65) # torch/nn/modules/batchnorm.py:276:11
      %296 : bool = aten::ne(%295, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%296) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %297 : bool = prim::GetAttr[name="training"](%294)
       = prim::If(%297) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%294)
          %299 : Tensor = aten::add(%298, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%294, %299)
          -> ()
        block1():
          -> ()
      %300 : bool = prim::GetAttr[name="training"](%294)
      %301 : Tensor = prim::GetAttr[name="running_mean"](%294)
      %302 : Tensor = prim::GetAttr[name="running_var"](%294)
      %303 : Tensor = prim::GetAttr[name="weight"](%294)
      %304 : Tensor = prim::GetAttr[name="bias"](%294)
       = prim::If(%300) # torch/nn/functional.py:2011:4
        block0():
          %305 : int[] = aten::size(%concated_features.65) # torch/nn/functional.py:2012:27
          %size_prods.416 : int = aten::__getitem__(%305, %24) # torch/nn/functional.py:1991:17
          %307 : int = aten::len(%305) # torch/nn/functional.py:1992:19
          %308 : int = aten::sub(%307, %26) # torch/nn/functional.py:1992:19
          %size_prods.417 : int = prim::Loop(%308, %25, %size_prods.416) # torch/nn/functional.py:1992:4
            block0(%i.105 : int, %size_prods.418 : int):
              %312 : int = aten::add(%i.105, %26) # torch/nn/functional.py:1993:27
              %313 : int = aten::__getitem__(%305, %312) # torch/nn/functional.py:1993:22
              %size_prods.419 : int = aten::mul(%size_prods.418, %313) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.419)
          %315 : bool = aten::eq(%size_prods.417, %27) # torch/nn/functional.py:1994:7
           = prim::If(%315) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %316 : Tensor = aten::batch_norm(%concated_features.65, %303, %304, %301, %302, %300, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.129 : Tensor = aten::relu_(%316) # torch/nn/functional.py:1117:17
      %318 : Tensor = prim::GetAttr[name="weight"](%293)
      %319 : Tensor? = prim::GetAttr[name="bias"](%293)
      %320 : int[] = prim::ListConstruct(%27, %27)
      %321 : int[] = prim::ListConstruct(%24, %24)
      %322 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.129 : Tensor = aten::conv2d(%result.129, %318, %319, %320, %321, %322, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.129)
  %324 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%76)
  %325 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%76)
  %326 : int = aten::dim(%bottleneck_output.128) # torch/nn/modules/batchnorm.py:276:11
  %327 : bool = aten::ne(%326, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%327) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %328 : bool = prim::GetAttr[name="training"](%325)
   = prim::If(%328) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %329 : Tensor = prim::GetAttr[name="num_batches_tracked"](%325)
      %330 : Tensor = aten::add(%329, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%325, %330)
      -> ()
    block1():
      -> ()
  %331 : bool = prim::GetAttr[name="training"](%325)
  %332 : Tensor = prim::GetAttr[name="running_mean"](%325)
  %333 : Tensor = prim::GetAttr[name="running_var"](%325)
  %334 : Tensor = prim::GetAttr[name="weight"](%325)
  %335 : Tensor = prim::GetAttr[name="bias"](%325)
   = prim::If(%331) # torch/nn/functional.py:2011:4
    block0():
      %336 : int[] = aten::size(%bottleneck_output.128) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%336, %24) # torch/nn/functional.py:1991:17
      %338 : int = aten::len(%336) # torch/nn/functional.py:1992:19
      %339 : int = aten::sub(%338, %26) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%339, %25, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %343 : int = aten::add(%i.106, %26) # torch/nn/functional.py:1993:27
          %344 : int = aten::__getitem__(%336, %343) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %344) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.423)
      %346 : bool = aten::eq(%size_prods.421, %27) # torch/nn/functional.py:1994:7
       = prim::If(%346) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %347 : Tensor = aten::batch_norm(%bottleneck_output.128, %334, %335, %332, %333, %331, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.130 : Tensor = aten::relu_(%347) # torch/nn/functional.py:1117:17
  %349 : Tensor = prim::GetAttr[name="weight"](%324)
  %350 : Tensor? = prim::GetAttr[name="bias"](%324)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %353 : int[] = prim::ListConstruct(%27, %27)
  %new_features.100 : Tensor = aten::conv2d(%result.130, %349, %350, %351, %352, %353, %27) # torch/nn/modules/conv.py:415:15
  %355 : float = prim::GetAttr[name="drop_rate"](%76)
  %356 : bool = aten::gt(%355, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.97 : Tensor = prim::If(%356) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %358 : float = prim::GetAttr[name="drop_rate"](%76)
      %359 : bool = prim::GetAttr[name="training"](%76)
      %360 : bool = aten::lt(%358, %16) # torch/nn/functional.py:968:7
      %361 : bool = prim::If(%360) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %362 : bool = aten::gt(%358, %17) # torch/nn/functional.py:968:17
          -> (%362)
       = prim::If(%361) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %363 : Tensor = aten::dropout(%new_features.100, %358, %359) # torch/nn/functional.py:973:17
      -> (%363)
    block1():
      -> (%new_features.100)
  %364 : Tensor[] = aten::append(%features.2, %new_features.97) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %365 : Tensor = prim::Uninitialized()
  %366 : bool = prim::GetAttr[name="memory_efficient"](%77)
  %367 : bool = prim::If(%366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %368 : bool = prim::Uninitialized()
      %369 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %370 : bool = aten::gt(%369, %24)
      %371 : bool, %372 : bool, %373 : int = prim::Loop(%18, %370, %19, %368, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%374 : int, %375 : bool, %376 : bool, %377 : int):
          %tensor.49 : Tensor = aten::__getitem__(%features.2, %377) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %379 : bool = prim::requires_grad(%tensor.49)
          %380 : bool, %381 : bool = prim::If(%379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %368)
          %382 : int = aten::add(%377, %27)
          %383 : bool = aten::lt(%382, %369)
          %384 : bool = aten::__and__(%383, %380)
          -> (%384, %379, %381, %382)
      %385 : bool = prim::If(%371)
        block0():
          -> (%372)
        block1():
          -> (%19)
      -> (%385)
    block1():
      -> (%19)
  %bottleneck_output.96 : Tensor = prim::If(%367) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%365)
    block1():
      %concated_features.49 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %388 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%77)
      %389 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%77)
      %390 : int = aten::dim(%concated_features.49) # torch/nn/modules/batchnorm.py:276:11
      %391 : bool = aten::ne(%390, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%391) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %392 : bool = prim::GetAttr[name="training"](%389)
       = prim::If(%392) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %393 : Tensor = prim::GetAttr[name="num_batches_tracked"](%389)
          %394 : Tensor = aten::add(%393, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%389, %394)
          -> ()
        block1():
          -> ()
      %395 : bool = prim::GetAttr[name="training"](%389)
      %396 : Tensor = prim::GetAttr[name="running_mean"](%389)
      %397 : Tensor = prim::GetAttr[name="running_var"](%389)
      %398 : Tensor = prim::GetAttr[name="weight"](%389)
      %399 : Tensor = prim::GetAttr[name="bias"](%389)
       = prim::If(%395) # torch/nn/functional.py:2011:4
        block0():
          %400 : int[] = aten::size(%concated_features.49) # torch/nn/functional.py:2012:27
          %size_prods.424 : int = aten::__getitem__(%400, %24) # torch/nn/functional.py:1991:17
          %402 : int = aten::len(%400) # torch/nn/functional.py:1992:19
          %403 : int = aten::sub(%402, %26) # torch/nn/functional.py:1992:19
          %size_prods.425 : int = prim::Loop(%403, %25, %size_prods.424) # torch/nn/functional.py:1992:4
            block0(%i.107 : int, %size_prods.426 : int):
              %407 : int = aten::add(%i.107, %26) # torch/nn/functional.py:1993:27
              %408 : int = aten::__getitem__(%400, %407) # torch/nn/functional.py:1993:22
              %size_prods.427 : int = aten::mul(%size_prods.426, %408) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.427)
          %410 : bool = aten::eq(%size_prods.425, %27) # torch/nn/functional.py:1994:7
           = prim::If(%410) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %411 : Tensor = aten::batch_norm(%concated_features.49, %398, %399, %396, %397, %395, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.97 : Tensor = aten::relu_(%411) # torch/nn/functional.py:1117:17
      %413 : Tensor = prim::GetAttr[name="weight"](%388)
      %414 : Tensor? = prim::GetAttr[name="bias"](%388)
      %415 : int[] = prim::ListConstruct(%27, %27)
      %416 : int[] = prim::ListConstruct(%24, %24)
      %417 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.97 : Tensor = aten::conv2d(%result.97, %413, %414, %415, %416, %417, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.97)
  %419 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%77)
  %420 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%77)
  %421 : int = aten::dim(%bottleneck_output.96) # torch/nn/modules/batchnorm.py:276:11
  %422 : bool = aten::ne(%421, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%422) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %423 : bool = prim::GetAttr[name="training"](%420)
   = prim::If(%423) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %424 : Tensor = prim::GetAttr[name="num_batches_tracked"](%420)
      %425 : Tensor = aten::add(%424, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%420, %425)
      -> ()
    block1():
      -> ()
  %426 : bool = prim::GetAttr[name="training"](%420)
  %427 : Tensor = prim::GetAttr[name="running_mean"](%420)
  %428 : Tensor = prim::GetAttr[name="running_var"](%420)
  %429 : Tensor = prim::GetAttr[name="weight"](%420)
  %430 : Tensor = prim::GetAttr[name="bias"](%420)
   = prim::If(%426) # torch/nn/functional.py:2011:4
    block0():
      %431 : int[] = aten::size(%bottleneck_output.96) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%431, %24) # torch/nn/functional.py:1991:17
      %433 : int = aten::len(%431) # torch/nn/functional.py:1992:19
      %434 : int = aten::sub(%433, %26) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%434, %25, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.430 : int):
          %438 : int = aten::add(%i.108, %26) # torch/nn/functional.py:1993:27
          %439 : int = aten::__getitem__(%431, %438) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %439) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.431)
      %441 : bool = aten::eq(%size_prods.429, %27) # torch/nn/functional.py:1994:7
       = prim::If(%441) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %442 : Tensor = aten::batch_norm(%bottleneck_output.96, %429, %430, %427, %428, %426, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.98 : Tensor = aten::relu_(%442) # torch/nn/functional.py:1117:17
  %444 : Tensor = prim::GetAttr[name="weight"](%419)
  %445 : Tensor? = prim::GetAttr[name="bias"](%419)
  %446 : int[] = prim::ListConstruct(%27, %27)
  %447 : int[] = prim::ListConstruct(%27, %27)
  %448 : int[] = prim::ListConstruct(%27, %27)
  %new_features.102 : Tensor = aten::conv2d(%result.98, %444, %445, %446, %447, %448, %27) # torch/nn/modules/conv.py:415:15
  %450 : float = prim::GetAttr[name="drop_rate"](%77)
  %451 : bool = aten::gt(%450, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.99 : Tensor = prim::If(%451) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %453 : float = prim::GetAttr[name="drop_rate"](%77)
      %454 : bool = prim::GetAttr[name="training"](%77)
      %455 : bool = aten::lt(%453, %16) # torch/nn/functional.py:968:7
      %456 : bool = prim::If(%455) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %457 : bool = aten::gt(%453, %17) # torch/nn/functional.py:968:17
          -> (%457)
       = prim::If(%456) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %458 : Tensor = aten::dropout(%new_features.102, %453, %454) # torch/nn/functional.py:973:17
      -> (%458)
    block1():
      -> (%new_features.102)
  %459 : Tensor[] = aten::append(%features.2, %new_features.99) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %460 : Tensor = prim::Uninitialized()
  %461 : bool = prim::GetAttr[name="memory_efficient"](%78)
  %462 : bool = prim::If(%461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %463 : bool = prim::Uninitialized()
      %464 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %465 : bool = aten::gt(%464, %24)
      %466 : bool, %467 : bool, %468 : int = prim::Loop(%18, %465, %19, %463, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%469 : int, %470 : bool, %471 : bool, %472 : int):
          %tensor.50 : Tensor = aten::__getitem__(%features.2, %472) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %474 : bool = prim::requires_grad(%tensor.50)
          %475 : bool, %476 : bool = prim::If(%474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %463)
          %477 : int = aten::add(%472, %27)
          %478 : bool = aten::lt(%477, %464)
          %479 : bool = aten::__and__(%478, %475)
          -> (%479, %474, %476, %477)
      %480 : bool = prim::If(%466)
        block0():
          -> (%467)
        block1():
          -> (%19)
      -> (%480)
    block1():
      -> (%19)
  %bottleneck_output.98 : Tensor = prim::If(%462) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%460)
    block1():
      %concated_features.50 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %483 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%78)
      %484 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%78)
      %485 : int = aten::dim(%concated_features.50) # torch/nn/modules/batchnorm.py:276:11
      %486 : bool = aten::ne(%485, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%486) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %487 : bool = prim::GetAttr[name="training"](%484)
       = prim::If(%487) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %488 : Tensor = prim::GetAttr[name="num_batches_tracked"](%484)
          %489 : Tensor = aten::add(%488, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%484, %489)
          -> ()
        block1():
          -> ()
      %490 : bool = prim::GetAttr[name="training"](%484)
      %491 : Tensor = prim::GetAttr[name="running_mean"](%484)
      %492 : Tensor = prim::GetAttr[name="running_var"](%484)
      %493 : Tensor = prim::GetAttr[name="weight"](%484)
      %494 : Tensor = prim::GetAttr[name="bias"](%484)
       = prim::If(%490) # torch/nn/functional.py:2011:4
        block0():
          %495 : int[] = aten::size(%concated_features.50) # torch/nn/functional.py:2012:27
          %size_prods.432 : int = aten::__getitem__(%495, %24) # torch/nn/functional.py:1991:17
          %497 : int = aten::len(%495) # torch/nn/functional.py:1992:19
          %498 : int = aten::sub(%497, %26) # torch/nn/functional.py:1992:19
          %size_prods.433 : int = prim::Loop(%498, %25, %size_prods.432) # torch/nn/functional.py:1992:4
            block0(%i.109 : int, %size_prods.434 : int):
              %502 : int = aten::add(%i.109, %26) # torch/nn/functional.py:1993:27
              %503 : int = aten::__getitem__(%495, %502) # torch/nn/functional.py:1993:22
              %size_prods.435 : int = aten::mul(%size_prods.434, %503) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.435)
          %505 : bool = aten::eq(%size_prods.433, %27) # torch/nn/functional.py:1994:7
           = prim::If(%505) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %506 : Tensor = aten::batch_norm(%concated_features.50, %493, %494, %491, %492, %490, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.99 : Tensor = aten::relu_(%506) # torch/nn/functional.py:1117:17
      %508 : Tensor = prim::GetAttr[name="weight"](%483)
      %509 : Tensor? = prim::GetAttr[name="bias"](%483)
      %510 : int[] = prim::ListConstruct(%27, %27)
      %511 : int[] = prim::ListConstruct(%24, %24)
      %512 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.99 : Tensor = aten::conv2d(%result.99, %508, %509, %510, %511, %512, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.99)
  %514 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%78)
  %515 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%78)
  %516 : int = aten::dim(%bottleneck_output.98) # torch/nn/modules/batchnorm.py:276:11
  %517 : bool = aten::ne(%516, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%517) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %518 : bool = prim::GetAttr[name="training"](%515)
   = prim::If(%518) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %519 : Tensor = prim::GetAttr[name="num_batches_tracked"](%515)
      %520 : Tensor = aten::add(%519, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%515, %520)
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%515)
  %522 : Tensor = prim::GetAttr[name="running_mean"](%515)
  %523 : Tensor = prim::GetAttr[name="running_var"](%515)
  %524 : Tensor = prim::GetAttr[name="weight"](%515)
  %525 : Tensor = prim::GetAttr[name="bias"](%515)
   = prim::If(%521) # torch/nn/functional.py:2011:4
    block0():
      %526 : int[] = aten::size(%bottleneck_output.98) # torch/nn/functional.py:2012:27
      %size_prods.436 : int = aten::__getitem__(%526, %24) # torch/nn/functional.py:1991:17
      %528 : int = aten::len(%526) # torch/nn/functional.py:1992:19
      %529 : int = aten::sub(%528, %26) # torch/nn/functional.py:1992:19
      %size_prods.437 : int = prim::Loop(%529, %25, %size_prods.436) # torch/nn/functional.py:1992:4
        block0(%i.110 : int, %size_prods.438 : int):
          %533 : int = aten::add(%i.110, %26) # torch/nn/functional.py:1993:27
          %534 : int = aten::__getitem__(%526, %533) # torch/nn/functional.py:1993:22
          %size_prods.439 : int = aten::mul(%size_prods.438, %534) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.439)
      %536 : bool = aten::eq(%size_prods.437, %27) # torch/nn/functional.py:1994:7
       = prim::If(%536) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %537 : Tensor = aten::batch_norm(%bottleneck_output.98, %524, %525, %522, %523, %521, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.100 : Tensor = aten::relu_(%537) # torch/nn/functional.py:1117:17
  %539 : Tensor = prim::GetAttr[name="weight"](%514)
  %540 : Tensor? = prim::GetAttr[name="bias"](%514)
  %541 : int[] = prim::ListConstruct(%27, %27)
  %542 : int[] = prim::ListConstruct(%27, %27)
  %543 : int[] = prim::ListConstruct(%27, %27)
  %new_features.104 : Tensor = aten::conv2d(%result.100, %539, %540, %541, %542, %543, %27) # torch/nn/modules/conv.py:415:15
  %545 : float = prim::GetAttr[name="drop_rate"](%78)
  %546 : bool = aten::gt(%545, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.101 : Tensor = prim::If(%546) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %548 : float = prim::GetAttr[name="drop_rate"](%78)
      %549 : bool = prim::GetAttr[name="training"](%78)
      %550 : bool = aten::lt(%548, %16) # torch/nn/functional.py:968:7
      %551 : bool = prim::If(%550) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %552 : bool = aten::gt(%548, %17) # torch/nn/functional.py:968:17
          -> (%552)
       = prim::If(%551) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %553 : Tensor = aten::dropout(%new_features.104, %548, %549) # torch/nn/functional.py:973:17
      -> (%553)
    block1():
      -> (%new_features.104)
  %554 : Tensor[] = aten::append(%features.2, %new_features.101) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %555 : Tensor = prim::Uninitialized()
  %556 : bool = prim::GetAttr[name="memory_efficient"](%79)
  %557 : bool = prim::If(%556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %558 : bool = prim::Uninitialized()
      %559 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %560 : bool = aten::gt(%559, %24)
      %561 : bool, %562 : bool, %563 : int = prim::Loop(%18, %560, %19, %558, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%564 : int, %565 : bool, %566 : bool, %567 : int):
          %tensor.52 : Tensor = aten::__getitem__(%features.2, %567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %569 : bool = prim::requires_grad(%tensor.52)
          %570 : bool, %571 : bool = prim::If(%569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %558)
          %572 : int = aten::add(%567, %27)
          %573 : bool = aten::lt(%572, %559)
          %574 : bool = aten::__and__(%573, %570)
          -> (%574, %569, %571, %572)
      %575 : bool = prim::If(%561)
        block0():
          -> (%562)
        block1():
          -> (%19)
      -> (%575)
    block1():
      -> (%19)
  %bottleneck_output.102 : Tensor = prim::If(%557) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%555)
    block1():
      %concated_features.52 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %578 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%79)
      %579 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%79)
      %580 : int = aten::dim(%concated_features.52) # torch/nn/modules/batchnorm.py:276:11
      %581 : bool = aten::ne(%580, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%581) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %582 : bool = prim::GetAttr[name="training"](%579)
       = prim::If(%582) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %583 : Tensor = prim::GetAttr[name="num_batches_tracked"](%579)
          %584 : Tensor = aten::add(%583, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%579, %584)
          -> ()
        block1():
          -> ()
      %585 : bool = prim::GetAttr[name="training"](%579)
      %586 : Tensor = prim::GetAttr[name="running_mean"](%579)
      %587 : Tensor = prim::GetAttr[name="running_var"](%579)
      %588 : Tensor = prim::GetAttr[name="weight"](%579)
      %589 : Tensor = prim::GetAttr[name="bias"](%579)
       = prim::If(%585) # torch/nn/functional.py:2011:4
        block0():
          %590 : int[] = aten::size(%concated_features.52) # torch/nn/functional.py:2012:27
          %size_prods.440 : int = aten::__getitem__(%590, %24) # torch/nn/functional.py:1991:17
          %592 : int = aten::len(%590) # torch/nn/functional.py:1992:19
          %593 : int = aten::sub(%592, %26) # torch/nn/functional.py:1992:19
          %size_prods.441 : int = prim::Loop(%593, %25, %size_prods.440) # torch/nn/functional.py:1992:4
            block0(%i.111 : int, %size_prods.442 : int):
              %597 : int = aten::add(%i.111, %26) # torch/nn/functional.py:1993:27
              %598 : int = aten::__getitem__(%590, %597) # torch/nn/functional.py:1993:22
              %size_prods.443 : int = aten::mul(%size_prods.442, %598) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.443)
          %600 : bool = aten::eq(%size_prods.441, %27) # torch/nn/functional.py:1994:7
           = prim::If(%600) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %601 : Tensor = aten::batch_norm(%concated_features.52, %588, %589, %586, %587, %585, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.103 : Tensor = aten::relu_(%601) # torch/nn/functional.py:1117:17
      %603 : Tensor = prim::GetAttr[name="weight"](%578)
      %604 : Tensor? = prim::GetAttr[name="bias"](%578)
      %605 : int[] = prim::ListConstruct(%27, %27)
      %606 : int[] = prim::ListConstruct(%24, %24)
      %607 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.103 : Tensor = aten::conv2d(%result.103, %603, %604, %605, %606, %607, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.103)
  %609 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%79)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%79)
  %611 : int = aten::dim(%bottleneck_output.102) # torch/nn/modules/batchnorm.py:276:11
  %612 : bool = aten::ne(%611, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %613 : bool = prim::GetAttr[name="training"](%610)
   = prim::If(%613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%610)
      %615 : Tensor = aten::add(%614, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%610, %615)
      -> ()
    block1():
      -> ()
  %616 : bool = prim::GetAttr[name="training"](%610)
  %617 : Tensor = prim::GetAttr[name="running_mean"](%610)
  %618 : Tensor = prim::GetAttr[name="running_var"](%610)
  %619 : Tensor = prim::GetAttr[name="weight"](%610)
  %620 : Tensor = prim::GetAttr[name="bias"](%610)
   = prim::If(%616) # torch/nn/functional.py:2011:4
    block0():
      %621 : int[] = aten::size(%bottleneck_output.102) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%621, %24) # torch/nn/functional.py:1991:17
      %623 : int = aten::len(%621) # torch/nn/functional.py:1992:19
      %624 : int = aten::sub(%623, %26) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%624, %25, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %628 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
          %629 : int = aten::__getitem__(%621, %628) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %629) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.399)
      %631 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
       = prim::If(%631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %632 : Tensor = aten::batch_norm(%bottleneck_output.102, %619, %620, %617, %618, %616, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.104 : Tensor = aten::relu_(%632) # torch/nn/functional.py:1117:17
  %634 : Tensor = prim::GetAttr[name="weight"](%609)
  %635 : Tensor? = prim::GetAttr[name="bias"](%609)
  %636 : int[] = prim::ListConstruct(%27, %27)
  %637 : int[] = prim::ListConstruct(%27, %27)
  %638 : int[] = prim::ListConstruct(%27, %27)
  %new_features.105 : Tensor = aten::conv2d(%result.104, %634, %635, %636, %637, %638, %27) # torch/nn/modules/conv.py:415:15
  %640 : float = prim::GetAttr[name="drop_rate"](%79)
  %641 : bool = aten::gt(%640, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.103 : Tensor = prim::If(%641) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %643 : float = prim::GetAttr[name="drop_rate"](%79)
      %644 : bool = prim::GetAttr[name="training"](%79)
      %645 : bool = aten::lt(%643, %16) # torch/nn/functional.py:968:7
      %646 : bool = prim::If(%645) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %647 : bool = aten::gt(%643, %17) # torch/nn/functional.py:968:17
          -> (%647)
       = prim::If(%646) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %648 : Tensor = aten::dropout(%new_features.105, %643, %644) # torch/nn/functional.py:973:17
      -> (%648)
    block1():
      -> (%new_features.105)
  %649 : Tensor[] = aten::append(%features.2, %new_features.103) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.11 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %651 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm"](%32)
  %652 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv"](%32)
  %653 : int = aten::dim(%input.11) # torch/nn/modules/batchnorm.py:276:11
  %654 : bool = aten::ne(%653, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%654) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %655 : bool = prim::GetAttr[name="training"](%651)
   = prim::If(%655) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %656 : Tensor = prim::GetAttr[name="num_batches_tracked"](%651)
      %657 : Tensor = aten::add(%656, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%651, %657)
      -> ()
    block1():
      -> ()
  %658 : bool = prim::GetAttr[name="training"](%651)
  %659 : Tensor = prim::GetAttr[name="running_mean"](%651)
  %660 : Tensor = prim::GetAttr[name="running_var"](%651)
  %661 : Tensor = prim::GetAttr[name="weight"](%651)
  %662 : Tensor = prim::GetAttr[name="bias"](%651)
   = prim::If(%658) # torch/nn/functional.py:2011:4
    block0():
      %663 : int[] = aten::size(%input.11) # torch/nn/functional.py:2012:27
      %size_prods.444 : int = aten::__getitem__(%663, %24) # torch/nn/functional.py:1991:17
      %665 : int = aten::len(%663) # torch/nn/functional.py:1992:19
      %666 : int = aten::sub(%665, %26) # torch/nn/functional.py:1992:19
      %size_prods.445 : int = prim::Loop(%666, %25, %size_prods.444) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.446 : int):
          %670 : int = aten::add(%i.112, %26) # torch/nn/functional.py:1993:27
          %671 : int = aten::__getitem__(%663, %670) # torch/nn/functional.py:1993:22
          %size_prods.447 : int = aten::mul(%size_prods.446, %671) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.447)
      %673 : bool = aten::eq(%size_prods.445, %27) # torch/nn/functional.py:1994:7
       = prim::If(%673) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.11, %661, %662, %659, %660, %658, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.12 : Tensor = aten::relu_(%input.16) # torch/nn/functional.py:1117:17
  %676 : Tensor = prim::GetAttr[name="weight"](%652)
  %677 : Tensor? = prim::GetAttr[name="bias"](%652)
  %678 : int[] = prim::ListConstruct(%27, %27)
  %679 : int[] = prim::ListConstruct(%24, %24)
  %680 : int[] = prim::ListConstruct(%27, %27)
  %input.14 : Tensor = aten::conv2d(%input.12, %676, %677, %678, %679, %680, %27) # torch/nn/modules/conv.py:415:15
  %682 : int[] = prim::ListConstruct(%26, %26)
  %683 : int[] = prim::ListConstruct(%26, %26)
  %684 : int[] = prim::ListConstruct(%24, %24)
  %input.13 : Tensor = aten::avg_pool2d(%input.14, %682, %683, %684, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.3 : Tensor[] = prim::ListConstruct(%input.13)
  %687 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer1"](%33)
  %688 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer2"](%33)
  %689 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer3"](%33)
  %690 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer4"](%33)
  %691 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer5"](%33)
  %692 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer6"](%33)
  %693 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer7"](%33)
  %694 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer8"](%33)
  %695 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer9"](%33)
  %696 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer10"](%33)
  %697 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer11"](%33)
  %698 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer12"](%33)
  %699 : Tensor = prim::Uninitialized()
  %700 : bool = prim::GetAttr[name="memory_efficient"](%687)
  %701 : bool = prim::If(%700) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %702 : bool = prim::Uninitialized()
      %703 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %704 : bool = aten::gt(%703, %24)
      %705 : bool, %706 : bool, %707 : int = prim::Loop(%18, %704, %19, %702, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%708 : int, %709 : bool, %710 : bool, %711 : int):
          %tensor.53 : Tensor = aten::__getitem__(%features.3, %711) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %713 : bool = prim::requires_grad(%tensor.53)
          %714 : bool, %715 : bool = prim::If(%713) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %702)
          %716 : int = aten::add(%711, %27)
          %717 : bool = aten::lt(%716, %703)
          %718 : bool = aten::__and__(%717, %714)
          -> (%718, %713, %715, %716)
      %719 : bool = prim::If(%705)
        block0():
          -> (%706)
        block1():
          -> (%19)
      -> (%719)
    block1():
      -> (%19)
  %bottleneck_output.104 : Tensor = prim::If(%701) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%699)
    block1():
      %concated_features.53 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %722 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%687)
      %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%687)
      %724 : int = aten::dim(%concated_features.53) # torch/nn/modules/batchnorm.py:276:11
      %725 : bool = aten::ne(%724, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%725) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %726 : bool = prim::GetAttr[name="training"](%723)
       = prim::If(%726) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
          %728 : Tensor = aten::add(%727, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%723, %728)
          -> ()
        block1():
          -> ()
      %729 : bool = prim::GetAttr[name="training"](%723)
      %730 : Tensor = prim::GetAttr[name="running_mean"](%723)
      %731 : Tensor = prim::GetAttr[name="running_var"](%723)
      %732 : Tensor = prim::GetAttr[name="weight"](%723)
      %733 : Tensor = prim::GetAttr[name="bias"](%723)
       = prim::If(%729) # torch/nn/functional.py:2011:4
        block0():
          %734 : int[] = aten::size(%concated_features.53) # torch/nn/functional.py:2012:27
          %size_prods.452 : int = aten::__getitem__(%734, %24) # torch/nn/functional.py:1991:17
          %736 : int = aten::len(%734) # torch/nn/functional.py:1992:19
          %737 : int = aten::sub(%736, %26) # torch/nn/functional.py:1992:19
          %size_prods.453 : int = prim::Loop(%737, %25, %size_prods.452) # torch/nn/functional.py:1992:4
            block0(%i.114 : int, %size_prods.454 : int):
              %741 : int = aten::add(%i.114, %26) # torch/nn/functional.py:1993:27
              %742 : int = aten::__getitem__(%734, %741) # torch/nn/functional.py:1993:22
              %size_prods.455 : int = aten::mul(%size_prods.454, %742) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.455)
          %744 : bool = aten::eq(%size_prods.453, %27) # torch/nn/functional.py:1994:7
           = prim::If(%744) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %745 : Tensor = aten::batch_norm(%concated_features.53, %732, %733, %730, %731, %729, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.105 : Tensor = aten::relu_(%745) # torch/nn/functional.py:1117:17
      %747 : Tensor = prim::GetAttr[name="weight"](%722)
      %748 : Tensor? = prim::GetAttr[name="bias"](%722)
      %749 : int[] = prim::ListConstruct(%27, %27)
      %750 : int[] = prim::ListConstruct(%24, %24)
      %751 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.105 : Tensor = aten::conv2d(%result.105, %747, %748, %749, %750, %751, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.105)
  %753 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%687)
  %754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%687)
  %755 : int = aten::dim(%bottleneck_output.104) # torch/nn/modules/batchnorm.py:276:11
  %756 : bool = aten::ne(%755, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%756) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %757 : bool = prim::GetAttr[name="training"](%754)
   = prim::If(%757) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %758 : Tensor = prim::GetAttr[name="num_batches_tracked"](%754)
      %759 : Tensor = aten::add(%758, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%754, %759)
      -> ()
    block1():
      -> ()
  %760 : bool = prim::GetAttr[name="training"](%754)
  %761 : Tensor = prim::GetAttr[name="running_mean"](%754)
  %762 : Tensor = prim::GetAttr[name="running_var"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Tensor = prim::GetAttr[name="bias"](%754)
   = prim::If(%760) # torch/nn/functional.py:2011:4
    block0():
      %765 : int[] = aten::size(%bottleneck_output.104) # torch/nn/functional.py:2012:27
      %size_prods.456 : int = aten::__getitem__(%765, %24) # torch/nn/functional.py:1991:17
      %767 : int = aten::len(%765) # torch/nn/functional.py:1992:19
      %768 : int = aten::sub(%767, %26) # torch/nn/functional.py:1992:19
      %size_prods.457 : int = prim::Loop(%768, %25, %size_prods.456) # torch/nn/functional.py:1992:4
        block0(%i.115 : int, %size_prods.458 : int):
          %772 : int = aten::add(%i.115, %26) # torch/nn/functional.py:1993:27
          %773 : int = aten::__getitem__(%765, %772) # torch/nn/functional.py:1993:22
          %size_prods.459 : int = aten::mul(%size_prods.458, %773) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.459)
      %775 : bool = aten::eq(%size_prods.457, %27) # torch/nn/functional.py:1994:7
       = prim::If(%775) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %776 : Tensor = aten::batch_norm(%bottleneck_output.104, %763, %764, %761, %762, %760, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.106 : Tensor = aten::relu_(%776) # torch/nn/functional.py:1117:17
  %778 : Tensor = prim::GetAttr[name="weight"](%753)
  %779 : Tensor? = prim::GetAttr[name="bias"](%753)
  %780 : int[] = prim::ListConstruct(%27, %27)
  %781 : int[] = prim::ListConstruct(%27, %27)
  %782 : int[] = prim::ListConstruct(%27, %27)
  %new_features.107 : Tensor = aten::conv2d(%result.106, %778, %779, %780, %781, %782, %27) # torch/nn/modules/conv.py:415:15
  %784 : float = prim::GetAttr[name="drop_rate"](%687)
  %785 : bool = aten::gt(%784, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.108 : Tensor = prim::If(%785) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %787 : float = prim::GetAttr[name="drop_rate"](%687)
      %788 : bool = prim::GetAttr[name="training"](%687)
      %789 : bool = aten::lt(%787, %16) # torch/nn/functional.py:968:7
      %790 : bool = prim::If(%789) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %791 : bool = aten::gt(%787, %17) # torch/nn/functional.py:968:17
          -> (%791)
       = prim::If(%790) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %792 : Tensor = aten::dropout(%new_features.107, %787, %788) # torch/nn/functional.py:973:17
      -> (%792)
    block1():
      -> (%new_features.107)
  %793 : Tensor[] = aten::append(%features.3, %new_features.108) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %794 : Tensor = prim::Uninitialized()
  %795 : bool = prim::GetAttr[name="memory_efficient"](%688)
  %796 : bool = prim::If(%795) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %797 : bool = prim::Uninitialized()
      %798 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %799 : bool = aten::gt(%798, %24)
      %800 : bool, %801 : bool, %802 : int = prim::Loop(%18, %799, %19, %797, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%803 : int, %804 : bool, %805 : bool, %806 : int):
          %tensor.54 : Tensor = aten::__getitem__(%features.3, %806) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %808 : bool = prim::requires_grad(%tensor.54)
          %809 : bool, %810 : bool = prim::If(%808) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %797)
          %811 : int = aten::add(%806, %27)
          %812 : bool = aten::lt(%811, %798)
          %813 : bool = aten::__and__(%812, %809)
          -> (%813, %808, %810, %811)
      %814 : bool = prim::If(%800)
        block0():
          -> (%801)
        block1():
          -> (%19)
      -> (%814)
    block1():
      -> (%19)
  %bottleneck_output.106 : Tensor = prim::If(%796) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%794)
    block1():
      %concated_features.54 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %817 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%688)
      %818 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%688)
      %819 : int = aten::dim(%concated_features.54) # torch/nn/modules/batchnorm.py:276:11
      %820 : bool = aten::ne(%819, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%820) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %821 : bool = prim::GetAttr[name="training"](%818)
       = prim::If(%821) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %822 : Tensor = prim::GetAttr[name="num_batches_tracked"](%818)
          %823 : Tensor = aten::add(%822, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%818, %823)
          -> ()
        block1():
          -> ()
      %824 : bool = prim::GetAttr[name="training"](%818)
      %825 : Tensor = prim::GetAttr[name="running_mean"](%818)
      %826 : Tensor = prim::GetAttr[name="running_var"](%818)
      %827 : Tensor = prim::GetAttr[name="weight"](%818)
      %828 : Tensor = prim::GetAttr[name="bias"](%818)
       = prim::If(%824) # torch/nn/functional.py:2011:4
        block0():
          %829 : int[] = aten::size(%concated_features.54) # torch/nn/functional.py:2012:27
          %size_prods.460 : int = aten::__getitem__(%829, %24) # torch/nn/functional.py:1991:17
          %831 : int = aten::len(%829) # torch/nn/functional.py:1992:19
          %832 : int = aten::sub(%831, %26) # torch/nn/functional.py:1992:19
          %size_prods.461 : int = prim::Loop(%832, %25, %size_prods.460) # torch/nn/functional.py:1992:4
            block0(%i.116 : int, %size_prods.462 : int):
              %836 : int = aten::add(%i.116, %26) # torch/nn/functional.py:1993:27
              %837 : int = aten::__getitem__(%829, %836) # torch/nn/functional.py:1993:22
              %size_prods.463 : int = aten::mul(%size_prods.462, %837) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.463)
          %839 : bool = aten::eq(%size_prods.461, %27) # torch/nn/functional.py:1994:7
           = prim::If(%839) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %840 : Tensor = aten::batch_norm(%concated_features.54, %827, %828, %825, %826, %824, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.107 : Tensor = aten::relu_(%840) # torch/nn/functional.py:1117:17
      %842 : Tensor = prim::GetAttr[name="weight"](%817)
      %843 : Tensor? = prim::GetAttr[name="bias"](%817)
      %844 : int[] = prim::ListConstruct(%27, %27)
      %845 : int[] = prim::ListConstruct(%24, %24)
      %846 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.107 : Tensor = aten::conv2d(%result.107, %842, %843, %844, %845, %846, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.107)
  %848 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%688)
  %849 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%688)
  %850 : int = aten::dim(%bottleneck_output.106) # torch/nn/modules/batchnorm.py:276:11
  %851 : bool = aten::ne(%850, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%851) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %852 : bool = prim::GetAttr[name="training"](%849)
   = prim::If(%852) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %853 : Tensor = prim::GetAttr[name="num_batches_tracked"](%849)
      %854 : Tensor = aten::add(%853, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%849, %854)
      -> ()
    block1():
      -> ()
  %855 : bool = prim::GetAttr[name="training"](%849)
  %856 : Tensor = prim::GetAttr[name="running_mean"](%849)
  %857 : Tensor = prim::GetAttr[name="running_var"](%849)
  %858 : Tensor = prim::GetAttr[name="weight"](%849)
  %859 : Tensor = prim::GetAttr[name="bias"](%849)
   = prim::If(%855) # torch/nn/functional.py:2011:4
    block0():
      %860 : int[] = aten::size(%bottleneck_output.106) # torch/nn/functional.py:2012:27
      %size_prods.464 : int = aten::__getitem__(%860, %24) # torch/nn/functional.py:1991:17
      %862 : int = aten::len(%860) # torch/nn/functional.py:1992:19
      %863 : int = aten::sub(%862, %26) # torch/nn/functional.py:1992:19
      %size_prods.465 : int = prim::Loop(%863, %25, %size_prods.464) # torch/nn/functional.py:1992:4
        block0(%i.117 : int, %size_prods.466 : int):
          %867 : int = aten::add(%i.117, %26) # torch/nn/functional.py:1993:27
          %868 : int = aten::__getitem__(%860, %867) # torch/nn/functional.py:1993:22
          %size_prods.467 : int = aten::mul(%size_prods.466, %868) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.467)
      %870 : bool = aten::eq(%size_prods.465, %27) # torch/nn/functional.py:1994:7
       = prim::If(%870) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %871 : Tensor = aten::batch_norm(%bottleneck_output.106, %858, %859, %856, %857, %855, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.108 : Tensor = aten::relu_(%871) # torch/nn/functional.py:1117:17
  %873 : Tensor = prim::GetAttr[name="weight"](%848)
  %874 : Tensor? = prim::GetAttr[name="bias"](%848)
  %875 : int[] = prim::ListConstruct(%27, %27)
  %876 : int[] = prim::ListConstruct(%27, %27)
  %877 : int[] = prim::ListConstruct(%27, %27)
  %new_features.109 : Tensor = aten::conv2d(%result.108, %873, %874, %875, %876, %877, %27) # torch/nn/modules/conv.py:415:15
  %879 : float = prim::GetAttr[name="drop_rate"](%688)
  %880 : bool = aten::gt(%879, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.110 : Tensor = prim::If(%880) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %882 : float = prim::GetAttr[name="drop_rate"](%688)
      %883 : bool = prim::GetAttr[name="training"](%688)
      %884 : bool = aten::lt(%882, %16) # torch/nn/functional.py:968:7
      %885 : bool = prim::If(%884) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %886 : bool = aten::gt(%882, %17) # torch/nn/functional.py:968:17
          -> (%886)
       = prim::If(%885) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %887 : Tensor = aten::dropout(%new_features.109, %882, %883) # torch/nn/functional.py:973:17
      -> (%887)
    block1():
      -> (%new_features.109)
  %888 : Tensor[] = aten::append(%features.3, %new_features.110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %889 : Tensor = prim::Uninitialized()
  %890 : bool = prim::GetAttr[name="memory_efficient"](%689)
  %891 : bool = prim::If(%890) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %892 : bool = prim::Uninitialized()
      %893 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %894 : bool = aten::gt(%893, %24)
      %895 : bool, %896 : bool, %897 : int = prim::Loop(%18, %894, %19, %892, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%898 : int, %899 : bool, %900 : bool, %901 : int):
          %tensor.55 : Tensor = aten::__getitem__(%features.3, %901) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %903 : bool = prim::requires_grad(%tensor.55)
          %904 : bool, %905 : bool = prim::If(%903) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %892)
          %906 : int = aten::add(%901, %27)
          %907 : bool = aten::lt(%906, %893)
          %908 : bool = aten::__and__(%907, %904)
          -> (%908, %903, %905, %906)
      %909 : bool = prim::If(%895)
        block0():
          -> (%896)
        block1():
          -> (%19)
      -> (%909)
    block1():
      -> (%19)
  %bottleneck_output.108 : Tensor = prim::If(%891) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%889)
    block1():
      %concated_features.55 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %912 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%689)
      %913 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%689)
      %914 : int = aten::dim(%concated_features.55) # torch/nn/modules/batchnorm.py:276:11
      %915 : bool = aten::ne(%914, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%915) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %916 : bool = prim::GetAttr[name="training"](%913)
       = prim::If(%916) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %917 : Tensor = prim::GetAttr[name="num_batches_tracked"](%913)
          %918 : Tensor = aten::add(%917, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%913, %918)
          -> ()
        block1():
          -> ()
      %919 : bool = prim::GetAttr[name="training"](%913)
      %920 : Tensor = prim::GetAttr[name="running_mean"](%913)
      %921 : Tensor = prim::GetAttr[name="running_var"](%913)
      %922 : Tensor = prim::GetAttr[name="weight"](%913)
      %923 : Tensor = prim::GetAttr[name="bias"](%913)
       = prim::If(%919) # torch/nn/functional.py:2011:4
        block0():
          %924 : int[] = aten::size(%concated_features.55) # torch/nn/functional.py:2012:27
          %size_prods.468 : int = aten::__getitem__(%924, %24) # torch/nn/functional.py:1991:17
          %926 : int = aten::len(%924) # torch/nn/functional.py:1992:19
          %927 : int = aten::sub(%926, %26) # torch/nn/functional.py:1992:19
          %size_prods.469 : int = prim::Loop(%927, %25, %size_prods.468) # torch/nn/functional.py:1992:4
            block0(%i.118 : int, %size_prods.470 : int):
              %931 : int = aten::add(%i.118, %26) # torch/nn/functional.py:1993:27
              %932 : int = aten::__getitem__(%924, %931) # torch/nn/functional.py:1993:22
              %size_prods.471 : int = aten::mul(%size_prods.470, %932) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.471)
          %934 : bool = aten::eq(%size_prods.469, %27) # torch/nn/functional.py:1994:7
           = prim::If(%934) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %935 : Tensor = aten::batch_norm(%concated_features.55, %922, %923, %920, %921, %919, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.109 : Tensor = aten::relu_(%935) # torch/nn/functional.py:1117:17
      %937 : Tensor = prim::GetAttr[name="weight"](%912)
      %938 : Tensor? = prim::GetAttr[name="bias"](%912)
      %939 : int[] = prim::ListConstruct(%27, %27)
      %940 : int[] = prim::ListConstruct(%24, %24)
      %941 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.109 : Tensor = aten::conv2d(%result.109, %937, %938, %939, %940, %941, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.109)
  %943 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%689)
  %944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%689)
  %945 : int = aten::dim(%bottleneck_output.108) # torch/nn/modules/batchnorm.py:276:11
  %946 : bool = aten::ne(%945, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%946) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %947 : bool = prim::GetAttr[name="training"](%944)
   = prim::If(%947) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %948 : Tensor = prim::GetAttr[name="num_batches_tracked"](%944)
      %949 : Tensor = aten::add(%948, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%944, %949)
      -> ()
    block1():
      -> ()
  %950 : bool = prim::GetAttr[name="training"](%944)
  %951 : Tensor = prim::GetAttr[name="running_mean"](%944)
  %952 : Tensor = prim::GetAttr[name="running_var"](%944)
  %953 : Tensor = prim::GetAttr[name="weight"](%944)
  %954 : Tensor = prim::GetAttr[name="bias"](%944)
   = prim::If(%950) # torch/nn/functional.py:2011:4
    block0():
      %955 : int[] = aten::size(%bottleneck_output.108) # torch/nn/functional.py:2012:27
      %size_prods.472 : int = aten::__getitem__(%955, %24) # torch/nn/functional.py:1991:17
      %957 : int = aten::len(%955) # torch/nn/functional.py:1992:19
      %958 : int = aten::sub(%957, %26) # torch/nn/functional.py:1992:19
      %size_prods.473 : int = prim::Loop(%958, %25, %size_prods.472) # torch/nn/functional.py:1992:4
        block0(%i.119 : int, %size_prods.474 : int):
          %962 : int = aten::add(%i.119, %26) # torch/nn/functional.py:1993:27
          %963 : int = aten::__getitem__(%955, %962) # torch/nn/functional.py:1993:22
          %size_prods.475 : int = aten::mul(%size_prods.474, %963) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.475)
      %965 : bool = aten::eq(%size_prods.473, %27) # torch/nn/functional.py:1994:7
       = prim::If(%965) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %966 : Tensor = aten::batch_norm(%bottleneck_output.108, %953, %954, %951, %952, %950, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.110 : Tensor = aten::relu_(%966) # torch/nn/functional.py:1117:17
  %968 : Tensor = prim::GetAttr[name="weight"](%943)
  %969 : Tensor? = prim::GetAttr[name="bias"](%943)
  %970 : int[] = prim::ListConstruct(%27, %27)
  %971 : int[] = prim::ListConstruct(%27, %27)
  %972 : int[] = prim::ListConstruct(%27, %27)
  %new_features.111 : Tensor = aten::conv2d(%result.110, %968, %969, %970, %971, %972, %27) # torch/nn/modules/conv.py:415:15
  %974 : float = prim::GetAttr[name="drop_rate"](%689)
  %975 : bool = aten::gt(%974, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.112 : Tensor = prim::If(%975) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %977 : float = prim::GetAttr[name="drop_rate"](%689)
      %978 : bool = prim::GetAttr[name="training"](%689)
      %979 : bool = aten::lt(%977, %16) # torch/nn/functional.py:968:7
      %980 : bool = prim::If(%979) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %981 : bool = aten::gt(%977, %17) # torch/nn/functional.py:968:17
          -> (%981)
       = prim::If(%980) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %982 : Tensor = aten::dropout(%new_features.111, %977, %978) # torch/nn/functional.py:973:17
      -> (%982)
    block1():
      -> (%new_features.111)
  %983 : Tensor[] = aten::append(%features.3, %new_features.112) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %984 : Tensor = prim::Uninitialized()
  %985 : bool = prim::GetAttr[name="memory_efficient"](%690)
  %986 : bool = prim::If(%985) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %987 : bool = prim::Uninitialized()
      %988 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %989 : bool = aten::gt(%988, %24)
      %990 : bool, %991 : bool, %992 : int = prim::Loop(%18, %989, %19, %987, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%993 : int, %994 : bool, %995 : bool, %996 : int):
          %tensor.56 : Tensor = aten::__getitem__(%features.3, %996) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %998 : bool = prim::requires_grad(%tensor.56)
          %999 : bool, %1000 : bool = prim::If(%998) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %987)
          %1001 : int = aten::add(%996, %27)
          %1002 : bool = aten::lt(%1001, %988)
          %1003 : bool = aten::__and__(%1002, %999)
          -> (%1003, %998, %1000, %1001)
      %1004 : bool = prim::If(%990)
        block0():
          -> (%991)
        block1():
          -> (%19)
      -> (%1004)
    block1():
      -> (%19)
  %bottleneck_output.110 : Tensor = prim::If(%986) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%984)
    block1():
      %concated_features.56 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1007 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%690)
      %1008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%690)
      %1009 : int = aten::dim(%concated_features.56) # torch/nn/modules/batchnorm.py:276:11
      %1010 : bool = aten::ne(%1009, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1010) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1011 : bool = prim::GetAttr[name="training"](%1008)
       = prim::If(%1011) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1008)
          %1013 : Tensor = aten::add(%1012, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1008, %1013)
          -> ()
        block1():
          -> ()
      %1014 : bool = prim::GetAttr[name="training"](%1008)
      %1015 : Tensor = prim::GetAttr[name="running_mean"](%1008)
      %1016 : Tensor = prim::GetAttr[name="running_var"](%1008)
      %1017 : Tensor = prim::GetAttr[name="weight"](%1008)
      %1018 : Tensor = prim::GetAttr[name="bias"](%1008)
       = prim::If(%1014) # torch/nn/functional.py:2011:4
        block0():
          %1019 : int[] = aten::size(%concated_features.56) # torch/nn/functional.py:2012:27
          %size_prods.476 : int = aten::__getitem__(%1019, %24) # torch/nn/functional.py:1991:17
          %1021 : int = aten::len(%1019) # torch/nn/functional.py:1992:19
          %1022 : int = aten::sub(%1021, %26) # torch/nn/functional.py:1992:19
          %size_prods.477 : int = prim::Loop(%1022, %25, %size_prods.476) # torch/nn/functional.py:1992:4
            block0(%i.120 : int, %size_prods.478 : int):
              %1026 : int = aten::add(%i.120, %26) # torch/nn/functional.py:1993:27
              %1027 : int = aten::__getitem__(%1019, %1026) # torch/nn/functional.py:1993:22
              %size_prods.479 : int = aten::mul(%size_prods.478, %1027) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.479)
          %1029 : bool = aten::eq(%size_prods.477, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1029) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1030 : Tensor = aten::batch_norm(%concated_features.56, %1017, %1018, %1015, %1016, %1014, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.111 : Tensor = aten::relu_(%1030) # torch/nn/functional.py:1117:17
      %1032 : Tensor = prim::GetAttr[name="weight"](%1007)
      %1033 : Tensor? = prim::GetAttr[name="bias"](%1007)
      %1034 : int[] = prim::ListConstruct(%27, %27)
      %1035 : int[] = prim::ListConstruct(%24, %24)
      %1036 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.111 : Tensor = aten::conv2d(%result.111, %1032, %1033, %1034, %1035, %1036, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.111)
  %1038 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%690)
  %1039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%690)
  %1040 : int = aten::dim(%bottleneck_output.110) # torch/nn/modules/batchnorm.py:276:11
  %1041 : bool = aten::ne(%1040, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1042 : bool = prim::GetAttr[name="training"](%1039)
   = prim::If(%1042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1039)
      %1044 : Tensor = aten::add(%1043, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1039, %1044)
      -> ()
    block1():
      -> ()
  %1045 : bool = prim::GetAttr[name="training"](%1039)
  %1046 : Tensor = prim::GetAttr[name="running_mean"](%1039)
  %1047 : Tensor = prim::GetAttr[name="running_var"](%1039)
  %1048 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1039)
   = prim::If(%1045) # torch/nn/functional.py:2011:4
    block0():
      %1050 : int[] = aten::size(%bottleneck_output.110) # torch/nn/functional.py:2012:27
      %size_prods.480 : int = aten::__getitem__(%1050, %24) # torch/nn/functional.py:1991:17
      %1052 : int = aten::len(%1050) # torch/nn/functional.py:1992:19
      %1053 : int = aten::sub(%1052, %26) # torch/nn/functional.py:1992:19
      %size_prods.481 : int = prim::Loop(%1053, %25, %size_prods.480) # torch/nn/functional.py:1992:4
        block0(%i.121 : int, %size_prods.482 : int):
          %1057 : int = aten::add(%i.121, %26) # torch/nn/functional.py:1993:27
          %1058 : int = aten::__getitem__(%1050, %1057) # torch/nn/functional.py:1993:22
          %size_prods.483 : int = aten::mul(%size_prods.482, %1058) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.483)
      %1060 : bool = aten::eq(%size_prods.481, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1061 : Tensor = aten::batch_norm(%bottleneck_output.110, %1048, %1049, %1046, %1047, %1045, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.112 : Tensor = aten::relu_(%1061) # torch/nn/functional.py:1117:17
  %1063 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1064 : Tensor? = prim::GetAttr[name="bias"](%1038)
  %1065 : int[] = prim::ListConstruct(%27, %27)
  %1066 : int[] = prim::ListConstruct(%27, %27)
  %1067 : int[] = prim::ListConstruct(%27, %27)
  %new_features.113 : Tensor = aten::conv2d(%result.112, %1063, %1064, %1065, %1066, %1067, %27) # torch/nn/modules/conv.py:415:15
  %1069 : float = prim::GetAttr[name="drop_rate"](%690)
  %1070 : bool = aten::gt(%1069, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.114 : Tensor = prim::If(%1070) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1072 : float = prim::GetAttr[name="drop_rate"](%690)
      %1073 : bool = prim::GetAttr[name="training"](%690)
      %1074 : bool = aten::lt(%1072, %16) # torch/nn/functional.py:968:7
      %1075 : bool = prim::If(%1074) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1076 : bool = aten::gt(%1072, %17) # torch/nn/functional.py:968:17
          -> (%1076)
       = prim::If(%1075) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1077 : Tensor = aten::dropout(%new_features.113, %1072, %1073) # torch/nn/functional.py:973:17
      -> (%1077)
    block1():
      -> (%new_features.113)
  %1078 : Tensor[] = aten::append(%features.3, %new_features.114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1079 : Tensor = prim::Uninitialized()
  %1080 : bool = prim::GetAttr[name="memory_efficient"](%691)
  %1081 : bool = prim::If(%1080) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1082 : bool = prim::Uninitialized()
      %1083 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1084 : bool = aten::gt(%1083, %24)
      %1085 : bool, %1086 : bool, %1087 : int = prim::Loop(%18, %1084, %19, %1082, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1088 : int, %1089 : bool, %1090 : bool, %1091 : int):
          %tensor.57 : Tensor = aten::__getitem__(%features.3, %1091) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1093 : bool = prim::requires_grad(%tensor.57)
          %1094 : bool, %1095 : bool = prim::If(%1093) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1082)
          %1096 : int = aten::add(%1091, %27)
          %1097 : bool = aten::lt(%1096, %1083)
          %1098 : bool = aten::__and__(%1097, %1094)
          -> (%1098, %1093, %1095, %1096)
      %1099 : bool = prim::If(%1085)
        block0():
          -> (%1086)
        block1():
          -> (%19)
      -> (%1099)
    block1():
      -> (%19)
  %bottleneck_output.112 : Tensor = prim::If(%1081) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1079)
    block1():
      %concated_features.57 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1102 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%691)
      %1103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%691)
      %1104 : int = aten::dim(%concated_features.57) # torch/nn/modules/batchnorm.py:276:11
      %1105 : bool = aten::ne(%1104, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1105) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1106 : bool = prim::GetAttr[name="training"](%1103)
       = prim::If(%1106) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1103)
          %1108 : Tensor = aten::add(%1107, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1103, %1108)
          -> ()
        block1():
          -> ()
      %1109 : bool = prim::GetAttr[name="training"](%1103)
      %1110 : Tensor = prim::GetAttr[name="running_mean"](%1103)
      %1111 : Tensor = prim::GetAttr[name="running_var"](%1103)
      %1112 : Tensor = prim::GetAttr[name="weight"](%1103)
      %1113 : Tensor = prim::GetAttr[name="bias"](%1103)
       = prim::If(%1109) # torch/nn/functional.py:2011:4
        block0():
          %1114 : int[] = aten::size(%concated_features.57) # torch/nn/functional.py:2012:27
          %size_prods.484 : int = aten::__getitem__(%1114, %24) # torch/nn/functional.py:1991:17
          %1116 : int = aten::len(%1114) # torch/nn/functional.py:1992:19
          %1117 : int = aten::sub(%1116, %26) # torch/nn/functional.py:1992:19
          %size_prods.485 : int = prim::Loop(%1117, %25, %size_prods.484) # torch/nn/functional.py:1992:4
            block0(%i.122 : int, %size_prods.486 : int):
              %1121 : int = aten::add(%i.122, %26) # torch/nn/functional.py:1993:27
              %1122 : int = aten::__getitem__(%1114, %1121) # torch/nn/functional.py:1993:22
              %size_prods.487 : int = aten::mul(%size_prods.486, %1122) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.487)
          %1124 : bool = aten::eq(%size_prods.485, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1124) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1125 : Tensor = aten::batch_norm(%concated_features.57, %1112, %1113, %1110, %1111, %1109, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.113 : Tensor = aten::relu_(%1125) # torch/nn/functional.py:1117:17
      %1127 : Tensor = prim::GetAttr[name="weight"](%1102)
      %1128 : Tensor? = prim::GetAttr[name="bias"](%1102)
      %1129 : int[] = prim::ListConstruct(%27, %27)
      %1130 : int[] = prim::ListConstruct(%24, %24)
      %1131 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.113 : Tensor = aten::conv2d(%result.113, %1127, %1128, %1129, %1130, %1131, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.113)
  %1133 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%691)
  %1134 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%691)
  %1135 : int = aten::dim(%bottleneck_output.112) # torch/nn/modules/batchnorm.py:276:11
  %1136 : bool = aten::ne(%1135, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1136) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1137 : bool = prim::GetAttr[name="training"](%1134)
   = prim::If(%1137) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1138 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1134)
      %1139 : Tensor = aten::add(%1138, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1134, %1139)
      -> ()
    block1():
      -> ()
  %1140 : bool = prim::GetAttr[name="training"](%1134)
  %1141 : Tensor = prim::GetAttr[name="running_mean"](%1134)
  %1142 : Tensor = prim::GetAttr[name="running_var"](%1134)
  %1143 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
   = prim::If(%1140) # torch/nn/functional.py:2011:4
    block0():
      %1145 : int[] = aten::size(%bottleneck_output.112) # torch/nn/functional.py:2012:27
      %size_prods.488 : int = aten::__getitem__(%1145, %24) # torch/nn/functional.py:1991:17
      %1147 : int = aten::len(%1145) # torch/nn/functional.py:1992:19
      %1148 : int = aten::sub(%1147, %26) # torch/nn/functional.py:1992:19
      %size_prods.489 : int = prim::Loop(%1148, %25, %size_prods.488) # torch/nn/functional.py:1992:4
        block0(%i.123 : int, %size_prods.490 : int):
          %1152 : int = aten::add(%i.123, %26) # torch/nn/functional.py:1993:27
          %1153 : int = aten::__getitem__(%1145, %1152) # torch/nn/functional.py:1993:22
          %size_prods.491 : int = aten::mul(%size_prods.490, %1153) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.491)
      %1155 : bool = aten::eq(%size_prods.489, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1155) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1156 : Tensor = aten::batch_norm(%bottleneck_output.112, %1143, %1144, %1141, %1142, %1140, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.114 : Tensor = aten::relu_(%1156) # torch/nn/functional.py:1117:17
  %1158 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1159 : Tensor? = prim::GetAttr[name="bias"](%1133)
  %1160 : int[] = prim::ListConstruct(%27, %27)
  %1161 : int[] = prim::ListConstruct(%27, %27)
  %1162 : int[] = prim::ListConstruct(%27, %27)
  %new_features.115 : Tensor = aten::conv2d(%result.114, %1158, %1159, %1160, %1161, %1162, %27) # torch/nn/modules/conv.py:415:15
  %1164 : float = prim::GetAttr[name="drop_rate"](%691)
  %1165 : bool = aten::gt(%1164, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.116 : Tensor = prim::If(%1165) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1167 : float = prim::GetAttr[name="drop_rate"](%691)
      %1168 : bool = prim::GetAttr[name="training"](%691)
      %1169 : bool = aten::lt(%1167, %16) # torch/nn/functional.py:968:7
      %1170 : bool = prim::If(%1169) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1171 : bool = aten::gt(%1167, %17) # torch/nn/functional.py:968:17
          -> (%1171)
       = prim::If(%1170) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1172 : Tensor = aten::dropout(%new_features.115, %1167, %1168) # torch/nn/functional.py:973:17
      -> (%1172)
    block1():
      -> (%new_features.115)
  %1173 : Tensor[] = aten::append(%features.3, %new_features.116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1174 : Tensor = prim::Uninitialized()
  %1175 : bool = prim::GetAttr[name="memory_efficient"](%692)
  %1176 : bool = prim::If(%1175) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1177 : bool = prim::Uninitialized()
      %1178 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1179 : bool = aten::gt(%1178, %24)
      %1180 : bool, %1181 : bool, %1182 : int = prim::Loop(%18, %1179, %19, %1177, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1183 : int, %1184 : bool, %1185 : bool, %1186 : int):
          %tensor.58 : Tensor = aten::__getitem__(%features.3, %1186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1188 : bool = prim::requires_grad(%tensor.58)
          %1189 : bool, %1190 : bool = prim::If(%1188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1177)
          %1191 : int = aten::add(%1186, %27)
          %1192 : bool = aten::lt(%1191, %1178)
          %1193 : bool = aten::__and__(%1192, %1189)
          -> (%1193, %1188, %1190, %1191)
      %1194 : bool = prim::If(%1180)
        block0():
          -> (%1181)
        block1():
          -> (%19)
      -> (%1194)
    block1():
      -> (%19)
  %bottleneck_output.114 : Tensor = prim::If(%1176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1174)
    block1():
      %concated_features.58 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1197 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%692)
      %1198 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%692)
      %1199 : int = aten::dim(%concated_features.58) # torch/nn/modules/batchnorm.py:276:11
      %1200 : bool = aten::ne(%1199, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1200) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1201 : bool = prim::GetAttr[name="training"](%1198)
       = prim::If(%1201) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1202 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1198)
          %1203 : Tensor = aten::add(%1202, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1198, %1203)
          -> ()
        block1():
          -> ()
      %1204 : bool = prim::GetAttr[name="training"](%1198)
      %1205 : Tensor = prim::GetAttr[name="running_mean"](%1198)
      %1206 : Tensor = prim::GetAttr[name="running_var"](%1198)
      %1207 : Tensor = prim::GetAttr[name="weight"](%1198)
      %1208 : Tensor = prim::GetAttr[name="bias"](%1198)
       = prim::If(%1204) # torch/nn/functional.py:2011:4
        block0():
          %1209 : int[] = aten::size(%concated_features.58) # torch/nn/functional.py:2012:27
          %size_prods.492 : int = aten::__getitem__(%1209, %24) # torch/nn/functional.py:1991:17
          %1211 : int = aten::len(%1209) # torch/nn/functional.py:1992:19
          %1212 : int = aten::sub(%1211, %26) # torch/nn/functional.py:1992:19
          %size_prods.493 : int = prim::Loop(%1212, %25, %size_prods.492) # torch/nn/functional.py:1992:4
            block0(%i.124 : int, %size_prods.494 : int):
              %1216 : int = aten::add(%i.124, %26) # torch/nn/functional.py:1993:27
              %1217 : int = aten::__getitem__(%1209, %1216) # torch/nn/functional.py:1993:22
              %size_prods.495 : int = aten::mul(%size_prods.494, %1217) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.495)
          %1219 : bool = aten::eq(%size_prods.493, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1219) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1220 : Tensor = aten::batch_norm(%concated_features.58, %1207, %1208, %1205, %1206, %1204, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.115 : Tensor = aten::relu_(%1220) # torch/nn/functional.py:1117:17
      %1222 : Tensor = prim::GetAttr[name="weight"](%1197)
      %1223 : Tensor? = prim::GetAttr[name="bias"](%1197)
      %1224 : int[] = prim::ListConstruct(%27, %27)
      %1225 : int[] = prim::ListConstruct(%24, %24)
      %1226 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.115 : Tensor = aten::conv2d(%result.115, %1222, %1223, %1224, %1225, %1226, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.115)
  %1228 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%692)
  %1229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%692)
  %1230 : int = aten::dim(%bottleneck_output.114) # torch/nn/modules/batchnorm.py:276:11
  %1231 : bool = aten::ne(%1230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1229)
   = prim::If(%1232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1229)
      %1234 : Tensor = aten::add(%1233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1229, %1234)
      -> ()
    block1():
      -> ()
  %1235 : bool = prim::GetAttr[name="training"](%1229)
  %1236 : Tensor = prim::GetAttr[name="running_mean"](%1229)
  %1237 : Tensor = prim::GetAttr[name="running_var"](%1229)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1239 : Tensor = prim::GetAttr[name="bias"](%1229)
   = prim::If(%1235) # torch/nn/functional.py:2011:4
    block0():
      %1240 : int[] = aten::size(%bottleneck_output.114) # torch/nn/functional.py:2012:27
      %size_prods.496 : int = aten::__getitem__(%1240, %24) # torch/nn/functional.py:1991:17
      %1242 : int = aten::len(%1240) # torch/nn/functional.py:1992:19
      %1243 : int = aten::sub(%1242, %26) # torch/nn/functional.py:1992:19
      %size_prods.497 : int = prim::Loop(%1243, %25, %size_prods.496) # torch/nn/functional.py:1992:4
        block0(%i.125 : int, %size_prods.498 : int):
          %1247 : int = aten::add(%i.125, %26) # torch/nn/functional.py:1993:27
          %1248 : int = aten::__getitem__(%1240, %1247) # torch/nn/functional.py:1993:22
          %size_prods.499 : int = aten::mul(%size_prods.498, %1248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.499)
      %1250 : bool = aten::eq(%size_prods.497, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1251 : Tensor = aten::batch_norm(%bottleneck_output.114, %1238, %1239, %1236, %1237, %1235, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.116 : Tensor = aten::relu_(%1251) # torch/nn/functional.py:1117:17
  %1253 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1254 : Tensor? = prim::GetAttr[name="bias"](%1228)
  %1255 : int[] = prim::ListConstruct(%27, %27)
  %1256 : int[] = prim::ListConstruct(%27, %27)
  %1257 : int[] = prim::ListConstruct(%27, %27)
  %new_features.117 : Tensor = aten::conv2d(%result.116, %1253, %1254, %1255, %1256, %1257, %27) # torch/nn/modules/conv.py:415:15
  %1259 : float = prim::GetAttr[name="drop_rate"](%692)
  %1260 : bool = aten::gt(%1259, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.118 : Tensor = prim::If(%1260) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1262 : float = prim::GetAttr[name="drop_rate"](%692)
      %1263 : bool = prim::GetAttr[name="training"](%692)
      %1264 : bool = aten::lt(%1262, %16) # torch/nn/functional.py:968:7
      %1265 : bool = prim::If(%1264) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1266 : bool = aten::gt(%1262, %17) # torch/nn/functional.py:968:17
          -> (%1266)
       = prim::If(%1265) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1267 : Tensor = aten::dropout(%new_features.117, %1262, %1263) # torch/nn/functional.py:973:17
      -> (%1267)
    block1():
      -> (%new_features.117)
  %1268 : Tensor[] = aten::append(%features.3, %new_features.118) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1269 : Tensor = prim::Uninitialized()
  %1270 : bool = prim::GetAttr[name="memory_efficient"](%693)
  %1271 : bool = prim::If(%1270) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1272 : bool = prim::Uninitialized()
      %1273 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1274 : bool = aten::gt(%1273, %24)
      %1275 : bool, %1276 : bool, %1277 : int = prim::Loop(%18, %1274, %19, %1272, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1278 : int, %1279 : bool, %1280 : bool, %1281 : int):
          %tensor.59 : Tensor = aten::__getitem__(%features.3, %1281) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1283 : bool = prim::requires_grad(%tensor.59)
          %1284 : bool, %1285 : bool = prim::If(%1283) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1272)
          %1286 : int = aten::add(%1281, %27)
          %1287 : bool = aten::lt(%1286, %1273)
          %1288 : bool = aten::__and__(%1287, %1284)
          -> (%1288, %1283, %1285, %1286)
      %1289 : bool = prim::If(%1275)
        block0():
          -> (%1276)
        block1():
          -> (%19)
      -> (%1289)
    block1():
      -> (%19)
  %bottleneck_output.116 : Tensor = prim::If(%1271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1269)
    block1():
      %concated_features.59 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1292 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%693)
      %1293 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%693)
      %1294 : int = aten::dim(%concated_features.59) # torch/nn/modules/batchnorm.py:276:11
      %1295 : bool = aten::ne(%1294, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1295) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1296 : bool = prim::GetAttr[name="training"](%1293)
       = prim::If(%1296) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1297 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1293)
          %1298 : Tensor = aten::add(%1297, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1293, %1298)
          -> ()
        block1():
          -> ()
      %1299 : bool = prim::GetAttr[name="training"](%1293)
      %1300 : Tensor = prim::GetAttr[name="running_mean"](%1293)
      %1301 : Tensor = prim::GetAttr[name="running_var"](%1293)
      %1302 : Tensor = prim::GetAttr[name="weight"](%1293)
      %1303 : Tensor = prim::GetAttr[name="bias"](%1293)
       = prim::If(%1299) # torch/nn/functional.py:2011:4
        block0():
          %1304 : int[] = aten::size(%concated_features.59) # torch/nn/functional.py:2012:27
          %size_prods.500 : int = aten::__getitem__(%1304, %24) # torch/nn/functional.py:1991:17
          %1306 : int = aten::len(%1304) # torch/nn/functional.py:1992:19
          %1307 : int = aten::sub(%1306, %26) # torch/nn/functional.py:1992:19
          %size_prods.501 : int = prim::Loop(%1307, %25, %size_prods.500) # torch/nn/functional.py:1992:4
            block0(%i.126 : int, %size_prods.502 : int):
              %1311 : int = aten::add(%i.126, %26) # torch/nn/functional.py:1993:27
              %1312 : int = aten::__getitem__(%1304, %1311) # torch/nn/functional.py:1993:22
              %size_prods.503 : int = aten::mul(%size_prods.502, %1312) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.503)
          %1314 : bool = aten::eq(%size_prods.501, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1314) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1315 : Tensor = aten::batch_norm(%concated_features.59, %1302, %1303, %1300, %1301, %1299, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.117 : Tensor = aten::relu_(%1315) # torch/nn/functional.py:1117:17
      %1317 : Tensor = prim::GetAttr[name="weight"](%1292)
      %1318 : Tensor? = prim::GetAttr[name="bias"](%1292)
      %1319 : int[] = prim::ListConstruct(%27, %27)
      %1320 : int[] = prim::ListConstruct(%24, %24)
      %1321 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.117 : Tensor = aten::conv2d(%result.117, %1317, %1318, %1319, %1320, %1321, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.117)
  %1323 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%693)
  %1324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%693)
  %1325 : int = aten::dim(%bottleneck_output.116) # torch/nn/modules/batchnorm.py:276:11
  %1326 : bool = aten::ne(%1325, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1326) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1327 : bool = prim::GetAttr[name="training"](%1324)
   = prim::If(%1327) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1328 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1324)
      %1329 : Tensor = aten::add(%1328, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1324, %1329)
      -> ()
    block1():
      -> ()
  %1330 : bool = prim::GetAttr[name="training"](%1324)
  %1331 : Tensor = prim::GetAttr[name="running_mean"](%1324)
  %1332 : Tensor = prim::GetAttr[name="running_var"](%1324)
  %1333 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1334 : Tensor = prim::GetAttr[name="bias"](%1324)
   = prim::If(%1330) # torch/nn/functional.py:2011:4
    block0():
      %1335 : int[] = aten::size(%bottleneck_output.116) # torch/nn/functional.py:2012:27
      %size_prods.504 : int = aten::__getitem__(%1335, %24) # torch/nn/functional.py:1991:17
      %1337 : int = aten::len(%1335) # torch/nn/functional.py:1992:19
      %1338 : int = aten::sub(%1337, %26) # torch/nn/functional.py:1992:19
      %size_prods.505 : int = prim::Loop(%1338, %25, %size_prods.504) # torch/nn/functional.py:1992:4
        block0(%i.127 : int, %size_prods.506 : int):
          %1342 : int = aten::add(%i.127, %26) # torch/nn/functional.py:1993:27
          %1343 : int = aten::__getitem__(%1335, %1342) # torch/nn/functional.py:1993:22
          %size_prods.507 : int = aten::mul(%size_prods.506, %1343) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.507)
      %1345 : bool = aten::eq(%size_prods.505, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1345) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1346 : Tensor = aten::batch_norm(%bottleneck_output.116, %1333, %1334, %1331, %1332, %1330, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.118 : Tensor = aten::relu_(%1346) # torch/nn/functional.py:1117:17
  %1348 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1349 : Tensor? = prim::GetAttr[name="bias"](%1323)
  %1350 : int[] = prim::ListConstruct(%27, %27)
  %1351 : int[] = prim::ListConstruct(%27, %27)
  %1352 : int[] = prim::ListConstruct(%27, %27)
  %new_features.119 : Tensor = aten::conv2d(%result.118, %1348, %1349, %1350, %1351, %1352, %27) # torch/nn/modules/conv.py:415:15
  %1354 : float = prim::GetAttr[name="drop_rate"](%693)
  %1355 : bool = aten::gt(%1354, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.120 : Tensor = prim::If(%1355) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1357 : float = prim::GetAttr[name="drop_rate"](%693)
      %1358 : bool = prim::GetAttr[name="training"](%693)
      %1359 : bool = aten::lt(%1357, %16) # torch/nn/functional.py:968:7
      %1360 : bool = prim::If(%1359) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1361 : bool = aten::gt(%1357, %17) # torch/nn/functional.py:968:17
          -> (%1361)
       = prim::If(%1360) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1362 : Tensor = aten::dropout(%new_features.119, %1357, %1358) # torch/nn/functional.py:973:17
      -> (%1362)
    block1():
      -> (%new_features.119)
  %1363 : Tensor[] = aten::append(%features.3, %new_features.120) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1364 : Tensor = prim::Uninitialized()
  %1365 : bool = prim::GetAttr[name="memory_efficient"](%694)
  %1366 : bool = prim::If(%1365) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1367 : bool = prim::Uninitialized()
      %1368 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1369 : bool = aten::gt(%1368, %24)
      %1370 : bool, %1371 : bool, %1372 : int = prim::Loop(%18, %1369, %19, %1367, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1373 : int, %1374 : bool, %1375 : bool, %1376 : int):
          %tensor.60 : Tensor = aten::__getitem__(%features.3, %1376) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1378 : bool = prim::requires_grad(%tensor.60)
          %1379 : bool, %1380 : bool = prim::If(%1378) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1367)
          %1381 : int = aten::add(%1376, %27)
          %1382 : bool = aten::lt(%1381, %1368)
          %1383 : bool = aten::__and__(%1382, %1379)
          -> (%1383, %1378, %1380, %1381)
      %1384 : bool = prim::If(%1370)
        block0():
          -> (%1371)
        block1():
          -> (%19)
      -> (%1384)
    block1():
      -> (%19)
  %bottleneck_output.118 : Tensor = prim::If(%1366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1364)
    block1():
      %concated_features.60 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1387 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%694)
      %1388 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%694)
      %1389 : int = aten::dim(%concated_features.60) # torch/nn/modules/batchnorm.py:276:11
      %1390 : bool = aten::ne(%1389, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1390) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1391 : bool = prim::GetAttr[name="training"](%1388)
       = prim::If(%1391) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1388)
          %1393 : Tensor = aten::add(%1392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1388, %1393)
          -> ()
        block1():
          -> ()
      %1394 : bool = prim::GetAttr[name="training"](%1388)
      %1395 : Tensor = prim::GetAttr[name="running_mean"](%1388)
      %1396 : Tensor = prim::GetAttr[name="running_var"](%1388)
      %1397 : Tensor = prim::GetAttr[name="weight"](%1388)
      %1398 : Tensor = prim::GetAttr[name="bias"](%1388)
       = prim::If(%1394) # torch/nn/functional.py:2011:4
        block0():
          %1399 : int[] = aten::size(%concated_features.60) # torch/nn/functional.py:2012:27
          %size_prods.508 : int = aten::__getitem__(%1399, %24) # torch/nn/functional.py:1991:17
          %1401 : int = aten::len(%1399) # torch/nn/functional.py:1992:19
          %1402 : int = aten::sub(%1401, %26) # torch/nn/functional.py:1992:19
          %size_prods.509 : int = prim::Loop(%1402, %25, %size_prods.508) # torch/nn/functional.py:1992:4
            block0(%i.128 : int, %size_prods.510 : int):
              %1406 : int = aten::add(%i.128, %26) # torch/nn/functional.py:1993:27
              %1407 : int = aten::__getitem__(%1399, %1406) # torch/nn/functional.py:1993:22
              %size_prods.511 : int = aten::mul(%size_prods.510, %1407) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.511)
          %1409 : bool = aten::eq(%size_prods.509, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1409) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1410 : Tensor = aten::batch_norm(%concated_features.60, %1397, %1398, %1395, %1396, %1394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.119 : Tensor = aten::relu_(%1410) # torch/nn/functional.py:1117:17
      %1412 : Tensor = prim::GetAttr[name="weight"](%1387)
      %1413 : Tensor? = prim::GetAttr[name="bias"](%1387)
      %1414 : int[] = prim::ListConstruct(%27, %27)
      %1415 : int[] = prim::ListConstruct(%24, %24)
      %1416 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.119 : Tensor = aten::conv2d(%result.119, %1412, %1413, %1414, %1415, %1416, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.119)
  %1418 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%694)
  %1419 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%694)
  %1420 : int = aten::dim(%bottleneck_output.118) # torch/nn/modules/batchnorm.py:276:11
  %1421 : bool = aten::ne(%1420, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1421) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1422 : bool = prim::GetAttr[name="training"](%1419)
   = prim::If(%1422) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1423 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1419)
      %1424 : Tensor = aten::add(%1423, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1419, %1424)
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1419)
  %1426 : Tensor = prim::GetAttr[name="running_mean"](%1419)
  %1427 : Tensor = prim::GetAttr[name="running_var"](%1419)
  %1428 : Tensor = prim::GetAttr[name="weight"](%1419)
  %1429 : Tensor = prim::GetAttr[name="bias"](%1419)
   = prim::If(%1425) # torch/nn/functional.py:2011:4
    block0():
      %1430 : int[] = aten::size(%bottleneck_output.118) # torch/nn/functional.py:2012:27
      %size_prods.512 : int = aten::__getitem__(%1430, %24) # torch/nn/functional.py:1991:17
      %1432 : int = aten::len(%1430) # torch/nn/functional.py:1992:19
      %1433 : int = aten::sub(%1432, %26) # torch/nn/functional.py:1992:19
      %size_prods.513 : int = prim::Loop(%1433, %25, %size_prods.512) # torch/nn/functional.py:1992:4
        block0(%i.129 : int, %size_prods.514 : int):
          %1437 : int = aten::add(%i.129, %26) # torch/nn/functional.py:1993:27
          %1438 : int = aten::__getitem__(%1430, %1437) # torch/nn/functional.py:1993:22
          %size_prods.515 : int = aten::mul(%size_prods.514, %1438) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.515)
      %1440 : bool = aten::eq(%size_prods.513, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1440) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1441 : Tensor = aten::batch_norm(%bottleneck_output.118, %1428, %1429, %1426, %1427, %1425, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.120 : Tensor = aten::relu_(%1441) # torch/nn/functional.py:1117:17
  %1443 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1444 : Tensor? = prim::GetAttr[name="bias"](%1418)
  %1445 : int[] = prim::ListConstruct(%27, %27)
  %1446 : int[] = prim::ListConstruct(%27, %27)
  %1447 : int[] = prim::ListConstruct(%27, %27)
  %new_features.121 : Tensor = aten::conv2d(%result.120, %1443, %1444, %1445, %1446, %1447, %27) # torch/nn/modules/conv.py:415:15
  %1449 : float = prim::GetAttr[name="drop_rate"](%694)
  %1450 : bool = aten::gt(%1449, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.122 : Tensor = prim::If(%1450) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1452 : float = prim::GetAttr[name="drop_rate"](%694)
      %1453 : bool = prim::GetAttr[name="training"](%694)
      %1454 : bool = aten::lt(%1452, %16) # torch/nn/functional.py:968:7
      %1455 : bool = prim::If(%1454) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1456 : bool = aten::gt(%1452, %17) # torch/nn/functional.py:968:17
          -> (%1456)
       = prim::If(%1455) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1457 : Tensor = aten::dropout(%new_features.121, %1452, %1453) # torch/nn/functional.py:973:17
      -> (%1457)
    block1():
      -> (%new_features.121)
  %1458 : Tensor[] = aten::append(%features.3, %new_features.122) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1459 : Tensor = prim::Uninitialized()
  %1460 : bool = prim::GetAttr[name="memory_efficient"](%695)
  %1461 : bool = prim::If(%1460) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1462 : bool = prim::Uninitialized()
      %1463 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1464 : bool = aten::gt(%1463, %24)
      %1465 : bool, %1466 : bool, %1467 : int = prim::Loop(%18, %1464, %19, %1462, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1468 : int, %1469 : bool, %1470 : bool, %1471 : int):
          %tensor.61 : Tensor = aten::__getitem__(%features.3, %1471) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1473 : bool = prim::requires_grad(%tensor.61)
          %1474 : bool, %1475 : bool = prim::If(%1473) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1462)
          %1476 : int = aten::add(%1471, %27)
          %1477 : bool = aten::lt(%1476, %1463)
          %1478 : bool = aten::__and__(%1477, %1474)
          -> (%1478, %1473, %1475, %1476)
      %1479 : bool = prim::If(%1465)
        block0():
          -> (%1466)
        block1():
          -> (%19)
      -> (%1479)
    block1():
      -> (%19)
  %bottleneck_output.120 : Tensor = prim::If(%1461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1459)
    block1():
      %concated_features.61 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1482 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%695)
      %1483 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%695)
      %1484 : int = aten::dim(%concated_features.61) # torch/nn/modules/batchnorm.py:276:11
      %1485 : bool = aten::ne(%1484, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1485) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1486 : bool = prim::GetAttr[name="training"](%1483)
       = prim::If(%1486) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1483)
          %1488 : Tensor = aten::add(%1487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1483, %1488)
          -> ()
        block1():
          -> ()
      %1489 : bool = prim::GetAttr[name="training"](%1483)
      %1490 : Tensor = prim::GetAttr[name="running_mean"](%1483)
      %1491 : Tensor = prim::GetAttr[name="running_var"](%1483)
      %1492 : Tensor = prim::GetAttr[name="weight"](%1483)
      %1493 : Tensor = prim::GetAttr[name="bias"](%1483)
       = prim::If(%1489) # torch/nn/functional.py:2011:4
        block0():
          %1494 : int[] = aten::size(%concated_features.61) # torch/nn/functional.py:2012:27
          %size_prods.516 : int = aten::__getitem__(%1494, %24) # torch/nn/functional.py:1991:17
          %1496 : int = aten::len(%1494) # torch/nn/functional.py:1992:19
          %1497 : int = aten::sub(%1496, %26) # torch/nn/functional.py:1992:19
          %size_prods.517 : int = prim::Loop(%1497, %25, %size_prods.516) # torch/nn/functional.py:1992:4
            block0(%i.130 : int, %size_prods.518 : int):
              %1501 : int = aten::add(%i.130, %26) # torch/nn/functional.py:1993:27
              %1502 : int = aten::__getitem__(%1494, %1501) # torch/nn/functional.py:1993:22
              %size_prods.519 : int = aten::mul(%size_prods.518, %1502) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.519)
          %1504 : bool = aten::eq(%size_prods.517, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1504) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1505 : Tensor = aten::batch_norm(%concated_features.61, %1492, %1493, %1490, %1491, %1489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.121 : Tensor = aten::relu_(%1505) # torch/nn/functional.py:1117:17
      %1507 : Tensor = prim::GetAttr[name="weight"](%1482)
      %1508 : Tensor? = prim::GetAttr[name="bias"](%1482)
      %1509 : int[] = prim::ListConstruct(%27, %27)
      %1510 : int[] = prim::ListConstruct(%24, %24)
      %1511 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.121 : Tensor = aten::conv2d(%result.121, %1507, %1508, %1509, %1510, %1511, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.121)
  %1513 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%695)
  %1514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%695)
  %1515 : int = aten::dim(%bottleneck_output.120) # torch/nn/modules/batchnorm.py:276:11
  %1516 : bool = aten::ne(%1515, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1516) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1517 : bool = prim::GetAttr[name="training"](%1514)
   = prim::If(%1517) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1518 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1514)
      %1519 : Tensor = aten::add(%1518, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1514, %1519)
      -> ()
    block1():
      -> ()
  %1520 : bool = prim::GetAttr[name="training"](%1514)
  %1521 : Tensor = prim::GetAttr[name="running_mean"](%1514)
  %1522 : Tensor = prim::GetAttr[name="running_var"](%1514)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1524 : Tensor = prim::GetAttr[name="bias"](%1514)
   = prim::If(%1520) # torch/nn/functional.py:2011:4
    block0():
      %1525 : int[] = aten::size(%bottleneck_output.120) # torch/nn/functional.py:2012:27
      %size_prods.520 : int = aten::__getitem__(%1525, %24) # torch/nn/functional.py:1991:17
      %1527 : int = aten::len(%1525) # torch/nn/functional.py:1992:19
      %1528 : int = aten::sub(%1527, %26) # torch/nn/functional.py:1992:19
      %size_prods.521 : int = prim::Loop(%1528, %25, %size_prods.520) # torch/nn/functional.py:1992:4
        block0(%i.131 : int, %size_prods.522 : int):
          %1532 : int = aten::add(%i.131, %26) # torch/nn/functional.py:1993:27
          %1533 : int = aten::__getitem__(%1525, %1532) # torch/nn/functional.py:1993:22
          %size_prods.523 : int = aten::mul(%size_prods.522, %1533) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.523)
      %1535 : bool = aten::eq(%size_prods.521, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1535) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1536 : Tensor = aten::batch_norm(%bottleneck_output.120, %1523, %1524, %1521, %1522, %1520, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.122 : Tensor = aten::relu_(%1536) # torch/nn/functional.py:1117:17
  %1538 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1539 : Tensor? = prim::GetAttr[name="bias"](%1513)
  %1540 : int[] = prim::ListConstruct(%27, %27)
  %1541 : int[] = prim::ListConstruct(%27, %27)
  %1542 : int[] = prim::ListConstruct(%27, %27)
  %new_features.123 : Tensor = aten::conv2d(%result.122, %1538, %1539, %1540, %1541, %1542, %27) # torch/nn/modules/conv.py:415:15
  %1544 : float = prim::GetAttr[name="drop_rate"](%695)
  %1545 : bool = aten::gt(%1544, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.124 : Tensor = prim::If(%1545) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1547 : float = prim::GetAttr[name="drop_rate"](%695)
      %1548 : bool = prim::GetAttr[name="training"](%695)
      %1549 : bool = aten::lt(%1547, %16) # torch/nn/functional.py:968:7
      %1550 : bool = prim::If(%1549) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1551 : bool = aten::gt(%1547, %17) # torch/nn/functional.py:968:17
          -> (%1551)
       = prim::If(%1550) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1552 : Tensor = aten::dropout(%new_features.123, %1547, %1548) # torch/nn/functional.py:973:17
      -> (%1552)
    block1():
      -> (%new_features.123)
  %1553 : Tensor[] = aten::append(%features.3, %new_features.124) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1554 : Tensor = prim::Uninitialized()
  %1555 : bool = prim::GetAttr[name="memory_efficient"](%696)
  %1556 : bool = prim::If(%1555) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1557 : bool = prim::Uninitialized()
      %1558 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1559 : bool = aten::gt(%1558, %24)
      %1560 : bool, %1561 : bool, %1562 : int = prim::Loop(%18, %1559, %19, %1557, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1563 : int, %1564 : bool, %1565 : bool, %1566 : int):
          %tensor.62 : Tensor = aten::__getitem__(%features.3, %1566) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1568 : bool = prim::requires_grad(%tensor.62)
          %1569 : bool, %1570 : bool = prim::If(%1568) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1557)
          %1571 : int = aten::add(%1566, %27)
          %1572 : bool = aten::lt(%1571, %1558)
          %1573 : bool = aten::__and__(%1572, %1569)
          -> (%1573, %1568, %1570, %1571)
      %1574 : bool = prim::If(%1560)
        block0():
          -> (%1561)
        block1():
          -> (%19)
      -> (%1574)
    block1():
      -> (%19)
  %bottleneck_output.122 : Tensor = prim::If(%1556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1554)
    block1():
      %concated_features.62 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1577 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%696)
      %1578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%696)
      %1579 : int = aten::dim(%concated_features.62) # torch/nn/modules/batchnorm.py:276:11
      %1580 : bool = aten::ne(%1579, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1580) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1581 : bool = prim::GetAttr[name="training"](%1578)
       = prim::If(%1581) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1578)
          %1583 : Tensor = aten::add(%1582, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1578, %1583)
          -> ()
        block1():
          -> ()
      %1584 : bool = prim::GetAttr[name="training"](%1578)
      %1585 : Tensor = prim::GetAttr[name="running_mean"](%1578)
      %1586 : Tensor = prim::GetAttr[name="running_var"](%1578)
      %1587 : Tensor = prim::GetAttr[name="weight"](%1578)
      %1588 : Tensor = prim::GetAttr[name="bias"](%1578)
       = prim::If(%1584) # torch/nn/functional.py:2011:4
        block0():
          %1589 : int[] = aten::size(%concated_features.62) # torch/nn/functional.py:2012:27
          %size_prods.524 : int = aten::__getitem__(%1589, %24) # torch/nn/functional.py:1991:17
          %1591 : int = aten::len(%1589) # torch/nn/functional.py:1992:19
          %1592 : int = aten::sub(%1591, %26) # torch/nn/functional.py:1992:19
          %size_prods.525 : int = prim::Loop(%1592, %25, %size_prods.524) # torch/nn/functional.py:1992:4
            block0(%i.132 : int, %size_prods.526 : int):
              %1596 : int = aten::add(%i.132, %26) # torch/nn/functional.py:1993:27
              %1597 : int = aten::__getitem__(%1589, %1596) # torch/nn/functional.py:1993:22
              %size_prods.527 : int = aten::mul(%size_prods.526, %1597) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.527)
          %1599 : bool = aten::eq(%size_prods.525, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1599) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1600 : Tensor = aten::batch_norm(%concated_features.62, %1587, %1588, %1585, %1586, %1584, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.123 : Tensor = aten::relu_(%1600) # torch/nn/functional.py:1117:17
      %1602 : Tensor = prim::GetAttr[name="weight"](%1577)
      %1603 : Tensor? = prim::GetAttr[name="bias"](%1577)
      %1604 : int[] = prim::ListConstruct(%27, %27)
      %1605 : int[] = prim::ListConstruct(%24, %24)
      %1606 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.123 : Tensor = aten::conv2d(%result.123, %1602, %1603, %1604, %1605, %1606, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.123)
  %1608 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%696)
  %1609 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%696)
  %1610 : int = aten::dim(%bottleneck_output.122) # torch/nn/modules/batchnorm.py:276:11
  %1611 : bool = aten::ne(%1610, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1611) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1612 : bool = prim::GetAttr[name="training"](%1609)
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1613 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1609)
      %1614 : Tensor = aten::add(%1613, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1609, %1614)
      -> ()
    block1():
      -> ()
  %1615 : bool = prim::GetAttr[name="training"](%1609)
  %1616 : Tensor = prim::GetAttr[name="running_mean"](%1609)
  %1617 : Tensor = prim::GetAttr[name="running_var"](%1609)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1609)
  %1619 : Tensor = prim::GetAttr[name="bias"](%1609)
   = prim::If(%1615) # torch/nn/functional.py:2011:4
    block0():
      %1620 : int[] = aten::size(%bottleneck_output.122) # torch/nn/functional.py:2012:27
      %size_prods.528 : int = aten::__getitem__(%1620, %24) # torch/nn/functional.py:1991:17
      %1622 : int = aten::len(%1620) # torch/nn/functional.py:1992:19
      %1623 : int = aten::sub(%1622, %26) # torch/nn/functional.py:1992:19
      %size_prods.529 : int = prim::Loop(%1623, %25, %size_prods.528) # torch/nn/functional.py:1992:4
        block0(%i.133 : int, %size_prods.530 : int):
          %1627 : int = aten::add(%i.133, %26) # torch/nn/functional.py:1993:27
          %1628 : int = aten::__getitem__(%1620, %1627) # torch/nn/functional.py:1993:22
          %size_prods.531 : int = aten::mul(%size_prods.530, %1628) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.531)
      %1630 : bool = aten::eq(%size_prods.529, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1630) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1631 : Tensor = aten::batch_norm(%bottleneck_output.122, %1618, %1619, %1616, %1617, %1615, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.124 : Tensor = aten::relu_(%1631) # torch/nn/functional.py:1117:17
  %1633 : Tensor = prim::GetAttr[name="weight"](%1608)
  %1634 : Tensor? = prim::GetAttr[name="bias"](%1608)
  %1635 : int[] = prim::ListConstruct(%27, %27)
  %1636 : int[] = prim::ListConstruct(%27, %27)
  %1637 : int[] = prim::ListConstruct(%27, %27)
  %new_features.125 : Tensor = aten::conv2d(%result.124, %1633, %1634, %1635, %1636, %1637, %27) # torch/nn/modules/conv.py:415:15
  %1639 : float = prim::GetAttr[name="drop_rate"](%696)
  %1640 : bool = aten::gt(%1639, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.126 : Tensor = prim::If(%1640) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1642 : float = prim::GetAttr[name="drop_rate"](%696)
      %1643 : bool = prim::GetAttr[name="training"](%696)
      %1644 : bool = aten::lt(%1642, %16) # torch/nn/functional.py:968:7
      %1645 : bool = prim::If(%1644) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1646 : bool = aten::gt(%1642, %17) # torch/nn/functional.py:968:17
          -> (%1646)
       = prim::If(%1645) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1647 : Tensor = aten::dropout(%new_features.125, %1642, %1643) # torch/nn/functional.py:973:17
      -> (%1647)
    block1():
      -> (%new_features.125)
  %1648 : Tensor[] = aten::append(%features.3, %new_features.126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1649 : Tensor = prim::Uninitialized()
  %1650 : bool = prim::GetAttr[name="memory_efficient"](%697)
  %1651 : bool = prim::If(%1650) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1652 : bool = prim::Uninitialized()
      %1653 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1654 : bool = aten::gt(%1653, %24)
      %1655 : bool, %1656 : bool, %1657 : int = prim::Loop(%18, %1654, %19, %1652, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1658 : int, %1659 : bool, %1660 : bool, %1661 : int):
          %tensor.63 : Tensor = aten::__getitem__(%features.3, %1661) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1663 : bool = prim::requires_grad(%tensor.63)
          %1664 : bool, %1665 : bool = prim::If(%1663) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1652)
          %1666 : int = aten::add(%1661, %27)
          %1667 : bool = aten::lt(%1666, %1653)
          %1668 : bool = aten::__and__(%1667, %1664)
          -> (%1668, %1663, %1665, %1666)
      %1669 : bool = prim::If(%1655)
        block0():
          -> (%1656)
        block1():
          -> (%19)
      -> (%1669)
    block1():
      -> (%19)
  %bottleneck_output.124 : Tensor = prim::If(%1651) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1649)
    block1():
      %concated_features.63 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1672 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%697)
      %1673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%697)
      %1674 : int = aten::dim(%concated_features.63) # torch/nn/modules/batchnorm.py:276:11
      %1675 : bool = aten::ne(%1674, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1675) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1676 : bool = prim::GetAttr[name="training"](%1673)
       = prim::If(%1676) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1673)
          %1678 : Tensor = aten::add(%1677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1673, %1678)
          -> ()
        block1():
          -> ()
      %1679 : bool = prim::GetAttr[name="training"](%1673)
      %1680 : Tensor = prim::GetAttr[name="running_mean"](%1673)
      %1681 : Tensor = prim::GetAttr[name="running_var"](%1673)
      %1682 : Tensor = prim::GetAttr[name="weight"](%1673)
      %1683 : Tensor = prim::GetAttr[name="bias"](%1673)
       = prim::If(%1679) # torch/nn/functional.py:2011:4
        block0():
          %1684 : int[] = aten::size(%concated_features.63) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%1684, %24) # torch/nn/functional.py:1991:17
          %1686 : int = aten::len(%1684) # torch/nn/functional.py:1992:19
          %1687 : int = aten::sub(%1686, %26) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%1687, %25, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %1691 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
              %1692 : int = aten::__getitem__(%1684, %1691) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %1692) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.387)
          %1694 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1694) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1695 : Tensor = aten::batch_norm(%concated_features.63, %1682, %1683, %1680, %1681, %1679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.125 : Tensor = aten::relu_(%1695) # torch/nn/functional.py:1117:17
      %1697 : Tensor = prim::GetAttr[name="weight"](%1672)
      %1698 : Tensor? = prim::GetAttr[name="bias"](%1672)
      %1699 : int[] = prim::ListConstruct(%27, %27)
      %1700 : int[] = prim::ListConstruct(%24, %24)
      %1701 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.125 : Tensor = aten::conv2d(%result.125, %1697, %1698, %1699, %1700, %1701, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.125)
  %1703 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%697)
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%697)
  %1705 : int = aten::dim(%bottleneck_output.124) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%bottleneck_output.124) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%1715, %24) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %26) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%1718, %25, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %1722 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %1723) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.391)
      %1725 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1726 : Tensor = aten::batch_norm(%bottleneck_output.124, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.126 : Tensor = aten::relu_(%1726) # torch/nn/functional.py:1117:17
  %1728 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1729 : Tensor? = prim::GetAttr[name="bias"](%1703)
  %1730 : int[] = prim::ListConstruct(%27, %27)
  %1731 : int[] = prim::ListConstruct(%27, %27)
  %1732 : int[] = prim::ListConstruct(%27, %27)
  %new_features.127 : Tensor = aten::conv2d(%result.126, %1728, %1729, %1730, %1731, %1732, %27) # torch/nn/modules/conv.py:415:15
  %1734 : float = prim::GetAttr[name="drop_rate"](%697)
  %1735 : bool = aten::gt(%1734, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.128 : Tensor = prim::If(%1735) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1737 : float = prim::GetAttr[name="drop_rate"](%697)
      %1738 : bool = prim::GetAttr[name="training"](%697)
      %1739 : bool = aten::lt(%1737, %16) # torch/nn/functional.py:968:7
      %1740 : bool = prim::If(%1739) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1741 : bool = aten::gt(%1737, %17) # torch/nn/functional.py:968:17
          -> (%1741)
       = prim::If(%1740) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1742 : Tensor = aten::dropout(%new_features.127, %1737, %1738) # torch/nn/functional.py:973:17
      -> (%1742)
    block1():
      -> (%new_features.127)
  %1743 : Tensor[] = aten::append(%features.3, %new_features.128) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1744 : Tensor = prim::Uninitialized()
  %1745 : bool = prim::GetAttr[name="memory_efficient"](%698)
  %1746 : bool = prim::If(%1745) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1747 : bool = prim::Uninitialized()
      %1748 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1749 : bool = aten::gt(%1748, %24)
      %1750 : bool, %1751 : bool, %1752 : int = prim::Loop(%18, %1749, %19, %1747, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1753 : int, %1754 : bool, %1755 : bool, %1756 : int):
          %tensor.66 : Tensor = aten::__getitem__(%features.3, %1756) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1758 : bool = prim::requires_grad(%tensor.66)
          %1759 : bool, %1760 : bool = prim::If(%1758) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1747)
          %1761 : int = aten::add(%1756, %27)
          %1762 : bool = aten::lt(%1761, %1748)
          %1763 : bool = aten::__and__(%1762, %1759)
          -> (%1763, %1758, %1760, %1761)
      %1764 : bool = prim::If(%1750)
        block0():
          -> (%1751)
        block1():
          -> (%19)
      -> (%1764)
    block1():
      -> (%19)
  %bottleneck_output.130 : Tensor = prim::If(%1746) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1744)
    block1():
      %concated_features.66 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1767 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%698)
      %1768 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%698)
      %1769 : int = aten::dim(%concated_features.66) # torch/nn/modules/batchnorm.py:276:11
      %1770 : bool = aten::ne(%1769, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1770) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1771 : bool = prim::GetAttr[name="training"](%1768)
       = prim::If(%1771) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1772 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1768)
          %1773 : Tensor = aten::add(%1772, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1768, %1773)
          -> ()
        block1():
          -> ()
      %1774 : bool = prim::GetAttr[name="training"](%1768)
      %1775 : Tensor = prim::GetAttr[name="running_mean"](%1768)
      %1776 : Tensor = prim::GetAttr[name="running_var"](%1768)
      %1777 : Tensor = prim::GetAttr[name="weight"](%1768)
      %1778 : Tensor = prim::GetAttr[name="bias"](%1768)
       = prim::If(%1774) # torch/nn/functional.py:2011:4
        block0():
          %1779 : int[] = aten::size(%concated_features.66) # torch/nn/functional.py:2012:27
          %size_prods.532 : int = aten::__getitem__(%1779, %24) # torch/nn/functional.py:1991:17
          %1781 : int = aten::len(%1779) # torch/nn/functional.py:1992:19
          %1782 : int = aten::sub(%1781, %26) # torch/nn/functional.py:1992:19
          %size_prods.533 : int = prim::Loop(%1782, %25, %size_prods.532) # torch/nn/functional.py:1992:4
            block0(%i.134 : int, %size_prods.534 : int):
              %1786 : int = aten::add(%i.134, %26) # torch/nn/functional.py:1993:27
              %1787 : int = aten::__getitem__(%1779, %1786) # torch/nn/functional.py:1993:22
              %size_prods.535 : int = aten::mul(%size_prods.534, %1787) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.535)
          %1789 : bool = aten::eq(%size_prods.533, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1789) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1790 : Tensor = aten::batch_norm(%concated_features.66, %1777, %1778, %1775, %1776, %1774, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.131 : Tensor = aten::relu_(%1790) # torch/nn/functional.py:1117:17
      %1792 : Tensor = prim::GetAttr[name="weight"](%1767)
      %1793 : Tensor? = prim::GetAttr[name="bias"](%1767)
      %1794 : int[] = prim::ListConstruct(%27, %27)
      %1795 : int[] = prim::ListConstruct(%24, %24)
      %1796 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.131 : Tensor = aten::conv2d(%result.131, %1792, %1793, %1794, %1795, %1796, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.131)
  %1798 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%698)
  %1799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%698)
  %1800 : int = aten::dim(%bottleneck_output.130) # torch/nn/modules/batchnorm.py:276:11
  %1801 : bool = aten::ne(%1800, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1802 : bool = prim::GetAttr[name="training"](%1799)
   = prim::If(%1802) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1803 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1799)
      %1804 : Tensor = aten::add(%1803, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1799, %1804)
      -> ()
    block1():
      -> ()
  %1805 : bool = prim::GetAttr[name="training"](%1799)
  %1806 : Tensor = prim::GetAttr[name="running_mean"](%1799)
  %1807 : Tensor = prim::GetAttr[name="running_var"](%1799)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1799)
  %1809 : Tensor = prim::GetAttr[name="bias"](%1799)
   = prim::If(%1805) # torch/nn/functional.py:2011:4
    block0():
      %1810 : int[] = aten::size(%bottleneck_output.130) # torch/nn/functional.py:2012:27
      %size_prods.448 : int = aten::__getitem__(%1810, %24) # torch/nn/functional.py:1991:17
      %1812 : int = aten::len(%1810) # torch/nn/functional.py:1992:19
      %1813 : int = aten::sub(%1812, %26) # torch/nn/functional.py:1992:19
      %size_prods.449 : int = prim::Loop(%1813, %25, %size_prods.448) # torch/nn/functional.py:1992:4
        block0(%i.113 : int, %size_prods.450 : int):
          %1817 : int = aten::add(%i.113, %26) # torch/nn/functional.py:1993:27
          %1818 : int = aten::__getitem__(%1810, %1817) # torch/nn/functional.py:1993:22
          %size_prods.451 : int = aten::mul(%size_prods.450, %1818) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.451)
      %1820 : bool = aten::eq(%size_prods.449, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1820) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1821 : Tensor = aten::batch_norm(%bottleneck_output.130, %1808, %1809, %1806, %1807, %1805, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.132 : Tensor = aten::relu_(%1821) # torch/nn/functional.py:1117:17
  %1823 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1824 : Tensor? = prim::GetAttr[name="bias"](%1798)
  %1825 : int[] = prim::ListConstruct(%27, %27)
  %1826 : int[] = prim::ListConstruct(%27, %27)
  %1827 : int[] = prim::ListConstruct(%27, %27)
  %new_features.131 : Tensor = aten::conv2d(%result.132, %1823, %1824, %1825, %1826, %1827, %27) # torch/nn/modules/conv.py:415:15
  %1829 : float = prim::GetAttr[name="drop_rate"](%698)
  %1830 : bool = aten::gt(%1829, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.130 : Tensor = prim::If(%1830) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1832 : float = prim::GetAttr[name="drop_rate"](%698)
      %1833 : bool = prim::GetAttr[name="training"](%698)
      %1834 : bool = aten::lt(%1832, %16) # torch/nn/functional.py:968:7
      %1835 : bool = prim::If(%1834) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1836 : bool = aten::gt(%1832, %17) # torch/nn/functional.py:968:17
          -> (%1836)
       = prim::If(%1835) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1837 : Tensor = aten::dropout(%new_features.131, %1832, %1833) # torch/nn/functional.py:973:17
      -> (%1837)
    block1():
      -> (%new_features.131)
  %1838 : Tensor[] = aten::append(%features.3, %new_features.130) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.15 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %1840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm"](%34)
  %1841 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv"](%34)
  %1842 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %1843 : bool = aten::ne(%1842, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1844 : bool = prim::GetAttr[name="training"](%1840)
   = prim::If(%1844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1840)
      %1846 : Tensor = aten::add(%1845, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1840, %1846)
      -> ()
    block1():
      -> ()
  %1847 : bool = prim::GetAttr[name="training"](%1840)
  %1848 : Tensor = prim::GetAttr[name="running_mean"](%1840)
  %1849 : Tensor = prim::GetAttr[name="running_var"](%1840)
  %1850 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1851 : Tensor = prim::GetAttr[name="bias"](%1840)
   = prim::If(%1847) # torch/nn/functional.py:2011:4
    block0():
      %1852 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.536 : int = aten::__getitem__(%1852, %24) # torch/nn/functional.py:1991:17
      %1854 : int = aten::len(%1852) # torch/nn/functional.py:1992:19
      %1855 : int = aten::sub(%1854, %26) # torch/nn/functional.py:1992:19
      %size_prods.537 : int = prim::Loop(%1855, %25, %size_prods.536) # torch/nn/functional.py:1992:4
        block0(%i.135 : int, %size_prods.538 : int):
          %1859 : int = aten::add(%i.135, %26) # torch/nn/functional.py:1993:27
          %1860 : int = aten::__getitem__(%1852, %1859) # torch/nn/functional.py:1993:22
          %size_prods.539 : int = aten::mul(%size_prods.538, %1860) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.539)
      %1862 : bool = aten::eq(%size_prods.537, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.15, %1850, %1851, %1848, %1849, %1847, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.18 : Tensor = aten::relu_(%input.22) # torch/nn/functional.py:1117:17
  %1865 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1866 : Tensor? = prim::GetAttr[name="bias"](%1841)
  %1867 : int[] = prim::ListConstruct(%27, %27)
  %1868 : int[] = prim::ListConstruct(%24, %24)
  %1869 : int[] = prim::ListConstruct(%27, %27)
  %input.20 : Tensor = aten::conv2d(%input.18, %1865, %1866, %1867, %1868, %1869, %27) # torch/nn/modules/conv.py:415:15
  %1871 : int[] = prim::ListConstruct(%26, %26)
  %1872 : int[] = prim::ListConstruct(%26, %26)
  %1873 : int[] = prim::ListConstruct(%24, %24)
  %input.17 : Tensor = aten::avg_pool2d(%input.20, %1871, %1872, %1873, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.4 : Tensor[] = prim::ListConstruct(%input.17)
  %1876 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer1"](%35)
  %1877 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer2"](%35)
  %1878 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer3"](%35)
  %1879 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer4"](%35)
  %1880 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer5"](%35)
  %1881 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer6"](%35)
  %1882 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer7"](%35)
  %1883 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer8"](%35)
  %1884 : __torch__.torchvision.models.densenet.___torch_mangle_111._DenseLayer = prim::GetAttr[name="denselayer9"](%35)
  %1885 : __torch__.torchvision.models.densenet.___torch_mangle_114._DenseLayer = prim::GetAttr[name="denselayer10"](%35)
  %1886 : __torch__.torchvision.models.densenet.___torch_mangle_117._DenseLayer = prim::GetAttr[name="denselayer11"](%35)
  %1887 : __torch__.torchvision.models.densenet.___torch_mangle_120._DenseLayer = prim::GetAttr[name="denselayer12"](%35)
  %1888 : __torch__.torchvision.models.densenet.___torch_mangle_123._DenseLayer = prim::GetAttr[name="denselayer13"](%35)
  %1889 : __torch__.torchvision.models.densenet.___torch_mangle_126._DenseLayer = prim::GetAttr[name="denselayer14"](%35)
  %1890 : __torch__.torchvision.models.densenet.___torch_mangle_129._DenseLayer = prim::GetAttr[name="denselayer15"](%35)
  %1891 : __torch__.torchvision.models.densenet.___torch_mangle_132._DenseLayer = prim::GetAttr[name="denselayer16"](%35)
  %1892 : __torch__.torchvision.models.densenet.___torch_mangle_135._DenseLayer = prim::GetAttr[name="denselayer17"](%35)
  %1893 : __torch__.torchvision.models.densenet.___torch_mangle_138._DenseLayer = prim::GetAttr[name="denselayer18"](%35)
  %1894 : __torch__.torchvision.models.densenet.___torch_mangle_141._DenseLayer = prim::GetAttr[name="denselayer19"](%35)
  %1895 : __torch__.torchvision.models.densenet.___torch_mangle_144._DenseLayer = prim::GetAttr[name="denselayer20"](%35)
  %1896 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer21"](%35)
  %1897 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer22"](%35)
  %1898 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer23"](%35)
  %1899 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer24"](%35)
  %1900 : __torch__.torchvision.models.densenet.___torch_mangle_300._DenseLayer = prim::GetAttr[name="denselayer25"](%35)
  %1901 : __torch__.torchvision.models.densenet.___torch_mangle_302._DenseLayer = prim::GetAttr[name="denselayer26"](%35)
  %1902 : __torch__.torchvision.models.densenet.___torch_mangle_305._DenseLayer = prim::GetAttr[name="denselayer27"](%35)
  %1903 : __torch__.torchvision.models.densenet.___torch_mangle_308._DenseLayer = prim::GetAttr[name="denselayer28"](%35)
  %1904 : __torch__.torchvision.models.densenet.___torch_mangle_310._DenseLayer = prim::GetAttr[name="denselayer29"](%35)
  %1905 : __torch__.torchvision.models.densenet.___torch_mangle_313._DenseLayer = prim::GetAttr[name="denselayer30"](%35)
  %1906 : __torch__.torchvision.models.densenet.___torch_mangle_316._DenseLayer = prim::GetAttr[name="denselayer31"](%35)
  %1907 : __torch__.torchvision.models.densenet.___torch_mangle_318._DenseLayer = prim::GetAttr[name="denselayer32"](%35)
  %1908 : __torch__.torchvision.models.densenet.___torch_mangle_324._DenseLayer = prim::GetAttr[name="denselayer33"](%35)
  %1909 : __torch__.torchvision.models.densenet.___torch_mangle_327._DenseLayer = prim::GetAttr[name="denselayer34"](%35)
  %1910 : __torch__.torchvision.models.densenet.___torch_mangle_329._DenseLayer = prim::GetAttr[name="denselayer35"](%35)
  %1911 : __torch__.torchvision.models.densenet.___torch_mangle_332._DenseLayer = prim::GetAttr[name="denselayer36"](%35)
  %1912 : __torch__.torchvision.models.densenet.___torch_mangle_335._DenseLayer = prim::GetAttr[name="denselayer37"](%35)
  %1913 : __torch__.torchvision.models.densenet.___torch_mangle_337._DenseLayer = prim::GetAttr[name="denselayer38"](%35)
  %1914 : __torch__.torchvision.models.densenet.___torch_mangle_340._DenseLayer = prim::GetAttr[name="denselayer39"](%35)
  %1915 : __torch__.torchvision.models.densenet.___torch_mangle_343._DenseLayer = prim::GetAttr[name="denselayer40"](%35)
  %1916 : __torch__.torchvision.models.densenet.___torch_mangle_345._DenseLayer = prim::GetAttr[name="denselayer41"](%35)
  %1917 : __torch__.torchvision.models.densenet.___torch_mangle_348._DenseLayer = prim::GetAttr[name="denselayer42"](%35)
  %1918 : __torch__.torchvision.models.densenet.___torch_mangle_351._DenseLayer = prim::GetAttr[name="denselayer43"](%35)
  %1919 : __torch__.torchvision.models.densenet.___torch_mangle_353._DenseLayer = prim::GetAttr[name="denselayer44"](%35)
  %1920 : __torch__.torchvision.models.densenet.___torch_mangle_360._DenseLayer = prim::GetAttr[name="denselayer45"](%35)
  %1921 : __torch__.torchvision.models.densenet.___torch_mangle_363._DenseLayer = prim::GetAttr[name="denselayer46"](%35)
  %1922 : __torch__.torchvision.models.densenet.___torch_mangle_365._DenseLayer = prim::GetAttr[name="denselayer47"](%35)
  %1923 : __torch__.torchvision.models.densenet.___torch_mangle_368._DenseLayer = prim::GetAttr[name="denselayer48"](%35)
  %1924 : Tensor = prim::Uninitialized()
  %1925 : bool = prim::GetAttr[name="memory_efficient"](%1876)
  %1926 : bool = prim::If(%1925) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1927 : bool = prim::Uninitialized()
      %1928 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1929 : bool = aten::gt(%1928, %24)
      %1930 : bool, %1931 : bool, %1932 : int = prim::Loop(%18, %1929, %19, %1927, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1933 : int, %1934 : bool, %1935 : bool, %1936 : int):
          %tensor.67 : Tensor = aten::__getitem__(%features.4, %1936) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1938 : bool = prim::requires_grad(%tensor.67)
          %1939 : bool, %1940 : bool = prim::If(%1938) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1927)
          %1941 : int = aten::add(%1936, %27)
          %1942 : bool = aten::lt(%1941, %1928)
          %1943 : bool = aten::__and__(%1942, %1939)
          -> (%1943, %1938, %1940, %1941)
      %1944 : bool = prim::If(%1930)
        block0():
          -> (%1931)
        block1():
          -> (%19)
      -> (%1944)
    block1():
      -> (%19)
  %bottleneck_output.132 : Tensor = prim::If(%1926) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1924)
    block1():
      %concated_features.67 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1947 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%1876)
      %1948 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%1876)
      %1949 : int = aten::dim(%concated_features.67) # torch/nn/modules/batchnorm.py:276:11
      %1950 : bool = aten::ne(%1949, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1950) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1951 : bool = prim::GetAttr[name="training"](%1948)
       = prim::If(%1951) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1952 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1948)
          %1953 : Tensor = aten::add(%1952, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1948, %1953)
          -> ()
        block1():
          -> ()
      %1954 : bool = prim::GetAttr[name="training"](%1948)
      %1955 : Tensor = prim::GetAttr[name="running_mean"](%1948)
      %1956 : Tensor = prim::GetAttr[name="running_var"](%1948)
      %1957 : Tensor = prim::GetAttr[name="weight"](%1948)
      %1958 : Tensor = prim::GetAttr[name="bias"](%1948)
       = prim::If(%1954) # torch/nn/functional.py:2011:4
        block0():
          %1959 : int[] = aten::size(%concated_features.67) # torch/nn/functional.py:2012:27
          %size_prods.544 : int = aten::__getitem__(%1959, %24) # torch/nn/functional.py:1991:17
          %1961 : int = aten::len(%1959) # torch/nn/functional.py:1992:19
          %1962 : int = aten::sub(%1961, %26) # torch/nn/functional.py:1992:19
          %size_prods.545 : int = prim::Loop(%1962, %25, %size_prods.544) # torch/nn/functional.py:1992:4
            block0(%i.137 : int, %size_prods.546 : int):
              %1966 : int = aten::add(%i.137, %26) # torch/nn/functional.py:1993:27
              %1967 : int = aten::__getitem__(%1959, %1966) # torch/nn/functional.py:1993:22
              %size_prods.547 : int = aten::mul(%size_prods.546, %1967) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.547)
          %1969 : bool = aten::eq(%size_prods.545, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1969) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1970 : Tensor = aten::batch_norm(%concated_features.67, %1957, %1958, %1955, %1956, %1954, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.133 : Tensor = aten::relu_(%1970) # torch/nn/functional.py:1117:17
      %1972 : Tensor = prim::GetAttr[name="weight"](%1947)
      %1973 : Tensor? = prim::GetAttr[name="bias"](%1947)
      %1974 : int[] = prim::ListConstruct(%27, %27)
      %1975 : int[] = prim::ListConstruct(%24, %24)
      %1976 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.133 : Tensor = aten::conv2d(%result.133, %1972, %1973, %1974, %1975, %1976, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.133)
  %1978 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1876)
  %1979 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1876)
  %1980 : int = aten::dim(%bottleneck_output.132) # torch/nn/modules/batchnorm.py:276:11
  %1981 : bool = aten::ne(%1980, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1981) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1982 : bool = prim::GetAttr[name="training"](%1979)
   = prim::If(%1982) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1983 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1979)
      %1984 : Tensor = aten::add(%1983, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1979, %1984)
      -> ()
    block1():
      -> ()
  %1985 : bool = prim::GetAttr[name="training"](%1979)
  %1986 : Tensor = prim::GetAttr[name="running_mean"](%1979)
  %1987 : Tensor = prim::GetAttr[name="running_var"](%1979)
  %1988 : Tensor = prim::GetAttr[name="weight"](%1979)
  %1989 : Tensor = prim::GetAttr[name="bias"](%1979)
   = prim::If(%1985) # torch/nn/functional.py:2011:4
    block0():
      %1990 : int[] = aten::size(%bottleneck_output.132) # torch/nn/functional.py:2012:27
      %size_prods.548 : int = aten::__getitem__(%1990, %24) # torch/nn/functional.py:1991:17
      %1992 : int = aten::len(%1990) # torch/nn/functional.py:1992:19
      %1993 : int = aten::sub(%1992, %26) # torch/nn/functional.py:1992:19
      %size_prods.549 : int = prim::Loop(%1993, %25, %size_prods.548) # torch/nn/functional.py:1992:4
        block0(%i.138 : int, %size_prods.550 : int):
          %1997 : int = aten::add(%i.138, %26) # torch/nn/functional.py:1993:27
          %1998 : int = aten::__getitem__(%1990, %1997) # torch/nn/functional.py:1993:22
          %size_prods.551 : int = aten::mul(%size_prods.550, %1998) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.551)
      %2000 : bool = aten::eq(%size_prods.549, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2000) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2001 : Tensor = aten::batch_norm(%bottleneck_output.132, %1988, %1989, %1986, %1987, %1985, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.134 : Tensor = aten::relu_(%2001) # torch/nn/functional.py:1117:17
  %2003 : Tensor = prim::GetAttr[name="weight"](%1978)
  %2004 : Tensor? = prim::GetAttr[name="bias"](%1978)
  %2005 : int[] = prim::ListConstruct(%27, %27)
  %2006 : int[] = prim::ListConstruct(%27, %27)
  %2007 : int[] = prim::ListConstruct(%27, %27)
  %new_features.133 : Tensor = aten::conv2d(%result.134, %2003, %2004, %2005, %2006, %2007, %27) # torch/nn/modules/conv.py:415:15
  %2009 : float = prim::GetAttr[name="drop_rate"](%1876)
  %2010 : bool = aten::gt(%2009, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.134 : Tensor = prim::If(%2010) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2012 : float = prim::GetAttr[name="drop_rate"](%1876)
      %2013 : bool = prim::GetAttr[name="training"](%1876)
      %2014 : bool = aten::lt(%2012, %16) # torch/nn/functional.py:968:7
      %2015 : bool = prim::If(%2014) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2016 : bool = aten::gt(%2012, %17) # torch/nn/functional.py:968:17
          -> (%2016)
       = prim::If(%2015) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2017 : Tensor = aten::dropout(%new_features.133, %2012, %2013) # torch/nn/functional.py:973:17
      -> (%2017)
    block1():
      -> (%new_features.133)
  %2018 : Tensor[] = aten::append(%features.4, %new_features.134) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2019 : Tensor = prim::Uninitialized()
  %2020 : bool = prim::GetAttr[name="memory_efficient"](%1877)
  %2021 : bool = prim::If(%2020) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2022 : bool = prim::Uninitialized()
      %2023 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2024 : bool = aten::gt(%2023, %24)
      %2025 : bool, %2026 : bool, %2027 : int = prim::Loop(%18, %2024, %19, %2022, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2028 : int, %2029 : bool, %2030 : bool, %2031 : int):
          %tensor.68 : Tensor = aten::__getitem__(%features.4, %2031) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2033 : bool = prim::requires_grad(%tensor.68)
          %2034 : bool, %2035 : bool = prim::If(%2033) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2022)
          %2036 : int = aten::add(%2031, %27)
          %2037 : bool = aten::lt(%2036, %2023)
          %2038 : bool = aten::__and__(%2037, %2034)
          -> (%2038, %2033, %2035, %2036)
      %2039 : bool = prim::If(%2025)
        block0():
          -> (%2026)
        block1():
          -> (%19)
      -> (%2039)
    block1():
      -> (%19)
  %bottleneck_output.134 : Tensor = prim::If(%2021) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2019)
    block1():
      %concated_features.68 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2042 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%1877)
      %2043 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%1877)
      %2044 : int = aten::dim(%concated_features.68) # torch/nn/modules/batchnorm.py:276:11
      %2045 : bool = aten::ne(%2044, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2045) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2046 : bool = prim::GetAttr[name="training"](%2043)
       = prim::If(%2046) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2047 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2043)
          %2048 : Tensor = aten::add(%2047, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2043, %2048)
          -> ()
        block1():
          -> ()
      %2049 : bool = prim::GetAttr[name="training"](%2043)
      %2050 : Tensor = prim::GetAttr[name="running_mean"](%2043)
      %2051 : Tensor = prim::GetAttr[name="running_var"](%2043)
      %2052 : Tensor = prim::GetAttr[name="weight"](%2043)
      %2053 : Tensor = prim::GetAttr[name="bias"](%2043)
       = prim::If(%2049) # torch/nn/functional.py:2011:4
        block0():
          %2054 : int[] = aten::size(%concated_features.68) # torch/nn/functional.py:2012:27
          %size_prods.552 : int = aten::__getitem__(%2054, %24) # torch/nn/functional.py:1991:17
          %2056 : int = aten::len(%2054) # torch/nn/functional.py:1992:19
          %2057 : int = aten::sub(%2056, %26) # torch/nn/functional.py:1992:19
          %size_prods.553 : int = prim::Loop(%2057, %25, %size_prods.552) # torch/nn/functional.py:1992:4
            block0(%i.139 : int, %size_prods.554 : int):
              %2061 : int = aten::add(%i.139, %26) # torch/nn/functional.py:1993:27
              %2062 : int = aten::__getitem__(%2054, %2061) # torch/nn/functional.py:1993:22
              %size_prods.555 : int = aten::mul(%size_prods.554, %2062) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.555)
          %2064 : bool = aten::eq(%size_prods.553, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2064) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2065 : Tensor = aten::batch_norm(%concated_features.68, %2052, %2053, %2050, %2051, %2049, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.135 : Tensor = aten::relu_(%2065) # torch/nn/functional.py:1117:17
      %2067 : Tensor = prim::GetAttr[name="weight"](%2042)
      %2068 : Tensor? = prim::GetAttr[name="bias"](%2042)
      %2069 : int[] = prim::ListConstruct(%27, %27)
      %2070 : int[] = prim::ListConstruct(%24, %24)
      %2071 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.135 : Tensor = aten::conv2d(%result.135, %2067, %2068, %2069, %2070, %2071, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.135)
  %2073 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1877)
  %2074 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1877)
  %2075 : int = aten::dim(%bottleneck_output.134) # torch/nn/modules/batchnorm.py:276:11
  %2076 : bool = aten::ne(%2075, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2076) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2077 : bool = prim::GetAttr[name="training"](%2074)
   = prim::If(%2077) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2078 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2074)
      %2079 : Tensor = aten::add(%2078, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2074, %2079)
      -> ()
    block1():
      -> ()
  %2080 : bool = prim::GetAttr[name="training"](%2074)
  %2081 : Tensor = prim::GetAttr[name="running_mean"](%2074)
  %2082 : Tensor = prim::GetAttr[name="running_var"](%2074)
  %2083 : Tensor = prim::GetAttr[name="weight"](%2074)
  %2084 : Tensor = prim::GetAttr[name="bias"](%2074)
   = prim::If(%2080) # torch/nn/functional.py:2011:4
    block0():
      %2085 : int[] = aten::size(%bottleneck_output.134) # torch/nn/functional.py:2012:27
      %size_prods.556 : int = aten::__getitem__(%2085, %24) # torch/nn/functional.py:1991:17
      %2087 : int = aten::len(%2085) # torch/nn/functional.py:1992:19
      %2088 : int = aten::sub(%2087, %26) # torch/nn/functional.py:1992:19
      %size_prods.557 : int = prim::Loop(%2088, %25, %size_prods.556) # torch/nn/functional.py:1992:4
        block0(%i.140 : int, %size_prods.558 : int):
          %2092 : int = aten::add(%i.140, %26) # torch/nn/functional.py:1993:27
          %2093 : int = aten::__getitem__(%2085, %2092) # torch/nn/functional.py:1993:22
          %size_prods.559 : int = aten::mul(%size_prods.558, %2093) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.559)
      %2095 : bool = aten::eq(%size_prods.557, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2095) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2096 : Tensor = aten::batch_norm(%bottleneck_output.134, %2083, %2084, %2081, %2082, %2080, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.136 : Tensor = aten::relu_(%2096) # torch/nn/functional.py:1117:17
  %2098 : Tensor = prim::GetAttr[name="weight"](%2073)
  %2099 : Tensor? = prim::GetAttr[name="bias"](%2073)
  %2100 : int[] = prim::ListConstruct(%27, %27)
  %2101 : int[] = prim::ListConstruct(%27, %27)
  %2102 : int[] = prim::ListConstruct(%27, %27)
  %new_features.135 : Tensor = aten::conv2d(%result.136, %2098, %2099, %2100, %2101, %2102, %27) # torch/nn/modules/conv.py:415:15
  %2104 : float = prim::GetAttr[name="drop_rate"](%1877)
  %2105 : bool = aten::gt(%2104, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.136 : Tensor = prim::If(%2105) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2107 : float = prim::GetAttr[name="drop_rate"](%1877)
      %2108 : bool = prim::GetAttr[name="training"](%1877)
      %2109 : bool = aten::lt(%2107, %16) # torch/nn/functional.py:968:7
      %2110 : bool = prim::If(%2109) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2111 : bool = aten::gt(%2107, %17) # torch/nn/functional.py:968:17
          -> (%2111)
       = prim::If(%2110) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2112 : Tensor = aten::dropout(%new_features.135, %2107, %2108) # torch/nn/functional.py:973:17
      -> (%2112)
    block1():
      -> (%new_features.135)
  %2113 : Tensor[] = aten::append(%features.4, %new_features.136) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2114 : Tensor = prim::Uninitialized()
  %2115 : bool = prim::GetAttr[name="memory_efficient"](%1878)
  %2116 : bool = prim::If(%2115) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2117 : bool = prim::Uninitialized()
      %2118 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2119 : bool = aten::gt(%2118, %24)
      %2120 : bool, %2121 : bool, %2122 : int = prim::Loop(%18, %2119, %19, %2117, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2123 : int, %2124 : bool, %2125 : bool, %2126 : int):
          %tensor.69 : Tensor = aten::__getitem__(%features.4, %2126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2128 : bool = prim::requires_grad(%tensor.69)
          %2129 : bool, %2130 : bool = prim::If(%2128) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2117)
          %2131 : int = aten::add(%2126, %27)
          %2132 : bool = aten::lt(%2131, %2118)
          %2133 : bool = aten::__and__(%2132, %2129)
          -> (%2133, %2128, %2130, %2131)
      %2134 : bool = prim::If(%2120)
        block0():
          -> (%2121)
        block1():
          -> (%19)
      -> (%2134)
    block1():
      -> (%19)
  %bottleneck_output.136 : Tensor = prim::If(%2116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2114)
    block1():
      %concated_features.69 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2137 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%1878)
      %2138 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%1878)
      %2139 : int = aten::dim(%concated_features.69) # torch/nn/modules/batchnorm.py:276:11
      %2140 : bool = aten::ne(%2139, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2140) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2141 : bool = prim::GetAttr[name="training"](%2138)
       = prim::If(%2141) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2142 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2138)
          %2143 : Tensor = aten::add(%2142, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2138, %2143)
          -> ()
        block1():
          -> ()
      %2144 : bool = prim::GetAttr[name="training"](%2138)
      %2145 : Tensor = prim::GetAttr[name="running_mean"](%2138)
      %2146 : Tensor = prim::GetAttr[name="running_var"](%2138)
      %2147 : Tensor = prim::GetAttr[name="weight"](%2138)
      %2148 : Tensor = prim::GetAttr[name="bias"](%2138)
       = prim::If(%2144) # torch/nn/functional.py:2011:4
        block0():
          %2149 : int[] = aten::size(%concated_features.69) # torch/nn/functional.py:2012:27
          %size_prods.560 : int = aten::__getitem__(%2149, %24) # torch/nn/functional.py:1991:17
          %2151 : int = aten::len(%2149) # torch/nn/functional.py:1992:19
          %2152 : int = aten::sub(%2151, %26) # torch/nn/functional.py:1992:19
          %size_prods.561 : int = prim::Loop(%2152, %25, %size_prods.560) # torch/nn/functional.py:1992:4
            block0(%i.141 : int, %size_prods.562 : int):
              %2156 : int = aten::add(%i.141, %26) # torch/nn/functional.py:1993:27
              %2157 : int = aten::__getitem__(%2149, %2156) # torch/nn/functional.py:1993:22
              %size_prods.563 : int = aten::mul(%size_prods.562, %2157) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.563)
          %2159 : bool = aten::eq(%size_prods.561, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2159) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2160 : Tensor = aten::batch_norm(%concated_features.69, %2147, %2148, %2145, %2146, %2144, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.137 : Tensor = aten::relu_(%2160) # torch/nn/functional.py:1117:17
      %2162 : Tensor = prim::GetAttr[name="weight"](%2137)
      %2163 : Tensor? = prim::GetAttr[name="bias"](%2137)
      %2164 : int[] = prim::ListConstruct(%27, %27)
      %2165 : int[] = prim::ListConstruct(%24, %24)
      %2166 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.137 : Tensor = aten::conv2d(%result.137, %2162, %2163, %2164, %2165, %2166, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.137)
  %2168 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1878)
  %2169 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1878)
  %2170 : int = aten::dim(%bottleneck_output.136) # torch/nn/modules/batchnorm.py:276:11
  %2171 : bool = aten::ne(%2170, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2171) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2172 : bool = prim::GetAttr[name="training"](%2169)
   = prim::If(%2172) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2173 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2169)
      %2174 : Tensor = aten::add(%2173, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2169, %2174)
      -> ()
    block1():
      -> ()
  %2175 : bool = prim::GetAttr[name="training"](%2169)
  %2176 : Tensor = prim::GetAttr[name="running_mean"](%2169)
  %2177 : Tensor = prim::GetAttr[name="running_var"](%2169)
  %2178 : Tensor = prim::GetAttr[name="weight"](%2169)
  %2179 : Tensor = prim::GetAttr[name="bias"](%2169)
   = prim::If(%2175) # torch/nn/functional.py:2011:4
    block0():
      %2180 : int[] = aten::size(%bottleneck_output.136) # torch/nn/functional.py:2012:27
      %size_prods.564 : int = aten::__getitem__(%2180, %24) # torch/nn/functional.py:1991:17
      %2182 : int = aten::len(%2180) # torch/nn/functional.py:1992:19
      %2183 : int = aten::sub(%2182, %26) # torch/nn/functional.py:1992:19
      %size_prods.565 : int = prim::Loop(%2183, %25, %size_prods.564) # torch/nn/functional.py:1992:4
        block0(%i.142 : int, %size_prods.566 : int):
          %2187 : int = aten::add(%i.142, %26) # torch/nn/functional.py:1993:27
          %2188 : int = aten::__getitem__(%2180, %2187) # torch/nn/functional.py:1993:22
          %size_prods.567 : int = aten::mul(%size_prods.566, %2188) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.567)
      %2190 : bool = aten::eq(%size_prods.565, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2190) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2191 : Tensor = aten::batch_norm(%bottleneck_output.136, %2178, %2179, %2176, %2177, %2175, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.138 : Tensor = aten::relu_(%2191) # torch/nn/functional.py:1117:17
  %2193 : Tensor = prim::GetAttr[name="weight"](%2168)
  %2194 : Tensor? = prim::GetAttr[name="bias"](%2168)
  %2195 : int[] = prim::ListConstruct(%27, %27)
  %2196 : int[] = prim::ListConstruct(%27, %27)
  %2197 : int[] = prim::ListConstruct(%27, %27)
  %new_features.137 : Tensor = aten::conv2d(%result.138, %2193, %2194, %2195, %2196, %2197, %27) # torch/nn/modules/conv.py:415:15
  %2199 : float = prim::GetAttr[name="drop_rate"](%1878)
  %2200 : bool = aten::gt(%2199, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.138 : Tensor = prim::If(%2200) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2202 : float = prim::GetAttr[name="drop_rate"](%1878)
      %2203 : bool = prim::GetAttr[name="training"](%1878)
      %2204 : bool = aten::lt(%2202, %16) # torch/nn/functional.py:968:7
      %2205 : bool = prim::If(%2204) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2206 : bool = aten::gt(%2202, %17) # torch/nn/functional.py:968:17
          -> (%2206)
       = prim::If(%2205) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2207 : Tensor = aten::dropout(%new_features.137, %2202, %2203) # torch/nn/functional.py:973:17
      -> (%2207)
    block1():
      -> (%new_features.137)
  %2208 : Tensor[] = aten::append(%features.4, %new_features.138) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2209 : Tensor = prim::Uninitialized()
  %2210 : bool = prim::GetAttr[name="memory_efficient"](%1879)
  %2211 : bool = prim::If(%2210) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2212 : bool = prim::Uninitialized()
      %2213 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2214 : bool = aten::gt(%2213, %24)
      %2215 : bool, %2216 : bool, %2217 : int = prim::Loop(%18, %2214, %19, %2212, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2218 : int, %2219 : bool, %2220 : bool, %2221 : int):
          %tensor.70 : Tensor = aten::__getitem__(%features.4, %2221) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2223 : bool = prim::requires_grad(%tensor.70)
          %2224 : bool, %2225 : bool = prim::If(%2223) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2212)
          %2226 : int = aten::add(%2221, %27)
          %2227 : bool = aten::lt(%2226, %2213)
          %2228 : bool = aten::__and__(%2227, %2224)
          -> (%2228, %2223, %2225, %2226)
      %2229 : bool = prim::If(%2215)
        block0():
          -> (%2216)
        block1():
          -> (%19)
      -> (%2229)
    block1():
      -> (%19)
  %bottleneck_output.138 : Tensor = prim::If(%2211) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2209)
    block1():
      %concated_features.70 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2232 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%1879)
      %2233 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%1879)
      %2234 : int = aten::dim(%concated_features.70) # torch/nn/modules/batchnorm.py:276:11
      %2235 : bool = aten::ne(%2234, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2235) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2236 : bool = prim::GetAttr[name="training"](%2233)
       = prim::If(%2236) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2237 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2233)
          %2238 : Tensor = aten::add(%2237, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2233, %2238)
          -> ()
        block1():
          -> ()
      %2239 : bool = prim::GetAttr[name="training"](%2233)
      %2240 : Tensor = prim::GetAttr[name="running_mean"](%2233)
      %2241 : Tensor = prim::GetAttr[name="running_var"](%2233)
      %2242 : Tensor = prim::GetAttr[name="weight"](%2233)
      %2243 : Tensor = prim::GetAttr[name="bias"](%2233)
       = prim::If(%2239) # torch/nn/functional.py:2011:4
        block0():
          %2244 : int[] = aten::size(%concated_features.70) # torch/nn/functional.py:2012:27
          %size_prods.568 : int = aten::__getitem__(%2244, %24) # torch/nn/functional.py:1991:17
          %2246 : int = aten::len(%2244) # torch/nn/functional.py:1992:19
          %2247 : int = aten::sub(%2246, %26) # torch/nn/functional.py:1992:19
          %size_prods.569 : int = prim::Loop(%2247, %25, %size_prods.568) # torch/nn/functional.py:1992:4
            block0(%i.143 : int, %size_prods.570 : int):
              %2251 : int = aten::add(%i.143, %26) # torch/nn/functional.py:1993:27
              %2252 : int = aten::__getitem__(%2244, %2251) # torch/nn/functional.py:1993:22
              %size_prods.571 : int = aten::mul(%size_prods.570, %2252) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.571)
          %2254 : bool = aten::eq(%size_prods.569, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2254) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2255 : Tensor = aten::batch_norm(%concated_features.70, %2242, %2243, %2240, %2241, %2239, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.139 : Tensor = aten::relu_(%2255) # torch/nn/functional.py:1117:17
      %2257 : Tensor = prim::GetAttr[name="weight"](%2232)
      %2258 : Tensor? = prim::GetAttr[name="bias"](%2232)
      %2259 : int[] = prim::ListConstruct(%27, %27)
      %2260 : int[] = prim::ListConstruct(%24, %24)
      %2261 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.139 : Tensor = aten::conv2d(%result.139, %2257, %2258, %2259, %2260, %2261, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.139)
  %2263 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1879)
  %2264 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1879)
  %2265 : int = aten::dim(%bottleneck_output.138) # torch/nn/modules/batchnorm.py:276:11
  %2266 : bool = aten::ne(%2265, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2266) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2267 : bool = prim::GetAttr[name="training"](%2264)
   = prim::If(%2267) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2268 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2264)
      %2269 : Tensor = aten::add(%2268, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2264, %2269)
      -> ()
    block1():
      -> ()
  %2270 : bool = prim::GetAttr[name="training"](%2264)
  %2271 : Tensor = prim::GetAttr[name="running_mean"](%2264)
  %2272 : Tensor = prim::GetAttr[name="running_var"](%2264)
  %2273 : Tensor = prim::GetAttr[name="weight"](%2264)
  %2274 : Tensor = prim::GetAttr[name="bias"](%2264)
   = prim::If(%2270) # torch/nn/functional.py:2011:4
    block0():
      %2275 : int[] = aten::size(%bottleneck_output.138) # torch/nn/functional.py:2012:27
      %size_prods.572 : int = aten::__getitem__(%2275, %24) # torch/nn/functional.py:1991:17
      %2277 : int = aten::len(%2275) # torch/nn/functional.py:1992:19
      %2278 : int = aten::sub(%2277, %26) # torch/nn/functional.py:1992:19
      %size_prods.573 : int = prim::Loop(%2278, %25, %size_prods.572) # torch/nn/functional.py:1992:4
        block0(%i.144 : int, %size_prods.574 : int):
          %2282 : int = aten::add(%i.144, %26) # torch/nn/functional.py:1993:27
          %2283 : int = aten::__getitem__(%2275, %2282) # torch/nn/functional.py:1993:22
          %size_prods.575 : int = aten::mul(%size_prods.574, %2283) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.575)
      %2285 : bool = aten::eq(%size_prods.573, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2285) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2286 : Tensor = aten::batch_norm(%bottleneck_output.138, %2273, %2274, %2271, %2272, %2270, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.140 : Tensor = aten::relu_(%2286) # torch/nn/functional.py:1117:17
  %2288 : Tensor = prim::GetAttr[name="weight"](%2263)
  %2289 : Tensor? = prim::GetAttr[name="bias"](%2263)
  %2290 : int[] = prim::ListConstruct(%27, %27)
  %2291 : int[] = prim::ListConstruct(%27, %27)
  %2292 : int[] = prim::ListConstruct(%27, %27)
  %new_features.139 : Tensor = aten::conv2d(%result.140, %2288, %2289, %2290, %2291, %2292, %27) # torch/nn/modules/conv.py:415:15
  %2294 : float = prim::GetAttr[name="drop_rate"](%1879)
  %2295 : bool = aten::gt(%2294, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.140 : Tensor = prim::If(%2295) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2297 : float = prim::GetAttr[name="drop_rate"](%1879)
      %2298 : bool = prim::GetAttr[name="training"](%1879)
      %2299 : bool = aten::lt(%2297, %16) # torch/nn/functional.py:968:7
      %2300 : bool = prim::If(%2299) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2301 : bool = aten::gt(%2297, %17) # torch/nn/functional.py:968:17
          -> (%2301)
       = prim::If(%2300) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2302 : Tensor = aten::dropout(%new_features.139, %2297, %2298) # torch/nn/functional.py:973:17
      -> (%2302)
    block1():
      -> (%new_features.139)
  %2303 : Tensor[] = aten::append(%features.4, %new_features.140) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2304 : Tensor = prim::Uninitialized()
  %2305 : bool = prim::GetAttr[name="memory_efficient"](%1880)
  %2306 : bool = prim::If(%2305) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2307 : bool = prim::Uninitialized()
      %2308 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2309 : bool = aten::gt(%2308, %24)
      %2310 : bool, %2311 : bool, %2312 : int = prim::Loop(%18, %2309, %19, %2307, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2313 : int, %2314 : bool, %2315 : bool, %2316 : int):
          %tensor.71 : Tensor = aten::__getitem__(%features.4, %2316) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2318 : bool = prim::requires_grad(%tensor.71)
          %2319 : bool, %2320 : bool = prim::If(%2318) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2307)
          %2321 : int = aten::add(%2316, %27)
          %2322 : bool = aten::lt(%2321, %2308)
          %2323 : bool = aten::__and__(%2322, %2319)
          -> (%2323, %2318, %2320, %2321)
      %2324 : bool = prim::If(%2310)
        block0():
          -> (%2311)
        block1():
          -> (%19)
      -> (%2324)
    block1():
      -> (%19)
  %bottleneck_output.140 : Tensor = prim::If(%2306) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2304)
    block1():
      %concated_features.71 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2327 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%1880)
      %2328 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%1880)
      %2329 : int = aten::dim(%concated_features.71) # torch/nn/modules/batchnorm.py:276:11
      %2330 : bool = aten::ne(%2329, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2330) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2331 : bool = prim::GetAttr[name="training"](%2328)
       = prim::If(%2331) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2332 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2328)
          %2333 : Tensor = aten::add(%2332, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2328, %2333)
          -> ()
        block1():
          -> ()
      %2334 : bool = prim::GetAttr[name="training"](%2328)
      %2335 : Tensor = prim::GetAttr[name="running_mean"](%2328)
      %2336 : Tensor = prim::GetAttr[name="running_var"](%2328)
      %2337 : Tensor = prim::GetAttr[name="weight"](%2328)
      %2338 : Tensor = prim::GetAttr[name="bias"](%2328)
       = prim::If(%2334) # torch/nn/functional.py:2011:4
        block0():
          %2339 : int[] = aten::size(%concated_features.71) # torch/nn/functional.py:2012:27
          %size_prods.576 : int = aten::__getitem__(%2339, %24) # torch/nn/functional.py:1991:17
          %2341 : int = aten::len(%2339) # torch/nn/functional.py:1992:19
          %2342 : int = aten::sub(%2341, %26) # torch/nn/functional.py:1992:19
          %size_prods.577 : int = prim::Loop(%2342, %25, %size_prods.576) # torch/nn/functional.py:1992:4
            block0(%i.145 : int, %size_prods.578 : int):
              %2346 : int = aten::add(%i.145, %26) # torch/nn/functional.py:1993:27
              %2347 : int = aten::__getitem__(%2339, %2346) # torch/nn/functional.py:1993:22
              %size_prods.579 : int = aten::mul(%size_prods.578, %2347) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.579)
          %2349 : bool = aten::eq(%size_prods.577, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2349) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2350 : Tensor = aten::batch_norm(%concated_features.71, %2337, %2338, %2335, %2336, %2334, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.141 : Tensor = aten::relu_(%2350) # torch/nn/functional.py:1117:17
      %2352 : Tensor = prim::GetAttr[name="weight"](%2327)
      %2353 : Tensor? = prim::GetAttr[name="bias"](%2327)
      %2354 : int[] = prim::ListConstruct(%27, %27)
      %2355 : int[] = prim::ListConstruct(%24, %24)
      %2356 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.141 : Tensor = aten::conv2d(%result.141, %2352, %2353, %2354, %2355, %2356, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.141)
  %2358 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1880)
  %2359 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1880)
  %2360 : int = aten::dim(%bottleneck_output.140) # torch/nn/modules/batchnorm.py:276:11
  %2361 : bool = aten::ne(%2360, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2361) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2362 : bool = prim::GetAttr[name="training"](%2359)
   = prim::If(%2362) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2363 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2359)
      %2364 : Tensor = aten::add(%2363, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2359, %2364)
      -> ()
    block1():
      -> ()
  %2365 : bool = prim::GetAttr[name="training"](%2359)
  %2366 : Tensor = prim::GetAttr[name="running_mean"](%2359)
  %2367 : Tensor = prim::GetAttr[name="running_var"](%2359)
  %2368 : Tensor = prim::GetAttr[name="weight"](%2359)
  %2369 : Tensor = prim::GetAttr[name="bias"](%2359)
   = prim::If(%2365) # torch/nn/functional.py:2011:4
    block0():
      %2370 : int[] = aten::size(%bottleneck_output.140) # torch/nn/functional.py:2012:27
      %size_prods.580 : int = aten::__getitem__(%2370, %24) # torch/nn/functional.py:1991:17
      %2372 : int = aten::len(%2370) # torch/nn/functional.py:1992:19
      %2373 : int = aten::sub(%2372, %26) # torch/nn/functional.py:1992:19
      %size_prods.581 : int = prim::Loop(%2373, %25, %size_prods.580) # torch/nn/functional.py:1992:4
        block0(%i.146 : int, %size_prods.582 : int):
          %2377 : int = aten::add(%i.146, %26) # torch/nn/functional.py:1993:27
          %2378 : int = aten::__getitem__(%2370, %2377) # torch/nn/functional.py:1993:22
          %size_prods.583 : int = aten::mul(%size_prods.582, %2378) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.583)
      %2380 : bool = aten::eq(%size_prods.581, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2380) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2381 : Tensor = aten::batch_norm(%bottleneck_output.140, %2368, %2369, %2366, %2367, %2365, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.142 : Tensor = aten::relu_(%2381) # torch/nn/functional.py:1117:17
  %2383 : Tensor = prim::GetAttr[name="weight"](%2358)
  %2384 : Tensor? = prim::GetAttr[name="bias"](%2358)
  %2385 : int[] = prim::ListConstruct(%27, %27)
  %2386 : int[] = prim::ListConstruct(%27, %27)
  %2387 : int[] = prim::ListConstruct(%27, %27)
  %new_features.141 : Tensor = aten::conv2d(%result.142, %2383, %2384, %2385, %2386, %2387, %27) # torch/nn/modules/conv.py:415:15
  %2389 : float = prim::GetAttr[name="drop_rate"](%1880)
  %2390 : bool = aten::gt(%2389, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.142 : Tensor = prim::If(%2390) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2392 : float = prim::GetAttr[name="drop_rate"](%1880)
      %2393 : bool = prim::GetAttr[name="training"](%1880)
      %2394 : bool = aten::lt(%2392, %16) # torch/nn/functional.py:968:7
      %2395 : bool = prim::If(%2394) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2396 : bool = aten::gt(%2392, %17) # torch/nn/functional.py:968:17
          -> (%2396)
       = prim::If(%2395) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2397 : Tensor = aten::dropout(%new_features.141, %2392, %2393) # torch/nn/functional.py:973:17
      -> (%2397)
    block1():
      -> (%new_features.141)
  %2398 : Tensor[] = aten::append(%features.4, %new_features.142) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2399 : Tensor = prim::Uninitialized()
  %2400 : bool = prim::GetAttr[name="memory_efficient"](%1881)
  %2401 : bool = prim::If(%2400) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2402 : bool = prim::Uninitialized()
      %2403 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2404 : bool = aten::gt(%2403, %24)
      %2405 : bool, %2406 : bool, %2407 : int = prim::Loop(%18, %2404, %19, %2402, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2408 : int, %2409 : bool, %2410 : bool, %2411 : int):
          %tensor.72 : Tensor = aten::__getitem__(%features.4, %2411) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2413 : bool = prim::requires_grad(%tensor.72)
          %2414 : bool, %2415 : bool = prim::If(%2413) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2402)
          %2416 : int = aten::add(%2411, %27)
          %2417 : bool = aten::lt(%2416, %2403)
          %2418 : bool = aten::__and__(%2417, %2414)
          -> (%2418, %2413, %2415, %2416)
      %2419 : bool = prim::If(%2405)
        block0():
          -> (%2406)
        block1():
          -> (%19)
      -> (%2419)
    block1():
      -> (%19)
  %bottleneck_output.142 : Tensor = prim::If(%2401) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2399)
    block1():
      %concated_features.72 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2422 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%1881)
      %2423 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%1881)
      %2424 : int = aten::dim(%concated_features.72) # torch/nn/modules/batchnorm.py:276:11
      %2425 : bool = aten::ne(%2424, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2425) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2426 : bool = prim::GetAttr[name="training"](%2423)
       = prim::If(%2426) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2427 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2423)
          %2428 : Tensor = aten::add(%2427, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2423, %2428)
          -> ()
        block1():
          -> ()
      %2429 : bool = prim::GetAttr[name="training"](%2423)
      %2430 : Tensor = prim::GetAttr[name="running_mean"](%2423)
      %2431 : Tensor = prim::GetAttr[name="running_var"](%2423)
      %2432 : Tensor = prim::GetAttr[name="weight"](%2423)
      %2433 : Tensor = prim::GetAttr[name="bias"](%2423)
       = prim::If(%2429) # torch/nn/functional.py:2011:4
        block0():
          %2434 : int[] = aten::size(%concated_features.72) # torch/nn/functional.py:2012:27
          %size_prods.584 : int = aten::__getitem__(%2434, %24) # torch/nn/functional.py:1991:17
          %2436 : int = aten::len(%2434) # torch/nn/functional.py:1992:19
          %2437 : int = aten::sub(%2436, %26) # torch/nn/functional.py:1992:19
          %size_prods.585 : int = prim::Loop(%2437, %25, %size_prods.584) # torch/nn/functional.py:1992:4
            block0(%i.147 : int, %size_prods.586 : int):
              %2441 : int = aten::add(%i.147, %26) # torch/nn/functional.py:1993:27
              %2442 : int = aten::__getitem__(%2434, %2441) # torch/nn/functional.py:1993:22
              %size_prods.587 : int = aten::mul(%size_prods.586, %2442) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.587)
          %2444 : bool = aten::eq(%size_prods.585, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2444) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2445 : Tensor = aten::batch_norm(%concated_features.72, %2432, %2433, %2430, %2431, %2429, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.143 : Tensor = aten::relu_(%2445) # torch/nn/functional.py:1117:17
      %2447 : Tensor = prim::GetAttr[name="weight"](%2422)
      %2448 : Tensor? = prim::GetAttr[name="bias"](%2422)
      %2449 : int[] = prim::ListConstruct(%27, %27)
      %2450 : int[] = prim::ListConstruct(%24, %24)
      %2451 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.143 : Tensor = aten::conv2d(%result.143, %2447, %2448, %2449, %2450, %2451, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.143)
  %2453 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1881)
  %2454 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1881)
  %2455 : int = aten::dim(%bottleneck_output.142) # torch/nn/modules/batchnorm.py:276:11
  %2456 : bool = aten::ne(%2455, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2456) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2457 : bool = prim::GetAttr[name="training"](%2454)
   = prim::If(%2457) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2458 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2454)
      %2459 : Tensor = aten::add(%2458, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2454, %2459)
      -> ()
    block1():
      -> ()
  %2460 : bool = prim::GetAttr[name="training"](%2454)
  %2461 : Tensor = prim::GetAttr[name="running_mean"](%2454)
  %2462 : Tensor = prim::GetAttr[name="running_var"](%2454)
  %2463 : Tensor = prim::GetAttr[name="weight"](%2454)
  %2464 : Tensor = prim::GetAttr[name="bias"](%2454)
   = prim::If(%2460) # torch/nn/functional.py:2011:4
    block0():
      %2465 : int[] = aten::size(%bottleneck_output.142) # torch/nn/functional.py:2012:27
      %size_prods.588 : int = aten::__getitem__(%2465, %24) # torch/nn/functional.py:1991:17
      %2467 : int = aten::len(%2465) # torch/nn/functional.py:1992:19
      %2468 : int = aten::sub(%2467, %26) # torch/nn/functional.py:1992:19
      %size_prods.589 : int = prim::Loop(%2468, %25, %size_prods.588) # torch/nn/functional.py:1992:4
        block0(%i.148 : int, %size_prods.590 : int):
          %2472 : int = aten::add(%i.148, %26) # torch/nn/functional.py:1993:27
          %2473 : int = aten::__getitem__(%2465, %2472) # torch/nn/functional.py:1993:22
          %size_prods.591 : int = aten::mul(%size_prods.590, %2473) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.591)
      %2475 : bool = aten::eq(%size_prods.589, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2475) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2476 : Tensor = aten::batch_norm(%bottleneck_output.142, %2463, %2464, %2461, %2462, %2460, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.144 : Tensor = aten::relu_(%2476) # torch/nn/functional.py:1117:17
  %2478 : Tensor = prim::GetAttr[name="weight"](%2453)
  %2479 : Tensor? = prim::GetAttr[name="bias"](%2453)
  %2480 : int[] = prim::ListConstruct(%27, %27)
  %2481 : int[] = prim::ListConstruct(%27, %27)
  %2482 : int[] = prim::ListConstruct(%27, %27)
  %new_features.143 : Tensor = aten::conv2d(%result.144, %2478, %2479, %2480, %2481, %2482, %27) # torch/nn/modules/conv.py:415:15
  %2484 : float = prim::GetAttr[name="drop_rate"](%1881)
  %2485 : bool = aten::gt(%2484, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.144 : Tensor = prim::If(%2485) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2487 : float = prim::GetAttr[name="drop_rate"](%1881)
      %2488 : bool = prim::GetAttr[name="training"](%1881)
      %2489 : bool = aten::lt(%2487, %16) # torch/nn/functional.py:968:7
      %2490 : bool = prim::If(%2489) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2491 : bool = aten::gt(%2487, %17) # torch/nn/functional.py:968:17
          -> (%2491)
       = prim::If(%2490) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2492 : Tensor = aten::dropout(%new_features.143, %2487, %2488) # torch/nn/functional.py:973:17
      -> (%2492)
    block1():
      -> (%new_features.143)
  %2493 : Tensor[] = aten::append(%features.4, %new_features.144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2494 : Tensor = prim::Uninitialized()
  %2495 : bool = prim::GetAttr[name="memory_efficient"](%1882)
  %2496 : bool = prim::If(%2495) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2497 : bool = prim::Uninitialized()
      %2498 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2499 : bool = aten::gt(%2498, %24)
      %2500 : bool, %2501 : bool, %2502 : int = prim::Loop(%18, %2499, %19, %2497, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2503 : int, %2504 : bool, %2505 : bool, %2506 : int):
          %tensor.73 : Tensor = aten::__getitem__(%features.4, %2506) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2508 : bool = prim::requires_grad(%tensor.73)
          %2509 : bool, %2510 : bool = prim::If(%2508) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2497)
          %2511 : int = aten::add(%2506, %27)
          %2512 : bool = aten::lt(%2511, %2498)
          %2513 : bool = aten::__and__(%2512, %2509)
          -> (%2513, %2508, %2510, %2511)
      %2514 : bool = prim::If(%2500)
        block0():
          -> (%2501)
        block1():
          -> (%19)
      -> (%2514)
    block1():
      -> (%19)
  %bottleneck_output.144 : Tensor = prim::If(%2496) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2494)
    block1():
      %concated_features.73 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2517 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%1882)
      %2518 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%1882)
      %2519 : int = aten::dim(%concated_features.73) # torch/nn/modules/batchnorm.py:276:11
      %2520 : bool = aten::ne(%2519, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2520) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2521 : bool = prim::GetAttr[name="training"](%2518)
       = prim::If(%2521) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2522 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2518)
          %2523 : Tensor = aten::add(%2522, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2518, %2523)
          -> ()
        block1():
          -> ()
      %2524 : bool = prim::GetAttr[name="training"](%2518)
      %2525 : Tensor = prim::GetAttr[name="running_mean"](%2518)
      %2526 : Tensor = prim::GetAttr[name="running_var"](%2518)
      %2527 : Tensor = prim::GetAttr[name="weight"](%2518)
      %2528 : Tensor = prim::GetAttr[name="bias"](%2518)
       = prim::If(%2524) # torch/nn/functional.py:2011:4
        block0():
          %2529 : int[] = aten::size(%concated_features.73) # torch/nn/functional.py:2012:27
          %size_prods.592 : int = aten::__getitem__(%2529, %24) # torch/nn/functional.py:1991:17
          %2531 : int = aten::len(%2529) # torch/nn/functional.py:1992:19
          %2532 : int = aten::sub(%2531, %26) # torch/nn/functional.py:1992:19
          %size_prods.593 : int = prim::Loop(%2532, %25, %size_prods.592) # torch/nn/functional.py:1992:4
            block0(%i.149 : int, %size_prods.594 : int):
              %2536 : int = aten::add(%i.149, %26) # torch/nn/functional.py:1993:27
              %2537 : int = aten::__getitem__(%2529, %2536) # torch/nn/functional.py:1993:22
              %size_prods.595 : int = aten::mul(%size_prods.594, %2537) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.595)
          %2539 : bool = aten::eq(%size_prods.593, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2539) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2540 : Tensor = aten::batch_norm(%concated_features.73, %2527, %2528, %2525, %2526, %2524, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.145 : Tensor = aten::relu_(%2540) # torch/nn/functional.py:1117:17
      %2542 : Tensor = prim::GetAttr[name="weight"](%2517)
      %2543 : Tensor? = prim::GetAttr[name="bias"](%2517)
      %2544 : int[] = prim::ListConstruct(%27, %27)
      %2545 : int[] = prim::ListConstruct(%24, %24)
      %2546 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.145 : Tensor = aten::conv2d(%result.145, %2542, %2543, %2544, %2545, %2546, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.145)
  %2548 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1882)
  %2549 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1882)
  %2550 : int = aten::dim(%bottleneck_output.144) # torch/nn/modules/batchnorm.py:276:11
  %2551 : bool = aten::ne(%2550, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2551) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2552 : bool = prim::GetAttr[name="training"](%2549)
   = prim::If(%2552) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2553 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2549)
      %2554 : Tensor = aten::add(%2553, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2549, %2554)
      -> ()
    block1():
      -> ()
  %2555 : bool = prim::GetAttr[name="training"](%2549)
  %2556 : Tensor = prim::GetAttr[name="running_mean"](%2549)
  %2557 : Tensor = prim::GetAttr[name="running_var"](%2549)
  %2558 : Tensor = prim::GetAttr[name="weight"](%2549)
  %2559 : Tensor = prim::GetAttr[name="bias"](%2549)
   = prim::If(%2555) # torch/nn/functional.py:2011:4
    block0():
      %2560 : int[] = aten::size(%bottleneck_output.144) # torch/nn/functional.py:2012:27
      %size_prods.596 : int = aten::__getitem__(%2560, %24) # torch/nn/functional.py:1991:17
      %2562 : int = aten::len(%2560) # torch/nn/functional.py:1992:19
      %2563 : int = aten::sub(%2562, %26) # torch/nn/functional.py:1992:19
      %size_prods.597 : int = prim::Loop(%2563, %25, %size_prods.596) # torch/nn/functional.py:1992:4
        block0(%i.150 : int, %size_prods.598 : int):
          %2567 : int = aten::add(%i.150, %26) # torch/nn/functional.py:1993:27
          %2568 : int = aten::__getitem__(%2560, %2567) # torch/nn/functional.py:1993:22
          %size_prods.599 : int = aten::mul(%size_prods.598, %2568) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.599)
      %2570 : bool = aten::eq(%size_prods.597, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2570) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2571 : Tensor = aten::batch_norm(%bottleneck_output.144, %2558, %2559, %2556, %2557, %2555, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.146 : Tensor = aten::relu_(%2571) # torch/nn/functional.py:1117:17
  %2573 : Tensor = prim::GetAttr[name="weight"](%2548)
  %2574 : Tensor? = prim::GetAttr[name="bias"](%2548)
  %2575 : int[] = prim::ListConstruct(%27, %27)
  %2576 : int[] = prim::ListConstruct(%27, %27)
  %2577 : int[] = prim::ListConstruct(%27, %27)
  %new_features.145 : Tensor = aten::conv2d(%result.146, %2573, %2574, %2575, %2576, %2577, %27) # torch/nn/modules/conv.py:415:15
  %2579 : float = prim::GetAttr[name="drop_rate"](%1882)
  %2580 : bool = aten::gt(%2579, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.146 : Tensor = prim::If(%2580) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2582 : float = prim::GetAttr[name="drop_rate"](%1882)
      %2583 : bool = prim::GetAttr[name="training"](%1882)
      %2584 : bool = aten::lt(%2582, %16) # torch/nn/functional.py:968:7
      %2585 : bool = prim::If(%2584) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2586 : bool = aten::gt(%2582, %17) # torch/nn/functional.py:968:17
          -> (%2586)
       = prim::If(%2585) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2587 : Tensor = aten::dropout(%new_features.145, %2582, %2583) # torch/nn/functional.py:973:17
      -> (%2587)
    block1():
      -> (%new_features.145)
  %2588 : Tensor[] = aten::append(%features.4, %new_features.146) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2589 : Tensor = prim::Uninitialized()
  %2590 : bool = prim::GetAttr[name="memory_efficient"](%1883)
  %2591 : bool = prim::If(%2590) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2592 : bool = prim::Uninitialized()
      %2593 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2594 : bool = aten::gt(%2593, %24)
      %2595 : bool, %2596 : bool, %2597 : int = prim::Loop(%18, %2594, %19, %2592, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2598 : int, %2599 : bool, %2600 : bool, %2601 : int):
          %tensor.74 : Tensor = aten::__getitem__(%features.4, %2601) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2603 : bool = prim::requires_grad(%tensor.74)
          %2604 : bool, %2605 : bool = prim::If(%2603) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2592)
          %2606 : int = aten::add(%2601, %27)
          %2607 : bool = aten::lt(%2606, %2593)
          %2608 : bool = aten::__and__(%2607, %2604)
          -> (%2608, %2603, %2605, %2606)
      %2609 : bool = prim::If(%2595)
        block0():
          -> (%2596)
        block1():
          -> (%19)
      -> (%2609)
    block1():
      -> (%19)
  %bottleneck_output.146 : Tensor = prim::If(%2591) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2589)
    block1():
      %concated_features.74 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2612 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%1883)
      %2613 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%1883)
      %2614 : int = aten::dim(%concated_features.74) # torch/nn/modules/batchnorm.py:276:11
      %2615 : bool = aten::ne(%2614, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2615) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2616 : bool = prim::GetAttr[name="training"](%2613)
       = prim::If(%2616) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2617 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2613)
          %2618 : Tensor = aten::add(%2617, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2613, %2618)
          -> ()
        block1():
          -> ()
      %2619 : bool = prim::GetAttr[name="training"](%2613)
      %2620 : Tensor = prim::GetAttr[name="running_mean"](%2613)
      %2621 : Tensor = prim::GetAttr[name="running_var"](%2613)
      %2622 : Tensor = prim::GetAttr[name="weight"](%2613)
      %2623 : Tensor = prim::GetAttr[name="bias"](%2613)
       = prim::If(%2619) # torch/nn/functional.py:2011:4
        block0():
          %2624 : int[] = aten::size(%concated_features.74) # torch/nn/functional.py:2012:27
          %size_prods.600 : int = aten::__getitem__(%2624, %24) # torch/nn/functional.py:1991:17
          %2626 : int = aten::len(%2624) # torch/nn/functional.py:1992:19
          %2627 : int = aten::sub(%2626, %26) # torch/nn/functional.py:1992:19
          %size_prods.601 : int = prim::Loop(%2627, %25, %size_prods.600) # torch/nn/functional.py:1992:4
            block0(%i.151 : int, %size_prods.602 : int):
              %2631 : int = aten::add(%i.151, %26) # torch/nn/functional.py:1993:27
              %2632 : int = aten::__getitem__(%2624, %2631) # torch/nn/functional.py:1993:22
              %size_prods.603 : int = aten::mul(%size_prods.602, %2632) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.603)
          %2634 : bool = aten::eq(%size_prods.601, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2634) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2635 : Tensor = aten::batch_norm(%concated_features.74, %2622, %2623, %2620, %2621, %2619, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.147 : Tensor = aten::relu_(%2635) # torch/nn/functional.py:1117:17
      %2637 : Tensor = prim::GetAttr[name="weight"](%2612)
      %2638 : Tensor? = prim::GetAttr[name="bias"](%2612)
      %2639 : int[] = prim::ListConstruct(%27, %27)
      %2640 : int[] = prim::ListConstruct(%24, %24)
      %2641 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.147 : Tensor = aten::conv2d(%result.147, %2637, %2638, %2639, %2640, %2641, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.147)
  %2643 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1883)
  %2644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1883)
  %2645 : int = aten::dim(%bottleneck_output.146) # torch/nn/modules/batchnorm.py:276:11
  %2646 : bool = aten::ne(%2645, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2646) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2647 : bool = prim::GetAttr[name="training"](%2644)
   = prim::If(%2647) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2648 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2644)
      %2649 : Tensor = aten::add(%2648, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2644, %2649)
      -> ()
    block1():
      -> ()
  %2650 : bool = prim::GetAttr[name="training"](%2644)
  %2651 : Tensor = prim::GetAttr[name="running_mean"](%2644)
  %2652 : Tensor = prim::GetAttr[name="running_var"](%2644)
  %2653 : Tensor = prim::GetAttr[name="weight"](%2644)
  %2654 : Tensor = prim::GetAttr[name="bias"](%2644)
   = prim::If(%2650) # torch/nn/functional.py:2011:4
    block0():
      %2655 : int[] = aten::size(%bottleneck_output.146) # torch/nn/functional.py:2012:27
      %size_prods.604 : int = aten::__getitem__(%2655, %24) # torch/nn/functional.py:1991:17
      %2657 : int = aten::len(%2655) # torch/nn/functional.py:1992:19
      %2658 : int = aten::sub(%2657, %26) # torch/nn/functional.py:1992:19
      %size_prods.605 : int = prim::Loop(%2658, %25, %size_prods.604) # torch/nn/functional.py:1992:4
        block0(%i.152 : int, %size_prods.606 : int):
          %2662 : int = aten::add(%i.152, %26) # torch/nn/functional.py:1993:27
          %2663 : int = aten::__getitem__(%2655, %2662) # torch/nn/functional.py:1993:22
          %size_prods.607 : int = aten::mul(%size_prods.606, %2663) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.607)
      %2665 : bool = aten::eq(%size_prods.605, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2665) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2666 : Tensor = aten::batch_norm(%bottleneck_output.146, %2653, %2654, %2651, %2652, %2650, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.148 : Tensor = aten::relu_(%2666) # torch/nn/functional.py:1117:17
  %2668 : Tensor = prim::GetAttr[name="weight"](%2643)
  %2669 : Tensor? = prim::GetAttr[name="bias"](%2643)
  %2670 : int[] = prim::ListConstruct(%27, %27)
  %2671 : int[] = prim::ListConstruct(%27, %27)
  %2672 : int[] = prim::ListConstruct(%27, %27)
  %new_features.147 : Tensor = aten::conv2d(%result.148, %2668, %2669, %2670, %2671, %2672, %27) # torch/nn/modules/conv.py:415:15
  %2674 : float = prim::GetAttr[name="drop_rate"](%1883)
  %2675 : bool = aten::gt(%2674, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.148 : Tensor = prim::If(%2675) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2677 : float = prim::GetAttr[name="drop_rate"](%1883)
      %2678 : bool = prim::GetAttr[name="training"](%1883)
      %2679 : bool = aten::lt(%2677, %16) # torch/nn/functional.py:968:7
      %2680 : bool = prim::If(%2679) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2681 : bool = aten::gt(%2677, %17) # torch/nn/functional.py:968:17
          -> (%2681)
       = prim::If(%2680) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2682 : Tensor = aten::dropout(%new_features.147, %2677, %2678) # torch/nn/functional.py:973:17
      -> (%2682)
    block1():
      -> (%new_features.147)
  %2683 : Tensor[] = aten::append(%features.4, %new_features.148) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2684 : Tensor = prim::Uninitialized()
  %2685 : bool = prim::GetAttr[name="memory_efficient"](%1884)
  %2686 : bool = prim::If(%2685) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2687 : bool = prim::Uninitialized()
      %2688 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2689 : bool = aten::gt(%2688, %24)
      %2690 : bool, %2691 : bool, %2692 : int = prim::Loop(%18, %2689, %19, %2687, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2693 : int, %2694 : bool, %2695 : bool, %2696 : int):
          %tensor.75 : Tensor = aten::__getitem__(%features.4, %2696) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2698 : bool = prim::requires_grad(%tensor.75)
          %2699 : bool, %2700 : bool = prim::If(%2698) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2687)
          %2701 : int = aten::add(%2696, %27)
          %2702 : bool = aten::lt(%2701, %2688)
          %2703 : bool = aten::__and__(%2702, %2699)
          -> (%2703, %2698, %2700, %2701)
      %2704 : bool = prim::If(%2690)
        block0():
          -> (%2691)
        block1():
          -> (%19)
      -> (%2704)
    block1():
      -> (%19)
  %bottleneck_output.148 : Tensor = prim::If(%2686) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2684)
    block1():
      %concated_features.75 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2707 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%1884)
      %2708 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm1"](%1884)
      %2709 : int = aten::dim(%concated_features.75) # torch/nn/modules/batchnorm.py:276:11
      %2710 : bool = aten::ne(%2709, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2710) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2711 : bool = prim::GetAttr[name="training"](%2708)
       = prim::If(%2711) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2712 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2708)
          %2713 : Tensor = aten::add(%2712, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2708, %2713)
          -> ()
        block1():
          -> ()
      %2714 : bool = prim::GetAttr[name="training"](%2708)
      %2715 : Tensor = prim::GetAttr[name="running_mean"](%2708)
      %2716 : Tensor = prim::GetAttr[name="running_var"](%2708)
      %2717 : Tensor = prim::GetAttr[name="weight"](%2708)
      %2718 : Tensor = prim::GetAttr[name="bias"](%2708)
       = prim::If(%2714) # torch/nn/functional.py:2011:4
        block0():
          %2719 : int[] = aten::size(%concated_features.75) # torch/nn/functional.py:2012:27
          %size_prods.608 : int = aten::__getitem__(%2719, %24) # torch/nn/functional.py:1991:17
          %2721 : int = aten::len(%2719) # torch/nn/functional.py:1992:19
          %2722 : int = aten::sub(%2721, %26) # torch/nn/functional.py:1992:19
          %size_prods.609 : int = prim::Loop(%2722, %25, %size_prods.608) # torch/nn/functional.py:1992:4
            block0(%i.153 : int, %size_prods.610 : int):
              %2726 : int = aten::add(%i.153, %26) # torch/nn/functional.py:1993:27
              %2727 : int = aten::__getitem__(%2719, %2726) # torch/nn/functional.py:1993:22
              %size_prods.611 : int = aten::mul(%size_prods.610, %2727) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.611)
          %2729 : bool = aten::eq(%size_prods.609, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2729) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2730 : Tensor = aten::batch_norm(%concated_features.75, %2717, %2718, %2715, %2716, %2714, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.149 : Tensor = aten::relu_(%2730) # torch/nn/functional.py:1117:17
      %2732 : Tensor = prim::GetAttr[name="weight"](%2707)
      %2733 : Tensor? = prim::GetAttr[name="bias"](%2707)
      %2734 : int[] = prim::ListConstruct(%27, %27)
      %2735 : int[] = prim::ListConstruct(%24, %24)
      %2736 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.149 : Tensor = aten::conv2d(%result.149, %2732, %2733, %2734, %2735, %2736, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.149)
  %2738 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1884)
  %2739 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1884)
  %2740 : int = aten::dim(%bottleneck_output.148) # torch/nn/modules/batchnorm.py:276:11
  %2741 : bool = aten::ne(%2740, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2741) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2742 : bool = prim::GetAttr[name="training"](%2739)
   = prim::If(%2742) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2743 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2739)
      %2744 : Tensor = aten::add(%2743, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2739, %2744)
      -> ()
    block1():
      -> ()
  %2745 : bool = prim::GetAttr[name="training"](%2739)
  %2746 : Tensor = prim::GetAttr[name="running_mean"](%2739)
  %2747 : Tensor = prim::GetAttr[name="running_var"](%2739)
  %2748 : Tensor = prim::GetAttr[name="weight"](%2739)
  %2749 : Tensor = prim::GetAttr[name="bias"](%2739)
   = prim::If(%2745) # torch/nn/functional.py:2011:4
    block0():
      %2750 : int[] = aten::size(%bottleneck_output.148) # torch/nn/functional.py:2012:27
      %size_prods.612 : int = aten::__getitem__(%2750, %24) # torch/nn/functional.py:1991:17
      %2752 : int = aten::len(%2750) # torch/nn/functional.py:1992:19
      %2753 : int = aten::sub(%2752, %26) # torch/nn/functional.py:1992:19
      %size_prods.613 : int = prim::Loop(%2753, %25, %size_prods.612) # torch/nn/functional.py:1992:4
        block0(%i.154 : int, %size_prods.614 : int):
          %2757 : int = aten::add(%i.154, %26) # torch/nn/functional.py:1993:27
          %2758 : int = aten::__getitem__(%2750, %2757) # torch/nn/functional.py:1993:22
          %size_prods.615 : int = aten::mul(%size_prods.614, %2758) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.615)
      %2760 : bool = aten::eq(%size_prods.613, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2760) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2761 : Tensor = aten::batch_norm(%bottleneck_output.148, %2748, %2749, %2746, %2747, %2745, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.150 : Tensor = aten::relu_(%2761) # torch/nn/functional.py:1117:17
  %2763 : Tensor = prim::GetAttr[name="weight"](%2738)
  %2764 : Tensor? = prim::GetAttr[name="bias"](%2738)
  %2765 : int[] = prim::ListConstruct(%27, %27)
  %2766 : int[] = prim::ListConstruct(%27, %27)
  %2767 : int[] = prim::ListConstruct(%27, %27)
  %new_features.149 : Tensor = aten::conv2d(%result.150, %2763, %2764, %2765, %2766, %2767, %27) # torch/nn/modules/conv.py:415:15
  %2769 : float = prim::GetAttr[name="drop_rate"](%1884)
  %2770 : bool = aten::gt(%2769, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.150 : Tensor = prim::If(%2770) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2772 : float = prim::GetAttr[name="drop_rate"](%1884)
      %2773 : bool = prim::GetAttr[name="training"](%1884)
      %2774 : bool = aten::lt(%2772, %16) # torch/nn/functional.py:968:7
      %2775 : bool = prim::If(%2774) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2776 : bool = aten::gt(%2772, %17) # torch/nn/functional.py:968:17
          -> (%2776)
       = prim::If(%2775) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2777 : Tensor = aten::dropout(%new_features.149, %2772, %2773) # torch/nn/functional.py:973:17
      -> (%2777)
    block1():
      -> (%new_features.149)
  %2778 : Tensor[] = aten::append(%features.4, %new_features.150) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2779 : Tensor = prim::Uninitialized()
  %2780 : bool = prim::GetAttr[name="memory_efficient"](%1885)
  %2781 : bool = prim::If(%2780) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2782 : bool = prim::Uninitialized()
      %2783 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2784 : bool = aten::gt(%2783, %24)
      %2785 : bool, %2786 : bool, %2787 : int = prim::Loop(%18, %2784, %19, %2782, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2788 : int, %2789 : bool, %2790 : bool, %2791 : int):
          %tensor.76 : Tensor = aten::__getitem__(%features.4, %2791) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2793 : bool = prim::requires_grad(%tensor.76)
          %2794 : bool, %2795 : bool = prim::If(%2793) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2782)
          %2796 : int = aten::add(%2791, %27)
          %2797 : bool = aten::lt(%2796, %2783)
          %2798 : bool = aten::__and__(%2797, %2794)
          -> (%2798, %2793, %2795, %2796)
      %2799 : bool = prim::If(%2785)
        block0():
          -> (%2786)
        block1():
          -> (%19)
      -> (%2799)
    block1():
      -> (%19)
  %bottleneck_output.150 : Tensor = prim::If(%2781) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2779)
    block1():
      %concated_features.76 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2802 : __torch__.torch.nn.modules.conv.___torch_mangle_113.Conv2d = prim::GetAttr[name="conv1"](%1885)
      %2803 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_112.BatchNorm2d = prim::GetAttr[name="norm1"](%1885)
      %2804 : int = aten::dim(%concated_features.76) # torch/nn/modules/batchnorm.py:276:11
      %2805 : bool = aten::ne(%2804, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2805) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2806 : bool = prim::GetAttr[name="training"](%2803)
       = prim::If(%2806) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2807 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2803)
          %2808 : Tensor = aten::add(%2807, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2803, %2808)
          -> ()
        block1():
          -> ()
      %2809 : bool = prim::GetAttr[name="training"](%2803)
      %2810 : Tensor = prim::GetAttr[name="running_mean"](%2803)
      %2811 : Tensor = prim::GetAttr[name="running_var"](%2803)
      %2812 : Tensor = prim::GetAttr[name="weight"](%2803)
      %2813 : Tensor = prim::GetAttr[name="bias"](%2803)
       = prim::If(%2809) # torch/nn/functional.py:2011:4
        block0():
          %2814 : int[] = aten::size(%concated_features.76) # torch/nn/functional.py:2012:27
          %size_prods.616 : int = aten::__getitem__(%2814, %24) # torch/nn/functional.py:1991:17
          %2816 : int = aten::len(%2814) # torch/nn/functional.py:1992:19
          %2817 : int = aten::sub(%2816, %26) # torch/nn/functional.py:1992:19
          %size_prods.617 : int = prim::Loop(%2817, %25, %size_prods.616) # torch/nn/functional.py:1992:4
            block0(%i.155 : int, %size_prods.618 : int):
              %2821 : int = aten::add(%i.155, %26) # torch/nn/functional.py:1993:27
              %2822 : int = aten::__getitem__(%2814, %2821) # torch/nn/functional.py:1993:22
              %size_prods.619 : int = aten::mul(%size_prods.618, %2822) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.619)
          %2824 : bool = aten::eq(%size_prods.617, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2824) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2825 : Tensor = aten::batch_norm(%concated_features.76, %2812, %2813, %2810, %2811, %2809, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.151 : Tensor = aten::relu_(%2825) # torch/nn/functional.py:1117:17
      %2827 : Tensor = prim::GetAttr[name="weight"](%2802)
      %2828 : Tensor? = prim::GetAttr[name="bias"](%2802)
      %2829 : int[] = prim::ListConstruct(%27, %27)
      %2830 : int[] = prim::ListConstruct(%24, %24)
      %2831 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.151 : Tensor = aten::conv2d(%result.151, %2827, %2828, %2829, %2830, %2831, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.151)
  %2833 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1885)
  %2834 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1885)
  %2835 : int = aten::dim(%bottleneck_output.150) # torch/nn/modules/batchnorm.py:276:11
  %2836 : bool = aten::ne(%2835, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2836) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2837 : bool = prim::GetAttr[name="training"](%2834)
   = prim::If(%2837) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2838 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2834)
      %2839 : Tensor = aten::add(%2838, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2834, %2839)
      -> ()
    block1():
      -> ()
  %2840 : bool = prim::GetAttr[name="training"](%2834)
  %2841 : Tensor = prim::GetAttr[name="running_mean"](%2834)
  %2842 : Tensor = prim::GetAttr[name="running_var"](%2834)
  %2843 : Tensor = prim::GetAttr[name="weight"](%2834)
  %2844 : Tensor = prim::GetAttr[name="bias"](%2834)
   = prim::If(%2840) # torch/nn/functional.py:2011:4
    block0():
      %2845 : int[] = aten::size(%bottleneck_output.150) # torch/nn/functional.py:2012:27
      %size_prods.620 : int = aten::__getitem__(%2845, %24) # torch/nn/functional.py:1991:17
      %2847 : int = aten::len(%2845) # torch/nn/functional.py:1992:19
      %2848 : int = aten::sub(%2847, %26) # torch/nn/functional.py:1992:19
      %size_prods.621 : int = prim::Loop(%2848, %25, %size_prods.620) # torch/nn/functional.py:1992:4
        block0(%i.156 : int, %size_prods.622 : int):
          %2852 : int = aten::add(%i.156, %26) # torch/nn/functional.py:1993:27
          %2853 : int = aten::__getitem__(%2845, %2852) # torch/nn/functional.py:1993:22
          %size_prods.623 : int = aten::mul(%size_prods.622, %2853) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.623)
      %2855 : bool = aten::eq(%size_prods.621, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2855) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2856 : Tensor = aten::batch_norm(%bottleneck_output.150, %2843, %2844, %2841, %2842, %2840, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.152 : Tensor = aten::relu_(%2856) # torch/nn/functional.py:1117:17
  %2858 : Tensor = prim::GetAttr[name="weight"](%2833)
  %2859 : Tensor? = prim::GetAttr[name="bias"](%2833)
  %2860 : int[] = prim::ListConstruct(%27, %27)
  %2861 : int[] = prim::ListConstruct(%27, %27)
  %2862 : int[] = prim::ListConstruct(%27, %27)
  %new_features.151 : Tensor = aten::conv2d(%result.152, %2858, %2859, %2860, %2861, %2862, %27) # torch/nn/modules/conv.py:415:15
  %2864 : float = prim::GetAttr[name="drop_rate"](%1885)
  %2865 : bool = aten::gt(%2864, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.152 : Tensor = prim::If(%2865) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2867 : float = prim::GetAttr[name="drop_rate"](%1885)
      %2868 : bool = prim::GetAttr[name="training"](%1885)
      %2869 : bool = aten::lt(%2867, %16) # torch/nn/functional.py:968:7
      %2870 : bool = prim::If(%2869) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2871 : bool = aten::gt(%2867, %17) # torch/nn/functional.py:968:17
          -> (%2871)
       = prim::If(%2870) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2872 : Tensor = aten::dropout(%new_features.151, %2867, %2868) # torch/nn/functional.py:973:17
      -> (%2872)
    block1():
      -> (%new_features.151)
  %2873 : Tensor[] = aten::append(%features.4, %new_features.152) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2874 : Tensor = prim::Uninitialized()
  %2875 : bool = prim::GetAttr[name="memory_efficient"](%1886)
  %2876 : bool = prim::If(%2875) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2877 : bool = prim::Uninitialized()
      %2878 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2879 : bool = aten::gt(%2878, %24)
      %2880 : bool, %2881 : bool, %2882 : int = prim::Loop(%18, %2879, %19, %2877, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2883 : int, %2884 : bool, %2885 : bool, %2886 : int):
          %tensor.77 : Tensor = aten::__getitem__(%features.4, %2886) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2888 : bool = prim::requires_grad(%tensor.77)
          %2889 : bool, %2890 : bool = prim::If(%2888) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2877)
          %2891 : int = aten::add(%2886, %27)
          %2892 : bool = aten::lt(%2891, %2878)
          %2893 : bool = aten::__and__(%2892, %2889)
          -> (%2893, %2888, %2890, %2891)
      %2894 : bool = prim::If(%2880)
        block0():
          -> (%2881)
        block1():
          -> (%19)
      -> (%2894)
    block1():
      -> (%19)
  %bottleneck_output.152 : Tensor = prim::If(%2876) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2874)
    block1():
      %concated_features.77 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2897 : __torch__.torch.nn.modules.conv.___torch_mangle_116.Conv2d = prim::GetAttr[name="conv1"](%1886)
      %2898 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%1886)
      %2899 : int = aten::dim(%concated_features.77) # torch/nn/modules/batchnorm.py:276:11
      %2900 : bool = aten::ne(%2899, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2900) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2901 : bool = prim::GetAttr[name="training"](%2898)
       = prim::If(%2901) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2902 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2898)
          %2903 : Tensor = aten::add(%2902, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2898, %2903)
          -> ()
        block1():
          -> ()
      %2904 : bool = prim::GetAttr[name="training"](%2898)
      %2905 : Tensor = prim::GetAttr[name="running_mean"](%2898)
      %2906 : Tensor = prim::GetAttr[name="running_var"](%2898)
      %2907 : Tensor = prim::GetAttr[name="weight"](%2898)
      %2908 : Tensor = prim::GetAttr[name="bias"](%2898)
       = prim::If(%2904) # torch/nn/functional.py:2011:4
        block0():
          %2909 : int[] = aten::size(%concated_features.77) # torch/nn/functional.py:2012:27
          %size_prods.624 : int = aten::__getitem__(%2909, %24) # torch/nn/functional.py:1991:17
          %2911 : int = aten::len(%2909) # torch/nn/functional.py:1992:19
          %2912 : int = aten::sub(%2911, %26) # torch/nn/functional.py:1992:19
          %size_prods.625 : int = prim::Loop(%2912, %25, %size_prods.624) # torch/nn/functional.py:1992:4
            block0(%i.157 : int, %size_prods.626 : int):
              %2916 : int = aten::add(%i.157, %26) # torch/nn/functional.py:1993:27
              %2917 : int = aten::__getitem__(%2909, %2916) # torch/nn/functional.py:1993:22
              %size_prods.627 : int = aten::mul(%size_prods.626, %2917) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.627)
          %2919 : bool = aten::eq(%size_prods.625, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2919) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2920 : Tensor = aten::batch_norm(%concated_features.77, %2907, %2908, %2905, %2906, %2904, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.153 : Tensor = aten::relu_(%2920) # torch/nn/functional.py:1117:17
      %2922 : Tensor = prim::GetAttr[name="weight"](%2897)
      %2923 : Tensor? = prim::GetAttr[name="bias"](%2897)
      %2924 : int[] = prim::ListConstruct(%27, %27)
      %2925 : int[] = prim::ListConstruct(%24, %24)
      %2926 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.153 : Tensor = aten::conv2d(%result.153, %2922, %2923, %2924, %2925, %2926, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.153)
  %2928 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1886)
  %2929 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1886)
  %2930 : int = aten::dim(%bottleneck_output.152) # torch/nn/modules/batchnorm.py:276:11
  %2931 : bool = aten::ne(%2930, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2931) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2932 : bool = prim::GetAttr[name="training"](%2929)
   = prim::If(%2932) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2933 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2929)
      %2934 : Tensor = aten::add(%2933, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2929, %2934)
      -> ()
    block1():
      -> ()
  %2935 : bool = prim::GetAttr[name="training"](%2929)
  %2936 : Tensor = prim::GetAttr[name="running_mean"](%2929)
  %2937 : Tensor = prim::GetAttr[name="running_var"](%2929)
  %2938 : Tensor = prim::GetAttr[name="weight"](%2929)
  %2939 : Tensor = prim::GetAttr[name="bias"](%2929)
   = prim::If(%2935) # torch/nn/functional.py:2011:4
    block0():
      %2940 : int[] = aten::size(%bottleneck_output.152) # torch/nn/functional.py:2012:27
      %size_prods.628 : int = aten::__getitem__(%2940, %24) # torch/nn/functional.py:1991:17
      %2942 : int = aten::len(%2940) # torch/nn/functional.py:1992:19
      %2943 : int = aten::sub(%2942, %26) # torch/nn/functional.py:1992:19
      %size_prods.629 : int = prim::Loop(%2943, %25, %size_prods.628) # torch/nn/functional.py:1992:4
        block0(%i.158 : int, %size_prods.630 : int):
          %2947 : int = aten::add(%i.158, %26) # torch/nn/functional.py:1993:27
          %2948 : int = aten::__getitem__(%2940, %2947) # torch/nn/functional.py:1993:22
          %size_prods.631 : int = aten::mul(%size_prods.630, %2948) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.631)
      %2950 : bool = aten::eq(%size_prods.629, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2950) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2951 : Tensor = aten::batch_norm(%bottleneck_output.152, %2938, %2939, %2936, %2937, %2935, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.154 : Tensor = aten::relu_(%2951) # torch/nn/functional.py:1117:17
  %2953 : Tensor = prim::GetAttr[name="weight"](%2928)
  %2954 : Tensor? = prim::GetAttr[name="bias"](%2928)
  %2955 : int[] = prim::ListConstruct(%27, %27)
  %2956 : int[] = prim::ListConstruct(%27, %27)
  %2957 : int[] = prim::ListConstruct(%27, %27)
  %new_features.153 : Tensor = aten::conv2d(%result.154, %2953, %2954, %2955, %2956, %2957, %27) # torch/nn/modules/conv.py:415:15
  %2959 : float = prim::GetAttr[name="drop_rate"](%1886)
  %2960 : bool = aten::gt(%2959, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.154 : Tensor = prim::If(%2960) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2962 : float = prim::GetAttr[name="drop_rate"](%1886)
      %2963 : bool = prim::GetAttr[name="training"](%1886)
      %2964 : bool = aten::lt(%2962, %16) # torch/nn/functional.py:968:7
      %2965 : bool = prim::If(%2964) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2966 : bool = aten::gt(%2962, %17) # torch/nn/functional.py:968:17
          -> (%2966)
       = prim::If(%2965) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2967 : Tensor = aten::dropout(%new_features.153, %2962, %2963) # torch/nn/functional.py:973:17
      -> (%2967)
    block1():
      -> (%new_features.153)
  %2968 : Tensor[] = aten::append(%features.4, %new_features.154) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2969 : Tensor = prim::Uninitialized()
  %2970 : bool = prim::GetAttr[name="memory_efficient"](%1887)
  %2971 : bool = prim::If(%2970) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2972 : bool = prim::Uninitialized()
      %2973 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2974 : bool = aten::gt(%2973, %24)
      %2975 : bool, %2976 : bool, %2977 : int = prim::Loop(%18, %2974, %19, %2972, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2978 : int, %2979 : bool, %2980 : bool, %2981 : int):
          %tensor.78 : Tensor = aten::__getitem__(%features.4, %2981) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2983 : bool = prim::requires_grad(%tensor.78)
          %2984 : bool, %2985 : bool = prim::If(%2983) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2972)
          %2986 : int = aten::add(%2981, %27)
          %2987 : bool = aten::lt(%2986, %2973)
          %2988 : bool = aten::__and__(%2987, %2984)
          -> (%2988, %2983, %2985, %2986)
      %2989 : bool = prim::If(%2975)
        block0():
          -> (%2976)
        block1():
          -> (%19)
      -> (%2989)
    block1():
      -> (%19)
  %bottleneck_output.154 : Tensor = prim::If(%2971) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2969)
    block1():
      %concated_features.78 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2992 : __torch__.torch.nn.modules.conv.___torch_mangle_119.Conv2d = prim::GetAttr[name="conv1"](%1887)
      %2993 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="norm1"](%1887)
      %2994 : int = aten::dim(%concated_features.78) # torch/nn/modules/batchnorm.py:276:11
      %2995 : bool = aten::ne(%2994, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2995) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2996 : bool = prim::GetAttr[name="training"](%2993)
       = prim::If(%2996) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2997 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2993)
          %2998 : Tensor = aten::add(%2997, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2993, %2998)
          -> ()
        block1():
          -> ()
      %2999 : bool = prim::GetAttr[name="training"](%2993)
      %3000 : Tensor = prim::GetAttr[name="running_mean"](%2993)
      %3001 : Tensor = prim::GetAttr[name="running_var"](%2993)
      %3002 : Tensor = prim::GetAttr[name="weight"](%2993)
      %3003 : Tensor = prim::GetAttr[name="bias"](%2993)
       = prim::If(%2999) # torch/nn/functional.py:2011:4
        block0():
          %3004 : int[] = aten::size(%concated_features.78) # torch/nn/functional.py:2012:27
          %size_prods.632 : int = aten::__getitem__(%3004, %24) # torch/nn/functional.py:1991:17
          %3006 : int = aten::len(%3004) # torch/nn/functional.py:1992:19
          %3007 : int = aten::sub(%3006, %26) # torch/nn/functional.py:1992:19
          %size_prods.633 : int = prim::Loop(%3007, %25, %size_prods.632) # torch/nn/functional.py:1992:4
            block0(%i.159 : int, %size_prods.634 : int):
              %3011 : int = aten::add(%i.159, %26) # torch/nn/functional.py:1993:27
              %3012 : int = aten::__getitem__(%3004, %3011) # torch/nn/functional.py:1993:22
              %size_prods.635 : int = aten::mul(%size_prods.634, %3012) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.635)
          %3014 : bool = aten::eq(%size_prods.633, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3014) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3015 : Tensor = aten::batch_norm(%concated_features.78, %3002, %3003, %3000, %3001, %2999, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.155 : Tensor = aten::relu_(%3015) # torch/nn/functional.py:1117:17
      %3017 : Tensor = prim::GetAttr[name="weight"](%2992)
      %3018 : Tensor? = prim::GetAttr[name="bias"](%2992)
      %3019 : int[] = prim::ListConstruct(%27, %27)
      %3020 : int[] = prim::ListConstruct(%24, %24)
      %3021 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.155 : Tensor = aten::conv2d(%result.155, %3017, %3018, %3019, %3020, %3021, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.155)
  %3023 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1887)
  %3024 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1887)
  %3025 : int = aten::dim(%bottleneck_output.154) # torch/nn/modules/batchnorm.py:276:11
  %3026 : bool = aten::ne(%3025, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3026) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3027 : bool = prim::GetAttr[name="training"](%3024)
   = prim::If(%3027) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3028 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3024)
      %3029 : Tensor = aten::add(%3028, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3024, %3029)
      -> ()
    block1():
      -> ()
  %3030 : bool = prim::GetAttr[name="training"](%3024)
  %3031 : Tensor = prim::GetAttr[name="running_mean"](%3024)
  %3032 : Tensor = prim::GetAttr[name="running_var"](%3024)
  %3033 : Tensor = prim::GetAttr[name="weight"](%3024)
  %3034 : Tensor = prim::GetAttr[name="bias"](%3024)
   = prim::If(%3030) # torch/nn/functional.py:2011:4
    block0():
      %3035 : int[] = aten::size(%bottleneck_output.154) # torch/nn/functional.py:2012:27
      %size_prods.636 : int = aten::__getitem__(%3035, %24) # torch/nn/functional.py:1991:17
      %3037 : int = aten::len(%3035) # torch/nn/functional.py:1992:19
      %3038 : int = aten::sub(%3037, %26) # torch/nn/functional.py:1992:19
      %size_prods.637 : int = prim::Loop(%3038, %25, %size_prods.636) # torch/nn/functional.py:1992:4
        block0(%i.160 : int, %size_prods.638 : int):
          %3042 : int = aten::add(%i.160, %26) # torch/nn/functional.py:1993:27
          %3043 : int = aten::__getitem__(%3035, %3042) # torch/nn/functional.py:1993:22
          %size_prods.639 : int = aten::mul(%size_prods.638, %3043) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.639)
      %3045 : bool = aten::eq(%size_prods.637, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3045) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3046 : Tensor = aten::batch_norm(%bottleneck_output.154, %3033, %3034, %3031, %3032, %3030, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.156 : Tensor = aten::relu_(%3046) # torch/nn/functional.py:1117:17
  %3048 : Tensor = prim::GetAttr[name="weight"](%3023)
  %3049 : Tensor? = prim::GetAttr[name="bias"](%3023)
  %3050 : int[] = prim::ListConstruct(%27, %27)
  %3051 : int[] = prim::ListConstruct(%27, %27)
  %3052 : int[] = prim::ListConstruct(%27, %27)
  %new_features.155 : Tensor = aten::conv2d(%result.156, %3048, %3049, %3050, %3051, %3052, %27) # torch/nn/modules/conv.py:415:15
  %3054 : float = prim::GetAttr[name="drop_rate"](%1887)
  %3055 : bool = aten::gt(%3054, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.156 : Tensor = prim::If(%3055) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3057 : float = prim::GetAttr[name="drop_rate"](%1887)
      %3058 : bool = prim::GetAttr[name="training"](%1887)
      %3059 : bool = aten::lt(%3057, %16) # torch/nn/functional.py:968:7
      %3060 : bool = prim::If(%3059) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3061 : bool = aten::gt(%3057, %17) # torch/nn/functional.py:968:17
          -> (%3061)
       = prim::If(%3060) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3062 : Tensor = aten::dropout(%new_features.155, %3057, %3058) # torch/nn/functional.py:973:17
      -> (%3062)
    block1():
      -> (%new_features.155)
  %3063 : Tensor[] = aten::append(%features.4, %new_features.156) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3064 : Tensor = prim::Uninitialized()
  %3065 : bool = prim::GetAttr[name="memory_efficient"](%1888)
  %3066 : bool = prim::If(%3065) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3067 : bool = prim::Uninitialized()
      %3068 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3069 : bool = aten::gt(%3068, %24)
      %3070 : bool, %3071 : bool, %3072 : int = prim::Loop(%18, %3069, %19, %3067, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3073 : int, %3074 : bool, %3075 : bool, %3076 : int):
          %tensor.79 : Tensor = aten::__getitem__(%features.4, %3076) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3078 : bool = prim::requires_grad(%tensor.79)
          %3079 : bool, %3080 : bool = prim::If(%3078) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3067)
          %3081 : int = aten::add(%3076, %27)
          %3082 : bool = aten::lt(%3081, %3068)
          %3083 : bool = aten::__and__(%3082, %3079)
          -> (%3083, %3078, %3080, %3081)
      %3084 : bool = prim::If(%3070)
        block0():
          -> (%3071)
        block1():
          -> (%19)
      -> (%3084)
    block1():
      -> (%19)
  %bottleneck_output.156 : Tensor = prim::If(%3066) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3064)
    block1():
      %concated_features.79 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3087 : __torch__.torch.nn.modules.conv.___torch_mangle_122.Conv2d = prim::GetAttr[name="conv1"](%1888)
      %3088 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_121.BatchNorm2d = prim::GetAttr[name="norm1"](%1888)
      %3089 : int = aten::dim(%concated_features.79) # torch/nn/modules/batchnorm.py:276:11
      %3090 : bool = aten::ne(%3089, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3090) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3091 : bool = prim::GetAttr[name="training"](%3088)
       = prim::If(%3091) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3092 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3088)
          %3093 : Tensor = aten::add(%3092, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3088, %3093)
          -> ()
        block1():
          -> ()
      %3094 : bool = prim::GetAttr[name="training"](%3088)
      %3095 : Tensor = prim::GetAttr[name="running_mean"](%3088)
      %3096 : Tensor = prim::GetAttr[name="running_var"](%3088)
      %3097 : Tensor = prim::GetAttr[name="weight"](%3088)
      %3098 : Tensor = prim::GetAttr[name="bias"](%3088)
       = prim::If(%3094) # torch/nn/functional.py:2011:4
        block0():
          %3099 : int[] = aten::size(%concated_features.79) # torch/nn/functional.py:2012:27
          %size_prods.640 : int = aten::__getitem__(%3099, %24) # torch/nn/functional.py:1991:17
          %3101 : int = aten::len(%3099) # torch/nn/functional.py:1992:19
          %3102 : int = aten::sub(%3101, %26) # torch/nn/functional.py:1992:19
          %size_prods.641 : int = prim::Loop(%3102, %25, %size_prods.640) # torch/nn/functional.py:1992:4
            block0(%i.161 : int, %size_prods.642 : int):
              %3106 : int = aten::add(%i.161, %26) # torch/nn/functional.py:1993:27
              %3107 : int = aten::__getitem__(%3099, %3106) # torch/nn/functional.py:1993:22
              %size_prods.643 : int = aten::mul(%size_prods.642, %3107) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.643)
          %3109 : bool = aten::eq(%size_prods.641, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3109) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3110 : Tensor = aten::batch_norm(%concated_features.79, %3097, %3098, %3095, %3096, %3094, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.157 : Tensor = aten::relu_(%3110) # torch/nn/functional.py:1117:17
      %3112 : Tensor = prim::GetAttr[name="weight"](%3087)
      %3113 : Tensor? = prim::GetAttr[name="bias"](%3087)
      %3114 : int[] = prim::ListConstruct(%27, %27)
      %3115 : int[] = prim::ListConstruct(%24, %24)
      %3116 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.157 : Tensor = aten::conv2d(%result.157, %3112, %3113, %3114, %3115, %3116, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.157)
  %3118 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1888)
  %3119 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1888)
  %3120 : int = aten::dim(%bottleneck_output.156) # torch/nn/modules/batchnorm.py:276:11
  %3121 : bool = aten::ne(%3120, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3121) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3122 : bool = prim::GetAttr[name="training"](%3119)
   = prim::If(%3122) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3123 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3119)
      %3124 : Tensor = aten::add(%3123, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3119, %3124)
      -> ()
    block1():
      -> ()
  %3125 : bool = prim::GetAttr[name="training"](%3119)
  %3126 : Tensor = prim::GetAttr[name="running_mean"](%3119)
  %3127 : Tensor = prim::GetAttr[name="running_var"](%3119)
  %3128 : Tensor = prim::GetAttr[name="weight"](%3119)
  %3129 : Tensor = prim::GetAttr[name="bias"](%3119)
   = prim::If(%3125) # torch/nn/functional.py:2011:4
    block0():
      %3130 : int[] = aten::size(%bottleneck_output.156) # torch/nn/functional.py:2012:27
      %size_prods.644 : int = aten::__getitem__(%3130, %24) # torch/nn/functional.py:1991:17
      %3132 : int = aten::len(%3130) # torch/nn/functional.py:1992:19
      %3133 : int = aten::sub(%3132, %26) # torch/nn/functional.py:1992:19
      %size_prods.645 : int = prim::Loop(%3133, %25, %size_prods.644) # torch/nn/functional.py:1992:4
        block0(%i.162 : int, %size_prods.646 : int):
          %3137 : int = aten::add(%i.162, %26) # torch/nn/functional.py:1993:27
          %3138 : int = aten::__getitem__(%3130, %3137) # torch/nn/functional.py:1993:22
          %size_prods.647 : int = aten::mul(%size_prods.646, %3138) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.647)
      %3140 : bool = aten::eq(%size_prods.645, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3140) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3141 : Tensor = aten::batch_norm(%bottleneck_output.156, %3128, %3129, %3126, %3127, %3125, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.158 : Tensor = aten::relu_(%3141) # torch/nn/functional.py:1117:17
  %3143 : Tensor = prim::GetAttr[name="weight"](%3118)
  %3144 : Tensor? = prim::GetAttr[name="bias"](%3118)
  %3145 : int[] = prim::ListConstruct(%27, %27)
  %3146 : int[] = prim::ListConstruct(%27, %27)
  %3147 : int[] = prim::ListConstruct(%27, %27)
  %new_features.157 : Tensor = aten::conv2d(%result.158, %3143, %3144, %3145, %3146, %3147, %27) # torch/nn/modules/conv.py:415:15
  %3149 : float = prim::GetAttr[name="drop_rate"](%1888)
  %3150 : bool = aten::gt(%3149, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.158 : Tensor = prim::If(%3150) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3152 : float = prim::GetAttr[name="drop_rate"](%1888)
      %3153 : bool = prim::GetAttr[name="training"](%1888)
      %3154 : bool = aten::lt(%3152, %16) # torch/nn/functional.py:968:7
      %3155 : bool = prim::If(%3154) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3156 : bool = aten::gt(%3152, %17) # torch/nn/functional.py:968:17
          -> (%3156)
       = prim::If(%3155) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3157 : Tensor = aten::dropout(%new_features.157, %3152, %3153) # torch/nn/functional.py:973:17
      -> (%3157)
    block1():
      -> (%new_features.157)
  %3158 : Tensor[] = aten::append(%features.4, %new_features.158) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3159 : Tensor = prim::Uninitialized()
  %3160 : bool = prim::GetAttr[name="memory_efficient"](%1889)
  %3161 : bool = prim::If(%3160) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3162 : bool = prim::Uninitialized()
      %3163 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3164 : bool = aten::gt(%3163, %24)
      %3165 : bool, %3166 : bool, %3167 : int = prim::Loop(%18, %3164, %19, %3162, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3168 : int, %3169 : bool, %3170 : bool, %3171 : int):
          %tensor.80 : Tensor = aten::__getitem__(%features.4, %3171) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3173 : bool = prim::requires_grad(%tensor.80)
          %3174 : bool, %3175 : bool = prim::If(%3173) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3162)
          %3176 : int = aten::add(%3171, %27)
          %3177 : bool = aten::lt(%3176, %3163)
          %3178 : bool = aten::__and__(%3177, %3174)
          -> (%3178, %3173, %3175, %3176)
      %3179 : bool = prim::If(%3165)
        block0():
          -> (%3166)
        block1():
          -> (%19)
      -> (%3179)
    block1():
      -> (%19)
  %bottleneck_output.158 : Tensor = prim::If(%3161) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3159)
    block1():
      %concated_features.80 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3182 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv1"](%1889)
      %3183 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%1889)
      %3184 : int = aten::dim(%concated_features.80) # torch/nn/modules/batchnorm.py:276:11
      %3185 : bool = aten::ne(%3184, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3185) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3186 : bool = prim::GetAttr[name="training"](%3183)
       = prim::If(%3186) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3187 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3183)
          %3188 : Tensor = aten::add(%3187, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3183, %3188)
          -> ()
        block1():
          -> ()
      %3189 : bool = prim::GetAttr[name="training"](%3183)
      %3190 : Tensor = prim::GetAttr[name="running_mean"](%3183)
      %3191 : Tensor = prim::GetAttr[name="running_var"](%3183)
      %3192 : Tensor = prim::GetAttr[name="weight"](%3183)
      %3193 : Tensor = prim::GetAttr[name="bias"](%3183)
       = prim::If(%3189) # torch/nn/functional.py:2011:4
        block0():
          %3194 : int[] = aten::size(%concated_features.80) # torch/nn/functional.py:2012:27
          %size_prods.648 : int = aten::__getitem__(%3194, %24) # torch/nn/functional.py:1991:17
          %3196 : int = aten::len(%3194) # torch/nn/functional.py:1992:19
          %3197 : int = aten::sub(%3196, %26) # torch/nn/functional.py:1992:19
          %size_prods.649 : int = prim::Loop(%3197, %25, %size_prods.648) # torch/nn/functional.py:1992:4
            block0(%i.163 : int, %size_prods.650 : int):
              %3201 : int = aten::add(%i.163, %26) # torch/nn/functional.py:1993:27
              %3202 : int = aten::__getitem__(%3194, %3201) # torch/nn/functional.py:1993:22
              %size_prods.651 : int = aten::mul(%size_prods.650, %3202) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.651)
          %3204 : bool = aten::eq(%size_prods.649, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3204) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3205 : Tensor = aten::batch_norm(%concated_features.80, %3192, %3193, %3190, %3191, %3189, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.159 : Tensor = aten::relu_(%3205) # torch/nn/functional.py:1117:17
      %3207 : Tensor = prim::GetAttr[name="weight"](%3182)
      %3208 : Tensor? = prim::GetAttr[name="bias"](%3182)
      %3209 : int[] = prim::ListConstruct(%27, %27)
      %3210 : int[] = prim::ListConstruct(%24, %24)
      %3211 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.159 : Tensor = aten::conv2d(%result.159, %3207, %3208, %3209, %3210, %3211, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.159)
  %3213 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1889)
  %3214 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1889)
  %3215 : int = aten::dim(%bottleneck_output.158) # torch/nn/modules/batchnorm.py:276:11
  %3216 : bool = aten::ne(%3215, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3216) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3217 : bool = prim::GetAttr[name="training"](%3214)
   = prim::If(%3217) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3218 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3214)
      %3219 : Tensor = aten::add(%3218, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3214, %3219)
      -> ()
    block1():
      -> ()
  %3220 : bool = prim::GetAttr[name="training"](%3214)
  %3221 : Tensor = prim::GetAttr[name="running_mean"](%3214)
  %3222 : Tensor = prim::GetAttr[name="running_var"](%3214)
  %3223 : Tensor = prim::GetAttr[name="weight"](%3214)
  %3224 : Tensor = prim::GetAttr[name="bias"](%3214)
   = prim::If(%3220) # torch/nn/functional.py:2011:4
    block0():
      %3225 : int[] = aten::size(%bottleneck_output.158) # torch/nn/functional.py:2012:27
      %size_prods.652 : int = aten::__getitem__(%3225, %24) # torch/nn/functional.py:1991:17
      %3227 : int = aten::len(%3225) # torch/nn/functional.py:1992:19
      %3228 : int = aten::sub(%3227, %26) # torch/nn/functional.py:1992:19
      %size_prods.653 : int = prim::Loop(%3228, %25, %size_prods.652) # torch/nn/functional.py:1992:4
        block0(%i.164 : int, %size_prods.654 : int):
          %3232 : int = aten::add(%i.164, %26) # torch/nn/functional.py:1993:27
          %3233 : int = aten::__getitem__(%3225, %3232) # torch/nn/functional.py:1993:22
          %size_prods.655 : int = aten::mul(%size_prods.654, %3233) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.655)
      %3235 : bool = aten::eq(%size_prods.653, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3235) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3236 : Tensor = aten::batch_norm(%bottleneck_output.158, %3223, %3224, %3221, %3222, %3220, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.160 : Tensor = aten::relu_(%3236) # torch/nn/functional.py:1117:17
  %3238 : Tensor = prim::GetAttr[name="weight"](%3213)
  %3239 : Tensor? = prim::GetAttr[name="bias"](%3213)
  %3240 : int[] = prim::ListConstruct(%27, %27)
  %3241 : int[] = prim::ListConstruct(%27, %27)
  %3242 : int[] = prim::ListConstruct(%27, %27)
  %new_features.159 : Tensor = aten::conv2d(%result.160, %3238, %3239, %3240, %3241, %3242, %27) # torch/nn/modules/conv.py:415:15
  %3244 : float = prim::GetAttr[name="drop_rate"](%1889)
  %3245 : bool = aten::gt(%3244, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.160 : Tensor = prim::If(%3245) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3247 : float = prim::GetAttr[name="drop_rate"](%1889)
      %3248 : bool = prim::GetAttr[name="training"](%1889)
      %3249 : bool = aten::lt(%3247, %16) # torch/nn/functional.py:968:7
      %3250 : bool = prim::If(%3249) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3251 : bool = aten::gt(%3247, %17) # torch/nn/functional.py:968:17
          -> (%3251)
       = prim::If(%3250) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3252 : Tensor = aten::dropout(%new_features.159, %3247, %3248) # torch/nn/functional.py:973:17
      -> (%3252)
    block1():
      -> (%new_features.159)
  %3253 : Tensor[] = aten::append(%features.4, %new_features.160) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3254 : Tensor = prim::Uninitialized()
  %3255 : bool = prim::GetAttr[name="memory_efficient"](%1890)
  %3256 : bool = prim::If(%3255) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3257 : bool = prim::Uninitialized()
      %3258 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3259 : bool = aten::gt(%3258, %24)
      %3260 : bool, %3261 : bool, %3262 : int = prim::Loop(%18, %3259, %19, %3257, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3263 : int, %3264 : bool, %3265 : bool, %3266 : int):
          %tensor.81 : Tensor = aten::__getitem__(%features.4, %3266) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3268 : bool = prim::requires_grad(%tensor.81)
          %3269 : bool, %3270 : bool = prim::If(%3268) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3257)
          %3271 : int = aten::add(%3266, %27)
          %3272 : bool = aten::lt(%3271, %3258)
          %3273 : bool = aten::__and__(%3272, %3269)
          -> (%3273, %3268, %3270, %3271)
      %3274 : bool = prim::If(%3260)
        block0():
          -> (%3261)
        block1():
          -> (%19)
      -> (%3274)
    block1():
      -> (%19)
  %bottleneck_output.160 : Tensor = prim::If(%3256) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3254)
    block1():
      %concated_features.81 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3277 : __torch__.torch.nn.modules.conv.___torch_mangle_128.Conv2d = prim::GetAttr[name="conv1"](%1890)
      %3278 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="norm1"](%1890)
      %3279 : int = aten::dim(%concated_features.81) # torch/nn/modules/batchnorm.py:276:11
      %3280 : bool = aten::ne(%3279, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3280) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3281 : bool = prim::GetAttr[name="training"](%3278)
       = prim::If(%3281) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3282 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3278)
          %3283 : Tensor = aten::add(%3282, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3278, %3283)
          -> ()
        block1():
          -> ()
      %3284 : bool = prim::GetAttr[name="training"](%3278)
      %3285 : Tensor = prim::GetAttr[name="running_mean"](%3278)
      %3286 : Tensor = prim::GetAttr[name="running_var"](%3278)
      %3287 : Tensor = prim::GetAttr[name="weight"](%3278)
      %3288 : Tensor = prim::GetAttr[name="bias"](%3278)
       = prim::If(%3284) # torch/nn/functional.py:2011:4
        block0():
          %3289 : int[] = aten::size(%concated_features.81) # torch/nn/functional.py:2012:27
          %size_prods.656 : int = aten::__getitem__(%3289, %24) # torch/nn/functional.py:1991:17
          %3291 : int = aten::len(%3289) # torch/nn/functional.py:1992:19
          %3292 : int = aten::sub(%3291, %26) # torch/nn/functional.py:1992:19
          %size_prods.657 : int = prim::Loop(%3292, %25, %size_prods.656) # torch/nn/functional.py:1992:4
            block0(%i.165 : int, %size_prods.658 : int):
              %3296 : int = aten::add(%i.165, %26) # torch/nn/functional.py:1993:27
              %3297 : int = aten::__getitem__(%3289, %3296) # torch/nn/functional.py:1993:22
              %size_prods.659 : int = aten::mul(%size_prods.658, %3297) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.659)
          %3299 : bool = aten::eq(%size_prods.657, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3299) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3300 : Tensor = aten::batch_norm(%concated_features.81, %3287, %3288, %3285, %3286, %3284, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.161 : Tensor = aten::relu_(%3300) # torch/nn/functional.py:1117:17
      %3302 : Tensor = prim::GetAttr[name="weight"](%3277)
      %3303 : Tensor? = prim::GetAttr[name="bias"](%3277)
      %3304 : int[] = prim::ListConstruct(%27, %27)
      %3305 : int[] = prim::ListConstruct(%24, %24)
      %3306 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.161 : Tensor = aten::conv2d(%result.161, %3302, %3303, %3304, %3305, %3306, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.161)
  %3308 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1890)
  %3309 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1890)
  %3310 : int = aten::dim(%bottleneck_output.160) # torch/nn/modules/batchnorm.py:276:11
  %3311 : bool = aten::ne(%3310, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3311) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3312 : bool = prim::GetAttr[name="training"](%3309)
   = prim::If(%3312) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3313 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3309)
      %3314 : Tensor = aten::add(%3313, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3309, %3314)
      -> ()
    block1():
      -> ()
  %3315 : bool = prim::GetAttr[name="training"](%3309)
  %3316 : Tensor = prim::GetAttr[name="running_mean"](%3309)
  %3317 : Tensor = prim::GetAttr[name="running_var"](%3309)
  %3318 : Tensor = prim::GetAttr[name="weight"](%3309)
  %3319 : Tensor = prim::GetAttr[name="bias"](%3309)
   = prim::If(%3315) # torch/nn/functional.py:2011:4
    block0():
      %3320 : int[] = aten::size(%bottleneck_output.160) # torch/nn/functional.py:2012:27
      %size_prods.660 : int = aten::__getitem__(%3320, %24) # torch/nn/functional.py:1991:17
      %3322 : int = aten::len(%3320) # torch/nn/functional.py:1992:19
      %3323 : int = aten::sub(%3322, %26) # torch/nn/functional.py:1992:19
      %size_prods.661 : int = prim::Loop(%3323, %25, %size_prods.660) # torch/nn/functional.py:1992:4
        block0(%i.166 : int, %size_prods.662 : int):
          %3327 : int = aten::add(%i.166, %26) # torch/nn/functional.py:1993:27
          %3328 : int = aten::__getitem__(%3320, %3327) # torch/nn/functional.py:1993:22
          %size_prods.663 : int = aten::mul(%size_prods.662, %3328) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.663)
      %3330 : bool = aten::eq(%size_prods.661, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3330) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3331 : Tensor = aten::batch_norm(%bottleneck_output.160, %3318, %3319, %3316, %3317, %3315, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.162 : Tensor = aten::relu_(%3331) # torch/nn/functional.py:1117:17
  %3333 : Tensor = prim::GetAttr[name="weight"](%3308)
  %3334 : Tensor? = prim::GetAttr[name="bias"](%3308)
  %3335 : int[] = prim::ListConstruct(%27, %27)
  %3336 : int[] = prim::ListConstruct(%27, %27)
  %3337 : int[] = prim::ListConstruct(%27, %27)
  %new_features.161 : Tensor = aten::conv2d(%result.162, %3333, %3334, %3335, %3336, %3337, %27) # torch/nn/modules/conv.py:415:15
  %3339 : float = prim::GetAttr[name="drop_rate"](%1890)
  %3340 : bool = aten::gt(%3339, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.162 : Tensor = prim::If(%3340) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3342 : float = prim::GetAttr[name="drop_rate"](%1890)
      %3343 : bool = prim::GetAttr[name="training"](%1890)
      %3344 : bool = aten::lt(%3342, %16) # torch/nn/functional.py:968:7
      %3345 : bool = prim::If(%3344) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3346 : bool = aten::gt(%3342, %17) # torch/nn/functional.py:968:17
          -> (%3346)
       = prim::If(%3345) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3347 : Tensor = aten::dropout(%new_features.161, %3342, %3343) # torch/nn/functional.py:973:17
      -> (%3347)
    block1():
      -> (%new_features.161)
  %3348 : Tensor[] = aten::append(%features.4, %new_features.162) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3349 : Tensor = prim::Uninitialized()
  %3350 : bool = prim::GetAttr[name="memory_efficient"](%1891)
  %3351 : bool = prim::If(%3350) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3352 : bool = prim::Uninitialized()
      %3353 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3354 : bool = aten::gt(%3353, %24)
      %3355 : bool, %3356 : bool, %3357 : int = prim::Loop(%18, %3354, %19, %3352, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3358 : int, %3359 : bool, %3360 : bool, %3361 : int):
          %tensor.82 : Tensor = aten::__getitem__(%features.4, %3361) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3363 : bool = prim::requires_grad(%tensor.82)
          %3364 : bool, %3365 : bool = prim::If(%3363) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3352)
          %3366 : int = aten::add(%3361, %27)
          %3367 : bool = aten::lt(%3366, %3353)
          %3368 : bool = aten::__and__(%3367, %3364)
          -> (%3368, %3363, %3365, %3366)
      %3369 : bool = prim::If(%3355)
        block0():
          -> (%3356)
        block1():
          -> (%19)
      -> (%3369)
    block1():
      -> (%19)
  %bottleneck_output.162 : Tensor = prim::If(%3351) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3349)
    block1():
      %concated_features.82 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3372 : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d = prim::GetAttr[name="conv1"](%1891)
      %3373 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_130.BatchNorm2d = prim::GetAttr[name="norm1"](%1891)
      %3374 : int = aten::dim(%concated_features.82) # torch/nn/modules/batchnorm.py:276:11
      %3375 : bool = aten::ne(%3374, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3375) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3376 : bool = prim::GetAttr[name="training"](%3373)
       = prim::If(%3376) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3377 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3373)
          %3378 : Tensor = aten::add(%3377, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3373, %3378)
          -> ()
        block1():
          -> ()
      %3379 : bool = prim::GetAttr[name="training"](%3373)
      %3380 : Tensor = prim::GetAttr[name="running_mean"](%3373)
      %3381 : Tensor = prim::GetAttr[name="running_var"](%3373)
      %3382 : Tensor = prim::GetAttr[name="weight"](%3373)
      %3383 : Tensor = prim::GetAttr[name="bias"](%3373)
       = prim::If(%3379) # torch/nn/functional.py:2011:4
        block0():
          %3384 : int[] = aten::size(%concated_features.82) # torch/nn/functional.py:2012:27
          %size_prods.664 : int = aten::__getitem__(%3384, %24) # torch/nn/functional.py:1991:17
          %3386 : int = aten::len(%3384) # torch/nn/functional.py:1992:19
          %3387 : int = aten::sub(%3386, %26) # torch/nn/functional.py:1992:19
          %size_prods.665 : int = prim::Loop(%3387, %25, %size_prods.664) # torch/nn/functional.py:1992:4
            block0(%i.167 : int, %size_prods.666 : int):
              %3391 : int = aten::add(%i.167, %26) # torch/nn/functional.py:1993:27
              %3392 : int = aten::__getitem__(%3384, %3391) # torch/nn/functional.py:1993:22
              %size_prods.667 : int = aten::mul(%size_prods.666, %3392) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.667)
          %3394 : bool = aten::eq(%size_prods.665, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3394) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3395 : Tensor = aten::batch_norm(%concated_features.82, %3382, %3383, %3380, %3381, %3379, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.163 : Tensor = aten::relu_(%3395) # torch/nn/functional.py:1117:17
      %3397 : Tensor = prim::GetAttr[name="weight"](%3372)
      %3398 : Tensor? = prim::GetAttr[name="bias"](%3372)
      %3399 : int[] = prim::ListConstruct(%27, %27)
      %3400 : int[] = prim::ListConstruct(%24, %24)
      %3401 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.163 : Tensor = aten::conv2d(%result.163, %3397, %3398, %3399, %3400, %3401, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.163)
  %3403 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1891)
  %3404 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1891)
  %3405 : int = aten::dim(%bottleneck_output.162) # torch/nn/modules/batchnorm.py:276:11
  %3406 : bool = aten::ne(%3405, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3406) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3407 : bool = prim::GetAttr[name="training"](%3404)
   = prim::If(%3407) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3408 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3404)
      %3409 : Tensor = aten::add(%3408, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3404, %3409)
      -> ()
    block1():
      -> ()
  %3410 : bool = prim::GetAttr[name="training"](%3404)
  %3411 : Tensor = prim::GetAttr[name="running_mean"](%3404)
  %3412 : Tensor = prim::GetAttr[name="running_var"](%3404)
  %3413 : Tensor = prim::GetAttr[name="weight"](%3404)
  %3414 : Tensor = prim::GetAttr[name="bias"](%3404)
   = prim::If(%3410) # torch/nn/functional.py:2011:4
    block0():
      %3415 : int[] = aten::size(%bottleneck_output.162) # torch/nn/functional.py:2012:27
      %size_prods.668 : int = aten::__getitem__(%3415, %24) # torch/nn/functional.py:1991:17
      %3417 : int = aten::len(%3415) # torch/nn/functional.py:1992:19
      %3418 : int = aten::sub(%3417, %26) # torch/nn/functional.py:1992:19
      %size_prods.669 : int = prim::Loop(%3418, %25, %size_prods.668) # torch/nn/functional.py:1992:4
        block0(%i.168 : int, %size_prods.670 : int):
          %3422 : int = aten::add(%i.168, %26) # torch/nn/functional.py:1993:27
          %3423 : int = aten::__getitem__(%3415, %3422) # torch/nn/functional.py:1993:22
          %size_prods.671 : int = aten::mul(%size_prods.670, %3423) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.671)
      %3425 : bool = aten::eq(%size_prods.669, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3425) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3426 : Tensor = aten::batch_norm(%bottleneck_output.162, %3413, %3414, %3411, %3412, %3410, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.164 : Tensor = aten::relu_(%3426) # torch/nn/functional.py:1117:17
  %3428 : Tensor = prim::GetAttr[name="weight"](%3403)
  %3429 : Tensor? = prim::GetAttr[name="bias"](%3403)
  %3430 : int[] = prim::ListConstruct(%27, %27)
  %3431 : int[] = prim::ListConstruct(%27, %27)
  %3432 : int[] = prim::ListConstruct(%27, %27)
  %new_features.163 : Tensor = aten::conv2d(%result.164, %3428, %3429, %3430, %3431, %3432, %27) # torch/nn/modules/conv.py:415:15
  %3434 : float = prim::GetAttr[name="drop_rate"](%1891)
  %3435 : bool = aten::gt(%3434, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.164 : Tensor = prim::If(%3435) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3437 : float = prim::GetAttr[name="drop_rate"](%1891)
      %3438 : bool = prim::GetAttr[name="training"](%1891)
      %3439 : bool = aten::lt(%3437, %16) # torch/nn/functional.py:968:7
      %3440 : bool = prim::If(%3439) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3441 : bool = aten::gt(%3437, %17) # torch/nn/functional.py:968:17
          -> (%3441)
       = prim::If(%3440) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3442 : Tensor = aten::dropout(%new_features.163, %3437, %3438) # torch/nn/functional.py:973:17
      -> (%3442)
    block1():
      -> (%new_features.163)
  %3443 : Tensor[] = aten::append(%features.4, %new_features.164) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3444 : Tensor = prim::Uninitialized()
  %3445 : bool = prim::GetAttr[name="memory_efficient"](%1892)
  %3446 : bool = prim::If(%3445) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3447 : bool = prim::Uninitialized()
      %3448 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3449 : bool = aten::gt(%3448, %24)
      %3450 : bool, %3451 : bool, %3452 : int = prim::Loop(%18, %3449, %19, %3447, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3453 : int, %3454 : bool, %3455 : bool, %3456 : int):
          %tensor.83 : Tensor = aten::__getitem__(%features.4, %3456) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3458 : bool = prim::requires_grad(%tensor.83)
          %3459 : bool, %3460 : bool = prim::If(%3458) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3447)
          %3461 : int = aten::add(%3456, %27)
          %3462 : bool = aten::lt(%3461, %3448)
          %3463 : bool = aten::__and__(%3462, %3459)
          -> (%3463, %3458, %3460, %3461)
      %3464 : bool = prim::If(%3450)
        block0():
          -> (%3451)
        block1():
          -> (%19)
      -> (%3464)
    block1():
      -> (%19)
  %bottleneck_output.164 : Tensor = prim::If(%3446) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3444)
    block1():
      %concated_features.83 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3467 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%1892)
      %3468 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%1892)
      %3469 : int = aten::dim(%concated_features.83) # torch/nn/modules/batchnorm.py:276:11
      %3470 : bool = aten::ne(%3469, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3470) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3471 : bool = prim::GetAttr[name="training"](%3468)
       = prim::If(%3471) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3472 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3468)
          %3473 : Tensor = aten::add(%3472, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3468, %3473)
          -> ()
        block1():
          -> ()
      %3474 : bool = prim::GetAttr[name="training"](%3468)
      %3475 : Tensor = prim::GetAttr[name="running_mean"](%3468)
      %3476 : Tensor = prim::GetAttr[name="running_var"](%3468)
      %3477 : Tensor = prim::GetAttr[name="weight"](%3468)
      %3478 : Tensor = prim::GetAttr[name="bias"](%3468)
       = prim::If(%3474) # torch/nn/functional.py:2011:4
        block0():
          %3479 : int[] = aten::size(%concated_features.83) # torch/nn/functional.py:2012:27
          %size_prods.672 : int = aten::__getitem__(%3479, %24) # torch/nn/functional.py:1991:17
          %3481 : int = aten::len(%3479) # torch/nn/functional.py:1992:19
          %3482 : int = aten::sub(%3481, %26) # torch/nn/functional.py:1992:19
          %size_prods.673 : int = prim::Loop(%3482, %25, %size_prods.672) # torch/nn/functional.py:1992:4
            block0(%i.169 : int, %size_prods.674 : int):
              %3486 : int = aten::add(%i.169, %26) # torch/nn/functional.py:1993:27
              %3487 : int = aten::__getitem__(%3479, %3486) # torch/nn/functional.py:1993:22
              %size_prods.675 : int = aten::mul(%size_prods.674, %3487) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.675)
          %3489 : bool = aten::eq(%size_prods.673, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3489) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3490 : Tensor = aten::batch_norm(%concated_features.83, %3477, %3478, %3475, %3476, %3474, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.165 : Tensor = aten::relu_(%3490) # torch/nn/functional.py:1117:17
      %3492 : Tensor = prim::GetAttr[name="weight"](%3467)
      %3493 : Tensor? = prim::GetAttr[name="bias"](%3467)
      %3494 : int[] = prim::ListConstruct(%27, %27)
      %3495 : int[] = prim::ListConstruct(%24, %24)
      %3496 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.165 : Tensor = aten::conv2d(%result.165, %3492, %3493, %3494, %3495, %3496, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.165)
  %3498 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1892)
  %3499 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1892)
  %3500 : int = aten::dim(%bottleneck_output.164) # torch/nn/modules/batchnorm.py:276:11
  %3501 : bool = aten::ne(%3500, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3501) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3502 : bool = prim::GetAttr[name="training"](%3499)
   = prim::If(%3502) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3503 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3499)
      %3504 : Tensor = aten::add(%3503, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3499, %3504)
      -> ()
    block1():
      -> ()
  %3505 : bool = prim::GetAttr[name="training"](%3499)
  %3506 : Tensor = prim::GetAttr[name="running_mean"](%3499)
  %3507 : Tensor = prim::GetAttr[name="running_var"](%3499)
  %3508 : Tensor = prim::GetAttr[name="weight"](%3499)
  %3509 : Tensor = prim::GetAttr[name="bias"](%3499)
   = prim::If(%3505) # torch/nn/functional.py:2011:4
    block0():
      %3510 : int[] = aten::size(%bottleneck_output.164) # torch/nn/functional.py:2012:27
      %size_prods.676 : int = aten::__getitem__(%3510, %24) # torch/nn/functional.py:1991:17
      %3512 : int = aten::len(%3510) # torch/nn/functional.py:1992:19
      %3513 : int = aten::sub(%3512, %26) # torch/nn/functional.py:1992:19
      %size_prods.677 : int = prim::Loop(%3513, %25, %size_prods.676) # torch/nn/functional.py:1992:4
        block0(%i.170 : int, %size_prods.678 : int):
          %3517 : int = aten::add(%i.170, %26) # torch/nn/functional.py:1993:27
          %3518 : int = aten::__getitem__(%3510, %3517) # torch/nn/functional.py:1993:22
          %size_prods.679 : int = aten::mul(%size_prods.678, %3518) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.679)
      %3520 : bool = aten::eq(%size_prods.677, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3520) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3521 : Tensor = aten::batch_norm(%bottleneck_output.164, %3508, %3509, %3506, %3507, %3505, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.166 : Tensor = aten::relu_(%3521) # torch/nn/functional.py:1117:17
  %3523 : Tensor = prim::GetAttr[name="weight"](%3498)
  %3524 : Tensor? = prim::GetAttr[name="bias"](%3498)
  %3525 : int[] = prim::ListConstruct(%27, %27)
  %3526 : int[] = prim::ListConstruct(%27, %27)
  %3527 : int[] = prim::ListConstruct(%27, %27)
  %new_features.165 : Tensor = aten::conv2d(%result.166, %3523, %3524, %3525, %3526, %3527, %27) # torch/nn/modules/conv.py:415:15
  %3529 : float = prim::GetAttr[name="drop_rate"](%1892)
  %3530 : bool = aten::gt(%3529, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.166 : Tensor = prim::If(%3530) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3532 : float = prim::GetAttr[name="drop_rate"](%1892)
      %3533 : bool = prim::GetAttr[name="training"](%1892)
      %3534 : bool = aten::lt(%3532, %16) # torch/nn/functional.py:968:7
      %3535 : bool = prim::If(%3534) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3536 : bool = aten::gt(%3532, %17) # torch/nn/functional.py:968:17
          -> (%3536)
       = prim::If(%3535) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3537 : Tensor = aten::dropout(%new_features.165, %3532, %3533) # torch/nn/functional.py:973:17
      -> (%3537)
    block1():
      -> (%new_features.165)
  %3538 : Tensor[] = aten::append(%features.4, %new_features.166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3539 : Tensor = prim::Uninitialized()
  %3540 : bool = prim::GetAttr[name="memory_efficient"](%1893)
  %3541 : bool = prim::If(%3540) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3542 : bool = prim::Uninitialized()
      %3543 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3544 : bool = aten::gt(%3543, %24)
      %3545 : bool, %3546 : bool, %3547 : int = prim::Loop(%18, %3544, %19, %3542, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3548 : int, %3549 : bool, %3550 : bool, %3551 : int):
          %tensor.84 : Tensor = aten::__getitem__(%features.4, %3551) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3553 : bool = prim::requires_grad(%tensor.84)
          %3554 : bool, %3555 : bool = prim::If(%3553) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3542)
          %3556 : int = aten::add(%3551, %27)
          %3557 : bool = aten::lt(%3556, %3543)
          %3558 : bool = aten::__and__(%3557, %3554)
          -> (%3558, %3553, %3555, %3556)
      %3559 : bool = prim::If(%3545)
        block0():
          -> (%3546)
        block1():
          -> (%19)
      -> (%3559)
    block1():
      -> (%19)
  %bottleneck_output.166 : Tensor = prim::If(%3541) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3539)
    block1():
      %concated_features.84 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3562 : __torch__.torch.nn.modules.conv.___torch_mangle_137.Conv2d = prim::GetAttr[name="conv1"](%1893)
      %3563 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_136.BatchNorm2d = prim::GetAttr[name="norm1"](%1893)
      %3564 : int = aten::dim(%concated_features.84) # torch/nn/modules/batchnorm.py:276:11
      %3565 : bool = aten::ne(%3564, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3565) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3566 : bool = prim::GetAttr[name="training"](%3563)
       = prim::If(%3566) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3567 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3563)
          %3568 : Tensor = aten::add(%3567, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3563, %3568)
          -> ()
        block1():
          -> ()
      %3569 : bool = prim::GetAttr[name="training"](%3563)
      %3570 : Tensor = prim::GetAttr[name="running_mean"](%3563)
      %3571 : Tensor = prim::GetAttr[name="running_var"](%3563)
      %3572 : Tensor = prim::GetAttr[name="weight"](%3563)
      %3573 : Tensor = prim::GetAttr[name="bias"](%3563)
       = prim::If(%3569) # torch/nn/functional.py:2011:4
        block0():
          %3574 : int[] = aten::size(%concated_features.84) # torch/nn/functional.py:2012:27
          %size_prods.680 : int = aten::__getitem__(%3574, %24) # torch/nn/functional.py:1991:17
          %3576 : int = aten::len(%3574) # torch/nn/functional.py:1992:19
          %3577 : int = aten::sub(%3576, %26) # torch/nn/functional.py:1992:19
          %size_prods.681 : int = prim::Loop(%3577, %25, %size_prods.680) # torch/nn/functional.py:1992:4
            block0(%i.171 : int, %size_prods.682 : int):
              %3581 : int = aten::add(%i.171, %26) # torch/nn/functional.py:1993:27
              %3582 : int = aten::__getitem__(%3574, %3581) # torch/nn/functional.py:1993:22
              %size_prods.683 : int = aten::mul(%size_prods.682, %3582) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.683)
          %3584 : bool = aten::eq(%size_prods.681, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3584) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3585 : Tensor = aten::batch_norm(%concated_features.84, %3572, %3573, %3570, %3571, %3569, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.167 : Tensor = aten::relu_(%3585) # torch/nn/functional.py:1117:17
      %3587 : Tensor = prim::GetAttr[name="weight"](%3562)
      %3588 : Tensor? = prim::GetAttr[name="bias"](%3562)
      %3589 : int[] = prim::ListConstruct(%27, %27)
      %3590 : int[] = prim::ListConstruct(%24, %24)
      %3591 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.167 : Tensor = aten::conv2d(%result.167, %3587, %3588, %3589, %3590, %3591, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.167)
  %3593 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1893)
  %3594 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1893)
  %3595 : int = aten::dim(%bottleneck_output.166) # torch/nn/modules/batchnorm.py:276:11
  %3596 : bool = aten::ne(%3595, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3596) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3597 : bool = prim::GetAttr[name="training"](%3594)
   = prim::If(%3597) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3598 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3594)
      %3599 : Tensor = aten::add(%3598, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3594, %3599)
      -> ()
    block1():
      -> ()
  %3600 : bool = prim::GetAttr[name="training"](%3594)
  %3601 : Tensor = prim::GetAttr[name="running_mean"](%3594)
  %3602 : Tensor = prim::GetAttr[name="running_var"](%3594)
  %3603 : Tensor = prim::GetAttr[name="weight"](%3594)
  %3604 : Tensor = prim::GetAttr[name="bias"](%3594)
   = prim::If(%3600) # torch/nn/functional.py:2011:4
    block0():
      %3605 : int[] = aten::size(%bottleneck_output.166) # torch/nn/functional.py:2012:27
      %size_prods.684 : int = aten::__getitem__(%3605, %24) # torch/nn/functional.py:1991:17
      %3607 : int = aten::len(%3605) # torch/nn/functional.py:1992:19
      %3608 : int = aten::sub(%3607, %26) # torch/nn/functional.py:1992:19
      %size_prods.685 : int = prim::Loop(%3608, %25, %size_prods.684) # torch/nn/functional.py:1992:4
        block0(%i.172 : int, %size_prods.686 : int):
          %3612 : int = aten::add(%i.172, %26) # torch/nn/functional.py:1993:27
          %3613 : int = aten::__getitem__(%3605, %3612) # torch/nn/functional.py:1993:22
          %size_prods.687 : int = aten::mul(%size_prods.686, %3613) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.687)
      %3615 : bool = aten::eq(%size_prods.685, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3615) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3616 : Tensor = aten::batch_norm(%bottleneck_output.166, %3603, %3604, %3601, %3602, %3600, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.168 : Tensor = aten::relu_(%3616) # torch/nn/functional.py:1117:17
  %3618 : Tensor = prim::GetAttr[name="weight"](%3593)
  %3619 : Tensor? = prim::GetAttr[name="bias"](%3593)
  %3620 : int[] = prim::ListConstruct(%27, %27)
  %3621 : int[] = prim::ListConstruct(%27, %27)
  %3622 : int[] = prim::ListConstruct(%27, %27)
  %new_features.167 : Tensor = aten::conv2d(%result.168, %3618, %3619, %3620, %3621, %3622, %27) # torch/nn/modules/conv.py:415:15
  %3624 : float = prim::GetAttr[name="drop_rate"](%1893)
  %3625 : bool = aten::gt(%3624, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.168 : Tensor = prim::If(%3625) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3627 : float = prim::GetAttr[name="drop_rate"](%1893)
      %3628 : bool = prim::GetAttr[name="training"](%1893)
      %3629 : bool = aten::lt(%3627, %16) # torch/nn/functional.py:968:7
      %3630 : bool = prim::If(%3629) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3631 : bool = aten::gt(%3627, %17) # torch/nn/functional.py:968:17
          -> (%3631)
       = prim::If(%3630) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3632 : Tensor = aten::dropout(%new_features.167, %3627, %3628) # torch/nn/functional.py:973:17
      -> (%3632)
    block1():
      -> (%new_features.167)
  %3633 : Tensor[] = aten::append(%features.4, %new_features.168) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3634 : Tensor = prim::Uninitialized()
  %3635 : bool = prim::GetAttr[name="memory_efficient"](%1894)
  %3636 : bool = prim::If(%3635) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3637 : bool = prim::Uninitialized()
      %3638 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3639 : bool = aten::gt(%3638, %24)
      %3640 : bool, %3641 : bool, %3642 : int = prim::Loop(%18, %3639, %19, %3637, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3643 : int, %3644 : bool, %3645 : bool, %3646 : int):
          %tensor.85 : Tensor = aten::__getitem__(%features.4, %3646) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3648 : bool = prim::requires_grad(%tensor.85)
          %3649 : bool, %3650 : bool = prim::If(%3648) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3637)
          %3651 : int = aten::add(%3646, %27)
          %3652 : bool = aten::lt(%3651, %3638)
          %3653 : bool = aten::__and__(%3652, %3649)
          -> (%3653, %3648, %3650, %3651)
      %3654 : bool = prim::If(%3640)
        block0():
          -> (%3641)
        block1():
          -> (%19)
      -> (%3654)
    block1():
      -> (%19)
  %bottleneck_output.168 : Tensor = prim::If(%3636) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3634)
    block1():
      %concated_features.85 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3657 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv1"](%1894)
      %3658 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="norm1"](%1894)
      %3659 : int = aten::dim(%concated_features.85) # torch/nn/modules/batchnorm.py:276:11
      %3660 : bool = aten::ne(%3659, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3660) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3661 : bool = prim::GetAttr[name="training"](%3658)
       = prim::If(%3661) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3662 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3658)
          %3663 : Tensor = aten::add(%3662, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3658, %3663)
          -> ()
        block1():
          -> ()
      %3664 : bool = prim::GetAttr[name="training"](%3658)
      %3665 : Tensor = prim::GetAttr[name="running_mean"](%3658)
      %3666 : Tensor = prim::GetAttr[name="running_var"](%3658)
      %3667 : Tensor = prim::GetAttr[name="weight"](%3658)
      %3668 : Tensor = prim::GetAttr[name="bias"](%3658)
       = prim::If(%3664) # torch/nn/functional.py:2011:4
        block0():
          %3669 : int[] = aten::size(%concated_features.85) # torch/nn/functional.py:2012:27
          %size_prods.688 : int = aten::__getitem__(%3669, %24) # torch/nn/functional.py:1991:17
          %3671 : int = aten::len(%3669) # torch/nn/functional.py:1992:19
          %3672 : int = aten::sub(%3671, %26) # torch/nn/functional.py:1992:19
          %size_prods.689 : int = prim::Loop(%3672, %25, %size_prods.688) # torch/nn/functional.py:1992:4
            block0(%i.173 : int, %size_prods.690 : int):
              %3676 : int = aten::add(%i.173, %26) # torch/nn/functional.py:1993:27
              %3677 : int = aten::__getitem__(%3669, %3676) # torch/nn/functional.py:1993:22
              %size_prods.691 : int = aten::mul(%size_prods.690, %3677) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.691)
          %3679 : bool = aten::eq(%size_prods.689, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3679) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3680 : Tensor = aten::batch_norm(%concated_features.85, %3667, %3668, %3665, %3666, %3664, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.169 : Tensor = aten::relu_(%3680) # torch/nn/functional.py:1117:17
      %3682 : Tensor = prim::GetAttr[name="weight"](%3657)
      %3683 : Tensor? = prim::GetAttr[name="bias"](%3657)
      %3684 : int[] = prim::ListConstruct(%27, %27)
      %3685 : int[] = prim::ListConstruct(%24, %24)
      %3686 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.169 : Tensor = aten::conv2d(%result.169, %3682, %3683, %3684, %3685, %3686, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.169)
  %3688 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1894)
  %3689 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1894)
  %3690 : int = aten::dim(%bottleneck_output.168) # torch/nn/modules/batchnorm.py:276:11
  %3691 : bool = aten::ne(%3690, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3691) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3692 : bool = prim::GetAttr[name="training"](%3689)
   = prim::If(%3692) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3693 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3689)
      %3694 : Tensor = aten::add(%3693, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3689, %3694)
      -> ()
    block1():
      -> ()
  %3695 : bool = prim::GetAttr[name="training"](%3689)
  %3696 : Tensor = prim::GetAttr[name="running_mean"](%3689)
  %3697 : Tensor = prim::GetAttr[name="running_var"](%3689)
  %3698 : Tensor = prim::GetAttr[name="weight"](%3689)
  %3699 : Tensor = prim::GetAttr[name="bias"](%3689)
   = prim::If(%3695) # torch/nn/functional.py:2011:4
    block0():
      %3700 : int[] = aten::size(%bottleneck_output.168) # torch/nn/functional.py:2012:27
      %size_prods.692 : int = aten::__getitem__(%3700, %24) # torch/nn/functional.py:1991:17
      %3702 : int = aten::len(%3700) # torch/nn/functional.py:1992:19
      %3703 : int = aten::sub(%3702, %26) # torch/nn/functional.py:1992:19
      %size_prods.693 : int = prim::Loop(%3703, %25, %size_prods.692) # torch/nn/functional.py:1992:4
        block0(%i.174 : int, %size_prods.694 : int):
          %3707 : int = aten::add(%i.174, %26) # torch/nn/functional.py:1993:27
          %3708 : int = aten::__getitem__(%3700, %3707) # torch/nn/functional.py:1993:22
          %size_prods.695 : int = aten::mul(%size_prods.694, %3708) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.695)
      %3710 : bool = aten::eq(%size_prods.693, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3710) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3711 : Tensor = aten::batch_norm(%bottleneck_output.168, %3698, %3699, %3696, %3697, %3695, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.170 : Tensor = aten::relu_(%3711) # torch/nn/functional.py:1117:17
  %3713 : Tensor = prim::GetAttr[name="weight"](%3688)
  %3714 : Tensor? = prim::GetAttr[name="bias"](%3688)
  %3715 : int[] = prim::ListConstruct(%27, %27)
  %3716 : int[] = prim::ListConstruct(%27, %27)
  %3717 : int[] = prim::ListConstruct(%27, %27)
  %new_features.169 : Tensor = aten::conv2d(%result.170, %3713, %3714, %3715, %3716, %3717, %27) # torch/nn/modules/conv.py:415:15
  %3719 : float = prim::GetAttr[name="drop_rate"](%1894)
  %3720 : bool = aten::gt(%3719, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.170 : Tensor = prim::If(%3720) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3722 : float = prim::GetAttr[name="drop_rate"](%1894)
      %3723 : bool = prim::GetAttr[name="training"](%1894)
      %3724 : bool = aten::lt(%3722, %16) # torch/nn/functional.py:968:7
      %3725 : bool = prim::If(%3724) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3726 : bool = aten::gt(%3722, %17) # torch/nn/functional.py:968:17
          -> (%3726)
       = prim::If(%3725) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3727 : Tensor = aten::dropout(%new_features.169, %3722, %3723) # torch/nn/functional.py:973:17
      -> (%3727)
    block1():
      -> (%new_features.169)
  %3728 : Tensor[] = aten::append(%features.4, %new_features.170) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3729 : Tensor = prim::Uninitialized()
  %3730 : bool = prim::GetAttr[name="memory_efficient"](%1895)
  %3731 : bool = prim::If(%3730) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3732 : bool = prim::Uninitialized()
      %3733 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3734 : bool = aten::gt(%3733, %24)
      %3735 : bool, %3736 : bool, %3737 : int = prim::Loop(%18, %3734, %19, %3732, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3738 : int, %3739 : bool, %3740 : bool, %3741 : int):
          %tensor.86 : Tensor = aten::__getitem__(%features.4, %3741) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3743 : bool = prim::requires_grad(%tensor.86)
          %3744 : bool, %3745 : bool = prim::If(%3743) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3732)
          %3746 : int = aten::add(%3741, %27)
          %3747 : bool = aten::lt(%3746, %3733)
          %3748 : bool = aten::__and__(%3747, %3744)
          -> (%3748, %3743, %3745, %3746)
      %3749 : bool = prim::If(%3735)
        block0():
          -> (%3736)
        block1():
          -> (%19)
      -> (%3749)
    block1():
      -> (%19)
  %bottleneck_output.170 : Tensor = prim::If(%3731) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3729)
    block1():
      %concated_features.86 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3752 : __torch__.torch.nn.modules.conv.___torch_mangle_143.Conv2d = prim::GetAttr[name="conv1"](%1895)
      %3753 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%1895)
      %3754 : int = aten::dim(%concated_features.86) # torch/nn/modules/batchnorm.py:276:11
      %3755 : bool = aten::ne(%3754, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3755) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3756 : bool = prim::GetAttr[name="training"](%3753)
       = prim::If(%3756) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3757 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3753)
          %3758 : Tensor = aten::add(%3757, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3753, %3758)
          -> ()
        block1():
          -> ()
      %3759 : bool = prim::GetAttr[name="training"](%3753)
      %3760 : Tensor = prim::GetAttr[name="running_mean"](%3753)
      %3761 : Tensor = prim::GetAttr[name="running_var"](%3753)
      %3762 : Tensor = prim::GetAttr[name="weight"](%3753)
      %3763 : Tensor = prim::GetAttr[name="bias"](%3753)
       = prim::If(%3759) # torch/nn/functional.py:2011:4
        block0():
          %3764 : int[] = aten::size(%concated_features.86) # torch/nn/functional.py:2012:27
          %size_prods.696 : int = aten::__getitem__(%3764, %24) # torch/nn/functional.py:1991:17
          %3766 : int = aten::len(%3764) # torch/nn/functional.py:1992:19
          %3767 : int = aten::sub(%3766, %26) # torch/nn/functional.py:1992:19
          %size_prods.697 : int = prim::Loop(%3767, %25, %size_prods.696) # torch/nn/functional.py:1992:4
            block0(%i.175 : int, %size_prods.698 : int):
              %3771 : int = aten::add(%i.175, %26) # torch/nn/functional.py:1993:27
              %3772 : int = aten::__getitem__(%3764, %3771) # torch/nn/functional.py:1993:22
              %size_prods.699 : int = aten::mul(%size_prods.698, %3772) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.699)
          %3774 : bool = aten::eq(%size_prods.697, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3774) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3775 : Tensor = aten::batch_norm(%concated_features.86, %3762, %3763, %3760, %3761, %3759, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.171 : Tensor = aten::relu_(%3775) # torch/nn/functional.py:1117:17
      %3777 : Tensor = prim::GetAttr[name="weight"](%3752)
      %3778 : Tensor? = prim::GetAttr[name="bias"](%3752)
      %3779 : int[] = prim::ListConstruct(%27, %27)
      %3780 : int[] = prim::ListConstruct(%24, %24)
      %3781 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.171 : Tensor = aten::conv2d(%result.171, %3777, %3778, %3779, %3780, %3781, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.171)
  %3783 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1895)
  %3784 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1895)
  %3785 : int = aten::dim(%bottleneck_output.170) # torch/nn/modules/batchnorm.py:276:11
  %3786 : bool = aten::ne(%3785, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3786) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3787 : bool = prim::GetAttr[name="training"](%3784)
   = prim::If(%3787) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3788 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3784)
      %3789 : Tensor = aten::add(%3788, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3784, %3789)
      -> ()
    block1():
      -> ()
  %3790 : bool = prim::GetAttr[name="training"](%3784)
  %3791 : Tensor = prim::GetAttr[name="running_mean"](%3784)
  %3792 : Tensor = prim::GetAttr[name="running_var"](%3784)
  %3793 : Tensor = prim::GetAttr[name="weight"](%3784)
  %3794 : Tensor = prim::GetAttr[name="bias"](%3784)
   = prim::If(%3790) # torch/nn/functional.py:2011:4
    block0():
      %3795 : int[] = aten::size(%bottleneck_output.170) # torch/nn/functional.py:2012:27
      %size_prods.700 : int = aten::__getitem__(%3795, %24) # torch/nn/functional.py:1991:17
      %3797 : int = aten::len(%3795) # torch/nn/functional.py:1992:19
      %3798 : int = aten::sub(%3797, %26) # torch/nn/functional.py:1992:19
      %size_prods.701 : int = prim::Loop(%3798, %25, %size_prods.700) # torch/nn/functional.py:1992:4
        block0(%i.176 : int, %size_prods.702 : int):
          %3802 : int = aten::add(%i.176, %26) # torch/nn/functional.py:1993:27
          %3803 : int = aten::__getitem__(%3795, %3802) # torch/nn/functional.py:1993:22
          %size_prods.703 : int = aten::mul(%size_prods.702, %3803) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.703)
      %3805 : bool = aten::eq(%size_prods.701, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3805) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3806 : Tensor = aten::batch_norm(%bottleneck_output.170, %3793, %3794, %3791, %3792, %3790, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.172 : Tensor = aten::relu_(%3806) # torch/nn/functional.py:1117:17
  %3808 : Tensor = prim::GetAttr[name="weight"](%3783)
  %3809 : Tensor? = prim::GetAttr[name="bias"](%3783)
  %3810 : int[] = prim::ListConstruct(%27, %27)
  %3811 : int[] = prim::ListConstruct(%27, %27)
  %3812 : int[] = prim::ListConstruct(%27, %27)
  %new_features.171 : Tensor = aten::conv2d(%result.172, %3808, %3809, %3810, %3811, %3812, %27) # torch/nn/modules/conv.py:415:15
  %3814 : float = prim::GetAttr[name="drop_rate"](%1895)
  %3815 : bool = aten::gt(%3814, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.172 : Tensor = prim::If(%3815) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3817 : float = prim::GetAttr[name="drop_rate"](%1895)
      %3818 : bool = prim::GetAttr[name="training"](%1895)
      %3819 : bool = aten::lt(%3817, %16) # torch/nn/functional.py:968:7
      %3820 : bool = prim::If(%3819) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3821 : bool = aten::gt(%3817, %17) # torch/nn/functional.py:968:17
          -> (%3821)
       = prim::If(%3820) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3822 : Tensor = aten::dropout(%new_features.171, %3817, %3818) # torch/nn/functional.py:973:17
      -> (%3822)
    block1():
      -> (%new_features.171)
  %3823 : Tensor[] = aten::append(%features.4, %new_features.172) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3824 : Tensor = prim::Uninitialized()
  %3825 : bool = prim::GetAttr[name="memory_efficient"](%1896)
  %3826 : bool = prim::If(%3825) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3827 : bool = prim::Uninitialized()
      %3828 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3829 : bool = aten::gt(%3828, %24)
      %3830 : bool, %3831 : bool, %3832 : int = prim::Loop(%18, %3829, %19, %3827, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3833 : int, %3834 : bool, %3835 : bool, %3836 : int):
          %tensor.87 : Tensor = aten::__getitem__(%features.4, %3836) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3838 : bool = prim::requires_grad(%tensor.87)
          %3839 : bool, %3840 : bool = prim::If(%3838) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3827)
          %3841 : int = aten::add(%3836, %27)
          %3842 : bool = aten::lt(%3841, %3828)
          %3843 : bool = aten::__and__(%3842, %3839)
          -> (%3843, %3838, %3840, %3841)
      %3844 : bool = prim::If(%3830)
        block0():
          -> (%3831)
        block1():
          -> (%19)
      -> (%3844)
    block1():
      -> (%19)
  %bottleneck_output.172 : Tensor = prim::If(%3826) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3824)
    block1():
      %concated_features.87 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3847 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%1896)
      %3848 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%1896)
      %3849 : int = aten::dim(%concated_features.87) # torch/nn/modules/batchnorm.py:276:11
      %3850 : bool = aten::ne(%3849, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3850) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3851 : bool = prim::GetAttr[name="training"](%3848)
       = prim::If(%3851) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3852 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3848)
          %3853 : Tensor = aten::add(%3852, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3848, %3853)
          -> ()
        block1():
          -> ()
      %3854 : bool = prim::GetAttr[name="training"](%3848)
      %3855 : Tensor = prim::GetAttr[name="running_mean"](%3848)
      %3856 : Tensor = prim::GetAttr[name="running_var"](%3848)
      %3857 : Tensor = prim::GetAttr[name="weight"](%3848)
      %3858 : Tensor = prim::GetAttr[name="bias"](%3848)
       = prim::If(%3854) # torch/nn/functional.py:2011:4
        block0():
          %3859 : int[] = aten::size(%concated_features.87) # torch/nn/functional.py:2012:27
          %size_prods.704 : int = aten::__getitem__(%3859, %24) # torch/nn/functional.py:1991:17
          %3861 : int = aten::len(%3859) # torch/nn/functional.py:1992:19
          %3862 : int = aten::sub(%3861, %26) # torch/nn/functional.py:1992:19
          %size_prods.705 : int = prim::Loop(%3862, %25, %size_prods.704) # torch/nn/functional.py:1992:4
            block0(%i.177 : int, %size_prods.706 : int):
              %3866 : int = aten::add(%i.177, %26) # torch/nn/functional.py:1993:27
              %3867 : int = aten::__getitem__(%3859, %3866) # torch/nn/functional.py:1993:22
              %size_prods.707 : int = aten::mul(%size_prods.706, %3867) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.707)
          %3869 : bool = aten::eq(%size_prods.705, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3869) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3870 : Tensor = aten::batch_norm(%concated_features.87, %3857, %3858, %3855, %3856, %3854, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.173 : Tensor = aten::relu_(%3870) # torch/nn/functional.py:1117:17
      %3872 : Tensor = prim::GetAttr[name="weight"](%3847)
      %3873 : Tensor? = prim::GetAttr[name="bias"](%3847)
      %3874 : int[] = prim::ListConstruct(%27, %27)
      %3875 : int[] = prim::ListConstruct(%24, %24)
      %3876 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.173 : Tensor = aten::conv2d(%result.173, %3872, %3873, %3874, %3875, %3876, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.173)
  %3878 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1896)
  %3879 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1896)
  %3880 : int = aten::dim(%bottleneck_output.172) # torch/nn/modules/batchnorm.py:276:11
  %3881 : bool = aten::ne(%3880, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3881) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3882 : bool = prim::GetAttr[name="training"](%3879)
   = prim::If(%3882) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3883 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3879)
      %3884 : Tensor = aten::add(%3883, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3879, %3884)
      -> ()
    block1():
      -> ()
  %3885 : bool = prim::GetAttr[name="training"](%3879)
  %3886 : Tensor = prim::GetAttr[name="running_mean"](%3879)
  %3887 : Tensor = prim::GetAttr[name="running_var"](%3879)
  %3888 : Tensor = prim::GetAttr[name="weight"](%3879)
  %3889 : Tensor = prim::GetAttr[name="bias"](%3879)
   = prim::If(%3885) # torch/nn/functional.py:2011:4
    block0():
      %3890 : int[] = aten::size(%bottleneck_output.172) # torch/nn/functional.py:2012:27
      %size_prods.708 : int = aten::__getitem__(%3890, %24) # torch/nn/functional.py:1991:17
      %3892 : int = aten::len(%3890) # torch/nn/functional.py:1992:19
      %3893 : int = aten::sub(%3892, %26) # torch/nn/functional.py:1992:19
      %size_prods.709 : int = prim::Loop(%3893, %25, %size_prods.708) # torch/nn/functional.py:1992:4
        block0(%i.178 : int, %size_prods.710 : int):
          %3897 : int = aten::add(%i.178, %26) # torch/nn/functional.py:1993:27
          %3898 : int = aten::__getitem__(%3890, %3897) # torch/nn/functional.py:1993:22
          %size_prods.711 : int = aten::mul(%size_prods.710, %3898) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.711)
      %3900 : bool = aten::eq(%size_prods.709, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3900) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3901 : Tensor = aten::batch_norm(%bottleneck_output.172, %3888, %3889, %3886, %3887, %3885, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.174 : Tensor = aten::relu_(%3901) # torch/nn/functional.py:1117:17
  %3903 : Tensor = prim::GetAttr[name="weight"](%3878)
  %3904 : Tensor? = prim::GetAttr[name="bias"](%3878)
  %3905 : int[] = prim::ListConstruct(%27, %27)
  %3906 : int[] = prim::ListConstruct(%27, %27)
  %3907 : int[] = prim::ListConstruct(%27, %27)
  %new_features.173 : Tensor = aten::conv2d(%result.174, %3903, %3904, %3905, %3906, %3907, %27) # torch/nn/modules/conv.py:415:15
  %3909 : float = prim::GetAttr[name="drop_rate"](%1896)
  %3910 : bool = aten::gt(%3909, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.174 : Tensor = prim::If(%3910) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3912 : float = prim::GetAttr[name="drop_rate"](%1896)
      %3913 : bool = prim::GetAttr[name="training"](%1896)
      %3914 : bool = aten::lt(%3912, %16) # torch/nn/functional.py:968:7
      %3915 : bool = prim::If(%3914) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3916 : bool = aten::gt(%3912, %17) # torch/nn/functional.py:968:17
          -> (%3916)
       = prim::If(%3915) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3917 : Tensor = aten::dropout(%new_features.173, %3912, %3913) # torch/nn/functional.py:973:17
      -> (%3917)
    block1():
      -> (%new_features.173)
  %3918 : Tensor[] = aten::append(%features.4, %new_features.174) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3919 : Tensor = prim::Uninitialized()
  %3920 : bool = prim::GetAttr[name="memory_efficient"](%1897)
  %3921 : bool = prim::If(%3920) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3922 : bool = prim::Uninitialized()
      %3923 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3924 : bool = aten::gt(%3923, %24)
      %3925 : bool, %3926 : bool, %3927 : int = prim::Loop(%18, %3924, %19, %3922, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3928 : int, %3929 : bool, %3930 : bool, %3931 : int):
          %tensor.88 : Tensor = aten::__getitem__(%features.4, %3931) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3933 : bool = prim::requires_grad(%tensor.88)
          %3934 : bool, %3935 : bool = prim::If(%3933) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3922)
          %3936 : int = aten::add(%3931, %27)
          %3937 : bool = aten::lt(%3936, %3923)
          %3938 : bool = aten::__and__(%3937, %3934)
          -> (%3938, %3933, %3935, %3936)
      %3939 : bool = prim::If(%3925)
        block0():
          -> (%3926)
        block1():
          -> (%19)
      -> (%3939)
    block1():
      -> (%19)
  %bottleneck_output.174 : Tensor = prim::If(%3921) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3919)
    block1():
      %concated_features.88 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3942 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%1897)
      %3943 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%1897)
      %3944 : int = aten::dim(%concated_features.88) # torch/nn/modules/batchnorm.py:276:11
      %3945 : bool = aten::ne(%3944, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3945) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3946 : bool = prim::GetAttr[name="training"](%3943)
       = prim::If(%3946) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3947 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3943)
          %3948 : Tensor = aten::add(%3947, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3943, %3948)
          -> ()
        block1():
          -> ()
      %3949 : bool = prim::GetAttr[name="training"](%3943)
      %3950 : Tensor = prim::GetAttr[name="running_mean"](%3943)
      %3951 : Tensor = prim::GetAttr[name="running_var"](%3943)
      %3952 : Tensor = prim::GetAttr[name="weight"](%3943)
      %3953 : Tensor = prim::GetAttr[name="bias"](%3943)
       = prim::If(%3949) # torch/nn/functional.py:2011:4
        block0():
          %3954 : int[] = aten::size(%concated_features.88) # torch/nn/functional.py:2012:27
          %size_prods.712 : int = aten::__getitem__(%3954, %24) # torch/nn/functional.py:1991:17
          %3956 : int = aten::len(%3954) # torch/nn/functional.py:1992:19
          %3957 : int = aten::sub(%3956, %26) # torch/nn/functional.py:1992:19
          %size_prods.713 : int = prim::Loop(%3957, %25, %size_prods.712) # torch/nn/functional.py:1992:4
            block0(%i.179 : int, %size_prods.714 : int):
              %3961 : int = aten::add(%i.179, %26) # torch/nn/functional.py:1993:27
              %3962 : int = aten::__getitem__(%3954, %3961) # torch/nn/functional.py:1993:22
              %size_prods.715 : int = aten::mul(%size_prods.714, %3962) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.715)
          %3964 : bool = aten::eq(%size_prods.713, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3964) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3965 : Tensor = aten::batch_norm(%concated_features.88, %3952, %3953, %3950, %3951, %3949, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.175 : Tensor = aten::relu_(%3965) # torch/nn/functional.py:1117:17
      %3967 : Tensor = prim::GetAttr[name="weight"](%3942)
      %3968 : Tensor? = prim::GetAttr[name="bias"](%3942)
      %3969 : int[] = prim::ListConstruct(%27, %27)
      %3970 : int[] = prim::ListConstruct(%24, %24)
      %3971 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.175 : Tensor = aten::conv2d(%result.175, %3967, %3968, %3969, %3970, %3971, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.175)
  %3973 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1897)
  %3974 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1897)
  %3975 : int = aten::dim(%bottleneck_output.174) # torch/nn/modules/batchnorm.py:276:11
  %3976 : bool = aten::ne(%3975, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3976) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3977 : bool = prim::GetAttr[name="training"](%3974)
   = prim::If(%3977) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3978 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3974)
      %3979 : Tensor = aten::add(%3978, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3974, %3979)
      -> ()
    block1():
      -> ()
  %3980 : bool = prim::GetAttr[name="training"](%3974)
  %3981 : Tensor = prim::GetAttr[name="running_mean"](%3974)
  %3982 : Tensor = prim::GetAttr[name="running_var"](%3974)
  %3983 : Tensor = prim::GetAttr[name="weight"](%3974)
  %3984 : Tensor = prim::GetAttr[name="bias"](%3974)
   = prim::If(%3980) # torch/nn/functional.py:2011:4
    block0():
      %3985 : int[] = aten::size(%bottleneck_output.174) # torch/nn/functional.py:2012:27
      %size_prods.716 : int = aten::__getitem__(%3985, %24) # torch/nn/functional.py:1991:17
      %3987 : int = aten::len(%3985) # torch/nn/functional.py:1992:19
      %3988 : int = aten::sub(%3987, %26) # torch/nn/functional.py:1992:19
      %size_prods.717 : int = prim::Loop(%3988, %25, %size_prods.716) # torch/nn/functional.py:1992:4
        block0(%i.180 : int, %size_prods.718 : int):
          %3992 : int = aten::add(%i.180, %26) # torch/nn/functional.py:1993:27
          %3993 : int = aten::__getitem__(%3985, %3992) # torch/nn/functional.py:1993:22
          %size_prods.719 : int = aten::mul(%size_prods.718, %3993) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.719)
      %3995 : bool = aten::eq(%size_prods.717, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3995) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3996 : Tensor = aten::batch_norm(%bottleneck_output.174, %3983, %3984, %3981, %3982, %3980, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.176 : Tensor = aten::relu_(%3996) # torch/nn/functional.py:1117:17
  %3998 : Tensor = prim::GetAttr[name="weight"](%3973)
  %3999 : Tensor? = prim::GetAttr[name="bias"](%3973)
  %4000 : int[] = prim::ListConstruct(%27, %27)
  %4001 : int[] = prim::ListConstruct(%27, %27)
  %4002 : int[] = prim::ListConstruct(%27, %27)
  %new_features.175 : Tensor = aten::conv2d(%result.176, %3998, %3999, %4000, %4001, %4002, %27) # torch/nn/modules/conv.py:415:15
  %4004 : float = prim::GetAttr[name="drop_rate"](%1897)
  %4005 : bool = aten::gt(%4004, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.176 : Tensor = prim::If(%4005) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4007 : float = prim::GetAttr[name="drop_rate"](%1897)
      %4008 : bool = prim::GetAttr[name="training"](%1897)
      %4009 : bool = aten::lt(%4007, %16) # torch/nn/functional.py:968:7
      %4010 : bool = prim::If(%4009) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4011 : bool = aten::gt(%4007, %17) # torch/nn/functional.py:968:17
          -> (%4011)
       = prim::If(%4010) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4012 : Tensor = aten::dropout(%new_features.175, %4007, %4008) # torch/nn/functional.py:973:17
      -> (%4012)
    block1():
      -> (%new_features.175)
  %4013 : Tensor[] = aten::append(%features.4, %new_features.176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4014 : Tensor = prim::Uninitialized()
  %4015 : bool = prim::GetAttr[name="memory_efficient"](%1898)
  %4016 : bool = prim::If(%4015) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4017 : bool = prim::Uninitialized()
      %4018 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4019 : bool = aten::gt(%4018, %24)
      %4020 : bool, %4021 : bool, %4022 : int = prim::Loop(%18, %4019, %19, %4017, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4023 : int, %4024 : bool, %4025 : bool, %4026 : int):
          %tensor.89 : Tensor = aten::__getitem__(%features.4, %4026) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4028 : bool = prim::requires_grad(%tensor.89)
          %4029 : bool, %4030 : bool = prim::If(%4028) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4017)
          %4031 : int = aten::add(%4026, %27)
          %4032 : bool = aten::lt(%4031, %4018)
          %4033 : bool = aten::__and__(%4032, %4029)
          -> (%4033, %4028, %4030, %4031)
      %4034 : bool = prim::If(%4020)
        block0():
          -> (%4021)
        block1():
          -> (%19)
      -> (%4034)
    block1():
      -> (%19)
  %bottleneck_output.176 : Tensor = prim::If(%4016) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4014)
    block1():
      %concated_features.89 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4037 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%1898)
      %4038 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%1898)
      %4039 : int = aten::dim(%concated_features.89) # torch/nn/modules/batchnorm.py:276:11
      %4040 : bool = aten::ne(%4039, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4040) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4041 : bool = prim::GetAttr[name="training"](%4038)
       = prim::If(%4041) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4042 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4038)
          %4043 : Tensor = aten::add(%4042, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4038, %4043)
          -> ()
        block1():
          -> ()
      %4044 : bool = prim::GetAttr[name="training"](%4038)
      %4045 : Tensor = prim::GetAttr[name="running_mean"](%4038)
      %4046 : Tensor = prim::GetAttr[name="running_var"](%4038)
      %4047 : Tensor = prim::GetAttr[name="weight"](%4038)
      %4048 : Tensor = prim::GetAttr[name="bias"](%4038)
       = prim::If(%4044) # torch/nn/functional.py:2011:4
        block0():
          %4049 : int[] = aten::size(%concated_features.89) # torch/nn/functional.py:2012:27
          %size_prods.720 : int = aten::__getitem__(%4049, %24) # torch/nn/functional.py:1991:17
          %4051 : int = aten::len(%4049) # torch/nn/functional.py:1992:19
          %4052 : int = aten::sub(%4051, %26) # torch/nn/functional.py:1992:19
          %size_prods.721 : int = prim::Loop(%4052, %25, %size_prods.720) # torch/nn/functional.py:1992:4
            block0(%i.181 : int, %size_prods.722 : int):
              %4056 : int = aten::add(%i.181, %26) # torch/nn/functional.py:1993:27
              %4057 : int = aten::__getitem__(%4049, %4056) # torch/nn/functional.py:1993:22
              %size_prods.723 : int = aten::mul(%size_prods.722, %4057) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.723)
          %4059 : bool = aten::eq(%size_prods.721, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4059) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4060 : Tensor = aten::batch_norm(%concated_features.89, %4047, %4048, %4045, %4046, %4044, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.177 : Tensor = aten::relu_(%4060) # torch/nn/functional.py:1117:17
      %4062 : Tensor = prim::GetAttr[name="weight"](%4037)
      %4063 : Tensor? = prim::GetAttr[name="bias"](%4037)
      %4064 : int[] = prim::ListConstruct(%27, %27)
      %4065 : int[] = prim::ListConstruct(%24, %24)
      %4066 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.177 : Tensor = aten::conv2d(%result.177, %4062, %4063, %4064, %4065, %4066, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.177)
  %4068 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1898)
  %4069 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1898)
  %4070 : int = aten::dim(%bottleneck_output.176) # torch/nn/modules/batchnorm.py:276:11
  %4071 : bool = aten::ne(%4070, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4071) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4072 : bool = prim::GetAttr[name="training"](%4069)
   = prim::If(%4072) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4073 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4069)
      %4074 : Tensor = aten::add(%4073, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4069, %4074)
      -> ()
    block1():
      -> ()
  %4075 : bool = prim::GetAttr[name="training"](%4069)
  %4076 : Tensor = prim::GetAttr[name="running_mean"](%4069)
  %4077 : Tensor = prim::GetAttr[name="running_var"](%4069)
  %4078 : Tensor = prim::GetAttr[name="weight"](%4069)
  %4079 : Tensor = prim::GetAttr[name="bias"](%4069)
   = prim::If(%4075) # torch/nn/functional.py:2011:4
    block0():
      %4080 : int[] = aten::size(%bottleneck_output.176) # torch/nn/functional.py:2012:27
      %size_prods.724 : int = aten::__getitem__(%4080, %24) # torch/nn/functional.py:1991:17
      %4082 : int = aten::len(%4080) # torch/nn/functional.py:1992:19
      %4083 : int = aten::sub(%4082, %26) # torch/nn/functional.py:1992:19
      %size_prods.725 : int = prim::Loop(%4083, %25, %size_prods.724) # torch/nn/functional.py:1992:4
        block0(%i.182 : int, %size_prods.726 : int):
          %4087 : int = aten::add(%i.182, %26) # torch/nn/functional.py:1993:27
          %4088 : int = aten::__getitem__(%4080, %4087) # torch/nn/functional.py:1993:22
          %size_prods.727 : int = aten::mul(%size_prods.726, %4088) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.727)
      %4090 : bool = aten::eq(%size_prods.725, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4090) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4091 : Tensor = aten::batch_norm(%bottleneck_output.176, %4078, %4079, %4076, %4077, %4075, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.178 : Tensor = aten::relu_(%4091) # torch/nn/functional.py:1117:17
  %4093 : Tensor = prim::GetAttr[name="weight"](%4068)
  %4094 : Tensor? = prim::GetAttr[name="bias"](%4068)
  %4095 : int[] = prim::ListConstruct(%27, %27)
  %4096 : int[] = prim::ListConstruct(%27, %27)
  %4097 : int[] = prim::ListConstruct(%27, %27)
  %new_features.177 : Tensor = aten::conv2d(%result.178, %4093, %4094, %4095, %4096, %4097, %27) # torch/nn/modules/conv.py:415:15
  %4099 : float = prim::GetAttr[name="drop_rate"](%1898)
  %4100 : bool = aten::gt(%4099, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.178 : Tensor = prim::If(%4100) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4102 : float = prim::GetAttr[name="drop_rate"](%1898)
      %4103 : bool = prim::GetAttr[name="training"](%1898)
      %4104 : bool = aten::lt(%4102, %16) # torch/nn/functional.py:968:7
      %4105 : bool = prim::If(%4104) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4106 : bool = aten::gt(%4102, %17) # torch/nn/functional.py:968:17
          -> (%4106)
       = prim::If(%4105) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4107 : Tensor = aten::dropout(%new_features.177, %4102, %4103) # torch/nn/functional.py:973:17
      -> (%4107)
    block1():
      -> (%new_features.177)
  %4108 : Tensor[] = aten::append(%features.4, %new_features.178) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4109 : Tensor = prim::Uninitialized()
  %4110 : bool = prim::GetAttr[name="memory_efficient"](%1899)
  %4111 : bool = prim::If(%4110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4112 : bool = prim::Uninitialized()
      %4113 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4114 : bool = aten::gt(%4113, %24)
      %4115 : bool, %4116 : bool, %4117 : int = prim::Loop(%18, %4114, %19, %4112, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4118 : int, %4119 : bool, %4120 : bool, %4121 : int):
          %tensor.90 : Tensor = aten::__getitem__(%features.4, %4121) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4123 : bool = prim::requires_grad(%tensor.90)
          %4124 : bool, %4125 : bool = prim::If(%4123) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4112)
          %4126 : int = aten::add(%4121, %27)
          %4127 : bool = aten::lt(%4126, %4113)
          %4128 : bool = aten::__and__(%4127, %4124)
          -> (%4128, %4123, %4125, %4126)
      %4129 : bool = prim::If(%4115)
        block0():
          -> (%4116)
        block1():
          -> (%19)
      -> (%4129)
    block1():
      -> (%19)
  %bottleneck_output.178 : Tensor = prim::If(%4111) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4109)
    block1():
      %concated_features.90 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4132 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%1899)
      %4133 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%1899)
      %4134 : int = aten::dim(%concated_features.90) # torch/nn/modules/batchnorm.py:276:11
      %4135 : bool = aten::ne(%4134, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4135) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4136 : bool = prim::GetAttr[name="training"](%4133)
       = prim::If(%4136) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4137 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4133)
          %4138 : Tensor = aten::add(%4137, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4133, %4138)
          -> ()
        block1():
          -> ()
      %4139 : bool = prim::GetAttr[name="training"](%4133)
      %4140 : Tensor = prim::GetAttr[name="running_mean"](%4133)
      %4141 : Tensor = prim::GetAttr[name="running_var"](%4133)
      %4142 : Tensor = prim::GetAttr[name="weight"](%4133)
      %4143 : Tensor = prim::GetAttr[name="bias"](%4133)
       = prim::If(%4139) # torch/nn/functional.py:2011:4
        block0():
          %4144 : int[] = aten::size(%concated_features.90) # torch/nn/functional.py:2012:27
          %size_prods.728 : int = aten::__getitem__(%4144, %24) # torch/nn/functional.py:1991:17
          %4146 : int = aten::len(%4144) # torch/nn/functional.py:1992:19
          %4147 : int = aten::sub(%4146, %26) # torch/nn/functional.py:1992:19
          %size_prods.729 : int = prim::Loop(%4147, %25, %size_prods.728) # torch/nn/functional.py:1992:4
            block0(%i.183 : int, %size_prods.730 : int):
              %4151 : int = aten::add(%i.183, %26) # torch/nn/functional.py:1993:27
              %4152 : int = aten::__getitem__(%4144, %4151) # torch/nn/functional.py:1993:22
              %size_prods.731 : int = aten::mul(%size_prods.730, %4152) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.731)
          %4154 : bool = aten::eq(%size_prods.729, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4154) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4155 : Tensor = aten::batch_norm(%concated_features.90, %4142, %4143, %4140, %4141, %4139, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.179 : Tensor = aten::relu_(%4155) # torch/nn/functional.py:1117:17
      %4157 : Tensor = prim::GetAttr[name="weight"](%4132)
      %4158 : Tensor? = prim::GetAttr[name="bias"](%4132)
      %4159 : int[] = prim::ListConstruct(%27, %27)
      %4160 : int[] = prim::ListConstruct(%24, %24)
      %4161 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.179 : Tensor = aten::conv2d(%result.179, %4157, %4158, %4159, %4160, %4161, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.179)
  %4163 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1899)
  %4164 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1899)
  %4165 : int = aten::dim(%bottleneck_output.178) # torch/nn/modules/batchnorm.py:276:11
  %4166 : bool = aten::ne(%4165, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4166) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4167 : bool = prim::GetAttr[name="training"](%4164)
   = prim::If(%4167) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4168 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4164)
      %4169 : Tensor = aten::add(%4168, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4164, %4169)
      -> ()
    block1():
      -> ()
  %4170 : bool = prim::GetAttr[name="training"](%4164)
  %4171 : Tensor = prim::GetAttr[name="running_mean"](%4164)
  %4172 : Tensor = prim::GetAttr[name="running_var"](%4164)
  %4173 : Tensor = prim::GetAttr[name="weight"](%4164)
  %4174 : Tensor = prim::GetAttr[name="bias"](%4164)
   = prim::If(%4170) # torch/nn/functional.py:2011:4
    block0():
      %4175 : int[] = aten::size(%bottleneck_output.178) # torch/nn/functional.py:2012:27
      %size_prods.732 : int = aten::__getitem__(%4175, %24) # torch/nn/functional.py:1991:17
      %4177 : int = aten::len(%4175) # torch/nn/functional.py:1992:19
      %4178 : int = aten::sub(%4177, %26) # torch/nn/functional.py:1992:19
      %size_prods.733 : int = prim::Loop(%4178, %25, %size_prods.732) # torch/nn/functional.py:1992:4
        block0(%i.184 : int, %size_prods.734 : int):
          %4182 : int = aten::add(%i.184, %26) # torch/nn/functional.py:1993:27
          %4183 : int = aten::__getitem__(%4175, %4182) # torch/nn/functional.py:1993:22
          %size_prods.735 : int = aten::mul(%size_prods.734, %4183) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.735)
      %4185 : bool = aten::eq(%size_prods.733, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4185) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4186 : Tensor = aten::batch_norm(%bottleneck_output.178, %4173, %4174, %4171, %4172, %4170, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.180 : Tensor = aten::relu_(%4186) # torch/nn/functional.py:1117:17
  %4188 : Tensor = prim::GetAttr[name="weight"](%4163)
  %4189 : Tensor? = prim::GetAttr[name="bias"](%4163)
  %4190 : int[] = prim::ListConstruct(%27, %27)
  %4191 : int[] = prim::ListConstruct(%27, %27)
  %4192 : int[] = prim::ListConstruct(%27, %27)
  %new_features.179 : Tensor = aten::conv2d(%result.180, %4188, %4189, %4190, %4191, %4192, %27) # torch/nn/modules/conv.py:415:15
  %4194 : float = prim::GetAttr[name="drop_rate"](%1899)
  %4195 : bool = aten::gt(%4194, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.180 : Tensor = prim::If(%4195) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4197 : float = prim::GetAttr[name="drop_rate"](%1899)
      %4198 : bool = prim::GetAttr[name="training"](%1899)
      %4199 : bool = aten::lt(%4197, %16) # torch/nn/functional.py:968:7
      %4200 : bool = prim::If(%4199) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4201 : bool = aten::gt(%4197, %17) # torch/nn/functional.py:968:17
          -> (%4201)
       = prim::If(%4200) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4202 : Tensor = aten::dropout(%new_features.179, %4197, %4198) # torch/nn/functional.py:973:17
      -> (%4202)
    block1():
      -> (%new_features.179)
  %4203 : Tensor[] = aten::append(%features.4, %new_features.180) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4204 : Tensor = prim::Uninitialized()
  %4205 : bool = prim::GetAttr[name="memory_efficient"](%1900)
  %4206 : bool = prim::If(%4205) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4207 : bool = prim::Uninitialized()
      %4208 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4209 : bool = aten::gt(%4208, %24)
      %4210 : bool, %4211 : bool, %4212 : int = prim::Loop(%18, %4209, %19, %4207, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4213 : int, %4214 : bool, %4215 : bool, %4216 : int):
          %tensor.91 : Tensor = aten::__getitem__(%features.4, %4216) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4218 : bool = prim::requires_grad(%tensor.91)
          %4219 : bool, %4220 : bool = prim::If(%4218) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4207)
          %4221 : int = aten::add(%4216, %27)
          %4222 : bool = aten::lt(%4221, %4208)
          %4223 : bool = aten::__and__(%4222, %4219)
          -> (%4223, %4218, %4220, %4221)
      %4224 : bool = prim::If(%4210)
        block0():
          -> (%4211)
        block1():
          -> (%19)
      -> (%4224)
    block1():
      -> (%19)
  %bottleneck_output.180 : Tensor = prim::If(%4206) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4204)
    block1():
      %concated_features.91 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4227 : __torch__.torch.nn.modules.conv.___torch_mangle_299.Conv2d = prim::GetAttr[name="conv1"](%1900)
      %4228 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm1"](%1900)
      %4229 : int = aten::dim(%concated_features.91) # torch/nn/modules/batchnorm.py:276:11
      %4230 : bool = aten::ne(%4229, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4230) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4231 : bool = prim::GetAttr[name="training"](%4228)
       = prim::If(%4231) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4232 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4228)
          %4233 : Tensor = aten::add(%4232, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4228, %4233)
          -> ()
        block1():
          -> ()
      %4234 : bool = prim::GetAttr[name="training"](%4228)
      %4235 : Tensor = prim::GetAttr[name="running_mean"](%4228)
      %4236 : Tensor = prim::GetAttr[name="running_var"](%4228)
      %4237 : Tensor = prim::GetAttr[name="weight"](%4228)
      %4238 : Tensor = prim::GetAttr[name="bias"](%4228)
       = prim::If(%4234) # torch/nn/functional.py:2011:4
        block0():
          %4239 : int[] = aten::size(%concated_features.91) # torch/nn/functional.py:2012:27
          %size_prods.736 : int = aten::__getitem__(%4239, %24) # torch/nn/functional.py:1991:17
          %4241 : int = aten::len(%4239) # torch/nn/functional.py:1992:19
          %4242 : int = aten::sub(%4241, %26) # torch/nn/functional.py:1992:19
          %size_prods.737 : int = prim::Loop(%4242, %25, %size_prods.736) # torch/nn/functional.py:1992:4
            block0(%i.185 : int, %size_prods.738 : int):
              %4246 : int = aten::add(%i.185, %26) # torch/nn/functional.py:1993:27
              %4247 : int = aten::__getitem__(%4239, %4246) # torch/nn/functional.py:1993:22
              %size_prods.739 : int = aten::mul(%size_prods.738, %4247) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.739)
          %4249 : bool = aten::eq(%size_prods.737, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4249) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4250 : Tensor = aten::batch_norm(%concated_features.91, %4237, %4238, %4235, %4236, %4234, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.181 : Tensor = aten::relu_(%4250) # torch/nn/functional.py:1117:17
      %4252 : Tensor = prim::GetAttr[name="weight"](%4227)
      %4253 : Tensor? = prim::GetAttr[name="bias"](%4227)
      %4254 : int[] = prim::ListConstruct(%27, %27)
      %4255 : int[] = prim::ListConstruct(%24, %24)
      %4256 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.181 : Tensor = aten::conv2d(%result.181, %4252, %4253, %4254, %4255, %4256, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.181)
  %4258 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1900)
  %4259 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1900)
  %4260 : int = aten::dim(%bottleneck_output.180) # torch/nn/modules/batchnorm.py:276:11
  %4261 : bool = aten::ne(%4260, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4261) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4262 : bool = prim::GetAttr[name="training"](%4259)
   = prim::If(%4262) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4263 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4259)
      %4264 : Tensor = aten::add(%4263, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4259, %4264)
      -> ()
    block1():
      -> ()
  %4265 : bool = prim::GetAttr[name="training"](%4259)
  %4266 : Tensor = prim::GetAttr[name="running_mean"](%4259)
  %4267 : Tensor = prim::GetAttr[name="running_var"](%4259)
  %4268 : Tensor = prim::GetAttr[name="weight"](%4259)
  %4269 : Tensor = prim::GetAttr[name="bias"](%4259)
   = prim::If(%4265) # torch/nn/functional.py:2011:4
    block0():
      %4270 : int[] = aten::size(%bottleneck_output.180) # torch/nn/functional.py:2012:27
      %size_prods.740 : int = aten::__getitem__(%4270, %24) # torch/nn/functional.py:1991:17
      %4272 : int = aten::len(%4270) # torch/nn/functional.py:1992:19
      %4273 : int = aten::sub(%4272, %26) # torch/nn/functional.py:1992:19
      %size_prods.741 : int = prim::Loop(%4273, %25, %size_prods.740) # torch/nn/functional.py:1992:4
        block0(%i.186 : int, %size_prods.742 : int):
          %4277 : int = aten::add(%i.186, %26) # torch/nn/functional.py:1993:27
          %4278 : int = aten::__getitem__(%4270, %4277) # torch/nn/functional.py:1993:22
          %size_prods.743 : int = aten::mul(%size_prods.742, %4278) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.743)
      %4280 : bool = aten::eq(%size_prods.741, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4280) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4281 : Tensor = aten::batch_norm(%bottleneck_output.180, %4268, %4269, %4266, %4267, %4265, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.182 : Tensor = aten::relu_(%4281) # torch/nn/functional.py:1117:17
  %4283 : Tensor = prim::GetAttr[name="weight"](%4258)
  %4284 : Tensor? = prim::GetAttr[name="bias"](%4258)
  %4285 : int[] = prim::ListConstruct(%27, %27)
  %4286 : int[] = prim::ListConstruct(%27, %27)
  %4287 : int[] = prim::ListConstruct(%27, %27)
  %new_features.181 : Tensor = aten::conv2d(%result.182, %4283, %4284, %4285, %4286, %4287, %27) # torch/nn/modules/conv.py:415:15
  %4289 : float = prim::GetAttr[name="drop_rate"](%1900)
  %4290 : bool = aten::gt(%4289, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.182 : Tensor = prim::If(%4290) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4292 : float = prim::GetAttr[name="drop_rate"](%1900)
      %4293 : bool = prim::GetAttr[name="training"](%1900)
      %4294 : bool = aten::lt(%4292, %16) # torch/nn/functional.py:968:7
      %4295 : bool = prim::If(%4294) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4296 : bool = aten::gt(%4292, %17) # torch/nn/functional.py:968:17
          -> (%4296)
       = prim::If(%4295) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4297 : Tensor = aten::dropout(%new_features.181, %4292, %4293) # torch/nn/functional.py:973:17
      -> (%4297)
    block1():
      -> (%new_features.181)
  %4298 : Tensor[] = aten::append(%features.4, %new_features.182) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4299 : Tensor = prim::Uninitialized()
  %4300 : bool = prim::GetAttr[name="memory_efficient"](%1901)
  %4301 : bool = prim::If(%4300) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4302 : bool = prim::Uninitialized()
      %4303 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4304 : bool = aten::gt(%4303, %24)
      %4305 : bool, %4306 : bool, %4307 : int = prim::Loop(%18, %4304, %19, %4302, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4308 : int, %4309 : bool, %4310 : bool, %4311 : int):
          %tensor.92 : Tensor = aten::__getitem__(%features.4, %4311) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4313 : bool = prim::requires_grad(%tensor.92)
          %4314 : bool, %4315 : bool = prim::If(%4313) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4302)
          %4316 : int = aten::add(%4311, %27)
          %4317 : bool = aten::lt(%4316, %4303)
          %4318 : bool = aten::__and__(%4317, %4314)
          -> (%4318, %4313, %4315, %4316)
      %4319 : bool = prim::If(%4305)
        block0():
          -> (%4306)
        block1():
          -> (%19)
      -> (%4319)
    block1():
      -> (%19)
  %bottleneck_output.182 : Tensor = prim::If(%4301) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4299)
    block1():
      %concated_features.92 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4322 : __torch__.torch.nn.modules.conv.___torch_mangle_301.Conv2d = prim::GetAttr[name="conv1"](%1901)
      %4323 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%1901)
      %4324 : int = aten::dim(%concated_features.92) # torch/nn/modules/batchnorm.py:276:11
      %4325 : bool = aten::ne(%4324, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4325) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4326 : bool = prim::GetAttr[name="training"](%4323)
       = prim::If(%4326) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4327 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4323)
          %4328 : Tensor = aten::add(%4327, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4323, %4328)
          -> ()
        block1():
          -> ()
      %4329 : bool = prim::GetAttr[name="training"](%4323)
      %4330 : Tensor = prim::GetAttr[name="running_mean"](%4323)
      %4331 : Tensor = prim::GetAttr[name="running_var"](%4323)
      %4332 : Tensor = prim::GetAttr[name="weight"](%4323)
      %4333 : Tensor = prim::GetAttr[name="bias"](%4323)
       = prim::If(%4329) # torch/nn/functional.py:2011:4
        block0():
          %4334 : int[] = aten::size(%concated_features.92) # torch/nn/functional.py:2012:27
          %size_prods.744 : int = aten::__getitem__(%4334, %24) # torch/nn/functional.py:1991:17
          %4336 : int = aten::len(%4334) # torch/nn/functional.py:1992:19
          %4337 : int = aten::sub(%4336, %26) # torch/nn/functional.py:1992:19
          %size_prods.745 : int = prim::Loop(%4337, %25, %size_prods.744) # torch/nn/functional.py:1992:4
            block0(%i.187 : int, %size_prods.746 : int):
              %4341 : int = aten::add(%i.187, %26) # torch/nn/functional.py:1993:27
              %4342 : int = aten::__getitem__(%4334, %4341) # torch/nn/functional.py:1993:22
              %size_prods.747 : int = aten::mul(%size_prods.746, %4342) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.747)
          %4344 : bool = aten::eq(%size_prods.745, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4344) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4345 : Tensor = aten::batch_norm(%concated_features.92, %4332, %4333, %4330, %4331, %4329, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.183 : Tensor = aten::relu_(%4345) # torch/nn/functional.py:1117:17
      %4347 : Tensor = prim::GetAttr[name="weight"](%4322)
      %4348 : Tensor? = prim::GetAttr[name="bias"](%4322)
      %4349 : int[] = prim::ListConstruct(%27, %27)
      %4350 : int[] = prim::ListConstruct(%24, %24)
      %4351 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.183 : Tensor = aten::conv2d(%result.183, %4347, %4348, %4349, %4350, %4351, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.183)
  %4353 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1901)
  %4354 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1901)
  %4355 : int = aten::dim(%bottleneck_output.182) # torch/nn/modules/batchnorm.py:276:11
  %4356 : bool = aten::ne(%4355, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4356) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4357 : bool = prim::GetAttr[name="training"](%4354)
   = prim::If(%4357) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4358 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4354)
      %4359 : Tensor = aten::add(%4358, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4354, %4359)
      -> ()
    block1():
      -> ()
  %4360 : bool = prim::GetAttr[name="training"](%4354)
  %4361 : Tensor = prim::GetAttr[name="running_mean"](%4354)
  %4362 : Tensor = prim::GetAttr[name="running_var"](%4354)
  %4363 : Tensor = prim::GetAttr[name="weight"](%4354)
  %4364 : Tensor = prim::GetAttr[name="bias"](%4354)
   = prim::If(%4360) # torch/nn/functional.py:2011:4
    block0():
      %4365 : int[] = aten::size(%bottleneck_output.182) # torch/nn/functional.py:2012:27
      %size_prods.748 : int = aten::__getitem__(%4365, %24) # torch/nn/functional.py:1991:17
      %4367 : int = aten::len(%4365) # torch/nn/functional.py:1992:19
      %4368 : int = aten::sub(%4367, %26) # torch/nn/functional.py:1992:19
      %size_prods.749 : int = prim::Loop(%4368, %25, %size_prods.748) # torch/nn/functional.py:1992:4
        block0(%i.188 : int, %size_prods.750 : int):
          %4372 : int = aten::add(%i.188, %26) # torch/nn/functional.py:1993:27
          %4373 : int = aten::__getitem__(%4365, %4372) # torch/nn/functional.py:1993:22
          %size_prods.751 : int = aten::mul(%size_prods.750, %4373) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.751)
      %4375 : bool = aten::eq(%size_prods.749, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4375) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4376 : Tensor = aten::batch_norm(%bottleneck_output.182, %4363, %4364, %4361, %4362, %4360, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.184 : Tensor = aten::relu_(%4376) # torch/nn/functional.py:1117:17
  %4378 : Tensor = prim::GetAttr[name="weight"](%4353)
  %4379 : Tensor? = prim::GetAttr[name="bias"](%4353)
  %4380 : int[] = prim::ListConstruct(%27, %27)
  %4381 : int[] = prim::ListConstruct(%27, %27)
  %4382 : int[] = prim::ListConstruct(%27, %27)
  %new_features.183 : Tensor = aten::conv2d(%result.184, %4378, %4379, %4380, %4381, %4382, %27) # torch/nn/modules/conv.py:415:15
  %4384 : float = prim::GetAttr[name="drop_rate"](%1901)
  %4385 : bool = aten::gt(%4384, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.184 : Tensor = prim::If(%4385) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4387 : float = prim::GetAttr[name="drop_rate"](%1901)
      %4388 : bool = prim::GetAttr[name="training"](%1901)
      %4389 : bool = aten::lt(%4387, %16) # torch/nn/functional.py:968:7
      %4390 : bool = prim::If(%4389) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4391 : bool = aten::gt(%4387, %17) # torch/nn/functional.py:968:17
          -> (%4391)
       = prim::If(%4390) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4392 : Tensor = aten::dropout(%new_features.183, %4387, %4388) # torch/nn/functional.py:973:17
      -> (%4392)
    block1():
      -> (%new_features.183)
  %4393 : Tensor[] = aten::append(%features.4, %new_features.184) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4394 : Tensor = prim::Uninitialized()
  %4395 : bool = prim::GetAttr[name="memory_efficient"](%1902)
  %4396 : bool = prim::If(%4395) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4397 : bool = prim::Uninitialized()
      %4398 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4399 : bool = aten::gt(%4398, %24)
      %4400 : bool, %4401 : bool, %4402 : int = prim::Loop(%18, %4399, %19, %4397, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4403 : int, %4404 : bool, %4405 : bool, %4406 : int):
          %tensor.93 : Tensor = aten::__getitem__(%features.4, %4406) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4408 : bool = prim::requires_grad(%tensor.93)
          %4409 : bool, %4410 : bool = prim::If(%4408) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4397)
          %4411 : int = aten::add(%4406, %27)
          %4412 : bool = aten::lt(%4411, %4398)
          %4413 : bool = aten::__and__(%4412, %4409)
          -> (%4413, %4408, %4410, %4411)
      %4414 : bool = prim::If(%4400)
        block0():
          -> (%4401)
        block1():
          -> (%19)
      -> (%4414)
    block1():
      -> (%19)
  %bottleneck_output.184 : Tensor = prim::If(%4396) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4394)
    block1():
      %concated_features.93 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4417 : __torch__.torch.nn.modules.conv.___torch_mangle_304.Conv2d = prim::GetAttr[name="conv1"](%1902)
      %4418 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_303.BatchNorm2d = prim::GetAttr[name="norm1"](%1902)
      %4419 : int = aten::dim(%concated_features.93) # torch/nn/modules/batchnorm.py:276:11
      %4420 : bool = aten::ne(%4419, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4420) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4421 : bool = prim::GetAttr[name="training"](%4418)
       = prim::If(%4421) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4422 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4418)
          %4423 : Tensor = aten::add(%4422, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4418, %4423)
          -> ()
        block1():
          -> ()
      %4424 : bool = prim::GetAttr[name="training"](%4418)
      %4425 : Tensor = prim::GetAttr[name="running_mean"](%4418)
      %4426 : Tensor = prim::GetAttr[name="running_var"](%4418)
      %4427 : Tensor = prim::GetAttr[name="weight"](%4418)
      %4428 : Tensor = prim::GetAttr[name="bias"](%4418)
       = prim::If(%4424) # torch/nn/functional.py:2011:4
        block0():
          %4429 : int[] = aten::size(%concated_features.93) # torch/nn/functional.py:2012:27
          %size_prods.752 : int = aten::__getitem__(%4429, %24) # torch/nn/functional.py:1991:17
          %4431 : int = aten::len(%4429) # torch/nn/functional.py:1992:19
          %4432 : int = aten::sub(%4431, %26) # torch/nn/functional.py:1992:19
          %size_prods.753 : int = prim::Loop(%4432, %25, %size_prods.752) # torch/nn/functional.py:1992:4
            block0(%i.189 : int, %size_prods.754 : int):
              %4436 : int = aten::add(%i.189, %26) # torch/nn/functional.py:1993:27
              %4437 : int = aten::__getitem__(%4429, %4436) # torch/nn/functional.py:1993:22
              %size_prods.755 : int = aten::mul(%size_prods.754, %4437) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.755)
          %4439 : bool = aten::eq(%size_prods.753, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4439) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4440 : Tensor = aten::batch_norm(%concated_features.93, %4427, %4428, %4425, %4426, %4424, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.185 : Tensor = aten::relu_(%4440) # torch/nn/functional.py:1117:17
      %4442 : Tensor = prim::GetAttr[name="weight"](%4417)
      %4443 : Tensor? = prim::GetAttr[name="bias"](%4417)
      %4444 : int[] = prim::ListConstruct(%27, %27)
      %4445 : int[] = prim::ListConstruct(%24, %24)
      %4446 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.185 : Tensor = aten::conv2d(%result.185, %4442, %4443, %4444, %4445, %4446, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.185)
  %4448 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1902)
  %4449 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1902)
  %4450 : int = aten::dim(%bottleneck_output.184) # torch/nn/modules/batchnorm.py:276:11
  %4451 : bool = aten::ne(%4450, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4451) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4452 : bool = prim::GetAttr[name="training"](%4449)
   = prim::If(%4452) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4453 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4449)
      %4454 : Tensor = aten::add(%4453, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4449, %4454)
      -> ()
    block1():
      -> ()
  %4455 : bool = prim::GetAttr[name="training"](%4449)
  %4456 : Tensor = prim::GetAttr[name="running_mean"](%4449)
  %4457 : Tensor = prim::GetAttr[name="running_var"](%4449)
  %4458 : Tensor = prim::GetAttr[name="weight"](%4449)
  %4459 : Tensor = prim::GetAttr[name="bias"](%4449)
   = prim::If(%4455) # torch/nn/functional.py:2011:4
    block0():
      %4460 : int[] = aten::size(%bottleneck_output.184) # torch/nn/functional.py:2012:27
      %size_prods.756 : int = aten::__getitem__(%4460, %24) # torch/nn/functional.py:1991:17
      %4462 : int = aten::len(%4460) # torch/nn/functional.py:1992:19
      %4463 : int = aten::sub(%4462, %26) # torch/nn/functional.py:1992:19
      %size_prods.757 : int = prim::Loop(%4463, %25, %size_prods.756) # torch/nn/functional.py:1992:4
        block0(%i.190 : int, %size_prods.758 : int):
          %4467 : int = aten::add(%i.190, %26) # torch/nn/functional.py:1993:27
          %4468 : int = aten::__getitem__(%4460, %4467) # torch/nn/functional.py:1993:22
          %size_prods.759 : int = aten::mul(%size_prods.758, %4468) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.759)
      %4470 : bool = aten::eq(%size_prods.757, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4470) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4471 : Tensor = aten::batch_norm(%bottleneck_output.184, %4458, %4459, %4456, %4457, %4455, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.186 : Tensor = aten::relu_(%4471) # torch/nn/functional.py:1117:17
  %4473 : Tensor = prim::GetAttr[name="weight"](%4448)
  %4474 : Tensor? = prim::GetAttr[name="bias"](%4448)
  %4475 : int[] = prim::ListConstruct(%27, %27)
  %4476 : int[] = prim::ListConstruct(%27, %27)
  %4477 : int[] = prim::ListConstruct(%27, %27)
  %new_features.185 : Tensor = aten::conv2d(%result.186, %4473, %4474, %4475, %4476, %4477, %27) # torch/nn/modules/conv.py:415:15
  %4479 : float = prim::GetAttr[name="drop_rate"](%1902)
  %4480 : bool = aten::gt(%4479, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.186 : Tensor = prim::If(%4480) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4482 : float = prim::GetAttr[name="drop_rate"](%1902)
      %4483 : bool = prim::GetAttr[name="training"](%1902)
      %4484 : bool = aten::lt(%4482, %16) # torch/nn/functional.py:968:7
      %4485 : bool = prim::If(%4484) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4486 : bool = aten::gt(%4482, %17) # torch/nn/functional.py:968:17
          -> (%4486)
       = prim::If(%4485) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4487 : Tensor = aten::dropout(%new_features.185, %4482, %4483) # torch/nn/functional.py:973:17
      -> (%4487)
    block1():
      -> (%new_features.185)
  %4488 : Tensor[] = aten::append(%features.4, %new_features.186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4489 : Tensor = prim::Uninitialized()
  %4490 : bool = prim::GetAttr[name="memory_efficient"](%1903)
  %4491 : bool = prim::If(%4490) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4492 : bool = prim::Uninitialized()
      %4493 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4494 : bool = aten::gt(%4493, %24)
      %4495 : bool, %4496 : bool, %4497 : int = prim::Loop(%18, %4494, %19, %4492, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4498 : int, %4499 : bool, %4500 : bool, %4501 : int):
          %tensor.94 : Tensor = aten::__getitem__(%features.4, %4501) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4503 : bool = prim::requires_grad(%tensor.94)
          %4504 : bool, %4505 : bool = prim::If(%4503) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4492)
          %4506 : int = aten::add(%4501, %27)
          %4507 : bool = aten::lt(%4506, %4493)
          %4508 : bool = aten::__and__(%4507, %4504)
          -> (%4508, %4503, %4505, %4506)
      %4509 : bool = prim::If(%4495)
        block0():
          -> (%4496)
        block1():
          -> (%19)
      -> (%4509)
    block1():
      -> (%19)
  %bottleneck_output.186 : Tensor = prim::If(%4491) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4489)
    block1():
      %concated_features.94 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4512 : __torch__.torch.nn.modules.conv.___torch_mangle_307.Conv2d = prim::GetAttr[name="conv1"](%1903)
      %4513 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="norm1"](%1903)
      %4514 : int = aten::dim(%concated_features.94) # torch/nn/modules/batchnorm.py:276:11
      %4515 : bool = aten::ne(%4514, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4515) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4516 : bool = prim::GetAttr[name="training"](%4513)
       = prim::If(%4516) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4517 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4513)
          %4518 : Tensor = aten::add(%4517, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4513, %4518)
          -> ()
        block1():
          -> ()
      %4519 : bool = prim::GetAttr[name="training"](%4513)
      %4520 : Tensor = prim::GetAttr[name="running_mean"](%4513)
      %4521 : Tensor = prim::GetAttr[name="running_var"](%4513)
      %4522 : Tensor = prim::GetAttr[name="weight"](%4513)
      %4523 : Tensor = prim::GetAttr[name="bias"](%4513)
       = prim::If(%4519) # torch/nn/functional.py:2011:4
        block0():
          %4524 : int[] = aten::size(%concated_features.94) # torch/nn/functional.py:2012:27
          %size_prods.760 : int = aten::__getitem__(%4524, %24) # torch/nn/functional.py:1991:17
          %4526 : int = aten::len(%4524) # torch/nn/functional.py:1992:19
          %4527 : int = aten::sub(%4526, %26) # torch/nn/functional.py:1992:19
          %size_prods.761 : int = prim::Loop(%4527, %25, %size_prods.760) # torch/nn/functional.py:1992:4
            block0(%i.191 : int, %size_prods.762 : int):
              %4531 : int = aten::add(%i.191, %26) # torch/nn/functional.py:1993:27
              %4532 : int = aten::__getitem__(%4524, %4531) # torch/nn/functional.py:1993:22
              %size_prods.763 : int = aten::mul(%size_prods.762, %4532) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.763)
          %4534 : bool = aten::eq(%size_prods.761, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4534) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4535 : Tensor = aten::batch_norm(%concated_features.94, %4522, %4523, %4520, %4521, %4519, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.187 : Tensor = aten::relu_(%4535) # torch/nn/functional.py:1117:17
      %4537 : Tensor = prim::GetAttr[name="weight"](%4512)
      %4538 : Tensor? = prim::GetAttr[name="bias"](%4512)
      %4539 : int[] = prim::ListConstruct(%27, %27)
      %4540 : int[] = prim::ListConstruct(%24, %24)
      %4541 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.187 : Tensor = aten::conv2d(%result.187, %4537, %4538, %4539, %4540, %4541, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.187)
  %4543 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1903)
  %4544 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1903)
  %4545 : int = aten::dim(%bottleneck_output.186) # torch/nn/modules/batchnorm.py:276:11
  %4546 : bool = aten::ne(%4545, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4546) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4547 : bool = prim::GetAttr[name="training"](%4544)
   = prim::If(%4547) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4548 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4544)
      %4549 : Tensor = aten::add(%4548, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4544, %4549)
      -> ()
    block1():
      -> ()
  %4550 : bool = prim::GetAttr[name="training"](%4544)
  %4551 : Tensor = prim::GetAttr[name="running_mean"](%4544)
  %4552 : Tensor = prim::GetAttr[name="running_var"](%4544)
  %4553 : Tensor = prim::GetAttr[name="weight"](%4544)
  %4554 : Tensor = prim::GetAttr[name="bias"](%4544)
   = prim::If(%4550) # torch/nn/functional.py:2011:4
    block0():
      %4555 : int[] = aten::size(%bottleneck_output.186) # torch/nn/functional.py:2012:27
      %size_prods.764 : int = aten::__getitem__(%4555, %24) # torch/nn/functional.py:1991:17
      %4557 : int = aten::len(%4555) # torch/nn/functional.py:1992:19
      %4558 : int = aten::sub(%4557, %26) # torch/nn/functional.py:1992:19
      %size_prods.765 : int = prim::Loop(%4558, %25, %size_prods.764) # torch/nn/functional.py:1992:4
        block0(%i.192 : int, %size_prods.766 : int):
          %4562 : int = aten::add(%i.192, %26) # torch/nn/functional.py:1993:27
          %4563 : int = aten::__getitem__(%4555, %4562) # torch/nn/functional.py:1993:22
          %size_prods.767 : int = aten::mul(%size_prods.766, %4563) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.767)
      %4565 : bool = aten::eq(%size_prods.765, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4565) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4566 : Tensor = aten::batch_norm(%bottleneck_output.186, %4553, %4554, %4551, %4552, %4550, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.188 : Tensor = aten::relu_(%4566) # torch/nn/functional.py:1117:17
  %4568 : Tensor = prim::GetAttr[name="weight"](%4543)
  %4569 : Tensor? = prim::GetAttr[name="bias"](%4543)
  %4570 : int[] = prim::ListConstruct(%27, %27)
  %4571 : int[] = prim::ListConstruct(%27, %27)
  %4572 : int[] = prim::ListConstruct(%27, %27)
  %new_features.187 : Tensor = aten::conv2d(%result.188, %4568, %4569, %4570, %4571, %4572, %27) # torch/nn/modules/conv.py:415:15
  %4574 : float = prim::GetAttr[name="drop_rate"](%1903)
  %4575 : bool = aten::gt(%4574, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.188 : Tensor = prim::If(%4575) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4577 : float = prim::GetAttr[name="drop_rate"](%1903)
      %4578 : bool = prim::GetAttr[name="training"](%1903)
      %4579 : bool = aten::lt(%4577, %16) # torch/nn/functional.py:968:7
      %4580 : bool = prim::If(%4579) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4581 : bool = aten::gt(%4577, %17) # torch/nn/functional.py:968:17
          -> (%4581)
       = prim::If(%4580) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4582 : Tensor = aten::dropout(%new_features.187, %4577, %4578) # torch/nn/functional.py:973:17
      -> (%4582)
    block1():
      -> (%new_features.187)
  %4583 : Tensor[] = aten::append(%features.4, %new_features.188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4584 : Tensor = prim::Uninitialized()
  %4585 : bool = prim::GetAttr[name="memory_efficient"](%1904)
  %4586 : bool = prim::If(%4585) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4587 : bool = prim::Uninitialized()
      %4588 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4589 : bool = aten::gt(%4588, %24)
      %4590 : bool, %4591 : bool, %4592 : int = prim::Loop(%18, %4589, %19, %4587, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4593 : int, %4594 : bool, %4595 : bool, %4596 : int):
          %tensor.95 : Tensor = aten::__getitem__(%features.4, %4596) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4598 : bool = prim::requires_grad(%tensor.95)
          %4599 : bool, %4600 : bool = prim::If(%4598) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4587)
          %4601 : int = aten::add(%4596, %27)
          %4602 : bool = aten::lt(%4601, %4588)
          %4603 : bool = aten::__and__(%4602, %4599)
          -> (%4603, %4598, %4600, %4601)
      %4604 : bool = prim::If(%4590)
        block0():
          -> (%4591)
        block1():
          -> (%19)
      -> (%4604)
    block1():
      -> (%19)
  %bottleneck_output.188 : Tensor = prim::If(%4586) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4584)
    block1():
      %concated_features.95 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4607 : __torch__.torch.nn.modules.conv.___torch_mangle_309.Conv2d = prim::GetAttr[name="conv1"](%1904)
      %4608 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%1904)
      %4609 : int = aten::dim(%concated_features.95) # torch/nn/modules/batchnorm.py:276:11
      %4610 : bool = aten::ne(%4609, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4610) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4611 : bool = prim::GetAttr[name="training"](%4608)
       = prim::If(%4611) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4612 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4608)
          %4613 : Tensor = aten::add(%4612, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4608, %4613)
          -> ()
        block1():
          -> ()
      %4614 : bool = prim::GetAttr[name="training"](%4608)
      %4615 : Tensor = prim::GetAttr[name="running_mean"](%4608)
      %4616 : Tensor = prim::GetAttr[name="running_var"](%4608)
      %4617 : Tensor = prim::GetAttr[name="weight"](%4608)
      %4618 : Tensor = prim::GetAttr[name="bias"](%4608)
       = prim::If(%4614) # torch/nn/functional.py:2011:4
        block0():
          %4619 : int[] = aten::size(%concated_features.95) # torch/nn/functional.py:2012:27
          %size_prods.768 : int = aten::__getitem__(%4619, %24) # torch/nn/functional.py:1991:17
          %4621 : int = aten::len(%4619) # torch/nn/functional.py:1992:19
          %4622 : int = aten::sub(%4621, %26) # torch/nn/functional.py:1992:19
          %size_prods.769 : int = prim::Loop(%4622, %25, %size_prods.768) # torch/nn/functional.py:1992:4
            block0(%i.193 : int, %size_prods.770 : int):
              %4626 : int = aten::add(%i.193, %26) # torch/nn/functional.py:1993:27
              %4627 : int = aten::__getitem__(%4619, %4626) # torch/nn/functional.py:1993:22
              %size_prods.771 : int = aten::mul(%size_prods.770, %4627) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.771)
          %4629 : bool = aten::eq(%size_prods.769, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4629) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4630 : Tensor = aten::batch_norm(%concated_features.95, %4617, %4618, %4615, %4616, %4614, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.189 : Tensor = aten::relu_(%4630) # torch/nn/functional.py:1117:17
      %4632 : Tensor = prim::GetAttr[name="weight"](%4607)
      %4633 : Tensor? = prim::GetAttr[name="bias"](%4607)
      %4634 : int[] = prim::ListConstruct(%27, %27)
      %4635 : int[] = prim::ListConstruct(%24, %24)
      %4636 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.189 : Tensor = aten::conv2d(%result.189, %4632, %4633, %4634, %4635, %4636, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.189)
  %4638 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1904)
  %4639 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1904)
  %4640 : int = aten::dim(%bottleneck_output.188) # torch/nn/modules/batchnorm.py:276:11
  %4641 : bool = aten::ne(%4640, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4641) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4642 : bool = prim::GetAttr[name="training"](%4639)
   = prim::If(%4642) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4643 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4639)
      %4644 : Tensor = aten::add(%4643, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4639, %4644)
      -> ()
    block1():
      -> ()
  %4645 : bool = prim::GetAttr[name="training"](%4639)
  %4646 : Tensor = prim::GetAttr[name="running_mean"](%4639)
  %4647 : Tensor = prim::GetAttr[name="running_var"](%4639)
  %4648 : Tensor = prim::GetAttr[name="weight"](%4639)
  %4649 : Tensor = prim::GetAttr[name="bias"](%4639)
   = prim::If(%4645) # torch/nn/functional.py:2011:4
    block0():
      %4650 : int[] = aten::size(%bottleneck_output.188) # torch/nn/functional.py:2012:27
      %size_prods.772 : int = aten::__getitem__(%4650, %24) # torch/nn/functional.py:1991:17
      %4652 : int = aten::len(%4650) # torch/nn/functional.py:1992:19
      %4653 : int = aten::sub(%4652, %26) # torch/nn/functional.py:1992:19
      %size_prods.773 : int = prim::Loop(%4653, %25, %size_prods.772) # torch/nn/functional.py:1992:4
        block0(%i.194 : int, %size_prods.774 : int):
          %4657 : int = aten::add(%i.194, %26) # torch/nn/functional.py:1993:27
          %4658 : int = aten::__getitem__(%4650, %4657) # torch/nn/functional.py:1993:22
          %size_prods.775 : int = aten::mul(%size_prods.774, %4658) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.775)
      %4660 : bool = aten::eq(%size_prods.773, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4660) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4661 : Tensor = aten::batch_norm(%bottleneck_output.188, %4648, %4649, %4646, %4647, %4645, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.190 : Tensor = aten::relu_(%4661) # torch/nn/functional.py:1117:17
  %4663 : Tensor = prim::GetAttr[name="weight"](%4638)
  %4664 : Tensor? = prim::GetAttr[name="bias"](%4638)
  %4665 : int[] = prim::ListConstruct(%27, %27)
  %4666 : int[] = prim::ListConstruct(%27, %27)
  %4667 : int[] = prim::ListConstruct(%27, %27)
  %new_features.189 : Tensor = aten::conv2d(%result.190, %4663, %4664, %4665, %4666, %4667, %27) # torch/nn/modules/conv.py:415:15
  %4669 : float = prim::GetAttr[name="drop_rate"](%1904)
  %4670 : bool = aten::gt(%4669, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.190 : Tensor = prim::If(%4670) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4672 : float = prim::GetAttr[name="drop_rate"](%1904)
      %4673 : bool = prim::GetAttr[name="training"](%1904)
      %4674 : bool = aten::lt(%4672, %16) # torch/nn/functional.py:968:7
      %4675 : bool = prim::If(%4674) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4676 : bool = aten::gt(%4672, %17) # torch/nn/functional.py:968:17
          -> (%4676)
       = prim::If(%4675) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4677 : Tensor = aten::dropout(%new_features.189, %4672, %4673) # torch/nn/functional.py:973:17
      -> (%4677)
    block1():
      -> (%new_features.189)
  %4678 : Tensor[] = aten::append(%features.4, %new_features.190) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4679 : Tensor = prim::Uninitialized()
  %4680 : bool = prim::GetAttr[name="memory_efficient"](%1905)
  %4681 : bool = prim::If(%4680) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4682 : bool = prim::Uninitialized()
      %4683 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4684 : bool = aten::gt(%4683, %24)
      %4685 : bool, %4686 : bool, %4687 : int = prim::Loop(%18, %4684, %19, %4682, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4688 : int, %4689 : bool, %4690 : bool, %4691 : int):
          %tensor.96 : Tensor = aten::__getitem__(%features.4, %4691) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4693 : bool = prim::requires_grad(%tensor.96)
          %4694 : bool, %4695 : bool = prim::If(%4693) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4682)
          %4696 : int = aten::add(%4691, %27)
          %4697 : bool = aten::lt(%4696, %4683)
          %4698 : bool = aten::__and__(%4697, %4694)
          -> (%4698, %4693, %4695, %4696)
      %4699 : bool = prim::If(%4685)
        block0():
          -> (%4686)
        block1():
          -> (%19)
      -> (%4699)
    block1():
      -> (%19)
  %bottleneck_output.190 : Tensor = prim::If(%4681) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4679)
    block1():
      %concated_features.96 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4702 : __torch__.torch.nn.modules.conv.___torch_mangle_312.Conv2d = prim::GetAttr[name="conv1"](%1905)
      %4703 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_311.BatchNorm2d = prim::GetAttr[name="norm1"](%1905)
      %4704 : int = aten::dim(%concated_features.96) # torch/nn/modules/batchnorm.py:276:11
      %4705 : bool = aten::ne(%4704, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4705) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4706 : bool = prim::GetAttr[name="training"](%4703)
       = prim::If(%4706) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4707 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4703)
          %4708 : Tensor = aten::add(%4707, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4703, %4708)
          -> ()
        block1():
          -> ()
      %4709 : bool = prim::GetAttr[name="training"](%4703)
      %4710 : Tensor = prim::GetAttr[name="running_mean"](%4703)
      %4711 : Tensor = prim::GetAttr[name="running_var"](%4703)
      %4712 : Tensor = prim::GetAttr[name="weight"](%4703)
      %4713 : Tensor = prim::GetAttr[name="bias"](%4703)
       = prim::If(%4709) # torch/nn/functional.py:2011:4
        block0():
          %4714 : int[] = aten::size(%concated_features.96) # torch/nn/functional.py:2012:27
          %size_prods.776 : int = aten::__getitem__(%4714, %24) # torch/nn/functional.py:1991:17
          %4716 : int = aten::len(%4714) # torch/nn/functional.py:1992:19
          %4717 : int = aten::sub(%4716, %26) # torch/nn/functional.py:1992:19
          %size_prods.777 : int = prim::Loop(%4717, %25, %size_prods.776) # torch/nn/functional.py:1992:4
            block0(%i.195 : int, %size_prods.778 : int):
              %4721 : int = aten::add(%i.195, %26) # torch/nn/functional.py:1993:27
              %4722 : int = aten::__getitem__(%4714, %4721) # torch/nn/functional.py:1993:22
              %size_prods.779 : int = aten::mul(%size_prods.778, %4722) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.779)
          %4724 : bool = aten::eq(%size_prods.777, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4724) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4725 : Tensor = aten::batch_norm(%concated_features.96, %4712, %4713, %4710, %4711, %4709, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.191 : Tensor = aten::relu_(%4725) # torch/nn/functional.py:1117:17
      %4727 : Tensor = prim::GetAttr[name="weight"](%4702)
      %4728 : Tensor? = prim::GetAttr[name="bias"](%4702)
      %4729 : int[] = prim::ListConstruct(%27, %27)
      %4730 : int[] = prim::ListConstruct(%24, %24)
      %4731 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.191 : Tensor = aten::conv2d(%result.191, %4727, %4728, %4729, %4730, %4731, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.191)
  %4733 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1905)
  %4734 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1905)
  %4735 : int = aten::dim(%bottleneck_output.190) # torch/nn/modules/batchnorm.py:276:11
  %4736 : bool = aten::ne(%4735, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4736) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4737 : bool = prim::GetAttr[name="training"](%4734)
   = prim::If(%4737) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4738 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4734)
      %4739 : Tensor = aten::add(%4738, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4734, %4739)
      -> ()
    block1():
      -> ()
  %4740 : bool = prim::GetAttr[name="training"](%4734)
  %4741 : Tensor = prim::GetAttr[name="running_mean"](%4734)
  %4742 : Tensor = prim::GetAttr[name="running_var"](%4734)
  %4743 : Tensor = prim::GetAttr[name="weight"](%4734)
  %4744 : Tensor = prim::GetAttr[name="bias"](%4734)
   = prim::If(%4740) # torch/nn/functional.py:2011:4
    block0():
      %4745 : int[] = aten::size(%bottleneck_output.190) # torch/nn/functional.py:2012:27
      %size_prods.780 : int = aten::__getitem__(%4745, %24) # torch/nn/functional.py:1991:17
      %4747 : int = aten::len(%4745) # torch/nn/functional.py:1992:19
      %4748 : int = aten::sub(%4747, %26) # torch/nn/functional.py:1992:19
      %size_prods.781 : int = prim::Loop(%4748, %25, %size_prods.780) # torch/nn/functional.py:1992:4
        block0(%i.196 : int, %size_prods.782 : int):
          %4752 : int = aten::add(%i.196, %26) # torch/nn/functional.py:1993:27
          %4753 : int = aten::__getitem__(%4745, %4752) # torch/nn/functional.py:1993:22
          %size_prods.783 : int = aten::mul(%size_prods.782, %4753) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.783)
      %4755 : bool = aten::eq(%size_prods.781, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4755) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4756 : Tensor = aten::batch_norm(%bottleneck_output.190, %4743, %4744, %4741, %4742, %4740, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.192 : Tensor = aten::relu_(%4756) # torch/nn/functional.py:1117:17
  %4758 : Tensor = prim::GetAttr[name="weight"](%4733)
  %4759 : Tensor? = prim::GetAttr[name="bias"](%4733)
  %4760 : int[] = prim::ListConstruct(%27, %27)
  %4761 : int[] = prim::ListConstruct(%27, %27)
  %4762 : int[] = prim::ListConstruct(%27, %27)
  %new_features.191 : Tensor = aten::conv2d(%result.192, %4758, %4759, %4760, %4761, %4762, %27) # torch/nn/modules/conv.py:415:15
  %4764 : float = prim::GetAttr[name="drop_rate"](%1905)
  %4765 : bool = aten::gt(%4764, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.192 : Tensor = prim::If(%4765) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4767 : float = prim::GetAttr[name="drop_rate"](%1905)
      %4768 : bool = prim::GetAttr[name="training"](%1905)
      %4769 : bool = aten::lt(%4767, %16) # torch/nn/functional.py:968:7
      %4770 : bool = prim::If(%4769) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4771 : bool = aten::gt(%4767, %17) # torch/nn/functional.py:968:17
          -> (%4771)
       = prim::If(%4770) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4772 : Tensor = aten::dropout(%new_features.191, %4767, %4768) # torch/nn/functional.py:973:17
      -> (%4772)
    block1():
      -> (%new_features.191)
  %4773 : Tensor[] = aten::append(%features.4, %new_features.192) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4774 : Tensor = prim::Uninitialized()
  %4775 : bool = prim::GetAttr[name="memory_efficient"](%1906)
  %4776 : bool = prim::If(%4775) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4777 : bool = prim::Uninitialized()
      %4778 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4779 : bool = aten::gt(%4778, %24)
      %4780 : bool, %4781 : bool, %4782 : int = prim::Loop(%18, %4779, %19, %4777, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4783 : int, %4784 : bool, %4785 : bool, %4786 : int):
          %tensor.97 : Tensor = aten::__getitem__(%features.4, %4786) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4788 : bool = prim::requires_grad(%tensor.97)
          %4789 : bool, %4790 : bool = prim::If(%4788) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4777)
          %4791 : int = aten::add(%4786, %27)
          %4792 : bool = aten::lt(%4791, %4778)
          %4793 : bool = aten::__and__(%4792, %4789)
          -> (%4793, %4788, %4790, %4791)
      %4794 : bool = prim::If(%4780)
        block0():
          -> (%4781)
        block1():
          -> (%19)
      -> (%4794)
    block1():
      -> (%19)
  %bottleneck_output.192 : Tensor = prim::If(%4776) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4774)
    block1():
      %concated_features.97 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4797 : __torch__.torch.nn.modules.conv.___torch_mangle_315.Conv2d = prim::GetAttr[name="conv1"](%1906)
      %4798 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_314.BatchNorm2d = prim::GetAttr[name="norm1"](%1906)
      %4799 : int = aten::dim(%concated_features.97) # torch/nn/modules/batchnorm.py:276:11
      %4800 : bool = aten::ne(%4799, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4800) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4801 : bool = prim::GetAttr[name="training"](%4798)
       = prim::If(%4801) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4802 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4798)
          %4803 : Tensor = aten::add(%4802, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4798, %4803)
          -> ()
        block1():
          -> ()
      %4804 : bool = prim::GetAttr[name="training"](%4798)
      %4805 : Tensor = prim::GetAttr[name="running_mean"](%4798)
      %4806 : Tensor = prim::GetAttr[name="running_var"](%4798)
      %4807 : Tensor = prim::GetAttr[name="weight"](%4798)
      %4808 : Tensor = prim::GetAttr[name="bias"](%4798)
       = prim::If(%4804) # torch/nn/functional.py:2011:4
        block0():
          %4809 : int[] = aten::size(%concated_features.97) # torch/nn/functional.py:2012:27
          %size_prods.784 : int = aten::__getitem__(%4809, %24) # torch/nn/functional.py:1991:17
          %4811 : int = aten::len(%4809) # torch/nn/functional.py:1992:19
          %4812 : int = aten::sub(%4811, %26) # torch/nn/functional.py:1992:19
          %size_prods.785 : int = prim::Loop(%4812, %25, %size_prods.784) # torch/nn/functional.py:1992:4
            block0(%i.197 : int, %size_prods.786 : int):
              %4816 : int = aten::add(%i.197, %26) # torch/nn/functional.py:1993:27
              %4817 : int = aten::__getitem__(%4809, %4816) # torch/nn/functional.py:1993:22
              %size_prods.787 : int = aten::mul(%size_prods.786, %4817) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.787)
          %4819 : bool = aten::eq(%size_prods.785, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4819) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4820 : Tensor = aten::batch_norm(%concated_features.97, %4807, %4808, %4805, %4806, %4804, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.193 : Tensor = aten::relu_(%4820) # torch/nn/functional.py:1117:17
      %4822 : Tensor = prim::GetAttr[name="weight"](%4797)
      %4823 : Tensor? = prim::GetAttr[name="bias"](%4797)
      %4824 : int[] = prim::ListConstruct(%27, %27)
      %4825 : int[] = prim::ListConstruct(%24, %24)
      %4826 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.193 : Tensor = aten::conv2d(%result.193, %4822, %4823, %4824, %4825, %4826, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.193)
  %4828 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1906)
  %4829 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1906)
  %4830 : int = aten::dim(%bottleneck_output.192) # torch/nn/modules/batchnorm.py:276:11
  %4831 : bool = aten::ne(%4830, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4831) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4832 : bool = prim::GetAttr[name="training"](%4829)
   = prim::If(%4832) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4833 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4829)
      %4834 : Tensor = aten::add(%4833, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4829, %4834)
      -> ()
    block1():
      -> ()
  %4835 : bool = prim::GetAttr[name="training"](%4829)
  %4836 : Tensor = prim::GetAttr[name="running_mean"](%4829)
  %4837 : Tensor = prim::GetAttr[name="running_var"](%4829)
  %4838 : Tensor = prim::GetAttr[name="weight"](%4829)
  %4839 : Tensor = prim::GetAttr[name="bias"](%4829)
   = prim::If(%4835) # torch/nn/functional.py:2011:4
    block0():
      %4840 : int[] = aten::size(%bottleneck_output.192) # torch/nn/functional.py:2012:27
      %size_prods.788 : int = aten::__getitem__(%4840, %24) # torch/nn/functional.py:1991:17
      %4842 : int = aten::len(%4840) # torch/nn/functional.py:1992:19
      %4843 : int = aten::sub(%4842, %26) # torch/nn/functional.py:1992:19
      %size_prods.789 : int = prim::Loop(%4843, %25, %size_prods.788) # torch/nn/functional.py:1992:4
        block0(%i.198 : int, %size_prods.790 : int):
          %4847 : int = aten::add(%i.198, %26) # torch/nn/functional.py:1993:27
          %4848 : int = aten::__getitem__(%4840, %4847) # torch/nn/functional.py:1993:22
          %size_prods.791 : int = aten::mul(%size_prods.790, %4848) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.791)
      %4850 : bool = aten::eq(%size_prods.789, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4850) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4851 : Tensor = aten::batch_norm(%bottleneck_output.192, %4838, %4839, %4836, %4837, %4835, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.194 : Tensor = aten::relu_(%4851) # torch/nn/functional.py:1117:17
  %4853 : Tensor = prim::GetAttr[name="weight"](%4828)
  %4854 : Tensor? = prim::GetAttr[name="bias"](%4828)
  %4855 : int[] = prim::ListConstruct(%27, %27)
  %4856 : int[] = prim::ListConstruct(%27, %27)
  %4857 : int[] = prim::ListConstruct(%27, %27)
  %new_features.193 : Tensor = aten::conv2d(%result.194, %4853, %4854, %4855, %4856, %4857, %27) # torch/nn/modules/conv.py:415:15
  %4859 : float = prim::GetAttr[name="drop_rate"](%1906)
  %4860 : bool = aten::gt(%4859, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.194 : Tensor = prim::If(%4860) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4862 : float = prim::GetAttr[name="drop_rate"](%1906)
      %4863 : bool = prim::GetAttr[name="training"](%1906)
      %4864 : bool = aten::lt(%4862, %16) # torch/nn/functional.py:968:7
      %4865 : bool = prim::If(%4864) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4866 : bool = aten::gt(%4862, %17) # torch/nn/functional.py:968:17
          -> (%4866)
       = prim::If(%4865) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4867 : Tensor = aten::dropout(%new_features.193, %4862, %4863) # torch/nn/functional.py:973:17
      -> (%4867)
    block1():
      -> (%new_features.193)
  %4868 : Tensor[] = aten::append(%features.4, %new_features.194) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4869 : Tensor = prim::Uninitialized()
  %4870 : bool = prim::GetAttr[name="memory_efficient"](%1907)
  %4871 : bool = prim::If(%4870) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4872 : bool = prim::Uninitialized()
      %4873 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4874 : bool = aten::gt(%4873, %24)
      %4875 : bool, %4876 : bool, %4877 : int = prim::Loop(%18, %4874, %19, %4872, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4878 : int, %4879 : bool, %4880 : bool, %4881 : int):
          %tensor.33 : Tensor = aten::__getitem__(%features.4, %4881) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4883 : bool = prim::requires_grad(%tensor.33)
          %4884 : bool, %4885 : bool = prim::If(%4883) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4872)
          %4886 : int = aten::add(%4881, %27)
          %4887 : bool = aten::lt(%4886, %4873)
          %4888 : bool = aten::__and__(%4887, %4884)
          -> (%4888, %4883, %4885, %4886)
      %4889 : bool = prim::If(%4875)
        block0():
          -> (%4876)
        block1():
          -> (%19)
      -> (%4889)
    block1():
      -> (%19)
  %bottleneck_output.64 : Tensor = prim::If(%4871) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4869)
    block1():
      %concated_features.33 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4892 : __torch__.torch.nn.modules.conv.___torch_mangle_317.Conv2d = prim::GetAttr[name="conv1"](%1907)
      %4893 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%1907)
      %4894 : int = aten::dim(%concated_features.33) # torch/nn/modules/batchnorm.py:276:11
      %4895 : bool = aten::ne(%4894, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4895) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4896 : bool = prim::GetAttr[name="training"](%4893)
       = prim::If(%4896) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4897 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4893)
          %4898 : Tensor = aten::add(%4897, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4893, %4898)
          -> ()
        block1():
          -> ()
      %4899 : bool = prim::GetAttr[name="training"](%4893)
      %4900 : Tensor = prim::GetAttr[name="running_mean"](%4893)
      %4901 : Tensor = prim::GetAttr[name="running_var"](%4893)
      %4902 : Tensor = prim::GetAttr[name="weight"](%4893)
      %4903 : Tensor = prim::GetAttr[name="bias"](%4893)
       = prim::If(%4899) # torch/nn/functional.py:2011:4
        block0():
          %4904 : int[] = aten::size(%concated_features.33) # torch/nn/functional.py:2012:27
          %size_prods.256 : int = aten::__getitem__(%4904, %24) # torch/nn/functional.py:1991:17
          %4906 : int = aten::len(%4904) # torch/nn/functional.py:1992:19
          %4907 : int = aten::sub(%4906, %26) # torch/nn/functional.py:1992:19
          %size_prods.257 : int = prim::Loop(%4907, %25, %size_prods.256) # torch/nn/functional.py:1992:4
            block0(%i.65 : int, %size_prods.258 : int):
              %4911 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
              %4912 : int = aten::__getitem__(%4904, %4911) # torch/nn/functional.py:1993:22
              %size_prods.259 : int = aten::mul(%size_prods.258, %4912) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.259)
          %4914 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4914) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4915 : Tensor = aten::batch_norm(%concated_features.33, %4902, %4903, %4900, %4901, %4899, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.65 : Tensor = aten::relu_(%4915) # torch/nn/functional.py:1117:17
      %4917 : Tensor = prim::GetAttr[name="weight"](%4892)
      %4918 : Tensor? = prim::GetAttr[name="bias"](%4892)
      %4919 : int[] = prim::ListConstruct(%27, %27)
      %4920 : int[] = prim::ListConstruct(%24, %24)
      %4921 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.65 : Tensor = aten::conv2d(%result.65, %4917, %4918, %4919, %4920, %4921, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.65)
  %4923 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1907)
  %4924 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1907)
  %4925 : int = aten::dim(%bottleneck_output.64) # torch/nn/modules/batchnorm.py:276:11
  %4926 : bool = aten::ne(%4925, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4926) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4927 : bool = prim::GetAttr[name="training"](%4924)
   = prim::If(%4927) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4928 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4924)
      %4929 : Tensor = aten::add(%4928, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4924, %4929)
      -> ()
    block1():
      -> ()
  %4930 : bool = prim::GetAttr[name="training"](%4924)
  %4931 : Tensor = prim::GetAttr[name="running_mean"](%4924)
  %4932 : Tensor = prim::GetAttr[name="running_var"](%4924)
  %4933 : Tensor = prim::GetAttr[name="weight"](%4924)
  %4934 : Tensor = prim::GetAttr[name="bias"](%4924)
   = prim::If(%4930) # torch/nn/functional.py:2011:4
    block0():
      %4935 : int[] = aten::size(%bottleneck_output.64) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%4935, %24) # torch/nn/functional.py:1991:17
      %4937 : int = aten::len(%4935) # torch/nn/functional.py:1992:19
      %4938 : int = aten::sub(%4937, %26) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%4938, %25, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %4942 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
          %4943 : int = aten::__getitem__(%4935, %4942) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %4943) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.263)
      %4945 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4945) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4946 : Tensor = aten::batch_norm(%bottleneck_output.64, %4933, %4934, %4931, %4932, %4930, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.66 : Tensor = aten::relu_(%4946) # torch/nn/functional.py:1117:17
  %4948 : Tensor = prim::GetAttr[name="weight"](%4923)
  %4949 : Tensor? = prim::GetAttr[name="bias"](%4923)
  %4950 : int[] = prim::ListConstruct(%27, %27)
  %4951 : int[] = prim::ListConstruct(%27, %27)
  %4952 : int[] = prim::ListConstruct(%27, %27)
  %new_features.66 : Tensor = aten::conv2d(%result.66, %4948, %4949, %4950, %4951, %4952, %27) # torch/nn/modules/conv.py:415:15
  %4954 : float = prim::GetAttr[name="drop_rate"](%1907)
  %4955 : bool = aten::gt(%4954, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.196 : Tensor = prim::If(%4955) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4957 : float = prim::GetAttr[name="drop_rate"](%1907)
      %4958 : bool = prim::GetAttr[name="training"](%1907)
      %4959 : bool = aten::lt(%4957, %16) # torch/nn/functional.py:968:7
      %4960 : bool = prim::If(%4959) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4961 : bool = aten::gt(%4957, %17) # torch/nn/functional.py:968:17
          -> (%4961)
       = prim::If(%4960) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4962 : Tensor = aten::dropout(%new_features.66, %4957, %4958) # torch/nn/functional.py:973:17
      -> (%4962)
    block1():
      -> (%new_features.66)
  %4963 : Tensor[] = aten::append(%features.4, %new_features.196) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4964 : Tensor = prim::Uninitialized()
  %4965 : bool = prim::GetAttr[name="memory_efficient"](%1908)
  %4966 : bool = prim::If(%4965) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4967 : bool = prim::Uninitialized()
      %4968 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4969 : bool = aten::gt(%4968, %24)
      %4970 : bool, %4971 : bool, %4972 : int = prim::Loop(%18, %4969, %19, %4967, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4973 : int, %4974 : bool, %4975 : bool, %4976 : int):
          %tensor.34 : Tensor = aten::__getitem__(%features.4, %4976) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4978 : bool = prim::requires_grad(%tensor.34)
          %4979 : bool, %4980 : bool = prim::If(%4978) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4967)
          %4981 : int = aten::add(%4976, %27)
          %4982 : bool = aten::lt(%4981, %4968)
          %4983 : bool = aten::__and__(%4982, %4979)
          -> (%4983, %4978, %4980, %4981)
      %4984 : bool = prim::If(%4970)
        block0():
          -> (%4971)
        block1():
          -> (%19)
      -> (%4984)
    block1():
      -> (%19)
  %bottleneck_output.66 : Tensor = prim::If(%4966) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4964)
    block1():
      %concated_features.34 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4987 : __torch__.torch.nn.modules.conv.___torch_mangle_323.Conv2d = prim::GetAttr[name="conv1"](%1908)
      %4988 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_320.BatchNorm2d = prim::GetAttr[name="norm1"](%1908)
      %4989 : int = aten::dim(%concated_features.34) # torch/nn/modules/batchnorm.py:276:11
      %4990 : bool = aten::ne(%4989, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4990) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4991 : bool = prim::GetAttr[name="training"](%4988)
       = prim::If(%4991) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4992 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4988)
          %4993 : Tensor = aten::add(%4992, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4988, %4993)
          -> ()
        block1():
          -> ()
      %4994 : bool = prim::GetAttr[name="training"](%4988)
      %4995 : Tensor = prim::GetAttr[name="running_mean"](%4988)
      %4996 : Tensor = prim::GetAttr[name="running_var"](%4988)
      %4997 : Tensor = prim::GetAttr[name="weight"](%4988)
      %4998 : Tensor = prim::GetAttr[name="bias"](%4988)
       = prim::If(%4994) # torch/nn/functional.py:2011:4
        block0():
          %4999 : int[] = aten::size(%concated_features.34) # torch/nn/functional.py:2012:27
          %size_prods.264 : int = aten::__getitem__(%4999, %24) # torch/nn/functional.py:1991:17
          %5001 : int = aten::len(%4999) # torch/nn/functional.py:1992:19
          %5002 : int = aten::sub(%5001, %26) # torch/nn/functional.py:1992:19
          %size_prods.265 : int = prim::Loop(%5002, %25, %size_prods.264) # torch/nn/functional.py:1992:4
            block0(%i.67 : int, %size_prods.266 : int):
              %5006 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
              %5007 : int = aten::__getitem__(%4999, %5006) # torch/nn/functional.py:1993:22
              %size_prods.267 : int = aten::mul(%size_prods.266, %5007) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.267)
          %5009 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5009) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5010 : Tensor = aten::batch_norm(%concated_features.34, %4997, %4998, %4995, %4996, %4994, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.67 : Tensor = aten::relu_(%5010) # torch/nn/functional.py:1117:17
      %5012 : Tensor = prim::GetAttr[name="weight"](%4987)
      %5013 : Tensor? = prim::GetAttr[name="bias"](%4987)
      %5014 : int[] = prim::ListConstruct(%27, %27)
      %5015 : int[] = prim::ListConstruct(%24, %24)
      %5016 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.67 : Tensor = aten::conv2d(%result.67, %5012, %5013, %5014, %5015, %5016, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.67)
  %5018 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1908)
  %5019 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1908)
  %5020 : int = aten::dim(%bottleneck_output.66) # torch/nn/modules/batchnorm.py:276:11
  %5021 : bool = aten::ne(%5020, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5021) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5022 : bool = prim::GetAttr[name="training"](%5019)
   = prim::If(%5022) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5023 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5019)
      %5024 : Tensor = aten::add(%5023, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5019, %5024)
      -> ()
    block1():
      -> ()
  %5025 : bool = prim::GetAttr[name="training"](%5019)
  %5026 : Tensor = prim::GetAttr[name="running_mean"](%5019)
  %5027 : Tensor = prim::GetAttr[name="running_var"](%5019)
  %5028 : Tensor = prim::GetAttr[name="weight"](%5019)
  %5029 : Tensor = prim::GetAttr[name="bias"](%5019)
   = prim::If(%5025) # torch/nn/functional.py:2011:4
    block0():
      %5030 : int[] = aten::size(%bottleneck_output.66) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%5030, %24) # torch/nn/functional.py:1991:17
      %5032 : int = aten::len(%5030) # torch/nn/functional.py:1992:19
      %5033 : int = aten::sub(%5032, %26) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%5033, %25, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %5037 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
          %5038 : int = aten::__getitem__(%5030, %5037) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %5038) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.271)
      %5040 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5040) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5041 : Tensor = aten::batch_norm(%bottleneck_output.66, %5028, %5029, %5026, %5027, %5025, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.68 : Tensor = aten::relu_(%5041) # torch/nn/functional.py:1117:17
  %5043 : Tensor = prim::GetAttr[name="weight"](%5018)
  %5044 : Tensor? = prim::GetAttr[name="bias"](%5018)
  %5045 : int[] = prim::ListConstruct(%27, %27)
  %5046 : int[] = prim::ListConstruct(%27, %27)
  %5047 : int[] = prim::ListConstruct(%27, %27)
  %new_features.68 : Tensor = aten::conv2d(%result.68, %5043, %5044, %5045, %5046, %5047, %27) # torch/nn/modules/conv.py:415:15
  %5049 : float = prim::GetAttr[name="drop_rate"](%1908)
  %5050 : bool = aten::gt(%5049, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.65 : Tensor = prim::If(%5050) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5052 : float = prim::GetAttr[name="drop_rate"](%1908)
      %5053 : bool = prim::GetAttr[name="training"](%1908)
      %5054 : bool = aten::lt(%5052, %16) # torch/nn/functional.py:968:7
      %5055 : bool = prim::If(%5054) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5056 : bool = aten::gt(%5052, %17) # torch/nn/functional.py:968:17
          -> (%5056)
       = prim::If(%5055) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5057 : Tensor = aten::dropout(%new_features.68, %5052, %5053) # torch/nn/functional.py:973:17
      -> (%5057)
    block1():
      -> (%new_features.68)
  %5058 : Tensor[] = aten::append(%features.4, %new_features.65) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5059 : Tensor = prim::Uninitialized()
  %5060 : bool = prim::GetAttr[name="memory_efficient"](%1909)
  %5061 : bool = prim::If(%5060) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5062 : bool = prim::Uninitialized()
      %5063 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5064 : bool = aten::gt(%5063, %24)
      %5065 : bool, %5066 : bool, %5067 : int = prim::Loop(%18, %5064, %19, %5062, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5068 : int, %5069 : bool, %5070 : bool, %5071 : int):
          %tensor.35 : Tensor = aten::__getitem__(%features.4, %5071) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5073 : bool = prim::requires_grad(%tensor.35)
          %5074 : bool, %5075 : bool = prim::If(%5073) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5062)
          %5076 : int = aten::add(%5071, %27)
          %5077 : bool = aten::lt(%5076, %5063)
          %5078 : bool = aten::__and__(%5077, %5074)
          -> (%5078, %5073, %5075, %5076)
      %5079 : bool = prim::If(%5065)
        block0():
          -> (%5066)
        block1():
          -> (%19)
      -> (%5079)
    block1():
      -> (%19)
  %bottleneck_output.68 : Tensor = prim::If(%5061) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5059)
    block1():
      %concated_features.35 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5082 : __torch__.torch.nn.modules.conv.___torch_mangle_326.Conv2d = prim::GetAttr[name="conv1"](%1909)
      %5083 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_325.BatchNorm2d = prim::GetAttr[name="norm1"](%1909)
      %5084 : int = aten::dim(%concated_features.35) # torch/nn/modules/batchnorm.py:276:11
      %5085 : bool = aten::ne(%5084, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5085) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5086 : bool = prim::GetAttr[name="training"](%5083)
       = prim::If(%5086) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5087 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5083)
          %5088 : Tensor = aten::add(%5087, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5083, %5088)
          -> ()
        block1():
          -> ()
      %5089 : bool = prim::GetAttr[name="training"](%5083)
      %5090 : Tensor = prim::GetAttr[name="running_mean"](%5083)
      %5091 : Tensor = prim::GetAttr[name="running_var"](%5083)
      %5092 : Tensor = prim::GetAttr[name="weight"](%5083)
      %5093 : Tensor = prim::GetAttr[name="bias"](%5083)
       = prim::If(%5089) # torch/nn/functional.py:2011:4
        block0():
          %5094 : int[] = aten::size(%concated_features.35) # torch/nn/functional.py:2012:27
          %size_prods.272 : int = aten::__getitem__(%5094, %24) # torch/nn/functional.py:1991:17
          %5096 : int = aten::len(%5094) # torch/nn/functional.py:1992:19
          %5097 : int = aten::sub(%5096, %26) # torch/nn/functional.py:1992:19
          %size_prods.273 : int = prim::Loop(%5097, %25, %size_prods.272) # torch/nn/functional.py:1992:4
            block0(%i.69 : int, %size_prods.274 : int):
              %5101 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
              %5102 : int = aten::__getitem__(%5094, %5101) # torch/nn/functional.py:1993:22
              %size_prods.275 : int = aten::mul(%size_prods.274, %5102) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.275)
          %5104 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5104) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5105 : Tensor = aten::batch_norm(%concated_features.35, %5092, %5093, %5090, %5091, %5089, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.69 : Tensor = aten::relu_(%5105) # torch/nn/functional.py:1117:17
      %5107 : Tensor = prim::GetAttr[name="weight"](%5082)
      %5108 : Tensor? = prim::GetAttr[name="bias"](%5082)
      %5109 : int[] = prim::ListConstruct(%27, %27)
      %5110 : int[] = prim::ListConstruct(%24, %24)
      %5111 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.69 : Tensor = aten::conv2d(%result.69, %5107, %5108, %5109, %5110, %5111, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.69)
  %5113 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1909)
  %5114 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1909)
  %5115 : int = aten::dim(%bottleneck_output.68) # torch/nn/modules/batchnorm.py:276:11
  %5116 : bool = aten::ne(%5115, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5116) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5117 : bool = prim::GetAttr[name="training"](%5114)
   = prim::If(%5117) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5118 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5114)
      %5119 : Tensor = aten::add(%5118, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5114, %5119)
      -> ()
    block1():
      -> ()
  %5120 : bool = prim::GetAttr[name="training"](%5114)
  %5121 : Tensor = prim::GetAttr[name="running_mean"](%5114)
  %5122 : Tensor = prim::GetAttr[name="running_var"](%5114)
  %5123 : Tensor = prim::GetAttr[name="weight"](%5114)
  %5124 : Tensor = prim::GetAttr[name="bias"](%5114)
   = prim::If(%5120) # torch/nn/functional.py:2011:4
    block0():
      %5125 : int[] = aten::size(%bottleneck_output.68) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%5125, %24) # torch/nn/functional.py:1991:17
      %5127 : int = aten::len(%5125) # torch/nn/functional.py:1992:19
      %5128 : int = aten::sub(%5127, %26) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%5128, %25, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %5132 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
          %5133 : int = aten::__getitem__(%5125, %5132) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %5133) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.279)
      %5135 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5135) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5136 : Tensor = aten::batch_norm(%bottleneck_output.68, %5123, %5124, %5121, %5122, %5120, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.70 : Tensor = aten::relu_(%5136) # torch/nn/functional.py:1117:17
  %5138 : Tensor = prim::GetAttr[name="weight"](%5113)
  %5139 : Tensor? = prim::GetAttr[name="bias"](%5113)
  %5140 : int[] = prim::ListConstruct(%27, %27)
  %5141 : int[] = prim::ListConstruct(%27, %27)
  %5142 : int[] = prim::ListConstruct(%27, %27)
  %new_features.70 : Tensor = aten::conv2d(%result.70, %5138, %5139, %5140, %5141, %5142, %27) # torch/nn/modules/conv.py:415:15
  %5144 : float = prim::GetAttr[name="drop_rate"](%1909)
  %5145 : bool = aten::gt(%5144, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.67 : Tensor = prim::If(%5145) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5147 : float = prim::GetAttr[name="drop_rate"](%1909)
      %5148 : bool = prim::GetAttr[name="training"](%1909)
      %5149 : bool = aten::lt(%5147, %16) # torch/nn/functional.py:968:7
      %5150 : bool = prim::If(%5149) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5151 : bool = aten::gt(%5147, %17) # torch/nn/functional.py:968:17
          -> (%5151)
       = prim::If(%5150) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5152 : Tensor = aten::dropout(%new_features.70, %5147, %5148) # torch/nn/functional.py:973:17
      -> (%5152)
    block1():
      -> (%new_features.70)
  %5153 : Tensor[] = aten::append(%features.4, %new_features.67) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5154 : Tensor = prim::Uninitialized()
  %5155 : bool = prim::GetAttr[name="memory_efficient"](%1910)
  %5156 : bool = prim::If(%5155) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5157 : bool = prim::Uninitialized()
      %5158 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5159 : bool = aten::gt(%5158, %24)
      %5160 : bool, %5161 : bool, %5162 : int = prim::Loop(%18, %5159, %19, %5157, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5163 : int, %5164 : bool, %5165 : bool, %5166 : int):
          %tensor.36 : Tensor = aten::__getitem__(%features.4, %5166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5168 : bool = prim::requires_grad(%tensor.36)
          %5169 : bool, %5170 : bool = prim::If(%5168) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5157)
          %5171 : int = aten::add(%5166, %27)
          %5172 : bool = aten::lt(%5171, %5158)
          %5173 : bool = aten::__and__(%5172, %5169)
          -> (%5173, %5168, %5170, %5171)
      %5174 : bool = prim::If(%5160)
        block0():
          -> (%5161)
        block1():
          -> (%19)
      -> (%5174)
    block1():
      -> (%19)
  %bottleneck_output.70 : Tensor = prim::If(%5156) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5154)
    block1():
      %concated_features.36 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5177 : __torch__.torch.nn.modules.conv.___torch_mangle_328.Conv2d = prim::GetAttr[name="conv1"](%1910)
      %5178 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_237.BatchNorm2d = prim::GetAttr[name="norm1"](%1910)
      %5179 : int = aten::dim(%concated_features.36) # torch/nn/modules/batchnorm.py:276:11
      %5180 : bool = aten::ne(%5179, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5180) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5181 : bool = prim::GetAttr[name="training"](%5178)
       = prim::If(%5181) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5182 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5178)
          %5183 : Tensor = aten::add(%5182, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5178, %5183)
          -> ()
        block1():
          -> ()
      %5184 : bool = prim::GetAttr[name="training"](%5178)
      %5185 : Tensor = prim::GetAttr[name="running_mean"](%5178)
      %5186 : Tensor = prim::GetAttr[name="running_var"](%5178)
      %5187 : Tensor = prim::GetAttr[name="weight"](%5178)
      %5188 : Tensor = prim::GetAttr[name="bias"](%5178)
       = prim::If(%5184) # torch/nn/functional.py:2011:4
        block0():
          %5189 : int[] = aten::size(%concated_features.36) # torch/nn/functional.py:2012:27
          %size_prods.280 : int = aten::__getitem__(%5189, %24) # torch/nn/functional.py:1991:17
          %5191 : int = aten::len(%5189) # torch/nn/functional.py:1992:19
          %5192 : int = aten::sub(%5191, %26) # torch/nn/functional.py:1992:19
          %size_prods.281 : int = prim::Loop(%5192, %25, %size_prods.280) # torch/nn/functional.py:1992:4
            block0(%i.71 : int, %size_prods.282 : int):
              %5196 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
              %5197 : int = aten::__getitem__(%5189, %5196) # torch/nn/functional.py:1993:22
              %size_prods.283 : int = aten::mul(%size_prods.282, %5197) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.283)
          %5199 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5199) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5200 : Tensor = aten::batch_norm(%concated_features.36, %5187, %5188, %5185, %5186, %5184, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.71 : Tensor = aten::relu_(%5200) # torch/nn/functional.py:1117:17
      %5202 : Tensor = prim::GetAttr[name="weight"](%5177)
      %5203 : Tensor? = prim::GetAttr[name="bias"](%5177)
      %5204 : int[] = prim::ListConstruct(%27, %27)
      %5205 : int[] = prim::ListConstruct(%24, %24)
      %5206 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.71 : Tensor = aten::conv2d(%result.71, %5202, %5203, %5204, %5205, %5206, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.71)
  %5208 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1910)
  %5209 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1910)
  %5210 : int = aten::dim(%bottleneck_output.70) # torch/nn/modules/batchnorm.py:276:11
  %5211 : bool = aten::ne(%5210, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5211) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5212 : bool = prim::GetAttr[name="training"](%5209)
   = prim::If(%5212) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5213 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5209)
      %5214 : Tensor = aten::add(%5213, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5209, %5214)
      -> ()
    block1():
      -> ()
  %5215 : bool = prim::GetAttr[name="training"](%5209)
  %5216 : Tensor = prim::GetAttr[name="running_mean"](%5209)
  %5217 : Tensor = prim::GetAttr[name="running_var"](%5209)
  %5218 : Tensor = prim::GetAttr[name="weight"](%5209)
  %5219 : Tensor = prim::GetAttr[name="bias"](%5209)
   = prim::If(%5215) # torch/nn/functional.py:2011:4
    block0():
      %5220 : int[] = aten::size(%bottleneck_output.70) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%5220, %24) # torch/nn/functional.py:1991:17
      %5222 : int = aten::len(%5220) # torch/nn/functional.py:1992:19
      %5223 : int = aten::sub(%5222, %26) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%5223, %25, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %5227 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
          %5228 : int = aten::__getitem__(%5220, %5227) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %5228) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.287)
      %5230 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5230) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5231 : Tensor = aten::batch_norm(%bottleneck_output.70, %5218, %5219, %5216, %5217, %5215, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.72 : Tensor = aten::relu_(%5231) # torch/nn/functional.py:1117:17
  %5233 : Tensor = prim::GetAttr[name="weight"](%5208)
  %5234 : Tensor? = prim::GetAttr[name="bias"](%5208)
  %5235 : int[] = prim::ListConstruct(%27, %27)
  %5236 : int[] = prim::ListConstruct(%27, %27)
  %5237 : int[] = prim::ListConstruct(%27, %27)
  %new_features.72 : Tensor = aten::conv2d(%result.72, %5233, %5234, %5235, %5236, %5237, %27) # torch/nn/modules/conv.py:415:15
  %5239 : float = prim::GetAttr[name="drop_rate"](%1910)
  %5240 : bool = aten::gt(%5239, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.69 : Tensor = prim::If(%5240) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5242 : float = prim::GetAttr[name="drop_rate"](%1910)
      %5243 : bool = prim::GetAttr[name="training"](%1910)
      %5244 : bool = aten::lt(%5242, %16) # torch/nn/functional.py:968:7
      %5245 : bool = prim::If(%5244) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5246 : bool = aten::gt(%5242, %17) # torch/nn/functional.py:968:17
          -> (%5246)
       = prim::If(%5245) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5247 : Tensor = aten::dropout(%new_features.72, %5242, %5243) # torch/nn/functional.py:973:17
      -> (%5247)
    block1():
      -> (%new_features.72)
  %5248 : Tensor[] = aten::append(%features.4, %new_features.69) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5249 : Tensor = prim::Uninitialized()
  %5250 : bool = prim::GetAttr[name="memory_efficient"](%1911)
  %5251 : bool = prim::If(%5250) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5252 : bool = prim::Uninitialized()
      %5253 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5254 : bool = aten::gt(%5253, %24)
      %5255 : bool, %5256 : bool, %5257 : int = prim::Loop(%18, %5254, %19, %5252, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5258 : int, %5259 : bool, %5260 : bool, %5261 : int):
          %tensor.37 : Tensor = aten::__getitem__(%features.4, %5261) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5263 : bool = prim::requires_grad(%tensor.37)
          %5264 : bool, %5265 : bool = prim::If(%5263) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5252)
          %5266 : int = aten::add(%5261, %27)
          %5267 : bool = aten::lt(%5266, %5253)
          %5268 : bool = aten::__and__(%5267, %5264)
          -> (%5268, %5263, %5265, %5266)
      %5269 : bool = prim::If(%5255)
        block0():
          -> (%5256)
        block1():
          -> (%19)
      -> (%5269)
    block1():
      -> (%19)
  %bottleneck_output.72 : Tensor = prim::If(%5251) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5249)
    block1():
      %concated_features.37 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5272 : __torch__.torch.nn.modules.conv.___torch_mangle_331.Conv2d = prim::GetAttr[name="conv1"](%1911)
      %5273 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_330.BatchNorm2d = prim::GetAttr[name="norm1"](%1911)
      %5274 : int = aten::dim(%concated_features.37) # torch/nn/modules/batchnorm.py:276:11
      %5275 : bool = aten::ne(%5274, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5275) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5276 : bool = prim::GetAttr[name="training"](%5273)
       = prim::If(%5276) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5277 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5273)
          %5278 : Tensor = aten::add(%5277, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5273, %5278)
          -> ()
        block1():
          -> ()
      %5279 : bool = prim::GetAttr[name="training"](%5273)
      %5280 : Tensor = prim::GetAttr[name="running_mean"](%5273)
      %5281 : Tensor = prim::GetAttr[name="running_var"](%5273)
      %5282 : Tensor = prim::GetAttr[name="weight"](%5273)
      %5283 : Tensor = prim::GetAttr[name="bias"](%5273)
       = prim::If(%5279) # torch/nn/functional.py:2011:4
        block0():
          %5284 : int[] = aten::size(%concated_features.37) # torch/nn/functional.py:2012:27
          %size_prods.288 : int = aten::__getitem__(%5284, %24) # torch/nn/functional.py:1991:17
          %5286 : int = aten::len(%5284) # torch/nn/functional.py:1992:19
          %5287 : int = aten::sub(%5286, %26) # torch/nn/functional.py:1992:19
          %size_prods.289 : int = prim::Loop(%5287, %25, %size_prods.288) # torch/nn/functional.py:1992:4
            block0(%i.73 : int, %size_prods.290 : int):
              %5291 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
              %5292 : int = aten::__getitem__(%5284, %5291) # torch/nn/functional.py:1993:22
              %size_prods.291 : int = aten::mul(%size_prods.290, %5292) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.291)
          %5294 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5294) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5295 : Tensor = aten::batch_norm(%concated_features.37, %5282, %5283, %5280, %5281, %5279, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.73 : Tensor = aten::relu_(%5295) # torch/nn/functional.py:1117:17
      %5297 : Tensor = prim::GetAttr[name="weight"](%5272)
      %5298 : Tensor? = prim::GetAttr[name="bias"](%5272)
      %5299 : int[] = prim::ListConstruct(%27, %27)
      %5300 : int[] = prim::ListConstruct(%24, %24)
      %5301 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.73 : Tensor = aten::conv2d(%result.73, %5297, %5298, %5299, %5300, %5301, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.73)
  %5303 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1911)
  %5304 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1911)
  %5305 : int = aten::dim(%bottleneck_output.72) # torch/nn/modules/batchnorm.py:276:11
  %5306 : bool = aten::ne(%5305, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5306) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5307 : bool = prim::GetAttr[name="training"](%5304)
   = prim::If(%5307) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5308 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5304)
      %5309 : Tensor = aten::add(%5308, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5304, %5309)
      -> ()
    block1():
      -> ()
  %5310 : bool = prim::GetAttr[name="training"](%5304)
  %5311 : Tensor = prim::GetAttr[name="running_mean"](%5304)
  %5312 : Tensor = prim::GetAttr[name="running_var"](%5304)
  %5313 : Tensor = prim::GetAttr[name="weight"](%5304)
  %5314 : Tensor = prim::GetAttr[name="bias"](%5304)
   = prim::If(%5310) # torch/nn/functional.py:2011:4
    block0():
      %5315 : int[] = aten::size(%bottleneck_output.72) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%5315, %24) # torch/nn/functional.py:1991:17
      %5317 : int = aten::len(%5315) # torch/nn/functional.py:1992:19
      %5318 : int = aten::sub(%5317, %26) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%5318, %25, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %5322 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
          %5323 : int = aten::__getitem__(%5315, %5322) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %5323) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.295)
      %5325 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5325) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5326 : Tensor = aten::batch_norm(%bottleneck_output.72, %5313, %5314, %5311, %5312, %5310, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.74 : Tensor = aten::relu_(%5326) # torch/nn/functional.py:1117:17
  %5328 : Tensor = prim::GetAttr[name="weight"](%5303)
  %5329 : Tensor? = prim::GetAttr[name="bias"](%5303)
  %5330 : int[] = prim::ListConstruct(%27, %27)
  %5331 : int[] = prim::ListConstruct(%27, %27)
  %5332 : int[] = prim::ListConstruct(%27, %27)
  %new_features.74 : Tensor = aten::conv2d(%result.74, %5328, %5329, %5330, %5331, %5332, %27) # torch/nn/modules/conv.py:415:15
  %5334 : float = prim::GetAttr[name="drop_rate"](%1911)
  %5335 : bool = aten::gt(%5334, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.71 : Tensor = prim::If(%5335) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5337 : float = prim::GetAttr[name="drop_rate"](%1911)
      %5338 : bool = prim::GetAttr[name="training"](%1911)
      %5339 : bool = aten::lt(%5337, %16) # torch/nn/functional.py:968:7
      %5340 : bool = prim::If(%5339) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5341 : bool = aten::gt(%5337, %17) # torch/nn/functional.py:968:17
          -> (%5341)
       = prim::If(%5340) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5342 : Tensor = aten::dropout(%new_features.74, %5337, %5338) # torch/nn/functional.py:973:17
      -> (%5342)
    block1():
      -> (%new_features.74)
  %5343 : Tensor[] = aten::append(%features.4, %new_features.71) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5344 : Tensor = prim::Uninitialized()
  %5345 : bool = prim::GetAttr[name="memory_efficient"](%1912)
  %5346 : bool = prim::If(%5345) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5347 : bool = prim::Uninitialized()
      %5348 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5349 : bool = aten::gt(%5348, %24)
      %5350 : bool, %5351 : bool, %5352 : int = prim::Loop(%18, %5349, %19, %5347, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5353 : int, %5354 : bool, %5355 : bool, %5356 : int):
          %tensor.38 : Tensor = aten::__getitem__(%features.4, %5356) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5358 : bool = prim::requires_grad(%tensor.38)
          %5359 : bool, %5360 : bool = prim::If(%5358) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5347)
          %5361 : int = aten::add(%5356, %27)
          %5362 : bool = aten::lt(%5361, %5348)
          %5363 : bool = aten::__and__(%5362, %5359)
          -> (%5363, %5358, %5360, %5361)
      %5364 : bool = prim::If(%5350)
        block0():
          -> (%5351)
        block1():
          -> (%19)
      -> (%5364)
    block1():
      -> (%19)
  %bottleneck_output.74 : Tensor = prim::If(%5346) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5344)
    block1():
      %concated_features.38 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5367 : __torch__.torch.nn.modules.conv.___torch_mangle_334.Conv2d = prim::GetAttr[name="conv1"](%1912)
      %5368 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_333.BatchNorm2d = prim::GetAttr[name="norm1"](%1912)
      %5369 : int = aten::dim(%concated_features.38) # torch/nn/modules/batchnorm.py:276:11
      %5370 : bool = aten::ne(%5369, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5370) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5371 : bool = prim::GetAttr[name="training"](%5368)
       = prim::If(%5371) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5372 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5368)
          %5373 : Tensor = aten::add(%5372, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5368, %5373)
          -> ()
        block1():
          -> ()
      %5374 : bool = prim::GetAttr[name="training"](%5368)
      %5375 : Tensor = prim::GetAttr[name="running_mean"](%5368)
      %5376 : Tensor = prim::GetAttr[name="running_var"](%5368)
      %5377 : Tensor = prim::GetAttr[name="weight"](%5368)
      %5378 : Tensor = prim::GetAttr[name="bias"](%5368)
       = prim::If(%5374) # torch/nn/functional.py:2011:4
        block0():
          %5379 : int[] = aten::size(%concated_features.38) # torch/nn/functional.py:2012:27
          %size_prods.296 : int = aten::__getitem__(%5379, %24) # torch/nn/functional.py:1991:17
          %5381 : int = aten::len(%5379) # torch/nn/functional.py:1992:19
          %5382 : int = aten::sub(%5381, %26) # torch/nn/functional.py:1992:19
          %size_prods.297 : int = prim::Loop(%5382, %25, %size_prods.296) # torch/nn/functional.py:1992:4
            block0(%i.75 : int, %size_prods.298 : int):
              %5386 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
              %5387 : int = aten::__getitem__(%5379, %5386) # torch/nn/functional.py:1993:22
              %size_prods.299 : int = aten::mul(%size_prods.298, %5387) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.299)
          %5389 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5389) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5390 : Tensor = aten::batch_norm(%concated_features.38, %5377, %5378, %5375, %5376, %5374, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.75 : Tensor = aten::relu_(%5390) # torch/nn/functional.py:1117:17
      %5392 : Tensor = prim::GetAttr[name="weight"](%5367)
      %5393 : Tensor? = prim::GetAttr[name="bias"](%5367)
      %5394 : int[] = prim::ListConstruct(%27, %27)
      %5395 : int[] = prim::ListConstruct(%24, %24)
      %5396 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.75 : Tensor = aten::conv2d(%result.75, %5392, %5393, %5394, %5395, %5396, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.75)
  %5398 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1912)
  %5399 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1912)
  %5400 : int = aten::dim(%bottleneck_output.74) # torch/nn/modules/batchnorm.py:276:11
  %5401 : bool = aten::ne(%5400, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5401) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5402 : bool = prim::GetAttr[name="training"](%5399)
   = prim::If(%5402) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5403 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5399)
      %5404 : Tensor = aten::add(%5403, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5399, %5404)
      -> ()
    block1():
      -> ()
  %5405 : bool = prim::GetAttr[name="training"](%5399)
  %5406 : Tensor = prim::GetAttr[name="running_mean"](%5399)
  %5407 : Tensor = prim::GetAttr[name="running_var"](%5399)
  %5408 : Tensor = prim::GetAttr[name="weight"](%5399)
  %5409 : Tensor = prim::GetAttr[name="bias"](%5399)
   = prim::If(%5405) # torch/nn/functional.py:2011:4
    block0():
      %5410 : int[] = aten::size(%bottleneck_output.74) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%5410, %24) # torch/nn/functional.py:1991:17
      %5412 : int = aten::len(%5410) # torch/nn/functional.py:1992:19
      %5413 : int = aten::sub(%5412, %26) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%5413, %25, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %5417 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
          %5418 : int = aten::__getitem__(%5410, %5417) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %5418) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.303)
      %5420 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5420) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5421 : Tensor = aten::batch_norm(%bottleneck_output.74, %5408, %5409, %5406, %5407, %5405, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.76 : Tensor = aten::relu_(%5421) # torch/nn/functional.py:1117:17
  %5423 : Tensor = prim::GetAttr[name="weight"](%5398)
  %5424 : Tensor? = prim::GetAttr[name="bias"](%5398)
  %5425 : int[] = prim::ListConstruct(%27, %27)
  %5426 : int[] = prim::ListConstruct(%27, %27)
  %5427 : int[] = prim::ListConstruct(%27, %27)
  %new_features.76 : Tensor = aten::conv2d(%result.76, %5423, %5424, %5425, %5426, %5427, %27) # torch/nn/modules/conv.py:415:15
  %5429 : float = prim::GetAttr[name="drop_rate"](%1912)
  %5430 : bool = aten::gt(%5429, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.73 : Tensor = prim::If(%5430) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5432 : float = prim::GetAttr[name="drop_rate"](%1912)
      %5433 : bool = prim::GetAttr[name="training"](%1912)
      %5434 : bool = aten::lt(%5432, %16) # torch/nn/functional.py:968:7
      %5435 : bool = prim::If(%5434) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5436 : bool = aten::gt(%5432, %17) # torch/nn/functional.py:968:17
          -> (%5436)
       = prim::If(%5435) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5437 : Tensor = aten::dropout(%new_features.76, %5432, %5433) # torch/nn/functional.py:973:17
      -> (%5437)
    block1():
      -> (%new_features.76)
  %5438 : Tensor[] = aten::append(%features.4, %new_features.73) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5439 : Tensor = prim::Uninitialized()
  %5440 : bool = prim::GetAttr[name="memory_efficient"](%1913)
  %5441 : bool = prim::If(%5440) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5442 : bool = prim::Uninitialized()
      %5443 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5444 : bool = aten::gt(%5443, %24)
      %5445 : bool, %5446 : bool, %5447 : int = prim::Loop(%18, %5444, %19, %5442, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5448 : int, %5449 : bool, %5450 : bool, %5451 : int):
          %tensor.39 : Tensor = aten::__getitem__(%features.4, %5451) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5453 : bool = prim::requires_grad(%tensor.39)
          %5454 : bool, %5455 : bool = prim::If(%5453) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5442)
          %5456 : int = aten::add(%5451, %27)
          %5457 : bool = aten::lt(%5456, %5443)
          %5458 : bool = aten::__and__(%5457, %5454)
          -> (%5458, %5453, %5455, %5456)
      %5459 : bool = prim::If(%5445)
        block0():
          -> (%5446)
        block1():
          -> (%19)
      -> (%5459)
    block1():
      -> (%19)
  %bottleneck_output.76 : Tensor = prim::If(%5441) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5439)
    block1():
      %concated_features.39 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5462 : __torch__.torch.nn.modules.conv.___torch_mangle_336.Conv2d = prim::GetAttr[name="conv1"](%1913)
      %5463 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_243.BatchNorm2d = prim::GetAttr[name="norm1"](%1913)
      %5464 : int = aten::dim(%concated_features.39) # torch/nn/modules/batchnorm.py:276:11
      %5465 : bool = aten::ne(%5464, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5465) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5466 : bool = prim::GetAttr[name="training"](%5463)
       = prim::If(%5466) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5467 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5463)
          %5468 : Tensor = aten::add(%5467, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5463, %5468)
          -> ()
        block1():
          -> ()
      %5469 : bool = prim::GetAttr[name="training"](%5463)
      %5470 : Tensor = prim::GetAttr[name="running_mean"](%5463)
      %5471 : Tensor = prim::GetAttr[name="running_var"](%5463)
      %5472 : Tensor = prim::GetAttr[name="weight"](%5463)
      %5473 : Tensor = prim::GetAttr[name="bias"](%5463)
       = prim::If(%5469) # torch/nn/functional.py:2011:4
        block0():
          %5474 : int[] = aten::size(%concated_features.39) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%5474, %24) # torch/nn/functional.py:1991:17
          %5476 : int = aten::len(%5474) # torch/nn/functional.py:1992:19
          %5477 : int = aten::sub(%5476, %26) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%5477, %25, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %5481 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
              %5482 : int = aten::__getitem__(%5474, %5481) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %5482) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.307)
          %5484 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5484) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5485 : Tensor = aten::batch_norm(%concated_features.39, %5472, %5473, %5470, %5471, %5469, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.77 : Tensor = aten::relu_(%5485) # torch/nn/functional.py:1117:17
      %5487 : Tensor = prim::GetAttr[name="weight"](%5462)
      %5488 : Tensor? = prim::GetAttr[name="bias"](%5462)
      %5489 : int[] = prim::ListConstruct(%27, %27)
      %5490 : int[] = prim::ListConstruct(%24, %24)
      %5491 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.77 : Tensor = aten::conv2d(%result.77, %5487, %5488, %5489, %5490, %5491, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.77)
  %5493 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1913)
  %5494 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1913)
  %5495 : int = aten::dim(%bottleneck_output.76) # torch/nn/modules/batchnorm.py:276:11
  %5496 : bool = aten::ne(%5495, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5496) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5497 : bool = prim::GetAttr[name="training"](%5494)
   = prim::If(%5497) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5498 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5494)
      %5499 : Tensor = aten::add(%5498, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5494, %5499)
      -> ()
    block1():
      -> ()
  %5500 : bool = prim::GetAttr[name="training"](%5494)
  %5501 : Tensor = prim::GetAttr[name="running_mean"](%5494)
  %5502 : Tensor = prim::GetAttr[name="running_var"](%5494)
  %5503 : Tensor = prim::GetAttr[name="weight"](%5494)
  %5504 : Tensor = prim::GetAttr[name="bias"](%5494)
   = prim::If(%5500) # torch/nn/functional.py:2011:4
    block0():
      %5505 : int[] = aten::size(%bottleneck_output.76) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%5505, %24) # torch/nn/functional.py:1991:17
      %5507 : int = aten::len(%5505) # torch/nn/functional.py:1992:19
      %5508 : int = aten::sub(%5507, %26) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%5508, %25, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %5512 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
          %5513 : int = aten::__getitem__(%5505, %5512) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %5513) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.311)
      %5515 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5515) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5516 : Tensor = aten::batch_norm(%bottleneck_output.76, %5503, %5504, %5501, %5502, %5500, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.78 : Tensor = aten::relu_(%5516) # torch/nn/functional.py:1117:17
  %5518 : Tensor = prim::GetAttr[name="weight"](%5493)
  %5519 : Tensor? = prim::GetAttr[name="bias"](%5493)
  %5520 : int[] = prim::ListConstruct(%27, %27)
  %5521 : int[] = prim::ListConstruct(%27, %27)
  %5522 : int[] = prim::ListConstruct(%27, %27)
  %new_features.78 : Tensor = aten::conv2d(%result.78, %5518, %5519, %5520, %5521, %5522, %27) # torch/nn/modules/conv.py:415:15
  %5524 : float = prim::GetAttr[name="drop_rate"](%1913)
  %5525 : bool = aten::gt(%5524, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.75 : Tensor = prim::If(%5525) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5527 : float = prim::GetAttr[name="drop_rate"](%1913)
      %5528 : bool = prim::GetAttr[name="training"](%1913)
      %5529 : bool = aten::lt(%5527, %16) # torch/nn/functional.py:968:7
      %5530 : bool = prim::If(%5529) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5531 : bool = aten::gt(%5527, %17) # torch/nn/functional.py:968:17
          -> (%5531)
       = prim::If(%5530) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5532 : Tensor = aten::dropout(%new_features.78, %5527, %5528) # torch/nn/functional.py:973:17
      -> (%5532)
    block1():
      -> (%new_features.78)
  %5533 : Tensor[] = aten::append(%features.4, %new_features.75) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5534 : Tensor = prim::Uninitialized()
  %5535 : bool = prim::GetAttr[name="memory_efficient"](%1914)
  %5536 : bool = prim::If(%5535) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5537 : bool = prim::Uninitialized()
      %5538 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5539 : bool = aten::gt(%5538, %24)
      %5540 : bool, %5541 : bool, %5542 : int = prim::Loop(%18, %5539, %19, %5537, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5543 : int, %5544 : bool, %5545 : bool, %5546 : int):
          %tensor.40 : Tensor = aten::__getitem__(%features.4, %5546) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5548 : bool = prim::requires_grad(%tensor.40)
          %5549 : bool, %5550 : bool = prim::If(%5548) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5537)
          %5551 : int = aten::add(%5546, %27)
          %5552 : bool = aten::lt(%5551, %5538)
          %5553 : bool = aten::__and__(%5552, %5549)
          -> (%5553, %5548, %5550, %5551)
      %5554 : bool = prim::If(%5540)
        block0():
          -> (%5541)
        block1():
          -> (%19)
      -> (%5554)
    block1():
      -> (%19)
  %bottleneck_output.78 : Tensor = prim::If(%5536) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5534)
    block1():
      %concated_features.40 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5557 : __torch__.torch.nn.modules.conv.___torch_mangle_339.Conv2d = prim::GetAttr[name="conv1"](%1914)
      %5558 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_338.BatchNorm2d = prim::GetAttr[name="norm1"](%1914)
      %5559 : int = aten::dim(%concated_features.40) # torch/nn/modules/batchnorm.py:276:11
      %5560 : bool = aten::ne(%5559, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5560) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5561 : bool = prim::GetAttr[name="training"](%5558)
       = prim::If(%5561) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5562 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5558)
          %5563 : Tensor = aten::add(%5562, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5558, %5563)
          -> ()
        block1():
          -> ()
      %5564 : bool = prim::GetAttr[name="training"](%5558)
      %5565 : Tensor = prim::GetAttr[name="running_mean"](%5558)
      %5566 : Tensor = prim::GetAttr[name="running_var"](%5558)
      %5567 : Tensor = prim::GetAttr[name="weight"](%5558)
      %5568 : Tensor = prim::GetAttr[name="bias"](%5558)
       = prim::If(%5564) # torch/nn/functional.py:2011:4
        block0():
          %5569 : int[] = aten::size(%concated_features.40) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%5569, %24) # torch/nn/functional.py:1991:17
          %5571 : int = aten::len(%5569) # torch/nn/functional.py:1992:19
          %5572 : int = aten::sub(%5571, %26) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%5572, %25, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %5576 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
              %5577 : int = aten::__getitem__(%5569, %5576) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %5577) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.315)
          %5579 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5579) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5580 : Tensor = aten::batch_norm(%concated_features.40, %5567, %5568, %5565, %5566, %5564, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.79 : Tensor = aten::relu_(%5580) # torch/nn/functional.py:1117:17
      %5582 : Tensor = prim::GetAttr[name="weight"](%5557)
      %5583 : Tensor? = prim::GetAttr[name="bias"](%5557)
      %5584 : int[] = prim::ListConstruct(%27, %27)
      %5585 : int[] = prim::ListConstruct(%24, %24)
      %5586 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.79 : Tensor = aten::conv2d(%result.79, %5582, %5583, %5584, %5585, %5586, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.79)
  %5588 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1914)
  %5589 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1914)
  %5590 : int = aten::dim(%bottleneck_output.78) # torch/nn/modules/batchnorm.py:276:11
  %5591 : bool = aten::ne(%5590, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5591) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5592 : bool = prim::GetAttr[name="training"](%5589)
   = prim::If(%5592) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5593 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5589)
      %5594 : Tensor = aten::add(%5593, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5589, %5594)
      -> ()
    block1():
      -> ()
  %5595 : bool = prim::GetAttr[name="training"](%5589)
  %5596 : Tensor = prim::GetAttr[name="running_mean"](%5589)
  %5597 : Tensor = prim::GetAttr[name="running_var"](%5589)
  %5598 : Tensor = prim::GetAttr[name="weight"](%5589)
  %5599 : Tensor = prim::GetAttr[name="bias"](%5589)
   = prim::If(%5595) # torch/nn/functional.py:2011:4
    block0():
      %5600 : int[] = aten::size(%bottleneck_output.78) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%5600, %24) # torch/nn/functional.py:1991:17
      %5602 : int = aten::len(%5600) # torch/nn/functional.py:1992:19
      %5603 : int = aten::sub(%5602, %26) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%5603, %25, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %5607 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
          %5608 : int = aten::__getitem__(%5600, %5607) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %5608) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.319)
      %5610 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5610) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5611 : Tensor = aten::batch_norm(%bottleneck_output.78, %5598, %5599, %5596, %5597, %5595, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.80 : Tensor = aten::relu_(%5611) # torch/nn/functional.py:1117:17
  %5613 : Tensor = prim::GetAttr[name="weight"](%5588)
  %5614 : Tensor? = prim::GetAttr[name="bias"](%5588)
  %5615 : int[] = prim::ListConstruct(%27, %27)
  %5616 : int[] = prim::ListConstruct(%27, %27)
  %5617 : int[] = prim::ListConstruct(%27, %27)
  %new_features.80 : Tensor = aten::conv2d(%result.80, %5613, %5614, %5615, %5616, %5617, %27) # torch/nn/modules/conv.py:415:15
  %5619 : float = prim::GetAttr[name="drop_rate"](%1914)
  %5620 : bool = aten::gt(%5619, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.77 : Tensor = prim::If(%5620) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5622 : float = prim::GetAttr[name="drop_rate"](%1914)
      %5623 : bool = prim::GetAttr[name="training"](%1914)
      %5624 : bool = aten::lt(%5622, %16) # torch/nn/functional.py:968:7
      %5625 : bool = prim::If(%5624) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5626 : bool = aten::gt(%5622, %17) # torch/nn/functional.py:968:17
          -> (%5626)
       = prim::If(%5625) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5627 : Tensor = aten::dropout(%new_features.80, %5622, %5623) # torch/nn/functional.py:973:17
      -> (%5627)
    block1():
      -> (%new_features.80)
  %5628 : Tensor[] = aten::append(%features.4, %new_features.77) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5629 : Tensor = prim::Uninitialized()
  %5630 : bool = prim::GetAttr[name="memory_efficient"](%1915)
  %5631 : bool = prim::If(%5630) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5632 : bool = prim::Uninitialized()
      %5633 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5634 : bool = aten::gt(%5633, %24)
      %5635 : bool, %5636 : bool, %5637 : int = prim::Loop(%18, %5634, %19, %5632, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5638 : int, %5639 : bool, %5640 : bool, %5641 : int):
          %tensor.41 : Tensor = aten::__getitem__(%features.4, %5641) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5643 : bool = prim::requires_grad(%tensor.41)
          %5644 : bool, %5645 : bool = prim::If(%5643) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5632)
          %5646 : int = aten::add(%5641, %27)
          %5647 : bool = aten::lt(%5646, %5633)
          %5648 : bool = aten::__and__(%5647, %5644)
          -> (%5648, %5643, %5645, %5646)
      %5649 : bool = prim::If(%5635)
        block0():
          -> (%5636)
        block1():
          -> (%19)
      -> (%5649)
    block1():
      -> (%19)
  %bottleneck_output.80 : Tensor = prim::If(%5631) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5629)
    block1():
      %concated_features.41 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5652 : __torch__.torch.nn.modules.conv.___torch_mangle_342.Conv2d = prim::GetAttr[name="conv1"](%1915)
      %5653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_341.BatchNorm2d = prim::GetAttr[name="norm1"](%1915)
      %5654 : int = aten::dim(%concated_features.41) # torch/nn/modules/batchnorm.py:276:11
      %5655 : bool = aten::ne(%5654, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5655) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5656 : bool = prim::GetAttr[name="training"](%5653)
       = prim::If(%5656) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5657 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5653)
          %5658 : Tensor = aten::add(%5657, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5653, %5658)
          -> ()
        block1():
          -> ()
      %5659 : bool = prim::GetAttr[name="training"](%5653)
      %5660 : Tensor = prim::GetAttr[name="running_mean"](%5653)
      %5661 : Tensor = prim::GetAttr[name="running_var"](%5653)
      %5662 : Tensor = prim::GetAttr[name="weight"](%5653)
      %5663 : Tensor = prim::GetAttr[name="bias"](%5653)
       = prim::If(%5659) # torch/nn/functional.py:2011:4
        block0():
          %5664 : int[] = aten::size(%concated_features.41) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%5664, %24) # torch/nn/functional.py:1991:17
          %5666 : int = aten::len(%5664) # torch/nn/functional.py:1992:19
          %5667 : int = aten::sub(%5666, %26) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%5667, %25, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %5671 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
              %5672 : int = aten::__getitem__(%5664, %5671) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %5672) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.323)
          %5674 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5674) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5675 : Tensor = aten::batch_norm(%concated_features.41, %5662, %5663, %5660, %5661, %5659, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.81 : Tensor = aten::relu_(%5675) # torch/nn/functional.py:1117:17
      %5677 : Tensor = prim::GetAttr[name="weight"](%5652)
      %5678 : Tensor? = prim::GetAttr[name="bias"](%5652)
      %5679 : int[] = prim::ListConstruct(%27, %27)
      %5680 : int[] = prim::ListConstruct(%24, %24)
      %5681 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.81 : Tensor = aten::conv2d(%result.81, %5677, %5678, %5679, %5680, %5681, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.81)
  %5683 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1915)
  %5684 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1915)
  %5685 : int = aten::dim(%bottleneck_output.80) # torch/nn/modules/batchnorm.py:276:11
  %5686 : bool = aten::ne(%5685, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5686) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5687 : bool = prim::GetAttr[name="training"](%5684)
   = prim::If(%5687) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5688 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5684)
      %5689 : Tensor = aten::add(%5688, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5684, %5689)
      -> ()
    block1():
      -> ()
  %5690 : bool = prim::GetAttr[name="training"](%5684)
  %5691 : Tensor = prim::GetAttr[name="running_mean"](%5684)
  %5692 : Tensor = prim::GetAttr[name="running_var"](%5684)
  %5693 : Tensor = prim::GetAttr[name="weight"](%5684)
  %5694 : Tensor = prim::GetAttr[name="bias"](%5684)
   = prim::If(%5690) # torch/nn/functional.py:2011:4
    block0():
      %5695 : int[] = aten::size(%bottleneck_output.80) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%5695, %24) # torch/nn/functional.py:1991:17
      %5697 : int = aten::len(%5695) # torch/nn/functional.py:1992:19
      %5698 : int = aten::sub(%5697, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%5698, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %5702 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %5703 : int = aten::__getitem__(%5695, %5702) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %5703) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %5705 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5705) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5706 : Tensor = aten::batch_norm(%bottleneck_output.80, %5693, %5694, %5691, %5692, %5690, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.82 : Tensor = aten::relu_(%5706) # torch/nn/functional.py:1117:17
  %5708 : Tensor = prim::GetAttr[name="weight"](%5683)
  %5709 : Tensor? = prim::GetAttr[name="bias"](%5683)
  %5710 : int[] = prim::ListConstruct(%27, %27)
  %5711 : int[] = prim::ListConstruct(%27, %27)
  %5712 : int[] = prim::ListConstruct(%27, %27)
  %new_features.82 : Tensor = aten::conv2d(%result.82, %5708, %5709, %5710, %5711, %5712, %27) # torch/nn/modules/conv.py:415:15
  %5714 : float = prim::GetAttr[name="drop_rate"](%1915)
  %5715 : bool = aten::gt(%5714, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.79 : Tensor = prim::If(%5715) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5717 : float = prim::GetAttr[name="drop_rate"](%1915)
      %5718 : bool = prim::GetAttr[name="training"](%1915)
      %5719 : bool = aten::lt(%5717, %16) # torch/nn/functional.py:968:7
      %5720 : bool = prim::If(%5719) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5721 : bool = aten::gt(%5717, %17) # torch/nn/functional.py:968:17
          -> (%5721)
       = prim::If(%5720) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5722 : Tensor = aten::dropout(%new_features.82, %5717, %5718) # torch/nn/functional.py:973:17
      -> (%5722)
    block1():
      -> (%new_features.82)
  %5723 : Tensor[] = aten::append(%features.4, %new_features.79) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5724 : Tensor = prim::Uninitialized()
  %5725 : bool = prim::GetAttr[name="memory_efficient"](%1916)
  %5726 : bool = prim::If(%5725) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5727 : bool = prim::Uninitialized()
      %5728 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5729 : bool = aten::gt(%5728, %24)
      %5730 : bool, %5731 : bool, %5732 : int = prim::Loop(%18, %5729, %19, %5727, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5733 : int, %5734 : bool, %5735 : bool, %5736 : int):
          %tensor.42 : Tensor = aten::__getitem__(%features.4, %5736) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5738 : bool = prim::requires_grad(%tensor.42)
          %5739 : bool, %5740 : bool = prim::If(%5738) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5727)
          %5741 : int = aten::add(%5736, %27)
          %5742 : bool = aten::lt(%5741, %5728)
          %5743 : bool = aten::__and__(%5742, %5739)
          -> (%5743, %5738, %5740, %5741)
      %5744 : bool = prim::If(%5730)
        block0():
          -> (%5731)
        block1():
          -> (%19)
      -> (%5744)
    block1():
      -> (%19)
  %bottleneck_output.82 : Tensor = prim::If(%5726) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5724)
    block1():
      %concated_features.42 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5747 : __torch__.torch.nn.modules.conv.___torch_mangle_344.Conv2d = prim::GetAttr[name="conv1"](%1916)
      %5748 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_249.BatchNorm2d = prim::GetAttr[name="norm1"](%1916)
      %5749 : int = aten::dim(%concated_features.42) # torch/nn/modules/batchnorm.py:276:11
      %5750 : bool = aten::ne(%5749, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5750) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5751 : bool = prim::GetAttr[name="training"](%5748)
       = prim::If(%5751) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5752 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5748)
          %5753 : Tensor = aten::add(%5752, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5748, %5753)
          -> ()
        block1():
          -> ()
      %5754 : bool = prim::GetAttr[name="training"](%5748)
      %5755 : Tensor = prim::GetAttr[name="running_mean"](%5748)
      %5756 : Tensor = prim::GetAttr[name="running_var"](%5748)
      %5757 : Tensor = prim::GetAttr[name="weight"](%5748)
      %5758 : Tensor = prim::GetAttr[name="bias"](%5748)
       = prim::If(%5754) # torch/nn/functional.py:2011:4
        block0():
          %5759 : int[] = aten::size(%concated_features.42) # torch/nn/functional.py:2012:27
          %size_prods.328 : int = aten::__getitem__(%5759, %24) # torch/nn/functional.py:1991:17
          %5761 : int = aten::len(%5759) # torch/nn/functional.py:1992:19
          %5762 : int = aten::sub(%5761, %26) # torch/nn/functional.py:1992:19
          %size_prods.329 : int = prim::Loop(%5762, %25, %size_prods.328) # torch/nn/functional.py:1992:4
            block0(%i.83 : int, %size_prods.330 : int):
              %5766 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
              %5767 : int = aten::__getitem__(%5759, %5766) # torch/nn/functional.py:1993:22
              %size_prods.331 : int = aten::mul(%size_prods.330, %5767) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.331)
          %5769 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5769) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5770 : Tensor = aten::batch_norm(%concated_features.42, %5757, %5758, %5755, %5756, %5754, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.83 : Tensor = aten::relu_(%5770) # torch/nn/functional.py:1117:17
      %5772 : Tensor = prim::GetAttr[name="weight"](%5747)
      %5773 : Tensor? = prim::GetAttr[name="bias"](%5747)
      %5774 : int[] = prim::ListConstruct(%27, %27)
      %5775 : int[] = prim::ListConstruct(%24, %24)
      %5776 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.83 : Tensor = aten::conv2d(%result.83, %5772, %5773, %5774, %5775, %5776, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.83)
  %5778 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1916)
  %5779 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1916)
  %5780 : int = aten::dim(%bottleneck_output.82) # torch/nn/modules/batchnorm.py:276:11
  %5781 : bool = aten::ne(%5780, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5781) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5782 : bool = prim::GetAttr[name="training"](%5779)
   = prim::If(%5782) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5783 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5779)
      %5784 : Tensor = aten::add(%5783, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5779, %5784)
      -> ()
    block1():
      -> ()
  %5785 : bool = prim::GetAttr[name="training"](%5779)
  %5786 : Tensor = prim::GetAttr[name="running_mean"](%5779)
  %5787 : Tensor = prim::GetAttr[name="running_var"](%5779)
  %5788 : Tensor = prim::GetAttr[name="weight"](%5779)
  %5789 : Tensor = prim::GetAttr[name="bias"](%5779)
   = prim::If(%5785) # torch/nn/functional.py:2011:4
    block0():
      %5790 : int[] = aten::size(%bottleneck_output.82) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%5790, %24) # torch/nn/functional.py:1991:17
      %5792 : int = aten::len(%5790) # torch/nn/functional.py:1992:19
      %5793 : int = aten::sub(%5792, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%5793, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %5797 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %5798 : int = aten::__getitem__(%5790, %5797) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %5798) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %5800 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5800) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5801 : Tensor = aten::batch_norm(%bottleneck_output.82, %5788, %5789, %5786, %5787, %5785, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.84 : Tensor = aten::relu_(%5801) # torch/nn/functional.py:1117:17
  %5803 : Tensor = prim::GetAttr[name="weight"](%5778)
  %5804 : Tensor? = prim::GetAttr[name="bias"](%5778)
  %5805 : int[] = prim::ListConstruct(%27, %27)
  %5806 : int[] = prim::ListConstruct(%27, %27)
  %5807 : int[] = prim::ListConstruct(%27, %27)
  %new_features.84 : Tensor = aten::conv2d(%result.84, %5803, %5804, %5805, %5806, %5807, %27) # torch/nn/modules/conv.py:415:15
  %5809 : float = prim::GetAttr[name="drop_rate"](%1916)
  %5810 : bool = aten::gt(%5809, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.81 : Tensor = prim::If(%5810) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5812 : float = prim::GetAttr[name="drop_rate"](%1916)
      %5813 : bool = prim::GetAttr[name="training"](%1916)
      %5814 : bool = aten::lt(%5812, %16) # torch/nn/functional.py:968:7
      %5815 : bool = prim::If(%5814) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5816 : bool = aten::gt(%5812, %17) # torch/nn/functional.py:968:17
          -> (%5816)
       = prim::If(%5815) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5817 : Tensor = aten::dropout(%new_features.84, %5812, %5813) # torch/nn/functional.py:973:17
      -> (%5817)
    block1():
      -> (%new_features.84)
  %5818 : Tensor[] = aten::append(%features.4, %new_features.81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5819 : Tensor = prim::Uninitialized()
  %5820 : bool = prim::GetAttr[name="memory_efficient"](%1917)
  %5821 : bool = prim::If(%5820) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5822 : bool = prim::Uninitialized()
      %5823 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5824 : bool = aten::gt(%5823, %24)
      %5825 : bool, %5826 : bool, %5827 : int = prim::Loop(%18, %5824, %19, %5822, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5828 : int, %5829 : bool, %5830 : bool, %5831 : int):
          %tensor.43 : Tensor = aten::__getitem__(%features.4, %5831) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5833 : bool = prim::requires_grad(%tensor.43)
          %5834 : bool, %5835 : bool = prim::If(%5833) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5822)
          %5836 : int = aten::add(%5831, %27)
          %5837 : bool = aten::lt(%5836, %5823)
          %5838 : bool = aten::__and__(%5837, %5834)
          -> (%5838, %5833, %5835, %5836)
      %5839 : bool = prim::If(%5825)
        block0():
          -> (%5826)
        block1():
          -> (%19)
      -> (%5839)
    block1():
      -> (%19)
  %bottleneck_output.84 : Tensor = prim::If(%5821) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5819)
    block1():
      %concated_features.43 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5842 : __torch__.torch.nn.modules.conv.___torch_mangle_347.Conv2d = prim::GetAttr[name="conv1"](%1917)
      %5843 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_346.BatchNorm2d = prim::GetAttr[name="norm1"](%1917)
      %5844 : int = aten::dim(%concated_features.43) # torch/nn/modules/batchnorm.py:276:11
      %5845 : bool = aten::ne(%5844, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5845) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5846 : bool = prim::GetAttr[name="training"](%5843)
       = prim::If(%5846) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5847 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5843)
          %5848 : Tensor = aten::add(%5847, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5843, %5848)
          -> ()
        block1():
          -> ()
      %5849 : bool = prim::GetAttr[name="training"](%5843)
      %5850 : Tensor = prim::GetAttr[name="running_mean"](%5843)
      %5851 : Tensor = prim::GetAttr[name="running_var"](%5843)
      %5852 : Tensor = prim::GetAttr[name="weight"](%5843)
      %5853 : Tensor = prim::GetAttr[name="bias"](%5843)
       = prim::If(%5849) # torch/nn/functional.py:2011:4
        block0():
          %5854 : int[] = aten::size(%concated_features.43) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%5854, %24) # torch/nn/functional.py:1991:17
          %5856 : int = aten::len(%5854) # torch/nn/functional.py:1992:19
          %5857 : int = aten::sub(%5856, %26) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%5857, %25, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %5861 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
              %5862 : int = aten::__getitem__(%5854, %5861) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %5862) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.339)
          %5864 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5864) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5865 : Tensor = aten::batch_norm(%concated_features.43, %5852, %5853, %5850, %5851, %5849, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.85 : Tensor = aten::relu_(%5865) # torch/nn/functional.py:1117:17
      %5867 : Tensor = prim::GetAttr[name="weight"](%5842)
      %5868 : Tensor? = prim::GetAttr[name="bias"](%5842)
      %5869 : int[] = prim::ListConstruct(%27, %27)
      %5870 : int[] = prim::ListConstruct(%24, %24)
      %5871 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.85 : Tensor = aten::conv2d(%result.85, %5867, %5868, %5869, %5870, %5871, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.85)
  %5873 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1917)
  %5874 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1917)
  %5875 : int = aten::dim(%bottleneck_output.84) # torch/nn/modules/batchnorm.py:276:11
  %5876 : bool = aten::ne(%5875, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5876) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5877 : bool = prim::GetAttr[name="training"](%5874)
   = prim::If(%5877) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5878 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5874)
      %5879 : Tensor = aten::add(%5878, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5874, %5879)
      -> ()
    block1():
      -> ()
  %5880 : bool = prim::GetAttr[name="training"](%5874)
  %5881 : Tensor = prim::GetAttr[name="running_mean"](%5874)
  %5882 : Tensor = prim::GetAttr[name="running_var"](%5874)
  %5883 : Tensor = prim::GetAttr[name="weight"](%5874)
  %5884 : Tensor = prim::GetAttr[name="bias"](%5874)
   = prim::If(%5880) # torch/nn/functional.py:2011:4
    block0():
      %5885 : int[] = aten::size(%bottleneck_output.84) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%5885, %24) # torch/nn/functional.py:1991:17
      %5887 : int = aten::len(%5885) # torch/nn/functional.py:1992:19
      %5888 : int = aten::sub(%5887, %26) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%5888, %25, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %5892 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
          %5893 : int = aten::__getitem__(%5885, %5892) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %5893) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.343)
      %5895 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5895) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5896 : Tensor = aten::batch_norm(%bottleneck_output.84, %5883, %5884, %5881, %5882, %5880, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.86 : Tensor = aten::relu_(%5896) # torch/nn/functional.py:1117:17
  %5898 : Tensor = prim::GetAttr[name="weight"](%5873)
  %5899 : Tensor? = prim::GetAttr[name="bias"](%5873)
  %5900 : int[] = prim::ListConstruct(%27, %27)
  %5901 : int[] = prim::ListConstruct(%27, %27)
  %5902 : int[] = prim::ListConstruct(%27, %27)
  %new_features.86 : Tensor = aten::conv2d(%result.86, %5898, %5899, %5900, %5901, %5902, %27) # torch/nn/modules/conv.py:415:15
  %5904 : float = prim::GetAttr[name="drop_rate"](%1917)
  %5905 : bool = aten::gt(%5904, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.83 : Tensor = prim::If(%5905) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5907 : float = prim::GetAttr[name="drop_rate"](%1917)
      %5908 : bool = prim::GetAttr[name="training"](%1917)
      %5909 : bool = aten::lt(%5907, %16) # torch/nn/functional.py:968:7
      %5910 : bool = prim::If(%5909) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5911 : bool = aten::gt(%5907, %17) # torch/nn/functional.py:968:17
          -> (%5911)
       = prim::If(%5910) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5912 : Tensor = aten::dropout(%new_features.86, %5907, %5908) # torch/nn/functional.py:973:17
      -> (%5912)
    block1():
      -> (%new_features.86)
  %5913 : Tensor[] = aten::append(%features.4, %new_features.83) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5914 : Tensor = prim::Uninitialized()
  %5915 : bool = prim::GetAttr[name="memory_efficient"](%1918)
  %5916 : bool = prim::If(%5915) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5917 : bool = prim::Uninitialized()
      %5918 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5919 : bool = aten::gt(%5918, %24)
      %5920 : bool, %5921 : bool, %5922 : int = prim::Loop(%18, %5919, %19, %5917, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5923 : int, %5924 : bool, %5925 : bool, %5926 : int):
          %tensor.44 : Tensor = aten::__getitem__(%features.4, %5926) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5928 : bool = prim::requires_grad(%tensor.44)
          %5929 : bool, %5930 : bool = prim::If(%5928) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5917)
          %5931 : int = aten::add(%5926, %27)
          %5932 : bool = aten::lt(%5931, %5918)
          %5933 : bool = aten::__and__(%5932, %5929)
          -> (%5933, %5928, %5930, %5931)
      %5934 : bool = prim::If(%5920)
        block0():
          -> (%5921)
        block1():
          -> (%19)
      -> (%5934)
    block1():
      -> (%19)
  %bottleneck_output.86 : Tensor = prim::If(%5916) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5914)
    block1():
      %concated_features.44 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5937 : __torch__.torch.nn.modules.conv.___torch_mangle_350.Conv2d = prim::GetAttr[name="conv1"](%1918)
      %5938 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_349.BatchNorm2d = prim::GetAttr[name="norm1"](%1918)
      %5939 : int = aten::dim(%concated_features.44) # torch/nn/modules/batchnorm.py:276:11
      %5940 : bool = aten::ne(%5939, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5940) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5941 : bool = prim::GetAttr[name="training"](%5938)
       = prim::If(%5941) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5942 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5938)
          %5943 : Tensor = aten::add(%5942, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5938, %5943)
          -> ()
        block1():
          -> ()
      %5944 : bool = prim::GetAttr[name="training"](%5938)
      %5945 : Tensor = prim::GetAttr[name="running_mean"](%5938)
      %5946 : Tensor = prim::GetAttr[name="running_var"](%5938)
      %5947 : Tensor = prim::GetAttr[name="weight"](%5938)
      %5948 : Tensor = prim::GetAttr[name="bias"](%5938)
       = prim::If(%5944) # torch/nn/functional.py:2011:4
        block0():
          %5949 : int[] = aten::size(%concated_features.44) # torch/nn/functional.py:2012:27
          %size_prods.344 : int = aten::__getitem__(%5949, %24) # torch/nn/functional.py:1991:17
          %5951 : int = aten::len(%5949) # torch/nn/functional.py:1992:19
          %5952 : int = aten::sub(%5951, %26) # torch/nn/functional.py:1992:19
          %size_prods.345 : int = prim::Loop(%5952, %25, %size_prods.344) # torch/nn/functional.py:1992:4
            block0(%i.87 : int, %size_prods.346 : int):
              %5956 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
              %5957 : int = aten::__getitem__(%5949, %5956) # torch/nn/functional.py:1993:22
              %size_prods.347 : int = aten::mul(%size_prods.346, %5957) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.347)
          %5959 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5959) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5960 : Tensor = aten::batch_norm(%concated_features.44, %5947, %5948, %5945, %5946, %5944, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.87 : Tensor = aten::relu_(%5960) # torch/nn/functional.py:1117:17
      %5962 : Tensor = prim::GetAttr[name="weight"](%5937)
      %5963 : Tensor? = prim::GetAttr[name="bias"](%5937)
      %5964 : int[] = prim::ListConstruct(%27, %27)
      %5965 : int[] = prim::ListConstruct(%24, %24)
      %5966 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.87 : Tensor = aten::conv2d(%result.87, %5962, %5963, %5964, %5965, %5966, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.87)
  %5968 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1918)
  %5969 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1918)
  %5970 : int = aten::dim(%bottleneck_output.86) # torch/nn/modules/batchnorm.py:276:11
  %5971 : bool = aten::ne(%5970, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5971) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5972 : bool = prim::GetAttr[name="training"](%5969)
   = prim::If(%5972) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5973 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5969)
      %5974 : Tensor = aten::add(%5973, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5969, %5974)
      -> ()
    block1():
      -> ()
  %5975 : bool = prim::GetAttr[name="training"](%5969)
  %5976 : Tensor = prim::GetAttr[name="running_mean"](%5969)
  %5977 : Tensor = prim::GetAttr[name="running_var"](%5969)
  %5978 : Tensor = prim::GetAttr[name="weight"](%5969)
  %5979 : Tensor = prim::GetAttr[name="bias"](%5969)
   = prim::If(%5975) # torch/nn/functional.py:2011:4
    block0():
      %5980 : int[] = aten::size(%bottleneck_output.86) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%5980, %24) # torch/nn/functional.py:1991:17
      %5982 : int = aten::len(%5980) # torch/nn/functional.py:1992:19
      %5983 : int = aten::sub(%5982, %26) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%5983, %25, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %5987 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
          %5988 : int = aten::__getitem__(%5980, %5987) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %5988) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.351)
      %5990 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5990) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5991 : Tensor = aten::batch_norm(%bottleneck_output.86, %5978, %5979, %5976, %5977, %5975, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.88 : Tensor = aten::relu_(%5991) # torch/nn/functional.py:1117:17
  %5993 : Tensor = prim::GetAttr[name="weight"](%5968)
  %5994 : Tensor? = prim::GetAttr[name="bias"](%5968)
  %5995 : int[] = prim::ListConstruct(%27, %27)
  %5996 : int[] = prim::ListConstruct(%27, %27)
  %5997 : int[] = prim::ListConstruct(%27, %27)
  %new_features.88 : Tensor = aten::conv2d(%result.88, %5993, %5994, %5995, %5996, %5997, %27) # torch/nn/modules/conv.py:415:15
  %5999 : float = prim::GetAttr[name="drop_rate"](%1918)
  %6000 : bool = aten::gt(%5999, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.85 : Tensor = prim::If(%6000) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6002 : float = prim::GetAttr[name="drop_rate"](%1918)
      %6003 : bool = prim::GetAttr[name="training"](%1918)
      %6004 : bool = aten::lt(%6002, %16) # torch/nn/functional.py:968:7
      %6005 : bool = prim::If(%6004) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6006 : bool = aten::gt(%6002, %17) # torch/nn/functional.py:968:17
          -> (%6006)
       = prim::If(%6005) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6007 : Tensor = aten::dropout(%new_features.88, %6002, %6003) # torch/nn/functional.py:973:17
      -> (%6007)
    block1():
      -> (%new_features.88)
  %6008 : Tensor[] = aten::append(%features.4, %new_features.85) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6009 : Tensor = prim::Uninitialized()
  %6010 : bool = prim::GetAttr[name="memory_efficient"](%1919)
  %6011 : bool = prim::If(%6010) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6012 : bool = prim::Uninitialized()
      %6013 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6014 : bool = aten::gt(%6013, %24)
      %6015 : bool, %6016 : bool, %6017 : int = prim::Loop(%18, %6014, %19, %6012, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6018 : int, %6019 : bool, %6020 : bool, %6021 : int):
          %tensor.45 : Tensor = aten::__getitem__(%features.4, %6021) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6023 : bool = prim::requires_grad(%tensor.45)
          %6024 : bool, %6025 : bool = prim::If(%6023) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6012)
          %6026 : int = aten::add(%6021, %27)
          %6027 : bool = aten::lt(%6026, %6013)
          %6028 : bool = aten::__and__(%6027, %6024)
          -> (%6028, %6023, %6025, %6026)
      %6029 : bool = prim::If(%6015)
        block0():
          -> (%6016)
        block1():
          -> (%19)
      -> (%6029)
    block1():
      -> (%19)
  %bottleneck_output.88 : Tensor = prim::If(%6011) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6009)
    block1():
      %concated_features.45 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6032 : __torch__.torch.nn.modules.conv.___torch_mangle_352.Conv2d = prim::GetAttr[name="conv1"](%1919)
      %6033 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_255.BatchNorm2d = prim::GetAttr[name="norm1"](%1919)
      %6034 : int = aten::dim(%concated_features.45) # torch/nn/modules/batchnorm.py:276:11
      %6035 : bool = aten::ne(%6034, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6035) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6036 : bool = prim::GetAttr[name="training"](%6033)
       = prim::If(%6036) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6037 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6033)
          %6038 : Tensor = aten::add(%6037, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6033, %6038)
          -> ()
        block1():
          -> ()
      %6039 : bool = prim::GetAttr[name="training"](%6033)
      %6040 : Tensor = prim::GetAttr[name="running_mean"](%6033)
      %6041 : Tensor = prim::GetAttr[name="running_var"](%6033)
      %6042 : Tensor = prim::GetAttr[name="weight"](%6033)
      %6043 : Tensor = prim::GetAttr[name="bias"](%6033)
       = prim::If(%6039) # torch/nn/functional.py:2011:4
        block0():
          %6044 : int[] = aten::size(%concated_features.45) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%6044, %24) # torch/nn/functional.py:1991:17
          %6046 : int = aten::len(%6044) # torch/nn/functional.py:1992:19
          %6047 : int = aten::sub(%6046, %26) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%6047, %25, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %6051 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
              %6052 : int = aten::__getitem__(%6044, %6051) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %6052) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.355)
          %6054 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6054) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6055 : Tensor = aten::batch_norm(%concated_features.45, %6042, %6043, %6040, %6041, %6039, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.89 : Tensor = aten::relu_(%6055) # torch/nn/functional.py:1117:17
      %6057 : Tensor = prim::GetAttr[name="weight"](%6032)
      %6058 : Tensor? = prim::GetAttr[name="bias"](%6032)
      %6059 : int[] = prim::ListConstruct(%27, %27)
      %6060 : int[] = prim::ListConstruct(%24, %24)
      %6061 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.89 : Tensor = aten::conv2d(%result.89, %6057, %6058, %6059, %6060, %6061, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.89)
  %6063 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1919)
  %6064 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1919)
  %6065 : int = aten::dim(%bottleneck_output.88) # torch/nn/modules/batchnorm.py:276:11
  %6066 : bool = aten::ne(%6065, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6066) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6067 : bool = prim::GetAttr[name="training"](%6064)
   = prim::If(%6067) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6068 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6064)
      %6069 : Tensor = aten::add(%6068, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6064, %6069)
      -> ()
    block1():
      -> ()
  %6070 : bool = prim::GetAttr[name="training"](%6064)
  %6071 : Tensor = prim::GetAttr[name="running_mean"](%6064)
  %6072 : Tensor = prim::GetAttr[name="running_var"](%6064)
  %6073 : Tensor = prim::GetAttr[name="weight"](%6064)
  %6074 : Tensor = prim::GetAttr[name="bias"](%6064)
   = prim::If(%6070) # torch/nn/functional.py:2011:4
    block0():
      %6075 : int[] = aten::size(%bottleneck_output.88) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%6075, %24) # torch/nn/functional.py:1991:17
      %6077 : int = aten::len(%6075) # torch/nn/functional.py:1992:19
      %6078 : int = aten::sub(%6077, %26) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%6078, %25, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %6082 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
          %6083 : int = aten::__getitem__(%6075, %6082) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %6083) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.359)
      %6085 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6085) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6086 : Tensor = aten::batch_norm(%bottleneck_output.88, %6073, %6074, %6071, %6072, %6070, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.90 : Tensor = aten::relu_(%6086) # torch/nn/functional.py:1117:17
  %6088 : Tensor = prim::GetAttr[name="weight"](%6063)
  %6089 : Tensor? = prim::GetAttr[name="bias"](%6063)
  %6090 : int[] = prim::ListConstruct(%27, %27)
  %6091 : int[] = prim::ListConstruct(%27, %27)
  %6092 : int[] = prim::ListConstruct(%27, %27)
  %new_features.90 : Tensor = aten::conv2d(%result.90, %6088, %6089, %6090, %6091, %6092, %27) # torch/nn/modules/conv.py:415:15
  %6094 : float = prim::GetAttr[name="drop_rate"](%1919)
  %6095 : bool = aten::gt(%6094, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.87 : Tensor = prim::If(%6095) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6097 : float = prim::GetAttr[name="drop_rate"](%1919)
      %6098 : bool = prim::GetAttr[name="training"](%1919)
      %6099 : bool = aten::lt(%6097, %16) # torch/nn/functional.py:968:7
      %6100 : bool = prim::If(%6099) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6101 : bool = aten::gt(%6097, %17) # torch/nn/functional.py:968:17
          -> (%6101)
       = prim::If(%6100) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6102 : Tensor = aten::dropout(%new_features.90, %6097, %6098) # torch/nn/functional.py:973:17
      -> (%6102)
    block1():
      -> (%new_features.90)
  %6103 : Tensor[] = aten::append(%features.4, %new_features.87) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6104 : Tensor = prim::Uninitialized()
  %6105 : bool = prim::GetAttr[name="memory_efficient"](%1920)
  %6106 : bool = prim::If(%6105) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6107 : bool = prim::Uninitialized()
      %6108 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6109 : bool = aten::gt(%6108, %24)
      %6110 : bool, %6111 : bool, %6112 : int = prim::Loop(%18, %6109, %19, %6107, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6113 : int, %6114 : bool, %6115 : bool, %6116 : int):
          %tensor.46 : Tensor = aten::__getitem__(%features.4, %6116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6118 : bool = prim::requires_grad(%tensor.46)
          %6119 : bool, %6120 : bool = prim::If(%6118) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6107)
          %6121 : int = aten::add(%6116, %27)
          %6122 : bool = aten::lt(%6121, %6108)
          %6123 : bool = aten::__and__(%6122, %6119)
          -> (%6123, %6118, %6120, %6121)
      %6124 : bool = prim::If(%6110)
        block0():
          -> (%6111)
        block1():
          -> (%19)
      -> (%6124)
    block1():
      -> (%19)
  %bottleneck_output.90 : Tensor = prim::If(%6106) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6104)
    block1():
      %concated_features.46 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6127 : __torch__.torch.nn.modules.conv.___torch_mangle_359.Conv2d = prim::GetAttr[name="conv1"](%1920)
      %6128 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_355.BatchNorm2d = prim::GetAttr[name="norm1"](%1920)
      %6129 : int = aten::dim(%concated_features.46) # torch/nn/modules/batchnorm.py:276:11
      %6130 : bool = aten::ne(%6129, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6130) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6131 : bool = prim::GetAttr[name="training"](%6128)
       = prim::If(%6131) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6132 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6128)
          %6133 : Tensor = aten::add(%6132, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6128, %6133)
          -> ()
        block1():
          -> ()
      %6134 : bool = prim::GetAttr[name="training"](%6128)
      %6135 : Tensor = prim::GetAttr[name="running_mean"](%6128)
      %6136 : Tensor = prim::GetAttr[name="running_var"](%6128)
      %6137 : Tensor = prim::GetAttr[name="weight"](%6128)
      %6138 : Tensor = prim::GetAttr[name="bias"](%6128)
       = prim::If(%6134) # torch/nn/functional.py:2011:4
        block0():
          %6139 : int[] = aten::size(%concated_features.46) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%6139, %24) # torch/nn/functional.py:1991:17
          %6141 : int = aten::len(%6139) # torch/nn/functional.py:1992:19
          %6142 : int = aten::sub(%6141, %26) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%6142, %25, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %6146 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
              %6147 : int = aten::__getitem__(%6139, %6146) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %6147) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.363)
          %6149 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6149) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6150 : Tensor = aten::batch_norm(%concated_features.46, %6137, %6138, %6135, %6136, %6134, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.91 : Tensor = aten::relu_(%6150) # torch/nn/functional.py:1117:17
      %6152 : Tensor = prim::GetAttr[name="weight"](%6127)
      %6153 : Tensor? = prim::GetAttr[name="bias"](%6127)
      %6154 : int[] = prim::ListConstruct(%27, %27)
      %6155 : int[] = prim::ListConstruct(%24, %24)
      %6156 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.91 : Tensor = aten::conv2d(%result.91, %6152, %6153, %6154, %6155, %6156, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.91)
  %6158 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1920)
  %6159 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1920)
  %6160 : int = aten::dim(%bottleneck_output.90) # torch/nn/modules/batchnorm.py:276:11
  %6161 : bool = aten::ne(%6160, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6161) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6162 : bool = prim::GetAttr[name="training"](%6159)
   = prim::If(%6162) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6163 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6159)
      %6164 : Tensor = aten::add(%6163, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6159, %6164)
      -> ()
    block1():
      -> ()
  %6165 : bool = prim::GetAttr[name="training"](%6159)
  %6166 : Tensor = prim::GetAttr[name="running_mean"](%6159)
  %6167 : Tensor = prim::GetAttr[name="running_var"](%6159)
  %6168 : Tensor = prim::GetAttr[name="weight"](%6159)
  %6169 : Tensor = prim::GetAttr[name="bias"](%6159)
   = prim::If(%6165) # torch/nn/functional.py:2011:4
    block0():
      %6170 : int[] = aten::size(%bottleneck_output.90) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%6170, %24) # torch/nn/functional.py:1991:17
      %6172 : int = aten::len(%6170) # torch/nn/functional.py:1992:19
      %6173 : int = aten::sub(%6172, %26) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%6173, %25, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %6177 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
          %6178 : int = aten::__getitem__(%6170, %6177) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %6178) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.367)
      %6180 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6180) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6181 : Tensor = aten::batch_norm(%bottleneck_output.90, %6168, %6169, %6166, %6167, %6165, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.92 : Tensor = aten::relu_(%6181) # torch/nn/functional.py:1117:17
  %6183 : Tensor = prim::GetAttr[name="weight"](%6158)
  %6184 : Tensor? = prim::GetAttr[name="bias"](%6158)
  %6185 : int[] = prim::ListConstruct(%27, %27)
  %6186 : int[] = prim::ListConstruct(%27, %27)
  %6187 : int[] = prim::ListConstruct(%27, %27)
  %new_features.92 : Tensor = aten::conv2d(%result.92, %6183, %6184, %6185, %6186, %6187, %27) # torch/nn/modules/conv.py:415:15
  %6189 : float = prim::GetAttr[name="drop_rate"](%1920)
  %6190 : bool = aten::gt(%6189, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.89 : Tensor = prim::If(%6190) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6192 : float = prim::GetAttr[name="drop_rate"](%1920)
      %6193 : bool = prim::GetAttr[name="training"](%1920)
      %6194 : bool = aten::lt(%6192, %16) # torch/nn/functional.py:968:7
      %6195 : bool = prim::If(%6194) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6196 : bool = aten::gt(%6192, %17) # torch/nn/functional.py:968:17
          -> (%6196)
       = prim::If(%6195) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6197 : Tensor = aten::dropout(%new_features.92, %6192, %6193) # torch/nn/functional.py:973:17
      -> (%6197)
    block1():
      -> (%new_features.92)
  %6198 : Tensor[] = aten::append(%features.4, %new_features.89) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6199 : Tensor = prim::Uninitialized()
  %6200 : bool = prim::GetAttr[name="memory_efficient"](%1921)
  %6201 : bool = prim::If(%6200) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6202 : bool = prim::Uninitialized()
      %6203 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6204 : bool = aten::gt(%6203, %24)
      %6205 : bool, %6206 : bool, %6207 : int = prim::Loop(%18, %6204, %19, %6202, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6208 : int, %6209 : bool, %6210 : bool, %6211 : int):
          %tensor.47 : Tensor = aten::__getitem__(%features.4, %6211) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6213 : bool = prim::requires_grad(%tensor.47)
          %6214 : bool, %6215 : bool = prim::If(%6213) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6202)
          %6216 : int = aten::add(%6211, %27)
          %6217 : bool = aten::lt(%6216, %6203)
          %6218 : bool = aten::__and__(%6217, %6214)
          -> (%6218, %6213, %6215, %6216)
      %6219 : bool = prim::If(%6205)
        block0():
          -> (%6206)
        block1():
          -> (%19)
      -> (%6219)
    block1():
      -> (%19)
  %bottleneck_output.92 : Tensor = prim::If(%6201) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6199)
    block1():
      %concated_features.47 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6222 : __torch__.torch.nn.modules.conv.___torch_mangle_362.Conv2d = prim::GetAttr[name="conv1"](%1921)
      %6223 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_361.BatchNorm2d = prim::GetAttr[name="norm1"](%1921)
      %6224 : int = aten::dim(%concated_features.47) # torch/nn/modules/batchnorm.py:276:11
      %6225 : bool = aten::ne(%6224, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6225) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6226 : bool = prim::GetAttr[name="training"](%6223)
       = prim::If(%6226) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6227 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6223)
          %6228 : Tensor = aten::add(%6227, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6223, %6228)
          -> ()
        block1():
          -> ()
      %6229 : bool = prim::GetAttr[name="training"](%6223)
      %6230 : Tensor = prim::GetAttr[name="running_mean"](%6223)
      %6231 : Tensor = prim::GetAttr[name="running_var"](%6223)
      %6232 : Tensor = prim::GetAttr[name="weight"](%6223)
      %6233 : Tensor = prim::GetAttr[name="bias"](%6223)
       = prim::If(%6229) # torch/nn/functional.py:2011:4
        block0():
          %6234 : int[] = aten::size(%concated_features.47) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%6234, %24) # torch/nn/functional.py:1991:17
          %6236 : int = aten::len(%6234) # torch/nn/functional.py:1992:19
          %6237 : int = aten::sub(%6236, %26) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%6237, %25, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %6241 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
              %6242 : int = aten::__getitem__(%6234, %6241) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %6242) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.371)
          %6244 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6244) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6245 : Tensor = aten::batch_norm(%concated_features.47, %6232, %6233, %6230, %6231, %6229, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.93 : Tensor = aten::relu_(%6245) # torch/nn/functional.py:1117:17
      %6247 : Tensor = prim::GetAttr[name="weight"](%6222)
      %6248 : Tensor? = prim::GetAttr[name="bias"](%6222)
      %6249 : int[] = prim::ListConstruct(%27, %27)
      %6250 : int[] = prim::ListConstruct(%24, %24)
      %6251 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.93 : Tensor = aten::conv2d(%result.93, %6247, %6248, %6249, %6250, %6251, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.93)
  %6253 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1921)
  %6254 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1921)
  %6255 : int = aten::dim(%bottleneck_output.92) # torch/nn/modules/batchnorm.py:276:11
  %6256 : bool = aten::ne(%6255, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6256) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6257 : bool = prim::GetAttr[name="training"](%6254)
   = prim::If(%6257) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6258 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6254)
      %6259 : Tensor = aten::add(%6258, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6254, %6259)
      -> ()
    block1():
      -> ()
  %6260 : bool = prim::GetAttr[name="training"](%6254)
  %6261 : Tensor = prim::GetAttr[name="running_mean"](%6254)
  %6262 : Tensor = prim::GetAttr[name="running_var"](%6254)
  %6263 : Tensor = prim::GetAttr[name="weight"](%6254)
  %6264 : Tensor = prim::GetAttr[name="bias"](%6254)
   = prim::If(%6260) # torch/nn/functional.py:2011:4
    block0():
      %6265 : int[] = aten::size(%bottleneck_output.92) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%6265, %24) # torch/nn/functional.py:1991:17
      %6267 : int = aten::len(%6265) # torch/nn/functional.py:1992:19
      %6268 : int = aten::sub(%6267, %26) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%6268, %25, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %6272 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
          %6273 : int = aten::__getitem__(%6265, %6272) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %6273) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.375)
      %6275 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6275) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6276 : Tensor = aten::batch_norm(%bottleneck_output.92, %6263, %6264, %6261, %6262, %6260, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.94 : Tensor = aten::relu_(%6276) # torch/nn/functional.py:1117:17
  %6278 : Tensor = prim::GetAttr[name="weight"](%6253)
  %6279 : Tensor? = prim::GetAttr[name="bias"](%6253)
  %6280 : int[] = prim::ListConstruct(%27, %27)
  %6281 : int[] = prim::ListConstruct(%27, %27)
  %6282 : int[] = prim::ListConstruct(%27, %27)
  %new_features.94 : Tensor = aten::conv2d(%result.94, %6278, %6279, %6280, %6281, %6282, %27) # torch/nn/modules/conv.py:415:15
  %6284 : float = prim::GetAttr[name="drop_rate"](%1921)
  %6285 : bool = aten::gt(%6284, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.91 : Tensor = prim::If(%6285) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6287 : float = prim::GetAttr[name="drop_rate"](%1921)
      %6288 : bool = prim::GetAttr[name="training"](%1921)
      %6289 : bool = aten::lt(%6287, %16) # torch/nn/functional.py:968:7
      %6290 : bool = prim::If(%6289) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6291 : bool = aten::gt(%6287, %17) # torch/nn/functional.py:968:17
          -> (%6291)
       = prim::If(%6290) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6292 : Tensor = aten::dropout(%new_features.94, %6287, %6288) # torch/nn/functional.py:973:17
      -> (%6292)
    block1():
      -> (%new_features.94)
  %6293 : Tensor[] = aten::append(%features.4, %new_features.91) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6294 : Tensor = prim::Uninitialized()
  %6295 : bool = prim::GetAttr[name="memory_efficient"](%1922)
  %6296 : bool = prim::If(%6295) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6297 : bool = prim::Uninitialized()
      %6298 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6299 : bool = aten::gt(%6298, %24)
      %6300 : bool, %6301 : bool, %6302 : int = prim::Loop(%18, %6299, %19, %6297, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6303 : int, %6304 : bool, %6305 : bool, %6306 : int):
          %tensor.48 : Tensor = aten::__getitem__(%features.4, %6306) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6308 : bool = prim::requires_grad(%tensor.48)
          %6309 : bool, %6310 : bool = prim::If(%6308) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6297)
          %6311 : int = aten::add(%6306, %27)
          %6312 : bool = aten::lt(%6311, %6298)
          %6313 : bool = aten::__and__(%6312, %6309)
          -> (%6313, %6308, %6310, %6311)
      %6314 : bool = prim::If(%6300)
        block0():
          -> (%6301)
        block1():
          -> (%19)
      -> (%6314)
    block1():
      -> (%19)
  %bottleneck_output.94 : Tensor = prim::If(%6296) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6294)
    block1():
      %concated_features.48 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6317 : __torch__.torch.nn.modules.conv.___torch_mangle_364.Conv2d = prim::GetAttr[name="conv1"](%1922)
      %6318 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_261.BatchNorm2d = prim::GetAttr[name="norm1"](%1922)
      %6319 : int = aten::dim(%concated_features.48) # torch/nn/modules/batchnorm.py:276:11
      %6320 : bool = aten::ne(%6319, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6320) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6321 : bool = prim::GetAttr[name="training"](%6318)
       = prim::If(%6321) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6322 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6318)
          %6323 : Tensor = aten::add(%6322, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6318, %6323)
          -> ()
        block1():
          -> ()
      %6324 : bool = prim::GetAttr[name="training"](%6318)
      %6325 : Tensor = prim::GetAttr[name="running_mean"](%6318)
      %6326 : Tensor = prim::GetAttr[name="running_var"](%6318)
      %6327 : Tensor = prim::GetAttr[name="weight"](%6318)
      %6328 : Tensor = prim::GetAttr[name="bias"](%6318)
       = prim::If(%6324) # torch/nn/functional.py:2011:4
        block0():
          %6329 : int[] = aten::size(%concated_features.48) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%6329, %24) # torch/nn/functional.py:1991:17
          %6331 : int = aten::len(%6329) # torch/nn/functional.py:1992:19
          %6332 : int = aten::sub(%6331, %26) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%6332, %25, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %6336 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
              %6337 : int = aten::__getitem__(%6329, %6336) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %6337) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.379)
          %6339 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6339) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6340 : Tensor = aten::batch_norm(%concated_features.48, %6327, %6328, %6325, %6326, %6324, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.95 : Tensor = aten::relu_(%6340) # torch/nn/functional.py:1117:17
      %6342 : Tensor = prim::GetAttr[name="weight"](%6317)
      %6343 : Tensor? = prim::GetAttr[name="bias"](%6317)
      %6344 : int[] = prim::ListConstruct(%27, %27)
      %6345 : int[] = prim::ListConstruct(%24, %24)
      %6346 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.95 : Tensor = aten::conv2d(%result.95, %6342, %6343, %6344, %6345, %6346, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.95)
  %6348 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1922)
  %6349 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1922)
  %6350 : int = aten::dim(%bottleneck_output.94) # torch/nn/modules/batchnorm.py:276:11
  %6351 : bool = aten::ne(%6350, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6351) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6352 : bool = prim::GetAttr[name="training"](%6349)
   = prim::If(%6352) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6353 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6349)
      %6354 : Tensor = aten::add(%6353, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6349, %6354)
      -> ()
    block1():
      -> ()
  %6355 : bool = prim::GetAttr[name="training"](%6349)
  %6356 : Tensor = prim::GetAttr[name="running_mean"](%6349)
  %6357 : Tensor = prim::GetAttr[name="running_var"](%6349)
  %6358 : Tensor = prim::GetAttr[name="weight"](%6349)
  %6359 : Tensor = prim::GetAttr[name="bias"](%6349)
   = prim::If(%6355) # torch/nn/functional.py:2011:4
    block0():
      %6360 : int[] = aten::size(%bottleneck_output.94) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%6360, %24) # torch/nn/functional.py:1991:17
      %6362 : int = aten::len(%6360) # torch/nn/functional.py:1992:19
      %6363 : int = aten::sub(%6362, %26) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%6363, %25, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %6367 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
          %6368 : int = aten::__getitem__(%6360, %6367) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %6368) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.383)
      %6370 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6370) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6371 : Tensor = aten::batch_norm(%bottleneck_output.94, %6358, %6359, %6356, %6357, %6355, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.96 : Tensor = aten::relu_(%6371) # torch/nn/functional.py:1117:17
  %6373 : Tensor = prim::GetAttr[name="weight"](%6348)
  %6374 : Tensor? = prim::GetAttr[name="bias"](%6348)
  %6375 : int[] = prim::ListConstruct(%27, %27)
  %6376 : int[] = prim::ListConstruct(%27, %27)
  %6377 : int[] = prim::ListConstruct(%27, %27)
  %new_features.96 : Tensor = aten::conv2d(%result.96, %6373, %6374, %6375, %6376, %6377, %27) # torch/nn/modules/conv.py:415:15
  %6379 : float = prim::GetAttr[name="drop_rate"](%1922)
  %6380 : bool = aten::gt(%6379, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.93 : Tensor = prim::If(%6380) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6382 : float = prim::GetAttr[name="drop_rate"](%1922)
      %6383 : bool = prim::GetAttr[name="training"](%1922)
      %6384 : bool = aten::lt(%6382, %16) # torch/nn/functional.py:968:7
      %6385 : bool = prim::If(%6384) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6386 : bool = aten::gt(%6382, %17) # torch/nn/functional.py:968:17
          -> (%6386)
       = prim::If(%6385) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6387 : Tensor = aten::dropout(%new_features.96, %6382, %6383) # torch/nn/functional.py:973:17
      -> (%6387)
    block1():
      -> (%new_features.96)
  %6388 : Tensor[] = aten::append(%features.4, %new_features.93) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6389 : Tensor = prim::Uninitialized()
  %6390 : bool = prim::GetAttr[name="memory_efficient"](%1923)
  %6391 : bool = prim::If(%6390) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6392 : bool = prim::Uninitialized()
      %6393 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6394 : bool = aten::gt(%6393, %24)
      %6395 : bool, %6396 : bool, %6397 : int = prim::Loop(%18, %6394, %19, %6392, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6398 : int, %6399 : bool, %6400 : bool, %6401 : int):
          %tensor.98 : Tensor = aten::__getitem__(%features.4, %6401) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6403 : bool = prim::requires_grad(%tensor.98)
          %6404 : bool, %6405 : bool = prim::If(%6403) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6392)
          %6406 : int = aten::add(%6401, %27)
          %6407 : bool = aten::lt(%6406, %6393)
          %6408 : bool = aten::__and__(%6407, %6404)
          -> (%6408, %6403, %6405, %6406)
      %6409 : bool = prim::If(%6395)
        block0():
          -> (%6396)
        block1():
          -> (%19)
      -> (%6409)
    block1():
      -> (%19)
  %bottleneck_output.194 : Tensor = prim::If(%6391) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6389)
    block1():
      %concated_features.98 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6412 : __torch__.torch.nn.modules.conv.___torch_mangle_367.Conv2d = prim::GetAttr[name="conv1"](%1923)
      %6413 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_366.BatchNorm2d = prim::GetAttr[name="norm1"](%1923)
      %6414 : int = aten::dim(%concated_features.98) # torch/nn/modules/batchnorm.py:276:11
      %6415 : bool = aten::ne(%6414, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6415) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6416 : bool = prim::GetAttr[name="training"](%6413)
       = prim::If(%6416) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6417 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6413)
          %6418 : Tensor = aten::add(%6417, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6413, %6418)
          -> ()
        block1():
          -> ()
      %6419 : bool = prim::GetAttr[name="training"](%6413)
      %6420 : Tensor = prim::GetAttr[name="running_mean"](%6413)
      %6421 : Tensor = prim::GetAttr[name="running_var"](%6413)
      %6422 : Tensor = prim::GetAttr[name="weight"](%6413)
      %6423 : Tensor = prim::GetAttr[name="bias"](%6413)
       = prim::If(%6419) # torch/nn/functional.py:2011:4
        block0():
          %6424 : int[] = aten::size(%concated_features.98) # torch/nn/functional.py:2012:27
          %size_prods.792 : int = aten::__getitem__(%6424, %24) # torch/nn/functional.py:1991:17
          %6426 : int = aten::len(%6424) # torch/nn/functional.py:1992:19
          %6427 : int = aten::sub(%6426, %26) # torch/nn/functional.py:1992:19
          %size_prods.793 : int = prim::Loop(%6427, %25, %size_prods.792) # torch/nn/functional.py:1992:4
            block0(%i.199 : int, %size_prods.794 : int):
              %6431 : int = aten::add(%i.199, %26) # torch/nn/functional.py:1993:27
              %6432 : int = aten::__getitem__(%6424, %6431) # torch/nn/functional.py:1993:22
              %size_prods.795 : int = aten::mul(%size_prods.794, %6432) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.795)
          %6434 : bool = aten::eq(%size_prods.793, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6434) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6435 : Tensor = aten::batch_norm(%concated_features.98, %6422, %6423, %6420, %6421, %6419, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.195 : Tensor = aten::relu_(%6435) # torch/nn/functional.py:1117:17
      %6437 : Tensor = prim::GetAttr[name="weight"](%6412)
      %6438 : Tensor? = prim::GetAttr[name="bias"](%6412)
      %6439 : int[] = prim::ListConstruct(%27, %27)
      %6440 : int[] = prim::ListConstruct(%24, %24)
      %6441 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.195 : Tensor = aten::conv2d(%result.195, %6437, %6438, %6439, %6440, %6441, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.195)
  %6443 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1923)
  %6444 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1923)
  %6445 : int = aten::dim(%bottleneck_output.194) # torch/nn/modules/batchnorm.py:276:11
  %6446 : bool = aten::ne(%6445, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6446) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6447 : bool = prim::GetAttr[name="training"](%6444)
   = prim::If(%6447) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6448 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6444)
      %6449 : Tensor = aten::add(%6448, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6444, %6449)
      -> ()
    block1():
      -> ()
  %6450 : bool = prim::GetAttr[name="training"](%6444)
  %6451 : Tensor = prim::GetAttr[name="running_mean"](%6444)
  %6452 : Tensor = prim::GetAttr[name="running_var"](%6444)
  %6453 : Tensor = prim::GetAttr[name="weight"](%6444)
  %6454 : Tensor = prim::GetAttr[name="bias"](%6444)
   = prim::If(%6450) # torch/nn/functional.py:2011:4
    block0():
      %6455 : int[] = aten::size(%bottleneck_output.194) # torch/nn/functional.py:2012:27
      %size_prods.540 : int = aten::__getitem__(%6455, %24) # torch/nn/functional.py:1991:17
      %6457 : int = aten::len(%6455) # torch/nn/functional.py:1992:19
      %6458 : int = aten::sub(%6457, %26) # torch/nn/functional.py:1992:19
      %size_prods.541 : int = prim::Loop(%6458, %25, %size_prods.540) # torch/nn/functional.py:1992:4
        block0(%i.136 : int, %size_prods.542 : int):
          %6462 : int = aten::add(%i.136, %26) # torch/nn/functional.py:1993:27
          %6463 : int = aten::__getitem__(%6455, %6462) # torch/nn/functional.py:1993:22
          %size_prods.543 : int = aten::mul(%size_prods.542, %6463) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.543)
      %6465 : bool = aten::eq(%size_prods.541, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6465) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6466 : Tensor = aten::batch_norm(%bottleneck_output.194, %6453, %6454, %6451, %6452, %6450, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.196 : Tensor = aten::relu_(%6466) # torch/nn/functional.py:1117:17
  %6468 : Tensor = prim::GetAttr[name="weight"](%6443)
  %6469 : Tensor? = prim::GetAttr[name="bias"](%6443)
  %6470 : int[] = prim::ListConstruct(%27, %27)
  %6471 : int[] = prim::ListConstruct(%27, %27)
  %6472 : int[] = prim::ListConstruct(%27, %27)
  %new_features.195 : Tensor = aten::conv2d(%result.196, %6468, %6469, %6470, %6471, %6472, %27) # torch/nn/modules/conv.py:415:15
  %6474 : float = prim::GetAttr[name="drop_rate"](%1923)
  %6475 : bool = aten::gt(%6474, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.95 : Tensor = prim::If(%6475) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6477 : float = prim::GetAttr[name="drop_rate"](%1923)
      %6478 : bool = prim::GetAttr[name="training"](%1923)
      %6479 : bool = aten::lt(%6477, %16) # torch/nn/functional.py:968:7
      %6480 : bool = prim::If(%6479) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6481 : bool = aten::gt(%6477, %17) # torch/nn/functional.py:968:17
          -> (%6481)
       = prim::If(%6480) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6482 : Tensor = aten::dropout(%new_features.195, %6477, %6478) # torch/nn/functional.py:973:17
      -> (%6482)
    block1():
      -> (%new_features.195)
  %6483 : Tensor[] = aten::append(%features.4, %new_features.95) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.19 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %6485 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_370.BatchNorm2d = prim::GetAttr[name="norm"](%36)
  %6486 : __torch__.torch.nn.modules.conv.___torch_mangle_371.Conv2d = prim::GetAttr[name="conv"](%36)
  %6487 : int = aten::dim(%input.19) # torch/nn/modules/batchnorm.py:276:11
  %6488 : bool = aten::ne(%6487, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6488) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6489 : bool = prim::GetAttr[name="training"](%6485)
   = prim::If(%6489) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6490 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6485)
      %6491 : Tensor = aten::add(%6490, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6485, %6491)
      -> ()
    block1():
      -> ()
  %6492 : bool = prim::GetAttr[name="training"](%6485)
  %6493 : Tensor = prim::GetAttr[name="running_mean"](%6485)
  %6494 : Tensor = prim::GetAttr[name="running_var"](%6485)
  %6495 : Tensor = prim::GetAttr[name="weight"](%6485)
  %6496 : Tensor = prim::GetAttr[name="bias"](%6485)
   = prim::If(%6492) # torch/nn/functional.py:2011:4
    block0():
      %6497 : int[] = aten::size(%input.19) # torch/nn/functional.py:2012:27
      %size_prods.796 : int = aten::__getitem__(%6497, %24) # torch/nn/functional.py:1991:17
      %6499 : int = aten::len(%6497) # torch/nn/functional.py:1992:19
      %6500 : int = aten::sub(%6499, %26) # torch/nn/functional.py:1992:19
      %size_prods.797 : int = prim::Loop(%6500, %25, %size_prods.796) # torch/nn/functional.py:1992:4
        block0(%i.200 : int, %size_prods.798 : int):
          %6504 : int = aten::add(%i.200, %26) # torch/nn/functional.py:1993:27
          %6505 : int = aten::__getitem__(%6497, %6504) # torch/nn/functional.py:1993:22
          %size_prods.799 : int = aten::mul(%size_prods.798, %6505) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.799)
      %6507 : bool = aten::eq(%size_prods.797, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6507) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.3 : Tensor = aten::batch_norm(%input.19, %6495, %6496, %6493, %6494, %6492, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.5 : Tensor = aten::relu_(%input.3) # torch/nn/functional.py:1117:17
  %6510 : Tensor = prim::GetAttr[name="weight"](%6486)
  %6511 : Tensor? = prim::GetAttr[name="bias"](%6486)
  %6512 : int[] = prim::ListConstruct(%27, %27)
  %6513 : int[] = prim::ListConstruct(%24, %24)
  %6514 : int[] = prim::ListConstruct(%27, %27)
  %input.7 : Tensor = aten::conv2d(%input.5, %6510, %6511, %6512, %6513, %6514, %27) # torch/nn/modules/conv.py:415:15
  %6516 : int[] = prim::ListConstruct(%26, %26)
  %6517 : int[] = prim::ListConstruct(%26, %26)
  %6518 : int[] = prim::ListConstruct(%24, %24)
  %input.21 : Tensor = aten::avg_pool2d(%input.7, %6516, %6517, %6518, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.1 : Tensor[] = prim::ListConstruct(%input.21)
  %6521 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer1"](%37)
  %6522 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer2"](%37)
  %6523 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer3"](%37)
  %6524 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer4"](%37)
  %6525 : __torch__.torchvision.models.densenet.___torch_mangle_300._DenseLayer = prim::GetAttr[name="denselayer5"](%37)
  %6526 : __torch__.torchvision.models.densenet.___torch_mangle_302._DenseLayer = prim::GetAttr[name="denselayer6"](%37)
  %6527 : __torch__.torchvision.models.densenet.___torch_mangle_305._DenseLayer = prim::GetAttr[name="denselayer7"](%37)
  %6528 : __torch__.torchvision.models.densenet.___torch_mangle_308._DenseLayer = prim::GetAttr[name="denselayer8"](%37)
  %6529 : __torch__.torchvision.models.densenet.___torch_mangle_310._DenseLayer = prim::GetAttr[name="denselayer9"](%37)
  %6530 : __torch__.torchvision.models.densenet.___torch_mangle_313._DenseLayer = prim::GetAttr[name="denselayer10"](%37)
  %6531 : __torch__.torchvision.models.densenet.___torch_mangle_316._DenseLayer = prim::GetAttr[name="denselayer11"](%37)
  %6532 : __torch__.torchvision.models.densenet.___torch_mangle_318._DenseLayer = prim::GetAttr[name="denselayer12"](%37)
  %6533 : __torch__.torchvision.models.densenet.___torch_mangle_324._DenseLayer = prim::GetAttr[name="denselayer13"](%37)
  %6534 : __torch__.torchvision.models.densenet.___torch_mangle_327._DenseLayer = prim::GetAttr[name="denselayer14"](%37)
  %6535 : __torch__.torchvision.models.densenet.___torch_mangle_329._DenseLayer = prim::GetAttr[name="denselayer15"](%37)
  %6536 : __torch__.torchvision.models.densenet.___torch_mangle_332._DenseLayer = prim::GetAttr[name="denselayer16"](%37)
  %6537 : __torch__.torchvision.models.densenet.___torch_mangle_335._DenseLayer = prim::GetAttr[name="denselayer17"](%37)
  %6538 : __torch__.torchvision.models.densenet.___torch_mangle_337._DenseLayer = prim::GetAttr[name="denselayer18"](%37)
  %6539 : __torch__.torchvision.models.densenet.___torch_mangle_340._DenseLayer = prim::GetAttr[name="denselayer19"](%37)
  %6540 : __torch__.torchvision.models.densenet.___torch_mangle_343._DenseLayer = prim::GetAttr[name="denselayer20"](%37)
  %6541 : __torch__.torchvision.models.densenet.___torch_mangle_345._DenseLayer = prim::GetAttr[name="denselayer21"](%37)
  %6542 : __torch__.torchvision.models.densenet.___torch_mangle_348._DenseLayer = prim::GetAttr[name="denselayer22"](%37)
  %6543 : __torch__.torchvision.models.densenet.___torch_mangle_351._DenseLayer = prim::GetAttr[name="denselayer23"](%37)
  %6544 : __torch__.torchvision.models.densenet.___torch_mangle_353._DenseLayer = prim::GetAttr[name="denselayer24"](%37)
  %6545 : __torch__.torchvision.models.densenet.___torch_mangle_360._DenseLayer = prim::GetAttr[name="denselayer25"](%37)
  %6546 : __torch__.torchvision.models.densenet.___torch_mangle_363._DenseLayer = prim::GetAttr[name="denselayer26"](%37)
  %6547 : __torch__.torchvision.models.densenet.___torch_mangle_365._DenseLayer = prim::GetAttr[name="denselayer27"](%37)
  %6548 : __torch__.torchvision.models.densenet.___torch_mangle_368._DenseLayer = prim::GetAttr[name="denselayer28"](%37)
  %6549 : __torch__.torchvision.models.densenet.___torch_mangle_374._DenseLayer = prim::GetAttr[name="denselayer29"](%37)
  %6550 : __torch__.torchvision.models.densenet.___torch_mangle_376._DenseLayer = prim::GetAttr[name="denselayer30"](%37)
  %6551 : __torch__.torchvision.models.densenet.___torch_mangle_379._DenseLayer = prim::GetAttr[name="denselayer31"](%37)
  %6552 : __torch__.torchvision.models.densenet.___torch_mangle_382._DenseLayer = prim::GetAttr[name="denselayer32"](%37)
  %6553 : Tensor = prim::Uninitialized()
  %6554 : bool = prim::GetAttr[name="memory_efficient"](%6521)
  %6555 : bool = prim::If(%6554) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6556 : bool = prim::Uninitialized()
      %6557 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6558 : bool = aten::gt(%6557, %24)
      %6559 : bool, %6560 : bool, %6561 : int = prim::Loop(%18, %6558, %19, %6556, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6562 : int, %6563 : bool, %6564 : bool, %6565 : int):
          %tensor.2 : Tensor = aten::__getitem__(%features.1, %6565) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6567 : bool = prim::requires_grad(%tensor.2)
          %6568 : bool, %6569 : bool = prim::If(%6567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6556)
          %6570 : int = aten::add(%6565, %27)
          %6571 : bool = aten::lt(%6570, %6557)
          %6572 : bool = aten::__and__(%6571, %6568)
          -> (%6572, %6567, %6569, %6570)
      %6573 : bool = prim::If(%6559)
        block0():
          -> (%6560)
        block1():
          -> (%19)
      -> (%6573)
    block1():
      -> (%19)
  %bottleneck_output.1 : Tensor = prim::If(%6555) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6553)
    block1():
      %concated_features.2 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6576 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%6521)
      %6577 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%6521)
      %6578 : int = aten::dim(%concated_features.2) # torch/nn/modules/batchnorm.py:276:11
      %6579 : bool = aten::ne(%6578, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6579) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6580 : bool = prim::GetAttr[name="training"](%6577)
       = prim::If(%6580) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6581 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6577)
          %6582 : Tensor = aten::add(%6581, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6577, %6582)
          -> ()
        block1():
          -> ()
      %6583 : bool = prim::GetAttr[name="training"](%6577)
      %6584 : Tensor = prim::GetAttr[name="running_mean"](%6577)
      %6585 : Tensor = prim::GetAttr[name="running_var"](%6577)
      %6586 : Tensor = prim::GetAttr[name="weight"](%6577)
      %6587 : Tensor = prim::GetAttr[name="bias"](%6577)
       = prim::If(%6583) # torch/nn/functional.py:2011:4
        block0():
          %6588 : int[] = aten::size(%concated_features.2) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%6588, %24) # torch/nn/functional.py:1991:17
          %6590 : int = aten::len(%6588) # torch/nn/functional.py:1992:19
          %6591 : int = aten::sub(%6590, %26) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%6591, %25, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %6595 : int = aten::add(%i.3, %26) # torch/nn/functional.py:1993:27
              %6596 : int = aten::__getitem__(%6588, %6595) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %6596) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.11)
          %6598 : bool = aten::eq(%size_prods.9, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6598) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6599 : Tensor = aten::batch_norm(%concated_features.2, %6586, %6587, %6584, %6585, %6583, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.3 : Tensor = aten::relu_(%6599) # torch/nn/functional.py:1117:17
      %6601 : Tensor = prim::GetAttr[name="weight"](%6576)
      %6602 : Tensor? = prim::GetAttr[name="bias"](%6576)
      %6603 : int[] = prim::ListConstruct(%27, %27)
      %6604 : int[] = prim::ListConstruct(%24, %24)
      %6605 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.3 : Tensor = aten::conv2d(%result.3, %6601, %6602, %6603, %6604, %6605, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.3)
  %6607 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6521)
  %6608 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6521)
  %6609 : int = aten::dim(%bottleneck_output.1) # torch/nn/modules/batchnorm.py:276:11
  %6610 : bool = aten::ne(%6609, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6610) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6611 : bool = prim::GetAttr[name="training"](%6608)
   = prim::If(%6611) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6612 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6608)
      %6613 : Tensor = aten::add(%6612, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6608, %6613)
      -> ()
    block1():
      -> ()
  %6614 : bool = prim::GetAttr[name="training"](%6608)
  %6615 : Tensor = prim::GetAttr[name="running_mean"](%6608)
  %6616 : Tensor = prim::GetAttr[name="running_var"](%6608)
  %6617 : Tensor = prim::GetAttr[name="weight"](%6608)
  %6618 : Tensor = prim::GetAttr[name="bias"](%6608)
   = prim::If(%6614) # torch/nn/functional.py:2011:4
    block0():
      %6619 : int[] = aten::size(%bottleneck_output.1) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%6619, %24) # torch/nn/functional.py:1991:17
      %6621 : int = aten::len(%6619) # torch/nn/functional.py:1992:19
      %6622 : int = aten::sub(%6621, %26) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%6622, %25, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %6626 : int = aten::add(%i.4, %26) # torch/nn/functional.py:1993:27
          %6627 : int = aten::__getitem__(%6619, %6626) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %6627) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.15)
      %6629 : bool = aten::eq(%size_prods.13, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6629) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6630 : Tensor = aten::batch_norm(%bottleneck_output.1, %6617, %6618, %6615, %6616, %6614, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.4 : Tensor = aten::relu_(%6630) # torch/nn/functional.py:1117:17
  %6632 : Tensor = prim::GetAttr[name="weight"](%6607)
  %6633 : Tensor? = prim::GetAttr[name="bias"](%6607)
  %6634 : int[] = prim::ListConstruct(%27, %27)
  %6635 : int[] = prim::ListConstruct(%27, %27)
  %6636 : int[] = prim::ListConstruct(%27, %27)
  %new_features.4 : Tensor = aten::conv2d(%result.4, %6632, %6633, %6634, %6635, %6636, %27) # torch/nn/modules/conv.py:415:15
  %6638 : float = prim::GetAttr[name="drop_rate"](%6521)
  %6639 : bool = aten::gt(%6638, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.2 : Tensor = prim::If(%6639) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6641 : float = prim::GetAttr[name="drop_rate"](%6521)
      %6642 : bool = prim::GetAttr[name="training"](%6521)
      %6643 : bool = aten::lt(%6641, %16) # torch/nn/functional.py:968:7
      %6644 : bool = prim::If(%6643) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6645 : bool = aten::gt(%6641, %17) # torch/nn/functional.py:968:17
          -> (%6645)
       = prim::If(%6644) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6646 : Tensor = aten::dropout(%new_features.4, %6641, %6642) # torch/nn/functional.py:973:17
      -> (%6646)
    block1():
      -> (%new_features.4)
  %6647 : Tensor[] = aten::append(%features.1, %new_features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6648 : Tensor = prim::Uninitialized()
  %6649 : bool = prim::GetAttr[name="memory_efficient"](%6522)
  %6650 : bool = prim::If(%6649) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6651 : bool = prim::Uninitialized()
      %6652 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6653 : bool = aten::gt(%6652, %24)
      %6654 : bool, %6655 : bool, %6656 : int = prim::Loop(%18, %6653, %19, %6651, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6657 : int, %6658 : bool, %6659 : bool, %6660 : int):
          %tensor.3 : Tensor = aten::__getitem__(%features.1, %6660) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6662 : bool = prim::requires_grad(%tensor.3)
          %6663 : bool, %6664 : bool = prim::If(%6662) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6651)
          %6665 : int = aten::add(%6660, %27)
          %6666 : bool = aten::lt(%6665, %6652)
          %6667 : bool = aten::__and__(%6666, %6663)
          -> (%6667, %6662, %6664, %6665)
      %6668 : bool = prim::If(%6654)
        block0():
          -> (%6655)
        block1():
          -> (%19)
      -> (%6668)
    block1():
      -> (%19)
  %bottleneck_output.4 : Tensor = prim::If(%6650) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6648)
    block1():
      %concated_features.3 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6671 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%6522)
      %6672 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%6522)
      %6673 : int = aten::dim(%concated_features.3) # torch/nn/modules/batchnorm.py:276:11
      %6674 : bool = aten::ne(%6673, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6674) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6675 : bool = prim::GetAttr[name="training"](%6672)
       = prim::If(%6675) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6676 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6672)
          %6677 : Tensor = aten::add(%6676, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6672, %6677)
          -> ()
        block1():
          -> ()
      %6678 : bool = prim::GetAttr[name="training"](%6672)
      %6679 : Tensor = prim::GetAttr[name="running_mean"](%6672)
      %6680 : Tensor = prim::GetAttr[name="running_var"](%6672)
      %6681 : Tensor = prim::GetAttr[name="weight"](%6672)
      %6682 : Tensor = prim::GetAttr[name="bias"](%6672)
       = prim::If(%6678) # torch/nn/functional.py:2011:4
        block0():
          %6683 : int[] = aten::size(%concated_features.3) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%6683, %24) # torch/nn/functional.py:1991:17
          %6685 : int = aten::len(%6683) # torch/nn/functional.py:1992:19
          %6686 : int = aten::sub(%6685, %26) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%6686, %25, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %6690 : int = aten::add(%i.5, %26) # torch/nn/functional.py:1993:27
              %6691 : int = aten::__getitem__(%6683, %6690) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %6691) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.19)
          %6693 : bool = aten::eq(%size_prods.17, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6693) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6694 : Tensor = aten::batch_norm(%concated_features.3, %6681, %6682, %6679, %6680, %6678, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.5 : Tensor = aten::relu_(%6694) # torch/nn/functional.py:1117:17
      %6696 : Tensor = prim::GetAttr[name="weight"](%6671)
      %6697 : Tensor? = prim::GetAttr[name="bias"](%6671)
      %6698 : int[] = prim::ListConstruct(%27, %27)
      %6699 : int[] = prim::ListConstruct(%24, %24)
      %6700 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.5 : Tensor = aten::conv2d(%result.5, %6696, %6697, %6698, %6699, %6700, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.5)
  %6702 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6522)
  %6703 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6522)
  %6704 : int = aten::dim(%bottleneck_output.4) # torch/nn/modules/batchnorm.py:276:11
  %6705 : bool = aten::ne(%6704, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6705) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6706 : bool = prim::GetAttr[name="training"](%6703)
   = prim::If(%6706) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6707 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6703)
      %6708 : Tensor = aten::add(%6707, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6703, %6708)
      -> ()
    block1():
      -> ()
  %6709 : bool = prim::GetAttr[name="training"](%6703)
  %6710 : Tensor = prim::GetAttr[name="running_mean"](%6703)
  %6711 : Tensor = prim::GetAttr[name="running_var"](%6703)
  %6712 : Tensor = prim::GetAttr[name="weight"](%6703)
  %6713 : Tensor = prim::GetAttr[name="bias"](%6703)
   = prim::If(%6709) # torch/nn/functional.py:2011:4
    block0():
      %6714 : int[] = aten::size(%bottleneck_output.4) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%6714, %24) # torch/nn/functional.py:1991:17
      %6716 : int = aten::len(%6714) # torch/nn/functional.py:1992:19
      %6717 : int = aten::sub(%6716, %26) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%6717, %25, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %6721 : int = aten::add(%i.6, %26) # torch/nn/functional.py:1993:27
          %6722 : int = aten::__getitem__(%6714, %6721) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %6722) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.23)
      %6724 : bool = aten::eq(%size_prods.21, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6724) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6725 : Tensor = aten::batch_norm(%bottleneck_output.4, %6712, %6713, %6710, %6711, %6709, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.6 : Tensor = aten::relu_(%6725) # torch/nn/functional.py:1117:17
  %6727 : Tensor = prim::GetAttr[name="weight"](%6702)
  %6728 : Tensor? = prim::GetAttr[name="bias"](%6702)
  %6729 : int[] = prim::ListConstruct(%27, %27)
  %6730 : int[] = prim::ListConstruct(%27, %27)
  %6731 : int[] = prim::ListConstruct(%27, %27)
  %new_features.6 : Tensor = aten::conv2d(%result.6, %6727, %6728, %6729, %6730, %6731, %27) # torch/nn/modules/conv.py:415:15
  %6733 : float = prim::GetAttr[name="drop_rate"](%6522)
  %6734 : bool = aten::gt(%6733, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.3 : Tensor = prim::If(%6734) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6736 : float = prim::GetAttr[name="drop_rate"](%6522)
      %6737 : bool = prim::GetAttr[name="training"](%6522)
      %6738 : bool = aten::lt(%6736, %16) # torch/nn/functional.py:968:7
      %6739 : bool = prim::If(%6738) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6740 : bool = aten::gt(%6736, %17) # torch/nn/functional.py:968:17
          -> (%6740)
       = prim::If(%6739) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6741 : Tensor = aten::dropout(%new_features.6, %6736, %6737) # torch/nn/functional.py:973:17
      -> (%6741)
    block1():
      -> (%new_features.6)
  %6742 : Tensor[] = aten::append(%features.1, %new_features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6743 : Tensor = prim::Uninitialized()
  %6744 : bool = prim::GetAttr[name="memory_efficient"](%6523)
  %6745 : bool = prim::If(%6744) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6746 : bool = prim::Uninitialized()
      %6747 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6748 : bool = aten::gt(%6747, %24)
      %6749 : bool, %6750 : bool, %6751 : int = prim::Loop(%18, %6748, %19, %6746, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6752 : int, %6753 : bool, %6754 : bool, %6755 : int):
          %tensor.4 : Tensor = aten::__getitem__(%features.1, %6755) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6757 : bool = prim::requires_grad(%tensor.4)
          %6758 : bool, %6759 : bool = prim::If(%6757) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6746)
          %6760 : int = aten::add(%6755, %27)
          %6761 : bool = aten::lt(%6760, %6747)
          %6762 : bool = aten::__and__(%6761, %6758)
          -> (%6762, %6757, %6759, %6760)
      %6763 : bool = prim::If(%6749)
        block0():
          -> (%6750)
        block1():
          -> (%19)
      -> (%6763)
    block1():
      -> (%19)
  %bottleneck_output.6 : Tensor = prim::If(%6745) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6743)
    block1():
      %concated_features.4 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6766 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%6523)
      %6767 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%6523)
      %6768 : int = aten::dim(%concated_features.4) # torch/nn/modules/batchnorm.py:276:11
      %6769 : bool = aten::ne(%6768, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6769) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6770 : bool = prim::GetAttr[name="training"](%6767)
       = prim::If(%6770) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6771 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6767)
          %6772 : Tensor = aten::add(%6771, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6767, %6772)
          -> ()
        block1():
          -> ()
      %6773 : bool = prim::GetAttr[name="training"](%6767)
      %6774 : Tensor = prim::GetAttr[name="running_mean"](%6767)
      %6775 : Tensor = prim::GetAttr[name="running_var"](%6767)
      %6776 : Tensor = prim::GetAttr[name="weight"](%6767)
      %6777 : Tensor = prim::GetAttr[name="bias"](%6767)
       = prim::If(%6773) # torch/nn/functional.py:2011:4
        block0():
          %6778 : int[] = aten::size(%concated_features.4) # torch/nn/functional.py:2012:27
          %size_prods.24 : int = aten::__getitem__(%6778, %24) # torch/nn/functional.py:1991:17
          %6780 : int = aten::len(%6778) # torch/nn/functional.py:1992:19
          %6781 : int = aten::sub(%6780, %26) # torch/nn/functional.py:1992:19
          %size_prods.25 : int = prim::Loop(%6781, %25, %size_prods.24) # torch/nn/functional.py:1992:4
            block0(%i.7 : int, %size_prods.26 : int):
              %6785 : int = aten::add(%i.7, %26) # torch/nn/functional.py:1993:27
              %6786 : int = aten::__getitem__(%6778, %6785) # torch/nn/functional.py:1993:22
              %size_prods.27 : int = aten::mul(%size_prods.26, %6786) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.27)
          %6788 : bool = aten::eq(%size_prods.25, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6788) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6789 : Tensor = aten::batch_norm(%concated_features.4, %6776, %6777, %6774, %6775, %6773, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.7 : Tensor = aten::relu_(%6789) # torch/nn/functional.py:1117:17
      %6791 : Tensor = prim::GetAttr[name="weight"](%6766)
      %6792 : Tensor? = prim::GetAttr[name="bias"](%6766)
      %6793 : int[] = prim::ListConstruct(%27, %27)
      %6794 : int[] = prim::ListConstruct(%24, %24)
      %6795 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.7 : Tensor = aten::conv2d(%result.7, %6791, %6792, %6793, %6794, %6795, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.7)
  %6797 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6523)
  %6798 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6523)
  %6799 : int = aten::dim(%bottleneck_output.6) # torch/nn/modules/batchnorm.py:276:11
  %6800 : bool = aten::ne(%6799, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6800) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6801 : bool = prim::GetAttr[name="training"](%6798)
   = prim::If(%6801) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6802 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6798)
      %6803 : Tensor = aten::add(%6802, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6798, %6803)
      -> ()
    block1():
      -> ()
  %6804 : bool = prim::GetAttr[name="training"](%6798)
  %6805 : Tensor = prim::GetAttr[name="running_mean"](%6798)
  %6806 : Tensor = prim::GetAttr[name="running_var"](%6798)
  %6807 : Tensor = prim::GetAttr[name="weight"](%6798)
  %6808 : Tensor = prim::GetAttr[name="bias"](%6798)
   = prim::If(%6804) # torch/nn/functional.py:2011:4
    block0():
      %6809 : int[] = aten::size(%bottleneck_output.6) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%6809, %24) # torch/nn/functional.py:1991:17
      %6811 : int = aten::len(%6809) # torch/nn/functional.py:1992:19
      %6812 : int = aten::sub(%6811, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%6812, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %6816 : int = aten::add(%i.8, %26) # torch/nn/functional.py:1993:27
          %6817 : int = aten::__getitem__(%6809, %6816) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %6817) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %6819 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6819) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6820 : Tensor = aten::batch_norm(%bottleneck_output.6, %6807, %6808, %6805, %6806, %6804, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.8 : Tensor = aten::relu_(%6820) # torch/nn/functional.py:1117:17
  %6822 : Tensor = prim::GetAttr[name="weight"](%6797)
  %6823 : Tensor? = prim::GetAttr[name="bias"](%6797)
  %6824 : int[] = prim::ListConstruct(%27, %27)
  %6825 : int[] = prim::ListConstruct(%27, %27)
  %6826 : int[] = prim::ListConstruct(%27, %27)
  %new_features.8 : Tensor = aten::conv2d(%result.8, %6822, %6823, %6824, %6825, %6826, %27) # torch/nn/modules/conv.py:415:15
  %6828 : float = prim::GetAttr[name="drop_rate"](%6523)
  %6829 : bool = aten::gt(%6828, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.5 : Tensor = prim::If(%6829) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6831 : float = prim::GetAttr[name="drop_rate"](%6523)
      %6832 : bool = prim::GetAttr[name="training"](%6523)
      %6833 : bool = aten::lt(%6831, %16) # torch/nn/functional.py:968:7
      %6834 : bool = prim::If(%6833) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6835 : bool = aten::gt(%6831, %17) # torch/nn/functional.py:968:17
          -> (%6835)
       = prim::If(%6834) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6836 : Tensor = aten::dropout(%new_features.8, %6831, %6832) # torch/nn/functional.py:973:17
      -> (%6836)
    block1():
      -> (%new_features.8)
  %6837 : Tensor[] = aten::append(%features.1, %new_features.5) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6838 : Tensor = prim::Uninitialized()
  %6839 : bool = prim::GetAttr[name="memory_efficient"](%6524)
  %6840 : bool = prim::If(%6839) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6841 : bool = prim::Uninitialized()
      %6842 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6843 : bool = aten::gt(%6842, %24)
      %6844 : bool, %6845 : bool, %6846 : int = prim::Loop(%18, %6843, %19, %6841, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6847 : int, %6848 : bool, %6849 : bool, %6850 : int):
          %tensor.5 : Tensor = aten::__getitem__(%features.1, %6850) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6852 : bool = prim::requires_grad(%tensor.5)
          %6853 : bool, %6854 : bool = prim::If(%6852) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6841)
          %6855 : int = aten::add(%6850, %27)
          %6856 : bool = aten::lt(%6855, %6842)
          %6857 : bool = aten::__and__(%6856, %6853)
          -> (%6857, %6852, %6854, %6855)
      %6858 : bool = prim::If(%6844)
        block0():
          -> (%6845)
        block1():
          -> (%19)
      -> (%6858)
    block1():
      -> (%19)
  %bottleneck_output.8 : Tensor = prim::If(%6840) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6838)
    block1():
      %concated_features.5 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6861 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%6524)
      %6862 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%6524)
      %6863 : int = aten::dim(%concated_features.5) # torch/nn/modules/batchnorm.py:276:11
      %6864 : bool = aten::ne(%6863, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6864) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6865 : bool = prim::GetAttr[name="training"](%6862)
       = prim::If(%6865) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6866 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6862)
          %6867 : Tensor = aten::add(%6866, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6862, %6867)
          -> ()
        block1():
          -> ()
      %6868 : bool = prim::GetAttr[name="training"](%6862)
      %6869 : Tensor = prim::GetAttr[name="running_mean"](%6862)
      %6870 : Tensor = prim::GetAttr[name="running_var"](%6862)
      %6871 : Tensor = prim::GetAttr[name="weight"](%6862)
      %6872 : Tensor = prim::GetAttr[name="bias"](%6862)
       = prim::If(%6868) # torch/nn/functional.py:2011:4
        block0():
          %6873 : int[] = aten::size(%concated_features.5) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%6873, %24) # torch/nn/functional.py:1991:17
          %6875 : int = aten::len(%6873) # torch/nn/functional.py:1992:19
          %6876 : int = aten::sub(%6875, %26) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%6876, %25, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %6880 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
              %6881 : int = aten::__getitem__(%6873, %6880) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %6881) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.35)
          %6883 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6883) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6884 : Tensor = aten::batch_norm(%concated_features.5, %6871, %6872, %6869, %6870, %6868, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.9 : Tensor = aten::relu_(%6884) # torch/nn/functional.py:1117:17
      %6886 : Tensor = prim::GetAttr[name="weight"](%6861)
      %6887 : Tensor? = prim::GetAttr[name="bias"](%6861)
      %6888 : int[] = prim::ListConstruct(%27, %27)
      %6889 : int[] = prim::ListConstruct(%24, %24)
      %6890 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.9 : Tensor = aten::conv2d(%result.9, %6886, %6887, %6888, %6889, %6890, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.9)
  %6892 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6524)
  %6893 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6524)
  %6894 : int = aten::dim(%bottleneck_output.8) # torch/nn/modules/batchnorm.py:276:11
  %6895 : bool = aten::ne(%6894, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6895) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6896 : bool = prim::GetAttr[name="training"](%6893)
   = prim::If(%6896) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6897 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6893)
      %6898 : Tensor = aten::add(%6897, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6893, %6898)
      -> ()
    block1():
      -> ()
  %6899 : bool = prim::GetAttr[name="training"](%6893)
  %6900 : Tensor = prim::GetAttr[name="running_mean"](%6893)
  %6901 : Tensor = prim::GetAttr[name="running_var"](%6893)
  %6902 : Tensor = prim::GetAttr[name="weight"](%6893)
  %6903 : Tensor = prim::GetAttr[name="bias"](%6893)
   = prim::If(%6899) # torch/nn/functional.py:2011:4
    block0():
      %6904 : int[] = aten::size(%bottleneck_output.8) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%6904, %24) # torch/nn/functional.py:1991:17
      %6906 : int = aten::len(%6904) # torch/nn/functional.py:1992:19
      %6907 : int = aten::sub(%6906, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%6907, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %6911 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %6912 : int = aten::__getitem__(%6904, %6911) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %6912) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %6914 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6914) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6915 : Tensor = aten::batch_norm(%bottleneck_output.8, %6902, %6903, %6900, %6901, %6899, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.10 : Tensor = aten::relu_(%6915) # torch/nn/functional.py:1117:17
  %6917 : Tensor = prim::GetAttr[name="weight"](%6892)
  %6918 : Tensor? = prim::GetAttr[name="bias"](%6892)
  %6919 : int[] = prim::ListConstruct(%27, %27)
  %6920 : int[] = prim::ListConstruct(%27, %27)
  %6921 : int[] = prim::ListConstruct(%27, %27)
  %new_features.10 : Tensor = aten::conv2d(%result.10, %6917, %6918, %6919, %6920, %6921, %27) # torch/nn/modules/conv.py:415:15
  %6923 : float = prim::GetAttr[name="drop_rate"](%6524)
  %6924 : bool = aten::gt(%6923, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.7 : Tensor = prim::If(%6924) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6926 : float = prim::GetAttr[name="drop_rate"](%6524)
      %6927 : bool = prim::GetAttr[name="training"](%6524)
      %6928 : bool = aten::lt(%6926, %16) # torch/nn/functional.py:968:7
      %6929 : bool = prim::If(%6928) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6930 : bool = aten::gt(%6926, %17) # torch/nn/functional.py:968:17
          -> (%6930)
       = prim::If(%6929) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6931 : Tensor = aten::dropout(%new_features.10, %6926, %6927) # torch/nn/functional.py:973:17
      -> (%6931)
    block1():
      -> (%new_features.10)
  %6932 : Tensor[] = aten::append(%features.1, %new_features.7) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6933 : Tensor = prim::Uninitialized()
  %6934 : bool = prim::GetAttr[name="memory_efficient"](%6525)
  %6935 : bool = prim::If(%6934) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6936 : bool = prim::Uninitialized()
      %6937 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6938 : bool = aten::gt(%6937, %24)
      %6939 : bool, %6940 : bool, %6941 : int = prim::Loop(%18, %6938, %19, %6936, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6942 : int, %6943 : bool, %6944 : bool, %6945 : int):
          %tensor.6 : Tensor = aten::__getitem__(%features.1, %6945) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6947 : bool = prim::requires_grad(%tensor.6)
          %6948 : bool, %6949 : bool = prim::If(%6947) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6936)
          %6950 : int = aten::add(%6945, %27)
          %6951 : bool = aten::lt(%6950, %6937)
          %6952 : bool = aten::__and__(%6951, %6948)
          -> (%6952, %6947, %6949, %6950)
      %6953 : bool = prim::If(%6939)
        block0():
          -> (%6940)
        block1():
          -> (%19)
      -> (%6953)
    block1():
      -> (%19)
  %bottleneck_output.10 : Tensor = prim::If(%6935) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6933)
    block1():
      %concated_features.6 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6956 : __torch__.torch.nn.modules.conv.___torch_mangle_299.Conv2d = prim::GetAttr[name="conv1"](%6525)
      %6957 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm1"](%6525)
      %6958 : int = aten::dim(%concated_features.6) # torch/nn/modules/batchnorm.py:276:11
      %6959 : bool = aten::ne(%6958, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6959) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6960 : bool = prim::GetAttr[name="training"](%6957)
       = prim::If(%6960) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6961 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6957)
          %6962 : Tensor = aten::add(%6961, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6957, %6962)
          -> ()
        block1():
          -> ()
      %6963 : bool = prim::GetAttr[name="training"](%6957)
      %6964 : Tensor = prim::GetAttr[name="running_mean"](%6957)
      %6965 : Tensor = prim::GetAttr[name="running_var"](%6957)
      %6966 : Tensor = prim::GetAttr[name="weight"](%6957)
      %6967 : Tensor = prim::GetAttr[name="bias"](%6957)
       = prim::If(%6963) # torch/nn/functional.py:2011:4
        block0():
          %6968 : int[] = aten::size(%concated_features.6) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%6968, %24) # torch/nn/functional.py:1991:17
          %6970 : int = aten::len(%6968) # torch/nn/functional.py:1992:19
          %6971 : int = aten::sub(%6970, %26) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%6971, %25, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %6975 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
              %6976 : int = aten::__getitem__(%6968, %6975) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %6976) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.43)
          %6978 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6978) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6979 : Tensor = aten::batch_norm(%concated_features.6, %6966, %6967, %6964, %6965, %6963, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.11 : Tensor = aten::relu_(%6979) # torch/nn/functional.py:1117:17
      %6981 : Tensor = prim::GetAttr[name="weight"](%6956)
      %6982 : Tensor? = prim::GetAttr[name="bias"](%6956)
      %6983 : int[] = prim::ListConstruct(%27, %27)
      %6984 : int[] = prim::ListConstruct(%24, %24)
      %6985 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.11 : Tensor = aten::conv2d(%result.11, %6981, %6982, %6983, %6984, %6985, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.11)
  %6987 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6525)
  %6988 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6525)
  %6989 : int = aten::dim(%bottleneck_output.10) # torch/nn/modules/batchnorm.py:276:11
  %6990 : bool = aten::ne(%6989, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6990) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6991 : bool = prim::GetAttr[name="training"](%6988)
   = prim::If(%6991) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6992 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6988)
      %6993 : Tensor = aten::add(%6992, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6988, %6993)
      -> ()
    block1():
      -> ()
  %6994 : bool = prim::GetAttr[name="training"](%6988)
  %6995 : Tensor = prim::GetAttr[name="running_mean"](%6988)
  %6996 : Tensor = prim::GetAttr[name="running_var"](%6988)
  %6997 : Tensor = prim::GetAttr[name="weight"](%6988)
  %6998 : Tensor = prim::GetAttr[name="bias"](%6988)
   = prim::If(%6994) # torch/nn/functional.py:2011:4
    block0():
      %6999 : int[] = aten::size(%bottleneck_output.10) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%6999, %24) # torch/nn/functional.py:1991:17
      %7001 : int = aten::len(%6999) # torch/nn/functional.py:1992:19
      %7002 : int = aten::sub(%7001, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%7002, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %7006 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %7007 : int = aten::__getitem__(%6999, %7006) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %7007) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %7009 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7009) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7010 : Tensor = aten::batch_norm(%bottleneck_output.10, %6997, %6998, %6995, %6996, %6994, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.12 : Tensor = aten::relu_(%7010) # torch/nn/functional.py:1117:17
  %7012 : Tensor = prim::GetAttr[name="weight"](%6987)
  %7013 : Tensor? = prim::GetAttr[name="bias"](%6987)
  %7014 : int[] = prim::ListConstruct(%27, %27)
  %7015 : int[] = prim::ListConstruct(%27, %27)
  %7016 : int[] = prim::ListConstruct(%27, %27)
  %new_features.12 : Tensor = aten::conv2d(%result.12, %7012, %7013, %7014, %7015, %7016, %27) # torch/nn/modules/conv.py:415:15
  %7018 : float = prim::GetAttr[name="drop_rate"](%6525)
  %7019 : bool = aten::gt(%7018, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.9 : Tensor = prim::If(%7019) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7021 : float = prim::GetAttr[name="drop_rate"](%6525)
      %7022 : bool = prim::GetAttr[name="training"](%6525)
      %7023 : bool = aten::lt(%7021, %16) # torch/nn/functional.py:968:7
      %7024 : bool = prim::If(%7023) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7025 : bool = aten::gt(%7021, %17) # torch/nn/functional.py:968:17
          -> (%7025)
       = prim::If(%7024) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7026 : Tensor = aten::dropout(%new_features.12, %7021, %7022) # torch/nn/functional.py:973:17
      -> (%7026)
    block1():
      -> (%new_features.12)
  %7027 : Tensor[] = aten::append(%features.1, %new_features.9) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7028 : Tensor = prim::Uninitialized()
  %7029 : bool = prim::GetAttr[name="memory_efficient"](%6526)
  %7030 : bool = prim::If(%7029) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7031 : bool = prim::Uninitialized()
      %7032 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7033 : bool = aten::gt(%7032, %24)
      %7034 : bool, %7035 : bool, %7036 : int = prim::Loop(%18, %7033, %19, %7031, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7037 : int, %7038 : bool, %7039 : bool, %7040 : int):
          %tensor.7 : Tensor = aten::__getitem__(%features.1, %7040) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7042 : bool = prim::requires_grad(%tensor.7)
          %7043 : bool, %7044 : bool = prim::If(%7042) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7031)
          %7045 : int = aten::add(%7040, %27)
          %7046 : bool = aten::lt(%7045, %7032)
          %7047 : bool = aten::__and__(%7046, %7043)
          -> (%7047, %7042, %7044, %7045)
      %7048 : bool = prim::If(%7034)
        block0():
          -> (%7035)
        block1():
          -> (%19)
      -> (%7048)
    block1():
      -> (%19)
  %bottleneck_output.12 : Tensor = prim::If(%7030) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7028)
    block1():
      %concated_features.7 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7051 : __torch__.torch.nn.modules.conv.___torch_mangle_301.Conv2d = prim::GetAttr[name="conv1"](%6526)
      %7052 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%6526)
      %7053 : int = aten::dim(%concated_features.7) # torch/nn/modules/batchnorm.py:276:11
      %7054 : bool = aten::ne(%7053, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7054) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7055 : bool = prim::GetAttr[name="training"](%7052)
       = prim::If(%7055) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7056 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7052)
          %7057 : Tensor = aten::add(%7056, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7052, %7057)
          -> ()
        block1():
          -> ()
      %7058 : bool = prim::GetAttr[name="training"](%7052)
      %7059 : Tensor = prim::GetAttr[name="running_mean"](%7052)
      %7060 : Tensor = prim::GetAttr[name="running_var"](%7052)
      %7061 : Tensor = prim::GetAttr[name="weight"](%7052)
      %7062 : Tensor = prim::GetAttr[name="bias"](%7052)
       = prim::If(%7058) # torch/nn/functional.py:2011:4
        block0():
          %7063 : int[] = aten::size(%concated_features.7) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%7063, %24) # torch/nn/functional.py:1991:17
          %7065 : int = aten::len(%7063) # torch/nn/functional.py:1992:19
          %7066 : int = aten::sub(%7065, %26) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%7066, %25, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %7070 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
              %7071 : int = aten::__getitem__(%7063, %7070) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %7071) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.51)
          %7073 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7073) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7074 : Tensor = aten::batch_norm(%concated_features.7, %7061, %7062, %7059, %7060, %7058, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.13 : Tensor = aten::relu_(%7074) # torch/nn/functional.py:1117:17
      %7076 : Tensor = prim::GetAttr[name="weight"](%7051)
      %7077 : Tensor? = prim::GetAttr[name="bias"](%7051)
      %7078 : int[] = prim::ListConstruct(%27, %27)
      %7079 : int[] = prim::ListConstruct(%24, %24)
      %7080 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.13 : Tensor = aten::conv2d(%result.13, %7076, %7077, %7078, %7079, %7080, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.13)
  %7082 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6526)
  %7083 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6526)
  %7084 : int = aten::dim(%bottleneck_output.12) # torch/nn/modules/batchnorm.py:276:11
  %7085 : bool = aten::ne(%7084, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7085) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7086 : bool = prim::GetAttr[name="training"](%7083)
   = prim::If(%7086) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7087 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7083)
      %7088 : Tensor = aten::add(%7087, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7083, %7088)
      -> ()
    block1():
      -> ()
  %7089 : bool = prim::GetAttr[name="training"](%7083)
  %7090 : Tensor = prim::GetAttr[name="running_mean"](%7083)
  %7091 : Tensor = prim::GetAttr[name="running_var"](%7083)
  %7092 : Tensor = prim::GetAttr[name="weight"](%7083)
  %7093 : Tensor = prim::GetAttr[name="bias"](%7083)
   = prim::If(%7089) # torch/nn/functional.py:2011:4
    block0():
      %7094 : int[] = aten::size(%bottleneck_output.12) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%7094, %24) # torch/nn/functional.py:1991:17
      %7096 : int = aten::len(%7094) # torch/nn/functional.py:1992:19
      %7097 : int = aten::sub(%7096, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%7097, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %7101 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %7102 : int = aten::__getitem__(%7094, %7101) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %7102) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %7104 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7104) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7105 : Tensor = aten::batch_norm(%bottleneck_output.12, %7092, %7093, %7090, %7091, %7089, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.14 : Tensor = aten::relu_(%7105) # torch/nn/functional.py:1117:17
  %7107 : Tensor = prim::GetAttr[name="weight"](%7082)
  %7108 : Tensor? = prim::GetAttr[name="bias"](%7082)
  %7109 : int[] = prim::ListConstruct(%27, %27)
  %7110 : int[] = prim::ListConstruct(%27, %27)
  %7111 : int[] = prim::ListConstruct(%27, %27)
  %new_features.14 : Tensor = aten::conv2d(%result.14, %7107, %7108, %7109, %7110, %7111, %27) # torch/nn/modules/conv.py:415:15
  %7113 : float = prim::GetAttr[name="drop_rate"](%6526)
  %7114 : bool = aten::gt(%7113, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.11 : Tensor = prim::If(%7114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7116 : float = prim::GetAttr[name="drop_rate"](%6526)
      %7117 : bool = prim::GetAttr[name="training"](%6526)
      %7118 : bool = aten::lt(%7116, %16) # torch/nn/functional.py:968:7
      %7119 : bool = prim::If(%7118) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7120 : bool = aten::gt(%7116, %17) # torch/nn/functional.py:968:17
          -> (%7120)
       = prim::If(%7119) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7121 : Tensor = aten::dropout(%new_features.14, %7116, %7117) # torch/nn/functional.py:973:17
      -> (%7121)
    block1():
      -> (%new_features.14)
  %7122 : Tensor[] = aten::append(%features.1, %new_features.11) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7123 : Tensor = prim::Uninitialized()
  %7124 : bool = prim::GetAttr[name="memory_efficient"](%6527)
  %7125 : bool = prim::If(%7124) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7126 : bool = prim::Uninitialized()
      %7127 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7128 : bool = aten::gt(%7127, %24)
      %7129 : bool, %7130 : bool, %7131 : int = prim::Loop(%18, %7128, %19, %7126, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7132 : int, %7133 : bool, %7134 : bool, %7135 : int):
          %tensor.8 : Tensor = aten::__getitem__(%features.1, %7135) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7137 : bool = prim::requires_grad(%tensor.8)
          %7138 : bool, %7139 : bool = prim::If(%7137) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7126)
          %7140 : int = aten::add(%7135, %27)
          %7141 : bool = aten::lt(%7140, %7127)
          %7142 : bool = aten::__and__(%7141, %7138)
          -> (%7142, %7137, %7139, %7140)
      %7143 : bool = prim::If(%7129)
        block0():
          -> (%7130)
        block1():
          -> (%19)
      -> (%7143)
    block1():
      -> (%19)
  %bottleneck_output.14 : Tensor = prim::If(%7125) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7123)
    block1():
      %concated_features.8 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7146 : __torch__.torch.nn.modules.conv.___torch_mangle_304.Conv2d = prim::GetAttr[name="conv1"](%6527)
      %7147 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_303.BatchNorm2d = prim::GetAttr[name="norm1"](%6527)
      %7148 : int = aten::dim(%concated_features.8) # torch/nn/modules/batchnorm.py:276:11
      %7149 : bool = aten::ne(%7148, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7149) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7150 : bool = prim::GetAttr[name="training"](%7147)
       = prim::If(%7150) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7151 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7147)
          %7152 : Tensor = aten::add(%7151, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7147, %7152)
          -> ()
        block1():
          -> ()
      %7153 : bool = prim::GetAttr[name="training"](%7147)
      %7154 : Tensor = prim::GetAttr[name="running_mean"](%7147)
      %7155 : Tensor = prim::GetAttr[name="running_var"](%7147)
      %7156 : Tensor = prim::GetAttr[name="weight"](%7147)
      %7157 : Tensor = prim::GetAttr[name="bias"](%7147)
       = prim::If(%7153) # torch/nn/functional.py:2011:4
        block0():
          %7158 : int[] = aten::size(%concated_features.8) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%7158, %24) # torch/nn/functional.py:1991:17
          %7160 : int = aten::len(%7158) # torch/nn/functional.py:1992:19
          %7161 : int = aten::sub(%7160, %26) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%7161, %25, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %7165 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
              %7166 : int = aten::__getitem__(%7158, %7165) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %7166) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.59)
          %7168 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7168) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7169 : Tensor = aten::batch_norm(%concated_features.8, %7156, %7157, %7154, %7155, %7153, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.15 : Tensor = aten::relu_(%7169) # torch/nn/functional.py:1117:17
      %7171 : Tensor = prim::GetAttr[name="weight"](%7146)
      %7172 : Tensor? = prim::GetAttr[name="bias"](%7146)
      %7173 : int[] = prim::ListConstruct(%27, %27)
      %7174 : int[] = prim::ListConstruct(%24, %24)
      %7175 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.15 : Tensor = aten::conv2d(%result.15, %7171, %7172, %7173, %7174, %7175, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.15)
  %7177 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6527)
  %7178 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6527)
  %7179 : int = aten::dim(%bottleneck_output.14) # torch/nn/modules/batchnorm.py:276:11
  %7180 : bool = aten::ne(%7179, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7180) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7181 : bool = prim::GetAttr[name="training"](%7178)
   = prim::If(%7181) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7182 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7178)
      %7183 : Tensor = aten::add(%7182, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7178, %7183)
      -> ()
    block1():
      -> ()
  %7184 : bool = prim::GetAttr[name="training"](%7178)
  %7185 : Tensor = prim::GetAttr[name="running_mean"](%7178)
  %7186 : Tensor = prim::GetAttr[name="running_var"](%7178)
  %7187 : Tensor = prim::GetAttr[name="weight"](%7178)
  %7188 : Tensor = prim::GetAttr[name="bias"](%7178)
   = prim::If(%7184) # torch/nn/functional.py:2011:4
    block0():
      %7189 : int[] = aten::size(%bottleneck_output.14) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%7189, %24) # torch/nn/functional.py:1991:17
      %7191 : int = aten::len(%7189) # torch/nn/functional.py:1992:19
      %7192 : int = aten::sub(%7191, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%7192, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %7196 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %7197 : int = aten::__getitem__(%7189, %7196) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %7197) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %7199 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7199) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7200 : Tensor = aten::batch_norm(%bottleneck_output.14, %7187, %7188, %7185, %7186, %7184, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.16 : Tensor = aten::relu_(%7200) # torch/nn/functional.py:1117:17
  %7202 : Tensor = prim::GetAttr[name="weight"](%7177)
  %7203 : Tensor? = prim::GetAttr[name="bias"](%7177)
  %7204 : int[] = prim::ListConstruct(%27, %27)
  %7205 : int[] = prim::ListConstruct(%27, %27)
  %7206 : int[] = prim::ListConstruct(%27, %27)
  %new_features.16 : Tensor = aten::conv2d(%result.16, %7202, %7203, %7204, %7205, %7206, %27) # torch/nn/modules/conv.py:415:15
  %7208 : float = prim::GetAttr[name="drop_rate"](%6527)
  %7209 : bool = aten::gt(%7208, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.13 : Tensor = prim::If(%7209) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7211 : float = prim::GetAttr[name="drop_rate"](%6527)
      %7212 : bool = prim::GetAttr[name="training"](%6527)
      %7213 : bool = aten::lt(%7211, %16) # torch/nn/functional.py:968:7
      %7214 : bool = prim::If(%7213) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7215 : bool = aten::gt(%7211, %17) # torch/nn/functional.py:968:17
          -> (%7215)
       = prim::If(%7214) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7216 : Tensor = aten::dropout(%new_features.16, %7211, %7212) # torch/nn/functional.py:973:17
      -> (%7216)
    block1():
      -> (%new_features.16)
  %7217 : Tensor[] = aten::append(%features.1, %new_features.13) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7218 : Tensor = prim::Uninitialized()
  %7219 : bool = prim::GetAttr[name="memory_efficient"](%6528)
  %7220 : bool = prim::If(%7219) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7221 : bool = prim::Uninitialized()
      %7222 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7223 : bool = aten::gt(%7222, %24)
      %7224 : bool, %7225 : bool, %7226 : int = prim::Loop(%18, %7223, %19, %7221, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7227 : int, %7228 : bool, %7229 : bool, %7230 : int):
          %tensor.9 : Tensor = aten::__getitem__(%features.1, %7230) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7232 : bool = prim::requires_grad(%tensor.9)
          %7233 : bool, %7234 : bool = prim::If(%7232) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7221)
          %7235 : int = aten::add(%7230, %27)
          %7236 : bool = aten::lt(%7235, %7222)
          %7237 : bool = aten::__and__(%7236, %7233)
          -> (%7237, %7232, %7234, %7235)
      %7238 : bool = prim::If(%7224)
        block0():
          -> (%7225)
        block1():
          -> (%19)
      -> (%7238)
    block1():
      -> (%19)
  %bottleneck_output.16 : Tensor = prim::If(%7220) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7218)
    block1():
      %concated_features.9 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7241 : __torch__.torch.nn.modules.conv.___torch_mangle_307.Conv2d = prim::GetAttr[name="conv1"](%6528)
      %7242 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="norm1"](%6528)
      %7243 : int = aten::dim(%concated_features.9) # torch/nn/modules/batchnorm.py:276:11
      %7244 : bool = aten::ne(%7243, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7244) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7245 : bool = prim::GetAttr[name="training"](%7242)
       = prim::If(%7245) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7246 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7242)
          %7247 : Tensor = aten::add(%7246, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7242, %7247)
          -> ()
        block1():
          -> ()
      %7248 : bool = prim::GetAttr[name="training"](%7242)
      %7249 : Tensor = prim::GetAttr[name="running_mean"](%7242)
      %7250 : Tensor = prim::GetAttr[name="running_var"](%7242)
      %7251 : Tensor = prim::GetAttr[name="weight"](%7242)
      %7252 : Tensor = prim::GetAttr[name="bias"](%7242)
       = prim::If(%7248) # torch/nn/functional.py:2011:4
        block0():
          %7253 : int[] = aten::size(%concated_features.9) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%7253, %24) # torch/nn/functional.py:1991:17
          %7255 : int = aten::len(%7253) # torch/nn/functional.py:1992:19
          %7256 : int = aten::sub(%7255, %26) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%7256, %25, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %7260 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
              %7261 : int = aten::__getitem__(%7253, %7260) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %7261) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.67)
          %7263 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7263) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7264 : Tensor = aten::batch_norm(%concated_features.9, %7251, %7252, %7249, %7250, %7248, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.17 : Tensor = aten::relu_(%7264) # torch/nn/functional.py:1117:17
      %7266 : Tensor = prim::GetAttr[name="weight"](%7241)
      %7267 : Tensor? = prim::GetAttr[name="bias"](%7241)
      %7268 : int[] = prim::ListConstruct(%27, %27)
      %7269 : int[] = prim::ListConstruct(%24, %24)
      %7270 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.17 : Tensor = aten::conv2d(%result.17, %7266, %7267, %7268, %7269, %7270, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.17)
  %7272 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6528)
  %7273 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6528)
  %7274 : int = aten::dim(%bottleneck_output.16) # torch/nn/modules/batchnorm.py:276:11
  %7275 : bool = aten::ne(%7274, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7275) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7276 : bool = prim::GetAttr[name="training"](%7273)
   = prim::If(%7276) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7277 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7273)
      %7278 : Tensor = aten::add(%7277, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7273, %7278)
      -> ()
    block1():
      -> ()
  %7279 : bool = prim::GetAttr[name="training"](%7273)
  %7280 : Tensor = prim::GetAttr[name="running_mean"](%7273)
  %7281 : Tensor = prim::GetAttr[name="running_var"](%7273)
  %7282 : Tensor = prim::GetAttr[name="weight"](%7273)
  %7283 : Tensor = prim::GetAttr[name="bias"](%7273)
   = prim::If(%7279) # torch/nn/functional.py:2011:4
    block0():
      %7284 : int[] = aten::size(%bottleneck_output.16) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%7284, %24) # torch/nn/functional.py:1991:17
      %7286 : int = aten::len(%7284) # torch/nn/functional.py:1992:19
      %7287 : int = aten::sub(%7286, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%7287, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %7291 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %7292 : int = aten::__getitem__(%7284, %7291) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %7292) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %7294 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7294) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7295 : Tensor = aten::batch_norm(%bottleneck_output.16, %7282, %7283, %7280, %7281, %7279, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.18 : Tensor = aten::relu_(%7295) # torch/nn/functional.py:1117:17
  %7297 : Tensor = prim::GetAttr[name="weight"](%7272)
  %7298 : Tensor? = prim::GetAttr[name="bias"](%7272)
  %7299 : int[] = prim::ListConstruct(%27, %27)
  %7300 : int[] = prim::ListConstruct(%27, %27)
  %7301 : int[] = prim::ListConstruct(%27, %27)
  %new_features.18 : Tensor = aten::conv2d(%result.18, %7297, %7298, %7299, %7300, %7301, %27) # torch/nn/modules/conv.py:415:15
  %7303 : float = prim::GetAttr[name="drop_rate"](%6528)
  %7304 : bool = aten::gt(%7303, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.15 : Tensor = prim::If(%7304) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7306 : float = prim::GetAttr[name="drop_rate"](%6528)
      %7307 : bool = prim::GetAttr[name="training"](%6528)
      %7308 : bool = aten::lt(%7306, %16) # torch/nn/functional.py:968:7
      %7309 : bool = prim::If(%7308) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7310 : bool = aten::gt(%7306, %17) # torch/nn/functional.py:968:17
          -> (%7310)
       = prim::If(%7309) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7311 : Tensor = aten::dropout(%new_features.18, %7306, %7307) # torch/nn/functional.py:973:17
      -> (%7311)
    block1():
      -> (%new_features.18)
  %7312 : Tensor[] = aten::append(%features.1, %new_features.15) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7313 : Tensor = prim::Uninitialized()
  %7314 : bool = prim::GetAttr[name="memory_efficient"](%6529)
  %7315 : bool = prim::If(%7314) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7316 : bool = prim::Uninitialized()
      %7317 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7318 : bool = aten::gt(%7317, %24)
      %7319 : bool, %7320 : bool, %7321 : int = prim::Loop(%18, %7318, %19, %7316, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7322 : int, %7323 : bool, %7324 : bool, %7325 : int):
          %tensor.10 : Tensor = aten::__getitem__(%features.1, %7325) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7327 : bool = prim::requires_grad(%tensor.10)
          %7328 : bool, %7329 : bool = prim::If(%7327) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7316)
          %7330 : int = aten::add(%7325, %27)
          %7331 : bool = aten::lt(%7330, %7317)
          %7332 : bool = aten::__and__(%7331, %7328)
          -> (%7332, %7327, %7329, %7330)
      %7333 : bool = prim::If(%7319)
        block0():
          -> (%7320)
        block1():
          -> (%19)
      -> (%7333)
    block1():
      -> (%19)
  %bottleneck_output.18 : Tensor = prim::If(%7315) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7313)
    block1():
      %concated_features.10 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7336 : __torch__.torch.nn.modules.conv.___torch_mangle_309.Conv2d = prim::GetAttr[name="conv1"](%6529)
      %7337 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%6529)
      %7338 : int = aten::dim(%concated_features.10) # torch/nn/modules/batchnorm.py:276:11
      %7339 : bool = aten::ne(%7338, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7339) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7340 : bool = prim::GetAttr[name="training"](%7337)
       = prim::If(%7340) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7341 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7337)
          %7342 : Tensor = aten::add(%7341, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7337, %7342)
          -> ()
        block1():
          -> ()
      %7343 : bool = prim::GetAttr[name="training"](%7337)
      %7344 : Tensor = prim::GetAttr[name="running_mean"](%7337)
      %7345 : Tensor = prim::GetAttr[name="running_var"](%7337)
      %7346 : Tensor = prim::GetAttr[name="weight"](%7337)
      %7347 : Tensor = prim::GetAttr[name="bias"](%7337)
       = prim::If(%7343) # torch/nn/functional.py:2011:4
        block0():
          %7348 : int[] = aten::size(%concated_features.10) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%7348, %24) # torch/nn/functional.py:1991:17
          %7350 : int = aten::len(%7348) # torch/nn/functional.py:1992:19
          %7351 : int = aten::sub(%7350, %26) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%7351, %25, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %7355 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
              %7356 : int = aten::__getitem__(%7348, %7355) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %7356) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.75)
          %7358 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7358) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7359 : Tensor = aten::batch_norm(%concated_features.10, %7346, %7347, %7344, %7345, %7343, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.19 : Tensor = aten::relu_(%7359) # torch/nn/functional.py:1117:17
      %7361 : Tensor = prim::GetAttr[name="weight"](%7336)
      %7362 : Tensor? = prim::GetAttr[name="bias"](%7336)
      %7363 : int[] = prim::ListConstruct(%27, %27)
      %7364 : int[] = prim::ListConstruct(%24, %24)
      %7365 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.19 : Tensor = aten::conv2d(%result.19, %7361, %7362, %7363, %7364, %7365, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.19)
  %7367 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6529)
  %7368 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6529)
  %7369 : int = aten::dim(%bottleneck_output.18) # torch/nn/modules/batchnorm.py:276:11
  %7370 : bool = aten::ne(%7369, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7370) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7371 : bool = prim::GetAttr[name="training"](%7368)
   = prim::If(%7371) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7372 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7368)
      %7373 : Tensor = aten::add(%7372, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7368, %7373)
      -> ()
    block1():
      -> ()
  %7374 : bool = prim::GetAttr[name="training"](%7368)
  %7375 : Tensor = prim::GetAttr[name="running_mean"](%7368)
  %7376 : Tensor = prim::GetAttr[name="running_var"](%7368)
  %7377 : Tensor = prim::GetAttr[name="weight"](%7368)
  %7378 : Tensor = prim::GetAttr[name="bias"](%7368)
   = prim::If(%7374) # torch/nn/functional.py:2011:4
    block0():
      %7379 : int[] = aten::size(%bottleneck_output.18) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%7379, %24) # torch/nn/functional.py:1991:17
      %7381 : int = aten::len(%7379) # torch/nn/functional.py:1992:19
      %7382 : int = aten::sub(%7381, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%7382, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %7386 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %7387 : int = aten::__getitem__(%7379, %7386) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %7387) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %7389 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7389) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7390 : Tensor = aten::batch_norm(%bottleneck_output.18, %7377, %7378, %7375, %7376, %7374, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.20 : Tensor = aten::relu_(%7390) # torch/nn/functional.py:1117:17
  %7392 : Tensor = prim::GetAttr[name="weight"](%7367)
  %7393 : Tensor? = prim::GetAttr[name="bias"](%7367)
  %7394 : int[] = prim::ListConstruct(%27, %27)
  %7395 : int[] = prim::ListConstruct(%27, %27)
  %7396 : int[] = prim::ListConstruct(%27, %27)
  %new_features.20 : Tensor = aten::conv2d(%result.20, %7392, %7393, %7394, %7395, %7396, %27) # torch/nn/modules/conv.py:415:15
  %7398 : float = prim::GetAttr[name="drop_rate"](%6529)
  %7399 : bool = aten::gt(%7398, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.17 : Tensor = prim::If(%7399) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7401 : float = prim::GetAttr[name="drop_rate"](%6529)
      %7402 : bool = prim::GetAttr[name="training"](%6529)
      %7403 : bool = aten::lt(%7401, %16) # torch/nn/functional.py:968:7
      %7404 : bool = prim::If(%7403) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7405 : bool = aten::gt(%7401, %17) # torch/nn/functional.py:968:17
          -> (%7405)
       = prim::If(%7404) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7406 : Tensor = aten::dropout(%new_features.20, %7401, %7402) # torch/nn/functional.py:973:17
      -> (%7406)
    block1():
      -> (%new_features.20)
  %7407 : Tensor[] = aten::append(%features.1, %new_features.17) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7408 : Tensor = prim::Uninitialized()
  %7409 : bool = prim::GetAttr[name="memory_efficient"](%6530)
  %7410 : bool = prim::If(%7409) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7411 : bool = prim::Uninitialized()
      %7412 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7413 : bool = aten::gt(%7412, %24)
      %7414 : bool, %7415 : bool, %7416 : int = prim::Loop(%18, %7413, %19, %7411, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7417 : int, %7418 : bool, %7419 : bool, %7420 : int):
          %tensor.11 : Tensor = aten::__getitem__(%features.1, %7420) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7422 : bool = prim::requires_grad(%tensor.11)
          %7423 : bool, %7424 : bool = prim::If(%7422) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7411)
          %7425 : int = aten::add(%7420, %27)
          %7426 : bool = aten::lt(%7425, %7412)
          %7427 : bool = aten::__and__(%7426, %7423)
          -> (%7427, %7422, %7424, %7425)
      %7428 : bool = prim::If(%7414)
        block0():
          -> (%7415)
        block1():
          -> (%19)
      -> (%7428)
    block1():
      -> (%19)
  %bottleneck_output.20 : Tensor = prim::If(%7410) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7408)
    block1():
      %concated_features.11 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7431 : __torch__.torch.nn.modules.conv.___torch_mangle_312.Conv2d = prim::GetAttr[name="conv1"](%6530)
      %7432 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_311.BatchNorm2d = prim::GetAttr[name="norm1"](%6530)
      %7433 : int = aten::dim(%concated_features.11) # torch/nn/modules/batchnorm.py:276:11
      %7434 : bool = aten::ne(%7433, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7434) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7435 : bool = prim::GetAttr[name="training"](%7432)
       = prim::If(%7435) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7436 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7432)
          %7437 : Tensor = aten::add(%7436, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7432, %7437)
          -> ()
        block1():
          -> ()
      %7438 : bool = prim::GetAttr[name="training"](%7432)
      %7439 : Tensor = prim::GetAttr[name="running_mean"](%7432)
      %7440 : Tensor = prim::GetAttr[name="running_var"](%7432)
      %7441 : Tensor = prim::GetAttr[name="weight"](%7432)
      %7442 : Tensor = prim::GetAttr[name="bias"](%7432)
       = prim::If(%7438) # torch/nn/functional.py:2011:4
        block0():
          %7443 : int[] = aten::size(%concated_features.11) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%7443, %24) # torch/nn/functional.py:1991:17
          %7445 : int = aten::len(%7443) # torch/nn/functional.py:1992:19
          %7446 : int = aten::sub(%7445, %26) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%7446, %25, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %7450 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
              %7451 : int = aten::__getitem__(%7443, %7450) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %7451) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.83)
          %7453 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7453) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7454 : Tensor = aten::batch_norm(%concated_features.11, %7441, %7442, %7439, %7440, %7438, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.21 : Tensor = aten::relu_(%7454) # torch/nn/functional.py:1117:17
      %7456 : Tensor = prim::GetAttr[name="weight"](%7431)
      %7457 : Tensor? = prim::GetAttr[name="bias"](%7431)
      %7458 : int[] = prim::ListConstruct(%27, %27)
      %7459 : int[] = prim::ListConstruct(%24, %24)
      %7460 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.21 : Tensor = aten::conv2d(%result.21, %7456, %7457, %7458, %7459, %7460, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.21)
  %7462 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6530)
  %7463 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6530)
  %7464 : int = aten::dim(%bottleneck_output.20) # torch/nn/modules/batchnorm.py:276:11
  %7465 : bool = aten::ne(%7464, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7465) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7466 : bool = prim::GetAttr[name="training"](%7463)
   = prim::If(%7466) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7467 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7463)
      %7468 : Tensor = aten::add(%7467, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7463, %7468)
      -> ()
    block1():
      -> ()
  %7469 : bool = prim::GetAttr[name="training"](%7463)
  %7470 : Tensor = prim::GetAttr[name="running_mean"](%7463)
  %7471 : Tensor = prim::GetAttr[name="running_var"](%7463)
  %7472 : Tensor = prim::GetAttr[name="weight"](%7463)
  %7473 : Tensor = prim::GetAttr[name="bias"](%7463)
   = prim::If(%7469) # torch/nn/functional.py:2011:4
    block0():
      %7474 : int[] = aten::size(%bottleneck_output.20) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%7474, %24) # torch/nn/functional.py:1991:17
      %7476 : int = aten::len(%7474) # torch/nn/functional.py:1992:19
      %7477 : int = aten::sub(%7476, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%7477, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %7481 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %7482 : int = aten::__getitem__(%7474, %7481) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %7482) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %7484 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7484) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7485 : Tensor = aten::batch_norm(%bottleneck_output.20, %7472, %7473, %7470, %7471, %7469, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.22 : Tensor = aten::relu_(%7485) # torch/nn/functional.py:1117:17
  %7487 : Tensor = prim::GetAttr[name="weight"](%7462)
  %7488 : Tensor? = prim::GetAttr[name="bias"](%7462)
  %7489 : int[] = prim::ListConstruct(%27, %27)
  %7490 : int[] = prim::ListConstruct(%27, %27)
  %7491 : int[] = prim::ListConstruct(%27, %27)
  %new_features.22 : Tensor = aten::conv2d(%result.22, %7487, %7488, %7489, %7490, %7491, %27) # torch/nn/modules/conv.py:415:15
  %7493 : float = prim::GetAttr[name="drop_rate"](%6530)
  %7494 : bool = aten::gt(%7493, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.19 : Tensor = prim::If(%7494) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7496 : float = prim::GetAttr[name="drop_rate"](%6530)
      %7497 : bool = prim::GetAttr[name="training"](%6530)
      %7498 : bool = aten::lt(%7496, %16) # torch/nn/functional.py:968:7
      %7499 : bool = prim::If(%7498) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7500 : bool = aten::gt(%7496, %17) # torch/nn/functional.py:968:17
          -> (%7500)
       = prim::If(%7499) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7501 : Tensor = aten::dropout(%new_features.22, %7496, %7497) # torch/nn/functional.py:973:17
      -> (%7501)
    block1():
      -> (%new_features.22)
  %7502 : Tensor[] = aten::append(%features.1, %new_features.19) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7503 : Tensor = prim::Uninitialized()
  %7504 : bool = prim::GetAttr[name="memory_efficient"](%6531)
  %7505 : bool = prim::If(%7504) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7506 : bool = prim::Uninitialized()
      %7507 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7508 : bool = aten::gt(%7507, %24)
      %7509 : bool, %7510 : bool, %7511 : int = prim::Loop(%18, %7508, %19, %7506, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7512 : int, %7513 : bool, %7514 : bool, %7515 : int):
          %tensor.12 : Tensor = aten::__getitem__(%features.1, %7515) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7517 : bool = prim::requires_grad(%tensor.12)
          %7518 : bool, %7519 : bool = prim::If(%7517) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7506)
          %7520 : int = aten::add(%7515, %27)
          %7521 : bool = aten::lt(%7520, %7507)
          %7522 : bool = aten::__and__(%7521, %7518)
          -> (%7522, %7517, %7519, %7520)
      %7523 : bool = prim::If(%7509)
        block0():
          -> (%7510)
        block1():
          -> (%19)
      -> (%7523)
    block1():
      -> (%19)
  %bottleneck_output.22 : Tensor = prim::If(%7505) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7503)
    block1():
      %concated_features.12 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7526 : __torch__.torch.nn.modules.conv.___torch_mangle_315.Conv2d = prim::GetAttr[name="conv1"](%6531)
      %7527 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_314.BatchNorm2d = prim::GetAttr[name="norm1"](%6531)
      %7528 : int = aten::dim(%concated_features.12) # torch/nn/modules/batchnorm.py:276:11
      %7529 : bool = aten::ne(%7528, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7529) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7530 : bool = prim::GetAttr[name="training"](%7527)
       = prim::If(%7530) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7531 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7527)
          %7532 : Tensor = aten::add(%7531, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7527, %7532)
          -> ()
        block1():
          -> ()
      %7533 : bool = prim::GetAttr[name="training"](%7527)
      %7534 : Tensor = prim::GetAttr[name="running_mean"](%7527)
      %7535 : Tensor = prim::GetAttr[name="running_var"](%7527)
      %7536 : Tensor = prim::GetAttr[name="weight"](%7527)
      %7537 : Tensor = prim::GetAttr[name="bias"](%7527)
       = prim::If(%7533) # torch/nn/functional.py:2011:4
        block0():
          %7538 : int[] = aten::size(%concated_features.12) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%7538, %24) # torch/nn/functional.py:1991:17
          %7540 : int = aten::len(%7538) # torch/nn/functional.py:1992:19
          %7541 : int = aten::sub(%7540, %26) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%7541, %25, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %7545 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
              %7546 : int = aten::__getitem__(%7538, %7545) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %7546) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.91)
          %7548 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7548) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7549 : Tensor = aten::batch_norm(%concated_features.12, %7536, %7537, %7534, %7535, %7533, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.23 : Tensor = aten::relu_(%7549) # torch/nn/functional.py:1117:17
      %7551 : Tensor = prim::GetAttr[name="weight"](%7526)
      %7552 : Tensor? = prim::GetAttr[name="bias"](%7526)
      %7553 : int[] = prim::ListConstruct(%27, %27)
      %7554 : int[] = prim::ListConstruct(%24, %24)
      %7555 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.23 : Tensor = aten::conv2d(%result.23, %7551, %7552, %7553, %7554, %7555, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.23)
  %7557 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6531)
  %7558 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6531)
  %7559 : int = aten::dim(%bottleneck_output.22) # torch/nn/modules/batchnorm.py:276:11
  %7560 : bool = aten::ne(%7559, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7560) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7561 : bool = prim::GetAttr[name="training"](%7558)
   = prim::If(%7561) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7562 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7558)
      %7563 : Tensor = aten::add(%7562, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7558, %7563)
      -> ()
    block1():
      -> ()
  %7564 : bool = prim::GetAttr[name="training"](%7558)
  %7565 : Tensor = prim::GetAttr[name="running_mean"](%7558)
  %7566 : Tensor = prim::GetAttr[name="running_var"](%7558)
  %7567 : Tensor = prim::GetAttr[name="weight"](%7558)
  %7568 : Tensor = prim::GetAttr[name="bias"](%7558)
   = prim::If(%7564) # torch/nn/functional.py:2011:4
    block0():
      %7569 : int[] = aten::size(%bottleneck_output.22) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%7569, %24) # torch/nn/functional.py:1991:17
      %7571 : int = aten::len(%7569) # torch/nn/functional.py:1992:19
      %7572 : int = aten::sub(%7571, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%7572, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %7576 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %7577 : int = aten::__getitem__(%7569, %7576) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %7577) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %7579 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7579) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7580 : Tensor = aten::batch_norm(%bottleneck_output.22, %7567, %7568, %7565, %7566, %7564, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.24 : Tensor = aten::relu_(%7580) # torch/nn/functional.py:1117:17
  %7582 : Tensor = prim::GetAttr[name="weight"](%7557)
  %7583 : Tensor? = prim::GetAttr[name="bias"](%7557)
  %7584 : int[] = prim::ListConstruct(%27, %27)
  %7585 : int[] = prim::ListConstruct(%27, %27)
  %7586 : int[] = prim::ListConstruct(%27, %27)
  %new_features.24 : Tensor = aten::conv2d(%result.24, %7582, %7583, %7584, %7585, %7586, %27) # torch/nn/modules/conv.py:415:15
  %7588 : float = prim::GetAttr[name="drop_rate"](%6531)
  %7589 : bool = aten::gt(%7588, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.21 : Tensor = prim::If(%7589) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7591 : float = prim::GetAttr[name="drop_rate"](%6531)
      %7592 : bool = prim::GetAttr[name="training"](%6531)
      %7593 : bool = aten::lt(%7591, %16) # torch/nn/functional.py:968:7
      %7594 : bool = prim::If(%7593) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7595 : bool = aten::gt(%7591, %17) # torch/nn/functional.py:968:17
          -> (%7595)
       = prim::If(%7594) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7596 : Tensor = aten::dropout(%new_features.24, %7591, %7592) # torch/nn/functional.py:973:17
      -> (%7596)
    block1():
      -> (%new_features.24)
  %7597 : Tensor[] = aten::append(%features.1, %new_features.21) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7598 : Tensor = prim::Uninitialized()
  %7599 : bool = prim::GetAttr[name="memory_efficient"](%6532)
  %7600 : bool = prim::If(%7599) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7601 : bool = prim::Uninitialized()
      %7602 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7603 : bool = aten::gt(%7602, %24)
      %7604 : bool, %7605 : bool, %7606 : int = prim::Loop(%18, %7603, %19, %7601, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7607 : int, %7608 : bool, %7609 : bool, %7610 : int):
          %tensor.13 : Tensor = aten::__getitem__(%features.1, %7610) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7612 : bool = prim::requires_grad(%tensor.13)
          %7613 : bool, %7614 : bool = prim::If(%7612) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7601)
          %7615 : int = aten::add(%7610, %27)
          %7616 : bool = aten::lt(%7615, %7602)
          %7617 : bool = aten::__and__(%7616, %7613)
          -> (%7617, %7612, %7614, %7615)
      %7618 : bool = prim::If(%7604)
        block0():
          -> (%7605)
        block1():
          -> (%19)
      -> (%7618)
    block1():
      -> (%19)
  %bottleneck_output.24 : Tensor = prim::If(%7600) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7598)
    block1():
      %concated_features.13 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7621 : __torch__.torch.nn.modules.conv.___torch_mangle_317.Conv2d = prim::GetAttr[name="conv1"](%6532)
      %7622 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%6532)
      %7623 : int = aten::dim(%concated_features.13) # torch/nn/modules/batchnorm.py:276:11
      %7624 : bool = aten::ne(%7623, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7624) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7625 : bool = prim::GetAttr[name="training"](%7622)
       = prim::If(%7625) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7626 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7622)
          %7627 : Tensor = aten::add(%7626, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7622, %7627)
          -> ()
        block1():
          -> ()
      %7628 : bool = prim::GetAttr[name="training"](%7622)
      %7629 : Tensor = prim::GetAttr[name="running_mean"](%7622)
      %7630 : Tensor = prim::GetAttr[name="running_var"](%7622)
      %7631 : Tensor = prim::GetAttr[name="weight"](%7622)
      %7632 : Tensor = prim::GetAttr[name="bias"](%7622)
       = prim::If(%7628) # torch/nn/functional.py:2011:4
        block0():
          %7633 : int[] = aten::size(%concated_features.13) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%7633, %24) # torch/nn/functional.py:1991:17
          %7635 : int = aten::len(%7633) # torch/nn/functional.py:1992:19
          %7636 : int = aten::sub(%7635, %26) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%7636, %25, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %7640 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
              %7641 : int = aten::__getitem__(%7633, %7640) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %7641) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.99)
          %7643 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7643) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7644 : Tensor = aten::batch_norm(%concated_features.13, %7631, %7632, %7629, %7630, %7628, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.25 : Tensor = aten::relu_(%7644) # torch/nn/functional.py:1117:17
      %7646 : Tensor = prim::GetAttr[name="weight"](%7621)
      %7647 : Tensor? = prim::GetAttr[name="bias"](%7621)
      %7648 : int[] = prim::ListConstruct(%27, %27)
      %7649 : int[] = prim::ListConstruct(%24, %24)
      %7650 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.25 : Tensor = aten::conv2d(%result.25, %7646, %7647, %7648, %7649, %7650, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.25)
  %7652 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6532)
  %7653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6532)
  %7654 : int = aten::dim(%bottleneck_output.24) # torch/nn/modules/batchnorm.py:276:11
  %7655 : bool = aten::ne(%7654, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7655) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7656 : bool = prim::GetAttr[name="training"](%7653)
   = prim::If(%7656) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7657 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7653)
      %7658 : Tensor = aten::add(%7657, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7653, %7658)
      -> ()
    block1():
      -> ()
  %7659 : bool = prim::GetAttr[name="training"](%7653)
  %7660 : Tensor = prim::GetAttr[name="running_mean"](%7653)
  %7661 : Tensor = prim::GetAttr[name="running_var"](%7653)
  %7662 : Tensor = prim::GetAttr[name="weight"](%7653)
  %7663 : Tensor = prim::GetAttr[name="bias"](%7653)
   = prim::If(%7659) # torch/nn/functional.py:2011:4
    block0():
      %7664 : int[] = aten::size(%bottleneck_output.24) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%7664, %24) # torch/nn/functional.py:1991:17
      %7666 : int = aten::len(%7664) # torch/nn/functional.py:1992:19
      %7667 : int = aten::sub(%7666, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%7667, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %7671 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %7672 : int = aten::__getitem__(%7664, %7671) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %7672) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %7674 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7674) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7675 : Tensor = aten::batch_norm(%bottleneck_output.24, %7662, %7663, %7660, %7661, %7659, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.26 : Tensor = aten::relu_(%7675) # torch/nn/functional.py:1117:17
  %7677 : Tensor = prim::GetAttr[name="weight"](%7652)
  %7678 : Tensor? = prim::GetAttr[name="bias"](%7652)
  %7679 : int[] = prim::ListConstruct(%27, %27)
  %7680 : int[] = prim::ListConstruct(%27, %27)
  %7681 : int[] = prim::ListConstruct(%27, %27)
  %new_features.26 : Tensor = aten::conv2d(%result.26, %7677, %7678, %7679, %7680, %7681, %27) # torch/nn/modules/conv.py:415:15
  %7683 : float = prim::GetAttr[name="drop_rate"](%6532)
  %7684 : bool = aten::gt(%7683, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.23 : Tensor = prim::If(%7684) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7686 : float = prim::GetAttr[name="drop_rate"](%6532)
      %7687 : bool = prim::GetAttr[name="training"](%6532)
      %7688 : bool = aten::lt(%7686, %16) # torch/nn/functional.py:968:7
      %7689 : bool = prim::If(%7688) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7690 : bool = aten::gt(%7686, %17) # torch/nn/functional.py:968:17
          -> (%7690)
       = prim::If(%7689) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7691 : Tensor = aten::dropout(%new_features.26, %7686, %7687) # torch/nn/functional.py:973:17
      -> (%7691)
    block1():
      -> (%new_features.26)
  %7692 : Tensor[] = aten::append(%features.1, %new_features.23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7693 : Tensor = prim::Uninitialized()
  %7694 : bool = prim::GetAttr[name="memory_efficient"](%6533)
  %7695 : bool = prim::If(%7694) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7696 : bool = prim::Uninitialized()
      %7697 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7698 : bool = aten::gt(%7697, %24)
      %7699 : bool, %7700 : bool, %7701 : int = prim::Loop(%18, %7698, %19, %7696, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7702 : int, %7703 : bool, %7704 : bool, %7705 : int):
          %tensor.14 : Tensor = aten::__getitem__(%features.1, %7705) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7707 : bool = prim::requires_grad(%tensor.14)
          %7708 : bool, %7709 : bool = prim::If(%7707) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7696)
          %7710 : int = aten::add(%7705, %27)
          %7711 : bool = aten::lt(%7710, %7697)
          %7712 : bool = aten::__and__(%7711, %7708)
          -> (%7712, %7707, %7709, %7710)
      %7713 : bool = prim::If(%7699)
        block0():
          -> (%7700)
        block1():
          -> (%19)
      -> (%7713)
    block1():
      -> (%19)
  %bottleneck_output.26 : Tensor = prim::If(%7695) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7693)
    block1():
      %concated_features.14 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7716 : __torch__.torch.nn.modules.conv.___torch_mangle_323.Conv2d = prim::GetAttr[name="conv1"](%6533)
      %7717 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_320.BatchNorm2d = prim::GetAttr[name="norm1"](%6533)
      %7718 : int = aten::dim(%concated_features.14) # torch/nn/modules/batchnorm.py:276:11
      %7719 : bool = aten::ne(%7718, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7719) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7720 : bool = prim::GetAttr[name="training"](%7717)
       = prim::If(%7720) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7721 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7717)
          %7722 : Tensor = aten::add(%7721, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7717, %7722)
          -> ()
        block1():
          -> ()
      %7723 : bool = prim::GetAttr[name="training"](%7717)
      %7724 : Tensor = prim::GetAttr[name="running_mean"](%7717)
      %7725 : Tensor = prim::GetAttr[name="running_var"](%7717)
      %7726 : Tensor = prim::GetAttr[name="weight"](%7717)
      %7727 : Tensor = prim::GetAttr[name="bias"](%7717)
       = prim::If(%7723) # torch/nn/functional.py:2011:4
        block0():
          %7728 : int[] = aten::size(%concated_features.14) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%7728, %24) # torch/nn/functional.py:1991:17
          %7730 : int = aten::len(%7728) # torch/nn/functional.py:1992:19
          %7731 : int = aten::sub(%7730, %26) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%7731, %25, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %7735 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
              %7736 : int = aten::__getitem__(%7728, %7735) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %7736) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.107)
          %7738 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7738) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7739 : Tensor = aten::batch_norm(%concated_features.14, %7726, %7727, %7724, %7725, %7723, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.27 : Tensor = aten::relu_(%7739) # torch/nn/functional.py:1117:17
      %7741 : Tensor = prim::GetAttr[name="weight"](%7716)
      %7742 : Tensor? = prim::GetAttr[name="bias"](%7716)
      %7743 : int[] = prim::ListConstruct(%27, %27)
      %7744 : int[] = prim::ListConstruct(%24, %24)
      %7745 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.27 : Tensor = aten::conv2d(%result.27, %7741, %7742, %7743, %7744, %7745, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.27)
  %7747 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6533)
  %7748 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6533)
  %7749 : int = aten::dim(%bottleneck_output.26) # torch/nn/modules/batchnorm.py:276:11
  %7750 : bool = aten::ne(%7749, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7750) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7751 : bool = prim::GetAttr[name="training"](%7748)
   = prim::If(%7751) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7752 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7748)
      %7753 : Tensor = aten::add(%7752, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7748, %7753)
      -> ()
    block1():
      -> ()
  %7754 : bool = prim::GetAttr[name="training"](%7748)
  %7755 : Tensor = prim::GetAttr[name="running_mean"](%7748)
  %7756 : Tensor = prim::GetAttr[name="running_var"](%7748)
  %7757 : Tensor = prim::GetAttr[name="weight"](%7748)
  %7758 : Tensor = prim::GetAttr[name="bias"](%7748)
   = prim::If(%7754) # torch/nn/functional.py:2011:4
    block0():
      %7759 : int[] = aten::size(%bottleneck_output.26) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%7759, %24) # torch/nn/functional.py:1991:17
      %7761 : int = aten::len(%7759) # torch/nn/functional.py:1992:19
      %7762 : int = aten::sub(%7761, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%7762, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %7766 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %7767 : int = aten::__getitem__(%7759, %7766) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %7767) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %7769 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7769) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7770 : Tensor = aten::batch_norm(%bottleneck_output.26, %7757, %7758, %7755, %7756, %7754, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.28 : Tensor = aten::relu_(%7770) # torch/nn/functional.py:1117:17
  %7772 : Tensor = prim::GetAttr[name="weight"](%7747)
  %7773 : Tensor? = prim::GetAttr[name="bias"](%7747)
  %7774 : int[] = prim::ListConstruct(%27, %27)
  %7775 : int[] = prim::ListConstruct(%27, %27)
  %7776 : int[] = prim::ListConstruct(%27, %27)
  %new_features.28 : Tensor = aten::conv2d(%result.28, %7772, %7773, %7774, %7775, %7776, %27) # torch/nn/modules/conv.py:415:15
  %7778 : float = prim::GetAttr[name="drop_rate"](%6533)
  %7779 : bool = aten::gt(%7778, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.25 : Tensor = prim::If(%7779) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7781 : float = prim::GetAttr[name="drop_rate"](%6533)
      %7782 : bool = prim::GetAttr[name="training"](%6533)
      %7783 : bool = aten::lt(%7781, %16) # torch/nn/functional.py:968:7
      %7784 : bool = prim::If(%7783) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7785 : bool = aten::gt(%7781, %17) # torch/nn/functional.py:968:17
          -> (%7785)
       = prim::If(%7784) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7786 : Tensor = aten::dropout(%new_features.28, %7781, %7782) # torch/nn/functional.py:973:17
      -> (%7786)
    block1():
      -> (%new_features.28)
  %7787 : Tensor[] = aten::append(%features.1, %new_features.25) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7788 : Tensor = prim::Uninitialized()
  %7789 : bool = prim::GetAttr[name="memory_efficient"](%6534)
  %7790 : bool = prim::If(%7789) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7791 : bool = prim::Uninitialized()
      %7792 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7793 : bool = aten::gt(%7792, %24)
      %7794 : bool, %7795 : bool, %7796 : int = prim::Loop(%18, %7793, %19, %7791, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7797 : int, %7798 : bool, %7799 : bool, %7800 : int):
          %tensor.15 : Tensor = aten::__getitem__(%features.1, %7800) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7802 : bool = prim::requires_grad(%tensor.15)
          %7803 : bool, %7804 : bool = prim::If(%7802) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7791)
          %7805 : int = aten::add(%7800, %27)
          %7806 : bool = aten::lt(%7805, %7792)
          %7807 : bool = aten::__and__(%7806, %7803)
          -> (%7807, %7802, %7804, %7805)
      %7808 : bool = prim::If(%7794)
        block0():
          -> (%7795)
        block1():
          -> (%19)
      -> (%7808)
    block1():
      -> (%19)
  %bottleneck_output.28 : Tensor = prim::If(%7790) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7788)
    block1():
      %concated_features.15 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7811 : __torch__.torch.nn.modules.conv.___torch_mangle_326.Conv2d = prim::GetAttr[name="conv1"](%6534)
      %7812 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_325.BatchNorm2d = prim::GetAttr[name="norm1"](%6534)
      %7813 : int = aten::dim(%concated_features.15) # torch/nn/modules/batchnorm.py:276:11
      %7814 : bool = aten::ne(%7813, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7814) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7815 : bool = prim::GetAttr[name="training"](%7812)
       = prim::If(%7815) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7816 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7812)
          %7817 : Tensor = aten::add(%7816, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7812, %7817)
          -> ()
        block1():
          -> ()
      %7818 : bool = prim::GetAttr[name="training"](%7812)
      %7819 : Tensor = prim::GetAttr[name="running_mean"](%7812)
      %7820 : Tensor = prim::GetAttr[name="running_var"](%7812)
      %7821 : Tensor = prim::GetAttr[name="weight"](%7812)
      %7822 : Tensor = prim::GetAttr[name="bias"](%7812)
       = prim::If(%7818) # torch/nn/functional.py:2011:4
        block0():
          %7823 : int[] = aten::size(%concated_features.15) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%7823, %24) # torch/nn/functional.py:1991:17
          %7825 : int = aten::len(%7823) # torch/nn/functional.py:1992:19
          %7826 : int = aten::sub(%7825, %26) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%7826, %25, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %7830 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
              %7831 : int = aten::__getitem__(%7823, %7830) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %7831) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.115)
          %7833 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7833) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7834 : Tensor = aten::batch_norm(%concated_features.15, %7821, %7822, %7819, %7820, %7818, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.29 : Tensor = aten::relu_(%7834) # torch/nn/functional.py:1117:17
      %7836 : Tensor = prim::GetAttr[name="weight"](%7811)
      %7837 : Tensor? = prim::GetAttr[name="bias"](%7811)
      %7838 : int[] = prim::ListConstruct(%27, %27)
      %7839 : int[] = prim::ListConstruct(%24, %24)
      %7840 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.29 : Tensor = aten::conv2d(%result.29, %7836, %7837, %7838, %7839, %7840, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.29)
  %7842 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6534)
  %7843 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6534)
  %7844 : int = aten::dim(%bottleneck_output.28) # torch/nn/modules/batchnorm.py:276:11
  %7845 : bool = aten::ne(%7844, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7845) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7846 : bool = prim::GetAttr[name="training"](%7843)
   = prim::If(%7846) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7847 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7843)
      %7848 : Tensor = aten::add(%7847, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7843, %7848)
      -> ()
    block1():
      -> ()
  %7849 : bool = prim::GetAttr[name="training"](%7843)
  %7850 : Tensor = prim::GetAttr[name="running_mean"](%7843)
  %7851 : Tensor = prim::GetAttr[name="running_var"](%7843)
  %7852 : Tensor = prim::GetAttr[name="weight"](%7843)
  %7853 : Tensor = prim::GetAttr[name="bias"](%7843)
   = prim::If(%7849) # torch/nn/functional.py:2011:4
    block0():
      %7854 : int[] = aten::size(%bottleneck_output.28) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%7854, %24) # torch/nn/functional.py:1991:17
      %7856 : int = aten::len(%7854) # torch/nn/functional.py:1992:19
      %7857 : int = aten::sub(%7856, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%7857, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %7861 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %7862 : int = aten::__getitem__(%7854, %7861) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %7862) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %7864 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7864) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7865 : Tensor = aten::batch_norm(%bottleneck_output.28, %7852, %7853, %7850, %7851, %7849, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.30 : Tensor = aten::relu_(%7865) # torch/nn/functional.py:1117:17
  %7867 : Tensor = prim::GetAttr[name="weight"](%7842)
  %7868 : Tensor? = prim::GetAttr[name="bias"](%7842)
  %7869 : int[] = prim::ListConstruct(%27, %27)
  %7870 : int[] = prim::ListConstruct(%27, %27)
  %7871 : int[] = prim::ListConstruct(%27, %27)
  %new_features.30 : Tensor = aten::conv2d(%result.30, %7867, %7868, %7869, %7870, %7871, %27) # torch/nn/modules/conv.py:415:15
  %7873 : float = prim::GetAttr[name="drop_rate"](%6534)
  %7874 : bool = aten::gt(%7873, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.27 : Tensor = prim::If(%7874) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7876 : float = prim::GetAttr[name="drop_rate"](%6534)
      %7877 : bool = prim::GetAttr[name="training"](%6534)
      %7878 : bool = aten::lt(%7876, %16) # torch/nn/functional.py:968:7
      %7879 : bool = prim::If(%7878) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7880 : bool = aten::gt(%7876, %17) # torch/nn/functional.py:968:17
          -> (%7880)
       = prim::If(%7879) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7881 : Tensor = aten::dropout(%new_features.30, %7876, %7877) # torch/nn/functional.py:973:17
      -> (%7881)
    block1():
      -> (%new_features.30)
  %7882 : Tensor[] = aten::append(%features.1, %new_features.27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7883 : Tensor = prim::Uninitialized()
  %7884 : bool = prim::GetAttr[name="memory_efficient"](%6535)
  %7885 : bool = prim::If(%7884) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7886 : bool = prim::Uninitialized()
      %7887 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7888 : bool = aten::gt(%7887, %24)
      %7889 : bool, %7890 : bool, %7891 : int = prim::Loop(%18, %7888, %19, %7886, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7892 : int, %7893 : bool, %7894 : bool, %7895 : int):
          %tensor.16 : Tensor = aten::__getitem__(%features.1, %7895) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7897 : bool = prim::requires_grad(%tensor.16)
          %7898 : bool, %7899 : bool = prim::If(%7897) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7886)
          %7900 : int = aten::add(%7895, %27)
          %7901 : bool = aten::lt(%7900, %7887)
          %7902 : bool = aten::__and__(%7901, %7898)
          -> (%7902, %7897, %7899, %7900)
      %7903 : bool = prim::If(%7889)
        block0():
          -> (%7890)
        block1():
          -> (%19)
      -> (%7903)
    block1():
      -> (%19)
  %bottleneck_output.30 : Tensor = prim::If(%7885) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7883)
    block1():
      %concated_features.16 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7906 : __torch__.torch.nn.modules.conv.___torch_mangle_328.Conv2d = prim::GetAttr[name="conv1"](%6535)
      %7907 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_237.BatchNorm2d = prim::GetAttr[name="norm1"](%6535)
      %7908 : int = aten::dim(%concated_features.16) # torch/nn/modules/batchnorm.py:276:11
      %7909 : bool = aten::ne(%7908, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7909) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7910 : bool = prim::GetAttr[name="training"](%7907)
       = prim::If(%7910) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7911 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7907)
          %7912 : Tensor = aten::add(%7911, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7907, %7912)
          -> ()
        block1():
          -> ()
      %7913 : bool = prim::GetAttr[name="training"](%7907)
      %7914 : Tensor = prim::GetAttr[name="running_mean"](%7907)
      %7915 : Tensor = prim::GetAttr[name="running_var"](%7907)
      %7916 : Tensor = prim::GetAttr[name="weight"](%7907)
      %7917 : Tensor = prim::GetAttr[name="bias"](%7907)
       = prim::If(%7913) # torch/nn/functional.py:2011:4
        block0():
          %7918 : int[] = aten::size(%concated_features.16) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%7918, %24) # torch/nn/functional.py:1991:17
          %7920 : int = aten::len(%7918) # torch/nn/functional.py:1992:19
          %7921 : int = aten::sub(%7920, %26) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%7921, %25, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %7925 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
              %7926 : int = aten::__getitem__(%7918, %7925) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %7926) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.123)
          %7928 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7928) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7929 : Tensor = aten::batch_norm(%concated_features.16, %7916, %7917, %7914, %7915, %7913, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.31 : Tensor = aten::relu_(%7929) # torch/nn/functional.py:1117:17
      %7931 : Tensor = prim::GetAttr[name="weight"](%7906)
      %7932 : Tensor? = prim::GetAttr[name="bias"](%7906)
      %7933 : int[] = prim::ListConstruct(%27, %27)
      %7934 : int[] = prim::ListConstruct(%24, %24)
      %7935 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.31 : Tensor = aten::conv2d(%result.31, %7931, %7932, %7933, %7934, %7935, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.31)
  %7937 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6535)
  %7938 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6535)
  %7939 : int = aten::dim(%bottleneck_output.30) # torch/nn/modules/batchnorm.py:276:11
  %7940 : bool = aten::ne(%7939, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7940) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7941 : bool = prim::GetAttr[name="training"](%7938)
   = prim::If(%7941) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7942 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7938)
      %7943 : Tensor = aten::add(%7942, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7938, %7943)
      -> ()
    block1():
      -> ()
  %7944 : bool = prim::GetAttr[name="training"](%7938)
  %7945 : Tensor = prim::GetAttr[name="running_mean"](%7938)
  %7946 : Tensor = prim::GetAttr[name="running_var"](%7938)
  %7947 : Tensor = prim::GetAttr[name="weight"](%7938)
  %7948 : Tensor = prim::GetAttr[name="bias"](%7938)
   = prim::If(%7944) # torch/nn/functional.py:2011:4
    block0():
      %7949 : int[] = aten::size(%bottleneck_output.30) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%7949, %24) # torch/nn/functional.py:1991:17
      %7951 : int = aten::len(%7949) # torch/nn/functional.py:1992:19
      %7952 : int = aten::sub(%7951, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%7952, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %7956 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %7957 : int = aten::__getitem__(%7949, %7956) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %7957) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %7959 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7959) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7960 : Tensor = aten::batch_norm(%bottleneck_output.30, %7947, %7948, %7945, %7946, %7944, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.32 : Tensor = aten::relu_(%7960) # torch/nn/functional.py:1117:17
  %7962 : Tensor = prim::GetAttr[name="weight"](%7937)
  %7963 : Tensor? = prim::GetAttr[name="bias"](%7937)
  %7964 : int[] = prim::ListConstruct(%27, %27)
  %7965 : int[] = prim::ListConstruct(%27, %27)
  %7966 : int[] = prim::ListConstruct(%27, %27)
  %new_features.32 : Tensor = aten::conv2d(%result.32, %7962, %7963, %7964, %7965, %7966, %27) # torch/nn/modules/conv.py:415:15
  %7968 : float = prim::GetAttr[name="drop_rate"](%6535)
  %7969 : bool = aten::gt(%7968, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.29 : Tensor = prim::If(%7969) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7971 : float = prim::GetAttr[name="drop_rate"](%6535)
      %7972 : bool = prim::GetAttr[name="training"](%6535)
      %7973 : bool = aten::lt(%7971, %16) # torch/nn/functional.py:968:7
      %7974 : bool = prim::If(%7973) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7975 : bool = aten::gt(%7971, %17) # torch/nn/functional.py:968:17
          -> (%7975)
       = prim::If(%7974) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7976 : Tensor = aten::dropout(%new_features.32, %7971, %7972) # torch/nn/functional.py:973:17
      -> (%7976)
    block1():
      -> (%new_features.32)
  %7977 : Tensor[] = aten::append(%features.1, %new_features.29) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7978 : Tensor = prim::Uninitialized()
  %7979 : bool = prim::GetAttr[name="memory_efficient"](%6536)
  %7980 : bool = prim::If(%7979) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7981 : bool = prim::Uninitialized()
      %7982 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7983 : bool = aten::gt(%7982, %24)
      %7984 : bool, %7985 : bool, %7986 : int = prim::Loop(%18, %7983, %19, %7981, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7987 : int, %7988 : bool, %7989 : bool, %7990 : int):
          %tensor.17 : Tensor = aten::__getitem__(%features.1, %7990) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7992 : bool = prim::requires_grad(%tensor.17)
          %7993 : bool, %7994 : bool = prim::If(%7992) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7981)
          %7995 : int = aten::add(%7990, %27)
          %7996 : bool = aten::lt(%7995, %7982)
          %7997 : bool = aten::__and__(%7996, %7993)
          -> (%7997, %7992, %7994, %7995)
      %7998 : bool = prim::If(%7984)
        block0():
          -> (%7985)
        block1():
          -> (%19)
      -> (%7998)
    block1():
      -> (%19)
  %bottleneck_output.32 : Tensor = prim::If(%7980) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7978)
    block1():
      %concated_features.17 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8001 : __torch__.torch.nn.modules.conv.___torch_mangle_331.Conv2d = prim::GetAttr[name="conv1"](%6536)
      %8002 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_330.BatchNorm2d = prim::GetAttr[name="norm1"](%6536)
      %8003 : int = aten::dim(%concated_features.17) # torch/nn/modules/batchnorm.py:276:11
      %8004 : bool = aten::ne(%8003, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8004) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8005 : bool = prim::GetAttr[name="training"](%8002)
       = prim::If(%8005) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8006 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8002)
          %8007 : Tensor = aten::add(%8006, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8002, %8007)
          -> ()
        block1():
          -> ()
      %8008 : bool = prim::GetAttr[name="training"](%8002)
      %8009 : Tensor = prim::GetAttr[name="running_mean"](%8002)
      %8010 : Tensor = prim::GetAttr[name="running_var"](%8002)
      %8011 : Tensor = prim::GetAttr[name="weight"](%8002)
      %8012 : Tensor = prim::GetAttr[name="bias"](%8002)
       = prim::If(%8008) # torch/nn/functional.py:2011:4
        block0():
          %8013 : int[] = aten::size(%concated_features.17) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%8013, %24) # torch/nn/functional.py:1991:17
          %8015 : int = aten::len(%8013) # torch/nn/functional.py:1992:19
          %8016 : int = aten::sub(%8015, %26) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%8016, %25, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %8020 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
              %8021 : int = aten::__getitem__(%8013, %8020) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %8021) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.131)
          %8023 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8023) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8024 : Tensor = aten::batch_norm(%concated_features.17, %8011, %8012, %8009, %8010, %8008, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.33 : Tensor = aten::relu_(%8024) # torch/nn/functional.py:1117:17
      %8026 : Tensor = prim::GetAttr[name="weight"](%8001)
      %8027 : Tensor? = prim::GetAttr[name="bias"](%8001)
      %8028 : int[] = prim::ListConstruct(%27, %27)
      %8029 : int[] = prim::ListConstruct(%24, %24)
      %8030 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.33 : Tensor = aten::conv2d(%result.33, %8026, %8027, %8028, %8029, %8030, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.33)
  %8032 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6536)
  %8033 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6536)
  %8034 : int = aten::dim(%bottleneck_output.32) # torch/nn/modules/batchnorm.py:276:11
  %8035 : bool = aten::ne(%8034, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8035) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8036 : bool = prim::GetAttr[name="training"](%8033)
   = prim::If(%8036) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8037 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8033)
      %8038 : Tensor = aten::add(%8037, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8033, %8038)
      -> ()
    block1():
      -> ()
  %8039 : bool = prim::GetAttr[name="training"](%8033)
  %8040 : Tensor = prim::GetAttr[name="running_mean"](%8033)
  %8041 : Tensor = prim::GetAttr[name="running_var"](%8033)
  %8042 : Tensor = prim::GetAttr[name="weight"](%8033)
  %8043 : Tensor = prim::GetAttr[name="bias"](%8033)
   = prim::If(%8039) # torch/nn/functional.py:2011:4
    block0():
      %8044 : int[] = aten::size(%bottleneck_output.32) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%8044, %24) # torch/nn/functional.py:1991:17
      %8046 : int = aten::len(%8044) # torch/nn/functional.py:1992:19
      %8047 : int = aten::sub(%8046, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%8047, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %8051 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %8052 : int = aten::__getitem__(%8044, %8051) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %8052) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %8054 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8054) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8055 : Tensor = aten::batch_norm(%bottleneck_output.32, %8042, %8043, %8040, %8041, %8039, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.34 : Tensor = aten::relu_(%8055) # torch/nn/functional.py:1117:17
  %8057 : Tensor = prim::GetAttr[name="weight"](%8032)
  %8058 : Tensor? = prim::GetAttr[name="bias"](%8032)
  %8059 : int[] = prim::ListConstruct(%27, %27)
  %8060 : int[] = prim::ListConstruct(%27, %27)
  %8061 : int[] = prim::ListConstruct(%27, %27)
  %new_features.34 : Tensor = aten::conv2d(%result.34, %8057, %8058, %8059, %8060, %8061, %27) # torch/nn/modules/conv.py:415:15
  %8063 : float = prim::GetAttr[name="drop_rate"](%6536)
  %8064 : bool = aten::gt(%8063, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.31 : Tensor = prim::If(%8064) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8066 : float = prim::GetAttr[name="drop_rate"](%6536)
      %8067 : bool = prim::GetAttr[name="training"](%6536)
      %8068 : bool = aten::lt(%8066, %16) # torch/nn/functional.py:968:7
      %8069 : bool = prim::If(%8068) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8070 : bool = aten::gt(%8066, %17) # torch/nn/functional.py:968:17
          -> (%8070)
       = prim::If(%8069) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8071 : Tensor = aten::dropout(%new_features.34, %8066, %8067) # torch/nn/functional.py:973:17
      -> (%8071)
    block1():
      -> (%new_features.34)
  %8072 : Tensor[] = aten::append(%features.1, %new_features.31) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8073 : Tensor = prim::Uninitialized()
  %8074 : bool = prim::GetAttr[name="memory_efficient"](%6537)
  %8075 : bool = prim::If(%8074) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8076 : bool = prim::Uninitialized()
      %8077 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8078 : bool = aten::gt(%8077, %24)
      %8079 : bool, %8080 : bool, %8081 : int = prim::Loop(%18, %8078, %19, %8076, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8082 : int, %8083 : bool, %8084 : bool, %8085 : int):
          %tensor.18 : Tensor = aten::__getitem__(%features.1, %8085) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8087 : bool = prim::requires_grad(%tensor.18)
          %8088 : bool, %8089 : bool = prim::If(%8087) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8076)
          %8090 : int = aten::add(%8085, %27)
          %8091 : bool = aten::lt(%8090, %8077)
          %8092 : bool = aten::__and__(%8091, %8088)
          -> (%8092, %8087, %8089, %8090)
      %8093 : bool = prim::If(%8079)
        block0():
          -> (%8080)
        block1():
          -> (%19)
      -> (%8093)
    block1():
      -> (%19)
  %bottleneck_output.34 : Tensor = prim::If(%8075) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8073)
    block1():
      %concated_features.18 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8096 : __torch__.torch.nn.modules.conv.___torch_mangle_334.Conv2d = prim::GetAttr[name="conv1"](%6537)
      %8097 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_333.BatchNorm2d = prim::GetAttr[name="norm1"](%6537)
      %8098 : int = aten::dim(%concated_features.18) # torch/nn/modules/batchnorm.py:276:11
      %8099 : bool = aten::ne(%8098, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8099) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8100 : bool = prim::GetAttr[name="training"](%8097)
       = prim::If(%8100) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8101 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8097)
          %8102 : Tensor = aten::add(%8101, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8097, %8102)
          -> ()
        block1():
          -> ()
      %8103 : bool = prim::GetAttr[name="training"](%8097)
      %8104 : Tensor = prim::GetAttr[name="running_mean"](%8097)
      %8105 : Tensor = prim::GetAttr[name="running_var"](%8097)
      %8106 : Tensor = prim::GetAttr[name="weight"](%8097)
      %8107 : Tensor = prim::GetAttr[name="bias"](%8097)
       = prim::If(%8103) # torch/nn/functional.py:2011:4
        block0():
          %8108 : int[] = aten::size(%concated_features.18) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%8108, %24) # torch/nn/functional.py:1991:17
          %8110 : int = aten::len(%8108) # torch/nn/functional.py:1992:19
          %8111 : int = aten::sub(%8110, %26) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%8111, %25, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %8115 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
              %8116 : int = aten::__getitem__(%8108, %8115) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %8116) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.139)
          %8118 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8118) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8119 : Tensor = aten::batch_norm(%concated_features.18, %8106, %8107, %8104, %8105, %8103, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.35 : Tensor = aten::relu_(%8119) # torch/nn/functional.py:1117:17
      %8121 : Tensor = prim::GetAttr[name="weight"](%8096)
      %8122 : Tensor? = prim::GetAttr[name="bias"](%8096)
      %8123 : int[] = prim::ListConstruct(%27, %27)
      %8124 : int[] = prim::ListConstruct(%24, %24)
      %8125 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.35 : Tensor = aten::conv2d(%result.35, %8121, %8122, %8123, %8124, %8125, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.35)
  %8127 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6537)
  %8128 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6537)
  %8129 : int = aten::dim(%bottleneck_output.34) # torch/nn/modules/batchnorm.py:276:11
  %8130 : bool = aten::ne(%8129, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8130) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8131 : bool = prim::GetAttr[name="training"](%8128)
   = prim::If(%8131) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8132 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8128)
      %8133 : Tensor = aten::add(%8132, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8128, %8133)
      -> ()
    block1():
      -> ()
  %8134 : bool = prim::GetAttr[name="training"](%8128)
  %8135 : Tensor = prim::GetAttr[name="running_mean"](%8128)
  %8136 : Tensor = prim::GetAttr[name="running_var"](%8128)
  %8137 : Tensor = prim::GetAttr[name="weight"](%8128)
  %8138 : Tensor = prim::GetAttr[name="bias"](%8128)
   = prim::If(%8134) # torch/nn/functional.py:2011:4
    block0():
      %8139 : int[] = aten::size(%bottleneck_output.34) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%8139, %24) # torch/nn/functional.py:1991:17
      %8141 : int = aten::len(%8139) # torch/nn/functional.py:1992:19
      %8142 : int = aten::sub(%8141, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%8142, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %8146 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %8147 : int = aten::__getitem__(%8139, %8146) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %8147) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %8149 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8149) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8150 : Tensor = aten::batch_norm(%bottleneck_output.34, %8137, %8138, %8135, %8136, %8134, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.36 : Tensor = aten::relu_(%8150) # torch/nn/functional.py:1117:17
  %8152 : Tensor = prim::GetAttr[name="weight"](%8127)
  %8153 : Tensor? = prim::GetAttr[name="bias"](%8127)
  %8154 : int[] = prim::ListConstruct(%27, %27)
  %8155 : int[] = prim::ListConstruct(%27, %27)
  %8156 : int[] = prim::ListConstruct(%27, %27)
  %new_features.36 : Tensor = aten::conv2d(%result.36, %8152, %8153, %8154, %8155, %8156, %27) # torch/nn/modules/conv.py:415:15
  %8158 : float = prim::GetAttr[name="drop_rate"](%6537)
  %8159 : bool = aten::gt(%8158, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.33 : Tensor = prim::If(%8159) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8161 : float = prim::GetAttr[name="drop_rate"](%6537)
      %8162 : bool = prim::GetAttr[name="training"](%6537)
      %8163 : bool = aten::lt(%8161, %16) # torch/nn/functional.py:968:7
      %8164 : bool = prim::If(%8163) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8165 : bool = aten::gt(%8161, %17) # torch/nn/functional.py:968:17
          -> (%8165)
       = prim::If(%8164) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8166 : Tensor = aten::dropout(%new_features.36, %8161, %8162) # torch/nn/functional.py:973:17
      -> (%8166)
    block1():
      -> (%new_features.36)
  %8167 : Tensor[] = aten::append(%features.1, %new_features.33) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8168 : Tensor = prim::Uninitialized()
  %8169 : bool = prim::GetAttr[name="memory_efficient"](%6538)
  %8170 : bool = prim::If(%8169) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8171 : bool = prim::Uninitialized()
      %8172 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8173 : bool = aten::gt(%8172, %24)
      %8174 : bool, %8175 : bool, %8176 : int = prim::Loop(%18, %8173, %19, %8171, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8177 : int, %8178 : bool, %8179 : bool, %8180 : int):
          %tensor.19 : Tensor = aten::__getitem__(%features.1, %8180) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8182 : bool = prim::requires_grad(%tensor.19)
          %8183 : bool, %8184 : bool = prim::If(%8182) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8171)
          %8185 : int = aten::add(%8180, %27)
          %8186 : bool = aten::lt(%8185, %8172)
          %8187 : bool = aten::__and__(%8186, %8183)
          -> (%8187, %8182, %8184, %8185)
      %8188 : bool = prim::If(%8174)
        block0():
          -> (%8175)
        block1():
          -> (%19)
      -> (%8188)
    block1():
      -> (%19)
  %bottleneck_output.36 : Tensor = prim::If(%8170) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8168)
    block1():
      %concated_features.19 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8191 : __torch__.torch.nn.modules.conv.___torch_mangle_336.Conv2d = prim::GetAttr[name="conv1"](%6538)
      %8192 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_243.BatchNorm2d = prim::GetAttr[name="norm1"](%6538)
      %8193 : int = aten::dim(%concated_features.19) # torch/nn/modules/batchnorm.py:276:11
      %8194 : bool = aten::ne(%8193, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8194) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8195 : bool = prim::GetAttr[name="training"](%8192)
       = prim::If(%8195) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8196 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8192)
          %8197 : Tensor = aten::add(%8196, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8192, %8197)
          -> ()
        block1():
          -> ()
      %8198 : bool = prim::GetAttr[name="training"](%8192)
      %8199 : Tensor = prim::GetAttr[name="running_mean"](%8192)
      %8200 : Tensor = prim::GetAttr[name="running_var"](%8192)
      %8201 : Tensor = prim::GetAttr[name="weight"](%8192)
      %8202 : Tensor = prim::GetAttr[name="bias"](%8192)
       = prim::If(%8198) # torch/nn/functional.py:2011:4
        block0():
          %8203 : int[] = aten::size(%concated_features.19) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%8203, %24) # torch/nn/functional.py:1991:17
          %8205 : int = aten::len(%8203) # torch/nn/functional.py:1992:19
          %8206 : int = aten::sub(%8205, %26) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%8206, %25, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %8210 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
              %8211 : int = aten::__getitem__(%8203, %8210) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %8211) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.147)
          %8213 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8213) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8214 : Tensor = aten::batch_norm(%concated_features.19, %8201, %8202, %8199, %8200, %8198, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.37 : Tensor = aten::relu_(%8214) # torch/nn/functional.py:1117:17
      %8216 : Tensor = prim::GetAttr[name="weight"](%8191)
      %8217 : Tensor? = prim::GetAttr[name="bias"](%8191)
      %8218 : int[] = prim::ListConstruct(%27, %27)
      %8219 : int[] = prim::ListConstruct(%24, %24)
      %8220 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.37 : Tensor = aten::conv2d(%result.37, %8216, %8217, %8218, %8219, %8220, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.37)
  %8222 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6538)
  %8223 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6538)
  %8224 : int = aten::dim(%bottleneck_output.36) # torch/nn/modules/batchnorm.py:276:11
  %8225 : bool = aten::ne(%8224, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8225) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8226 : bool = prim::GetAttr[name="training"](%8223)
   = prim::If(%8226) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8227 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8223)
      %8228 : Tensor = aten::add(%8227, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8223, %8228)
      -> ()
    block1():
      -> ()
  %8229 : bool = prim::GetAttr[name="training"](%8223)
  %8230 : Tensor = prim::GetAttr[name="running_mean"](%8223)
  %8231 : Tensor = prim::GetAttr[name="running_var"](%8223)
  %8232 : Tensor = prim::GetAttr[name="weight"](%8223)
  %8233 : Tensor = prim::GetAttr[name="bias"](%8223)
   = prim::If(%8229) # torch/nn/functional.py:2011:4
    block0():
      %8234 : int[] = aten::size(%bottleneck_output.36) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%8234, %24) # torch/nn/functional.py:1991:17
      %8236 : int = aten::len(%8234) # torch/nn/functional.py:1992:19
      %8237 : int = aten::sub(%8236, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%8237, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %8241 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %8242 : int = aten::__getitem__(%8234, %8241) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %8242) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %8244 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8244) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8245 : Tensor = aten::batch_norm(%bottleneck_output.36, %8232, %8233, %8230, %8231, %8229, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.38 : Tensor = aten::relu_(%8245) # torch/nn/functional.py:1117:17
  %8247 : Tensor = prim::GetAttr[name="weight"](%8222)
  %8248 : Tensor? = prim::GetAttr[name="bias"](%8222)
  %8249 : int[] = prim::ListConstruct(%27, %27)
  %8250 : int[] = prim::ListConstruct(%27, %27)
  %8251 : int[] = prim::ListConstruct(%27, %27)
  %new_features.38 : Tensor = aten::conv2d(%result.38, %8247, %8248, %8249, %8250, %8251, %27) # torch/nn/modules/conv.py:415:15
  %8253 : float = prim::GetAttr[name="drop_rate"](%6538)
  %8254 : bool = aten::gt(%8253, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.35 : Tensor = prim::If(%8254) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8256 : float = prim::GetAttr[name="drop_rate"](%6538)
      %8257 : bool = prim::GetAttr[name="training"](%6538)
      %8258 : bool = aten::lt(%8256, %16) # torch/nn/functional.py:968:7
      %8259 : bool = prim::If(%8258) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8260 : bool = aten::gt(%8256, %17) # torch/nn/functional.py:968:17
          -> (%8260)
       = prim::If(%8259) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8261 : Tensor = aten::dropout(%new_features.38, %8256, %8257) # torch/nn/functional.py:973:17
      -> (%8261)
    block1():
      -> (%new_features.38)
  %8262 : Tensor[] = aten::append(%features.1, %new_features.35) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8263 : Tensor = prim::Uninitialized()
  %8264 : bool = prim::GetAttr[name="memory_efficient"](%6539)
  %8265 : bool = prim::If(%8264) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8266 : bool = prim::Uninitialized()
      %8267 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8268 : bool = aten::gt(%8267, %24)
      %8269 : bool, %8270 : bool, %8271 : int = prim::Loop(%18, %8268, %19, %8266, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8272 : int, %8273 : bool, %8274 : bool, %8275 : int):
          %tensor.20 : Tensor = aten::__getitem__(%features.1, %8275) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8277 : bool = prim::requires_grad(%tensor.20)
          %8278 : bool, %8279 : bool = prim::If(%8277) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8266)
          %8280 : int = aten::add(%8275, %27)
          %8281 : bool = aten::lt(%8280, %8267)
          %8282 : bool = aten::__and__(%8281, %8278)
          -> (%8282, %8277, %8279, %8280)
      %8283 : bool = prim::If(%8269)
        block0():
          -> (%8270)
        block1():
          -> (%19)
      -> (%8283)
    block1():
      -> (%19)
  %bottleneck_output.38 : Tensor = prim::If(%8265) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8263)
    block1():
      %concated_features.20 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8286 : __torch__.torch.nn.modules.conv.___torch_mangle_339.Conv2d = prim::GetAttr[name="conv1"](%6539)
      %8287 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_338.BatchNorm2d = prim::GetAttr[name="norm1"](%6539)
      %8288 : int = aten::dim(%concated_features.20) # torch/nn/modules/batchnorm.py:276:11
      %8289 : bool = aten::ne(%8288, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8289) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8290 : bool = prim::GetAttr[name="training"](%8287)
       = prim::If(%8290) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8291 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8287)
          %8292 : Tensor = aten::add(%8291, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8287, %8292)
          -> ()
        block1():
          -> ()
      %8293 : bool = prim::GetAttr[name="training"](%8287)
      %8294 : Tensor = prim::GetAttr[name="running_mean"](%8287)
      %8295 : Tensor = prim::GetAttr[name="running_var"](%8287)
      %8296 : Tensor = prim::GetAttr[name="weight"](%8287)
      %8297 : Tensor = prim::GetAttr[name="bias"](%8287)
       = prim::If(%8293) # torch/nn/functional.py:2011:4
        block0():
          %8298 : int[] = aten::size(%concated_features.20) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%8298, %24) # torch/nn/functional.py:1991:17
          %8300 : int = aten::len(%8298) # torch/nn/functional.py:1992:19
          %8301 : int = aten::sub(%8300, %26) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%8301, %25, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %8305 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
              %8306 : int = aten::__getitem__(%8298, %8305) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %8306) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.155)
          %8308 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8308) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8309 : Tensor = aten::batch_norm(%concated_features.20, %8296, %8297, %8294, %8295, %8293, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.39 : Tensor = aten::relu_(%8309) # torch/nn/functional.py:1117:17
      %8311 : Tensor = prim::GetAttr[name="weight"](%8286)
      %8312 : Tensor? = prim::GetAttr[name="bias"](%8286)
      %8313 : int[] = prim::ListConstruct(%27, %27)
      %8314 : int[] = prim::ListConstruct(%24, %24)
      %8315 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.39 : Tensor = aten::conv2d(%result.39, %8311, %8312, %8313, %8314, %8315, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.39)
  %8317 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6539)
  %8318 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6539)
  %8319 : int = aten::dim(%bottleneck_output.38) # torch/nn/modules/batchnorm.py:276:11
  %8320 : bool = aten::ne(%8319, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8320) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8321 : bool = prim::GetAttr[name="training"](%8318)
   = prim::If(%8321) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8322 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8318)
      %8323 : Tensor = aten::add(%8322, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8318, %8323)
      -> ()
    block1():
      -> ()
  %8324 : bool = prim::GetAttr[name="training"](%8318)
  %8325 : Tensor = prim::GetAttr[name="running_mean"](%8318)
  %8326 : Tensor = prim::GetAttr[name="running_var"](%8318)
  %8327 : Tensor = prim::GetAttr[name="weight"](%8318)
  %8328 : Tensor = prim::GetAttr[name="bias"](%8318)
   = prim::If(%8324) # torch/nn/functional.py:2011:4
    block0():
      %8329 : int[] = aten::size(%bottleneck_output.38) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%8329, %24) # torch/nn/functional.py:1991:17
      %8331 : int = aten::len(%8329) # torch/nn/functional.py:1992:19
      %8332 : int = aten::sub(%8331, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%8332, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %8336 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %8337 : int = aten::__getitem__(%8329, %8336) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %8337) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %8339 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8339) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8340 : Tensor = aten::batch_norm(%bottleneck_output.38, %8327, %8328, %8325, %8326, %8324, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.40 : Tensor = aten::relu_(%8340) # torch/nn/functional.py:1117:17
  %8342 : Tensor = prim::GetAttr[name="weight"](%8317)
  %8343 : Tensor? = prim::GetAttr[name="bias"](%8317)
  %8344 : int[] = prim::ListConstruct(%27, %27)
  %8345 : int[] = prim::ListConstruct(%27, %27)
  %8346 : int[] = prim::ListConstruct(%27, %27)
  %new_features.40 : Tensor = aten::conv2d(%result.40, %8342, %8343, %8344, %8345, %8346, %27) # torch/nn/modules/conv.py:415:15
  %8348 : float = prim::GetAttr[name="drop_rate"](%6539)
  %8349 : bool = aten::gt(%8348, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.37 : Tensor = prim::If(%8349) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8351 : float = prim::GetAttr[name="drop_rate"](%6539)
      %8352 : bool = prim::GetAttr[name="training"](%6539)
      %8353 : bool = aten::lt(%8351, %16) # torch/nn/functional.py:968:7
      %8354 : bool = prim::If(%8353) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8355 : bool = aten::gt(%8351, %17) # torch/nn/functional.py:968:17
          -> (%8355)
       = prim::If(%8354) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8356 : Tensor = aten::dropout(%new_features.40, %8351, %8352) # torch/nn/functional.py:973:17
      -> (%8356)
    block1():
      -> (%new_features.40)
  %8357 : Tensor[] = aten::append(%features.1, %new_features.37) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8358 : Tensor = prim::Uninitialized()
  %8359 : bool = prim::GetAttr[name="memory_efficient"](%6540)
  %8360 : bool = prim::If(%8359) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8361 : bool = prim::Uninitialized()
      %8362 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8363 : bool = aten::gt(%8362, %24)
      %8364 : bool, %8365 : bool, %8366 : int = prim::Loop(%18, %8363, %19, %8361, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8367 : int, %8368 : bool, %8369 : bool, %8370 : int):
          %tensor.21 : Tensor = aten::__getitem__(%features.1, %8370) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8372 : bool = prim::requires_grad(%tensor.21)
          %8373 : bool, %8374 : bool = prim::If(%8372) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8361)
          %8375 : int = aten::add(%8370, %27)
          %8376 : bool = aten::lt(%8375, %8362)
          %8377 : bool = aten::__and__(%8376, %8373)
          -> (%8377, %8372, %8374, %8375)
      %8378 : bool = prim::If(%8364)
        block0():
          -> (%8365)
        block1():
          -> (%19)
      -> (%8378)
    block1():
      -> (%19)
  %bottleneck_output.40 : Tensor = prim::If(%8360) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8358)
    block1():
      %concated_features.21 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8381 : __torch__.torch.nn.modules.conv.___torch_mangle_342.Conv2d = prim::GetAttr[name="conv1"](%6540)
      %8382 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_341.BatchNorm2d = prim::GetAttr[name="norm1"](%6540)
      %8383 : int = aten::dim(%concated_features.21) # torch/nn/modules/batchnorm.py:276:11
      %8384 : bool = aten::ne(%8383, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8384) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8385 : bool = prim::GetAttr[name="training"](%8382)
       = prim::If(%8385) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8386 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8382)
          %8387 : Tensor = aten::add(%8386, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8382, %8387)
          -> ()
        block1():
          -> ()
      %8388 : bool = prim::GetAttr[name="training"](%8382)
      %8389 : Tensor = prim::GetAttr[name="running_mean"](%8382)
      %8390 : Tensor = prim::GetAttr[name="running_var"](%8382)
      %8391 : Tensor = prim::GetAttr[name="weight"](%8382)
      %8392 : Tensor = prim::GetAttr[name="bias"](%8382)
       = prim::If(%8388) # torch/nn/functional.py:2011:4
        block0():
          %8393 : int[] = aten::size(%concated_features.21) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%8393, %24) # torch/nn/functional.py:1991:17
          %8395 : int = aten::len(%8393) # torch/nn/functional.py:1992:19
          %8396 : int = aten::sub(%8395, %26) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%8396, %25, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %8400 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
              %8401 : int = aten::__getitem__(%8393, %8400) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %8401) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.163)
          %8403 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8403) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8404 : Tensor = aten::batch_norm(%concated_features.21, %8391, %8392, %8389, %8390, %8388, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.41 : Tensor = aten::relu_(%8404) # torch/nn/functional.py:1117:17
      %8406 : Tensor = prim::GetAttr[name="weight"](%8381)
      %8407 : Tensor? = prim::GetAttr[name="bias"](%8381)
      %8408 : int[] = prim::ListConstruct(%27, %27)
      %8409 : int[] = prim::ListConstruct(%24, %24)
      %8410 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.41 : Tensor = aten::conv2d(%result.41, %8406, %8407, %8408, %8409, %8410, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.41)
  %8412 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6540)
  %8413 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6540)
  %8414 : int = aten::dim(%bottleneck_output.40) # torch/nn/modules/batchnorm.py:276:11
  %8415 : bool = aten::ne(%8414, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8415) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8416 : bool = prim::GetAttr[name="training"](%8413)
   = prim::If(%8416) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8417 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8413)
      %8418 : Tensor = aten::add(%8417, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8413, %8418)
      -> ()
    block1():
      -> ()
  %8419 : bool = prim::GetAttr[name="training"](%8413)
  %8420 : Tensor = prim::GetAttr[name="running_mean"](%8413)
  %8421 : Tensor = prim::GetAttr[name="running_var"](%8413)
  %8422 : Tensor = prim::GetAttr[name="weight"](%8413)
  %8423 : Tensor = prim::GetAttr[name="bias"](%8413)
   = prim::If(%8419) # torch/nn/functional.py:2011:4
    block0():
      %8424 : int[] = aten::size(%bottleneck_output.40) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%8424, %24) # torch/nn/functional.py:1991:17
      %8426 : int = aten::len(%8424) # torch/nn/functional.py:1992:19
      %8427 : int = aten::sub(%8426, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%8427, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %8431 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %8432 : int = aten::__getitem__(%8424, %8431) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %8432) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %8434 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8434) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8435 : Tensor = aten::batch_norm(%bottleneck_output.40, %8422, %8423, %8420, %8421, %8419, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.42 : Tensor = aten::relu_(%8435) # torch/nn/functional.py:1117:17
  %8437 : Tensor = prim::GetAttr[name="weight"](%8412)
  %8438 : Tensor? = prim::GetAttr[name="bias"](%8412)
  %8439 : int[] = prim::ListConstruct(%27, %27)
  %8440 : int[] = prim::ListConstruct(%27, %27)
  %8441 : int[] = prim::ListConstruct(%27, %27)
  %new_features.42 : Tensor = aten::conv2d(%result.42, %8437, %8438, %8439, %8440, %8441, %27) # torch/nn/modules/conv.py:415:15
  %8443 : float = prim::GetAttr[name="drop_rate"](%6540)
  %8444 : bool = aten::gt(%8443, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.39 : Tensor = prim::If(%8444) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8446 : float = prim::GetAttr[name="drop_rate"](%6540)
      %8447 : bool = prim::GetAttr[name="training"](%6540)
      %8448 : bool = aten::lt(%8446, %16) # torch/nn/functional.py:968:7
      %8449 : bool = prim::If(%8448) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8450 : bool = aten::gt(%8446, %17) # torch/nn/functional.py:968:17
          -> (%8450)
       = prim::If(%8449) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8451 : Tensor = aten::dropout(%new_features.42, %8446, %8447) # torch/nn/functional.py:973:17
      -> (%8451)
    block1():
      -> (%new_features.42)
  %8452 : Tensor[] = aten::append(%features.1, %new_features.39) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8453 : Tensor = prim::Uninitialized()
  %8454 : bool = prim::GetAttr[name="memory_efficient"](%6541)
  %8455 : bool = prim::If(%8454) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8456 : bool = prim::Uninitialized()
      %8457 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8458 : bool = aten::gt(%8457, %24)
      %8459 : bool, %8460 : bool, %8461 : int = prim::Loop(%18, %8458, %19, %8456, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8462 : int, %8463 : bool, %8464 : bool, %8465 : int):
          %tensor.22 : Tensor = aten::__getitem__(%features.1, %8465) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8467 : bool = prim::requires_grad(%tensor.22)
          %8468 : bool, %8469 : bool = prim::If(%8467) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8456)
          %8470 : int = aten::add(%8465, %27)
          %8471 : bool = aten::lt(%8470, %8457)
          %8472 : bool = aten::__and__(%8471, %8468)
          -> (%8472, %8467, %8469, %8470)
      %8473 : bool = prim::If(%8459)
        block0():
          -> (%8460)
        block1():
          -> (%19)
      -> (%8473)
    block1():
      -> (%19)
  %bottleneck_output.42 : Tensor = prim::If(%8455) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8453)
    block1():
      %concated_features.22 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8476 : __torch__.torch.nn.modules.conv.___torch_mangle_344.Conv2d = prim::GetAttr[name="conv1"](%6541)
      %8477 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_249.BatchNorm2d = prim::GetAttr[name="norm1"](%6541)
      %8478 : int = aten::dim(%concated_features.22) # torch/nn/modules/batchnorm.py:276:11
      %8479 : bool = aten::ne(%8478, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8479) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8480 : bool = prim::GetAttr[name="training"](%8477)
       = prim::If(%8480) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8481 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8477)
          %8482 : Tensor = aten::add(%8481, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8477, %8482)
          -> ()
        block1():
          -> ()
      %8483 : bool = prim::GetAttr[name="training"](%8477)
      %8484 : Tensor = prim::GetAttr[name="running_mean"](%8477)
      %8485 : Tensor = prim::GetAttr[name="running_var"](%8477)
      %8486 : Tensor = prim::GetAttr[name="weight"](%8477)
      %8487 : Tensor = prim::GetAttr[name="bias"](%8477)
       = prim::If(%8483) # torch/nn/functional.py:2011:4
        block0():
          %8488 : int[] = aten::size(%concated_features.22) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%8488, %24) # torch/nn/functional.py:1991:17
          %8490 : int = aten::len(%8488) # torch/nn/functional.py:1992:19
          %8491 : int = aten::sub(%8490, %26) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%8491, %25, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %8495 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
              %8496 : int = aten::__getitem__(%8488, %8495) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %8496) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.171)
          %8498 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8498) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8499 : Tensor = aten::batch_norm(%concated_features.22, %8486, %8487, %8484, %8485, %8483, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.43 : Tensor = aten::relu_(%8499) # torch/nn/functional.py:1117:17
      %8501 : Tensor = prim::GetAttr[name="weight"](%8476)
      %8502 : Tensor? = prim::GetAttr[name="bias"](%8476)
      %8503 : int[] = prim::ListConstruct(%27, %27)
      %8504 : int[] = prim::ListConstruct(%24, %24)
      %8505 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.43 : Tensor = aten::conv2d(%result.43, %8501, %8502, %8503, %8504, %8505, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.43)
  %8507 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6541)
  %8508 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6541)
  %8509 : int = aten::dim(%bottleneck_output.42) # torch/nn/modules/batchnorm.py:276:11
  %8510 : bool = aten::ne(%8509, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8510) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8511 : bool = prim::GetAttr[name="training"](%8508)
   = prim::If(%8511) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8512 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8508)
      %8513 : Tensor = aten::add(%8512, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8508, %8513)
      -> ()
    block1():
      -> ()
  %8514 : bool = prim::GetAttr[name="training"](%8508)
  %8515 : Tensor = prim::GetAttr[name="running_mean"](%8508)
  %8516 : Tensor = prim::GetAttr[name="running_var"](%8508)
  %8517 : Tensor = prim::GetAttr[name="weight"](%8508)
  %8518 : Tensor = prim::GetAttr[name="bias"](%8508)
   = prim::If(%8514) # torch/nn/functional.py:2011:4
    block0():
      %8519 : int[] = aten::size(%bottleneck_output.42) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%8519, %24) # torch/nn/functional.py:1991:17
      %8521 : int = aten::len(%8519) # torch/nn/functional.py:1992:19
      %8522 : int = aten::sub(%8521, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%8522, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %8526 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %8527 : int = aten::__getitem__(%8519, %8526) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %8527) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %8529 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8529) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8530 : Tensor = aten::batch_norm(%bottleneck_output.42, %8517, %8518, %8515, %8516, %8514, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.44 : Tensor = aten::relu_(%8530) # torch/nn/functional.py:1117:17
  %8532 : Tensor = prim::GetAttr[name="weight"](%8507)
  %8533 : Tensor? = prim::GetAttr[name="bias"](%8507)
  %8534 : int[] = prim::ListConstruct(%27, %27)
  %8535 : int[] = prim::ListConstruct(%27, %27)
  %8536 : int[] = prim::ListConstruct(%27, %27)
  %new_features.44 : Tensor = aten::conv2d(%result.44, %8532, %8533, %8534, %8535, %8536, %27) # torch/nn/modules/conv.py:415:15
  %8538 : float = prim::GetAttr[name="drop_rate"](%6541)
  %8539 : bool = aten::gt(%8538, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.41 : Tensor = prim::If(%8539) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8541 : float = prim::GetAttr[name="drop_rate"](%6541)
      %8542 : bool = prim::GetAttr[name="training"](%6541)
      %8543 : bool = aten::lt(%8541, %16) # torch/nn/functional.py:968:7
      %8544 : bool = prim::If(%8543) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8545 : bool = aten::gt(%8541, %17) # torch/nn/functional.py:968:17
          -> (%8545)
       = prim::If(%8544) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8546 : Tensor = aten::dropout(%new_features.44, %8541, %8542) # torch/nn/functional.py:973:17
      -> (%8546)
    block1():
      -> (%new_features.44)
  %8547 : Tensor[] = aten::append(%features.1, %new_features.41) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8548 : Tensor = prim::Uninitialized()
  %8549 : bool = prim::GetAttr[name="memory_efficient"](%6542)
  %8550 : bool = prim::If(%8549) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8551 : bool = prim::Uninitialized()
      %8552 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8553 : bool = aten::gt(%8552, %24)
      %8554 : bool, %8555 : bool, %8556 : int = prim::Loop(%18, %8553, %19, %8551, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8557 : int, %8558 : bool, %8559 : bool, %8560 : int):
          %tensor.23 : Tensor = aten::__getitem__(%features.1, %8560) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8562 : bool = prim::requires_grad(%tensor.23)
          %8563 : bool, %8564 : bool = prim::If(%8562) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8551)
          %8565 : int = aten::add(%8560, %27)
          %8566 : bool = aten::lt(%8565, %8552)
          %8567 : bool = aten::__and__(%8566, %8563)
          -> (%8567, %8562, %8564, %8565)
      %8568 : bool = prim::If(%8554)
        block0():
          -> (%8555)
        block1():
          -> (%19)
      -> (%8568)
    block1():
      -> (%19)
  %bottleneck_output.44 : Tensor = prim::If(%8550) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8548)
    block1():
      %concated_features.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8571 : __torch__.torch.nn.modules.conv.___torch_mangle_347.Conv2d = prim::GetAttr[name="conv1"](%6542)
      %8572 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_346.BatchNorm2d = prim::GetAttr[name="norm1"](%6542)
      %8573 : int = aten::dim(%concated_features.23) # torch/nn/modules/batchnorm.py:276:11
      %8574 : bool = aten::ne(%8573, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8574) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8575 : bool = prim::GetAttr[name="training"](%8572)
       = prim::If(%8575) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8576 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8572)
          %8577 : Tensor = aten::add(%8576, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8572, %8577)
          -> ()
        block1():
          -> ()
      %8578 : bool = prim::GetAttr[name="training"](%8572)
      %8579 : Tensor = prim::GetAttr[name="running_mean"](%8572)
      %8580 : Tensor = prim::GetAttr[name="running_var"](%8572)
      %8581 : Tensor = prim::GetAttr[name="weight"](%8572)
      %8582 : Tensor = prim::GetAttr[name="bias"](%8572)
       = prim::If(%8578) # torch/nn/functional.py:2011:4
        block0():
          %8583 : int[] = aten::size(%concated_features.23) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%8583, %24) # torch/nn/functional.py:1991:17
          %8585 : int = aten::len(%8583) # torch/nn/functional.py:1992:19
          %8586 : int = aten::sub(%8585, %26) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%8586, %25, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %8590 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
              %8591 : int = aten::__getitem__(%8583, %8590) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %8591) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.179)
          %8593 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8593) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8594 : Tensor = aten::batch_norm(%concated_features.23, %8581, %8582, %8579, %8580, %8578, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.45 : Tensor = aten::relu_(%8594) # torch/nn/functional.py:1117:17
      %8596 : Tensor = prim::GetAttr[name="weight"](%8571)
      %8597 : Tensor? = prim::GetAttr[name="bias"](%8571)
      %8598 : int[] = prim::ListConstruct(%27, %27)
      %8599 : int[] = prim::ListConstruct(%24, %24)
      %8600 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.45 : Tensor = aten::conv2d(%result.45, %8596, %8597, %8598, %8599, %8600, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.45)
  %8602 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6542)
  %8603 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6542)
  %8604 : int = aten::dim(%bottleneck_output.44) # torch/nn/modules/batchnorm.py:276:11
  %8605 : bool = aten::ne(%8604, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8605) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8606 : bool = prim::GetAttr[name="training"](%8603)
   = prim::If(%8606) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8607 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8603)
      %8608 : Tensor = aten::add(%8607, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8603, %8608)
      -> ()
    block1():
      -> ()
  %8609 : bool = prim::GetAttr[name="training"](%8603)
  %8610 : Tensor = prim::GetAttr[name="running_mean"](%8603)
  %8611 : Tensor = prim::GetAttr[name="running_var"](%8603)
  %8612 : Tensor = prim::GetAttr[name="weight"](%8603)
  %8613 : Tensor = prim::GetAttr[name="bias"](%8603)
   = prim::If(%8609) # torch/nn/functional.py:2011:4
    block0():
      %8614 : int[] = aten::size(%bottleneck_output.44) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%8614, %24) # torch/nn/functional.py:1991:17
      %8616 : int = aten::len(%8614) # torch/nn/functional.py:1992:19
      %8617 : int = aten::sub(%8616, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%8617, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %8621 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %8622 : int = aten::__getitem__(%8614, %8621) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %8622) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %8624 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8624) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8625 : Tensor = aten::batch_norm(%bottleneck_output.44, %8612, %8613, %8610, %8611, %8609, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.46 : Tensor = aten::relu_(%8625) # torch/nn/functional.py:1117:17
  %8627 : Tensor = prim::GetAttr[name="weight"](%8602)
  %8628 : Tensor? = prim::GetAttr[name="bias"](%8602)
  %8629 : int[] = prim::ListConstruct(%27, %27)
  %8630 : int[] = prim::ListConstruct(%27, %27)
  %8631 : int[] = prim::ListConstruct(%27, %27)
  %new_features.46 : Tensor = aten::conv2d(%result.46, %8627, %8628, %8629, %8630, %8631, %27) # torch/nn/modules/conv.py:415:15
  %8633 : float = prim::GetAttr[name="drop_rate"](%6542)
  %8634 : bool = aten::gt(%8633, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.43 : Tensor = prim::If(%8634) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8636 : float = prim::GetAttr[name="drop_rate"](%6542)
      %8637 : bool = prim::GetAttr[name="training"](%6542)
      %8638 : bool = aten::lt(%8636, %16) # torch/nn/functional.py:968:7
      %8639 : bool = prim::If(%8638) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8640 : bool = aten::gt(%8636, %17) # torch/nn/functional.py:968:17
          -> (%8640)
       = prim::If(%8639) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8641 : Tensor = aten::dropout(%new_features.46, %8636, %8637) # torch/nn/functional.py:973:17
      -> (%8641)
    block1():
      -> (%new_features.46)
  %8642 : Tensor[] = aten::append(%features.1, %new_features.43) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8643 : Tensor = prim::Uninitialized()
  %8644 : bool = prim::GetAttr[name="memory_efficient"](%6543)
  %8645 : bool = prim::If(%8644) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8646 : bool = prim::Uninitialized()
      %8647 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8648 : bool = aten::gt(%8647, %24)
      %8649 : bool, %8650 : bool, %8651 : int = prim::Loop(%18, %8648, %19, %8646, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8652 : int, %8653 : bool, %8654 : bool, %8655 : int):
          %tensor.24 : Tensor = aten::__getitem__(%features.1, %8655) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8657 : bool = prim::requires_grad(%tensor.24)
          %8658 : bool, %8659 : bool = prim::If(%8657) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8646)
          %8660 : int = aten::add(%8655, %27)
          %8661 : bool = aten::lt(%8660, %8647)
          %8662 : bool = aten::__and__(%8661, %8658)
          -> (%8662, %8657, %8659, %8660)
      %8663 : bool = prim::If(%8649)
        block0():
          -> (%8650)
        block1():
          -> (%19)
      -> (%8663)
    block1():
      -> (%19)
  %bottleneck_output.46 : Tensor = prim::If(%8645) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8643)
    block1():
      %concated_features.24 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8666 : __torch__.torch.nn.modules.conv.___torch_mangle_350.Conv2d = prim::GetAttr[name="conv1"](%6543)
      %8667 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_349.BatchNorm2d = prim::GetAttr[name="norm1"](%6543)
      %8668 : int = aten::dim(%concated_features.24) # torch/nn/modules/batchnorm.py:276:11
      %8669 : bool = aten::ne(%8668, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8669) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8670 : bool = prim::GetAttr[name="training"](%8667)
       = prim::If(%8670) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8671 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8667)
          %8672 : Tensor = aten::add(%8671, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8667, %8672)
          -> ()
        block1():
          -> ()
      %8673 : bool = prim::GetAttr[name="training"](%8667)
      %8674 : Tensor = prim::GetAttr[name="running_mean"](%8667)
      %8675 : Tensor = prim::GetAttr[name="running_var"](%8667)
      %8676 : Tensor = prim::GetAttr[name="weight"](%8667)
      %8677 : Tensor = prim::GetAttr[name="bias"](%8667)
       = prim::If(%8673) # torch/nn/functional.py:2011:4
        block0():
          %8678 : int[] = aten::size(%concated_features.24) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%8678, %24) # torch/nn/functional.py:1991:17
          %8680 : int = aten::len(%8678) # torch/nn/functional.py:1992:19
          %8681 : int = aten::sub(%8680, %26) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%8681, %25, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %8685 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
              %8686 : int = aten::__getitem__(%8678, %8685) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %8686) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.187)
          %8688 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8688) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8689 : Tensor = aten::batch_norm(%concated_features.24, %8676, %8677, %8674, %8675, %8673, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.47 : Tensor = aten::relu_(%8689) # torch/nn/functional.py:1117:17
      %8691 : Tensor = prim::GetAttr[name="weight"](%8666)
      %8692 : Tensor? = prim::GetAttr[name="bias"](%8666)
      %8693 : int[] = prim::ListConstruct(%27, %27)
      %8694 : int[] = prim::ListConstruct(%24, %24)
      %8695 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.47 : Tensor = aten::conv2d(%result.47, %8691, %8692, %8693, %8694, %8695, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.47)
  %8697 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6543)
  %8698 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6543)
  %8699 : int = aten::dim(%bottleneck_output.46) # torch/nn/modules/batchnorm.py:276:11
  %8700 : bool = aten::ne(%8699, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8700) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8701 : bool = prim::GetAttr[name="training"](%8698)
   = prim::If(%8701) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8702 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8698)
      %8703 : Tensor = aten::add(%8702, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8698, %8703)
      -> ()
    block1():
      -> ()
  %8704 : bool = prim::GetAttr[name="training"](%8698)
  %8705 : Tensor = prim::GetAttr[name="running_mean"](%8698)
  %8706 : Tensor = prim::GetAttr[name="running_var"](%8698)
  %8707 : Tensor = prim::GetAttr[name="weight"](%8698)
  %8708 : Tensor = prim::GetAttr[name="bias"](%8698)
   = prim::If(%8704) # torch/nn/functional.py:2011:4
    block0():
      %8709 : int[] = aten::size(%bottleneck_output.46) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%8709, %24) # torch/nn/functional.py:1991:17
      %8711 : int = aten::len(%8709) # torch/nn/functional.py:1992:19
      %8712 : int = aten::sub(%8711, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%8712, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %8716 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %8717 : int = aten::__getitem__(%8709, %8716) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %8717) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %8719 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8719) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8720 : Tensor = aten::batch_norm(%bottleneck_output.46, %8707, %8708, %8705, %8706, %8704, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.48 : Tensor = aten::relu_(%8720) # torch/nn/functional.py:1117:17
  %8722 : Tensor = prim::GetAttr[name="weight"](%8697)
  %8723 : Tensor? = prim::GetAttr[name="bias"](%8697)
  %8724 : int[] = prim::ListConstruct(%27, %27)
  %8725 : int[] = prim::ListConstruct(%27, %27)
  %8726 : int[] = prim::ListConstruct(%27, %27)
  %new_features.48 : Tensor = aten::conv2d(%result.48, %8722, %8723, %8724, %8725, %8726, %27) # torch/nn/modules/conv.py:415:15
  %8728 : float = prim::GetAttr[name="drop_rate"](%6543)
  %8729 : bool = aten::gt(%8728, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.45 : Tensor = prim::If(%8729) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8731 : float = prim::GetAttr[name="drop_rate"](%6543)
      %8732 : bool = prim::GetAttr[name="training"](%6543)
      %8733 : bool = aten::lt(%8731, %16) # torch/nn/functional.py:968:7
      %8734 : bool = prim::If(%8733) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8735 : bool = aten::gt(%8731, %17) # torch/nn/functional.py:968:17
          -> (%8735)
       = prim::If(%8734) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8736 : Tensor = aten::dropout(%new_features.48, %8731, %8732) # torch/nn/functional.py:973:17
      -> (%8736)
    block1():
      -> (%new_features.48)
  %8737 : Tensor[] = aten::append(%features.1, %new_features.45) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8738 : Tensor = prim::Uninitialized()
  %8739 : bool = prim::GetAttr[name="memory_efficient"](%6544)
  %8740 : bool = prim::If(%8739) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8741 : bool = prim::Uninitialized()
      %8742 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8743 : bool = aten::gt(%8742, %24)
      %8744 : bool, %8745 : bool, %8746 : int = prim::Loop(%18, %8743, %19, %8741, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8747 : int, %8748 : bool, %8749 : bool, %8750 : int):
          %tensor.25 : Tensor = aten::__getitem__(%features.1, %8750) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8752 : bool = prim::requires_grad(%tensor.25)
          %8753 : bool, %8754 : bool = prim::If(%8752) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8741)
          %8755 : int = aten::add(%8750, %27)
          %8756 : bool = aten::lt(%8755, %8742)
          %8757 : bool = aten::__and__(%8756, %8753)
          -> (%8757, %8752, %8754, %8755)
      %8758 : bool = prim::If(%8744)
        block0():
          -> (%8745)
        block1():
          -> (%19)
      -> (%8758)
    block1():
      -> (%19)
  %bottleneck_output.48 : Tensor = prim::If(%8740) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8738)
    block1():
      %concated_features.25 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8761 : __torch__.torch.nn.modules.conv.___torch_mangle_352.Conv2d = prim::GetAttr[name="conv1"](%6544)
      %8762 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_255.BatchNorm2d = prim::GetAttr[name="norm1"](%6544)
      %8763 : int = aten::dim(%concated_features.25) # torch/nn/modules/batchnorm.py:276:11
      %8764 : bool = aten::ne(%8763, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8764) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8765 : bool = prim::GetAttr[name="training"](%8762)
       = prim::If(%8765) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8766 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8762)
          %8767 : Tensor = aten::add(%8766, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8762, %8767)
          -> ()
        block1():
          -> ()
      %8768 : bool = prim::GetAttr[name="training"](%8762)
      %8769 : Tensor = prim::GetAttr[name="running_mean"](%8762)
      %8770 : Tensor = prim::GetAttr[name="running_var"](%8762)
      %8771 : Tensor = prim::GetAttr[name="weight"](%8762)
      %8772 : Tensor = prim::GetAttr[name="bias"](%8762)
       = prim::If(%8768) # torch/nn/functional.py:2011:4
        block0():
          %8773 : int[] = aten::size(%concated_features.25) # torch/nn/functional.py:2012:27
          %size_prods.192 : int = aten::__getitem__(%8773, %24) # torch/nn/functional.py:1991:17
          %8775 : int = aten::len(%8773) # torch/nn/functional.py:1992:19
          %8776 : int = aten::sub(%8775, %26) # torch/nn/functional.py:1992:19
          %size_prods.193 : int = prim::Loop(%8776, %25, %size_prods.192) # torch/nn/functional.py:1992:4
            block0(%i.49 : int, %size_prods.194 : int):
              %8780 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
              %8781 : int = aten::__getitem__(%8773, %8780) # torch/nn/functional.py:1993:22
              %size_prods.195 : int = aten::mul(%size_prods.194, %8781) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.195)
          %8783 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8783) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8784 : Tensor = aten::batch_norm(%concated_features.25, %8771, %8772, %8769, %8770, %8768, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.49 : Tensor = aten::relu_(%8784) # torch/nn/functional.py:1117:17
      %8786 : Tensor = prim::GetAttr[name="weight"](%8761)
      %8787 : Tensor? = prim::GetAttr[name="bias"](%8761)
      %8788 : int[] = prim::ListConstruct(%27, %27)
      %8789 : int[] = prim::ListConstruct(%24, %24)
      %8790 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.49 : Tensor = aten::conv2d(%result.49, %8786, %8787, %8788, %8789, %8790, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.49)
  %8792 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6544)
  %8793 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6544)
  %8794 : int = aten::dim(%bottleneck_output.48) # torch/nn/modules/batchnorm.py:276:11
  %8795 : bool = aten::ne(%8794, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8795) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8796 : bool = prim::GetAttr[name="training"](%8793)
   = prim::If(%8796) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8797 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8793)
      %8798 : Tensor = aten::add(%8797, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8793, %8798)
      -> ()
    block1():
      -> ()
  %8799 : bool = prim::GetAttr[name="training"](%8793)
  %8800 : Tensor = prim::GetAttr[name="running_mean"](%8793)
  %8801 : Tensor = prim::GetAttr[name="running_var"](%8793)
  %8802 : Tensor = prim::GetAttr[name="weight"](%8793)
  %8803 : Tensor = prim::GetAttr[name="bias"](%8793)
   = prim::If(%8799) # torch/nn/functional.py:2011:4
    block0():
      %8804 : int[] = aten::size(%bottleneck_output.48) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%8804, %24) # torch/nn/functional.py:1991:17
      %8806 : int = aten::len(%8804) # torch/nn/functional.py:1992:19
      %8807 : int = aten::sub(%8806, %26) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%8807, %25, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %8811 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
          %8812 : int = aten::__getitem__(%8804, %8811) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %8812) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.199)
      %8814 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8814) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8815 : Tensor = aten::batch_norm(%bottleneck_output.48, %8802, %8803, %8800, %8801, %8799, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.50 : Tensor = aten::relu_(%8815) # torch/nn/functional.py:1117:17
  %8817 : Tensor = prim::GetAttr[name="weight"](%8792)
  %8818 : Tensor? = prim::GetAttr[name="bias"](%8792)
  %8819 : int[] = prim::ListConstruct(%27, %27)
  %8820 : int[] = prim::ListConstruct(%27, %27)
  %8821 : int[] = prim::ListConstruct(%27, %27)
  %new_features.50 : Tensor = aten::conv2d(%result.50, %8817, %8818, %8819, %8820, %8821, %27) # torch/nn/modules/conv.py:415:15
  %8823 : float = prim::GetAttr[name="drop_rate"](%6544)
  %8824 : bool = aten::gt(%8823, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.47 : Tensor = prim::If(%8824) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8826 : float = prim::GetAttr[name="drop_rate"](%6544)
      %8827 : bool = prim::GetAttr[name="training"](%6544)
      %8828 : bool = aten::lt(%8826, %16) # torch/nn/functional.py:968:7
      %8829 : bool = prim::If(%8828) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8830 : bool = aten::gt(%8826, %17) # torch/nn/functional.py:968:17
          -> (%8830)
       = prim::If(%8829) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8831 : Tensor = aten::dropout(%new_features.50, %8826, %8827) # torch/nn/functional.py:973:17
      -> (%8831)
    block1():
      -> (%new_features.50)
  %8832 : Tensor[] = aten::append(%features.1, %new_features.47) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8833 : Tensor = prim::Uninitialized()
  %8834 : bool = prim::GetAttr[name="memory_efficient"](%6545)
  %8835 : bool = prim::If(%8834) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8836 : bool = prim::Uninitialized()
      %8837 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8838 : bool = aten::gt(%8837, %24)
      %8839 : bool, %8840 : bool, %8841 : int = prim::Loop(%18, %8838, %19, %8836, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8842 : int, %8843 : bool, %8844 : bool, %8845 : int):
          %tensor.26 : Tensor = aten::__getitem__(%features.1, %8845) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8847 : bool = prim::requires_grad(%tensor.26)
          %8848 : bool, %8849 : bool = prim::If(%8847) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8836)
          %8850 : int = aten::add(%8845, %27)
          %8851 : bool = aten::lt(%8850, %8837)
          %8852 : bool = aten::__and__(%8851, %8848)
          -> (%8852, %8847, %8849, %8850)
      %8853 : bool = prim::If(%8839)
        block0():
          -> (%8840)
        block1():
          -> (%19)
      -> (%8853)
    block1():
      -> (%19)
  %bottleneck_output.50 : Tensor = prim::If(%8835) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8833)
    block1():
      %concated_features.26 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8856 : __torch__.torch.nn.modules.conv.___torch_mangle_359.Conv2d = prim::GetAttr[name="conv1"](%6545)
      %8857 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_355.BatchNorm2d = prim::GetAttr[name="norm1"](%6545)
      %8858 : int = aten::dim(%concated_features.26) # torch/nn/modules/batchnorm.py:276:11
      %8859 : bool = aten::ne(%8858, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8859) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8860 : bool = prim::GetAttr[name="training"](%8857)
       = prim::If(%8860) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8861 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8857)
          %8862 : Tensor = aten::add(%8861, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8857, %8862)
          -> ()
        block1():
          -> ()
      %8863 : bool = prim::GetAttr[name="training"](%8857)
      %8864 : Tensor = prim::GetAttr[name="running_mean"](%8857)
      %8865 : Tensor = prim::GetAttr[name="running_var"](%8857)
      %8866 : Tensor = prim::GetAttr[name="weight"](%8857)
      %8867 : Tensor = prim::GetAttr[name="bias"](%8857)
       = prim::If(%8863) # torch/nn/functional.py:2011:4
        block0():
          %8868 : int[] = aten::size(%concated_features.26) # torch/nn/functional.py:2012:27
          %size_prods.200 : int = aten::__getitem__(%8868, %24) # torch/nn/functional.py:1991:17
          %8870 : int = aten::len(%8868) # torch/nn/functional.py:1992:19
          %8871 : int = aten::sub(%8870, %26) # torch/nn/functional.py:1992:19
          %size_prods.201 : int = prim::Loop(%8871, %25, %size_prods.200) # torch/nn/functional.py:1992:4
            block0(%i.51 : int, %size_prods.202 : int):
              %8875 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
              %8876 : int = aten::__getitem__(%8868, %8875) # torch/nn/functional.py:1993:22
              %size_prods.203 : int = aten::mul(%size_prods.202, %8876) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.203)
          %8878 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8878) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8879 : Tensor = aten::batch_norm(%concated_features.26, %8866, %8867, %8864, %8865, %8863, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.51 : Tensor = aten::relu_(%8879) # torch/nn/functional.py:1117:17
      %8881 : Tensor = prim::GetAttr[name="weight"](%8856)
      %8882 : Tensor? = prim::GetAttr[name="bias"](%8856)
      %8883 : int[] = prim::ListConstruct(%27, %27)
      %8884 : int[] = prim::ListConstruct(%24, %24)
      %8885 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.51 : Tensor = aten::conv2d(%result.51, %8881, %8882, %8883, %8884, %8885, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.51)
  %8887 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6545)
  %8888 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6545)
  %8889 : int = aten::dim(%bottleneck_output.50) # torch/nn/modules/batchnorm.py:276:11
  %8890 : bool = aten::ne(%8889, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8890) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8891 : bool = prim::GetAttr[name="training"](%8888)
   = prim::If(%8891) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8892 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8888)
      %8893 : Tensor = aten::add(%8892, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8888, %8893)
      -> ()
    block1():
      -> ()
  %8894 : bool = prim::GetAttr[name="training"](%8888)
  %8895 : Tensor = prim::GetAttr[name="running_mean"](%8888)
  %8896 : Tensor = prim::GetAttr[name="running_var"](%8888)
  %8897 : Tensor = prim::GetAttr[name="weight"](%8888)
  %8898 : Tensor = prim::GetAttr[name="bias"](%8888)
   = prim::If(%8894) # torch/nn/functional.py:2011:4
    block0():
      %8899 : int[] = aten::size(%bottleneck_output.50) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%8899, %24) # torch/nn/functional.py:1991:17
      %8901 : int = aten::len(%8899) # torch/nn/functional.py:1992:19
      %8902 : int = aten::sub(%8901, %26) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%8902, %25, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %8906 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
          %8907 : int = aten::__getitem__(%8899, %8906) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %8907) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.207)
      %8909 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8909) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8910 : Tensor = aten::batch_norm(%bottleneck_output.50, %8897, %8898, %8895, %8896, %8894, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.52 : Tensor = aten::relu_(%8910) # torch/nn/functional.py:1117:17
  %8912 : Tensor = prim::GetAttr[name="weight"](%8887)
  %8913 : Tensor? = prim::GetAttr[name="bias"](%8887)
  %8914 : int[] = prim::ListConstruct(%27, %27)
  %8915 : int[] = prim::ListConstruct(%27, %27)
  %8916 : int[] = prim::ListConstruct(%27, %27)
  %new_features.52 : Tensor = aten::conv2d(%result.52, %8912, %8913, %8914, %8915, %8916, %27) # torch/nn/modules/conv.py:415:15
  %8918 : float = prim::GetAttr[name="drop_rate"](%6545)
  %8919 : bool = aten::gt(%8918, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.49 : Tensor = prim::If(%8919) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8921 : float = prim::GetAttr[name="drop_rate"](%6545)
      %8922 : bool = prim::GetAttr[name="training"](%6545)
      %8923 : bool = aten::lt(%8921, %16) # torch/nn/functional.py:968:7
      %8924 : bool = prim::If(%8923) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8925 : bool = aten::gt(%8921, %17) # torch/nn/functional.py:968:17
          -> (%8925)
       = prim::If(%8924) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8926 : Tensor = aten::dropout(%new_features.52, %8921, %8922) # torch/nn/functional.py:973:17
      -> (%8926)
    block1():
      -> (%new_features.52)
  %8927 : Tensor[] = aten::append(%features.1, %new_features.49) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %8928 : Tensor = prim::Uninitialized()
  %8929 : bool = prim::GetAttr[name="memory_efficient"](%6546)
  %8930 : bool = prim::If(%8929) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %8931 : bool = prim::Uninitialized()
      %8932 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %8933 : bool = aten::gt(%8932, %24)
      %8934 : bool, %8935 : bool, %8936 : int = prim::Loop(%18, %8933, %19, %8931, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%8937 : int, %8938 : bool, %8939 : bool, %8940 : int):
          %tensor.27 : Tensor = aten::__getitem__(%features.1, %8940) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %8942 : bool = prim::requires_grad(%tensor.27)
          %8943 : bool, %8944 : bool = prim::If(%8942) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %8931)
          %8945 : int = aten::add(%8940, %27)
          %8946 : bool = aten::lt(%8945, %8932)
          %8947 : bool = aten::__and__(%8946, %8943)
          -> (%8947, %8942, %8944, %8945)
      %8948 : bool = prim::If(%8934)
        block0():
          -> (%8935)
        block1():
          -> (%19)
      -> (%8948)
    block1():
      -> (%19)
  %bottleneck_output.52 : Tensor = prim::If(%8930) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%8928)
    block1():
      %concated_features.27 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %8951 : __torch__.torch.nn.modules.conv.___torch_mangle_362.Conv2d = prim::GetAttr[name="conv1"](%6546)
      %8952 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_361.BatchNorm2d = prim::GetAttr[name="norm1"](%6546)
      %8953 : int = aten::dim(%concated_features.27) # torch/nn/modules/batchnorm.py:276:11
      %8954 : bool = aten::ne(%8953, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%8954) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %8955 : bool = prim::GetAttr[name="training"](%8952)
       = prim::If(%8955) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %8956 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8952)
          %8957 : Tensor = aten::add(%8956, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%8952, %8957)
          -> ()
        block1():
          -> ()
      %8958 : bool = prim::GetAttr[name="training"](%8952)
      %8959 : Tensor = prim::GetAttr[name="running_mean"](%8952)
      %8960 : Tensor = prim::GetAttr[name="running_var"](%8952)
      %8961 : Tensor = prim::GetAttr[name="weight"](%8952)
      %8962 : Tensor = prim::GetAttr[name="bias"](%8952)
       = prim::If(%8958) # torch/nn/functional.py:2011:4
        block0():
          %8963 : int[] = aten::size(%concated_features.27) # torch/nn/functional.py:2012:27
          %size_prods.208 : int = aten::__getitem__(%8963, %24) # torch/nn/functional.py:1991:17
          %8965 : int = aten::len(%8963) # torch/nn/functional.py:1992:19
          %8966 : int = aten::sub(%8965, %26) # torch/nn/functional.py:1992:19
          %size_prods.209 : int = prim::Loop(%8966, %25, %size_prods.208) # torch/nn/functional.py:1992:4
            block0(%i.53 : int, %size_prods.210 : int):
              %8970 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
              %8971 : int = aten::__getitem__(%8963, %8970) # torch/nn/functional.py:1993:22
              %size_prods.211 : int = aten::mul(%size_prods.210, %8971) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.211)
          %8973 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8973) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8974 : Tensor = aten::batch_norm(%concated_features.27, %8961, %8962, %8959, %8960, %8958, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.53 : Tensor = aten::relu_(%8974) # torch/nn/functional.py:1117:17
      %8976 : Tensor = prim::GetAttr[name="weight"](%8951)
      %8977 : Tensor? = prim::GetAttr[name="bias"](%8951)
      %8978 : int[] = prim::ListConstruct(%27, %27)
      %8979 : int[] = prim::ListConstruct(%24, %24)
      %8980 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.53 : Tensor = aten::conv2d(%result.53, %8976, %8977, %8978, %8979, %8980, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.53)
  %8982 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6546)
  %8983 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6546)
  %8984 : int = aten::dim(%bottleneck_output.52) # torch/nn/modules/batchnorm.py:276:11
  %8985 : bool = aten::ne(%8984, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8985) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8986 : bool = prim::GetAttr[name="training"](%8983)
   = prim::If(%8986) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8987 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8983)
      %8988 : Tensor = aten::add(%8987, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8983, %8988)
      -> ()
    block1():
      -> ()
  %8989 : bool = prim::GetAttr[name="training"](%8983)
  %8990 : Tensor = prim::GetAttr[name="running_mean"](%8983)
  %8991 : Tensor = prim::GetAttr[name="running_var"](%8983)
  %8992 : Tensor = prim::GetAttr[name="weight"](%8983)
  %8993 : Tensor = prim::GetAttr[name="bias"](%8983)
   = prim::If(%8989) # torch/nn/functional.py:2011:4
    block0():
      %8994 : int[] = aten::size(%bottleneck_output.52) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%8994, %24) # torch/nn/functional.py:1991:17
      %8996 : int = aten::len(%8994) # torch/nn/functional.py:1992:19
      %8997 : int = aten::sub(%8996, %26) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%8997, %25, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %9001 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
          %9002 : int = aten::__getitem__(%8994, %9001) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %9002) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.215)
      %9004 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9004) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9005 : Tensor = aten::batch_norm(%bottleneck_output.52, %8992, %8993, %8990, %8991, %8989, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.54 : Tensor = aten::relu_(%9005) # torch/nn/functional.py:1117:17
  %9007 : Tensor = prim::GetAttr[name="weight"](%8982)
  %9008 : Tensor? = prim::GetAttr[name="bias"](%8982)
  %9009 : int[] = prim::ListConstruct(%27, %27)
  %9010 : int[] = prim::ListConstruct(%27, %27)
  %9011 : int[] = prim::ListConstruct(%27, %27)
  %new_features.54 : Tensor = aten::conv2d(%result.54, %9007, %9008, %9009, %9010, %9011, %27) # torch/nn/modules/conv.py:415:15
  %9013 : float = prim::GetAttr[name="drop_rate"](%6546)
  %9014 : bool = aten::gt(%9013, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.51 : Tensor = prim::If(%9014) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9016 : float = prim::GetAttr[name="drop_rate"](%6546)
      %9017 : bool = prim::GetAttr[name="training"](%6546)
      %9018 : bool = aten::lt(%9016, %16) # torch/nn/functional.py:968:7
      %9019 : bool = prim::If(%9018) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9020 : bool = aten::gt(%9016, %17) # torch/nn/functional.py:968:17
          -> (%9020)
       = prim::If(%9019) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9021 : Tensor = aten::dropout(%new_features.54, %9016, %9017) # torch/nn/functional.py:973:17
      -> (%9021)
    block1():
      -> (%new_features.54)
  %9022 : Tensor[] = aten::append(%features.1, %new_features.51) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9023 : Tensor = prim::Uninitialized()
  %9024 : bool = prim::GetAttr[name="memory_efficient"](%6547)
  %9025 : bool = prim::If(%9024) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9026 : bool = prim::Uninitialized()
      %9027 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9028 : bool = aten::gt(%9027, %24)
      %9029 : bool, %9030 : bool, %9031 : int = prim::Loop(%18, %9028, %19, %9026, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9032 : int, %9033 : bool, %9034 : bool, %9035 : int):
          %tensor.28 : Tensor = aten::__getitem__(%features.1, %9035) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9037 : bool = prim::requires_grad(%tensor.28)
          %9038 : bool, %9039 : bool = prim::If(%9037) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9026)
          %9040 : int = aten::add(%9035, %27)
          %9041 : bool = aten::lt(%9040, %9027)
          %9042 : bool = aten::__and__(%9041, %9038)
          -> (%9042, %9037, %9039, %9040)
      %9043 : bool = prim::If(%9029)
        block0():
          -> (%9030)
        block1():
          -> (%19)
      -> (%9043)
    block1():
      -> (%19)
  %bottleneck_output.54 : Tensor = prim::If(%9025) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9023)
    block1():
      %concated_features.28 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9046 : __torch__.torch.nn.modules.conv.___torch_mangle_364.Conv2d = prim::GetAttr[name="conv1"](%6547)
      %9047 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_261.BatchNorm2d = prim::GetAttr[name="norm1"](%6547)
      %9048 : int = aten::dim(%concated_features.28) # torch/nn/modules/batchnorm.py:276:11
      %9049 : bool = aten::ne(%9048, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9049) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9050 : bool = prim::GetAttr[name="training"](%9047)
       = prim::If(%9050) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9051 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9047)
          %9052 : Tensor = aten::add(%9051, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9047, %9052)
          -> ()
        block1():
          -> ()
      %9053 : bool = prim::GetAttr[name="training"](%9047)
      %9054 : Tensor = prim::GetAttr[name="running_mean"](%9047)
      %9055 : Tensor = prim::GetAttr[name="running_var"](%9047)
      %9056 : Tensor = prim::GetAttr[name="weight"](%9047)
      %9057 : Tensor = prim::GetAttr[name="bias"](%9047)
       = prim::If(%9053) # torch/nn/functional.py:2011:4
        block0():
          %9058 : int[] = aten::size(%concated_features.28) # torch/nn/functional.py:2012:27
          %size_prods.216 : int = aten::__getitem__(%9058, %24) # torch/nn/functional.py:1991:17
          %9060 : int = aten::len(%9058) # torch/nn/functional.py:1992:19
          %9061 : int = aten::sub(%9060, %26) # torch/nn/functional.py:1992:19
          %size_prods.217 : int = prim::Loop(%9061, %25, %size_prods.216) # torch/nn/functional.py:1992:4
            block0(%i.55 : int, %size_prods.218 : int):
              %9065 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
              %9066 : int = aten::__getitem__(%9058, %9065) # torch/nn/functional.py:1993:22
              %size_prods.219 : int = aten::mul(%size_prods.218, %9066) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.219)
          %9068 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9068) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9069 : Tensor = aten::batch_norm(%concated_features.28, %9056, %9057, %9054, %9055, %9053, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.55 : Tensor = aten::relu_(%9069) # torch/nn/functional.py:1117:17
      %9071 : Tensor = prim::GetAttr[name="weight"](%9046)
      %9072 : Tensor? = prim::GetAttr[name="bias"](%9046)
      %9073 : int[] = prim::ListConstruct(%27, %27)
      %9074 : int[] = prim::ListConstruct(%24, %24)
      %9075 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.55 : Tensor = aten::conv2d(%result.55, %9071, %9072, %9073, %9074, %9075, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.55)
  %9077 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6547)
  %9078 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6547)
  %9079 : int = aten::dim(%bottleneck_output.54) # torch/nn/modules/batchnorm.py:276:11
  %9080 : bool = aten::ne(%9079, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9080) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9081 : bool = prim::GetAttr[name="training"](%9078)
   = prim::If(%9081) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9082 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9078)
      %9083 : Tensor = aten::add(%9082, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9078, %9083)
      -> ()
    block1():
      -> ()
  %9084 : bool = prim::GetAttr[name="training"](%9078)
  %9085 : Tensor = prim::GetAttr[name="running_mean"](%9078)
  %9086 : Tensor = prim::GetAttr[name="running_var"](%9078)
  %9087 : Tensor = prim::GetAttr[name="weight"](%9078)
  %9088 : Tensor = prim::GetAttr[name="bias"](%9078)
   = prim::If(%9084) # torch/nn/functional.py:2011:4
    block0():
      %9089 : int[] = aten::size(%bottleneck_output.54) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%9089, %24) # torch/nn/functional.py:1991:17
      %9091 : int = aten::len(%9089) # torch/nn/functional.py:1992:19
      %9092 : int = aten::sub(%9091, %26) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%9092, %25, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %9096 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
          %9097 : int = aten::__getitem__(%9089, %9096) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %9097) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.223)
      %9099 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9099) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9100 : Tensor = aten::batch_norm(%bottleneck_output.54, %9087, %9088, %9085, %9086, %9084, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.56 : Tensor = aten::relu_(%9100) # torch/nn/functional.py:1117:17
  %9102 : Tensor = prim::GetAttr[name="weight"](%9077)
  %9103 : Tensor? = prim::GetAttr[name="bias"](%9077)
  %9104 : int[] = prim::ListConstruct(%27, %27)
  %9105 : int[] = prim::ListConstruct(%27, %27)
  %9106 : int[] = prim::ListConstruct(%27, %27)
  %new_features.56 : Tensor = aten::conv2d(%result.56, %9102, %9103, %9104, %9105, %9106, %27) # torch/nn/modules/conv.py:415:15
  %9108 : float = prim::GetAttr[name="drop_rate"](%6547)
  %9109 : bool = aten::gt(%9108, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.53 : Tensor = prim::If(%9109) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9111 : float = prim::GetAttr[name="drop_rate"](%6547)
      %9112 : bool = prim::GetAttr[name="training"](%6547)
      %9113 : bool = aten::lt(%9111, %16) # torch/nn/functional.py:968:7
      %9114 : bool = prim::If(%9113) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9115 : bool = aten::gt(%9111, %17) # torch/nn/functional.py:968:17
          -> (%9115)
       = prim::If(%9114) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9116 : Tensor = aten::dropout(%new_features.56, %9111, %9112) # torch/nn/functional.py:973:17
      -> (%9116)
    block1():
      -> (%new_features.56)
  %9117 : Tensor[] = aten::append(%features.1, %new_features.53) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9118 : Tensor = prim::Uninitialized()
  %9119 : bool = prim::GetAttr[name="memory_efficient"](%6548)
  %9120 : bool = prim::If(%9119) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9121 : bool = prim::Uninitialized()
      %9122 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9123 : bool = aten::gt(%9122, %24)
      %9124 : bool, %9125 : bool, %9126 : int = prim::Loop(%18, %9123, %19, %9121, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9127 : int, %9128 : bool, %9129 : bool, %9130 : int):
          %tensor.29 : Tensor = aten::__getitem__(%features.1, %9130) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9132 : bool = prim::requires_grad(%tensor.29)
          %9133 : bool, %9134 : bool = prim::If(%9132) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9121)
          %9135 : int = aten::add(%9130, %27)
          %9136 : bool = aten::lt(%9135, %9122)
          %9137 : bool = aten::__and__(%9136, %9133)
          -> (%9137, %9132, %9134, %9135)
      %9138 : bool = prim::If(%9124)
        block0():
          -> (%9125)
        block1():
          -> (%19)
      -> (%9138)
    block1():
      -> (%19)
  %bottleneck_output.56 : Tensor = prim::If(%9120) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9118)
    block1():
      %concated_features.29 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9141 : __torch__.torch.nn.modules.conv.___torch_mangle_367.Conv2d = prim::GetAttr[name="conv1"](%6548)
      %9142 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_366.BatchNorm2d = prim::GetAttr[name="norm1"](%6548)
      %9143 : int = aten::dim(%concated_features.29) # torch/nn/modules/batchnorm.py:276:11
      %9144 : bool = aten::ne(%9143, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9144) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9145 : bool = prim::GetAttr[name="training"](%9142)
       = prim::If(%9145) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9146 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9142)
          %9147 : Tensor = aten::add(%9146, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9142, %9147)
          -> ()
        block1():
          -> ()
      %9148 : bool = prim::GetAttr[name="training"](%9142)
      %9149 : Tensor = prim::GetAttr[name="running_mean"](%9142)
      %9150 : Tensor = prim::GetAttr[name="running_var"](%9142)
      %9151 : Tensor = prim::GetAttr[name="weight"](%9142)
      %9152 : Tensor = prim::GetAttr[name="bias"](%9142)
       = prim::If(%9148) # torch/nn/functional.py:2011:4
        block0():
          %9153 : int[] = aten::size(%concated_features.29) # torch/nn/functional.py:2012:27
          %size_prods.224 : int = aten::__getitem__(%9153, %24) # torch/nn/functional.py:1991:17
          %9155 : int = aten::len(%9153) # torch/nn/functional.py:1992:19
          %9156 : int = aten::sub(%9155, %26) # torch/nn/functional.py:1992:19
          %size_prods.225 : int = prim::Loop(%9156, %25, %size_prods.224) # torch/nn/functional.py:1992:4
            block0(%i.57 : int, %size_prods.226 : int):
              %9160 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
              %9161 : int = aten::__getitem__(%9153, %9160) # torch/nn/functional.py:1993:22
              %size_prods.227 : int = aten::mul(%size_prods.226, %9161) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.227)
          %9163 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9163) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9164 : Tensor = aten::batch_norm(%concated_features.29, %9151, %9152, %9149, %9150, %9148, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.57 : Tensor = aten::relu_(%9164) # torch/nn/functional.py:1117:17
      %9166 : Tensor = prim::GetAttr[name="weight"](%9141)
      %9167 : Tensor? = prim::GetAttr[name="bias"](%9141)
      %9168 : int[] = prim::ListConstruct(%27, %27)
      %9169 : int[] = prim::ListConstruct(%24, %24)
      %9170 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.57 : Tensor = aten::conv2d(%result.57, %9166, %9167, %9168, %9169, %9170, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.57)
  %9172 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6548)
  %9173 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6548)
  %9174 : int = aten::dim(%bottleneck_output.56) # torch/nn/modules/batchnorm.py:276:11
  %9175 : bool = aten::ne(%9174, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9175) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9176 : bool = prim::GetAttr[name="training"](%9173)
   = prim::If(%9176) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9177 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9173)
      %9178 : Tensor = aten::add(%9177, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9173, %9178)
      -> ()
    block1():
      -> ()
  %9179 : bool = prim::GetAttr[name="training"](%9173)
  %9180 : Tensor = prim::GetAttr[name="running_mean"](%9173)
  %9181 : Tensor = prim::GetAttr[name="running_var"](%9173)
  %9182 : Tensor = prim::GetAttr[name="weight"](%9173)
  %9183 : Tensor = prim::GetAttr[name="bias"](%9173)
   = prim::If(%9179) # torch/nn/functional.py:2011:4
    block0():
      %9184 : int[] = aten::size(%bottleneck_output.56) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%9184, %24) # torch/nn/functional.py:1991:17
      %9186 : int = aten::len(%9184) # torch/nn/functional.py:1992:19
      %9187 : int = aten::sub(%9186, %26) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%9187, %25, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %9191 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
          %9192 : int = aten::__getitem__(%9184, %9191) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %9192) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.231)
      %9194 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9194) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9195 : Tensor = aten::batch_norm(%bottleneck_output.56, %9182, %9183, %9180, %9181, %9179, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.58 : Tensor = aten::relu_(%9195) # torch/nn/functional.py:1117:17
  %9197 : Tensor = prim::GetAttr[name="weight"](%9172)
  %9198 : Tensor? = prim::GetAttr[name="bias"](%9172)
  %9199 : int[] = prim::ListConstruct(%27, %27)
  %9200 : int[] = prim::ListConstruct(%27, %27)
  %9201 : int[] = prim::ListConstruct(%27, %27)
  %new_features.58 : Tensor = aten::conv2d(%result.58, %9197, %9198, %9199, %9200, %9201, %27) # torch/nn/modules/conv.py:415:15
  %9203 : float = prim::GetAttr[name="drop_rate"](%6548)
  %9204 : bool = aten::gt(%9203, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.55 : Tensor = prim::If(%9204) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9206 : float = prim::GetAttr[name="drop_rate"](%6548)
      %9207 : bool = prim::GetAttr[name="training"](%6548)
      %9208 : bool = aten::lt(%9206, %16) # torch/nn/functional.py:968:7
      %9209 : bool = prim::If(%9208) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9210 : bool = aten::gt(%9206, %17) # torch/nn/functional.py:968:17
          -> (%9210)
       = prim::If(%9209) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9211 : Tensor = aten::dropout(%new_features.58, %9206, %9207) # torch/nn/functional.py:973:17
      -> (%9211)
    block1():
      -> (%new_features.58)
  %9212 : Tensor[] = aten::append(%features.1, %new_features.55) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9213 : Tensor = prim::Uninitialized()
  %9214 : bool = prim::GetAttr[name="memory_efficient"](%6549)
  %9215 : bool = prim::If(%9214) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9216 : bool = prim::Uninitialized()
      %9217 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9218 : bool = aten::gt(%9217, %24)
      %9219 : bool, %9220 : bool, %9221 : int = prim::Loop(%18, %9218, %19, %9216, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9222 : int, %9223 : bool, %9224 : bool, %9225 : int):
          %tensor.30 : Tensor = aten::__getitem__(%features.1, %9225) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9227 : bool = prim::requires_grad(%tensor.30)
          %9228 : bool, %9229 : bool = prim::If(%9227) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9216)
          %9230 : int = aten::add(%9225, %27)
          %9231 : bool = aten::lt(%9230, %9217)
          %9232 : bool = aten::__and__(%9231, %9228)
          -> (%9232, %9227, %9229, %9230)
      %9233 : bool = prim::If(%9219)
        block0():
          -> (%9220)
        block1():
          -> (%19)
      -> (%9233)
    block1():
      -> (%19)
  %bottleneck_output.58 : Tensor = prim::If(%9215) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9213)
    block1():
      %concated_features.30 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9236 : __torch__.torch.nn.modules.conv.___torch_mangle_373.Conv2d = prim::GetAttr[name="conv1"](%6549)
      %9237 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_370.BatchNorm2d = prim::GetAttr[name="norm1"](%6549)
      %9238 : int = aten::dim(%concated_features.30) # torch/nn/modules/batchnorm.py:276:11
      %9239 : bool = aten::ne(%9238, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9239) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9240 : bool = prim::GetAttr[name="training"](%9237)
       = prim::If(%9240) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9241 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9237)
          %9242 : Tensor = aten::add(%9241, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9237, %9242)
          -> ()
        block1():
          -> ()
      %9243 : bool = prim::GetAttr[name="training"](%9237)
      %9244 : Tensor = prim::GetAttr[name="running_mean"](%9237)
      %9245 : Tensor = prim::GetAttr[name="running_var"](%9237)
      %9246 : Tensor = prim::GetAttr[name="weight"](%9237)
      %9247 : Tensor = prim::GetAttr[name="bias"](%9237)
       = prim::If(%9243) # torch/nn/functional.py:2011:4
        block0():
          %9248 : int[] = aten::size(%concated_features.30) # torch/nn/functional.py:2012:27
          %size_prods.232 : int = aten::__getitem__(%9248, %24) # torch/nn/functional.py:1991:17
          %9250 : int = aten::len(%9248) # torch/nn/functional.py:1992:19
          %9251 : int = aten::sub(%9250, %26) # torch/nn/functional.py:1992:19
          %size_prods.233 : int = prim::Loop(%9251, %25, %size_prods.232) # torch/nn/functional.py:1992:4
            block0(%i.59 : int, %size_prods.234 : int):
              %9255 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
              %9256 : int = aten::__getitem__(%9248, %9255) # torch/nn/functional.py:1993:22
              %size_prods.235 : int = aten::mul(%size_prods.234, %9256) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.235)
          %9258 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9258) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9259 : Tensor = aten::batch_norm(%concated_features.30, %9246, %9247, %9244, %9245, %9243, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.59 : Tensor = aten::relu_(%9259) # torch/nn/functional.py:1117:17
      %9261 : Tensor = prim::GetAttr[name="weight"](%9236)
      %9262 : Tensor? = prim::GetAttr[name="bias"](%9236)
      %9263 : int[] = prim::ListConstruct(%27, %27)
      %9264 : int[] = prim::ListConstruct(%24, %24)
      %9265 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.59 : Tensor = aten::conv2d(%result.59, %9261, %9262, %9263, %9264, %9265, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.59)
  %9267 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6549)
  %9268 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6549)
  %9269 : int = aten::dim(%bottleneck_output.58) # torch/nn/modules/batchnorm.py:276:11
  %9270 : bool = aten::ne(%9269, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9270) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9271 : bool = prim::GetAttr[name="training"](%9268)
   = prim::If(%9271) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9272 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9268)
      %9273 : Tensor = aten::add(%9272, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9268, %9273)
      -> ()
    block1():
      -> ()
  %9274 : bool = prim::GetAttr[name="training"](%9268)
  %9275 : Tensor = prim::GetAttr[name="running_mean"](%9268)
  %9276 : Tensor = prim::GetAttr[name="running_var"](%9268)
  %9277 : Tensor = prim::GetAttr[name="weight"](%9268)
  %9278 : Tensor = prim::GetAttr[name="bias"](%9268)
   = prim::If(%9274) # torch/nn/functional.py:2011:4
    block0():
      %9279 : int[] = aten::size(%bottleneck_output.58) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%9279, %24) # torch/nn/functional.py:1991:17
      %9281 : int = aten::len(%9279) # torch/nn/functional.py:1992:19
      %9282 : int = aten::sub(%9281, %26) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%9282, %25, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %9286 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
          %9287 : int = aten::__getitem__(%9279, %9286) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %9287) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.239)
      %9289 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9289) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9290 : Tensor = aten::batch_norm(%bottleneck_output.58, %9277, %9278, %9275, %9276, %9274, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.60 : Tensor = aten::relu_(%9290) # torch/nn/functional.py:1117:17
  %9292 : Tensor = prim::GetAttr[name="weight"](%9267)
  %9293 : Tensor? = prim::GetAttr[name="bias"](%9267)
  %9294 : int[] = prim::ListConstruct(%27, %27)
  %9295 : int[] = prim::ListConstruct(%27, %27)
  %9296 : int[] = prim::ListConstruct(%27, %27)
  %new_features.60 : Tensor = aten::conv2d(%result.60, %9292, %9293, %9294, %9295, %9296, %27) # torch/nn/modules/conv.py:415:15
  %9298 : float = prim::GetAttr[name="drop_rate"](%6549)
  %9299 : bool = aten::gt(%9298, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.57 : Tensor = prim::If(%9299) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9301 : float = prim::GetAttr[name="drop_rate"](%6549)
      %9302 : bool = prim::GetAttr[name="training"](%6549)
      %9303 : bool = aten::lt(%9301, %16) # torch/nn/functional.py:968:7
      %9304 : bool = prim::If(%9303) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9305 : bool = aten::gt(%9301, %17) # torch/nn/functional.py:968:17
          -> (%9305)
       = prim::If(%9304) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9306 : Tensor = aten::dropout(%new_features.60, %9301, %9302) # torch/nn/functional.py:973:17
      -> (%9306)
    block1():
      -> (%new_features.60)
  %9307 : Tensor[] = aten::append(%features.1, %new_features.57) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9308 : Tensor = prim::Uninitialized()
  %9309 : bool = prim::GetAttr[name="memory_efficient"](%6550)
  %9310 : bool = prim::If(%9309) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9311 : bool = prim::Uninitialized()
      %9312 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9313 : bool = aten::gt(%9312, %24)
      %9314 : bool, %9315 : bool, %9316 : int = prim::Loop(%18, %9313, %19, %9311, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9317 : int, %9318 : bool, %9319 : bool, %9320 : int):
          %tensor.31 : Tensor = aten::__getitem__(%features.1, %9320) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9322 : bool = prim::requires_grad(%tensor.31)
          %9323 : bool, %9324 : bool = prim::If(%9322) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9311)
          %9325 : int = aten::add(%9320, %27)
          %9326 : bool = aten::lt(%9325, %9312)
          %9327 : bool = aten::__and__(%9326, %9323)
          -> (%9327, %9322, %9324, %9325)
      %9328 : bool = prim::If(%9314)
        block0():
          -> (%9315)
        block1():
          -> (%19)
      -> (%9328)
    block1():
      -> (%19)
  %bottleneck_output.60 : Tensor = prim::If(%9310) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9308)
    block1():
      %concated_features.31 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9331 : __torch__.torch.nn.modules.conv.___torch_mangle_375.Conv2d = prim::GetAttr[name="conv1"](%6550)
      %9332 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_267.BatchNorm2d = prim::GetAttr[name="norm1"](%6550)
      %9333 : int = aten::dim(%concated_features.31) # torch/nn/modules/batchnorm.py:276:11
      %9334 : bool = aten::ne(%9333, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9334) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9335 : bool = prim::GetAttr[name="training"](%9332)
       = prim::If(%9335) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9336 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9332)
          %9337 : Tensor = aten::add(%9336, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9332, %9337)
          -> ()
        block1():
          -> ()
      %9338 : bool = prim::GetAttr[name="training"](%9332)
      %9339 : Tensor = prim::GetAttr[name="running_mean"](%9332)
      %9340 : Tensor = prim::GetAttr[name="running_var"](%9332)
      %9341 : Tensor = prim::GetAttr[name="weight"](%9332)
      %9342 : Tensor = prim::GetAttr[name="bias"](%9332)
       = prim::If(%9338) # torch/nn/functional.py:2011:4
        block0():
          %9343 : int[] = aten::size(%concated_features.31) # torch/nn/functional.py:2012:27
          %size_prods.240 : int = aten::__getitem__(%9343, %24) # torch/nn/functional.py:1991:17
          %9345 : int = aten::len(%9343) # torch/nn/functional.py:1992:19
          %9346 : int = aten::sub(%9345, %26) # torch/nn/functional.py:1992:19
          %size_prods.241 : int = prim::Loop(%9346, %25, %size_prods.240) # torch/nn/functional.py:1992:4
            block0(%i.61 : int, %size_prods.242 : int):
              %9350 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
              %9351 : int = aten::__getitem__(%9343, %9350) # torch/nn/functional.py:1993:22
              %size_prods.243 : int = aten::mul(%size_prods.242, %9351) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.243)
          %9353 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9353) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9354 : Tensor = aten::batch_norm(%concated_features.31, %9341, %9342, %9339, %9340, %9338, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.61 : Tensor = aten::relu_(%9354) # torch/nn/functional.py:1117:17
      %9356 : Tensor = prim::GetAttr[name="weight"](%9331)
      %9357 : Tensor? = prim::GetAttr[name="bias"](%9331)
      %9358 : int[] = prim::ListConstruct(%27, %27)
      %9359 : int[] = prim::ListConstruct(%24, %24)
      %9360 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.61 : Tensor = aten::conv2d(%result.61, %9356, %9357, %9358, %9359, %9360, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.61)
  %9362 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6550)
  %9363 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6550)
  %9364 : int = aten::dim(%bottleneck_output.60) # torch/nn/modules/batchnorm.py:276:11
  %9365 : bool = aten::ne(%9364, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9365) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9366 : bool = prim::GetAttr[name="training"](%9363)
   = prim::If(%9366) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9367 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9363)
      %9368 : Tensor = aten::add(%9367, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9363, %9368)
      -> ()
    block1():
      -> ()
  %9369 : bool = prim::GetAttr[name="training"](%9363)
  %9370 : Tensor = prim::GetAttr[name="running_mean"](%9363)
  %9371 : Tensor = prim::GetAttr[name="running_var"](%9363)
  %9372 : Tensor = prim::GetAttr[name="weight"](%9363)
  %9373 : Tensor = prim::GetAttr[name="bias"](%9363)
   = prim::If(%9369) # torch/nn/functional.py:2011:4
    block0():
      %9374 : int[] = aten::size(%bottleneck_output.60) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%9374, %24) # torch/nn/functional.py:1991:17
      %9376 : int = aten::len(%9374) # torch/nn/functional.py:1992:19
      %9377 : int = aten::sub(%9376, %26) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%9377, %25, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %9381 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
          %9382 : int = aten::__getitem__(%9374, %9381) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %9382) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.247)
      %9384 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9384) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9385 : Tensor = aten::batch_norm(%bottleneck_output.60, %9372, %9373, %9370, %9371, %9369, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.62 : Tensor = aten::relu_(%9385) # torch/nn/functional.py:1117:17
  %9387 : Tensor = prim::GetAttr[name="weight"](%9362)
  %9388 : Tensor? = prim::GetAttr[name="bias"](%9362)
  %9389 : int[] = prim::ListConstruct(%27, %27)
  %9390 : int[] = prim::ListConstruct(%27, %27)
  %9391 : int[] = prim::ListConstruct(%27, %27)
  %new_features.62 : Tensor = aten::conv2d(%result.62, %9387, %9388, %9389, %9390, %9391, %27) # torch/nn/modules/conv.py:415:15
  %9393 : float = prim::GetAttr[name="drop_rate"](%6550)
  %9394 : bool = aten::gt(%9393, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.59 : Tensor = prim::If(%9394) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9396 : float = prim::GetAttr[name="drop_rate"](%6550)
      %9397 : bool = prim::GetAttr[name="training"](%6550)
      %9398 : bool = aten::lt(%9396, %16) # torch/nn/functional.py:968:7
      %9399 : bool = prim::If(%9398) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9400 : bool = aten::gt(%9396, %17) # torch/nn/functional.py:968:17
          -> (%9400)
       = prim::If(%9399) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9401 : Tensor = aten::dropout(%new_features.62, %9396, %9397) # torch/nn/functional.py:973:17
      -> (%9401)
    block1():
      -> (%new_features.62)
  %9402 : Tensor[] = aten::append(%features.1, %new_features.59) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9403 : Tensor = prim::Uninitialized()
  %9404 : bool = prim::GetAttr[name="memory_efficient"](%6551)
  %9405 : bool = prim::If(%9404) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9406 : bool = prim::Uninitialized()
      %9407 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9408 : bool = aten::gt(%9407, %24)
      %9409 : bool, %9410 : bool, %9411 : int = prim::Loop(%18, %9408, %19, %9406, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9412 : int, %9413 : bool, %9414 : bool, %9415 : int):
          %tensor.32 : Tensor = aten::__getitem__(%features.1, %9415) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9417 : bool = prim::requires_grad(%tensor.32)
          %9418 : bool, %9419 : bool = prim::If(%9417) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9406)
          %9420 : int = aten::add(%9415, %27)
          %9421 : bool = aten::lt(%9420, %9407)
          %9422 : bool = aten::__and__(%9421, %9418)
          -> (%9422, %9417, %9419, %9420)
      %9423 : bool = prim::If(%9409)
        block0():
          -> (%9410)
        block1():
          -> (%19)
      -> (%9423)
    block1():
      -> (%19)
  %bottleneck_output.62 : Tensor = prim::If(%9405) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9403)
    block1():
      %concated_features.32 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9426 : __torch__.torch.nn.modules.conv.___torch_mangle_378.Conv2d = prim::GetAttr[name="conv1"](%6551)
      %9427 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_377.BatchNorm2d = prim::GetAttr[name="norm1"](%6551)
      %9428 : int = aten::dim(%concated_features.32) # torch/nn/modules/batchnorm.py:276:11
      %9429 : bool = aten::ne(%9428, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9429) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9430 : bool = prim::GetAttr[name="training"](%9427)
       = prim::If(%9430) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9431 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9427)
          %9432 : Tensor = aten::add(%9431, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9427, %9432)
          -> ()
        block1():
          -> ()
      %9433 : bool = prim::GetAttr[name="training"](%9427)
      %9434 : Tensor = prim::GetAttr[name="running_mean"](%9427)
      %9435 : Tensor = prim::GetAttr[name="running_var"](%9427)
      %9436 : Tensor = prim::GetAttr[name="weight"](%9427)
      %9437 : Tensor = prim::GetAttr[name="bias"](%9427)
       = prim::If(%9433) # torch/nn/functional.py:2011:4
        block0():
          %9438 : int[] = aten::size(%concated_features.32) # torch/nn/functional.py:2012:27
          %size_prods.248 : int = aten::__getitem__(%9438, %24) # torch/nn/functional.py:1991:17
          %9440 : int = aten::len(%9438) # torch/nn/functional.py:1992:19
          %9441 : int = aten::sub(%9440, %26) # torch/nn/functional.py:1992:19
          %size_prods.249 : int = prim::Loop(%9441, %25, %size_prods.248) # torch/nn/functional.py:1992:4
            block0(%i.63 : int, %size_prods.250 : int):
              %9445 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
              %9446 : int = aten::__getitem__(%9438, %9445) # torch/nn/functional.py:1993:22
              %size_prods.251 : int = aten::mul(%size_prods.250, %9446) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.251)
          %9448 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9448) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9449 : Tensor = aten::batch_norm(%concated_features.32, %9436, %9437, %9434, %9435, %9433, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.63 : Tensor = aten::relu_(%9449) # torch/nn/functional.py:1117:17
      %9451 : Tensor = prim::GetAttr[name="weight"](%9426)
      %9452 : Tensor? = prim::GetAttr[name="bias"](%9426)
      %9453 : int[] = prim::ListConstruct(%27, %27)
      %9454 : int[] = prim::ListConstruct(%24, %24)
      %9455 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.63 : Tensor = aten::conv2d(%result.63, %9451, %9452, %9453, %9454, %9455, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.63)
  %9457 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6551)
  %9458 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6551)
  %9459 : int = aten::dim(%bottleneck_output.62) # torch/nn/modules/batchnorm.py:276:11
  %9460 : bool = aten::ne(%9459, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9460) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9461 : bool = prim::GetAttr[name="training"](%9458)
   = prim::If(%9461) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9462 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9458)
      %9463 : Tensor = aten::add(%9462, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9458, %9463)
      -> ()
    block1():
      -> ()
  %9464 : bool = prim::GetAttr[name="training"](%9458)
  %9465 : Tensor = prim::GetAttr[name="running_mean"](%9458)
  %9466 : Tensor = prim::GetAttr[name="running_var"](%9458)
  %9467 : Tensor = prim::GetAttr[name="weight"](%9458)
  %9468 : Tensor = prim::GetAttr[name="bias"](%9458)
   = prim::If(%9464) # torch/nn/functional.py:2011:4
    block0():
      %9469 : int[] = aten::size(%bottleneck_output.62) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%9469, %24) # torch/nn/functional.py:1991:17
      %9471 : int = aten::len(%9469) # torch/nn/functional.py:1992:19
      %9472 : int = aten::sub(%9471, %26) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%9472, %25, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %9476 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
          %9477 : int = aten::__getitem__(%9469, %9476) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %9477) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.255)
      %9479 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9479) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9480 : Tensor = aten::batch_norm(%bottleneck_output.62, %9467, %9468, %9465, %9466, %9464, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.64 : Tensor = aten::relu_(%9480) # torch/nn/functional.py:1117:17
  %9482 : Tensor = prim::GetAttr[name="weight"](%9457)
  %9483 : Tensor? = prim::GetAttr[name="bias"](%9457)
  %9484 : int[] = prim::ListConstruct(%27, %27)
  %9485 : int[] = prim::ListConstruct(%27, %27)
  %9486 : int[] = prim::ListConstruct(%27, %27)
  %new_features.64 : Tensor = aten::conv2d(%result.64, %9482, %9483, %9484, %9485, %9486, %27) # torch/nn/modules/conv.py:415:15
  %9488 : float = prim::GetAttr[name="drop_rate"](%6551)
  %9489 : bool = aten::gt(%9488, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.61 : Tensor = prim::If(%9489) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9491 : float = prim::GetAttr[name="drop_rate"](%6551)
      %9492 : bool = prim::GetAttr[name="training"](%6551)
      %9493 : bool = aten::lt(%9491, %16) # torch/nn/functional.py:968:7
      %9494 : bool = prim::If(%9493) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9495 : bool = aten::gt(%9491, %17) # torch/nn/functional.py:968:17
          -> (%9495)
       = prim::If(%9494) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9496 : Tensor = aten::dropout(%new_features.64, %9491, %9492) # torch/nn/functional.py:973:17
      -> (%9496)
    block1():
      -> (%new_features.64)
  %9497 : Tensor[] = aten::append(%features.1, %new_features.61) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %9498 : Tensor = prim::Uninitialized()
  %9499 : bool = prim::GetAttr[name="memory_efficient"](%6552)
  %9500 : bool = prim::If(%9499) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %9501 : bool = prim::Uninitialized()
      %9502 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %9503 : bool = aten::gt(%9502, %24)
      %9504 : bool, %9505 : bool, %9506 : int = prim::Loop(%18, %9503, %19, %9501, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%9507 : int, %9508 : bool, %9509 : bool, %9510 : int):
          %tensor.1 : Tensor = aten::__getitem__(%features.1, %9510) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %9512 : bool = prim::requires_grad(%tensor.1)
          %9513 : bool, %9514 : bool = prim::If(%9512) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %9501)
          %9515 : int = aten::add(%9510, %27)
          %9516 : bool = aten::lt(%9515, %9502)
          %9517 : bool = aten::__and__(%9516, %9513)
          -> (%9517, %9512, %9514, %9515)
      %9518 : bool = prim::If(%9504)
        block0():
          -> (%9505)
        block1():
          -> (%19)
      -> (%9518)
    block1():
      -> (%19)
  %bottleneck_output : Tensor = prim::If(%9500) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%9498)
    block1():
      %concated_features.1 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %9521 : __torch__.torch.nn.modules.conv.___torch_mangle_381.Conv2d = prim::GetAttr[name="conv1"](%6552)
      %9522 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_380.BatchNorm2d = prim::GetAttr[name="norm1"](%6552)
      %9523 : int = aten::dim(%concated_features.1) # torch/nn/modules/batchnorm.py:276:11
      %9524 : bool = aten::ne(%9523, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%9524) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %9525 : bool = prim::GetAttr[name="training"](%9522)
       = prim::If(%9525) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %9526 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9522)
          %9527 : Tensor = aten::add(%9526, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%9522, %9527)
          -> ()
        block1():
          -> ()
      %9528 : bool = prim::GetAttr[name="training"](%9522)
      %9529 : Tensor = prim::GetAttr[name="running_mean"](%9522)
      %9530 : Tensor = prim::GetAttr[name="running_var"](%9522)
      %9531 : Tensor = prim::GetAttr[name="weight"](%9522)
      %9532 : Tensor = prim::GetAttr[name="bias"](%9522)
       = prim::If(%9528) # torch/nn/functional.py:2011:4
        block0():
          %9533 : int[] = aten::size(%concated_features.1) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%9533, %24) # torch/nn/functional.py:1991:17
          %9535 : int = aten::len(%9533) # torch/nn/functional.py:1992:19
          %9536 : int = aten::sub(%9535, %26) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%9536, %25, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %9540 : int = aten::add(%i.2, %26) # torch/nn/functional.py:1993:27
              %9541 : int = aten::__getitem__(%9533, %9540) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %9541) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.5)
          %9543 : bool = aten::eq(%size_prods.4, %27) # torch/nn/functional.py:1994:7
           = prim::If(%9543) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %9544 : Tensor = aten::batch_norm(%concated_features.1, %9531, %9532, %9529, %9530, %9528, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.198 : Tensor = aten::relu_(%9544) # torch/nn/functional.py:1117:17
      %9546 : Tensor = prim::GetAttr[name="weight"](%9521)
      %9547 : Tensor? = prim::GetAttr[name="bias"](%9521)
      %9548 : int[] = prim::ListConstruct(%27, %27)
      %9549 : int[] = prim::ListConstruct(%24, %24)
      %9550 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.2 : Tensor = aten::conv2d(%result.198, %9546, %9547, %9548, %9549, %9550, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.2)
  %9552 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%6552)
  %9553 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%6552)
  %9554 : int = aten::dim(%bottleneck_output) # torch/nn/modules/batchnorm.py:276:11
  %9555 : bool = aten::ne(%9554, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9555) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9556 : bool = prim::GetAttr[name="training"](%9553)
   = prim::If(%9556) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9557 : Tensor = prim::GetAttr[name="num_batches_tracked"](%9553)
      %9558 : Tensor = aten::add(%9557, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%9553, %9558)
      -> ()
    block1():
      -> ()
  %9559 : bool = prim::GetAttr[name="training"](%9553)
  %9560 : Tensor = prim::GetAttr[name="running_mean"](%9553)
  %9561 : Tensor = prim::GetAttr[name="running_var"](%9553)
  %9562 : Tensor = prim::GetAttr[name="weight"](%9553)
  %9563 : Tensor = prim::GetAttr[name="bias"](%9553)
   = prim::If(%9559) # torch/nn/functional.py:2011:4
    block0():
      %9564 : int[] = aten::size(%bottleneck_output) # torch/nn/functional.py:2012:27
      %size_prods.800 : int = aten::__getitem__(%9564, %24) # torch/nn/functional.py:1991:17
      %9566 : int = aten::len(%9564) # torch/nn/functional.py:1992:19
      %9567 : int = aten::sub(%9566, %26) # torch/nn/functional.py:1992:19
      %size_prods.801 : int = prim::Loop(%9567, %25, %size_prods.800) # torch/nn/functional.py:1992:4
        block0(%i.201 : int, %size_prods.802 : int):
          %9571 : int = aten::add(%i.201, %26) # torch/nn/functional.py:1993:27
          %9572 : int = aten::__getitem__(%9564, %9571) # torch/nn/functional.py:1993:22
          %size_prods.803 : int = aten::mul(%size_prods.802, %9572) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.803)
      %9574 : bool = aten::eq(%size_prods.801, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9574) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %9575 : Tensor = aten::batch_norm(%bottleneck_output, %9562, %9563, %9560, %9561, %9559, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.197 : Tensor = aten::relu_(%9575) # torch/nn/functional.py:1117:17
  %9577 : Tensor = prim::GetAttr[name="weight"](%9552)
  %9578 : Tensor? = prim::GetAttr[name="bias"](%9552)
  %9579 : int[] = prim::ListConstruct(%27, %27)
  %9580 : int[] = prim::ListConstruct(%27, %27)
  %9581 : int[] = prim::ListConstruct(%27, %27)
  %new_features.1 : Tensor = aten::conv2d(%result.197, %9577, %9578, %9579, %9580, %9581, %27) # torch/nn/modules/conv.py:415:15
  %9583 : float = prim::GetAttr[name="drop_rate"](%6552)
  %9584 : bool = aten::gt(%9583, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.63 : Tensor = prim::If(%9584) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %9586 : float = prim::GetAttr[name="drop_rate"](%6552)
      %9587 : bool = prim::GetAttr[name="training"](%6552)
      %9588 : bool = aten::lt(%9586, %16) # torch/nn/functional.py:968:7
      %9589 : bool = prim::If(%9588) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %9590 : bool = aten::gt(%9586, %17) # torch/nn/functional.py:968:17
          -> (%9590)
       = prim::If(%9589) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %9591 : Tensor = aten::dropout(%new_features.1, %9586, %9587) # torch/nn/functional.py:973:17
      -> (%9591)
    block1():
      -> (%new_features.1)
  %9592 : Tensor[] = aten::append(%features.1, %new_features.63) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %9594 : int = aten::dim(%input.23) # torch/nn/modules/batchnorm.py:276:11
  %9595 : bool = aten::ne(%9594, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%9595) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %9596 : bool = prim::GetAttr[name="training"](%38)
   = prim::If(%9596) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %9597 : Tensor = prim::GetAttr[name="num_batches_tracked"](%38)
      %9598 : Tensor = aten::add(%9597, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%38, %9598)
      -> ()
    block1():
      -> ()
  %9599 : bool = prim::GetAttr[name="training"](%38)
  %9600 : Tensor = prim::GetAttr[name="running_mean"](%38)
  %9601 : Tensor = prim::GetAttr[name="running_var"](%38)
  %9602 : Tensor = prim::GetAttr[name="weight"](%38)
  %9603 : Tensor = prim::GetAttr[name="bias"](%38)
   = prim::If(%9599) # torch/nn/functional.py:2011:4
    block0():
      %9604 : int[] = aten::size(%input.23) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%9604, %24) # torch/nn/functional.py:1991:17
      %9606 : int = aten::len(%9604) # torch/nn/functional.py:1992:19
      %9607 : int = aten::sub(%9606, %26) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%9607, %25, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %9611 : int = aten::add(%i.1, %26) # torch/nn/functional.py:1993:27
          %9612 : int = aten::__getitem__(%9604, %9611) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %9612) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.3)
      %9614 : bool = aten::eq(%size_prods, %27) # torch/nn/functional.py:1994:7
       = prim::If(%9614) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %features.5 : Tensor = aten::batch_norm(%input.23, %9602, %9603, %9600, %9601, %9599, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %out.1 : Tensor = prim::If(%5) # torch/nn/functional.py:1116:4
    block0():
      %result.1 : Tensor = aten::relu_(%features.5) # torch/nn/functional.py:1117:17
      -> (%result.1)
    block1():
      %result.2 : Tensor = aten::relu(%features.5) # torch/nn/functional.py:1119:17
      -> (%result.2)
  %10 : int[] = prim::ListConstruct(%6, %6)
  %9619 : str = prim::Constant[value="Exception"]() # <string>:5:2
  %9620 : int[] = aten::size(%out.1) # torch/nn/functional.py:925:51
  %9621 : int = aten::len(%9620) # <string>:5:9
  %9622 : int = aten::len(%10) # <string>:5:25
  %9623 : bool = aten::gt(%9621, %9622) # <string>:5:9
   = prim::If(%9623) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%9619) # <string>:5:2
      -> ()
  %out.3 : Tensor = aten::adaptive_avg_pool2d(%out.1, %10) # torch/nn/functional.py:926:11
  %out.5 : Tensor = aten::flatten(%out.3, %6, %2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:195:14
  %13 : __torch__.torch.nn.modules.linear.___torch_mangle_385.Linear = prim::GetAttr[name="classifier"](%self)
  %9625 : int = prim::Constant[value=1]()
  %9626 : int = prim::Constant[value=2]() # torch/nn/functional.py:1672:22
  %9627 : Tensor = prim::GetAttr[name="weight"](%13)
  %9628 : Tensor = prim::GetAttr[name="bias"](%13)
  %9629 : int = aten::dim(%out.5) # torch/nn/functional.py:1672:7
  %9630 : bool = aten::eq(%9629, %9626) # torch/nn/functional.py:1672:7
  %out.7 : Tensor = prim::If(%9630) # torch/nn/functional.py:1672:4
    block0():
      %9632 : Tensor = aten::t(%9627) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%9628, %out.5, %9632, %9625, %9625) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %9634 : Tensor = aten::t(%9627) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%out.5, %9634) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %9628, %9625) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%out.7)
