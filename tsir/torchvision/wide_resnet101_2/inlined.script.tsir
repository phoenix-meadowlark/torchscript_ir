graph(%self : __torch__.torchvision.models.resnet.___torch_mangle_1143.ResNet,
      %x.1 : Tensor):
  %3 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %4 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.1 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %6 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %7 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %8 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %9 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %10 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %11 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %12 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:29
  %13 : int = prim::Constant[value=-1]()
  %14 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv1"](%self)
  %15 : Tensor = prim::GetAttr[name="weight"](%14)
  %16 : Tensor? = prim::GetAttr[name="bias"](%14)
  %17 : int[] = prim::ListConstruct(%10, %10)
  %18 : int[] = prim::ListConstruct(%11, %11)
  %19 : int[] = prim::ListConstruct(%12, %12)
  %x.3 : Tensor = aten::conv2d(%x.1, %15, %16, %17, %18, %19, %12) # torch/nn/modules/conv.py:415:15
  %21 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%self)
  %22 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %23 : bool = aten::ne(%22, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%23) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %24 : bool = prim::GetAttr[name="training"](%21)
   = prim::If(%24) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %25 : Tensor = prim::GetAttr[name="num_batches_tracked"](%21)
      %26 : Tensor = aten::add(%25, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%21, %26)
      -> ()
    block1():
      -> ()
  %27 : bool = prim::GetAttr[name="training"](%21)
  %28 : Tensor = prim::GetAttr[name="running_mean"](%21)
  %29 : Tensor = prim::GetAttr[name="running_var"](%21)
  %30 : Tensor = prim::GetAttr[name="weight"](%21)
  %31 : Tensor = prim::GetAttr[name="bias"](%21)
   = prim::If(%27) # torch/nn/functional.py:2011:4
    block0():
      %32 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%32, %8) # torch/nn/functional.py:1991:17
      %34 : int = aten::len(%32) # torch/nn/functional.py:1992:19
      %35 : int = aten::sub(%34, %10) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%35, %9, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %39 : int = aten::add(%i.82, %10) # torch/nn/functional.py:1993:27
          %40 : int = aten::__getitem__(%32, %39) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %40) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.327)
      %42 : bool = aten::eq(%size_prods.325, %12) # torch/nn/functional.py:1994:7
       = prim::If(%42) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.5 : Tensor = aten::batch_norm(%x.3, %30, %31, %28, %29, %27, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %x.7 : Tensor = aten::relu_(%x.5) # torch/nn/functional.py:1117:17
  %45 : int[] = prim::ListConstruct(%11, %11)
  %46 : int[] = prim::ListConstruct(%10, %10)
  %47 : int[] = prim::ListConstruct(%12, %12)
  %48 : int[] = prim::ListConstruct(%12, %12)
  %x.9 : Tensor = aten::max_pool2d(%x.7, %45, %46, %47, %48, %3) # torch/nn/functional.py:575:11
  %50 : __torch__.torch.nn.modules.container.___torch_mangle_1131.Sequential = prim::GetAttr[name="layer1"](%self)
  %51 : __torch__.torchvision.models.resnet.___torch_mangle_1129.Bottleneck = prim::GetAttr[name="0"](%50)
  %52 : __torch__.torchvision.models.resnet.___torch_mangle_1130.Bottleneck = prim::GetAttr[name="1"](%50)
  %53 : __torch__.torchvision.models.resnet.___torch_mangle_1130.Bottleneck = prim::GetAttr[name="2"](%50)
  %54 : __torch__.torch.nn.modules.conv.___torch_mangle_71.Conv2d = prim::GetAttr[name="conv1"](%51)
  %55 : Tensor = prim::GetAttr[name="weight"](%54)
  %56 : Tensor? = prim::GetAttr[name="bias"](%54)
  %57 : int[] = prim::ListConstruct(%12, %12)
  %58 : int[] = prim::ListConstruct(%8, %8)
  %59 : int[] = prim::ListConstruct(%12, %12)
  %out.19 : Tensor = aten::conv2d(%x.9, %55, %56, %57, %58, %59, %12) # torch/nn/modules/conv.py:415:15
  %61 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%51)
  %62 : int = aten::dim(%out.19) # torch/nn/modules/batchnorm.py:276:11
  %63 : bool = aten::ne(%62, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%63) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %64 : bool = prim::GetAttr[name="training"](%61)
   = prim::If(%64) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %65 : Tensor = prim::GetAttr[name="num_batches_tracked"](%61)
      %66 : Tensor = aten::add(%65, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%61, %66)
      -> ()
    block1():
      -> ()
  %67 : bool = prim::GetAttr[name="training"](%61)
  %68 : Tensor = prim::GetAttr[name="running_mean"](%61)
  %69 : Tensor = prim::GetAttr[name="running_var"](%61)
  %70 : Tensor = prim::GetAttr[name="weight"](%61)
  %71 : Tensor = prim::GetAttr[name="bias"](%61)
   = prim::If(%67) # torch/nn/functional.py:2011:4
    block0():
      %72 : int[] = aten::size(%out.19) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%72, %8) # torch/nn/functional.py:1991:17
      %74 : int = aten::len(%72) # torch/nn/functional.py:1992:19
      %75 : int = aten::sub(%74, %10) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%75, %9, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %79 : int = aten::add(%i.83, %10) # torch/nn/functional.py:1993:27
          %80 : int = aten::__getitem__(%72, %79) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %80) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.331)
      %82 : bool = aten::eq(%size_prods.329, %12) # torch/nn/functional.py:1994:7
       = prim::If(%82) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.209 : Tensor = aten::batch_norm(%out.19, %70, %71, %68, %69, %67, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.210 : Tensor = aten::relu_(%out.209) # torch/nn/functional.py:1117:17
  %85 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%51)
  %86 : Tensor = prim::GetAttr[name="weight"](%85)
  %87 : Tensor? = prim::GetAttr[name="bias"](%85)
  %88 : int[] = prim::ListConstruct(%12, %12)
  %89 : int[] = prim::ListConstruct(%12, %12)
  %90 : int[] = prim::ListConstruct(%12, %12)
  %out.211 : Tensor = aten::conv2d(%out.210, %86, %87, %88, %89, %90, %12) # torch/nn/modules/conv.py:415:15
  %92 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%51)
  %93 : int = aten::dim(%out.211) # torch/nn/modules/batchnorm.py:276:11
  %94 : bool = aten::ne(%93, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%94) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %95 : bool = prim::GetAttr[name="training"](%92)
   = prim::If(%95) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %96 : Tensor = prim::GetAttr[name="num_batches_tracked"](%92)
      %97 : Tensor = aten::add(%96, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%92, %97)
      -> ()
    block1():
      -> ()
  %98 : bool = prim::GetAttr[name="training"](%92)
  %99 : Tensor = prim::GetAttr[name="running_mean"](%92)
  %100 : Tensor = prim::GetAttr[name="running_var"](%92)
  %101 : Tensor = prim::GetAttr[name="weight"](%92)
  %102 : Tensor = prim::GetAttr[name="bias"](%92)
   = prim::If(%98) # torch/nn/functional.py:2011:4
    block0():
      %103 : int[] = aten::size(%out.211) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%103, %8) # torch/nn/functional.py:1991:17
      %105 : int = aten::len(%103) # torch/nn/functional.py:1992:19
      %106 : int = aten::sub(%105, %10) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%106, %9, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %110 : int = aten::add(%i.84, %10) # torch/nn/functional.py:1993:27
          %111 : int = aten::__getitem__(%103, %110) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %111) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.335)
      %113 : bool = aten::eq(%size_prods.333, %12) # torch/nn/functional.py:1994:7
       = prim::If(%113) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.212 : Tensor = aten::batch_norm(%out.211, %101, %102, %99, %100, %98, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.213 : Tensor = aten::relu_(%out.212) # torch/nn/functional.py:1117:17
  %116 : __torch__.torch.nn.modules.conv.___torch_mangle_1006.Conv2d = prim::GetAttr[name="conv3"](%51)
  %117 : Tensor = prim::GetAttr[name="weight"](%116)
  %118 : Tensor? = prim::GetAttr[name="bias"](%116)
  %119 : int[] = prim::ListConstruct(%12, %12)
  %120 : int[] = prim::ListConstruct(%8, %8)
  %121 : int[] = prim::ListConstruct(%12, %12)
  %out.214 : Tensor = aten::conv2d(%out.213, %117, %118, %119, %120, %121, %12) # torch/nn/modules/conv.py:415:15
  %123 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%51)
  %124 : int = aten::dim(%out.214) # torch/nn/modules/batchnorm.py:276:11
  %125 : bool = aten::ne(%124, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%125) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %126 : bool = prim::GetAttr[name="training"](%123)
   = prim::If(%126) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %127 : Tensor = prim::GetAttr[name="num_batches_tracked"](%123)
      %128 : Tensor = aten::add(%127, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%123, %128)
      -> ()
    block1():
      -> ()
  %129 : bool = prim::GetAttr[name="training"](%123)
  %130 : Tensor = prim::GetAttr[name="running_mean"](%123)
  %131 : Tensor = prim::GetAttr[name="running_var"](%123)
  %132 : Tensor = prim::GetAttr[name="weight"](%123)
  %133 : Tensor = prim::GetAttr[name="bias"](%123)
   = prim::If(%129) # torch/nn/functional.py:2011:4
    block0():
      %134 : int[] = aten::size(%out.214) # torch/nn/functional.py:2012:27
      %size_prods.304 : int = aten::__getitem__(%134, %8) # torch/nn/functional.py:1991:17
      %136 : int = aten::len(%134) # torch/nn/functional.py:1992:19
      %137 : int = aten::sub(%136, %10) # torch/nn/functional.py:1992:19
      %size_prods.305 : int = prim::Loop(%137, %9, %size_prods.304) # torch/nn/functional.py:1992:4
        block0(%i.77 : int, %size_prods.306 : int):
          %141 : int = aten::add(%i.77, %10) # torch/nn/functional.py:1993:27
          %142 : int = aten::__getitem__(%134, %141) # torch/nn/functional.py:1993:22
          %size_prods.307 : int = aten::mul(%size_prods.306, %142) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.307)
      %144 : bool = aten::eq(%size_prods.305, %12) # torch/nn/functional.py:1994:7
       = prim::If(%144) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.215 : Tensor = aten::batch_norm(%out.214, %132, %133, %130, %131, %129, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %146 : __torch__.torch.nn.modules.container.___torch_mangle_13.Sequential = prim::GetAttr[name="downsample"](%51)
  %147 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="0"](%146)
  %148 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%146)
  %149 : Tensor = prim::GetAttr[name="weight"](%147)
  %150 : Tensor? = prim::GetAttr[name="bias"](%147)
  %151 : int[] = prim::ListConstruct(%12, %12)
  %152 : int[] = prim::ListConstruct(%8, %8)
  %153 : int[] = prim::ListConstruct(%12, %12)
  %input.6 : Tensor = aten::conv2d(%x.9, %149, %150, %151, %152, %153, %12) # torch/nn/modules/conv.py:415:15
  %155 : int = aten::dim(%input.6) # torch/nn/modules/batchnorm.py:276:11
  %156 : bool = aten::ne(%155, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%156) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %157 : bool = prim::GetAttr[name="training"](%148)
   = prim::If(%157) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %158 : Tensor = prim::GetAttr[name="num_batches_tracked"](%148)
      %159 : Tensor = aten::add(%158, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%148, %159)
      -> ()
    block1():
      -> ()
  %160 : bool = prim::GetAttr[name="training"](%148)
  %161 : Tensor = prim::GetAttr[name="running_mean"](%148)
  %162 : Tensor = prim::GetAttr[name="running_var"](%148)
  %163 : Tensor = prim::GetAttr[name="weight"](%148)
  %164 : Tensor = prim::GetAttr[name="bias"](%148)
   = prim::If(%160) # torch/nn/functional.py:2011:4
    block0():
      %165 : int[] = aten::size(%input.6) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%165, %8) # torch/nn/functional.py:1991:17
      %167 : int = aten::len(%165) # torch/nn/functional.py:1992:19
      %168 : int = aten::sub(%167, %10) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%168, %9, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %172 : int = aten::add(%i.78, %10) # torch/nn/functional.py:1993:27
          %173 : int = aten::__getitem__(%165, %172) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %173) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.311)
      %175 : bool = aten::eq(%size_prods.309, %12) # torch/nn/functional.py:1994:7
       = prim::If(%175) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.2 : Tensor = aten::batch_norm(%input.6, %163, %164, %161, %162, %160, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.216 : Tensor = aten::add_(%out.215, %identity.2, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.10 : Tensor = aten::relu_(%out.216) # torch/nn/functional.py:1117:17
  %179 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%52)
  %180 : Tensor = prim::GetAttr[name="weight"](%179)
  %181 : Tensor? = prim::GetAttr[name="bias"](%179)
  %182 : int[] = prim::ListConstruct(%12, %12)
  %183 : int[] = prim::ListConstruct(%8, %8)
  %184 : int[] = prim::ListConstruct(%12, %12)
  %out.226 : Tensor = aten::conv2d(%input.10, %180, %181, %182, %183, %184, %12) # torch/nn/modules/conv.py:415:15
  %186 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%52)
  %187 : int = aten::dim(%out.226) # torch/nn/modules/batchnorm.py:276:11
  %188 : bool = aten::ne(%187, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%188) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %189 : bool = prim::GetAttr[name="training"](%186)
   = prim::If(%189) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %190 : Tensor = prim::GetAttr[name="num_batches_tracked"](%186)
      %191 : Tensor = aten::add(%190, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%186, %191)
      -> ()
    block1():
      -> ()
  %192 : bool = prim::GetAttr[name="training"](%186)
  %193 : Tensor = prim::GetAttr[name="running_mean"](%186)
  %194 : Tensor = prim::GetAttr[name="running_var"](%186)
  %195 : Tensor = prim::GetAttr[name="weight"](%186)
  %196 : Tensor = prim::GetAttr[name="bias"](%186)
   = prim::If(%192) # torch/nn/functional.py:2011:4
    block0():
      %197 : int[] = aten::size(%out.226) # torch/nn/functional.py:2012:27
      %size_prods.312 : int = aten::__getitem__(%197, %8) # torch/nn/functional.py:1991:17
      %199 : int = aten::len(%197) # torch/nn/functional.py:1992:19
      %200 : int = aten::sub(%199, %10) # torch/nn/functional.py:1992:19
      %size_prods.313 : int = prim::Loop(%200, %9, %size_prods.312) # torch/nn/functional.py:1992:4
        block0(%i.79 : int, %size_prods.314 : int):
          %204 : int = aten::add(%i.79, %10) # torch/nn/functional.py:1993:27
          %205 : int = aten::__getitem__(%197, %204) # torch/nn/functional.py:1993:22
          %size_prods.315 : int = aten::mul(%size_prods.314, %205) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.315)
      %207 : bool = aten::eq(%size_prods.313, %12) # torch/nn/functional.py:1994:7
       = prim::If(%207) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.254 : Tensor = aten::batch_norm(%out.226, %195, %196, %193, %194, %192, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.255 : Tensor = aten::relu_(%out.254) # torch/nn/functional.py:1117:17
  %210 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%52)
  %211 : Tensor = prim::GetAttr[name="weight"](%210)
  %212 : Tensor? = prim::GetAttr[name="bias"](%210)
  %213 : int[] = prim::ListConstruct(%12, %12)
  %214 : int[] = prim::ListConstruct(%12, %12)
  %215 : int[] = prim::ListConstruct(%12, %12)
  %out.256 : Tensor = aten::conv2d(%out.255, %211, %212, %213, %214, %215, %12) # torch/nn/modules/conv.py:415:15
  %217 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%52)
  %218 : int = aten::dim(%out.256) # torch/nn/modules/batchnorm.py:276:11
  %219 : bool = aten::ne(%218, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%219) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %220 : bool = prim::GetAttr[name="training"](%217)
   = prim::If(%220) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %221 : Tensor = prim::GetAttr[name="num_batches_tracked"](%217)
      %222 : Tensor = aten::add(%221, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%217, %222)
      -> ()
    block1():
      -> ()
  %223 : bool = prim::GetAttr[name="training"](%217)
  %224 : Tensor = prim::GetAttr[name="running_mean"](%217)
  %225 : Tensor = prim::GetAttr[name="running_var"](%217)
  %226 : Tensor = prim::GetAttr[name="weight"](%217)
  %227 : Tensor = prim::GetAttr[name="bias"](%217)
   = prim::If(%223) # torch/nn/functional.py:2011:4
    block0():
      %228 : int[] = aten::size(%out.256) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%228, %8) # torch/nn/functional.py:1991:17
      %230 : int = aten::len(%228) # torch/nn/functional.py:1992:19
      %231 : int = aten::sub(%230, %10) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%231, %9, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %235 : int = aten::add(%i.80, %10) # torch/nn/functional.py:1993:27
          %236 : int = aten::__getitem__(%228, %235) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %236) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.319)
      %238 : bool = aten::eq(%size_prods.317, %12) # torch/nn/functional.py:1994:7
       = prim::If(%238) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.257 : Tensor = aten::batch_norm(%out.256, %226, %227, %224, %225, %223, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.258 : Tensor = aten::relu_(%out.257) # torch/nn/functional.py:1117:17
  %241 : __torch__.torch.nn.modules.conv.___torch_mangle_1006.Conv2d = prim::GetAttr[name="conv3"](%52)
  %242 : Tensor = prim::GetAttr[name="weight"](%241)
  %243 : Tensor? = prim::GetAttr[name="bias"](%241)
  %244 : int[] = prim::ListConstruct(%12, %12)
  %245 : int[] = prim::ListConstruct(%8, %8)
  %246 : int[] = prim::ListConstruct(%12, %12)
  %out.259 : Tensor = aten::conv2d(%out.258, %242, %243, %244, %245, %246, %12) # torch/nn/modules/conv.py:415:15
  %248 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%52)
  %249 : int = aten::dim(%out.259) # torch/nn/modules/batchnorm.py:276:11
  %250 : bool = aten::ne(%249, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%250) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %251 : bool = prim::GetAttr[name="training"](%248)
   = prim::If(%251) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %252 : Tensor = prim::GetAttr[name="num_batches_tracked"](%248)
      %253 : Tensor = aten::add(%252, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%248, %253)
      -> ()
    block1():
      -> ()
  %254 : bool = prim::GetAttr[name="training"](%248)
  %255 : Tensor = prim::GetAttr[name="running_mean"](%248)
  %256 : Tensor = prim::GetAttr[name="running_var"](%248)
  %257 : Tensor = prim::GetAttr[name="weight"](%248)
  %258 : Tensor = prim::GetAttr[name="bias"](%248)
   = prim::If(%254) # torch/nn/functional.py:2011:4
    block0():
      %259 : int[] = aten::size(%out.259) # torch/nn/functional.py:2012:27
      %size_prods.320 : int = aten::__getitem__(%259, %8) # torch/nn/functional.py:1991:17
      %261 : int = aten::len(%259) # torch/nn/functional.py:1992:19
      %262 : int = aten::sub(%261, %10) # torch/nn/functional.py:1992:19
      %size_prods.321 : int = prim::Loop(%262, %9, %size_prods.320) # torch/nn/functional.py:1992:4
        block0(%i.81 : int, %size_prods.322 : int):
          %266 : int = aten::add(%i.81, %10) # torch/nn/functional.py:1993:27
          %267 : int = aten::__getitem__(%259, %266) # torch/nn/functional.py:1993:22
          %size_prods.323 : int = aten::mul(%size_prods.322, %267) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.323)
      %269 : bool = aten::eq(%size_prods.321, %12) # torch/nn/functional.py:1994:7
       = prim::If(%269) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.260 : Tensor = aten::batch_norm(%out.259, %257, %258, %255, %256, %254, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.261 : Tensor = aten::add_(%out.260, %input.10, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.8 : Tensor = aten::relu_(%out.261) # torch/nn/functional.py:1117:17
  %273 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%53)
  %274 : Tensor = prim::GetAttr[name="weight"](%273)
  %275 : Tensor? = prim::GetAttr[name="bias"](%273)
  %276 : int[] = prim::ListConstruct(%12, %12)
  %277 : int[] = prim::ListConstruct(%8, %8)
  %278 : int[] = prim::ListConstruct(%12, %12)
  %out.235 : Tensor = aten::conv2d(%input.8, %274, %275, %276, %277, %278, %12) # torch/nn/modules/conv.py:415:15
  %280 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%53)
  %281 : int = aten::dim(%out.235) # torch/nn/modules/batchnorm.py:276:11
  %282 : bool = aten::ne(%281, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%282) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %283 : bool = prim::GetAttr[name="training"](%280)
   = prim::If(%283) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %284 : Tensor = prim::GetAttr[name="num_batches_tracked"](%280)
      %285 : Tensor = aten::add(%284, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%280, %285)
      -> ()
    block1():
      -> ()
  %286 : bool = prim::GetAttr[name="training"](%280)
  %287 : Tensor = prim::GetAttr[name="running_mean"](%280)
  %288 : Tensor = prim::GetAttr[name="running_var"](%280)
  %289 : Tensor = prim::GetAttr[name="weight"](%280)
  %290 : Tensor = prim::GetAttr[name="bias"](%280)
   = prim::If(%286) # torch/nn/functional.py:2011:4
    block0():
      %291 : int[] = aten::size(%out.235) # torch/nn/functional.py:2012:27
      %size_prods.336 : int = aten::__getitem__(%291, %8) # torch/nn/functional.py:1991:17
      %293 : int = aten::len(%291) # torch/nn/functional.py:1992:19
      %294 : int = aten::sub(%293, %10) # torch/nn/functional.py:1992:19
      %size_prods.337 : int = prim::Loop(%294, %9, %size_prods.336) # torch/nn/functional.py:1992:4
        block0(%i.85 : int, %size_prods.338 : int):
          %298 : int = aten::add(%i.85, %10) # torch/nn/functional.py:1993:27
          %299 : int = aten::__getitem__(%291, %298) # torch/nn/functional.py:1993:22
          %size_prods.339 : int = aten::mul(%size_prods.338, %299) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.339)
      %301 : bool = aten::eq(%size_prods.337, %12) # torch/nn/functional.py:1994:7
       = prim::If(%301) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.218 : Tensor = aten::batch_norm(%out.235, %289, %290, %287, %288, %286, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.219 : Tensor = aten::relu_(%out.218) # torch/nn/functional.py:1117:17
  %304 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%53)
  %305 : Tensor = prim::GetAttr[name="weight"](%304)
  %306 : Tensor? = prim::GetAttr[name="bias"](%304)
  %307 : int[] = prim::ListConstruct(%12, %12)
  %308 : int[] = prim::ListConstruct(%12, %12)
  %309 : int[] = prim::ListConstruct(%12, %12)
  %out.220 : Tensor = aten::conv2d(%out.219, %305, %306, %307, %308, %309, %12) # torch/nn/modules/conv.py:415:15
  %311 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%53)
  %312 : int = aten::dim(%out.220) # torch/nn/modules/batchnorm.py:276:11
  %313 : bool = aten::ne(%312, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%313) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %314 : bool = prim::GetAttr[name="training"](%311)
   = prim::If(%314) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%311)
      %316 : Tensor = aten::add(%315, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%311, %316)
      -> ()
    block1():
      -> ()
  %317 : bool = prim::GetAttr[name="training"](%311)
  %318 : Tensor = prim::GetAttr[name="running_mean"](%311)
  %319 : Tensor = prim::GetAttr[name="running_var"](%311)
  %320 : Tensor = prim::GetAttr[name="weight"](%311)
  %321 : Tensor = prim::GetAttr[name="bias"](%311)
   = prim::If(%317) # torch/nn/functional.py:2011:4
    block0():
      %322 : int[] = aten::size(%out.220) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%322, %8) # torch/nn/functional.py:1991:17
      %324 : int = aten::len(%322) # torch/nn/functional.py:1992:19
      %325 : int = aten::sub(%324, %10) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%325, %9, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %329 : int = aten::add(%i.86, %10) # torch/nn/functional.py:1993:27
          %330 : int = aten::__getitem__(%322, %329) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %330) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.343)
      %332 : bool = aten::eq(%size_prods.341, %12) # torch/nn/functional.py:1994:7
       = prim::If(%332) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.221 : Tensor = aten::batch_norm(%out.220, %320, %321, %318, %319, %317, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.222 : Tensor = aten::relu_(%out.221) # torch/nn/functional.py:1117:17
  %335 : __torch__.torch.nn.modules.conv.___torch_mangle_1006.Conv2d = prim::GetAttr[name="conv3"](%53)
  %336 : Tensor = prim::GetAttr[name="weight"](%335)
  %337 : Tensor? = prim::GetAttr[name="bias"](%335)
  %338 : int[] = prim::ListConstruct(%12, %12)
  %339 : int[] = prim::ListConstruct(%8, %8)
  %340 : int[] = prim::ListConstruct(%12, %12)
  %out.223 : Tensor = aten::conv2d(%out.222, %336, %337, %338, %339, %340, %12) # torch/nn/modules/conv.py:415:15
  %342 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%53)
  %343 : int = aten::dim(%out.223) # torch/nn/modules/batchnorm.py:276:11
  %344 : bool = aten::ne(%343, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%344) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %345 : bool = prim::GetAttr[name="training"](%342)
   = prim::If(%345) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %346 : Tensor = prim::GetAttr[name="num_batches_tracked"](%342)
      %347 : Tensor = aten::add(%346, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%342, %347)
      -> ()
    block1():
      -> ()
  %348 : bool = prim::GetAttr[name="training"](%342)
  %349 : Tensor = prim::GetAttr[name="running_mean"](%342)
  %350 : Tensor = prim::GetAttr[name="running_var"](%342)
  %351 : Tensor = prim::GetAttr[name="weight"](%342)
  %352 : Tensor = prim::GetAttr[name="bias"](%342)
   = prim::If(%348) # torch/nn/functional.py:2011:4
    block0():
      %353 : int[] = aten::size(%out.223) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%353, %8) # torch/nn/functional.py:1991:17
      %355 : int = aten::len(%353) # torch/nn/functional.py:1992:19
      %356 : int = aten::sub(%355, %10) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%356, %9, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %360 : int = aten::add(%i.87, %10) # torch/nn/functional.py:1993:27
          %361 : int = aten::__getitem__(%353, %360) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %361) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.347)
      %363 : bool = aten::eq(%size_prods.345, %12) # torch/nn/functional.py:1994:7
       = prim::If(%363) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.224 : Tensor = aten::batch_norm(%out.223, %351, %352, %349, %350, %348, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.225 : Tensor = aten::add_(%out.224, %input.8, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.11 : Tensor = aten::relu_(%out.225) # torch/nn/functional.py:1117:17
  %367 : __torch__.torch.nn.modules.container.___torch_mangle_1134.Sequential = prim::GetAttr[name="layer2"](%self)
  %368 : __torch__.torchvision.models.resnet.___torch_mangle_1132.Bottleneck = prim::GetAttr[name="0"](%367)
  %369 : __torch__.torchvision.models.resnet.___torch_mangle_1133.Bottleneck = prim::GetAttr[name="1"](%367)
  %370 : __torch__.torchvision.models.resnet.___torch_mangle_1133.Bottleneck = prim::GetAttr[name="2"](%367)
  %371 : __torch__.torchvision.models.resnet.___torch_mangle_1133.Bottleneck = prim::GetAttr[name="3"](%367)
  %372 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv1"](%368)
  %373 : Tensor = prim::GetAttr[name="weight"](%372)
  %374 : Tensor? = prim::GetAttr[name="bias"](%372)
  %375 : int[] = prim::ListConstruct(%12, %12)
  %376 : int[] = prim::ListConstruct(%8, %8)
  %377 : int[] = prim::ListConstruct(%12, %12)
  %out.244 : Tensor = aten::conv2d(%x.11, %373, %374, %375, %376, %377, %12) # torch/nn/modules/conv.py:415:15
  %379 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%368)
  %380 : int = aten::dim(%out.244) # torch/nn/modules/batchnorm.py:276:11
  %381 : bool = aten::ne(%380, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%381) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %382 : bool = prim::GetAttr[name="training"](%379)
   = prim::If(%382) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %383 : Tensor = prim::GetAttr[name="num_batches_tracked"](%379)
      %384 : Tensor = aten::add(%383, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%379, %384)
      -> ()
    block1():
      -> ()
  %385 : bool = prim::GetAttr[name="training"](%379)
  %386 : Tensor = prim::GetAttr[name="running_mean"](%379)
  %387 : Tensor = prim::GetAttr[name="running_var"](%379)
  %388 : Tensor = prim::GetAttr[name="weight"](%379)
  %389 : Tensor = prim::GetAttr[name="bias"](%379)
   = prim::If(%385) # torch/nn/functional.py:2011:4
    block0():
      %390 : int[] = aten::size(%out.244) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%390, %8) # torch/nn/functional.py:1991:17
      %392 : int = aten::len(%390) # torch/nn/functional.py:1992:19
      %393 : int = aten::sub(%392, %10) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%393, %9, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %397 : int = aten::add(%i.88, %10) # torch/nn/functional.py:1993:27
          %398 : int = aten::__getitem__(%390, %397) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %398) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.351)
      %400 : bool = aten::eq(%size_prods.349, %12) # torch/nn/functional.py:1994:7
       = prim::If(%400) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.227 : Tensor = aten::batch_norm(%out.244, %388, %389, %386, %387, %385, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.228 : Tensor = aten::relu_(%out.227) # torch/nn/functional.py:1117:17
  %403 : __torch__.torch.nn.modules.conv.___torch_mangle_938.Conv2d = prim::GetAttr[name="conv2"](%368)
  %404 : Tensor = prim::GetAttr[name="weight"](%403)
  %405 : Tensor? = prim::GetAttr[name="bias"](%403)
  %406 : int[] = prim::ListConstruct(%10, %10)
  %407 : int[] = prim::ListConstruct(%12, %12)
  %408 : int[] = prim::ListConstruct(%12, %12)
  %out.229 : Tensor = aten::conv2d(%out.228, %404, %405, %406, %407, %408, %12) # torch/nn/modules/conv.py:415:15
  %410 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%368)
  %411 : int = aten::dim(%out.229) # torch/nn/modules/batchnorm.py:276:11
  %412 : bool = aten::ne(%411, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%412) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %413 : bool = prim::GetAttr[name="training"](%410)
   = prim::If(%413) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %414 : Tensor = prim::GetAttr[name="num_batches_tracked"](%410)
      %415 : Tensor = aten::add(%414, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%410, %415)
      -> ()
    block1():
      -> ()
  %416 : bool = prim::GetAttr[name="training"](%410)
  %417 : Tensor = prim::GetAttr[name="running_mean"](%410)
  %418 : Tensor = prim::GetAttr[name="running_var"](%410)
  %419 : Tensor = prim::GetAttr[name="weight"](%410)
  %420 : Tensor = prim::GetAttr[name="bias"](%410)
   = prim::If(%416) # torch/nn/functional.py:2011:4
    block0():
      %421 : int[] = aten::size(%out.229) # torch/nn/functional.py:2012:27
      %size_prods.352 : int = aten::__getitem__(%421, %8) # torch/nn/functional.py:1991:17
      %423 : int = aten::len(%421) # torch/nn/functional.py:1992:19
      %424 : int = aten::sub(%423, %10) # torch/nn/functional.py:1992:19
      %size_prods.353 : int = prim::Loop(%424, %9, %size_prods.352) # torch/nn/functional.py:1992:4
        block0(%i.89 : int, %size_prods.354 : int):
          %428 : int = aten::add(%i.89, %10) # torch/nn/functional.py:1993:27
          %429 : int = aten::__getitem__(%421, %428) # torch/nn/functional.py:1993:22
          %size_prods.355 : int = aten::mul(%size_prods.354, %429) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.355)
      %431 : bool = aten::eq(%size_prods.353, %12) # torch/nn/functional.py:1994:7
       = prim::If(%431) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.230 : Tensor = aten::batch_norm(%out.229, %419, %420, %417, %418, %416, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.231 : Tensor = aten::relu_(%out.230) # torch/nn/functional.py:1117:17
  %434 : __torch__.torch.nn.modules.conv.___torch_mangle_985.Conv2d = prim::GetAttr[name="conv3"](%368)
  %435 : Tensor = prim::GetAttr[name="weight"](%434)
  %436 : Tensor? = prim::GetAttr[name="bias"](%434)
  %437 : int[] = prim::ListConstruct(%12, %12)
  %438 : int[] = prim::ListConstruct(%8, %8)
  %439 : int[] = prim::ListConstruct(%12, %12)
  %out.232 : Tensor = aten::conv2d(%out.231, %435, %436, %437, %438, %439, %12) # torch/nn/modules/conv.py:415:15
  %441 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%368)
  %442 : int = aten::dim(%out.232) # torch/nn/modules/batchnorm.py:276:11
  %443 : bool = aten::ne(%442, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%443) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %444 : bool = prim::GetAttr[name="training"](%441)
   = prim::If(%444) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %445 : Tensor = prim::GetAttr[name="num_batches_tracked"](%441)
      %446 : Tensor = aten::add(%445, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%441, %446)
      -> ()
    block1():
      -> ()
  %447 : bool = prim::GetAttr[name="training"](%441)
  %448 : Tensor = prim::GetAttr[name="running_mean"](%441)
  %449 : Tensor = prim::GetAttr[name="running_var"](%441)
  %450 : Tensor = prim::GetAttr[name="weight"](%441)
  %451 : Tensor = prim::GetAttr[name="bias"](%441)
   = prim::If(%447) # torch/nn/functional.py:2011:4
    block0():
      %452 : int[] = aten::size(%out.232) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%452, %8) # torch/nn/functional.py:1991:17
      %454 : int = aten::len(%452) # torch/nn/functional.py:1992:19
      %455 : int = aten::sub(%454, %10) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%455, %9, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %459 : int = aten::add(%i.90, %10) # torch/nn/functional.py:1993:27
          %460 : int = aten::__getitem__(%452, %459) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %460) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.359)
      %462 : bool = aten::eq(%size_prods.357, %12) # torch/nn/functional.py:1994:7
       = prim::If(%462) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.233 : Tensor = aten::batch_norm(%out.232, %450, %451, %448, %449, %447, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %464 : __torch__.torch.nn.modules.container.___torch_mangle_23.Sequential = prim::GetAttr[name="downsample"](%368)
  %465 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="0"](%464)
  %466 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="1"](%464)
  %467 : Tensor = prim::GetAttr[name="weight"](%465)
  %468 : Tensor? = prim::GetAttr[name="bias"](%465)
  %469 : int[] = prim::ListConstruct(%10, %10)
  %470 : int[] = prim::ListConstruct(%8, %8)
  %471 : int[] = prim::ListConstruct(%12, %12)
  %input.14 : Tensor = aten::conv2d(%x.11, %467, %468, %469, %470, %471, %12) # torch/nn/modules/conv.py:415:15
  %473 : int = aten::dim(%input.14) # torch/nn/modules/batchnorm.py:276:11
  %474 : bool = aten::ne(%473, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%474) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %475 : bool = prim::GetAttr[name="training"](%466)
   = prim::If(%475) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %476 : Tensor = prim::GetAttr[name="num_batches_tracked"](%466)
      %477 : Tensor = aten::add(%476, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%466, %477)
      -> ()
    block1():
      -> ()
  %478 : bool = prim::GetAttr[name="training"](%466)
  %479 : Tensor = prim::GetAttr[name="running_mean"](%466)
  %480 : Tensor = prim::GetAttr[name="running_var"](%466)
  %481 : Tensor = prim::GetAttr[name="weight"](%466)
  %482 : Tensor = prim::GetAttr[name="bias"](%466)
   = prim::If(%478) # torch/nn/functional.py:2011:4
    block0():
      %483 : int[] = aten::size(%input.14) # torch/nn/functional.py:2012:27
      %size_prods.360 : int = aten::__getitem__(%483, %8) # torch/nn/functional.py:1991:17
      %485 : int = aten::len(%483) # torch/nn/functional.py:1992:19
      %486 : int = aten::sub(%485, %10) # torch/nn/functional.py:1992:19
      %size_prods.361 : int = prim::Loop(%486, %9, %size_prods.360) # torch/nn/functional.py:1992:4
        block0(%i.91 : int, %size_prods.362 : int):
          %490 : int = aten::add(%i.91, %10) # torch/nn/functional.py:1993:27
          %491 : int = aten::__getitem__(%483, %490) # torch/nn/functional.py:1993:22
          %size_prods.363 : int = aten::mul(%size_prods.362, %491) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.363)
      %493 : bool = aten::eq(%size_prods.361, %12) # torch/nn/functional.py:1994:7
       = prim::If(%493) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.3 : Tensor = aten::batch_norm(%input.14, %481, %482, %479, %480, %478, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.234 : Tensor = aten::add_(%out.233, %identity.3, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.18 : Tensor = aten::relu_(%out.234) # torch/nn/functional.py:1117:17
  %497 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%369)
  %498 : Tensor = prim::GetAttr[name="weight"](%497)
  %499 : Tensor? = prim::GetAttr[name="bias"](%497)
  %500 : int[] = prim::ListConstruct(%12, %12)
  %501 : int[] = prim::ListConstruct(%8, %8)
  %502 : int[] = prim::ListConstruct(%12, %12)
  %out.253 : Tensor = aten::conv2d(%input.18, %498, %499, %500, %501, %502, %12) # torch/nn/modules/conv.py:415:15
  %504 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%369)
  %505 : int = aten::dim(%out.253) # torch/nn/modules/batchnorm.py:276:11
  %506 : bool = aten::ne(%505, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%506) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %507 : bool = prim::GetAttr[name="training"](%504)
   = prim::If(%507) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %508 : Tensor = prim::GetAttr[name="num_batches_tracked"](%504)
      %509 : Tensor = aten::add(%508, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%504, %509)
      -> ()
    block1():
      -> ()
  %510 : bool = prim::GetAttr[name="training"](%504)
  %511 : Tensor = prim::GetAttr[name="running_mean"](%504)
  %512 : Tensor = prim::GetAttr[name="running_var"](%504)
  %513 : Tensor = prim::GetAttr[name="weight"](%504)
  %514 : Tensor = prim::GetAttr[name="bias"](%504)
   = prim::If(%510) # torch/nn/functional.py:2011:4
    block0():
      %515 : int[] = aten::size(%out.253) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%515, %8) # torch/nn/functional.py:1991:17
      %517 : int = aten::len(%515) # torch/nn/functional.py:1992:19
      %518 : int = aten::sub(%517, %10) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%518, %9, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %522 : int = aten::add(%i.71, %10) # torch/nn/functional.py:1993:27
          %523 : int = aten::__getitem__(%515, %522) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %523) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.283)
      %525 : bool = aten::eq(%size_prods.281, %12) # torch/nn/functional.py:1994:7
       = prim::If(%525) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.236 : Tensor = aten::batch_norm(%out.253, %513, %514, %511, %512, %510, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.237 : Tensor = aten::relu_(%out.236) # torch/nn/functional.py:1117:17
  %528 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%369)
  %529 : Tensor = prim::GetAttr[name="weight"](%528)
  %530 : Tensor? = prim::GetAttr[name="bias"](%528)
  %531 : int[] = prim::ListConstruct(%12, %12)
  %532 : int[] = prim::ListConstruct(%12, %12)
  %533 : int[] = prim::ListConstruct(%12, %12)
  %out.238 : Tensor = aten::conv2d(%out.237, %529, %530, %531, %532, %533, %12) # torch/nn/modules/conv.py:415:15
  %535 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%369)
  %536 : int = aten::dim(%out.238) # torch/nn/modules/batchnorm.py:276:11
  %537 : bool = aten::ne(%536, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%537) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %538 : bool = prim::GetAttr[name="training"](%535)
   = prim::If(%538) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %539 : Tensor = prim::GetAttr[name="num_batches_tracked"](%535)
      %540 : Tensor = aten::add(%539, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%535, %540)
      -> ()
    block1():
      -> ()
  %541 : bool = prim::GetAttr[name="training"](%535)
  %542 : Tensor = prim::GetAttr[name="running_mean"](%535)
  %543 : Tensor = prim::GetAttr[name="running_var"](%535)
  %544 : Tensor = prim::GetAttr[name="weight"](%535)
  %545 : Tensor = prim::GetAttr[name="bias"](%535)
   = prim::If(%541) # torch/nn/functional.py:2011:4
    block0():
      %546 : int[] = aten::size(%out.238) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%546, %8) # torch/nn/functional.py:1991:17
      %548 : int = aten::len(%546) # torch/nn/functional.py:1992:19
      %549 : int = aten::sub(%548, %10) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%549, %9, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %553 : int = aten::add(%i.72, %10) # torch/nn/functional.py:1993:27
          %554 : int = aten::__getitem__(%546, %553) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %554) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.287)
      %556 : bool = aten::eq(%size_prods.285, %12) # torch/nn/functional.py:1994:7
       = prim::If(%556) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.239 : Tensor = aten::batch_norm(%out.238, %544, %545, %542, %543, %541, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.240 : Tensor = aten::relu_(%out.239) # torch/nn/functional.py:1117:17
  %559 : __torch__.torch.nn.modules.conv.___torch_mangle_985.Conv2d = prim::GetAttr[name="conv3"](%369)
  %560 : Tensor = prim::GetAttr[name="weight"](%559)
  %561 : Tensor? = prim::GetAttr[name="bias"](%559)
  %562 : int[] = prim::ListConstruct(%12, %12)
  %563 : int[] = prim::ListConstruct(%8, %8)
  %564 : int[] = prim::ListConstruct(%12, %12)
  %out.241 : Tensor = aten::conv2d(%out.240, %560, %561, %562, %563, %564, %12) # torch/nn/modules/conv.py:415:15
  %566 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%369)
  %567 : int = aten::dim(%out.241) # torch/nn/modules/batchnorm.py:276:11
  %568 : bool = aten::ne(%567, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%568) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %569 : bool = prim::GetAttr[name="training"](%566)
   = prim::If(%569) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %570 : Tensor = prim::GetAttr[name="num_batches_tracked"](%566)
      %571 : Tensor = aten::add(%570, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%566, %571)
      -> ()
    block1():
      -> ()
  %572 : bool = prim::GetAttr[name="training"](%566)
  %573 : Tensor = prim::GetAttr[name="running_mean"](%566)
  %574 : Tensor = prim::GetAttr[name="running_var"](%566)
  %575 : Tensor = prim::GetAttr[name="weight"](%566)
  %576 : Tensor = prim::GetAttr[name="bias"](%566)
   = prim::If(%572) # torch/nn/functional.py:2011:4
    block0():
      %577 : int[] = aten::size(%out.241) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%577, %8) # torch/nn/functional.py:1991:17
      %579 : int = aten::len(%577) # torch/nn/functional.py:1992:19
      %580 : int = aten::sub(%579, %10) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%580, %9, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %584 : int = aten::add(%i.73, %10) # torch/nn/functional.py:1993:27
          %585 : int = aten::__getitem__(%577, %584) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %585) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.291)
      %587 : bool = aten::eq(%size_prods.289, %12) # torch/nn/functional.py:1994:7
       = prim::If(%587) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.242 : Tensor = aten::batch_norm(%out.241, %575, %576, %573, %574, %572, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.243 : Tensor = aten::add_(%out.242, %input.18, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.12 : Tensor = aten::relu_(%out.243) # torch/nn/functional.py:1117:17
  %591 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%370)
  %592 : Tensor = prim::GetAttr[name="weight"](%591)
  %593 : Tensor? = prim::GetAttr[name="bias"](%591)
  %594 : int[] = prim::ListConstruct(%12, %12)
  %595 : int[] = prim::ListConstruct(%8, %8)
  %596 : int[] = prim::ListConstruct(%12, %12)
  %out.217 : Tensor = aten::conv2d(%input.12, %592, %593, %594, %595, %596, %12) # torch/nn/modules/conv.py:415:15
  %598 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%370)
  %599 : int = aten::dim(%out.217) # torch/nn/modules/batchnorm.py:276:11
  %600 : bool = aten::ne(%599, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%600) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %601 : bool = prim::GetAttr[name="training"](%598)
   = prim::If(%601) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %602 : Tensor = prim::GetAttr[name="num_batches_tracked"](%598)
      %603 : Tensor = aten::add(%602, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%598, %603)
      -> ()
    block1():
      -> ()
  %604 : bool = prim::GetAttr[name="training"](%598)
  %605 : Tensor = prim::GetAttr[name="running_mean"](%598)
  %606 : Tensor = prim::GetAttr[name="running_var"](%598)
  %607 : Tensor = prim::GetAttr[name="weight"](%598)
  %608 : Tensor = prim::GetAttr[name="bias"](%598)
   = prim::If(%604) # torch/nn/functional.py:2011:4
    block0():
      %609 : int[] = aten::size(%out.217) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%609, %8) # torch/nn/functional.py:1991:17
      %611 : int = aten::len(%609) # torch/nn/functional.py:1992:19
      %612 : int = aten::sub(%611, %10) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%612, %9, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %616 : int = aten::add(%i.74, %10) # torch/nn/functional.py:1993:27
          %617 : int = aten::__getitem__(%609, %616) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %617) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.295)
      %619 : bool = aten::eq(%size_prods.293, %12) # torch/nn/functional.py:1994:7
       = prim::If(%619) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.245 : Tensor = aten::batch_norm(%out.217, %607, %608, %605, %606, %604, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.246 : Tensor = aten::relu_(%out.245) # torch/nn/functional.py:1117:17
  %622 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%370)
  %623 : Tensor = prim::GetAttr[name="weight"](%622)
  %624 : Tensor? = prim::GetAttr[name="bias"](%622)
  %625 : int[] = prim::ListConstruct(%12, %12)
  %626 : int[] = prim::ListConstruct(%12, %12)
  %627 : int[] = prim::ListConstruct(%12, %12)
  %out.247 : Tensor = aten::conv2d(%out.246, %623, %624, %625, %626, %627, %12) # torch/nn/modules/conv.py:415:15
  %629 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%370)
  %630 : int = aten::dim(%out.247) # torch/nn/modules/batchnorm.py:276:11
  %631 : bool = aten::ne(%630, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%631) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %632 : bool = prim::GetAttr[name="training"](%629)
   = prim::If(%632) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %633 : Tensor = prim::GetAttr[name="num_batches_tracked"](%629)
      %634 : Tensor = aten::add(%633, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%629, %634)
      -> ()
    block1():
      -> ()
  %635 : bool = prim::GetAttr[name="training"](%629)
  %636 : Tensor = prim::GetAttr[name="running_mean"](%629)
  %637 : Tensor = prim::GetAttr[name="running_var"](%629)
  %638 : Tensor = prim::GetAttr[name="weight"](%629)
  %639 : Tensor = prim::GetAttr[name="bias"](%629)
   = prim::If(%635) # torch/nn/functional.py:2011:4
    block0():
      %640 : int[] = aten::size(%out.247) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%640, %8) # torch/nn/functional.py:1991:17
      %642 : int = aten::len(%640) # torch/nn/functional.py:1992:19
      %643 : int = aten::sub(%642, %10) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%643, %9, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %647 : int = aten::add(%i.75, %10) # torch/nn/functional.py:1993:27
          %648 : int = aten::__getitem__(%640, %647) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %648) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.299)
      %650 : bool = aten::eq(%size_prods.297, %12) # torch/nn/functional.py:1994:7
       = prim::If(%650) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.248 : Tensor = aten::batch_norm(%out.247, %638, %639, %636, %637, %635, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.249 : Tensor = aten::relu_(%out.248) # torch/nn/functional.py:1117:17
  %653 : __torch__.torch.nn.modules.conv.___torch_mangle_985.Conv2d = prim::GetAttr[name="conv3"](%370)
  %654 : Tensor = prim::GetAttr[name="weight"](%653)
  %655 : Tensor? = prim::GetAttr[name="bias"](%653)
  %656 : int[] = prim::ListConstruct(%12, %12)
  %657 : int[] = prim::ListConstruct(%8, %8)
  %658 : int[] = prim::ListConstruct(%12, %12)
  %out.250 : Tensor = aten::conv2d(%out.249, %654, %655, %656, %657, %658, %12) # torch/nn/modules/conv.py:415:15
  %660 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%370)
  %661 : int = aten::dim(%out.250) # torch/nn/modules/batchnorm.py:276:11
  %662 : bool = aten::ne(%661, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%662) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %663 : bool = prim::GetAttr[name="training"](%660)
   = prim::If(%663) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %664 : Tensor = prim::GetAttr[name="num_batches_tracked"](%660)
      %665 : Tensor = aten::add(%664, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%660, %665)
      -> ()
    block1():
      -> ()
  %666 : bool = prim::GetAttr[name="training"](%660)
  %667 : Tensor = prim::GetAttr[name="running_mean"](%660)
  %668 : Tensor = prim::GetAttr[name="running_var"](%660)
  %669 : Tensor = prim::GetAttr[name="weight"](%660)
  %670 : Tensor = prim::GetAttr[name="bias"](%660)
   = prim::If(%666) # torch/nn/functional.py:2011:4
    block0():
      %671 : int[] = aten::size(%out.250) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%671, %8) # torch/nn/functional.py:1991:17
      %673 : int = aten::len(%671) # torch/nn/functional.py:1992:19
      %674 : int = aten::sub(%673, %10) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%674, %9, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %678 : int = aten::add(%i.76, %10) # torch/nn/functional.py:1993:27
          %679 : int = aten::__getitem__(%671, %678) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %679) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.303)
      %681 : bool = aten::eq(%size_prods.301, %12) # torch/nn/functional.py:1994:7
       = prim::If(%681) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.251 : Tensor = aten::batch_norm(%out.250, %669, %670, %667, %668, %666, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.252 : Tensor = aten::add_(%out.251, %input.12, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.16 : Tensor = aten::relu_(%out.252) # torch/nn/functional.py:1117:17
  %685 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%371)
  %686 : Tensor = prim::GetAttr[name="weight"](%685)
  %687 : Tensor? = prim::GetAttr[name="bias"](%685)
  %688 : int[] = prim::ListConstruct(%12, %12)
  %689 : int[] = prim::ListConstruct(%8, %8)
  %690 : int[] = prim::ListConstruct(%12, %12)
  %out.262 : Tensor = aten::conv2d(%input.16, %686, %687, %688, %689, %690, %12) # torch/nn/modules/conv.py:415:15
  %692 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%371)
  %693 : int = aten::dim(%out.262) # torch/nn/modules/batchnorm.py:276:11
  %694 : bool = aten::ne(%693, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%694) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %695 : bool = prim::GetAttr[name="training"](%692)
   = prim::If(%695) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %696 : Tensor = prim::GetAttr[name="num_batches_tracked"](%692)
      %697 : Tensor = aten::add(%696, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%692, %697)
      -> ()
    block1():
      -> ()
  %698 : bool = prim::GetAttr[name="training"](%692)
  %699 : Tensor = prim::GetAttr[name="running_mean"](%692)
  %700 : Tensor = prim::GetAttr[name="running_var"](%692)
  %701 : Tensor = prim::GetAttr[name="weight"](%692)
  %702 : Tensor = prim::GetAttr[name="bias"](%692)
   = prim::If(%698) # torch/nn/functional.py:2011:4
    block0():
      %703 : int[] = aten::size(%out.262) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%703, %8) # torch/nn/functional.py:1991:17
      %705 : int = aten::len(%703) # torch/nn/functional.py:1992:19
      %706 : int = aten::sub(%705, %10) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%706, %9, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %710 : int = aten::add(%i.92, %10) # torch/nn/functional.py:1993:27
          %711 : int = aten::__getitem__(%703, %710) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %711) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.367)
      %713 : bool = aten::eq(%size_prods.365, %12) # torch/nn/functional.py:1994:7
       = prim::If(%713) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.263 : Tensor = aten::batch_norm(%out.262, %701, %702, %699, %700, %698, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.264 : Tensor = aten::relu_(%out.263) # torch/nn/functional.py:1117:17
  %716 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%371)
  %717 : Tensor = prim::GetAttr[name="weight"](%716)
  %718 : Tensor? = prim::GetAttr[name="bias"](%716)
  %719 : int[] = prim::ListConstruct(%12, %12)
  %720 : int[] = prim::ListConstruct(%12, %12)
  %721 : int[] = prim::ListConstruct(%12, %12)
  %out.265 : Tensor = aten::conv2d(%out.264, %717, %718, %719, %720, %721, %12) # torch/nn/modules/conv.py:415:15
  %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%371)
  %724 : int = aten::dim(%out.265) # torch/nn/modules/batchnorm.py:276:11
  %725 : bool = aten::ne(%724, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%725) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %726 : bool = prim::GetAttr[name="training"](%723)
   = prim::If(%726) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
      %728 : Tensor = aten::add(%727, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%723, %728)
      -> ()
    block1():
      -> ()
  %729 : bool = prim::GetAttr[name="training"](%723)
  %730 : Tensor = prim::GetAttr[name="running_mean"](%723)
  %731 : Tensor = prim::GetAttr[name="running_var"](%723)
  %732 : Tensor = prim::GetAttr[name="weight"](%723)
  %733 : Tensor = prim::GetAttr[name="bias"](%723)
   = prim::If(%729) # torch/nn/functional.py:2011:4
    block0():
      %734 : int[] = aten::size(%out.265) # torch/nn/functional.py:2012:27
      %size_prods.368 : int = aten::__getitem__(%734, %8) # torch/nn/functional.py:1991:17
      %736 : int = aten::len(%734) # torch/nn/functional.py:1992:19
      %737 : int = aten::sub(%736, %10) # torch/nn/functional.py:1992:19
      %size_prods.369 : int = prim::Loop(%737, %9, %size_prods.368) # torch/nn/functional.py:1992:4
        block0(%i.93 : int, %size_prods.370 : int):
          %741 : int = aten::add(%i.93, %10) # torch/nn/functional.py:1993:27
          %742 : int = aten::__getitem__(%734, %741) # torch/nn/functional.py:1993:22
          %size_prods.371 : int = aten::mul(%size_prods.370, %742) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.371)
      %744 : bool = aten::eq(%size_prods.369, %12) # torch/nn/functional.py:1994:7
       = prim::If(%744) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.266 : Tensor = aten::batch_norm(%out.265, %732, %733, %730, %731, %729, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.267 : Tensor = aten::relu_(%out.266) # torch/nn/functional.py:1117:17
  %747 : __torch__.torch.nn.modules.conv.___torch_mangle_985.Conv2d = prim::GetAttr[name="conv3"](%371)
  %748 : Tensor = prim::GetAttr[name="weight"](%747)
  %749 : Tensor? = prim::GetAttr[name="bias"](%747)
  %750 : int[] = prim::ListConstruct(%12, %12)
  %751 : int[] = prim::ListConstruct(%8, %8)
  %752 : int[] = prim::ListConstruct(%12, %12)
  %out.268 : Tensor = aten::conv2d(%out.267, %748, %749, %750, %751, %752, %12) # torch/nn/modules/conv.py:415:15
  %754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%371)
  %755 : int = aten::dim(%out.268) # torch/nn/modules/batchnorm.py:276:11
  %756 : bool = aten::ne(%755, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%756) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %757 : bool = prim::GetAttr[name="training"](%754)
   = prim::If(%757) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %758 : Tensor = prim::GetAttr[name="num_batches_tracked"](%754)
      %759 : Tensor = aten::add(%758, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%754, %759)
      -> ()
    block1():
      -> ()
  %760 : bool = prim::GetAttr[name="training"](%754)
  %761 : Tensor = prim::GetAttr[name="running_mean"](%754)
  %762 : Tensor = prim::GetAttr[name="running_var"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Tensor = prim::GetAttr[name="bias"](%754)
   = prim::If(%760) # torch/nn/functional.py:2011:4
    block0():
      %765 : int[] = aten::size(%out.268) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%765, %8) # torch/nn/functional.py:1991:17
      %767 : int = aten::len(%765) # torch/nn/functional.py:1992:19
      %768 : int = aten::sub(%767, %10) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%768, %9, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %772 : int = aten::add(%i.94, %10) # torch/nn/functional.py:1993:27
          %773 : int = aten::__getitem__(%765, %772) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %773) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.375)
      %775 : bool = aten::eq(%size_prods.373, %12) # torch/nn/functional.py:1994:7
       = prim::If(%775) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.269 : Tensor = aten::batch_norm(%out.268, %763, %764, %761, %762, %760, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.270 : Tensor = aten::add_(%out.269, %input.16, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.13 : Tensor = aten::relu_(%out.270) # torch/nn/functional.py:1117:17
  %779 : __torch__.torch.nn.modules.container.___torch_mangle_1137.Sequential = prim::GetAttr[name="layer3"](%self)
  %780 : __torch__.torchvision.models.resnet.___torch_mangle_1135.Bottleneck = prim::GetAttr[name="0"](%779)
  %781 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="1"](%779)
  %782 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="2"](%779)
  %783 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="3"](%779)
  %784 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="4"](%779)
  %785 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="5"](%779)
  %786 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="6"](%779)
  %787 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="7"](%779)
  %788 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="8"](%779)
  %789 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="9"](%779)
  %790 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="10"](%779)
  %791 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="11"](%779)
  %792 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="12"](%779)
  %793 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="13"](%779)
  %794 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="14"](%779)
  %795 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="15"](%779)
  %796 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="16"](%779)
  %797 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="17"](%779)
  %798 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="18"](%779)
  %799 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="19"](%779)
  %800 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="20"](%779)
  %801 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="21"](%779)
  %802 : __torch__.torchvision.models.resnet.___torch_mangle_1136.Bottleneck = prim::GetAttr[name="22"](%779)
  %803 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv1"](%780)
  %804 : Tensor = prim::GetAttr[name="weight"](%803)
  %805 : Tensor? = prim::GetAttr[name="bias"](%803)
  %806 : int[] = prim::ListConstruct(%12, %12)
  %807 : int[] = prim::ListConstruct(%8, %8)
  %808 : int[] = prim::ListConstruct(%12, %12)
  %out.271 : Tensor = aten::conv2d(%x.13, %804, %805, %806, %807, %808, %12) # torch/nn/modules/conv.py:415:15
  %810 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%780)
  %811 : int = aten::dim(%out.271) # torch/nn/modules/batchnorm.py:276:11
  %812 : bool = aten::ne(%811, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%812) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %813 : bool = prim::GetAttr[name="training"](%810)
   = prim::If(%813) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %814 : Tensor = prim::GetAttr[name="num_batches_tracked"](%810)
      %815 : Tensor = aten::add(%814, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%810, %815)
      -> ()
    block1():
      -> ()
  %816 : bool = prim::GetAttr[name="training"](%810)
  %817 : Tensor = prim::GetAttr[name="running_mean"](%810)
  %818 : Tensor = prim::GetAttr[name="running_var"](%810)
  %819 : Tensor = prim::GetAttr[name="weight"](%810)
  %820 : Tensor = prim::GetAttr[name="bias"](%810)
   = prim::If(%816) # torch/nn/functional.py:2011:4
    block0():
      %821 : int[] = aten::size(%out.271) # torch/nn/functional.py:2012:27
      %size_prods.376 : int = aten::__getitem__(%821, %8) # torch/nn/functional.py:1991:17
      %823 : int = aten::len(%821) # torch/nn/functional.py:1992:19
      %824 : int = aten::sub(%823, %10) # torch/nn/functional.py:1992:19
      %size_prods.377 : int = prim::Loop(%824, %9, %size_prods.376) # torch/nn/functional.py:1992:4
        block0(%i.95 : int, %size_prods.378 : int):
          %828 : int = aten::add(%i.95, %10) # torch/nn/functional.py:1993:27
          %829 : int = aten::__getitem__(%821, %828) # torch/nn/functional.py:1993:22
          %size_prods.379 : int = aten::mul(%size_prods.378, %829) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.379)
      %831 : bool = aten::eq(%size_prods.377, %12) # torch/nn/functional.py:1994:7
       = prim::If(%831) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.272 : Tensor = aten::batch_norm(%out.271, %819, %820, %817, %818, %816, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.273 : Tensor = aten::relu_(%out.272) # torch/nn/functional.py:1117:17
  %834 : __torch__.torch.nn.modules.conv.___torch_mangle_944.Conv2d = prim::GetAttr[name="conv2"](%780)
  %835 : Tensor = prim::GetAttr[name="weight"](%834)
  %836 : Tensor? = prim::GetAttr[name="bias"](%834)
  %837 : int[] = prim::ListConstruct(%10, %10)
  %838 : int[] = prim::ListConstruct(%12, %12)
  %839 : int[] = prim::ListConstruct(%12, %12)
  %out.274 : Tensor = aten::conv2d(%out.273, %835, %836, %837, %838, %839, %12) # torch/nn/modules/conv.py:415:15
  %841 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%780)
  %842 : int = aten::dim(%out.274) # torch/nn/modules/batchnorm.py:276:11
  %843 : bool = aten::ne(%842, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %844 : bool = prim::GetAttr[name="training"](%841)
   = prim::If(%844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%841)
      %846 : Tensor = aten::add(%845, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%841, %846)
      -> ()
    block1():
      -> ()
  %847 : bool = prim::GetAttr[name="training"](%841)
  %848 : Tensor = prim::GetAttr[name="running_mean"](%841)
  %849 : Tensor = prim::GetAttr[name="running_var"](%841)
  %850 : Tensor = prim::GetAttr[name="weight"](%841)
  %851 : Tensor = prim::GetAttr[name="bias"](%841)
   = prim::If(%847) # torch/nn/functional.py:2011:4
    block0():
      %852 : int[] = aten::size(%out.274) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%852, %8) # torch/nn/functional.py:1991:17
      %854 : int = aten::len(%852) # torch/nn/functional.py:1992:19
      %855 : int = aten::sub(%854, %10) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%855, %9, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %859 : int = aten::add(%i.96, %10) # torch/nn/functional.py:1993:27
          %860 : int = aten::__getitem__(%852, %859) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %860) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.383)
      %862 : bool = aten::eq(%size_prods.381, %12) # torch/nn/functional.py:1994:7
       = prim::If(%862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.275 : Tensor = aten::batch_norm(%out.274, %850, %851, %848, %849, %847, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.276 : Tensor = aten::relu_(%out.275) # torch/nn/functional.py:1117:17
  %865 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%780)
  %866 : Tensor = prim::GetAttr[name="weight"](%865)
  %867 : Tensor? = prim::GetAttr[name="bias"](%865)
  %868 : int[] = prim::ListConstruct(%12, %12)
  %869 : int[] = prim::ListConstruct(%8, %8)
  %870 : int[] = prim::ListConstruct(%12, %12)
  %out.277 : Tensor = aten::conv2d(%out.276, %866, %867, %868, %869, %870, %12) # torch/nn/modules/conv.py:415:15
  %872 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%780)
  %873 : int = aten::dim(%out.277) # torch/nn/modules/batchnorm.py:276:11
  %874 : bool = aten::ne(%873, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%874) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %875 : bool = prim::GetAttr[name="training"](%872)
   = prim::If(%875) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %876 : Tensor = prim::GetAttr[name="num_batches_tracked"](%872)
      %877 : Tensor = aten::add(%876, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%872, %877)
      -> ()
    block1():
      -> ()
  %878 : bool = prim::GetAttr[name="training"](%872)
  %879 : Tensor = prim::GetAttr[name="running_mean"](%872)
  %880 : Tensor = prim::GetAttr[name="running_var"](%872)
  %881 : Tensor = prim::GetAttr[name="weight"](%872)
  %882 : Tensor = prim::GetAttr[name="bias"](%872)
   = prim::If(%878) # torch/nn/functional.py:2011:4
    block0():
      %883 : int[] = aten::size(%out.277) # torch/nn/functional.py:2012:27
      %size_prods.384 : int = aten::__getitem__(%883, %8) # torch/nn/functional.py:1991:17
      %885 : int = aten::len(%883) # torch/nn/functional.py:1992:19
      %886 : int = aten::sub(%885, %10) # torch/nn/functional.py:1992:19
      %size_prods.385 : int = prim::Loop(%886, %9, %size_prods.384) # torch/nn/functional.py:1992:4
        block0(%i.97 : int, %size_prods.386 : int):
          %890 : int = aten::add(%i.97, %10) # torch/nn/functional.py:1993:27
          %891 : int = aten::__getitem__(%883, %890) # torch/nn/functional.py:1993:22
          %size_prods.387 : int = aten::mul(%size_prods.386, %891) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.387)
      %893 : bool = aten::eq(%size_prods.385, %12) # torch/nn/functional.py:1994:7
       = prim::If(%893) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.278 : Tensor = aten::batch_norm(%out.277, %881, %882, %879, %880, %878, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %895 : __torch__.torch.nn.modules.container.___torch_mangle_940.Sequential = prim::GetAttr[name="downsample"](%780)
  %896 : __torch__.torch.nn.modules.conv.___torch_mangle_939.Conv2d = prim::GetAttr[name="0"](%895)
  %897 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="1"](%895)
  %898 : Tensor = prim::GetAttr[name="weight"](%896)
  %899 : Tensor? = prim::GetAttr[name="bias"](%896)
  %900 : int[] = prim::ListConstruct(%10, %10)
  %901 : int[] = prim::ListConstruct(%8, %8)
  %902 : int[] = prim::ListConstruct(%12, %12)
  %input.20 : Tensor = aten::conv2d(%x.13, %898, %899, %900, %901, %902, %12) # torch/nn/modules/conv.py:415:15
  %904 : int = aten::dim(%input.20) # torch/nn/modules/batchnorm.py:276:11
  %905 : bool = aten::ne(%904, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%905) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %906 : bool = prim::GetAttr[name="training"](%897)
   = prim::If(%906) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %907 : Tensor = prim::GetAttr[name="num_batches_tracked"](%897)
      %908 : Tensor = aten::add(%907, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%897, %908)
      -> ()
    block1():
      -> ()
  %909 : bool = prim::GetAttr[name="training"](%897)
  %910 : Tensor = prim::GetAttr[name="running_mean"](%897)
  %911 : Tensor = prim::GetAttr[name="running_var"](%897)
  %912 : Tensor = prim::GetAttr[name="weight"](%897)
  %913 : Tensor = prim::GetAttr[name="bias"](%897)
   = prim::If(%909) # torch/nn/functional.py:2011:4
    block0():
      %914 : int[] = aten::size(%input.20) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%914, %8) # torch/nn/functional.py:1991:17
      %916 : int = aten::len(%914) # torch/nn/functional.py:1992:19
      %917 : int = aten::sub(%916, %10) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%917, %9, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %921 : int = aten::add(%i.98, %10) # torch/nn/functional.py:1993:27
          %922 : int = aten::__getitem__(%914, %921) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %922) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.391)
      %924 : bool = aten::eq(%size_prods.389, %12) # torch/nn/functional.py:1994:7
       = prim::If(%924) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.4 : Tensor = aten::batch_norm(%input.20, %912, %913, %910, %911, %909, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.279 : Tensor = aten::add_(%out.278, %identity.4, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.22 : Tensor = aten::relu_(%out.279) # torch/nn/functional.py:1117:17
  %928 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%781)
  %929 : Tensor = prim::GetAttr[name="weight"](%928)
  %930 : Tensor? = prim::GetAttr[name="bias"](%928)
  %931 : int[] = prim::ListConstruct(%12, %12)
  %932 : int[] = prim::ListConstruct(%8, %8)
  %933 : int[] = prim::ListConstruct(%12, %12)
  %out.280 : Tensor = aten::conv2d(%input.22, %929, %930, %931, %932, %933, %12) # torch/nn/modules/conv.py:415:15
  %935 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%781)
  %936 : int = aten::dim(%out.280) # torch/nn/modules/batchnorm.py:276:11
  %937 : bool = aten::ne(%936, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%937) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %938 : bool = prim::GetAttr[name="training"](%935)
   = prim::If(%938) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %939 : Tensor = prim::GetAttr[name="num_batches_tracked"](%935)
      %940 : Tensor = aten::add(%939, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%935, %940)
      -> ()
    block1():
      -> ()
  %941 : bool = prim::GetAttr[name="training"](%935)
  %942 : Tensor = prim::GetAttr[name="running_mean"](%935)
  %943 : Tensor = prim::GetAttr[name="running_var"](%935)
  %944 : Tensor = prim::GetAttr[name="weight"](%935)
  %945 : Tensor = prim::GetAttr[name="bias"](%935)
   = prim::If(%941) # torch/nn/functional.py:2011:4
    block0():
      %946 : int[] = aten::size(%out.280) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%946, %8) # torch/nn/functional.py:1991:17
      %948 : int = aten::len(%946) # torch/nn/functional.py:1992:19
      %949 : int = aten::sub(%948, %10) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%949, %9, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %953 : int = aten::add(%i.99, %10) # torch/nn/functional.py:1993:27
          %954 : int = aten::__getitem__(%946, %953) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %954) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.395)
      %956 : bool = aten::eq(%size_prods.393, %12) # torch/nn/functional.py:1994:7
       = prim::If(%956) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.281 : Tensor = aten::batch_norm(%out.280, %944, %945, %942, %943, %941, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.282 : Tensor = aten::relu_(%out.281) # torch/nn/functional.py:1117:17
  %959 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%781)
  %960 : Tensor = prim::GetAttr[name="weight"](%959)
  %961 : Tensor? = prim::GetAttr[name="bias"](%959)
  %962 : int[] = prim::ListConstruct(%12, %12)
  %963 : int[] = prim::ListConstruct(%12, %12)
  %964 : int[] = prim::ListConstruct(%12, %12)
  %out.283 : Tensor = aten::conv2d(%out.282, %960, %961, %962, %963, %964, %12) # torch/nn/modules/conv.py:415:15
  %966 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%781)
  %967 : int = aten::dim(%out.283) # torch/nn/modules/batchnorm.py:276:11
  %968 : bool = aten::ne(%967, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%968) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %969 : bool = prim::GetAttr[name="training"](%966)
   = prim::If(%969) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %970 : Tensor = prim::GetAttr[name="num_batches_tracked"](%966)
      %971 : Tensor = aten::add(%970, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%966, %971)
      -> ()
    block1():
      -> ()
  %972 : bool = prim::GetAttr[name="training"](%966)
  %973 : Tensor = prim::GetAttr[name="running_mean"](%966)
  %974 : Tensor = prim::GetAttr[name="running_var"](%966)
  %975 : Tensor = prim::GetAttr[name="weight"](%966)
  %976 : Tensor = prim::GetAttr[name="bias"](%966)
   = prim::If(%972) # torch/nn/functional.py:2011:4
    block0():
      %977 : int[] = aten::size(%out.283) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%977, %8) # torch/nn/functional.py:1991:17
      %979 : int = aten::len(%977) # torch/nn/functional.py:1992:19
      %980 : int = aten::sub(%979, %10) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%980, %9, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %984 : int = aten::add(%i.100, %10) # torch/nn/functional.py:1993:27
          %985 : int = aten::__getitem__(%977, %984) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %985) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.399)
      %987 : bool = aten::eq(%size_prods.397, %12) # torch/nn/functional.py:1994:7
       = prim::If(%987) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.284 : Tensor = aten::batch_norm(%out.283, %975, %976, %973, %974, %972, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.285 : Tensor = aten::relu_(%out.284) # torch/nn/functional.py:1117:17
  %990 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%781)
  %991 : Tensor = prim::GetAttr[name="weight"](%990)
  %992 : Tensor? = prim::GetAttr[name="bias"](%990)
  %993 : int[] = prim::ListConstruct(%12, %12)
  %994 : int[] = prim::ListConstruct(%8, %8)
  %995 : int[] = prim::ListConstruct(%12, %12)
  %out.286 : Tensor = aten::conv2d(%out.285, %991, %992, %993, %994, %995, %12) # torch/nn/modules/conv.py:415:15
  %997 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%781)
  %998 : int = aten::dim(%out.286) # torch/nn/modules/batchnorm.py:276:11
  %999 : bool = aten::ne(%998, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%999) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1000 : bool = prim::GetAttr[name="training"](%997)
   = prim::If(%1000) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1001 : Tensor = prim::GetAttr[name="num_batches_tracked"](%997)
      %1002 : Tensor = aten::add(%1001, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%997, %1002)
      -> ()
    block1():
      -> ()
  %1003 : bool = prim::GetAttr[name="training"](%997)
  %1004 : Tensor = prim::GetAttr[name="running_mean"](%997)
  %1005 : Tensor = prim::GetAttr[name="running_var"](%997)
  %1006 : Tensor = prim::GetAttr[name="weight"](%997)
  %1007 : Tensor = prim::GetAttr[name="bias"](%997)
   = prim::If(%1003) # torch/nn/functional.py:2011:4
    block0():
      %1008 : int[] = aten::size(%out.286) # torch/nn/functional.py:2012:27
      %size_prods.400 : int = aten::__getitem__(%1008, %8) # torch/nn/functional.py:1991:17
      %1010 : int = aten::len(%1008) # torch/nn/functional.py:1992:19
      %1011 : int = aten::sub(%1010, %10) # torch/nn/functional.py:1992:19
      %size_prods.401 : int = prim::Loop(%1011, %9, %size_prods.400) # torch/nn/functional.py:1992:4
        block0(%i.101 : int, %size_prods.402 : int):
          %1015 : int = aten::add(%i.101, %10) # torch/nn/functional.py:1993:27
          %1016 : int = aten::__getitem__(%1008, %1015) # torch/nn/functional.py:1993:22
          %size_prods.403 : int = aten::mul(%size_prods.402, %1016) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.403)
      %1018 : bool = aten::eq(%size_prods.401, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1018) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.287 : Tensor = aten::batch_norm(%out.286, %1006, %1007, %1004, %1005, %1003, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.288 : Tensor = aten::add_(%out.287, %input.22, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.24 : Tensor = aten::relu_(%out.288) # torch/nn/functional.py:1117:17
  %1022 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%782)
  %1023 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1024 : Tensor? = prim::GetAttr[name="bias"](%1022)
  %1025 : int[] = prim::ListConstruct(%12, %12)
  %1026 : int[] = prim::ListConstruct(%8, %8)
  %1027 : int[] = prim::ListConstruct(%12, %12)
  %out.37 : Tensor = aten::conv2d(%input.24, %1023, %1024, %1025, %1026, %1027, %12) # torch/nn/modules/conv.py:415:15
  %1029 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%782)
  %1030 : int = aten::dim(%out.37) # torch/nn/modules/batchnorm.py:276:11
  %1031 : bool = aten::ne(%1030, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1031) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1032 : bool = prim::GetAttr[name="training"](%1029)
   = prim::If(%1032) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1033 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1029)
      %1034 : Tensor = aten::add(%1033, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1029, %1034)
      -> ()
    block1():
      -> ()
  %1035 : bool = prim::GetAttr[name="training"](%1029)
  %1036 : Tensor = prim::GetAttr[name="running_mean"](%1029)
  %1037 : Tensor = prim::GetAttr[name="running_var"](%1029)
  %1038 : Tensor = prim::GetAttr[name="weight"](%1029)
  %1039 : Tensor = prim::GetAttr[name="bias"](%1029)
   = prim::If(%1035) # torch/nn/functional.py:2011:4
    block0():
      %1040 : int[] = aten::size(%out.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%1040, %8) # torch/nn/functional.py:1991:17
      %1042 : int = aten::len(%1040) # torch/nn/functional.py:1992:19
      %1043 : int = aten::sub(%1042, %10) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%1043, %9, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %1047 : int = aten::add(%i.11, %10) # torch/nn/functional.py:1993:27
          %1048 : int = aten::__getitem__(%1040, %1047) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %1048) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.43)
      %1050 : bool = aten::eq(%size_prods.41, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1050) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.29 : Tensor = aten::batch_norm(%out.37, %1038, %1039, %1036, %1037, %1035, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.30 : Tensor = aten::relu_(%out.29) # torch/nn/functional.py:1117:17
  %1053 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%782)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1053)
  %1055 : Tensor? = prim::GetAttr[name="bias"](%1053)
  %1056 : int[] = prim::ListConstruct(%12, %12)
  %1057 : int[] = prim::ListConstruct(%12, %12)
  %1058 : int[] = prim::ListConstruct(%12, %12)
  %out.31 : Tensor = aten::conv2d(%out.30, %1054, %1055, %1056, %1057, %1058, %12) # torch/nn/modules/conv.py:415:15
  %1060 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%782)
  %1061 : int = aten::dim(%out.31) # torch/nn/modules/batchnorm.py:276:11
  %1062 : bool = aten::ne(%1061, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1062) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1063 : bool = prim::GetAttr[name="training"](%1060)
   = prim::If(%1063) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1064 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1060)
      %1065 : Tensor = aten::add(%1064, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1060, %1065)
      -> ()
    block1():
      -> ()
  %1066 : bool = prim::GetAttr[name="training"](%1060)
  %1067 : Tensor = prim::GetAttr[name="running_mean"](%1060)
  %1068 : Tensor = prim::GetAttr[name="running_var"](%1060)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1060)
  %1070 : Tensor = prim::GetAttr[name="bias"](%1060)
   = prim::If(%1066) # torch/nn/functional.py:2011:4
    block0():
      %1071 : int[] = aten::size(%out.31) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%1071, %8) # torch/nn/functional.py:1991:17
      %1073 : int = aten::len(%1071) # torch/nn/functional.py:1992:19
      %1074 : int = aten::sub(%1073, %10) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%1074, %9, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %1078 : int = aten::add(%i.12, %10) # torch/nn/functional.py:1993:27
          %1079 : int = aten::__getitem__(%1071, %1078) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %1079) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.47)
      %1081 : bool = aten::eq(%size_prods.45, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1081) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.32 : Tensor = aten::batch_norm(%out.31, %1069, %1070, %1067, %1068, %1066, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.33 : Tensor = aten::relu_(%out.32) # torch/nn/functional.py:1117:17
  %1084 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%782)
  %1085 : Tensor = prim::GetAttr[name="weight"](%1084)
  %1086 : Tensor? = prim::GetAttr[name="bias"](%1084)
  %1087 : int[] = prim::ListConstruct(%12, %12)
  %1088 : int[] = prim::ListConstruct(%8, %8)
  %1089 : int[] = prim::ListConstruct(%12, %12)
  %out.34 : Tensor = aten::conv2d(%out.33, %1085, %1086, %1087, %1088, %1089, %12) # torch/nn/modules/conv.py:415:15
  %1091 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%782)
  %1092 : int = aten::dim(%out.34) # torch/nn/modules/batchnorm.py:276:11
  %1093 : bool = aten::ne(%1092, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1093) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1094 : bool = prim::GetAttr[name="training"](%1091)
   = prim::If(%1094) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1095 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1091)
      %1096 : Tensor = aten::add(%1095, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1091, %1096)
      -> ()
    block1():
      -> ()
  %1097 : bool = prim::GetAttr[name="training"](%1091)
  %1098 : Tensor = prim::GetAttr[name="running_mean"](%1091)
  %1099 : Tensor = prim::GetAttr[name="running_var"](%1091)
  %1100 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1101 : Tensor = prim::GetAttr[name="bias"](%1091)
   = prim::If(%1097) # torch/nn/functional.py:2011:4
    block0():
      %1102 : int[] = aten::size(%out.34) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%1102, %8) # torch/nn/functional.py:1991:17
      %1104 : int = aten::len(%1102) # torch/nn/functional.py:1992:19
      %1105 : int = aten::sub(%1104, %10) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%1105, %9, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %1109 : int = aten::add(%i.13, %10) # torch/nn/functional.py:1993:27
          %1110 : int = aten::__getitem__(%1102, %1109) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %1110) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.51)
      %1112 : bool = aten::eq(%size_prods.49, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1112) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.35 : Tensor = aten::batch_norm(%out.34, %1100, %1101, %1098, %1099, %1097, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.36 : Tensor = aten::add_(%out.35, %input.24, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.26 : Tensor = aten::relu_(%out.36) # torch/nn/functional.py:1117:17
  %1116 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%783)
  %1117 : Tensor = prim::GetAttr[name="weight"](%1116)
  %1118 : Tensor? = prim::GetAttr[name="bias"](%1116)
  %1119 : int[] = prim::ListConstruct(%12, %12)
  %1120 : int[] = prim::ListConstruct(%8, %8)
  %1121 : int[] = prim::ListConstruct(%12, %12)
  %out.46 : Tensor = aten::conv2d(%input.26, %1117, %1118, %1119, %1120, %1121, %12) # torch/nn/modules/conv.py:415:15
  %1123 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%783)
  %1124 : int = aten::dim(%out.46) # torch/nn/modules/batchnorm.py:276:11
  %1125 : bool = aten::ne(%1124, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1125) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1126 : bool = prim::GetAttr[name="training"](%1123)
   = prim::If(%1126) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1127 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1123)
      %1128 : Tensor = aten::add(%1127, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1123, %1128)
      -> ()
    block1():
      -> ()
  %1129 : bool = prim::GetAttr[name="training"](%1123)
  %1130 : Tensor = prim::GetAttr[name="running_mean"](%1123)
  %1131 : Tensor = prim::GetAttr[name="running_var"](%1123)
  %1132 : Tensor = prim::GetAttr[name="weight"](%1123)
  %1133 : Tensor = prim::GetAttr[name="bias"](%1123)
   = prim::If(%1129) # torch/nn/functional.py:2011:4
    block0():
      %1134 : int[] = aten::size(%out.46) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%1134, %8) # torch/nn/functional.py:1991:17
      %1136 : int = aten::len(%1134) # torch/nn/functional.py:1992:19
      %1137 : int = aten::sub(%1136, %10) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%1137, %9, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %1141 : int = aten::add(%i.14, %10) # torch/nn/functional.py:1993:27
          %1142 : int = aten::__getitem__(%1134, %1141) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %1142) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.55)
      %1144 : bool = aten::eq(%size_prods.53, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1144) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.38 : Tensor = aten::batch_norm(%out.46, %1132, %1133, %1130, %1131, %1129, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.39 : Tensor = aten::relu_(%out.38) # torch/nn/functional.py:1117:17
  %1147 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%783)
  %1148 : Tensor = prim::GetAttr[name="weight"](%1147)
  %1149 : Tensor? = prim::GetAttr[name="bias"](%1147)
  %1150 : int[] = prim::ListConstruct(%12, %12)
  %1151 : int[] = prim::ListConstruct(%12, %12)
  %1152 : int[] = prim::ListConstruct(%12, %12)
  %out.40 : Tensor = aten::conv2d(%out.39, %1148, %1149, %1150, %1151, %1152, %12) # torch/nn/modules/conv.py:415:15
  %1154 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%783)
  %1155 : int = aten::dim(%out.40) # torch/nn/modules/batchnorm.py:276:11
  %1156 : bool = aten::ne(%1155, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1156) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1157 : bool = prim::GetAttr[name="training"](%1154)
   = prim::If(%1157) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1158 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1154)
      %1159 : Tensor = aten::add(%1158, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1154, %1159)
      -> ()
    block1():
      -> ()
  %1160 : bool = prim::GetAttr[name="training"](%1154)
  %1161 : Tensor = prim::GetAttr[name="running_mean"](%1154)
  %1162 : Tensor = prim::GetAttr[name="running_var"](%1154)
  %1163 : Tensor = prim::GetAttr[name="weight"](%1154)
  %1164 : Tensor = prim::GetAttr[name="bias"](%1154)
   = prim::If(%1160) # torch/nn/functional.py:2011:4
    block0():
      %1165 : int[] = aten::size(%out.40) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%1165, %8) # torch/nn/functional.py:1991:17
      %1167 : int = aten::len(%1165) # torch/nn/functional.py:1992:19
      %1168 : int = aten::sub(%1167, %10) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%1168, %9, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %1172 : int = aten::add(%i.15, %10) # torch/nn/functional.py:1993:27
          %1173 : int = aten::__getitem__(%1165, %1172) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %1173) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.59)
      %1175 : bool = aten::eq(%size_prods.57, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1175) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.41 : Tensor = aten::batch_norm(%out.40, %1163, %1164, %1161, %1162, %1160, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.42 : Tensor = aten::relu_(%out.41) # torch/nn/functional.py:1117:17
  %1178 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%783)
  %1179 : Tensor = prim::GetAttr[name="weight"](%1178)
  %1180 : Tensor? = prim::GetAttr[name="bias"](%1178)
  %1181 : int[] = prim::ListConstruct(%12, %12)
  %1182 : int[] = prim::ListConstruct(%8, %8)
  %1183 : int[] = prim::ListConstruct(%12, %12)
  %out.43 : Tensor = aten::conv2d(%out.42, %1179, %1180, %1181, %1182, %1183, %12) # torch/nn/modules/conv.py:415:15
  %1185 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%783)
  %1186 : int = aten::dim(%out.43) # torch/nn/modules/batchnorm.py:276:11
  %1187 : bool = aten::ne(%1186, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1187) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1188 : bool = prim::GetAttr[name="training"](%1185)
   = prim::If(%1188) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1189 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1185)
      %1190 : Tensor = aten::add(%1189, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1185, %1190)
      -> ()
    block1():
      -> ()
  %1191 : bool = prim::GetAttr[name="training"](%1185)
  %1192 : Tensor = prim::GetAttr[name="running_mean"](%1185)
  %1193 : Tensor = prim::GetAttr[name="running_var"](%1185)
  %1194 : Tensor = prim::GetAttr[name="weight"](%1185)
  %1195 : Tensor = prim::GetAttr[name="bias"](%1185)
   = prim::If(%1191) # torch/nn/functional.py:2011:4
    block0():
      %1196 : int[] = aten::size(%out.43) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%1196, %8) # torch/nn/functional.py:1991:17
      %1198 : int = aten::len(%1196) # torch/nn/functional.py:1992:19
      %1199 : int = aten::sub(%1198, %10) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%1199, %9, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %1203 : int = aten::add(%i.16, %10) # torch/nn/functional.py:1993:27
          %1204 : int = aten::__getitem__(%1196, %1203) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %1204) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.63)
      %1206 : bool = aten::eq(%size_prods.61, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1206) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.44 : Tensor = aten::batch_norm(%out.43, %1194, %1195, %1192, %1193, %1191, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.45 : Tensor = aten::add_(%out.44, %input.26, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.9 : Tensor = aten::relu_(%out.45) # torch/nn/functional.py:1117:17
  %1210 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%784)
  %1211 : Tensor = prim::GetAttr[name="weight"](%1210)
  %1212 : Tensor? = prim::GetAttr[name="bias"](%1210)
  %1213 : int[] = prim::ListConstruct(%12, %12)
  %1214 : int[] = prim::ListConstruct(%8, %8)
  %1215 : int[] = prim::ListConstruct(%12, %12)
  %out.55 : Tensor = aten::conv2d(%input.9, %1211, %1212, %1213, %1214, %1215, %12) # torch/nn/modules/conv.py:415:15
  %1217 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%784)
  %1218 : int = aten::dim(%out.55) # torch/nn/modules/batchnorm.py:276:11
  %1219 : bool = aten::ne(%1218, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1219) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1220 : bool = prim::GetAttr[name="training"](%1217)
   = prim::If(%1220) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1221 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1217)
      %1222 : Tensor = aten::add(%1221, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1217, %1222)
      -> ()
    block1():
      -> ()
  %1223 : bool = prim::GetAttr[name="training"](%1217)
  %1224 : Tensor = prim::GetAttr[name="running_mean"](%1217)
  %1225 : Tensor = prim::GetAttr[name="running_var"](%1217)
  %1226 : Tensor = prim::GetAttr[name="weight"](%1217)
  %1227 : Tensor = prim::GetAttr[name="bias"](%1217)
   = prim::If(%1223) # torch/nn/functional.py:2011:4
    block0():
      %1228 : int[] = aten::size(%out.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%1228, %8) # torch/nn/functional.py:1991:17
      %1230 : int = aten::len(%1228) # torch/nn/functional.py:1992:19
      %1231 : int = aten::sub(%1230, %10) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%1231, %9, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %1235 : int = aten::add(%i.17, %10) # torch/nn/functional.py:1993:27
          %1236 : int = aten::__getitem__(%1228, %1235) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %1236) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.67)
      %1238 : bool = aten::eq(%size_prods.65, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1238) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.47 : Tensor = aten::batch_norm(%out.55, %1226, %1227, %1224, %1225, %1223, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.48 : Tensor = aten::relu_(%out.47) # torch/nn/functional.py:1117:17
  %1241 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%784)
  %1242 : Tensor = prim::GetAttr[name="weight"](%1241)
  %1243 : Tensor? = prim::GetAttr[name="bias"](%1241)
  %1244 : int[] = prim::ListConstruct(%12, %12)
  %1245 : int[] = prim::ListConstruct(%12, %12)
  %1246 : int[] = prim::ListConstruct(%12, %12)
  %out.49 : Tensor = aten::conv2d(%out.48, %1242, %1243, %1244, %1245, %1246, %12) # torch/nn/modules/conv.py:415:15
  %1248 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%784)
  %1249 : int = aten::dim(%out.49) # torch/nn/modules/batchnorm.py:276:11
  %1250 : bool = aten::ne(%1249, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1250) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1251 : bool = prim::GetAttr[name="training"](%1248)
   = prim::If(%1251) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1252 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1248)
      %1253 : Tensor = aten::add(%1252, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1248, %1253)
      -> ()
    block1():
      -> ()
  %1254 : bool = prim::GetAttr[name="training"](%1248)
  %1255 : Tensor = prim::GetAttr[name="running_mean"](%1248)
  %1256 : Tensor = prim::GetAttr[name="running_var"](%1248)
  %1257 : Tensor = prim::GetAttr[name="weight"](%1248)
  %1258 : Tensor = prim::GetAttr[name="bias"](%1248)
   = prim::If(%1254) # torch/nn/functional.py:2011:4
    block0():
      %1259 : int[] = aten::size(%out.49) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%1259, %8) # torch/nn/functional.py:1991:17
      %1261 : int = aten::len(%1259) # torch/nn/functional.py:1992:19
      %1262 : int = aten::sub(%1261, %10) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%1262, %9, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %1266 : int = aten::add(%i.18, %10) # torch/nn/functional.py:1993:27
          %1267 : int = aten::__getitem__(%1259, %1266) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %1267) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.71)
      %1269 : bool = aten::eq(%size_prods.69, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1269) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.50 : Tensor = aten::batch_norm(%out.49, %1257, %1258, %1255, %1256, %1254, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.51 : Tensor = aten::relu_(%out.50) # torch/nn/functional.py:1117:17
  %1272 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%784)
  %1273 : Tensor = prim::GetAttr[name="weight"](%1272)
  %1274 : Tensor? = prim::GetAttr[name="bias"](%1272)
  %1275 : int[] = prim::ListConstruct(%12, %12)
  %1276 : int[] = prim::ListConstruct(%8, %8)
  %1277 : int[] = prim::ListConstruct(%12, %12)
  %out.52 : Tensor = aten::conv2d(%out.51, %1273, %1274, %1275, %1276, %1277, %12) # torch/nn/modules/conv.py:415:15
  %1279 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%784)
  %1280 : int = aten::dim(%out.52) # torch/nn/modules/batchnorm.py:276:11
  %1281 : bool = aten::ne(%1280, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1281) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1282 : bool = prim::GetAttr[name="training"](%1279)
   = prim::If(%1282) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1283 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1279)
      %1284 : Tensor = aten::add(%1283, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1279, %1284)
      -> ()
    block1():
      -> ()
  %1285 : bool = prim::GetAttr[name="training"](%1279)
  %1286 : Tensor = prim::GetAttr[name="running_mean"](%1279)
  %1287 : Tensor = prim::GetAttr[name="running_var"](%1279)
  %1288 : Tensor = prim::GetAttr[name="weight"](%1279)
  %1289 : Tensor = prim::GetAttr[name="bias"](%1279)
   = prim::If(%1285) # torch/nn/functional.py:2011:4
    block0():
      %1290 : int[] = aten::size(%out.52) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%1290, %8) # torch/nn/functional.py:1991:17
      %1292 : int = aten::len(%1290) # torch/nn/functional.py:1992:19
      %1293 : int = aten::sub(%1292, %10) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%1293, %9, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %1297 : int = aten::add(%i.19, %10) # torch/nn/functional.py:1993:27
          %1298 : int = aten::__getitem__(%1290, %1297) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %1298) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.75)
      %1300 : bool = aten::eq(%size_prods.73, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1300) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.53 : Tensor = aten::batch_norm(%out.52, %1288, %1289, %1286, %1287, %1285, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.54 : Tensor = aten::add_(%out.53, %input.9, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.11 : Tensor = aten::relu_(%out.54) # torch/nn/functional.py:1117:17
  %1304 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%785)
  %1305 : Tensor = prim::GetAttr[name="weight"](%1304)
  %1306 : Tensor? = prim::GetAttr[name="bias"](%1304)
  %1307 : int[] = prim::ListConstruct(%12, %12)
  %1308 : int[] = prim::ListConstruct(%8, %8)
  %1309 : int[] = prim::ListConstruct(%12, %12)
  %out.64 : Tensor = aten::conv2d(%input.11, %1305, %1306, %1307, %1308, %1309, %12) # torch/nn/modules/conv.py:415:15
  %1311 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%785)
  %1312 : int = aten::dim(%out.64) # torch/nn/modules/batchnorm.py:276:11
  %1313 : bool = aten::ne(%1312, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1313) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1314 : bool = prim::GetAttr[name="training"](%1311)
   = prim::If(%1314) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1311)
      %1316 : Tensor = aten::add(%1315, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1311, %1316)
      -> ()
    block1():
      -> ()
  %1317 : bool = prim::GetAttr[name="training"](%1311)
  %1318 : Tensor = prim::GetAttr[name="running_mean"](%1311)
  %1319 : Tensor = prim::GetAttr[name="running_var"](%1311)
  %1320 : Tensor = prim::GetAttr[name="weight"](%1311)
  %1321 : Tensor = prim::GetAttr[name="bias"](%1311)
   = prim::If(%1317) # torch/nn/functional.py:2011:4
    block0():
      %1322 : int[] = aten::size(%out.64) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%1322, %8) # torch/nn/functional.py:1991:17
      %1324 : int = aten::len(%1322) # torch/nn/functional.py:1992:19
      %1325 : int = aten::sub(%1324, %10) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%1325, %9, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %1329 : int = aten::add(%i.20, %10) # torch/nn/functional.py:1993:27
          %1330 : int = aten::__getitem__(%1322, %1329) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %1330) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.79)
      %1332 : bool = aten::eq(%size_prods.77, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1332) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.56 : Tensor = aten::batch_norm(%out.64, %1320, %1321, %1318, %1319, %1317, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.57 : Tensor = aten::relu_(%out.56) # torch/nn/functional.py:1117:17
  %1335 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%785)
  %1336 : Tensor = prim::GetAttr[name="weight"](%1335)
  %1337 : Tensor? = prim::GetAttr[name="bias"](%1335)
  %1338 : int[] = prim::ListConstruct(%12, %12)
  %1339 : int[] = prim::ListConstruct(%12, %12)
  %1340 : int[] = prim::ListConstruct(%12, %12)
  %out.58 : Tensor = aten::conv2d(%out.57, %1336, %1337, %1338, %1339, %1340, %12) # torch/nn/modules/conv.py:415:15
  %1342 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%785)
  %1343 : int = aten::dim(%out.58) # torch/nn/modules/batchnorm.py:276:11
  %1344 : bool = aten::ne(%1343, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1344) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1345 : bool = prim::GetAttr[name="training"](%1342)
   = prim::If(%1345) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1346 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1342)
      %1347 : Tensor = aten::add(%1346, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1342, %1347)
      -> ()
    block1():
      -> ()
  %1348 : bool = prim::GetAttr[name="training"](%1342)
  %1349 : Tensor = prim::GetAttr[name="running_mean"](%1342)
  %1350 : Tensor = prim::GetAttr[name="running_var"](%1342)
  %1351 : Tensor = prim::GetAttr[name="weight"](%1342)
  %1352 : Tensor = prim::GetAttr[name="bias"](%1342)
   = prim::If(%1348) # torch/nn/functional.py:2011:4
    block0():
      %1353 : int[] = aten::size(%out.58) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%1353, %8) # torch/nn/functional.py:1991:17
      %1355 : int = aten::len(%1353) # torch/nn/functional.py:1992:19
      %1356 : int = aten::sub(%1355, %10) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%1356, %9, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %1360 : int = aten::add(%i.21, %10) # torch/nn/functional.py:1993:27
          %1361 : int = aten::__getitem__(%1353, %1360) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %1361) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.83)
      %1363 : bool = aten::eq(%size_prods.81, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1363) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.59 : Tensor = aten::batch_norm(%out.58, %1351, %1352, %1349, %1350, %1348, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.60 : Tensor = aten::relu_(%out.59) # torch/nn/functional.py:1117:17
  %1366 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%785)
  %1367 : Tensor = prim::GetAttr[name="weight"](%1366)
  %1368 : Tensor? = prim::GetAttr[name="bias"](%1366)
  %1369 : int[] = prim::ListConstruct(%12, %12)
  %1370 : int[] = prim::ListConstruct(%8, %8)
  %1371 : int[] = prim::ListConstruct(%12, %12)
  %out.61 : Tensor = aten::conv2d(%out.60, %1367, %1368, %1369, %1370, %1371, %12) # torch/nn/modules/conv.py:415:15
  %1373 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%785)
  %1374 : int = aten::dim(%out.61) # torch/nn/modules/batchnorm.py:276:11
  %1375 : bool = aten::ne(%1374, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1375) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1376 : bool = prim::GetAttr[name="training"](%1373)
   = prim::If(%1376) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1377 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1373)
      %1378 : Tensor = aten::add(%1377, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1373, %1378)
      -> ()
    block1():
      -> ()
  %1379 : bool = prim::GetAttr[name="training"](%1373)
  %1380 : Tensor = prim::GetAttr[name="running_mean"](%1373)
  %1381 : Tensor = prim::GetAttr[name="running_var"](%1373)
  %1382 : Tensor = prim::GetAttr[name="weight"](%1373)
  %1383 : Tensor = prim::GetAttr[name="bias"](%1373)
   = prim::If(%1379) # torch/nn/functional.py:2011:4
    block0():
      %1384 : int[] = aten::size(%out.61) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%1384, %8) # torch/nn/functional.py:1991:17
      %1386 : int = aten::len(%1384) # torch/nn/functional.py:1992:19
      %1387 : int = aten::sub(%1386, %10) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%1387, %9, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %1391 : int = aten::add(%i.22, %10) # torch/nn/functional.py:1993:27
          %1392 : int = aten::__getitem__(%1384, %1391) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %1392) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.87)
      %1394 : bool = aten::eq(%size_prods.85, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1394) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.62 : Tensor = aten::batch_norm(%out.61, %1382, %1383, %1380, %1381, %1379, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.63 : Tensor = aten::add_(%out.62, %input.11, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.13 : Tensor = aten::relu_(%out.63) # torch/nn/functional.py:1117:17
  %1398 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%786)
  %1399 : Tensor = prim::GetAttr[name="weight"](%1398)
  %1400 : Tensor? = prim::GetAttr[name="bias"](%1398)
  %1401 : int[] = prim::ListConstruct(%12, %12)
  %1402 : int[] = prim::ListConstruct(%8, %8)
  %1403 : int[] = prim::ListConstruct(%12, %12)
  %out.73 : Tensor = aten::conv2d(%input.13, %1399, %1400, %1401, %1402, %1403, %12) # torch/nn/modules/conv.py:415:15
  %1405 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%786)
  %1406 : int = aten::dim(%out.73) # torch/nn/modules/batchnorm.py:276:11
  %1407 : bool = aten::ne(%1406, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1407) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1408 : bool = prim::GetAttr[name="training"](%1405)
   = prim::If(%1408) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1409 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1405)
      %1410 : Tensor = aten::add(%1409, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1405, %1410)
      -> ()
    block1():
      -> ()
  %1411 : bool = prim::GetAttr[name="training"](%1405)
  %1412 : Tensor = prim::GetAttr[name="running_mean"](%1405)
  %1413 : Tensor = prim::GetAttr[name="running_var"](%1405)
  %1414 : Tensor = prim::GetAttr[name="weight"](%1405)
  %1415 : Tensor = prim::GetAttr[name="bias"](%1405)
   = prim::If(%1411) # torch/nn/functional.py:2011:4
    block0():
      %1416 : int[] = aten::size(%out.73) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%1416, %8) # torch/nn/functional.py:1991:17
      %1418 : int = aten::len(%1416) # torch/nn/functional.py:1992:19
      %1419 : int = aten::sub(%1418, %10) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%1419, %9, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %1423 : int = aten::add(%i.23, %10) # torch/nn/functional.py:1993:27
          %1424 : int = aten::__getitem__(%1416, %1423) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %1424) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.91)
      %1426 : bool = aten::eq(%size_prods.89, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1426) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.65 : Tensor = aten::batch_norm(%out.73, %1414, %1415, %1412, %1413, %1411, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.66 : Tensor = aten::relu_(%out.65) # torch/nn/functional.py:1117:17
  %1429 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%786)
  %1430 : Tensor = prim::GetAttr[name="weight"](%1429)
  %1431 : Tensor? = prim::GetAttr[name="bias"](%1429)
  %1432 : int[] = prim::ListConstruct(%12, %12)
  %1433 : int[] = prim::ListConstruct(%12, %12)
  %1434 : int[] = prim::ListConstruct(%12, %12)
  %out.67 : Tensor = aten::conv2d(%out.66, %1430, %1431, %1432, %1433, %1434, %12) # torch/nn/modules/conv.py:415:15
  %1436 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%786)
  %1437 : int = aten::dim(%out.67) # torch/nn/modules/batchnorm.py:276:11
  %1438 : bool = aten::ne(%1437, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1438) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1439 : bool = prim::GetAttr[name="training"](%1436)
   = prim::If(%1439) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1440 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1436)
      %1441 : Tensor = aten::add(%1440, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1436, %1441)
      -> ()
    block1():
      -> ()
  %1442 : bool = prim::GetAttr[name="training"](%1436)
  %1443 : Tensor = prim::GetAttr[name="running_mean"](%1436)
  %1444 : Tensor = prim::GetAttr[name="running_var"](%1436)
  %1445 : Tensor = prim::GetAttr[name="weight"](%1436)
  %1446 : Tensor = prim::GetAttr[name="bias"](%1436)
   = prim::If(%1442) # torch/nn/functional.py:2011:4
    block0():
      %1447 : int[] = aten::size(%out.67) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%1447, %8) # torch/nn/functional.py:1991:17
      %1449 : int = aten::len(%1447) # torch/nn/functional.py:1992:19
      %1450 : int = aten::sub(%1449, %10) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%1450, %9, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %1454 : int = aten::add(%i.24, %10) # torch/nn/functional.py:1993:27
          %1455 : int = aten::__getitem__(%1447, %1454) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %1455) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.95)
      %1457 : bool = aten::eq(%size_prods.93, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1457) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.68 : Tensor = aten::batch_norm(%out.67, %1445, %1446, %1443, %1444, %1442, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.69 : Tensor = aten::relu_(%out.68) # torch/nn/functional.py:1117:17
  %1460 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%786)
  %1461 : Tensor = prim::GetAttr[name="weight"](%1460)
  %1462 : Tensor? = prim::GetAttr[name="bias"](%1460)
  %1463 : int[] = prim::ListConstruct(%12, %12)
  %1464 : int[] = prim::ListConstruct(%8, %8)
  %1465 : int[] = prim::ListConstruct(%12, %12)
  %out.70 : Tensor = aten::conv2d(%out.69, %1461, %1462, %1463, %1464, %1465, %12) # torch/nn/modules/conv.py:415:15
  %1467 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%786)
  %1468 : int = aten::dim(%out.70) # torch/nn/modules/batchnorm.py:276:11
  %1469 : bool = aten::ne(%1468, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1469) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1470 : bool = prim::GetAttr[name="training"](%1467)
   = prim::If(%1470) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1471 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1467)
      %1472 : Tensor = aten::add(%1471, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1467, %1472)
      -> ()
    block1():
      -> ()
  %1473 : bool = prim::GetAttr[name="training"](%1467)
  %1474 : Tensor = prim::GetAttr[name="running_mean"](%1467)
  %1475 : Tensor = prim::GetAttr[name="running_var"](%1467)
  %1476 : Tensor = prim::GetAttr[name="weight"](%1467)
  %1477 : Tensor = prim::GetAttr[name="bias"](%1467)
   = prim::If(%1473) # torch/nn/functional.py:2011:4
    block0():
      %1478 : int[] = aten::size(%out.70) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%1478, %8) # torch/nn/functional.py:1991:17
      %1480 : int = aten::len(%1478) # torch/nn/functional.py:1992:19
      %1481 : int = aten::sub(%1480, %10) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%1481, %9, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %1485 : int = aten::add(%i.25, %10) # torch/nn/functional.py:1993:27
          %1486 : int = aten::__getitem__(%1478, %1485) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %1486) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.99)
      %1488 : bool = aten::eq(%size_prods.97, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1488) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.71 : Tensor = aten::batch_norm(%out.70, %1476, %1477, %1474, %1475, %1473, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.72 : Tensor = aten::add_(%out.71, %input.13, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.15 : Tensor = aten::relu_(%out.72) # torch/nn/functional.py:1117:17
  %1492 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%787)
  %1493 : Tensor = prim::GetAttr[name="weight"](%1492)
  %1494 : Tensor? = prim::GetAttr[name="bias"](%1492)
  %1495 : int[] = prim::ListConstruct(%12, %12)
  %1496 : int[] = prim::ListConstruct(%8, %8)
  %1497 : int[] = prim::ListConstruct(%12, %12)
  %out.82 : Tensor = aten::conv2d(%input.15, %1493, %1494, %1495, %1496, %1497, %12) # torch/nn/modules/conv.py:415:15
  %1499 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%787)
  %1500 : int = aten::dim(%out.82) # torch/nn/modules/batchnorm.py:276:11
  %1501 : bool = aten::ne(%1500, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1501) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1502 : bool = prim::GetAttr[name="training"](%1499)
   = prim::If(%1502) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1503 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1499)
      %1504 : Tensor = aten::add(%1503, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1499, %1504)
      -> ()
    block1():
      -> ()
  %1505 : bool = prim::GetAttr[name="training"](%1499)
  %1506 : Tensor = prim::GetAttr[name="running_mean"](%1499)
  %1507 : Tensor = prim::GetAttr[name="running_var"](%1499)
  %1508 : Tensor = prim::GetAttr[name="weight"](%1499)
  %1509 : Tensor = prim::GetAttr[name="bias"](%1499)
   = prim::If(%1505) # torch/nn/functional.py:2011:4
    block0():
      %1510 : int[] = aten::size(%out.82) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%1510, %8) # torch/nn/functional.py:1991:17
      %1512 : int = aten::len(%1510) # torch/nn/functional.py:1992:19
      %1513 : int = aten::sub(%1512, %10) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%1513, %9, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %1517 : int = aten::add(%i.26, %10) # torch/nn/functional.py:1993:27
          %1518 : int = aten::__getitem__(%1510, %1517) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %1518) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.103)
      %1520 : bool = aten::eq(%size_prods.101, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1520) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.74 : Tensor = aten::batch_norm(%out.82, %1508, %1509, %1506, %1507, %1505, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.75 : Tensor = aten::relu_(%out.74) # torch/nn/functional.py:1117:17
  %1523 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%787)
  %1524 : Tensor = prim::GetAttr[name="weight"](%1523)
  %1525 : Tensor? = prim::GetAttr[name="bias"](%1523)
  %1526 : int[] = prim::ListConstruct(%12, %12)
  %1527 : int[] = prim::ListConstruct(%12, %12)
  %1528 : int[] = prim::ListConstruct(%12, %12)
  %out.76 : Tensor = aten::conv2d(%out.75, %1524, %1525, %1526, %1527, %1528, %12) # torch/nn/modules/conv.py:415:15
  %1530 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%787)
  %1531 : int = aten::dim(%out.76) # torch/nn/modules/batchnorm.py:276:11
  %1532 : bool = aten::ne(%1531, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1532) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1533 : bool = prim::GetAttr[name="training"](%1530)
   = prim::If(%1533) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1534 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1530)
      %1535 : Tensor = aten::add(%1534, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1530, %1535)
      -> ()
    block1():
      -> ()
  %1536 : bool = prim::GetAttr[name="training"](%1530)
  %1537 : Tensor = prim::GetAttr[name="running_mean"](%1530)
  %1538 : Tensor = prim::GetAttr[name="running_var"](%1530)
  %1539 : Tensor = prim::GetAttr[name="weight"](%1530)
  %1540 : Tensor = prim::GetAttr[name="bias"](%1530)
   = prim::If(%1536) # torch/nn/functional.py:2011:4
    block0():
      %1541 : int[] = aten::size(%out.76) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%1541, %8) # torch/nn/functional.py:1991:17
      %1543 : int = aten::len(%1541) # torch/nn/functional.py:1992:19
      %1544 : int = aten::sub(%1543, %10) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%1544, %9, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %1548 : int = aten::add(%i.27, %10) # torch/nn/functional.py:1993:27
          %1549 : int = aten::__getitem__(%1541, %1548) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %1549) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.107)
      %1551 : bool = aten::eq(%size_prods.105, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1551) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.77 : Tensor = aten::batch_norm(%out.76, %1539, %1540, %1537, %1538, %1536, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.78 : Tensor = aten::relu_(%out.77) # torch/nn/functional.py:1117:17
  %1554 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%787)
  %1555 : Tensor = prim::GetAttr[name="weight"](%1554)
  %1556 : Tensor? = prim::GetAttr[name="bias"](%1554)
  %1557 : int[] = prim::ListConstruct(%12, %12)
  %1558 : int[] = prim::ListConstruct(%8, %8)
  %1559 : int[] = prim::ListConstruct(%12, %12)
  %out.79 : Tensor = aten::conv2d(%out.78, %1555, %1556, %1557, %1558, %1559, %12) # torch/nn/modules/conv.py:415:15
  %1561 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%787)
  %1562 : int = aten::dim(%out.79) # torch/nn/modules/batchnorm.py:276:11
  %1563 : bool = aten::ne(%1562, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1563) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1564 : bool = prim::GetAttr[name="training"](%1561)
   = prim::If(%1564) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1565 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1561)
      %1566 : Tensor = aten::add(%1565, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1561, %1566)
      -> ()
    block1():
      -> ()
  %1567 : bool = prim::GetAttr[name="training"](%1561)
  %1568 : Tensor = prim::GetAttr[name="running_mean"](%1561)
  %1569 : Tensor = prim::GetAttr[name="running_var"](%1561)
  %1570 : Tensor = prim::GetAttr[name="weight"](%1561)
  %1571 : Tensor = prim::GetAttr[name="bias"](%1561)
   = prim::If(%1567) # torch/nn/functional.py:2011:4
    block0():
      %1572 : int[] = aten::size(%out.79) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%1572, %8) # torch/nn/functional.py:1991:17
      %1574 : int = aten::len(%1572) # torch/nn/functional.py:1992:19
      %1575 : int = aten::sub(%1574, %10) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%1575, %9, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %1579 : int = aten::add(%i.28, %10) # torch/nn/functional.py:1993:27
          %1580 : int = aten::__getitem__(%1572, %1579) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %1580) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.111)
      %1582 : bool = aten::eq(%size_prods.109, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1582) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.80 : Tensor = aten::batch_norm(%out.79, %1570, %1571, %1568, %1569, %1567, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.81 : Tensor = aten::add_(%out.80, %input.15, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.17 : Tensor = aten::relu_(%out.81) # torch/nn/functional.py:1117:17
  %1586 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%788)
  %1587 : Tensor = prim::GetAttr[name="weight"](%1586)
  %1588 : Tensor? = prim::GetAttr[name="bias"](%1586)
  %1589 : int[] = prim::ListConstruct(%12, %12)
  %1590 : int[] = prim::ListConstruct(%8, %8)
  %1591 : int[] = prim::ListConstruct(%12, %12)
  %out.91 : Tensor = aten::conv2d(%input.17, %1587, %1588, %1589, %1590, %1591, %12) # torch/nn/modules/conv.py:415:15
  %1593 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%788)
  %1594 : int = aten::dim(%out.91) # torch/nn/modules/batchnorm.py:276:11
  %1595 : bool = aten::ne(%1594, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1595) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1596 : bool = prim::GetAttr[name="training"](%1593)
   = prim::If(%1596) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1597 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1593)
      %1598 : Tensor = aten::add(%1597, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1593, %1598)
      -> ()
    block1():
      -> ()
  %1599 : bool = prim::GetAttr[name="training"](%1593)
  %1600 : Tensor = prim::GetAttr[name="running_mean"](%1593)
  %1601 : Tensor = prim::GetAttr[name="running_var"](%1593)
  %1602 : Tensor = prim::GetAttr[name="weight"](%1593)
  %1603 : Tensor = prim::GetAttr[name="bias"](%1593)
   = prim::If(%1599) # torch/nn/functional.py:2011:4
    block0():
      %1604 : int[] = aten::size(%out.91) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%1604, %8) # torch/nn/functional.py:1991:17
      %1606 : int = aten::len(%1604) # torch/nn/functional.py:1992:19
      %1607 : int = aten::sub(%1606, %10) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%1607, %9, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %1611 : int = aten::add(%i.29, %10) # torch/nn/functional.py:1993:27
          %1612 : int = aten::__getitem__(%1604, %1611) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %1612) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.115)
      %1614 : bool = aten::eq(%size_prods.113, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1614) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.83 : Tensor = aten::batch_norm(%out.91, %1602, %1603, %1600, %1601, %1599, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.84 : Tensor = aten::relu_(%out.83) # torch/nn/functional.py:1117:17
  %1617 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%788)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1617)
  %1619 : Tensor? = prim::GetAttr[name="bias"](%1617)
  %1620 : int[] = prim::ListConstruct(%12, %12)
  %1621 : int[] = prim::ListConstruct(%12, %12)
  %1622 : int[] = prim::ListConstruct(%12, %12)
  %out.85 : Tensor = aten::conv2d(%out.84, %1618, %1619, %1620, %1621, %1622, %12) # torch/nn/modules/conv.py:415:15
  %1624 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%788)
  %1625 : int = aten::dim(%out.85) # torch/nn/modules/batchnorm.py:276:11
  %1626 : bool = aten::ne(%1625, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1626) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1627 : bool = prim::GetAttr[name="training"](%1624)
   = prim::If(%1627) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1628 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1624)
      %1629 : Tensor = aten::add(%1628, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1624, %1629)
      -> ()
    block1():
      -> ()
  %1630 : bool = prim::GetAttr[name="training"](%1624)
  %1631 : Tensor = prim::GetAttr[name="running_mean"](%1624)
  %1632 : Tensor = prim::GetAttr[name="running_var"](%1624)
  %1633 : Tensor = prim::GetAttr[name="weight"](%1624)
  %1634 : Tensor = prim::GetAttr[name="bias"](%1624)
   = prim::If(%1630) # torch/nn/functional.py:2011:4
    block0():
      %1635 : int[] = aten::size(%out.85) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%1635, %8) # torch/nn/functional.py:1991:17
      %1637 : int = aten::len(%1635) # torch/nn/functional.py:1992:19
      %1638 : int = aten::sub(%1637, %10) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%1638, %9, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %1642 : int = aten::add(%i.30, %10) # torch/nn/functional.py:1993:27
          %1643 : int = aten::__getitem__(%1635, %1642) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %1643) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.119)
      %1645 : bool = aten::eq(%size_prods.117, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1645) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.86 : Tensor = aten::batch_norm(%out.85, %1633, %1634, %1631, %1632, %1630, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.87 : Tensor = aten::relu_(%out.86) # torch/nn/functional.py:1117:17
  %1648 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%788)
  %1649 : Tensor = prim::GetAttr[name="weight"](%1648)
  %1650 : Tensor? = prim::GetAttr[name="bias"](%1648)
  %1651 : int[] = prim::ListConstruct(%12, %12)
  %1652 : int[] = prim::ListConstruct(%8, %8)
  %1653 : int[] = prim::ListConstruct(%12, %12)
  %out.88 : Tensor = aten::conv2d(%out.87, %1649, %1650, %1651, %1652, %1653, %12) # torch/nn/modules/conv.py:415:15
  %1655 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%788)
  %1656 : int = aten::dim(%out.88) # torch/nn/modules/batchnorm.py:276:11
  %1657 : bool = aten::ne(%1656, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1657) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1658 : bool = prim::GetAttr[name="training"](%1655)
   = prim::If(%1658) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1659 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1655)
      %1660 : Tensor = aten::add(%1659, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1655, %1660)
      -> ()
    block1():
      -> ()
  %1661 : bool = prim::GetAttr[name="training"](%1655)
  %1662 : Tensor = prim::GetAttr[name="running_mean"](%1655)
  %1663 : Tensor = prim::GetAttr[name="running_var"](%1655)
  %1664 : Tensor = prim::GetAttr[name="weight"](%1655)
  %1665 : Tensor = prim::GetAttr[name="bias"](%1655)
   = prim::If(%1661) # torch/nn/functional.py:2011:4
    block0():
      %1666 : int[] = aten::size(%out.88) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%1666, %8) # torch/nn/functional.py:1991:17
      %1668 : int = aten::len(%1666) # torch/nn/functional.py:1992:19
      %1669 : int = aten::sub(%1668, %10) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%1669, %9, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %1673 : int = aten::add(%i.31, %10) # torch/nn/functional.py:1993:27
          %1674 : int = aten::__getitem__(%1666, %1673) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %1674) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.123)
      %1676 : bool = aten::eq(%size_prods.121, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1676) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.89 : Tensor = aten::batch_norm(%out.88, %1664, %1665, %1662, %1663, %1661, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.90 : Tensor = aten::add_(%out.89, %input.17, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.19 : Tensor = aten::relu_(%out.90) # torch/nn/functional.py:1117:17
  %1680 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%789)
  %1681 : Tensor = prim::GetAttr[name="weight"](%1680)
  %1682 : Tensor? = prim::GetAttr[name="bias"](%1680)
  %1683 : int[] = prim::ListConstruct(%12, %12)
  %1684 : int[] = prim::ListConstruct(%8, %8)
  %1685 : int[] = prim::ListConstruct(%12, %12)
  %out.100 : Tensor = aten::conv2d(%input.19, %1681, %1682, %1683, %1684, %1685, %12) # torch/nn/modules/conv.py:415:15
  %1687 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%789)
  %1688 : int = aten::dim(%out.100) # torch/nn/modules/batchnorm.py:276:11
  %1689 : bool = aten::ne(%1688, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1689) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1690 : bool = prim::GetAttr[name="training"](%1687)
   = prim::If(%1690) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1691 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1687)
      %1692 : Tensor = aten::add(%1691, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1687, %1692)
      -> ()
    block1():
      -> ()
  %1693 : bool = prim::GetAttr[name="training"](%1687)
  %1694 : Tensor = prim::GetAttr[name="running_mean"](%1687)
  %1695 : Tensor = prim::GetAttr[name="running_var"](%1687)
  %1696 : Tensor = prim::GetAttr[name="weight"](%1687)
  %1697 : Tensor = prim::GetAttr[name="bias"](%1687)
   = prim::If(%1693) # torch/nn/functional.py:2011:4
    block0():
      %1698 : int[] = aten::size(%out.100) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%1698, %8) # torch/nn/functional.py:1991:17
      %1700 : int = aten::len(%1698) # torch/nn/functional.py:1992:19
      %1701 : int = aten::sub(%1700, %10) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%1701, %9, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %1705 : int = aten::add(%i.32, %10) # torch/nn/functional.py:1993:27
          %1706 : int = aten::__getitem__(%1698, %1705) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %1706) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.127)
      %1708 : bool = aten::eq(%size_prods.125, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1708) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.92 : Tensor = aten::batch_norm(%out.100, %1696, %1697, %1694, %1695, %1693, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.93 : Tensor = aten::relu_(%out.92) # torch/nn/functional.py:1117:17
  %1711 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%789)
  %1712 : Tensor = prim::GetAttr[name="weight"](%1711)
  %1713 : Tensor? = prim::GetAttr[name="bias"](%1711)
  %1714 : int[] = prim::ListConstruct(%12, %12)
  %1715 : int[] = prim::ListConstruct(%12, %12)
  %1716 : int[] = prim::ListConstruct(%12, %12)
  %out.94 : Tensor = aten::conv2d(%out.93, %1712, %1713, %1714, %1715, %1716, %12) # torch/nn/modules/conv.py:415:15
  %1718 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%789)
  %1719 : int = aten::dim(%out.94) # torch/nn/modules/batchnorm.py:276:11
  %1720 : bool = aten::ne(%1719, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1720) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1721 : bool = prim::GetAttr[name="training"](%1718)
   = prim::If(%1721) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1722 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1718)
      %1723 : Tensor = aten::add(%1722, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1718, %1723)
      -> ()
    block1():
      -> ()
  %1724 : bool = prim::GetAttr[name="training"](%1718)
  %1725 : Tensor = prim::GetAttr[name="running_mean"](%1718)
  %1726 : Tensor = prim::GetAttr[name="running_var"](%1718)
  %1727 : Tensor = prim::GetAttr[name="weight"](%1718)
  %1728 : Tensor = prim::GetAttr[name="bias"](%1718)
   = prim::If(%1724) # torch/nn/functional.py:2011:4
    block0():
      %1729 : int[] = aten::size(%out.94) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%1729, %8) # torch/nn/functional.py:1991:17
      %1731 : int = aten::len(%1729) # torch/nn/functional.py:1992:19
      %1732 : int = aten::sub(%1731, %10) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%1732, %9, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %1736 : int = aten::add(%i.33, %10) # torch/nn/functional.py:1993:27
          %1737 : int = aten::__getitem__(%1729, %1736) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %1737) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.131)
      %1739 : bool = aten::eq(%size_prods.129, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1739) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.95 : Tensor = aten::batch_norm(%out.94, %1727, %1728, %1725, %1726, %1724, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.96 : Tensor = aten::relu_(%out.95) # torch/nn/functional.py:1117:17
  %1742 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%789)
  %1743 : Tensor = prim::GetAttr[name="weight"](%1742)
  %1744 : Tensor? = prim::GetAttr[name="bias"](%1742)
  %1745 : int[] = prim::ListConstruct(%12, %12)
  %1746 : int[] = prim::ListConstruct(%8, %8)
  %1747 : int[] = prim::ListConstruct(%12, %12)
  %out.97 : Tensor = aten::conv2d(%out.96, %1743, %1744, %1745, %1746, %1747, %12) # torch/nn/modules/conv.py:415:15
  %1749 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%789)
  %1750 : int = aten::dim(%out.97) # torch/nn/modules/batchnorm.py:276:11
  %1751 : bool = aten::ne(%1750, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1751) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1752 : bool = prim::GetAttr[name="training"](%1749)
   = prim::If(%1752) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1753 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1749)
      %1754 : Tensor = aten::add(%1753, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1749, %1754)
      -> ()
    block1():
      -> ()
  %1755 : bool = prim::GetAttr[name="training"](%1749)
  %1756 : Tensor = prim::GetAttr[name="running_mean"](%1749)
  %1757 : Tensor = prim::GetAttr[name="running_var"](%1749)
  %1758 : Tensor = prim::GetAttr[name="weight"](%1749)
  %1759 : Tensor = prim::GetAttr[name="bias"](%1749)
   = prim::If(%1755) # torch/nn/functional.py:2011:4
    block0():
      %1760 : int[] = aten::size(%out.97) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%1760, %8) # torch/nn/functional.py:1991:17
      %1762 : int = aten::len(%1760) # torch/nn/functional.py:1992:19
      %1763 : int = aten::sub(%1762, %10) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%1763, %9, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %1767 : int = aten::add(%i.34, %10) # torch/nn/functional.py:1993:27
          %1768 : int = aten::__getitem__(%1760, %1767) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %1768) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.135)
      %1770 : bool = aten::eq(%size_prods.133, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1770) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.98 : Tensor = aten::batch_norm(%out.97, %1758, %1759, %1756, %1757, %1755, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.99 : Tensor = aten::add_(%out.98, %input.19, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.21 : Tensor = aten::relu_(%out.99) # torch/nn/functional.py:1117:17
  %1774 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%790)
  %1775 : Tensor = prim::GetAttr[name="weight"](%1774)
  %1776 : Tensor? = prim::GetAttr[name="bias"](%1774)
  %1777 : int[] = prim::ListConstruct(%12, %12)
  %1778 : int[] = prim::ListConstruct(%8, %8)
  %1779 : int[] = prim::ListConstruct(%12, %12)
  %out.109 : Tensor = aten::conv2d(%input.21, %1775, %1776, %1777, %1778, %1779, %12) # torch/nn/modules/conv.py:415:15
  %1781 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%790)
  %1782 : int = aten::dim(%out.109) # torch/nn/modules/batchnorm.py:276:11
  %1783 : bool = aten::ne(%1782, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1783) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1784 : bool = prim::GetAttr[name="training"](%1781)
   = prim::If(%1784) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1785 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1781)
      %1786 : Tensor = aten::add(%1785, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1781, %1786)
      -> ()
    block1():
      -> ()
  %1787 : bool = prim::GetAttr[name="training"](%1781)
  %1788 : Tensor = prim::GetAttr[name="running_mean"](%1781)
  %1789 : Tensor = prim::GetAttr[name="running_var"](%1781)
  %1790 : Tensor = prim::GetAttr[name="weight"](%1781)
  %1791 : Tensor = prim::GetAttr[name="bias"](%1781)
   = prim::If(%1787) # torch/nn/functional.py:2011:4
    block0():
      %1792 : int[] = aten::size(%out.109) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%1792, %8) # torch/nn/functional.py:1991:17
      %1794 : int = aten::len(%1792) # torch/nn/functional.py:1992:19
      %1795 : int = aten::sub(%1794, %10) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%1795, %9, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %1799 : int = aten::add(%i.35, %10) # torch/nn/functional.py:1993:27
          %1800 : int = aten::__getitem__(%1792, %1799) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %1800) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.139)
      %1802 : bool = aten::eq(%size_prods.137, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1802) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.101 : Tensor = aten::batch_norm(%out.109, %1790, %1791, %1788, %1789, %1787, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.102 : Tensor = aten::relu_(%out.101) # torch/nn/functional.py:1117:17
  %1805 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%790)
  %1806 : Tensor = prim::GetAttr[name="weight"](%1805)
  %1807 : Tensor? = prim::GetAttr[name="bias"](%1805)
  %1808 : int[] = prim::ListConstruct(%12, %12)
  %1809 : int[] = prim::ListConstruct(%12, %12)
  %1810 : int[] = prim::ListConstruct(%12, %12)
  %out.103 : Tensor = aten::conv2d(%out.102, %1806, %1807, %1808, %1809, %1810, %12) # torch/nn/modules/conv.py:415:15
  %1812 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%790)
  %1813 : int = aten::dim(%out.103) # torch/nn/modules/batchnorm.py:276:11
  %1814 : bool = aten::ne(%1813, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1814) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1815 : bool = prim::GetAttr[name="training"](%1812)
   = prim::If(%1815) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1816 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1812)
      %1817 : Tensor = aten::add(%1816, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1812, %1817)
      -> ()
    block1():
      -> ()
  %1818 : bool = prim::GetAttr[name="training"](%1812)
  %1819 : Tensor = prim::GetAttr[name="running_mean"](%1812)
  %1820 : Tensor = prim::GetAttr[name="running_var"](%1812)
  %1821 : Tensor = prim::GetAttr[name="weight"](%1812)
  %1822 : Tensor = prim::GetAttr[name="bias"](%1812)
   = prim::If(%1818) # torch/nn/functional.py:2011:4
    block0():
      %1823 : int[] = aten::size(%out.103) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%1823, %8) # torch/nn/functional.py:1991:17
      %1825 : int = aten::len(%1823) # torch/nn/functional.py:1992:19
      %1826 : int = aten::sub(%1825, %10) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%1826, %9, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %1830 : int = aten::add(%i.36, %10) # torch/nn/functional.py:1993:27
          %1831 : int = aten::__getitem__(%1823, %1830) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %1831) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.143)
      %1833 : bool = aten::eq(%size_prods.141, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1833) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.104 : Tensor = aten::batch_norm(%out.103, %1821, %1822, %1819, %1820, %1818, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.105 : Tensor = aten::relu_(%out.104) # torch/nn/functional.py:1117:17
  %1836 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%790)
  %1837 : Tensor = prim::GetAttr[name="weight"](%1836)
  %1838 : Tensor? = prim::GetAttr[name="bias"](%1836)
  %1839 : int[] = prim::ListConstruct(%12, %12)
  %1840 : int[] = prim::ListConstruct(%8, %8)
  %1841 : int[] = prim::ListConstruct(%12, %12)
  %out.106 : Tensor = aten::conv2d(%out.105, %1837, %1838, %1839, %1840, %1841, %12) # torch/nn/modules/conv.py:415:15
  %1843 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%790)
  %1844 : int = aten::dim(%out.106) # torch/nn/modules/batchnorm.py:276:11
  %1845 : bool = aten::ne(%1844, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1845) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1846 : bool = prim::GetAttr[name="training"](%1843)
   = prim::If(%1846) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1847 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1843)
      %1848 : Tensor = aten::add(%1847, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1843, %1848)
      -> ()
    block1():
      -> ()
  %1849 : bool = prim::GetAttr[name="training"](%1843)
  %1850 : Tensor = prim::GetAttr[name="running_mean"](%1843)
  %1851 : Tensor = prim::GetAttr[name="running_var"](%1843)
  %1852 : Tensor = prim::GetAttr[name="weight"](%1843)
  %1853 : Tensor = prim::GetAttr[name="bias"](%1843)
   = prim::If(%1849) # torch/nn/functional.py:2011:4
    block0():
      %1854 : int[] = aten::size(%out.106) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%1854, %8) # torch/nn/functional.py:1991:17
      %1856 : int = aten::len(%1854) # torch/nn/functional.py:1992:19
      %1857 : int = aten::sub(%1856, %10) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%1857, %9, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %1861 : int = aten::add(%i.37, %10) # torch/nn/functional.py:1993:27
          %1862 : int = aten::__getitem__(%1854, %1861) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %1862) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.147)
      %1864 : bool = aten::eq(%size_prods.145, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1864) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.107 : Tensor = aten::batch_norm(%out.106, %1852, %1853, %1850, %1851, %1849, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.108 : Tensor = aten::add_(%out.107, %input.21, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.23 : Tensor = aten::relu_(%out.108) # torch/nn/functional.py:1117:17
  %1868 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%791)
  %1869 : Tensor = prim::GetAttr[name="weight"](%1868)
  %1870 : Tensor? = prim::GetAttr[name="bias"](%1868)
  %1871 : int[] = prim::ListConstruct(%12, %12)
  %1872 : int[] = prim::ListConstruct(%8, %8)
  %1873 : int[] = prim::ListConstruct(%12, %12)
  %out.118 : Tensor = aten::conv2d(%input.23, %1869, %1870, %1871, %1872, %1873, %12) # torch/nn/modules/conv.py:415:15
  %1875 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%791)
  %1876 : int = aten::dim(%out.118) # torch/nn/modules/batchnorm.py:276:11
  %1877 : bool = aten::ne(%1876, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1877) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1878 : bool = prim::GetAttr[name="training"](%1875)
   = prim::If(%1878) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1879 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1875)
      %1880 : Tensor = aten::add(%1879, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1875, %1880)
      -> ()
    block1():
      -> ()
  %1881 : bool = prim::GetAttr[name="training"](%1875)
  %1882 : Tensor = prim::GetAttr[name="running_mean"](%1875)
  %1883 : Tensor = prim::GetAttr[name="running_var"](%1875)
  %1884 : Tensor = prim::GetAttr[name="weight"](%1875)
  %1885 : Tensor = prim::GetAttr[name="bias"](%1875)
   = prim::If(%1881) # torch/nn/functional.py:2011:4
    block0():
      %1886 : int[] = aten::size(%out.118) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%1886, %8) # torch/nn/functional.py:1991:17
      %1888 : int = aten::len(%1886) # torch/nn/functional.py:1992:19
      %1889 : int = aten::sub(%1888, %10) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%1889, %9, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %1893 : int = aten::add(%i.38, %10) # torch/nn/functional.py:1993:27
          %1894 : int = aten::__getitem__(%1886, %1893) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %1894) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.151)
      %1896 : bool = aten::eq(%size_prods.149, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1896) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.110 : Tensor = aten::batch_norm(%out.118, %1884, %1885, %1882, %1883, %1881, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.111 : Tensor = aten::relu_(%out.110) # torch/nn/functional.py:1117:17
  %1899 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%791)
  %1900 : Tensor = prim::GetAttr[name="weight"](%1899)
  %1901 : Tensor? = prim::GetAttr[name="bias"](%1899)
  %1902 : int[] = prim::ListConstruct(%12, %12)
  %1903 : int[] = prim::ListConstruct(%12, %12)
  %1904 : int[] = prim::ListConstruct(%12, %12)
  %out.112 : Tensor = aten::conv2d(%out.111, %1900, %1901, %1902, %1903, %1904, %12) # torch/nn/modules/conv.py:415:15
  %1906 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%791)
  %1907 : int = aten::dim(%out.112) # torch/nn/modules/batchnorm.py:276:11
  %1908 : bool = aten::ne(%1907, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1908) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1909 : bool = prim::GetAttr[name="training"](%1906)
   = prim::If(%1909) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1910 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1906)
      %1911 : Tensor = aten::add(%1910, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1906, %1911)
      -> ()
    block1():
      -> ()
  %1912 : bool = prim::GetAttr[name="training"](%1906)
  %1913 : Tensor = prim::GetAttr[name="running_mean"](%1906)
  %1914 : Tensor = prim::GetAttr[name="running_var"](%1906)
  %1915 : Tensor = prim::GetAttr[name="weight"](%1906)
  %1916 : Tensor = prim::GetAttr[name="bias"](%1906)
   = prim::If(%1912) # torch/nn/functional.py:2011:4
    block0():
      %1917 : int[] = aten::size(%out.112) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%1917, %8) # torch/nn/functional.py:1991:17
      %1919 : int = aten::len(%1917) # torch/nn/functional.py:1992:19
      %1920 : int = aten::sub(%1919, %10) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%1920, %9, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %1924 : int = aten::add(%i.39, %10) # torch/nn/functional.py:1993:27
          %1925 : int = aten::__getitem__(%1917, %1924) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %1925) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.155)
      %1927 : bool = aten::eq(%size_prods.153, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1927) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.113 : Tensor = aten::batch_norm(%out.112, %1915, %1916, %1913, %1914, %1912, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.114 : Tensor = aten::relu_(%out.113) # torch/nn/functional.py:1117:17
  %1930 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%791)
  %1931 : Tensor = prim::GetAttr[name="weight"](%1930)
  %1932 : Tensor? = prim::GetAttr[name="bias"](%1930)
  %1933 : int[] = prim::ListConstruct(%12, %12)
  %1934 : int[] = prim::ListConstruct(%8, %8)
  %1935 : int[] = prim::ListConstruct(%12, %12)
  %out.115 : Tensor = aten::conv2d(%out.114, %1931, %1932, %1933, %1934, %1935, %12) # torch/nn/modules/conv.py:415:15
  %1937 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%791)
  %1938 : int = aten::dim(%out.115) # torch/nn/modules/batchnorm.py:276:11
  %1939 : bool = aten::ne(%1938, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1939) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1940 : bool = prim::GetAttr[name="training"](%1937)
   = prim::If(%1940) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1941 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1937)
      %1942 : Tensor = aten::add(%1941, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1937, %1942)
      -> ()
    block1():
      -> ()
  %1943 : bool = prim::GetAttr[name="training"](%1937)
  %1944 : Tensor = prim::GetAttr[name="running_mean"](%1937)
  %1945 : Tensor = prim::GetAttr[name="running_var"](%1937)
  %1946 : Tensor = prim::GetAttr[name="weight"](%1937)
  %1947 : Tensor = prim::GetAttr[name="bias"](%1937)
   = prim::If(%1943) # torch/nn/functional.py:2011:4
    block0():
      %1948 : int[] = aten::size(%out.115) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%1948, %8) # torch/nn/functional.py:1991:17
      %1950 : int = aten::len(%1948) # torch/nn/functional.py:1992:19
      %1951 : int = aten::sub(%1950, %10) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%1951, %9, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %1955 : int = aten::add(%i.40, %10) # torch/nn/functional.py:1993:27
          %1956 : int = aten::__getitem__(%1948, %1955) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %1956) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.159)
      %1958 : bool = aten::eq(%size_prods.157, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1958) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.116 : Tensor = aten::batch_norm(%out.115, %1946, %1947, %1944, %1945, %1943, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.117 : Tensor = aten::add_(%out.116, %input.23, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.25 : Tensor = aten::relu_(%out.117) # torch/nn/functional.py:1117:17
  %1962 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%792)
  %1963 : Tensor = prim::GetAttr[name="weight"](%1962)
  %1964 : Tensor? = prim::GetAttr[name="bias"](%1962)
  %1965 : int[] = prim::ListConstruct(%12, %12)
  %1966 : int[] = prim::ListConstruct(%8, %8)
  %1967 : int[] = prim::ListConstruct(%12, %12)
  %out.127 : Tensor = aten::conv2d(%input.25, %1963, %1964, %1965, %1966, %1967, %12) # torch/nn/modules/conv.py:415:15
  %1969 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%792)
  %1970 : int = aten::dim(%out.127) # torch/nn/modules/batchnorm.py:276:11
  %1971 : bool = aten::ne(%1970, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1971) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1972 : bool = prim::GetAttr[name="training"](%1969)
   = prim::If(%1972) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1973 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1969)
      %1974 : Tensor = aten::add(%1973, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1969, %1974)
      -> ()
    block1():
      -> ()
  %1975 : bool = prim::GetAttr[name="training"](%1969)
  %1976 : Tensor = prim::GetAttr[name="running_mean"](%1969)
  %1977 : Tensor = prim::GetAttr[name="running_var"](%1969)
  %1978 : Tensor = prim::GetAttr[name="weight"](%1969)
  %1979 : Tensor = prim::GetAttr[name="bias"](%1969)
   = prim::If(%1975) # torch/nn/functional.py:2011:4
    block0():
      %1980 : int[] = aten::size(%out.127) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%1980, %8) # torch/nn/functional.py:1991:17
      %1982 : int = aten::len(%1980) # torch/nn/functional.py:1992:19
      %1983 : int = aten::sub(%1982, %10) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%1983, %9, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %1987 : int = aten::add(%i.41, %10) # torch/nn/functional.py:1993:27
          %1988 : int = aten::__getitem__(%1980, %1987) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %1988) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.163)
      %1990 : bool = aten::eq(%size_prods.161, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1990) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.119 : Tensor = aten::batch_norm(%out.127, %1978, %1979, %1976, %1977, %1975, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.120 : Tensor = aten::relu_(%out.119) # torch/nn/functional.py:1117:17
  %1993 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%792)
  %1994 : Tensor = prim::GetAttr[name="weight"](%1993)
  %1995 : Tensor? = prim::GetAttr[name="bias"](%1993)
  %1996 : int[] = prim::ListConstruct(%12, %12)
  %1997 : int[] = prim::ListConstruct(%12, %12)
  %1998 : int[] = prim::ListConstruct(%12, %12)
  %out.121 : Tensor = aten::conv2d(%out.120, %1994, %1995, %1996, %1997, %1998, %12) # torch/nn/modules/conv.py:415:15
  %2000 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%792)
  %2001 : int = aten::dim(%out.121) # torch/nn/modules/batchnorm.py:276:11
  %2002 : bool = aten::ne(%2001, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2002) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2003 : bool = prim::GetAttr[name="training"](%2000)
   = prim::If(%2003) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2004 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2000)
      %2005 : Tensor = aten::add(%2004, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2000, %2005)
      -> ()
    block1():
      -> ()
  %2006 : bool = prim::GetAttr[name="training"](%2000)
  %2007 : Tensor = prim::GetAttr[name="running_mean"](%2000)
  %2008 : Tensor = prim::GetAttr[name="running_var"](%2000)
  %2009 : Tensor = prim::GetAttr[name="weight"](%2000)
  %2010 : Tensor = prim::GetAttr[name="bias"](%2000)
   = prim::If(%2006) # torch/nn/functional.py:2011:4
    block0():
      %2011 : int[] = aten::size(%out.121) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%2011, %8) # torch/nn/functional.py:1991:17
      %2013 : int = aten::len(%2011) # torch/nn/functional.py:1992:19
      %2014 : int = aten::sub(%2013, %10) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%2014, %9, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %2018 : int = aten::add(%i.42, %10) # torch/nn/functional.py:1993:27
          %2019 : int = aten::__getitem__(%2011, %2018) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %2019) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.167)
      %2021 : bool = aten::eq(%size_prods.165, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2021) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.122 : Tensor = aten::batch_norm(%out.121, %2009, %2010, %2007, %2008, %2006, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.123 : Tensor = aten::relu_(%out.122) # torch/nn/functional.py:1117:17
  %2024 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%792)
  %2025 : Tensor = prim::GetAttr[name="weight"](%2024)
  %2026 : Tensor? = prim::GetAttr[name="bias"](%2024)
  %2027 : int[] = prim::ListConstruct(%12, %12)
  %2028 : int[] = prim::ListConstruct(%8, %8)
  %2029 : int[] = prim::ListConstruct(%12, %12)
  %out.124 : Tensor = aten::conv2d(%out.123, %2025, %2026, %2027, %2028, %2029, %12) # torch/nn/modules/conv.py:415:15
  %2031 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%792)
  %2032 : int = aten::dim(%out.124) # torch/nn/modules/batchnorm.py:276:11
  %2033 : bool = aten::ne(%2032, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2033) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2034 : bool = prim::GetAttr[name="training"](%2031)
   = prim::If(%2034) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2035 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2031)
      %2036 : Tensor = aten::add(%2035, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2031, %2036)
      -> ()
    block1():
      -> ()
  %2037 : bool = prim::GetAttr[name="training"](%2031)
  %2038 : Tensor = prim::GetAttr[name="running_mean"](%2031)
  %2039 : Tensor = prim::GetAttr[name="running_var"](%2031)
  %2040 : Tensor = prim::GetAttr[name="weight"](%2031)
  %2041 : Tensor = prim::GetAttr[name="bias"](%2031)
   = prim::If(%2037) # torch/nn/functional.py:2011:4
    block0():
      %2042 : int[] = aten::size(%out.124) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%2042, %8) # torch/nn/functional.py:1991:17
      %2044 : int = aten::len(%2042) # torch/nn/functional.py:1992:19
      %2045 : int = aten::sub(%2044, %10) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%2045, %9, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %2049 : int = aten::add(%i.43, %10) # torch/nn/functional.py:1993:27
          %2050 : int = aten::__getitem__(%2042, %2049) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %2050) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.171)
      %2052 : bool = aten::eq(%size_prods.169, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2052) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.125 : Tensor = aten::batch_norm(%out.124, %2040, %2041, %2038, %2039, %2037, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.126 : Tensor = aten::add_(%out.125, %input.25, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.27 : Tensor = aten::relu_(%out.126) # torch/nn/functional.py:1117:17
  %2056 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%793)
  %2057 : Tensor = prim::GetAttr[name="weight"](%2056)
  %2058 : Tensor? = prim::GetAttr[name="bias"](%2056)
  %2059 : int[] = prim::ListConstruct(%12, %12)
  %2060 : int[] = prim::ListConstruct(%8, %8)
  %2061 : int[] = prim::ListConstruct(%12, %12)
  %out.136 : Tensor = aten::conv2d(%input.27, %2057, %2058, %2059, %2060, %2061, %12) # torch/nn/modules/conv.py:415:15
  %2063 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%793)
  %2064 : int = aten::dim(%out.136) # torch/nn/modules/batchnorm.py:276:11
  %2065 : bool = aten::ne(%2064, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2065) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2066 : bool = prim::GetAttr[name="training"](%2063)
   = prim::If(%2066) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2067 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2063)
      %2068 : Tensor = aten::add(%2067, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2063, %2068)
      -> ()
    block1():
      -> ()
  %2069 : bool = prim::GetAttr[name="training"](%2063)
  %2070 : Tensor = prim::GetAttr[name="running_mean"](%2063)
  %2071 : Tensor = prim::GetAttr[name="running_var"](%2063)
  %2072 : Tensor = prim::GetAttr[name="weight"](%2063)
  %2073 : Tensor = prim::GetAttr[name="bias"](%2063)
   = prim::If(%2069) # torch/nn/functional.py:2011:4
    block0():
      %2074 : int[] = aten::size(%out.136) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%2074, %8) # torch/nn/functional.py:1991:17
      %2076 : int = aten::len(%2074) # torch/nn/functional.py:1992:19
      %2077 : int = aten::sub(%2076, %10) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%2077, %9, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %2081 : int = aten::add(%i.44, %10) # torch/nn/functional.py:1993:27
          %2082 : int = aten::__getitem__(%2074, %2081) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %2082) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.175)
      %2084 : bool = aten::eq(%size_prods.173, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2084) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.128 : Tensor = aten::batch_norm(%out.136, %2072, %2073, %2070, %2071, %2069, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.129 : Tensor = aten::relu_(%out.128) # torch/nn/functional.py:1117:17
  %2087 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%793)
  %2088 : Tensor = prim::GetAttr[name="weight"](%2087)
  %2089 : Tensor? = prim::GetAttr[name="bias"](%2087)
  %2090 : int[] = prim::ListConstruct(%12, %12)
  %2091 : int[] = prim::ListConstruct(%12, %12)
  %2092 : int[] = prim::ListConstruct(%12, %12)
  %out.130 : Tensor = aten::conv2d(%out.129, %2088, %2089, %2090, %2091, %2092, %12) # torch/nn/modules/conv.py:415:15
  %2094 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%793)
  %2095 : int = aten::dim(%out.130) # torch/nn/modules/batchnorm.py:276:11
  %2096 : bool = aten::ne(%2095, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2096) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2097 : bool = prim::GetAttr[name="training"](%2094)
   = prim::If(%2097) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2098 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2094)
      %2099 : Tensor = aten::add(%2098, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2094, %2099)
      -> ()
    block1():
      -> ()
  %2100 : bool = prim::GetAttr[name="training"](%2094)
  %2101 : Tensor = prim::GetAttr[name="running_mean"](%2094)
  %2102 : Tensor = prim::GetAttr[name="running_var"](%2094)
  %2103 : Tensor = prim::GetAttr[name="weight"](%2094)
  %2104 : Tensor = prim::GetAttr[name="bias"](%2094)
   = prim::If(%2100) # torch/nn/functional.py:2011:4
    block0():
      %2105 : int[] = aten::size(%out.130) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%2105, %8) # torch/nn/functional.py:1991:17
      %2107 : int = aten::len(%2105) # torch/nn/functional.py:1992:19
      %2108 : int = aten::sub(%2107, %10) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%2108, %9, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %2112 : int = aten::add(%i.45, %10) # torch/nn/functional.py:1993:27
          %2113 : int = aten::__getitem__(%2105, %2112) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %2113) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.179)
      %2115 : bool = aten::eq(%size_prods.177, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2115) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.131 : Tensor = aten::batch_norm(%out.130, %2103, %2104, %2101, %2102, %2100, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.132 : Tensor = aten::relu_(%out.131) # torch/nn/functional.py:1117:17
  %2118 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%793)
  %2119 : Tensor = prim::GetAttr[name="weight"](%2118)
  %2120 : Tensor? = prim::GetAttr[name="bias"](%2118)
  %2121 : int[] = prim::ListConstruct(%12, %12)
  %2122 : int[] = prim::ListConstruct(%8, %8)
  %2123 : int[] = prim::ListConstruct(%12, %12)
  %out.133 : Tensor = aten::conv2d(%out.132, %2119, %2120, %2121, %2122, %2123, %12) # torch/nn/modules/conv.py:415:15
  %2125 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%793)
  %2126 : int = aten::dim(%out.133) # torch/nn/modules/batchnorm.py:276:11
  %2127 : bool = aten::ne(%2126, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2127) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2128 : bool = prim::GetAttr[name="training"](%2125)
   = prim::If(%2128) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2129 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2125)
      %2130 : Tensor = aten::add(%2129, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2125, %2130)
      -> ()
    block1():
      -> ()
  %2131 : bool = prim::GetAttr[name="training"](%2125)
  %2132 : Tensor = prim::GetAttr[name="running_mean"](%2125)
  %2133 : Tensor = prim::GetAttr[name="running_var"](%2125)
  %2134 : Tensor = prim::GetAttr[name="weight"](%2125)
  %2135 : Tensor = prim::GetAttr[name="bias"](%2125)
   = prim::If(%2131) # torch/nn/functional.py:2011:4
    block0():
      %2136 : int[] = aten::size(%out.133) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%2136, %8) # torch/nn/functional.py:1991:17
      %2138 : int = aten::len(%2136) # torch/nn/functional.py:1992:19
      %2139 : int = aten::sub(%2138, %10) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%2139, %9, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %2143 : int = aten::add(%i.46, %10) # torch/nn/functional.py:1993:27
          %2144 : int = aten::__getitem__(%2136, %2143) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %2144) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.183)
      %2146 : bool = aten::eq(%size_prods.181, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2146) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.134 : Tensor = aten::batch_norm(%out.133, %2134, %2135, %2132, %2133, %2131, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.135 : Tensor = aten::add_(%out.134, %input.27, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.29 : Tensor = aten::relu_(%out.135) # torch/nn/functional.py:1117:17
  %2150 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%794)
  %2151 : Tensor = prim::GetAttr[name="weight"](%2150)
  %2152 : Tensor? = prim::GetAttr[name="bias"](%2150)
  %2153 : int[] = prim::ListConstruct(%12, %12)
  %2154 : int[] = prim::ListConstruct(%8, %8)
  %2155 : int[] = prim::ListConstruct(%12, %12)
  %out.145 : Tensor = aten::conv2d(%input.29, %2151, %2152, %2153, %2154, %2155, %12) # torch/nn/modules/conv.py:415:15
  %2157 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%794)
  %2158 : int = aten::dim(%out.145) # torch/nn/modules/batchnorm.py:276:11
  %2159 : bool = aten::ne(%2158, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2159) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2160 : bool = prim::GetAttr[name="training"](%2157)
   = prim::If(%2160) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2161 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2157)
      %2162 : Tensor = aten::add(%2161, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2157, %2162)
      -> ()
    block1():
      -> ()
  %2163 : bool = prim::GetAttr[name="training"](%2157)
  %2164 : Tensor = prim::GetAttr[name="running_mean"](%2157)
  %2165 : Tensor = prim::GetAttr[name="running_var"](%2157)
  %2166 : Tensor = prim::GetAttr[name="weight"](%2157)
  %2167 : Tensor = prim::GetAttr[name="bias"](%2157)
   = prim::If(%2163) # torch/nn/functional.py:2011:4
    block0():
      %2168 : int[] = aten::size(%out.145) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%2168, %8) # torch/nn/functional.py:1991:17
      %2170 : int = aten::len(%2168) # torch/nn/functional.py:1992:19
      %2171 : int = aten::sub(%2170, %10) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%2171, %9, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %2175 : int = aten::add(%i.47, %10) # torch/nn/functional.py:1993:27
          %2176 : int = aten::__getitem__(%2168, %2175) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %2176) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.187)
      %2178 : bool = aten::eq(%size_prods.185, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2178) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.137 : Tensor = aten::batch_norm(%out.145, %2166, %2167, %2164, %2165, %2163, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.138 : Tensor = aten::relu_(%out.137) # torch/nn/functional.py:1117:17
  %2181 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%794)
  %2182 : Tensor = prim::GetAttr[name="weight"](%2181)
  %2183 : Tensor? = prim::GetAttr[name="bias"](%2181)
  %2184 : int[] = prim::ListConstruct(%12, %12)
  %2185 : int[] = prim::ListConstruct(%12, %12)
  %2186 : int[] = prim::ListConstruct(%12, %12)
  %out.139 : Tensor = aten::conv2d(%out.138, %2182, %2183, %2184, %2185, %2186, %12) # torch/nn/modules/conv.py:415:15
  %2188 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%794)
  %2189 : int = aten::dim(%out.139) # torch/nn/modules/batchnorm.py:276:11
  %2190 : bool = aten::ne(%2189, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2190) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2191 : bool = prim::GetAttr[name="training"](%2188)
   = prim::If(%2191) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2192 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2188)
      %2193 : Tensor = aten::add(%2192, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2188, %2193)
      -> ()
    block1():
      -> ()
  %2194 : bool = prim::GetAttr[name="training"](%2188)
  %2195 : Tensor = prim::GetAttr[name="running_mean"](%2188)
  %2196 : Tensor = prim::GetAttr[name="running_var"](%2188)
  %2197 : Tensor = prim::GetAttr[name="weight"](%2188)
  %2198 : Tensor = prim::GetAttr[name="bias"](%2188)
   = prim::If(%2194) # torch/nn/functional.py:2011:4
    block0():
      %2199 : int[] = aten::size(%out.139) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%2199, %8) # torch/nn/functional.py:1991:17
      %2201 : int = aten::len(%2199) # torch/nn/functional.py:1992:19
      %2202 : int = aten::sub(%2201, %10) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%2202, %9, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %2206 : int = aten::add(%i.48, %10) # torch/nn/functional.py:1993:27
          %2207 : int = aten::__getitem__(%2199, %2206) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %2207) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.191)
      %2209 : bool = aten::eq(%size_prods.189, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2209) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.140 : Tensor = aten::batch_norm(%out.139, %2197, %2198, %2195, %2196, %2194, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.141 : Tensor = aten::relu_(%out.140) # torch/nn/functional.py:1117:17
  %2212 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%794)
  %2213 : Tensor = prim::GetAttr[name="weight"](%2212)
  %2214 : Tensor? = prim::GetAttr[name="bias"](%2212)
  %2215 : int[] = prim::ListConstruct(%12, %12)
  %2216 : int[] = prim::ListConstruct(%8, %8)
  %2217 : int[] = prim::ListConstruct(%12, %12)
  %out.142 : Tensor = aten::conv2d(%out.141, %2213, %2214, %2215, %2216, %2217, %12) # torch/nn/modules/conv.py:415:15
  %2219 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%794)
  %2220 : int = aten::dim(%out.142) # torch/nn/modules/batchnorm.py:276:11
  %2221 : bool = aten::ne(%2220, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2221) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2222 : bool = prim::GetAttr[name="training"](%2219)
   = prim::If(%2222) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2223 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2219)
      %2224 : Tensor = aten::add(%2223, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2219, %2224)
      -> ()
    block1():
      -> ()
  %2225 : bool = prim::GetAttr[name="training"](%2219)
  %2226 : Tensor = prim::GetAttr[name="running_mean"](%2219)
  %2227 : Tensor = prim::GetAttr[name="running_var"](%2219)
  %2228 : Tensor = prim::GetAttr[name="weight"](%2219)
  %2229 : Tensor = prim::GetAttr[name="bias"](%2219)
   = prim::If(%2225) # torch/nn/functional.py:2011:4
    block0():
      %2230 : int[] = aten::size(%out.142) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%2230, %8) # torch/nn/functional.py:1991:17
      %2232 : int = aten::len(%2230) # torch/nn/functional.py:1992:19
      %2233 : int = aten::sub(%2232, %10) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%2233, %9, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %2237 : int = aten::add(%i.49, %10) # torch/nn/functional.py:1993:27
          %2238 : int = aten::__getitem__(%2230, %2237) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %2238) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.195)
      %2240 : bool = aten::eq(%size_prods.193, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2240) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.143 : Tensor = aten::batch_norm(%out.142, %2228, %2229, %2226, %2227, %2225, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.144 : Tensor = aten::add_(%out.143, %input.29, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.31 : Tensor = aten::relu_(%out.144) # torch/nn/functional.py:1117:17
  %2244 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%795)
  %2245 : Tensor = prim::GetAttr[name="weight"](%2244)
  %2246 : Tensor? = prim::GetAttr[name="bias"](%2244)
  %2247 : int[] = prim::ListConstruct(%12, %12)
  %2248 : int[] = prim::ListConstruct(%8, %8)
  %2249 : int[] = prim::ListConstruct(%12, %12)
  %out.154 : Tensor = aten::conv2d(%input.31, %2245, %2246, %2247, %2248, %2249, %12) # torch/nn/modules/conv.py:415:15
  %2251 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%795)
  %2252 : int = aten::dim(%out.154) # torch/nn/modules/batchnorm.py:276:11
  %2253 : bool = aten::ne(%2252, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2253) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2254 : bool = prim::GetAttr[name="training"](%2251)
   = prim::If(%2254) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2255 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2251)
      %2256 : Tensor = aten::add(%2255, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2251, %2256)
      -> ()
    block1():
      -> ()
  %2257 : bool = prim::GetAttr[name="training"](%2251)
  %2258 : Tensor = prim::GetAttr[name="running_mean"](%2251)
  %2259 : Tensor = prim::GetAttr[name="running_var"](%2251)
  %2260 : Tensor = prim::GetAttr[name="weight"](%2251)
  %2261 : Tensor = prim::GetAttr[name="bias"](%2251)
   = prim::If(%2257) # torch/nn/functional.py:2011:4
    block0():
      %2262 : int[] = aten::size(%out.154) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%2262, %8) # torch/nn/functional.py:1991:17
      %2264 : int = aten::len(%2262) # torch/nn/functional.py:1992:19
      %2265 : int = aten::sub(%2264, %10) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%2265, %9, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %2269 : int = aten::add(%i.50, %10) # torch/nn/functional.py:1993:27
          %2270 : int = aten::__getitem__(%2262, %2269) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %2270) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.199)
      %2272 : bool = aten::eq(%size_prods.197, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2272) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.146 : Tensor = aten::batch_norm(%out.154, %2260, %2261, %2258, %2259, %2257, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.147 : Tensor = aten::relu_(%out.146) # torch/nn/functional.py:1117:17
  %2275 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%795)
  %2276 : Tensor = prim::GetAttr[name="weight"](%2275)
  %2277 : Tensor? = prim::GetAttr[name="bias"](%2275)
  %2278 : int[] = prim::ListConstruct(%12, %12)
  %2279 : int[] = prim::ListConstruct(%12, %12)
  %2280 : int[] = prim::ListConstruct(%12, %12)
  %out.148 : Tensor = aten::conv2d(%out.147, %2276, %2277, %2278, %2279, %2280, %12) # torch/nn/modules/conv.py:415:15
  %2282 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%795)
  %2283 : int = aten::dim(%out.148) # torch/nn/modules/batchnorm.py:276:11
  %2284 : bool = aten::ne(%2283, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2284) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2285 : bool = prim::GetAttr[name="training"](%2282)
   = prim::If(%2285) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2286 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2282)
      %2287 : Tensor = aten::add(%2286, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2282, %2287)
      -> ()
    block1():
      -> ()
  %2288 : bool = prim::GetAttr[name="training"](%2282)
  %2289 : Tensor = prim::GetAttr[name="running_mean"](%2282)
  %2290 : Tensor = prim::GetAttr[name="running_var"](%2282)
  %2291 : Tensor = prim::GetAttr[name="weight"](%2282)
  %2292 : Tensor = prim::GetAttr[name="bias"](%2282)
   = prim::If(%2288) # torch/nn/functional.py:2011:4
    block0():
      %2293 : int[] = aten::size(%out.148) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%2293, %8) # torch/nn/functional.py:1991:17
      %2295 : int = aten::len(%2293) # torch/nn/functional.py:1992:19
      %2296 : int = aten::sub(%2295, %10) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%2296, %9, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %2300 : int = aten::add(%i.51, %10) # torch/nn/functional.py:1993:27
          %2301 : int = aten::__getitem__(%2293, %2300) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %2301) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.203)
      %2303 : bool = aten::eq(%size_prods.201, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2303) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.149 : Tensor = aten::batch_norm(%out.148, %2291, %2292, %2289, %2290, %2288, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.150 : Tensor = aten::relu_(%out.149) # torch/nn/functional.py:1117:17
  %2306 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%795)
  %2307 : Tensor = prim::GetAttr[name="weight"](%2306)
  %2308 : Tensor? = prim::GetAttr[name="bias"](%2306)
  %2309 : int[] = prim::ListConstruct(%12, %12)
  %2310 : int[] = prim::ListConstruct(%8, %8)
  %2311 : int[] = prim::ListConstruct(%12, %12)
  %out.151 : Tensor = aten::conv2d(%out.150, %2307, %2308, %2309, %2310, %2311, %12) # torch/nn/modules/conv.py:415:15
  %2313 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%795)
  %2314 : int = aten::dim(%out.151) # torch/nn/modules/batchnorm.py:276:11
  %2315 : bool = aten::ne(%2314, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2315) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2316 : bool = prim::GetAttr[name="training"](%2313)
   = prim::If(%2316) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2317 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2313)
      %2318 : Tensor = aten::add(%2317, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2313, %2318)
      -> ()
    block1():
      -> ()
  %2319 : bool = prim::GetAttr[name="training"](%2313)
  %2320 : Tensor = prim::GetAttr[name="running_mean"](%2313)
  %2321 : Tensor = prim::GetAttr[name="running_var"](%2313)
  %2322 : Tensor = prim::GetAttr[name="weight"](%2313)
  %2323 : Tensor = prim::GetAttr[name="bias"](%2313)
   = prim::If(%2319) # torch/nn/functional.py:2011:4
    block0():
      %2324 : int[] = aten::size(%out.151) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%2324, %8) # torch/nn/functional.py:1991:17
      %2326 : int = aten::len(%2324) # torch/nn/functional.py:1992:19
      %2327 : int = aten::sub(%2326, %10) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%2327, %9, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %2331 : int = aten::add(%i.52, %10) # torch/nn/functional.py:1993:27
          %2332 : int = aten::__getitem__(%2324, %2331) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %2332) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.207)
      %2334 : bool = aten::eq(%size_prods.205, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2334) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.152 : Tensor = aten::batch_norm(%out.151, %2322, %2323, %2320, %2321, %2319, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.153 : Tensor = aten::add_(%out.152, %input.31, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.33 : Tensor = aten::relu_(%out.153) # torch/nn/functional.py:1117:17
  %2338 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%796)
  %2339 : Tensor = prim::GetAttr[name="weight"](%2338)
  %2340 : Tensor? = prim::GetAttr[name="bias"](%2338)
  %2341 : int[] = prim::ListConstruct(%12, %12)
  %2342 : int[] = prim::ListConstruct(%8, %8)
  %2343 : int[] = prim::ListConstruct(%12, %12)
  %out.163 : Tensor = aten::conv2d(%input.33, %2339, %2340, %2341, %2342, %2343, %12) # torch/nn/modules/conv.py:415:15
  %2345 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%796)
  %2346 : int = aten::dim(%out.163) # torch/nn/modules/batchnorm.py:276:11
  %2347 : bool = aten::ne(%2346, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2347) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2348 : bool = prim::GetAttr[name="training"](%2345)
   = prim::If(%2348) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2349 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2345)
      %2350 : Tensor = aten::add(%2349, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2345, %2350)
      -> ()
    block1():
      -> ()
  %2351 : bool = prim::GetAttr[name="training"](%2345)
  %2352 : Tensor = prim::GetAttr[name="running_mean"](%2345)
  %2353 : Tensor = prim::GetAttr[name="running_var"](%2345)
  %2354 : Tensor = prim::GetAttr[name="weight"](%2345)
  %2355 : Tensor = prim::GetAttr[name="bias"](%2345)
   = prim::If(%2351) # torch/nn/functional.py:2011:4
    block0():
      %2356 : int[] = aten::size(%out.163) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%2356, %8) # torch/nn/functional.py:1991:17
      %2358 : int = aten::len(%2356) # torch/nn/functional.py:1992:19
      %2359 : int = aten::sub(%2358, %10) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%2359, %9, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %2363 : int = aten::add(%i.53, %10) # torch/nn/functional.py:1993:27
          %2364 : int = aten::__getitem__(%2356, %2363) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %2364) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.211)
      %2366 : bool = aten::eq(%size_prods.209, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2366) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.155 : Tensor = aten::batch_norm(%out.163, %2354, %2355, %2352, %2353, %2351, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.156 : Tensor = aten::relu_(%out.155) # torch/nn/functional.py:1117:17
  %2369 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%796)
  %2370 : Tensor = prim::GetAttr[name="weight"](%2369)
  %2371 : Tensor? = prim::GetAttr[name="bias"](%2369)
  %2372 : int[] = prim::ListConstruct(%12, %12)
  %2373 : int[] = prim::ListConstruct(%12, %12)
  %2374 : int[] = prim::ListConstruct(%12, %12)
  %out.157 : Tensor = aten::conv2d(%out.156, %2370, %2371, %2372, %2373, %2374, %12) # torch/nn/modules/conv.py:415:15
  %2376 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%796)
  %2377 : int = aten::dim(%out.157) # torch/nn/modules/batchnorm.py:276:11
  %2378 : bool = aten::ne(%2377, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2378) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2379 : bool = prim::GetAttr[name="training"](%2376)
   = prim::If(%2379) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2380 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2376)
      %2381 : Tensor = aten::add(%2380, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2376, %2381)
      -> ()
    block1():
      -> ()
  %2382 : bool = prim::GetAttr[name="training"](%2376)
  %2383 : Tensor = prim::GetAttr[name="running_mean"](%2376)
  %2384 : Tensor = prim::GetAttr[name="running_var"](%2376)
  %2385 : Tensor = prim::GetAttr[name="weight"](%2376)
  %2386 : Tensor = prim::GetAttr[name="bias"](%2376)
   = prim::If(%2382) # torch/nn/functional.py:2011:4
    block0():
      %2387 : int[] = aten::size(%out.157) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%2387, %8) # torch/nn/functional.py:1991:17
      %2389 : int = aten::len(%2387) # torch/nn/functional.py:1992:19
      %2390 : int = aten::sub(%2389, %10) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%2390, %9, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %2394 : int = aten::add(%i.54, %10) # torch/nn/functional.py:1993:27
          %2395 : int = aten::__getitem__(%2387, %2394) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %2395) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.215)
      %2397 : bool = aten::eq(%size_prods.213, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2397) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.158 : Tensor = aten::batch_norm(%out.157, %2385, %2386, %2383, %2384, %2382, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.159 : Tensor = aten::relu_(%out.158) # torch/nn/functional.py:1117:17
  %2400 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%796)
  %2401 : Tensor = prim::GetAttr[name="weight"](%2400)
  %2402 : Tensor? = prim::GetAttr[name="bias"](%2400)
  %2403 : int[] = prim::ListConstruct(%12, %12)
  %2404 : int[] = prim::ListConstruct(%8, %8)
  %2405 : int[] = prim::ListConstruct(%12, %12)
  %out.160 : Tensor = aten::conv2d(%out.159, %2401, %2402, %2403, %2404, %2405, %12) # torch/nn/modules/conv.py:415:15
  %2407 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%796)
  %2408 : int = aten::dim(%out.160) # torch/nn/modules/batchnorm.py:276:11
  %2409 : bool = aten::ne(%2408, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2409) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2410 : bool = prim::GetAttr[name="training"](%2407)
   = prim::If(%2410) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2411 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2407)
      %2412 : Tensor = aten::add(%2411, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2407, %2412)
      -> ()
    block1():
      -> ()
  %2413 : bool = prim::GetAttr[name="training"](%2407)
  %2414 : Tensor = prim::GetAttr[name="running_mean"](%2407)
  %2415 : Tensor = prim::GetAttr[name="running_var"](%2407)
  %2416 : Tensor = prim::GetAttr[name="weight"](%2407)
  %2417 : Tensor = prim::GetAttr[name="bias"](%2407)
   = prim::If(%2413) # torch/nn/functional.py:2011:4
    block0():
      %2418 : int[] = aten::size(%out.160) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%2418, %8) # torch/nn/functional.py:1991:17
      %2420 : int = aten::len(%2418) # torch/nn/functional.py:1992:19
      %2421 : int = aten::sub(%2420, %10) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%2421, %9, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %2425 : int = aten::add(%i.55, %10) # torch/nn/functional.py:1993:27
          %2426 : int = aten::__getitem__(%2418, %2425) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %2426) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.219)
      %2428 : bool = aten::eq(%size_prods.217, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2428) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.161 : Tensor = aten::batch_norm(%out.160, %2416, %2417, %2414, %2415, %2413, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.162 : Tensor = aten::add_(%out.161, %input.33, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.35 : Tensor = aten::relu_(%out.162) # torch/nn/functional.py:1117:17
  %2432 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%797)
  %2433 : Tensor = prim::GetAttr[name="weight"](%2432)
  %2434 : Tensor? = prim::GetAttr[name="bias"](%2432)
  %2435 : int[] = prim::ListConstruct(%12, %12)
  %2436 : int[] = prim::ListConstruct(%8, %8)
  %2437 : int[] = prim::ListConstruct(%12, %12)
  %out.172 : Tensor = aten::conv2d(%input.35, %2433, %2434, %2435, %2436, %2437, %12) # torch/nn/modules/conv.py:415:15
  %2439 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%797)
  %2440 : int = aten::dim(%out.172) # torch/nn/modules/batchnorm.py:276:11
  %2441 : bool = aten::ne(%2440, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2441) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2442 : bool = prim::GetAttr[name="training"](%2439)
   = prim::If(%2442) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2443 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2439)
      %2444 : Tensor = aten::add(%2443, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2439, %2444)
      -> ()
    block1():
      -> ()
  %2445 : bool = prim::GetAttr[name="training"](%2439)
  %2446 : Tensor = prim::GetAttr[name="running_mean"](%2439)
  %2447 : Tensor = prim::GetAttr[name="running_var"](%2439)
  %2448 : Tensor = prim::GetAttr[name="weight"](%2439)
  %2449 : Tensor = prim::GetAttr[name="bias"](%2439)
   = prim::If(%2445) # torch/nn/functional.py:2011:4
    block0():
      %2450 : int[] = aten::size(%out.172) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%2450, %8) # torch/nn/functional.py:1991:17
      %2452 : int = aten::len(%2450) # torch/nn/functional.py:1992:19
      %2453 : int = aten::sub(%2452, %10) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%2453, %9, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %2457 : int = aten::add(%i.56, %10) # torch/nn/functional.py:1993:27
          %2458 : int = aten::__getitem__(%2450, %2457) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %2458) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.223)
      %2460 : bool = aten::eq(%size_prods.221, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2460) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.164 : Tensor = aten::batch_norm(%out.172, %2448, %2449, %2446, %2447, %2445, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.165 : Tensor = aten::relu_(%out.164) # torch/nn/functional.py:1117:17
  %2463 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%797)
  %2464 : Tensor = prim::GetAttr[name="weight"](%2463)
  %2465 : Tensor? = prim::GetAttr[name="bias"](%2463)
  %2466 : int[] = prim::ListConstruct(%12, %12)
  %2467 : int[] = prim::ListConstruct(%12, %12)
  %2468 : int[] = prim::ListConstruct(%12, %12)
  %out.166 : Tensor = aten::conv2d(%out.165, %2464, %2465, %2466, %2467, %2468, %12) # torch/nn/modules/conv.py:415:15
  %2470 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%797)
  %2471 : int = aten::dim(%out.166) # torch/nn/modules/batchnorm.py:276:11
  %2472 : bool = aten::ne(%2471, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2472) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2473 : bool = prim::GetAttr[name="training"](%2470)
   = prim::If(%2473) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2474 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2470)
      %2475 : Tensor = aten::add(%2474, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2470, %2475)
      -> ()
    block1():
      -> ()
  %2476 : bool = prim::GetAttr[name="training"](%2470)
  %2477 : Tensor = prim::GetAttr[name="running_mean"](%2470)
  %2478 : Tensor = prim::GetAttr[name="running_var"](%2470)
  %2479 : Tensor = prim::GetAttr[name="weight"](%2470)
  %2480 : Tensor = prim::GetAttr[name="bias"](%2470)
   = prim::If(%2476) # torch/nn/functional.py:2011:4
    block0():
      %2481 : int[] = aten::size(%out.166) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%2481, %8) # torch/nn/functional.py:1991:17
      %2483 : int = aten::len(%2481) # torch/nn/functional.py:1992:19
      %2484 : int = aten::sub(%2483, %10) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%2484, %9, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %2488 : int = aten::add(%i.57, %10) # torch/nn/functional.py:1993:27
          %2489 : int = aten::__getitem__(%2481, %2488) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %2489) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.227)
      %2491 : bool = aten::eq(%size_prods.225, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2491) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.167 : Tensor = aten::batch_norm(%out.166, %2479, %2480, %2477, %2478, %2476, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.168 : Tensor = aten::relu_(%out.167) # torch/nn/functional.py:1117:17
  %2494 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%797)
  %2495 : Tensor = prim::GetAttr[name="weight"](%2494)
  %2496 : Tensor? = prim::GetAttr[name="bias"](%2494)
  %2497 : int[] = prim::ListConstruct(%12, %12)
  %2498 : int[] = prim::ListConstruct(%8, %8)
  %2499 : int[] = prim::ListConstruct(%12, %12)
  %out.169 : Tensor = aten::conv2d(%out.168, %2495, %2496, %2497, %2498, %2499, %12) # torch/nn/modules/conv.py:415:15
  %2501 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%797)
  %2502 : int = aten::dim(%out.169) # torch/nn/modules/batchnorm.py:276:11
  %2503 : bool = aten::ne(%2502, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2503) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2504 : bool = prim::GetAttr[name="training"](%2501)
   = prim::If(%2504) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2505 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2501)
      %2506 : Tensor = aten::add(%2505, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2501, %2506)
      -> ()
    block1():
      -> ()
  %2507 : bool = prim::GetAttr[name="training"](%2501)
  %2508 : Tensor = prim::GetAttr[name="running_mean"](%2501)
  %2509 : Tensor = prim::GetAttr[name="running_var"](%2501)
  %2510 : Tensor = prim::GetAttr[name="weight"](%2501)
  %2511 : Tensor = prim::GetAttr[name="bias"](%2501)
   = prim::If(%2507) # torch/nn/functional.py:2011:4
    block0():
      %2512 : int[] = aten::size(%out.169) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%2512, %8) # torch/nn/functional.py:1991:17
      %2514 : int = aten::len(%2512) # torch/nn/functional.py:1992:19
      %2515 : int = aten::sub(%2514, %10) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%2515, %9, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %2519 : int = aten::add(%i.58, %10) # torch/nn/functional.py:1993:27
          %2520 : int = aten::__getitem__(%2512, %2519) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %2520) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.231)
      %2522 : bool = aten::eq(%size_prods.229, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2522) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.170 : Tensor = aten::batch_norm(%out.169, %2510, %2511, %2508, %2509, %2507, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.171 : Tensor = aten::add_(%out.170, %input.35, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.37 : Tensor = aten::relu_(%out.171) # torch/nn/functional.py:1117:17
  %2526 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%798)
  %2527 : Tensor = prim::GetAttr[name="weight"](%2526)
  %2528 : Tensor? = prim::GetAttr[name="bias"](%2526)
  %2529 : int[] = prim::ListConstruct(%12, %12)
  %2530 : int[] = prim::ListConstruct(%8, %8)
  %2531 : int[] = prim::ListConstruct(%12, %12)
  %out.181 : Tensor = aten::conv2d(%input.37, %2527, %2528, %2529, %2530, %2531, %12) # torch/nn/modules/conv.py:415:15
  %2533 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%798)
  %2534 : int = aten::dim(%out.181) # torch/nn/modules/batchnorm.py:276:11
  %2535 : bool = aten::ne(%2534, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2535) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2536 : bool = prim::GetAttr[name="training"](%2533)
   = prim::If(%2536) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2537 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2533)
      %2538 : Tensor = aten::add(%2537, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2533, %2538)
      -> ()
    block1():
      -> ()
  %2539 : bool = prim::GetAttr[name="training"](%2533)
  %2540 : Tensor = prim::GetAttr[name="running_mean"](%2533)
  %2541 : Tensor = prim::GetAttr[name="running_var"](%2533)
  %2542 : Tensor = prim::GetAttr[name="weight"](%2533)
  %2543 : Tensor = prim::GetAttr[name="bias"](%2533)
   = prim::If(%2539) # torch/nn/functional.py:2011:4
    block0():
      %2544 : int[] = aten::size(%out.181) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%2544, %8) # torch/nn/functional.py:1991:17
      %2546 : int = aten::len(%2544) # torch/nn/functional.py:1992:19
      %2547 : int = aten::sub(%2546, %10) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%2547, %9, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %2551 : int = aten::add(%i.59, %10) # torch/nn/functional.py:1993:27
          %2552 : int = aten::__getitem__(%2544, %2551) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %2552) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.235)
      %2554 : bool = aten::eq(%size_prods.233, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2554) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.173 : Tensor = aten::batch_norm(%out.181, %2542, %2543, %2540, %2541, %2539, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.174 : Tensor = aten::relu_(%out.173) # torch/nn/functional.py:1117:17
  %2557 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%798)
  %2558 : Tensor = prim::GetAttr[name="weight"](%2557)
  %2559 : Tensor? = prim::GetAttr[name="bias"](%2557)
  %2560 : int[] = prim::ListConstruct(%12, %12)
  %2561 : int[] = prim::ListConstruct(%12, %12)
  %2562 : int[] = prim::ListConstruct(%12, %12)
  %out.175 : Tensor = aten::conv2d(%out.174, %2558, %2559, %2560, %2561, %2562, %12) # torch/nn/modules/conv.py:415:15
  %2564 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%798)
  %2565 : int = aten::dim(%out.175) # torch/nn/modules/batchnorm.py:276:11
  %2566 : bool = aten::ne(%2565, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2566) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2567 : bool = prim::GetAttr[name="training"](%2564)
   = prim::If(%2567) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2568 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2564)
      %2569 : Tensor = aten::add(%2568, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2564, %2569)
      -> ()
    block1():
      -> ()
  %2570 : bool = prim::GetAttr[name="training"](%2564)
  %2571 : Tensor = prim::GetAttr[name="running_mean"](%2564)
  %2572 : Tensor = prim::GetAttr[name="running_var"](%2564)
  %2573 : Tensor = prim::GetAttr[name="weight"](%2564)
  %2574 : Tensor = prim::GetAttr[name="bias"](%2564)
   = prim::If(%2570) # torch/nn/functional.py:2011:4
    block0():
      %2575 : int[] = aten::size(%out.175) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%2575, %8) # torch/nn/functional.py:1991:17
      %2577 : int = aten::len(%2575) # torch/nn/functional.py:1992:19
      %2578 : int = aten::sub(%2577, %10) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%2578, %9, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %2582 : int = aten::add(%i.60, %10) # torch/nn/functional.py:1993:27
          %2583 : int = aten::__getitem__(%2575, %2582) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %2583) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.239)
      %2585 : bool = aten::eq(%size_prods.237, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2585) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.176 : Tensor = aten::batch_norm(%out.175, %2573, %2574, %2571, %2572, %2570, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.177 : Tensor = aten::relu_(%out.176) # torch/nn/functional.py:1117:17
  %2588 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%798)
  %2589 : Tensor = prim::GetAttr[name="weight"](%2588)
  %2590 : Tensor? = prim::GetAttr[name="bias"](%2588)
  %2591 : int[] = prim::ListConstruct(%12, %12)
  %2592 : int[] = prim::ListConstruct(%8, %8)
  %2593 : int[] = prim::ListConstruct(%12, %12)
  %out.178 : Tensor = aten::conv2d(%out.177, %2589, %2590, %2591, %2592, %2593, %12) # torch/nn/modules/conv.py:415:15
  %2595 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%798)
  %2596 : int = aten::dim(%out.178) # torch/nn/modules/batchnorm.py:276:11
  %2597 : bool = aten::ne(%2596, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2597) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2598 : bool = prim::GetAttr[name="training"](%2595)
   = prim::If(%2598) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2599 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2595)
      %2600 : Tensor = aten::add(%2599, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2595, %2600)
      -> ()
    block1():
      -> ()
  %2601 : bool = prim::GetAttr[name="training"](%2595)
  %2602 : Tensor = prim::GetAttr[name="running_mean"](%2595)
  %2603 : Tensor = prim::GetAttr[name="running_var"](%2595)
  %2604 : Tensor = prim::GetAttr[name="weight"](%2595)
  %2605 : Tensor = prim::GetAttr[name="bias"](%2595)
   = prim::If(%2601) # torch/nn/functional.py:2011:4
    block0():
      %2606 : int[] = aten::size(%out.178) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%2606, %8) # torch/nn/functional.py:1991:17
      %2608 : int = aten::len(%2606) # torch/nn/functional.py:1992:19
      %2609 : int = aten::sub(%2608, %10) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%2609, %9, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %2613 : int = aten::add(%i.61, %10) # torch/nn/functional.py:1993:27
          %2614 : int = aten::__getitem__(%2606, %2613) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %2614) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.243)
      %2616 : bool = aten::eq(%size_prods.241, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2616) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.179 : Tensor = aten::batch_norm(%out.178, %2604, %2605, %2602, %2603, %2601, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.180 : Tensor = aten::add_(%out.179, %input.37, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.39 : Tensor = aten::relu_(%out.180) # torch/nn/functional.py:1117:17
  %2620 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%799)
  %2621 : Tensor = prim::GetAttr[name="weight"](%2620)
  %2622 : Tensor? = prim::GetAttr[name="bias"](%2620)
  %2623 : int[] = prim::ListConstruct(%12, %12)
  %2624 : int[] = prim::ListConstruct(%8, %8)
  %2625 : int[] = prim::ListConstruct(%12, %12)
  %out.190 : Tensor = aten::conv2d(%input.39, %2621, %2622, %2623, %2624, %2625, %12) # torch/nn/modules/conv.py:415:15
  %2627 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%799)
  %2628 : int = aten::dim(%out.190) # torch/nn/modules/batchnorm.py:276:11
  %2629 : bool = aten::ne(%2628, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2629) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2630 : bool = prim::GetAttr[name="training"](%2627)
   = prim::If(%2630) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2631 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2627)
      %2632 : Tensor = aten::add(%2631, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2627, %2632)
      -> ()
    block1():
      -> ()
  %2633 : bool = prim::GetAttr[name="training"](%2627)
  %2634 : Tensor = prim::GetAttr[name="running_mean"](%2627)
  %2635 : Tensor = prim::GetAttr[name="running_var"](%2627)
  %2636 : Tensor = prim::GetAttr[name="weight"](%2627)
  %2637 : Tensor = prim::GetAttr[name="bias"](%2627)
   = prim::If(%2633) # torch/nn/functional.py:2011:4
    block0():
      %2638 : int[] = aten::size(%out.190) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%2638, %8) # torch/nn/functional.py:1991:17
      %2640 : int = aten::len(%2638) # torch/nn/functional.py:1992:19
      %2641 : int = aten::sub(%2640, %10) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%2641, %9, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %2645 : int = aten::add(%i.62, %10) # torch/nn/functional.py:1993:27
          %2646 : int = aten::__getitem__(%2638, %2645) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %2646) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.247)
      %2648 : bool = aten::eq(%size_prods.245, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2648) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.182 : Tensor = aten::batch_norm(%out.190, %2636, %2637, %2634, %2635, %2633, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.183 : Tensor = aten::relu_(%out.182) # torch/nn/functional.py:1117:17
  %2651 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%799)
  %2652 : Tensor = prim::GetAttr[name="weight"](%2651)
  %2653 : Tensor? = prim::GetAttr[name="bias"](%2651)
  %2654 : int[] = prim::ListConstruct(%12, %12)
  %2655 : int[] = prim::ListConstruct(%12, %12)
  %2656 : int[] = prim::ListConstruct(%12, %12)
  %out.184 : Tensor = aten::conv2d(%out.183, %2652, %2653, %2654, %2655, %2656, %12) # torch/nn/modules/conv.py:415:15
  %2658 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%799)
  %2659 : int = aten::dim(%out.184) # torch/nn/modules/batchnorm.py:276:11
  %2660 : bool = aten::ne(%2659, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2660) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2661 : bool = prim::GetAttr[name="training"](%2658)
   = prim::If(%2661) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2662 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2658)
      %2663 : Tensor = aten::add(%2662, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2658, %2663)
      -> ()
    block1():
      -> ()
  %2664 : bool = prim::GetAttr[name="training"](%2658)
  %2665 : Tensor = prim::GetAttr[name="running_mean"](%2658)
  %2666 : Tensor = prim::GetAttr[name="running_var"](%2658)
  %2667 : Tensor = prim::GetAttr[name="weight"](%2658)
  %2668 : Tensor = prim::GetAttr[name="bias"](%2658)
   = prim::If(%2664) # torch/nn/functional.py:2011:4
    block0():
      %2669 : int[] = aten::size(%out.184) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%2669, %8) # torch/nn/functional.py:1991:17
      %2671 : int = aten::len(%2669) # torch/nn/functional.py:1992:19
      %2672 : int = aten::sub(%2671, %10) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%2672, %9, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %2676 : int = aten::add(%i.63, %10) # torch/nn/functional.py:1993:27
          %2677 : int = aten::__getitem__(%2669, %2676) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %2677) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.251)
      %2679 : bool = aten::eq(%size_prods.249, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2679) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.185 : Tensor = aten::batch_norm(%out.184, %2667, %2668, %2665, %2666, %2664, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.186 : Tensor = aten::relu_(%out.185) # torch/nn/functional.py:1117:17
  %2682 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%799)
  %2683 : Tensor = prim::GetAttr[name="weight"](%2682)
  %2684 : Tensor? = prim::GetAttr[name="bias"](%2682)
  %2685 : int[] = prim::ListConstruct(%12, %12)
  %2686 : int[] = prim::ListConstruct(%8, %8)
  %2687 : int[] = prim::ListConstruct(%12, %12)
  %out.187 : Tensor = aten::conv2d(%out.186, %2683, %2684, %2685, %2686, %2687, %12) # torch/nn/modules/conv.py:415:15
  %2689 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%799)
  %2690 : int = aten::dim(%out.187) # torch/nn/modules/batchnorm.py:276:11
  %2691 : bool = aten::ne(%2690, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2691) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2692 : bool = prim::GetAttr[name="training"](%2689)
   = prim::If(%2692) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2693 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2689)
      %2694 : Tensor = aten::add(%2693, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2689, %2694)
      -> ()
    block1():
      -> ()
  %2695 : bool = prim::GetAttr[name="training"](%2689)
  %2696 : Tensor = prim::GetAttr[name="running_mean"](%2689)
  %2697 : Tensor = prim::GetAttr[name="running_var"](%2689)
  %2698 : Tensor = prim::GetAttr[name="weight"](%2689)
  %2699 : Tensor = prim::GetAttr[name="bias"](%2689)
   = prim::If(%2695) # torch/nn/functional.py:2011:4
    block0():
      %2700 : int[] = aten::size(%out.187) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%2700, %8) # torch/nn/functional.py:1991:17
      %2702 : int = aten::len(%2700) # torch/nn/functional.py:1992:19
      %2703 : int = aten::sub(%2702, %10) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%2703, %9, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %2707 : int = aten::add(%i.64, %10) # torch/nn/functional.py:1993:27
          %2708 : int = aten::__getitem__(%2700, %2707) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %2708) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.255)
      %2710 : bool = aten::eq(%size_prods.253, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2710) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.188 : Tensor = aten::batch_norm(%out.187, %2698, %2699, %2696, %2697, %2695, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.189 : Tensor = aten::add_(%out.188, %input.39, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.41 : Tensor = aten::relu_(%out.189) # torch/nn/functional.py:1117:17
  %2714 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%800)
  %2715 : Tensor = prim::GetAttr[name="weight"](%2714)
  %2716 : Tensor? = prim::GetAttr[name="bias"](%2714)
  %2717 : int[] = prim::ListConstruct(%12, %12)
  %2718 : int[] = prim::ListConstruct(%8, %8)
  %2719 : int[] = prim::ListConstruct(%12, %12)
  %out.199 : Tensor = aten::conv2d(%input.41, %2715, %2716, %2717, %2718, %2719, %12) # torch/nn/modules/conv.py:415:15
  %2721 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%800)
  %2722 : int = aten::dim(%out.199) # torch/nn/modules/batchnorm.py:276:11
  %2723 : bool = aten::ne(%2722, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2723) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2724 : bool = prim::GetAttr[name="training"](%2721)
   = prim::If(%2724) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2725 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2721)
      %2726 : Tensor = aten::add(%2725, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2721, %2726)
      -> ()
    block1():
      -> ()
  %2727 : bool = prim::GetAttr[name="training"](%2721)
  %2728 : Tensor = prim::GetAttr[name="running_mean"](%2721)
  %2729 : Tensor = prim::GetAttr[name="running_var"](%2721)
  %2730 : Tensor = prim::GetAttr[name="weight"](%2721)
  %2731 : Tensor = prim::GetAttr[name="bias"](%2721)
   = prim::If(%2727) # torch/nn/functional.py:2011:4
    block0():
      %2732 : int[] = aten::size(%out.199) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%2732, %8) # torch/nn/functional.py:1991:17
      %2734 : int = aten::len(%2732) # torch/nn/functional.py:1992:19
      %2735 : int = aten::sub(%2734, %10) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%2735, %9, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %2739 : int = aten::add(%i.65, %10) # torch/nn/functional.py:1993:27
          %2740 : int = aten::__getitem__(%2732, %2739) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %2740) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.259)
      %2742 : bool = aten::eq(%size_prods.257, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2742) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.191 : Tensor = aten::batch_norm(%out.199, %2730, %2731, %2728, %2729, %2727, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.192 : Tensor = aten::relu_(%out.191) # torch/nn/functional.py:1117:17
  %2745 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%800)
  %2746 : Tensor = prim::GetAttr[name="weight"](%2745)
  %2747 : Tensor? = prim::GetAttr[name="bias"](%2745)
  %2748 : int[] = prim::ListConstruct(%12, %12)
  %2749 : int[] = prim::ListConstruct(%12, %12)
  %2750 : int[] = prim::ListConstruct(%12, %12)
  %out.193 : Tensor = aten::conv2d(%out.192, %2746, %2747, %2748, %2749, %2750, %12) # torch/nn/modules/conv.py:415:15
  %2752 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%800)
  %2753 : int = aten::dim(%out.193) # torch/nn/modules/batchnorm.py:276:11
  %2754 : bool = aten::ne(%2753, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2754) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2755 : bool = prim::GetAttr[name="training"](%2752)
   = prim::If(%2755) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2756 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2752)
      %2757 : Tensor = aten::add(%2756, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2752, %2757)
      -> ()
    block1():
      -> ()
  %2758 : bool = prim::GetAttr[name="training"](%2752)
  %2759 : Tensor = prim::GetAttr[name="running_mean"](%2752)
  %2760 : Tensor = prim::GetAttr[name="running_var"](%2752)
  %2761 : Tensor = prim::GetAttr[name="weight"](%2752)
  %2762 : Tensor = prim::GetAttr[name="bias"](%2752)
   = prim::If(%2758) # torch/nn/functional.py:2011:4
    block0():
      %2763 : int[] = aten::size(%out.193) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%2763, %8) # torch/nn/functional.py:1991:17
      %2765 : int = aten::len(%2763) # torch/nn/functional.py:1992:19
      %2766 : int = aten::sub(%2765, %10) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%2766, %9, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %2770 : int = aten::add(%i.66, %10) # torch/nn/functional.py:1993:27
          %2771 : int = aten::__getitem__(%2763, %2770) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %2771) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.263)
      %2773 : bool = aten::eq(%size_prods.261, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2773) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.194 : Tensor = aten::batch_norm(%out.193, %2761, %2762, %2759, %2760, %2758, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.195 : Tensor = aten::relu_(%out.194) # torch/nn/functional.py:1117:17
  %2776 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%800)
  %2777 : Tensor = prim::GetAttr[name="weight"](%2776)
  %2778 : Tensor? = prim::GetAttr[name="bias"](%2776)
  %2779 : int[] = prim::ListConstruct(%12, %12)
  %2780 : int[] = prim::ListConstruct(%8, %8)
  %2781 : int[] = prim::ListConstruct(%12, %12)
  %out.196 : Tensor = aten::conv2d(%out.195, %2777, %2778, %2779, %2780, %2781, %12) # torch/nn/modules/conv.py:415:15
  %2783 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%800)
  %2784 : int = aten::dim(%out.196) # torch/nn/modules/batchnorm.py:276:11
  %2785 : bool = aten::ne(%2784, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2785) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2786 : bool = prim::GetAttr[name="training"](%2783)
   = prim::If(%2786) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2787 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2783)
      %2788 : Tensor = aten::add(%2787, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2783, %2788)
      -> ()
    block1():
      -> ()
  %2789 : bool = prim::GetAttr[name="training"](%2783)
  %2790 : Tensor = prim::GetAttr[name="running_mean"](%2783)
  %2791 : Tensor = prim::GetAttr[name="running_var"](%2783)
  %2792 : Tensor = prim::GetAttr[name="weight"](%2783)
  %2793 : Tensor = prim::GetAttr[name="bias"](%2783)
   = prim::If(%2789) # torch/nn/functional.py:2011:4
    block0():
      %2794 : int[] = aten::size(%out.196) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%2794, %8) # torch/nn/functional.py:1991:17
      %2796 : int = aten::len(%2794) # torch/nn/functional.py:1992:19
      %2797 : int = aten::sub(%2796, %10) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%2797, %9, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %2801 : int = aten::add(%i.67, %10) # torch/nn/functional.py:1993:27
          %2802 : int = aten::__getitem__(%2794, %2801) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %2802) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.267)
      %2804 : bool = aten::eq(%size_prods.265, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2804) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.197 : Tensor = aten::batch_norm(%out.196, %2792, %2793, %2790, %2791, %2789, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.198 : Tensor = aten::add_(%out.197, %input.41, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.43 : Tensor = aten::relu_(%out.198) # torch/nn/functional.py:1117:17
  %2808 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%801)
  %2809 : Tensor = prim::GetAttr[name="weight"](%2808)
  %2810 : Tensor? = prim::GetAttr[name="bias"](%2808)
  %2811 : int[] = prim::ListConstruct(%12, %12)
  %2812 : int[] = prim::ListConstruct(%8, %8)
  %2813 : int[] = prim::ListConstruct(%12, %12)
  %out.208 : Tensor = aten::conv2d(%input.43, %2809, %2810, %2811, %2812, %2813, %12) # torch/nn/modules/conv.py:415:15
  %2815 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%801)
  %2816 : int = aten::dim(%out.208) # torch/nn/modules/batchnorm.py:276:11
  %2817 : bool = aten::ne(%2816, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2817) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2818 : bool = prim::GetAttr[name="training"](%2815)
   = prim::If(%2818) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2819 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2815)
      %2820 : Tensor = aten::add(%2819, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2815, %2820)
      -> ()
    block1():
      -> ()
  %2821 : bool = prim::GetAttr[name="training"](%2815)
  %2822 : Tensor = prim::GetAttr[name="running_mean"](%2815)
  %2823 : Tensor = prim::GetAttr[name="running_var"](%2815)
  %2824 : Tensor = prim::GetAttr[name="weight"](%2815)
  %2825 : Tensor = prim::GetAttr[name="bias"](%2815)
   = prim::If(%2821) # torch/nn/functional.py:2011:4
    block0():
      %2826 : int[] = aten::size(%out.208) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%2826, %8) # torch/nn/functional.py:1991:17
      %2828 : int = aten::len(%2826) # torch/nn/functional.py:1992:19
      %2829 : int = aten::sub(%2828, %10) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%2829, %9, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %2833 : int = aten::add(%i.68, %10) # torch/nn/functional.py:1993:27
          %2834 : int = aten::__getitem__(%2826, %2833) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %2834) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.271)
      %2836 : bool = aten::eq(%size_prods.269, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2836) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.200 : Tensor = aten::batch_norm(%out.208, %2824, %2825, %2822, %2823, %2821, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.201 : Tensor = aten::relu_(%out.200) # torch/nn/functional.py:1117:17
  %2839 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%801)
  %2840 : Tensor = prim::GetAttr[name="weight"](%2839)
  %2841 : Tensor? = prim::GetAttr[name="bias"](%2839)
  %2842 : int[] = prim::ListConstruct(%12, %12)
  %2843 : int[] = prim::ListConstruct(%12, %12)
  %2844 : int[] = prim::ListConstruct(%12, %12)
  %out.202 : Tensor = aten::conv2d(%out.201, %2840, %2841, %2842, %2843, %2844, %12) # torch/nn/modules/conv.py:415:15
  %2846 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%801)
  %2847 : int = aten::dim(%out.202) # torch/nn/modules/batchnorm.py:276:11
  %2848 : bool = aten::ne(%2847, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2848) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2849 : bool = prim::GetAttr[name="training"](%2846)
   = prim::If(%2849) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2850 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2846)
      %2851 : Tensor = aten::add(%2850, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2846, %2851)
      -> ()
    block1():
      -> ()
  %2852 : bool = prim::GetAttr[name="training"](%2846)
  %2853 : Tensor = prim::GetAttr[name="running_mean"](%2846)
  %2854 : Tensor = prim::GetAttr[name="running_var"](%2846)
  %2855 : Tensor = prim::GetAttr[name="weight"](%2846)
  %2856 : Tensor = prim::GetAttr[name="bias"](%2846)
   = prim::If(%2852) # torch/nn/functional.py:2011:4
    block0():
      %2857 : int[] = aten::size(%out.202) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%2857, %8) # torch/nn/functional.py:1991:17
      %2859 : int = aten::len(%2857) # torch/nn/functional.py:1992:19
      %2860 : int = aten::sub(%2859, %10) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%2860, %9, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %2864 : int = aten::add(%i.69, %10) # torch/nn/functional.py:1993:27
          %2865 : int = aten::__getitem__(%2857, %2864) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %2865) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.275)
      %2867 : bool = aten::eq(%size_prods.273, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2867) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.203 : Tensor = aten::batch_norm(%out.202, %2855, %2856, %2853, %2854, %2852, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.204 : Tensor = aten::relu_(%out.203) # torch/nn/functional.py:1117:17
  %2870 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%801)
  %2871 : Tensor = prim::GetAttr[name="weight"](%2870)
  %2872 : Tensor? = prim::GetAttr[name="bias"](%2870)
  %2873 : int[] = prim::ListConstruct(%12, %12)
  %2874 : int[] = prim::ListConstruct(%8, %8)
  %2875 : int[] = prim::ListConstruct(%12, %12)
  %out.205 : Tensor = aten::conv2d(%out.204, %2871, %2872, %2873, %2874, %2875, %12) # torch/nn/modules/conv.py:415:15
  %2877 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%801)
  %2878 : int = aten::dim(%out.205) # torch/nn/modules/batchnorm.py:276:11
  %2879 : bool = aten::ne(%2878, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2879) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2880 : bool = prim::GetAttr[name="training"](%2877)
   = prim::If(%2880) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2881 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2877)
      %2882 : Tensor = aten::add(%2881, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2877, %2882)
      -> ()
    block1():
      -> ()
  %2883 : bool = prim::GetAttr[name="training"](%2877)
  %2884 : Tensor = prim::GetAttr[name="running_mean"](%2877)
  %2885 : Tensor = prim::GetAttr[name="running_var"](%2877)
  %2886 : Tensor = prim::GetAttr[name="weight"](%2877)
  %2887 : Tensor = prim::GetAttr[name="bias"](%2877)
   = prim::If(%2883) # torch/nn/functional.py:2011:4
    block0():
      %2888 : int[] = aten::size(%out.205) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%2888, %8) # torch/nn/functional.py:1991:17
      %2890 : int = aten::len(%2888) # torch/nn/functional.py:1992:19
      %2891 : int = aten::sub(%2890, %10) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%2891, %9, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %2895 : int = aten::add(%i.70, %10) # torch/nn/functional.py:1993:27
          %2896 : int = aten::__getitem__(%2888, %2895) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %2896) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.279)
      %2898 : bool = aten::eq(%size_prods.277, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2898) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.206 : Tensor = aten::batch_norm(%out.205, %2886, %2887, %2884, %2885, %2883, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.207 : Tensor = aten::add_(%out.206, %input.43, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.45 : Tensor = aten::relu_(%out.207) # torch/nn/functional.py:1117:17
  %2902 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%802)
  %2903 : Tensor = prim::GetAttr[name="weight"](%2902)
  %2904 : Tensor? = prim::GetAttr[name="bias"](%2902)
  %2905 : int[] = prim::ListConstruct(%12, %12)
  %2906 : int[] = prim::ListConstruct(%8, %8)
  %2907 : int[] = prim::ListConstruct(%12, %12)
  %out.289 : Tensor = aten::conv2d(%input.45, %2903, %2904, %2905, %2906, %2907, %12) # torch/nn/modules/conv.py:415:15
  %2909 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%802)
  %2910 : int = aten::dim(%out.289) # torch/nn/modules/batchnorm.py:276:11
  %2911 : bool = aten::ne(%2910, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2911) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2912 : bool = prim::GetAttr[name="training"](%2909)
   = prim::If(%2912) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2913 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2909)
      %2914 : Tensor = aten::add(%2913, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2909, %2914)
      -> ()
    block1():
      -> ()
  %2915 : bool = prim::GetAttr[name="training"](%2909)
  %2916 : Tensor = prim::GetAttr[name="running_mean"](%2909)
  %2917 : Tensor = prim::GetAttr[name="running_var"](%2909)
  %2918 : Tensor = prim::GetAttr[name="weight"](%2909)
  %2919 : Tensor = prim::GetAttr[name="bias"](%2909)
   = prim::If(%2915) # torch/nn/functional.py:2011:4
    block0():
      %2920 : int[] = aten::size(%out.289) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%2920, %8) # torch/nn/functional.py:1991:17
      %2922 : int = aten::len(%2920) # torch/nn/functional.py:1992:19
      %2923 : int = aten::sub(%2922, %10) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%2923, %9, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %2927 : int = aten::add(%i.102, %10) # torch/nn/functional.py:1993:27
          %2928 : int = aten::__getitem__(%2920, %2927) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %2928) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.407)
      %2930 : bool = aten::eq(%size_prods.405, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2930) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.290 : Tensor = aten::batch_norm(%out.289, %2918, %2919, %2916, %2917, %2915, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.291 : Tensor = aten::relu_(%out.290) # torch/nn/functional.py:1117:17
  %2933 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%802)
  %2934 : Tensor = prim::GetAttr[name="weight"](%2933)
  %2935 : Tensor? = prim::GetAttr[name="bias"](%2933)
  %2936 : int[] = prim::ListConstruct(%12, %12)
  %2937 : int[] = prim::ListConstruct(%12, %12)
  %2938 : int[] = prim::ListConstruct(%12, %12)
  %out.292 : Tensor = aten::conv2d(%out.291, %2934, %2935, %2936, %2937, %2938, %12) # torch/nn/modules/conv.py:415:15
  %2940 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%802)
  %2941 : int = aten::dim(%out.292) # torch/nn/modules/batchnorm.py:276:11
  %2942 : bool = aten::ne(%2941, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2942) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2943 : bool = prim::GetAttr[name="training"](%2940)
   = prim::If(%2943) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2944 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2940)
      %2945 : Tensor = aten::add(%2944, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2940, %2945)
      -> ()
    block1():
      -> ()
  %2946 : bool = prim::GetAttr[name="training"](%2940)
  %2947 : Tensor = prim::GetAttr[name="running_mean"](%2940)
  %2948 : Tensor = prim::GetAttr[name="running_var"](%2940)
  %2949 : Tensor = prim::GetAttr[name="weight"](%2940)
  %2950 : Tensor = prim::GetAttr[name="bias"](%2940)
   = prim::If(%2946) # torch/nn/functional.py:2011:4
    block0():
      %2951 : int[] = aten::size(%out.292) # torch/nn/functional.py:2012:27
      %size_prods.408 : int = aten::__getitem__(%2951, %8) # torch/nn/functional.py:1991:17
      %2953 : int = aten::len(%2951) # torch/nn/functional.py:1992:19
      %2954 : int = aten::sub(%2953, %10) # torch/nn/functional.py:1992:19
      %size_prods.409 : int = prim::Loop(%2954, %9, %size_prods.408) # torch/nn/functional.py:1992:4
        block0(%i.103 : int, %size_prods.410 : int):
          %2958 : int = aten::add(%i.103, %10) # torch/nn/functional.py:1993:27
          %2959 : int = aten::__getitem__(%2951, %2958) # torch/nn/functional.py:1993:22
          %size_prods.411 : int = aten::mul(%size_prods.410, %2959) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.411)
      %2961 : bool = aten::eq(%size_prods.409, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2961) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.293 : Tensor = aten::batch_norm(%out.292, %2949, %2950, %2947, %2948, %2946, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.294 : Tensor = aten::relu_(%out.293) # torch/nn/functional.py:1117:17
  %2964 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv3"](%802)
  %2965 : Tensor = prim::GetAttr[name="weight"](%2964)
  %2966 : Tensor? = prim::GetAttr[name="bias"](%2964)
  %2967 : int[] = prim::ListConstruct(%12, %12)
  %2968 : int[] = prim::ListConstruct(%8, %8)
  %2969 : int[] = prim::ListConstruct(%12, %12)
  %out.295 : Tensor = aten::conv2d(%out.294, %2965, %2966, %2967, %2968, %2969, %12) # torch/nn/modules/conv.py:415:15
  %2971 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%802)
  %2972 : int = aten::dim(%out.295) # torch/nn/modules/batchnorm.py:276:11
  %2973 : bool = aten::ne(%2972, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2973) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2974 : bool = prim::GetAttr[name="training"](%2971)
   = prim::If(%2974) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2975 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2971)
      %2976 : Tensor = aten::add(%2975, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2971, %2976)
      -> ()
    block1():
      -> ()
  %2977 : bool = prim::GetAttr[name="training"](%2971)
  %2978 : Tensor = prim::GetAttr[name="running_mean"](%2971)
  %2979 : Tensor = prim::GetAttr[name="running_var"](%2971)
  %2980 : Tensor = prim::GetAttr[name="weight"](%2971)
  %2981 : Tensor = prim::GetAttr[name="bias"](%2971)
   = prim::If(%2977) # torch/nn/functional.py:2011:4
    block0():
      %2982 : int[] = aten::size(%out.295) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%2982, %8) # torch/nn/functional.py:1991:17
      %2984 : int = aten::len(%2982) # torch/nn/functional.py:1992:19
      %2985 : int = aten::sub(%2984, %10) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%2985, %9, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %2989 : int = aten::add(%i.104, %10) # torch/nn/functional.py:1993:27
          %2990 : int = aten::__getitem__(%2982, %2989) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %2990) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.415)
      %2992 : bool = aten::eq(%size_prods.413, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2992) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.296 : Tensor = aten::batch_norm(%out.295, %2980, %2981, %2978, %2979, %2977, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.297 : Tensor = aten::add_(%out.296, %input.45, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.15 : Tensor = aten::relu_(%out.297) # torch/nn/functional.py:1117:17
  %2996 : __torch__.torch.nn.modules.container.___torch_mangle_1142.Sequential = prim::GetAttr[name="layer4"](%self)
  %2997 : __torch__.torchvision.models.resnet.___torch_mangle_1139.Bottleneck = prim::GetAttr[name="0"](%2996)
  %2998 : __torch__.torchvision.models.resnet.___torch_mangle_1141.Bottleneck = prim::GetAttr[name="1"](%2996)
  %2999 : __torch__.torchvision.models.resnet.___torch_mangle_1141.Bottleneck = prim::GetAttr[name="2"](%2996)
  %3000 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%2997)
  %3001 : Tensor = prim::GetAttr[name="weight"](%3000)
  %3002 : Tensor? = prim::GetAttr[name="bias"](%3000)
  %3003 : int[] = prim::ListConstruct(%12, %12)
  %3004 : int[] = prim::ListConstruct(%8, %8)
  %3005 : int[] = prim::ListConstruct(%12, %12)
  %out.2 : Tensor = aten::conv2d(%x.15, %3001, %3002, %3003, %3004, %3005, %12) # torch/nn/modules/conv.py:415:15
  %3007 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%2997)
  %3008 : int = aten::dim(%out.2) # torch/nn/modules/batchnorm.py:276:11
  %3009 : bool = aten::ne(%3008, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3009) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3010 : bool = prim::GetAttr[name="training"](%3007)
   = prim::If(%3010) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3011 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3007)
      %3012 : Tensor = aten::add(%3011, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3007, %3012)
      -> ()
    block1():
      -> ()
  %3013 : bool = prim::GetAttr[name="training"](%3007)
  %3014 : Tensor = prim::GetAttr[name="running_mean"](%3007)
  %3015 : Tensor = prim::GetAttr[name="running_var"](%3007)
  %3016 : Tensor = prim::GetAttr[name="weight"](%3007)
  %3017 : Tensor = prim::GetAttr[name="bias"](%3007)
   = prim::If(%3013) # torch/nn/functional.py:2011:4
    block0():
      %3018 : int[] = aten::size(%out.2) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%3018, %8) # torch/nn/functional.py:1991:17
      %3020 : int = aten::len(%3018) # torch/nn/functional.py:1992:19
      %3021 : int = aten::sub(%3020, %10) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%3021, %9, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.18 : int):
          %3025 : int = aten::add(%i.5, %10) # torch/nn/functional.py:1993:27
          %3026 : int = aten::__getitem__(%3018, %3025) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %3026) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.19)
      %3028 : bool = aten::eq(%size_prods.17, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3028) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.4 : Tensor = aten::batch_norm(%out.2, %3016, %3017, %3014, %3015, %3013, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.6 : Tensor = aten::relu_(%out.4) # torch/nn/functional.py:1117:17
  %3031 : __torch__.torch.nn.modules.conv.___torch_mangle_1138.Conv2d = prim::GetAttr[name="conv2"](%2997)
  %3032 : Tensor = prim::GetAttr[name="weight"](%3031)
  %3033 : Tensor? = prim::GetAttr[name="bias"](%3031)
  %3034 : int[] = prim::ListConstruct(%10, %10)
  %3035 : int[] = prim::ListConstruct(%12, %12)
  %3036 : int[] = prim::ListConstruct(%12, %12)
  %out.8 : Tensor = aten::conv2d(%out.6, %3032, %3033, %3034, %3035, %3036, %12) # torch/nn/modules/conv.py:415:15
  %3038 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%2997)
  %3039 : int = aten::dim(%out.8) # torch/nn/modules/batchnorm.py:276:11
  %3040 : bool = aten::ne(%3039, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3040) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3041 : bool = prim::GetAttr[name="training"](%3038)
   = prim::If(%3041) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3042 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3038)
      %3043 : Tensor = aten::add(%3042, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3038, %3043)
      -> ()
    block1():
      -> ()
  %3044 : bool = prim::GetAttr[name="training"](%3038)
  %3045 : Tensor = prim::GetAttr[name="running_mean"](%3038)
  %3046 : Tensor = prim::GetAttr[name="running_var"](%3038)
  %3047 : Tensor = prim::GetAttr[name="weight"](%3038)
  %3048 : Tensor = prim::GetAttr[name="bias"](%3038)
   = prim::If(%3044) # torch/nn/functional.py:2011:4
    block0():
      %3049 : int[] = aten::size(%out.8) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%3049, %8) # torch/nn/functional.py:1991:17
      %3051 : int = aten::len(%3049) # torch/nn/functional.py:1992:19
      %3052 : int = aten::sub(%3051, %10) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%3052, %9, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %3056 : int = aten::add(%i.6, %10) # torch/nn/functional.py:1993:27
          %3057 : int = aten::__getitem__(%3049, %3056) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %3057) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.23)
      %3059 : bool = aten::eq(%size_prods.21, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3059) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.10 : Tensor = aten::batch_norm(%out.8, %3047, %3048, %3045, %3046, %3044, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.12 : Tensor = aten::relu_(%out.10) # torch/nn/functional.py:1117:17
  %3062 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="conv3"](%2997)
  %3063 : Tensor = prim::GetAttr[name="weight"](%3062)
  %3064 : Tensor? = prim::GetAttr[name="bias"](%3062)
  %3065 : int[] = prim::ListConstruct(%12, %12)
  %3066 : int[] = prim::ListConstruct(%8, %8)
  %3067 : int[] = prim::ListConstruct(%12, %12)
  %out.14 : Tensor = aten::conv2d(%out.12, %3063, %3064, %3065, %3066, %3067, %12) # torch/nn/modules/conv.py:415:15
  %3069 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%2997)
  %3070 : int = aten::dim(%out.14) # torch/nn/modules/batchnorm.py:276:11
  %3071 : bool = aten::ne(%3070, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3071) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3072 : bool = prim::GetAttr[name="training"](%3069)
   = prim::If(%3072) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3073 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3069)
      %3074 : Tensor = aten::add(%3073, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3069, %3074)
      -> ()
    block1():
      -> ()
  %3075 : bool = prim::GetAttr[name="training"](%3069)
  %3076 : Tensor = prim::GetAttr[name="running_mean"](%3069)
  %3077 : Tensor = prim::GetAttr[name="running_var"](%3069)
  %3078 : Tensor = prim::GetAttr[name="weight"](%3069)
  %3079 : Tensor = prim::GetAttr[name="bias"](%3069)
   = prim::If(%3075) # torch/nn/functional.py:2011:4
    block0():
      %3080 : int[] = aten::size(%out.14) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%3080, %8) # torch/nn/functional.py:1991:17
      %3082 : int = aten::len(%3080) # torch/nn/functional.py:1992:19
      %3083 : int = aten::sub(%3082, %10) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%3083, %9, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %3087 : int = aten::add(%i.4, %10) # torch/nn/functional.py:1993:27
          %3088 : int = aten::__getitem__(%3080, %3087) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %3088) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.15)
      %3090 : bool = aten::eq(%size_prods.13, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3090) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.16 : Tensor = aten::batch_norm(%out.14, %3078, %3079, %3076, %3077, %3075, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %3092 : __torch__.torch.nn.modules.container.___torch_mangle_946.Sequential = prim::GetAttr[name="downsample"](%2997)
  %3093 : __torch__.torch.nn.modules.conv.___torch_mangle_945.Conv2d = prim::GetAttr[name="0"](%3092)
  %3094 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="1"](%3092)
  %3095 : Tensor = prim::GetAttr[name="weight"](%3093)
  %3096 : Tensor? = prim::GetAttr[name="bias"](%3093)
  %3097 : int[] = prim::ListConstruct(%10, %10)
  %3098 : int[] = prim::ListConstruct(%8, %8)
  %3099 : int[] = prim::ListConstruct(%12, %12)
  %input.3 : Tensor = aten::conv2d(%x.15, %3095, %3096, %3097, %3098, %3099, %12) # torch/nn/modules/conv.py:415:15
  %3101 : int = aten::dim(%input.3) # torch/nn/modules/batchnorm.py:276:11
  %3102 : bool = aten::ne(%3101, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3102) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3103 : bool = prim::GetAttr[name="training"](%3094)
   = prim::If(%3103) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3104 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3094)
      %3105 : Tensor = aten::add(%3104, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3094, %3105)
      -> ()
    block1():
      -> ()
  %3106 : bool = prim::GetAttr[name="training"](%3094)
  %3107 : Tensor = prim::GetAttr[name="running_mean"](%3094)
  %3108 : Tensor = prim::GetAttr[name="running_var"](%3094)
  %3109 : Tensor = prim::GetAttr[name="weight"](%3094)
  %3110 : Tensor = prim::GetAttr[name="bias"](%3094)
   = prim::If(%3106) # torch/nn/functional.py:2011:4
    block0():
      %3111 : int[] = aten::size(%input.3) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%3111, %8) # torch/nn/functional.py:1991:17
      %3113 : int = aten::len(%3111) # torch/nn/functional.py:1992:19
      %3114 : int = aten::sub(%3113, %10) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%3114, %9, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %3118 : int = aten::add(%i.7, %10) # torch/nn/functional.py:1993:27
          %3119 : int = aten::__getitem__(%3111, %3118) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %3119) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.27)
      %3121 : bool = aten::eq(%size_prods.25, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3121) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.1 : Tensor = aten::batch_norm(%input.3, %3109, %3110, %3107, %3108, %3106, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.18 : Tensor = aten::add_(%out.16, %identity.1, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.4 : Tensor = aten::relu_(%out.18) # torch/nn/functional.py:1117:17
  %3125 : __torch__.torch.nn.modules.conv.___torch_mangle_1018.Conv2d = prim::GetAttr[name="conv1"](%2998)
  %3126 : Tensor = prim::GetAttr[name="weight"](%3125)
  %3127 : Tensor? = prim::GetAttr[name="bias"](%3125)
  %3128 : int[] = prim::ListConstruct(%12, %12)
  %3129 : int[] = prim::ListConstruct(%8, %8)
  %3130 : int[] = prim::ListConstruct(%12, %12)
  %out.28 : Tensor = aten::conv2d(%input.4, %3126, %3127, %3128, %3129, %3130, %12) # torch/nn/modules/conv.py:415:15
  %3132 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%2998)
  %3133 : int = aten::dim(%out.28) # torch/nn/modules/batchnorm.py:276:11
  %3134 : bool = aten::ne(%3133, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3134) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3135 : bool = prim::GetAttr[name="training"](%3132)
   = prim::If(%3135) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3136 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3132)
      %3137 : Tensor = aten::add(%3136, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3132, %3137)
      -> ()
    block1():
      -> ()
  %3138 : bool = prim::GetAttr[name="training"](%3132)
  %3139 : Tensor = prim::GetAttr[name="running_mean"](%3132)
  %3140 : Tensor = prim::GetAttr[name="running_var"](%3132)
  %3141 : Tensor = prim::GetAttr[name="weight"](%3132)
  %3142 : Tensor = prim::GetAttr[name="bias"](%3132)
   = prim::If(%3138) # torch/nn/functional.py:2011:4
    block0():
      %3143 : int[] = aten::size(%out.28) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%3143, %8) # torch/nn/functional.py:1991:17
      %3145 : int = aten::len(%3143) # torch/nn/functional.py:1992:19
      %3146 : int = aten::sub(%3145, %10) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%3146, %9, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %3150 : int = aten::add(%i.8, %10) # torch/nn/functional.py:1993:27
          %3151 : int = aten::__getitem__(%3143, %3150) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %3151) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.31)
      %3153 : bool = aten::eq(%size_prods.29, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3153) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.20 : Tensor = aten::batch_norm(%out.28, %3141, %3142, %3139, %3140, %3138, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.21 : Tensor = aten::relu_(%out.20) # torch/nn/functional.py:1117:17
  %3156 : __torch__.torch.nn.modules.conv.___torch_mangle_1140.Conv2d = prim::GetAttr[name="conv2"](%2998)
  %3157 : Tensor = prim::GetAttr[name="weight"](%3156)
  %3158 : Tensor? = prim::GetAttr[name="bias"](%3156)
  %3159 : int[] = prim::ListConstruct(%12, %12)
  %3160 : int[] = prim::ListConstruct(%12, %12)
  %3161 : int[] = prim::ListConstruct(%12, %12)
  %out.22 : Tensor = aten::conv2d(%out.21, %3157, %3158, %3159, %3160, %3161, %12) # torch/nn/modules/conv.py:415:15
  %3163 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%2998)
  %3164 : int = aten::dim(%out.22) # torch/nn/modules/batchnorm.py:276:11
  %3165 : bool = aten::ne(%3164, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3165) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3166 : bool = prim::GetAttr[name="training"](%3163)
   = prim::If(%3166) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3167 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3163)
      %3168 : Tensor = aten::add(%3167, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3163, %3168)
      -> ()
    block1():
      -> ()
  %3169 : bool = prim::GetAttr[name="training"](%3163)
  %3170 : Tensor = prim::GetAttr[name="running_mean"](%3163)
  %3171 : Tensor = prim::GetAttr[name="running_var"](%3163)
  %3172 : Tensor = prim::GetAttr[name="weight"](%3163)
  %3173 : Tensor = prim::GetAttr[name="bias"](%3163)
   = prim::If(%3169) # torch/nn/functional.py:2011:4
    block0():
      %3174 : int[] = aten::size(%out.22) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%3174, %8) # torch/nn/functional.py:1991:17
      %3176 : int = aten::len(%3174) # torch/nn/functional.py:1992:19
      %3177 : int = aten::sub(%3176, %10) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%3177, %9, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %3181 : int = aten::add(%i.9, %10) # torch/nn/functional.py:1993:27
          %3182 : int = aten::__getitem__(%3174, %3181) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %3182) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.35)
      %3184 : bool = aten::eq(%size_prods.33, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3184) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.23 : Tensor = aten::batch_norm(%out.22, %3172, %3173, %3170, %3171, %3169, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.24 : Tensor = aten::relu_(%out.23) # torch/nn/functional.py:1117:17
  %3187 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="conv3"](%2998)
  %3188 : Tensor = prim::GetAttr[name="weight"](%3187)
  %3189 : Tensor? = prim::GetAttr[name="bias"](%3187)
  %3190 : int[] = prim::ListConstruct(%12, %12)
  %3191 : int[] = prim::ListConstruct(%8, %8)
  %3192 : int[] = prim::ListConstruct(%12, %12)
  %out.25 : Tensor = aten::conv2d(%out.24, %3188, %3189, %3190, %3191, %3192, %12) # torch/nn/modules/conv.py:415:15
  %3194 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%2998)
  %3195 : int = aten::dim(%out.25) # torch/nn/modules/batchnorm.py:276:11
  %3196 : bool = aten::ne(%3195, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3196) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3197 : bool = prim::GetAttr[name="training"](%3194)
   = prim::If(%3197) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3198 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3194)
      %3199 : Tensor = aten::add(%3198, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3194, %3199)
      -> ()
    block1():
      -> ()
  %3200 : bool = prim::GetAttr[name="training"](%3194)
  %3201 : Tensor = prim::GetAttr[name="running_mean"](%3194)
  %3202 : Tensor = prim::GetAttr[name="running_var"](%3194)
  %3203 : Tensor = prim::GetAttr[name="weight"](%3194)
  %3204 : Tensor = prim::GetAttr[name="bias"](%3194)
   = prim::If(%3200) # torch/nn/functional.py:2011:4
    block0():
      %3205 : int[] = aten::size(%out.25) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%3205, %8) # torch/nn/functional.py:1991:17
      %3207 : int = aten::len(%3205) # torch/nn/functional.py:1992:19
      %3208 : int = aten::sub(%3207, %10) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%3208, %9, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %3212 : int = aten::add(%i.10, %10) # torch/nn/functional.py:1993:27
          %3213 : int = aten::__getitem__(%3205, %3212) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %3213) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.39)
      %3215 : bool = aten::eq(%size_prods.37, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3215) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.26 : Tensor = aten::batch_norm(%out.25, %3203, %3204, %3201, %3202, %3200, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.27 : Tensor = aten::add_(%out.26, %input.4, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.5 : Tensor = aten::relu_(%out.27) # torch/nn/functional.py:1117:17
  %3219 : __torch__.torch.nn.modules.conv.___torch_mangle_1018.Conv2d = prim::GetAttr[name="conv1"](%2999)
  %3220 : Tensor = prim::GetAttr[name="weight"](%3219)
  %3221 : Tensor? = prim::GetAttr[name="bias"](%3219)
  %3222 : int[] = prim::ListConstruct(%12, %12)
  %3223 : int[] = prim::ListConstruct(%8, %8)
  %3224 : int[] = prim::ListConstruct(%12, %12)
  %out.1 : Tensor = aten::conv2d(%input.5, %3220, %3221, %3222, %3223, %3224, %12) # torch/nn/modules/conv.py:415:15
  %3226 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%2999)
  %3227 : int = aten::dim(%out.1) # torch/nn/modules/batchnorm.py:276:11
  %3228 : bool = aten::ne(%3227, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3228) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3229 : bool = prim::GetAttr[name="training"](%3226)
   = prim::If(%3229) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3230 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3226)
      %3231 : Tensor = aten::add(%3230, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3226, %3231)
      -> ()
    block1():
      -> ()
  %3232 : bool = prim::GetAttr[name="training"](%3226)
  %3233 : Tensor = prim::GetAttr[name="running_mean"](%3226)
  %3234 : Tensor = prim::GetAttr[name="running_var"](%3226)
  %3235 : Tensor = prim::GetAttr[name="weight"](%3226)
  %3236 : Tensor = prim::GetAttr[name="bias"](%3226)
   = prim::If(%3232) # torch/nn/functional.py:2011:4
    block0():
      %3237 : int[] = aten::size(%out.1) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%3237, %8) # torch/nn/functional.py:1991:17
      %3239 : int = aten::len(%3237) # torch/nn/functional.py:1992:19
      %3240 : int = aten::sub(%3239, %10) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%3240, %9, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.7 : int):
          %3244 : int = aten::add(%i.2, %10) # torch/nn/functional.py:1993:27
          %3245 : int = aten::__getitem__(%3237, %3244) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %3245) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.5)
      %3247 : bool = aten::eq(%size_prods.4, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3247) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.3 : Tensor = aten::batch_norm(%out.1, %3235, %3236, %3233, %3234, %3232, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.5 : Tensor = aten::relu_(%out.3) # torch/nn/functional.py:1117:17
  %3250 : __torch__.torch.nn.modules.conv.___torch_mangle_1140.Conv2d = prim::GetAttr[name="conv2"](%2999)
  %3251 : Tensor = prim::GetAttr[name="weight"](%3250)
  %3252 : Tensor? = prim::GetAttr[name="bias"](%3250)
  %3253 : int[] = prim::ListConstruct(%12, %12)
  %3254 : int[] = prim::ListConstruct(%12, %12)
  %3255 : int[] = prim::ListConstruct(%12, %12)
  %out.7 : Tensor = aten::conv2d(%out.5, %3251, %3252, %3253, %3254, %3255, %12) # torch/nn/modules/conv.py:415:15
  %3257 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%2999)
  %3258 : int = aten::dim(%out.7) # torch/nn/modules/batchnorm.py:276:11
  %3259 : bool = aten::ne(%3258, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3259) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3260 : bool = prim::GetAttr[name="training"](%3257)
   = prim::If(%3260) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3261 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3257)
      %3262 : Tensor = aten::add(%3261, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3257, %3262)
      -> ()
    block1():
      -> ()
  %3263 : bool = prim::GetAttr[name="training"](%3257)
  %3264 : Tensor = prim::GetAttr[name="running_mean"](%3257)
  %3265 : Tensor = prim::GetAttr[name="running_var"](%3257)
  %3266 : Tensor = prim::GetAttr[name="weight"](%3257)
  %3267 : Tensor = prim::GetAttr[name="bias"](%3257)
   = prim::If(%3263) # torch/nn/functional.py:2011:4
    block0():
      %3268 : int[] = aten::size(%out.7) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%3268, %8) # torch/nn/functional.py:1991:17
      %3270 : int = aten::len(%3268) # torch/nn/functional.py:1992:19
      %3271 : int = aten::sub(%3270, %10) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%3271, %9, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %3275 : int = aten::add(%i.3, %10) # torch/nn/functional.py:1993:27
          %3276 : int = aten::__getitem__(%3268, %3275) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %3276) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.11)
      %3278 : bool = aten::eq(%size_prods.9, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3278) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.9 : Tensor = aten::batch_norm(%out.7, %3266, %3267, %3264, %3265, %3263, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.11 : Tensor = aten::relu_(%out.9) # torch/nn/functional.py:1117:17
  %3281 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="conv3"](%2999)
  %3282 : Tensor = prim::GetAttr[name="weight"](%3281)
  %3283 : Tensor? = prim::GetAttr[name="bias"](%3281)
  %3284 : int[] = prim::ListConstruct(%12, %12)
  %3285 : int[] = prim::ListConstruct(%8, %8)
  %3286 : int[] = prim::ListConstruct(%12, %12)
  %out.13 : Tensor = aten::conv2d(%out.11, %3282, %3283, %3284, %3285, %3286, %12) # torch/nn/modules/conv.py:415:15
  %3288 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%2999)
  %3289 : int = aten::dim(%out.13) # torch/nn/modules/batchnorm.py:276:11
  %3290 : bool = aten::ne(%3289, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3290) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3291 : bool = prim::GetAttr[name="training"](%3288)
   = prim::If(%3291) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3292 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3288)
      %3293 : Tensor = aten::add(%3292, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3288, %3293)
      -> ()
    block1():
      -> ()
  %3294 : bool = prim::GetAttr[name="training"](%3288)
  %3295 : Tensor = prim::GetAttr[name="running_mean"](%3288)
  %3296 : Tensor = prim::GetAttr[name="running_var"](%3288)
  %3297 : Tensor = prim::GetAttr[name="weight"](%3288)
  %3298 : Tensor = prim::GetAttr[name="bias"](%3288)
   = prim::If(%3294) # torch/nn/functional.py:2011:4
    block0():
      %3299 : int[] = aten::size(%out.13) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3299, %8) # torch/nn/functional.py:1991:17
      %3301 : int = aten::len(%3299) # torch/nn/functional.py:1992:19
      %3302 : int = aten::sub(%3301, %10) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3302, %9, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %3306 : int = aten::add(%i.1, %10) # torch/nn/functional.py:1993:27
          %3307 : int = aten::__getitem__(%3299, %3306) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3307) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.3)
      %3309 : bool = aten::eq(%size_prods, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3309) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.15 : Tensor = aten::batch_norm(%out.13, %3297, %3298, %3295, %3296, %3294, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.17 : Tensor = aten::add_(%out.15, %input.5, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.17 : Tensor = aten::relu_(%out.17) # torch/nn/functional.py:1117:17
  %3313 : int[] = prim::ListConstruct(%12, %12)
  %3314 : int[] = aten::size(%x.17) # torch/nn/functional.py:925:51
  %3315 : int = aten::len(%3314) # <string>:5:9
  %3316 : bool = aten::gt(%3315, %10) # <string>:5:9
   = prim::If(%3316) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%7) # <string>:5:2
      -> ()
  %x.19 : Tensor = aten::adaptive_avg_pool2d(%x.17, %3313) # torch/nn/functional.py:926:11
  %x.21 : Tensor = aten::flatten(%x.19, %12, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:12
  %3319 : __torch__.torch.nn.modules.linear.___torch_mangle_621.Linear = prim::GetAttr[name="fc"](%self)
  %3320 : Tensor = prim::GetAttr[name="weight"](%3319)
  %3321 : Tensor = prim::GetAttr[name="bias"](%3319)
  %3322 : int = aten::dim(%x.21) # torch/nn/functional.py:1672:7
  %3323 : bool = aten::eq(%3322, %10) # torch/nn/functional.py:1672:7
  %x.23 : Tensor = prim::If(%3323) # torch/nn/functional.py:1672:4
    block0():
      %3325 : Tensor = aten::t(%3320) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%3321, %x.21, %3325, %12, %12) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %3327 : Tensor = aten::t(%3320) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%x.21, %3327) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %3321, %12) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%x.23)
