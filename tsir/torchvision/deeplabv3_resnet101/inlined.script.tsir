graph(%self : __torch__.torchvision.models.segmentation.deeplabv3.DeepLabV3,
      %x.1 : Tensor):
  %2 : Function = prim::Constant[name="interpolate"]()
  %3 : None = prim::Constant()
  %4 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:25:78
  %5 : str = prim::Constant[value="bilinear"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:25:52
  %6 : str = prim::Constant[value="out"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:23:21
  %7 : int = prim::Constant[value=9223372036854775807]()
  %8 : int = prim::Constant[value=-2]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:30
  %9 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:22
  %10 : int[] = aten::size(%x.1) # <string>:7:9
  %input_shape.1 : int[] = aten::slice(%10, %8, %7, %9) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:22
  %12 : __torch__.torchvision.models._utils.IntermediateLayerGetter = prim::GetAttr[name="backbone"](%self)
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.1 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %name.1 : str = prim::Constant[value="conv1"]()
  %name.4 : str = prim::Constant[value="bn1"]()
  %name.7 : str = prim::Constant[value="relu"]()
  %name.10 : str = prim::Constant[value="maxpool"]()
  %name.13 : str = prim::Constant[value="layer1"]()
  %name.16 : str = prim::Constant[value="layer2"]()
  %name.19 : str = prim::Constant[value="layer3"]()
  %name.22 : str = prim::Constant[value="layer4"]()
  %features.1 : Dict(str, Tensor) = aten::dict() # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:61:14
  %38 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv1"](%12)
  %39 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%12)
  %40 : __torch__.torch.nn.modules.container.___torch_mangle_16.Sequential = prim::GetAttr[name="layer1"](%12)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential = prim::GetAttr[name="layer2"](%12)
  %42 : __torch__.torch.nn.modules.container.___torch_mangle_38.Sequential = prim::GetAttr[name="layer3"](%12)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_49.Sequential = prim::GetAttr[name="layer4"](%12)
  %44 : Tensor = prim::GetAttr[name="weight"](%38)
  %45 : Tensor? = prim::GetAttr[name="bias"](%38)
  %46 : int[] = prim::ListConstruct(%26, %26)
  %47 : int[] = prim::ListConstruct(%28, %28)
  %48 : int[] = prim::ListConstruct(%27, %27)
  %x.3 : Tensor = aten::conv2d(%x.1, %44, %45, %46, %47, %48, %27) # torch/nn/modules/conv.py:415:15
  %50 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %51 : bool = aten::__contains__(%50, %name.1) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%51) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %52 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.1 : str = aten::__getitem__(%52, %name.1) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.1, %x.3) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %54 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %55 : bool = aten::ne(%54, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%55) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %56 : bool = prim::GetAttr[name="training"](%39)
   = prim::If(%56) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %57 : Tensor = prim::GetAttr[name="num_batches_tracked"](%39)
      %58 : Tensor = aten::add(%57, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%39, %58)
      -> ()
    block1():
      -> ()
  %59 : bool = prim::GetAttr[name="training"](%39)
  %60 : Tensor = prim::GetAttr[name="running_mean"](%39)
  %61 : Tensor = prim::GetAttr[name="running_var"](%39)
  %62 : Tensor = prim::GetAttr[name="weight"](%39)
  %63 : Tensor = prim::GetAttr[name="bias"](%39)
   = prim::If(%59) # torch/nn/functional.py:2011:4
    block0():
      %64 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%64, %24) # torch/nn/functional.py:1991:17
      %66 : int = aten::len(%64) # torch/nn/functional.py:1992:19
      %67 : int = aten::sub(%66, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%67, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %71 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %72 : int = aten::__getitem__(%64, %71) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %72) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %74 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%74) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.13 : Tensor = aten::batch_norm(%x.3, %62, %63, %60, %61, %59, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %76 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %77 : bool = aten::__contains__(%76, %name.4) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%77) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %78 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.3 : str = aten::__getitem__(%78, %name.4) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.3, %x.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %x.9 : Tensor = aten::relu_(%x.13) # torch/nn/functional.py:1117:17
  %81 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %82 : bool = aten::__contains__(%81, %name.7) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %83 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.5 : str = aten::__getitem__(%83, %name.7) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.5, %x.9) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %85 : int[] = prim::ListConstruct(%28, %28)
  %86 : int[] = prim::ListConstruct(%26, %26)
  %87 : int[] = prim::ListConstruct(%27, %27)
  %88 : int[] = prim::ListConstruct(%27, %27)
  %x.12 : Tensor = aten::max_pool2d(%x.9, %85, %86, %87, %88, %19) # torch/nn/functional.py:575:11
  %90 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %91 : bool = aten::__contains__(%90, %name.10) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%91) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %92 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.7 : str = aten::__getitem__(%92, %name.10) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.7, %x.12) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %94 : __torch__.torchvision.models.resnet.Bottleneck = prim::GetAttr[name="0"](%40)
  %95 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="1"](%40)
  %96 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="2"](%40)
  %97 : __torch__.torch.nn.modules.conv.___torch_mangle_9.Conv2d = prim::GetAttr[name="conv1"](%94)
  %98 : Tensor = prim::GetAttr[name="weight"](%97)
  %99 : Tensor? = prim::GetAttr[name="bias"](%97)
  %100 : int[] = prim::ListConstruct(%27, %27)
  %101 : int[] = prim::ListConstruct(%24, %24)
  %102 : int[] = prim::ListConstruct(%27, %27)
  %out.213 : Tensor = aten::conv2d(%x.12, %98, %99, %100, %101, %102, %27) # torch/nn/modules/conv.py:415:15
  %104 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%94)
  %105 : int = aten::dim(%out.213) # torch/nn/modules/batchnorm.py:276:11
  %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %107 : bool = prim::GetAttr[name="training"](%104)
   = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
      %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%104, %109)
      -> ()
    block1():
      -> ()
  %110 : bool = prim::GetAttr[name="training"](%104)
  %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
  %112 : Tensor = prim::GetAttr[name="running_var"](%104)
  %113 : Tensor = prim::GetAttr[name="weight"](%104)
  %114 : Tensor = prim::GetAttr[name="bias"](%104)
   = prim::If(%110) # torch/nn/functional.py:2011:4
    block0():
      %115 : int[] = aten::size(%out.213) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
      %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
      %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%118, %25, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %122 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
          %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %123) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.331)
      %125 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
       = prim::If(%125) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.214 : Tensor = aten::batch_norm(%out.213, %113, %114, %111, %112, %110, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.215 : Tensor = aten::relu_(%out.214) # torch/nn/functional.py:1117:17
  %128 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%94)
  %129 : Tensor = prim::GetAttr[name="weight"](%128)
  %130 : Tensor? = prim::GetAttr[name="bias"](%128)
  %131 : int[] = prim::ListConstruct(%27, %27)
  %132 : int[] = prim::ListConstruct(%27, %27)
  %133 : int[] = prim::ListConstruct(%27, %27)
  %out.216 : Tensor = aten::conv2d(%out.215, %129, %130, %131, %132, %133, %27) # torch/nn/modules/conv.py:415:15
  %135 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%94)
  %136 : int = aten::dim(%out.216) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%out.216) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%149, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %153 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %156 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.217 : Tensor = aten::batch_norm(%out.216, %144, %145, %142, %143, %141, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.209 : Tensor = aten::relu_(%out.217) # torch/nn/functional.py:1117:17
  %159 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%94)
  %160 : Tensor = prim::GetAttr[name="weight"](%159)
  %161 : Tensor? = prim::GetAttr[name="bias"](%159)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%24, %24)
  %164 : int[] = prim::ListConstruct(%27, %27)
  %out.210 : Tensor = aten::conv2d(%out.209, %160, %161, %162, %163, %164, %27) # torch/nn/modules/conv.py:415:15
  %166 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%94)
  %167 : int = aten::dim(%out.210) # torch/nn/modules/batchnorm.py:276:11
  %168 : bool = aten::ne(%167, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%168) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %169 : bool = prim::GetAttr[name="training"](%166)
   = prim::If(%169) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %170 : Tensor = prim::GetAttr[name="num_batches_tracked"](%166)
      %171 : Tensor = aten::add(%170, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%166, %171)
      -> ()
    block1():
      -> ()
  %172 : bool = prim::GetAttr[name="training"](%166)
  %173 : Tensor = prim::GetAttr[name="running_mean"](%166)
  %174 : Tensor = prim::GetAttr[name="running_var"](%166)
  %175 : Tensor = prim::GetAttr[name="weight"](%166)
  %176 : Tensor = prim::GetAttr[name="bias"](%166)
   = prim::If(%172) # torch/nn/functional.py:2011:4
    block0():
      %177 : int[] = aten::size(%out.210) # torch/nn/functional.py:2012:27
      %size_prods.304 : int = aten::__getitem__(%177, %24) # torch/nn/functional.py:1991:17
      %179 : int = aten::len(%177) # torch/nn/functional.py:1992:19
      %180 : int = aten::sub(%179, %26) # torch/nn/functional.py:1992:19
      %size_prods.305 : int = prim::Loop(%180, %25, %size_prods.304) # torch/nn/functional.py:1992:4
        block0(%i.77 : int, %size_prods.306 : int):
          %184 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
          %185 : int = aten::__getitem__(%177, %184) # torch/nn/functional.py:1993:22
          %size_prods.307 : int = aten::mul(%size_prods.306, %185) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.307)
      %187 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
       = prim::If(%187) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.211 : Tensor = aten::batch_norm(%out.210, %175, %176, %173, %174, %172, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %189 : __torch__.torch.nn.modules.container.___torch_mangle_13.Sequential = prim::GetAttr[name="downsample"](%94)
  %190 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="0"](%189)
  %191 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%189)
  %192 : Tensor = prim::GetAttr[name="weight"](%190)
  %193 : Tensor? = prim::GetAttr[name="bias"](%190)
  %194 : int[] = prim::ListConstruct(%27, %27)
  %195 : int[] = prim::ListConstruct(%24, %24)
  %196 : int[] = prim::ListConstruct(%27, %27)
  %input.28 : Tensor = aten::conv2d(%x.12, %192, %193, %194, %195, %196, %27) # torch/nn/modules/conv.py:415:15
  %198 : int = aten::dim(%input.28) # torch/nn/modules/batchnorm.py:276:11
  %199 : bool = aten::ne(%198, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%199) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %200 : bool = prim::GetAttr[name="training"](%191)
   = prim::If(%200) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %201 : Tensor = prim::GetAttr[name="num_batches_tracked"](%191)
      %202 : Tensor = aten::add(%201, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%191, %202)
      -> ()
    block1():
      -> ()
  %203 : bool = prim::GetAttr[name="training"](%191)
  %204 : Tensor = prim::GetAttr[name="running_mean"](%191)
  %205 : Tensor = prim::GetAttr[name="running_var"](%191)
  %206 : Tensor = prim::GetAttr[name="weight"](%191)
  %207 : Tensor = prim::GetAttr[name="bias"](%191)
   = prim::If(%203) # torch/nn/functional.py:2011:4
    block0():
      %208 : int[] = aten::size(%input.28) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%208, %24) # torch/nn/functional.py:1991:17
      %210 : int = aten::len(%208) # torch/nn/functional.py:1992:19
      %211 : int = aten::sub(%210, %26) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%211, %25, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %215 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
          %216 : int = aten::__getitem__(%208, %215) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %216) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.311)
      %218 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
       = prim::If(%218) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.2 : Tensor = aten::batch_norm(%input.28, %206, %207, %204, %205, %203, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.212 : Tensor = aten::add_(%out.211, %identity.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.34 : Tensor = aten::relu_(%out.212) # torch/nn/functional.py:1117:17
  %222 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%95)
  %223 : Tensor = prim::GetAttr[name="weight"](%222)
  %224 : Tensor? = prim::GetAttr[name="bias"](%222)
  %225 : int[] = prim::ListConstruct(%27, %27)
  %226 : int[] = prim::ListConstruct(%24, %24)
  %227 : int[] = prim::ListConstruct(%27, %27)
  %out.258 : Tensor = aten::conv2d(%input.34, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
  %229 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%95)
  %230 : int = aten::dim(%out.258) # torch/nn/modules/batchnorm.py:276:11
  %231 : bool = aten::ne(%230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %232 : bool = prim::GetAttr[name="training"](%229)
   = prim::If(%232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%229)
      %234 : Tensor = aten::add(%233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%229, %234)
      -> ()
    block1():
      -> ()
  %235 : bool = prim::GetAttr[name="training"](%229)
  %236 : Tensor = prim::GetAttr[name="running_mean"](%229)
  %237 : Tensor = prim::GetAttr[name="running_var"](%229)
  %238 : Tensor = prim::GetAttr[name="weight"](%229)
  %239 : Tensor = prim::GetAttr[name="bias"](%229)
   = prim::If(%235) # torch/nn/functional.py:2011:4
    block0():
      %240 : int[] = aten::size(%out.258) # torch/nn/functional.py:2012:27
      %size_prods.312 : int = aten::__getitem__(%240, %24) # torch/nn/functional.py:1991:17
      %242 : int = aten::len(%240) # torch/nn/functional.py:1992:19
      %243 : int = aten::sub(%242, %26) # torch/nn/functional.py:1992:19
      %size_prods.313 : int = prim::Loop(%243, %25, %size_prods.312) # torch/nn/functional.py:1992:4
        block0(%i.79 : int, %size_prods.314 : int):
          %247 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
          %248 : int = aten::__getitem__(%240, %247) # torch/nn/functional.py:1993:22
          %size_prods.315 : int = aten::mul(%size_prods.314, %248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.315)
      %250 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
       = prim::If(%250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.259 : Tensor = aten::batch_norm(%out.258, %238, %239, %236, %237, %235, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.260 : Tensor = aten::relu_(%out.259) # torch/nn/functional.py:1117:17
  %253 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%95)
  %254 : Tensor = prim::GetAttr[name="weight"](%253)
  %255 : Tensor? = prim::GetAttr[name="bias"](%253)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %out.261 : Tensor = aten::conv2d(%out.260, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%95)
  %261 : int = aten::dim(%out.261) # torch/nn/modules/batchnorm.py:276:11
  %262 : bool = aten::ne(%261, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%262) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %263 : bool = prim::GetAttr[name="training"](%260)
   = prim::If(%263) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %264 : Tensor = prim::GetAttr[name="num_batches_tracked"](%260)
      %265 : Tensor = aten::add(%264, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%260, %265)
      -> ()
    block1():
      -> ()
  %266 : bool = prim::GetAttr[name="training"](%260)
  %267 : Tensor = prim::GetAttr[name="running_mean"](%260)
  %268 : Tensor = prim::GetAttr[name="running_var"](%260)
  %269 : Tensor = prim::GetAttr[name="weight"](%260)
  %270 : Tensor = prim::GetAttr[name="bias"](%260)
   = prim::If(%266) # torch/nn/functional.py:2011:4
    block0():
      %271 : int[] = aten::size(%out.261) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%271, %24) # torch/nn/functional.py:1991:17
      %273 : int = aten::len(%271) # torch/nn/functional.py:1992:19
      %274 : int = aten::sub(%273, %26) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%274, %25, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %278 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
          %279 : int = aten::__getitem__(%271, %278) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %279) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.319)
      %281 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
       = prim::If(%281) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.262 : Tensor = aten::batch_norm(%out.261, %269, %270, %267, %268, %266, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.254 : Tensor = aten::relu_(%out.262) # torch/nn/functional.py:1117:17
  %284 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%95)
  %285 : Tensor = prim::GetAttr[name="weight"](%284)
  %286 : Tensor? = prim::GetAttr[name="bias"](%284)
  %287 : int[] = prim::ListConstruct(%27, %27)
  %288 : int[] = prim::ListConstruct(%24, %24)
  %289 : int[] = prim::ListConstruct(%27, %27)
  %out.255 : Tensor = aten::conv2d(%out.254, %285, %286, %287, %288, %289, %27) # torch/nn/modules/conv.py:415:15
  %291 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%95)
  %292 : int = aten::dim(%out.255) # torch/nn/modules/batchnorm.py:276:11
  %293 : bool = aten::ne(%292, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%293) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %294 : bool = prim::GetAttr[name="training"](%291)
   = prim::If(%294) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %295 : Tensor = prim::GetAttr[name="num_batches_tracked"](%291)
      %296 : Tensor = aten::add(%295, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%291, %296)
      -> ()
    block1():
      -> ()
  %297 : bool = prim::GetAttr[name="training"](%291)
  %298 : Tensor = prim::GetAttr[name="running_mean"](%291)
  %299 : Tensor = prim::GetAttr[name="running_var"](%291)
  %300 : Tensor = prim::GetAttr[name="weight"](%291)
  %301 : Tensor = prim::GetAttr[name="bias"](%291)
   = prim::If(%297) # torch/nn/functional.py:2011:4
    block0():
      %302 : int[] = aten::size(%out.255) # torch/nn/functional.py:2012:27
      %size_prods.320 : int = aten::__getitem__(%302, %24) # torch/nn/functional.py:1991:17
      %304 : int = aten::len(%302) # torch/nn/functional.py:1992:19
      %305 : int = aten::sub(%304, %26) # torch/nn/functional.py:1992:19
      %size_prods.321 : int = prim::Loop(%305, %25, %size_prods.320) # torch/nn/functional.py:1992:4
        block0(%i.81 : int, %size_prods.322 : int):
          %309 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
          %310 : int = aten::__getitem__(%302, %309) # torch/nn/functional.py:1993:22
          %size_prods.323 : int = aten::mul(%size_prods.322, %310) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.323)
      %312 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
       = prim::If(%312) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.256 : Tensor = aten::batch_norm(%out.255, %300, %301, %298, %299, %297, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.257 : Tensor = aten::add_(%out.256, %input.34, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.30 : Tensor = aten::relu_(%out.257) # torch/nn/functional.py:1117:17
  %316 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%96)
  %317 : Tensor = prim::GetAttr[name="weight"](%316)
  %318 : Tensor? = prim::GetAttr[name="bias"](%316)
  %319 : int[] = prim::ListConstruct(%27, %27)
  %320 : int[] = prim::ListConstruct(%24, %24)
  %321 : int[] = prim::ListConstruct(%27, %27)
  %out.221 : Tensor = aten::conv2d(%input.30, %317, %318, %319, %320, %321, %27) # torch/nn/modules/conv.py:415:15
  %323 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%96)
  %324 : int = aten::dim(%out.221) # torch/nn/modules/batchnorm.py:276:11
  %325 : bool = aten::ne(%324, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%325) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %326 : bool = prim::GetAttr[name="training"](%323)
   = prim::If(%326) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %327 : Tensor = prim::GetAttr[name="num_batches_tracked"](%323)
      %328 : Tensor = aten::add(%327, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%323, %328)
      -> ()
    block1():
      -> ()
  %329 : bool = prim::GetAttr[name="training"](%323)
  %330 : Tensor = prim::GetAttr[name="running_mean"](%323)
  %331 : Tensor = prim::GetAttr[name="running_var"](%323)
  %332 : Tensor = prim::GetAttr[name="weight"](%323)
  %333 : Tensor = prim::GetAttr[name="bias"](%323)
   = prim::If(%329) # torch/nn/functional.py:2011:4
    block0():
      %334 : int[] = aten::size(%out.221) # torch/nn/functional.py:2012:27
      %size_prods.336 : int = aten::__getitem__(%334, %24) # torch/nn/functional.py:1991:17
      %336 : int = aten::len(%334) # torch/nn/functional.py:1992:19
      %337 : int = aten::sub(%336, %26) # torch/nn/functional.py:1992:19
      %size_prods.337 : int = prim::Loop(%337, %25, %size_prods.336) # torch/nn/functional.py:1992:4
        block0(%i.85 : int, %size_prods.338 : int):
          %341 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
          %342 : int = aten::__getitem__(%334, %341) # torch/nn/functional.py:1993:22
          %size_prods.339 : int = aten::mul(%size_prods.338, %342) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.339)
      %344 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
       = prim::If(%344) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.222 : Tensor = aten::batch_norm(%out.221, %332, %333, %330, %331, %329, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.223 : Tensor = aten::relu_(%out.222) # torch/nn/functional.py:1117:17
  %347 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%96)
  %348 : Tensor = prim::GetAttr[name="weight"](%347)
  %349 : Tensor? = prim::GetAttr[name="bias"](%347)
  %350 : int[] = prim::ListConstruct(%27, %27)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %out.224 : Tensor = aten::conv2d(%out.223, %348, %349, %350, %351, %352, %27) # torch/nn/modules/conv.py:415:15
  %354 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%96)
  %355 : int = aten::dim(%out.224) # torch/nn/modules/batchnorm.py:276:11
  %356 : bool = aten::ne(%355, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%356) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %357 : bool = prim::GetAttr[name="training"](%354)
   = prim::If(%357) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %358 : Tensor = prim::GetAttr[name="num_batches_tracked"](%354)
      %359 : Tensor = aten::add(%358, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%354, %359)
      -> ()
    block1():
      -> ()
  %360 : bool = prim::GetAttr[name="training"](%354)
  %361 : Tensor = prim::GetAttr[name="running_mean"](%354)
  %362 : Tensor = prim::GetAttr[name="running_var"](%354)
  %363 : Tensor = prim::GetAttr[name="weight"](%354)
  %364 : Tensor = prim::GetAttr[name="bias"](%354)
   = prim::If(%360) # torch/nn/functional.py:2011:4
    block0():
      %365 : int[] = aten::size(%out.224) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%365, %24) # torch/nn/functional.py:1991:17
      %367 : int = aten::len(%365) # torch/nn/functional.py:1992:19
      %368 : int = aten::sub(%367, %26) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%368, %25, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %372 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
          %373 : int = aten::__getitem__(%365, %372) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %373) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.343)
      %375 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
       = prim::If(%375) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.225 : Tensor = aten::batch_norm(%out.224, %363, %364, %361, %362, %360, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.226 : Tensor = aten::relu_(%out.225) # torch/nn/functional.py:1117:17
  %378 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%96)
  %379 : Tensor = prim::GetAttr[name="weight"](%378)
  %380 : Tensor? = prim::GetAttr[name="bias"](%378)
  %381 : int[] = prim::ListConstruct(%27, %27)
  %382 : int[] = prim::ListConstruct(%24, %24)
  %383 : int[] = prim::ListConstruct(%27, %27)
  %out.218 : Tensor = aten::conv2d(%out.226, %379, %380, %381, %382, %383, %27) # torch/nn/modules/conv.py:415:15
  %385 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%96)
  %386 : int = aten::dim(%out.218) # torch/nn/modules/batchnorm.py:276:11
  %387 : bool = aten::ne(%386, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%387) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %388 : bool = prim::GetAttr[name="training"](%385)
   = prim::If(%388) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %389 : Tensor = prim::GetAttr[name="num_batches_tracked"](%385)
      %390 : Tensor = aten::add(%389, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%385, %390)
      -> ()
    block1():
      -> ()
  %391 : bool = prim::GetAttr[name="training"](%385)
  %392 : Tensor = prim::GetAttr[name="running_mean"](%385)
  %393 : Tensor = prim::GetAttr[name="running_var"](%385)
  %394 : Tensor = prim::GetAttr[name="weight"](%385)
  %395 : Tensor = prim::GetAttr[name="bias"](%385)
   = prim::If(%391) # torch/nn/functional.py:2011:4
    block0():
      %396 : int[] = aten::size(%out.218) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%396, %24) # torch/nn/functional.py:1991:17
      %398 : int = aten::len(%396) # torch/nn/functional.py:1992:19
      %399 : int = aten::sub(%398, %26) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%399, %25, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %403 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
          %404 : int = aten::__getitem__(%396, %403) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %404) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.347)
      %406 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
       = prim::If(%406) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.219 : Tensor = aten::batch_norm(%out.218, %394, %395, %392, %393, %391, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.220 : Tensor = aten::add_(%out.219, %input.30, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.15 : Tensor = aten::relu_(%out.220) # torch/nn/functional.py:1117:17
  %410 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %411 : bool = aten::__contains__(%410, %name.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%411) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %412 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.9 : str = aten::__getitem__(%412, %name.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.9, %x.15) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %414 : __torch__.torchvision.models.resnet.___torch_mangle_24.Bottleneck = prim::GetAttr[name="0"](%41)
  %415 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="1"](%41)
  %416 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="2"](%41)
  %417 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="3"](%41)
  %418 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%414)
  %419 : Tensor = prim::GetAttr[name="weight"](%418)
  %420 : Tensor? = prim::GetAttr[name="bias"](%418)
  %421 : int[] = prim::ListConstruct(%27, %27)
  %422 : int[] = prim::ListConstruct(%24, %24)
  %423 : int[] = prim::ListConstruct(%27, %27)
  %out.230 : Tensor = aten::conv2d(%x.15, %419, %420, %421, %422, %423, %27) # torch/nn/modules/conv.py:415:15
  %425 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%414)
  %426 : int = aten::dim(%out.230) # torch/nn/modules/batchnorm.py:276:11
  %427 : bool = aten::ne(%426, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%427) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %428 : bool = prim::GetAttr[name="training"](%425)
   = prim::If(%428) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %429 : Tensor = prim::GetAttr[name="num_batches_tracked"](%425)
      %430 : Tensor = aten::add(%429, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%425, %430)
      -> ()
    block1():
      -> ()
  %431 : bool = prim::GetAttr[name="training"](%425)
  %432 : Tensor = prim::GetAttr[name="running_mean"](%425)
  %433 : Tensor = prim::GetAttr[name="running_var"](%425)
  %434 : Tensor = prim::GetAttr[name="weight"](%425)
  %435 : Tensor = prim::GetAttr[name="bias"](%425)
   = prim::If(%431) # torch/nn/functional.py:2011:4
    block0():
      %436 : int[] = aten::size(%out.230) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%436, %24) # torch/nn/functional.py:1991:17
      %438 : int = aten::len(%436) # torch/nn/functional.py:1992:19
      %439 : int = aten::sub(%438, %26) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%439, %25, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %443 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
          %444 : int = aten::__getitem__(%436, %443) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %444) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.351)
      %446 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
       = prim::If(%446) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.231 : Tensor = aten::batch_norm(%out.230, %434, %435, %432, %433, %431, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.232 : Tensor = aten::relu_(%out.231) # torch/nn/functional.py:1117:17
  %449 : __torch__.torch.nn.modules.conv.___torch_mangle_19.Conv2d = prim::GetAttr[name="conv2"](%414)
  %450 : Tensor = prim::GetAttr[name="weight"](%449)
  %451 : Tensor? = prim::GetAttr[name="bias"](%449)
  %452 : int[] = prim::ListConstruct(%26, %26)
  %453 : int[] = prim::ListConstruct(%27, %27)
  %454 : int[] = prim::ListConstruct(%27, %27)
  %out.233 : Tensor = aten::conv2d(%out.232, %450, %451, %452, %453, %454, %27) # torch/nn/modules/conv.py:415:15
  %456 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%414)
  %457 : int = aten::dim(%out.233) # torch/nn/modules/batchnorm.py:276:11
  %458 : bool = aten::ne(%457, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%458) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %459 : bool = prim::GetAttr[name="training"](%456)
   = prim::If(%459) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %460 : Tensor = prim::GetAttr[name="num_batches_tracked"](%456)
      %461 : Tensor = aten::add(%460, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%456, %461)
      -> ()
    block1():
      -> ()
  %462 : bool = prim::GetAttr[name="training"](%456)
  %463 : Tensor = prim::GetAttr[name="running_mean"](%456)
  %464 : Tensor = prim::GetAttr[name="running_var"](%456)
  %465 : Tensor = prim::GetAttr[name="weight"](%456)
  %466 : Tensor = prim::GetAttr[name="bias"](%456)
   = prim::If(%462) # torch/nn/functional.py:2011:4
    block0():
      %467 : int[] = aten::size(%out.233) # torch/nn/functional.py:2012:27
      %size_prods.352 : int = aten::__getitem__(%467, %24) # torch/nn/functional.py:1991:17
      %469 : int = aten::len(%467) # torch/nn/functional.py:1992:19
      %470 : int = aten::sub(%469, %26) # torch/nn/functional.py:1992:19
      %size_prods.353 : int = prim::Loop(%470, %25, %size_prods.352) # torch/nn/functional.py:1992:4
        block0(%i.89 : int, %size_prods.354 : int):
          %474 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
          %475 : int = aten::__getitem__(%467, %474) # torch/nn/functional.py:1993:22
          %size_prods.355 : int = aten::mul(%size_prods.354, %475) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.355)
      %477 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
       = prim::If(%477) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.234 : Tensor = aten::batch_norm(%out.233, %465, %466, %463, %464, %462, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.235 : Tensor = aten::relu_(%out.234) # torch/nn/functional.py:1117:17
  %480 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%414)
  %481 : Tensor = prim::GetAttr[name="weight"](%480)
  %482 : Tensor? = prim::GetAttr[name="bias"](%480)
  %483 : int[] = prim::ListConstruct(%27, %27)
  %484 : int[] = prim::ListConstruct(%24, %24)
  %485 : int[] = prim::ListConstruct(%27, %27)
  %out.227 : Tensor = aten::conv2d(%out.235, %481, %482, %483, %484, %485, %27) # torch/nn/modules/conv.py:415:15
  %487 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%414)
  %488 : int = aten::dim(%out.227) # torch/nn/modules/batchnorm.py:276:11
  %489 : bool = aten::ne(%488, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%489) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %490 : bool = prim::GetAttr[name="training"](%487)
   = prim::If(%490) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %491 : Tensor = prim::GetAttr[name="num_batches_tracked"](%487)
      %492 : Tensor = aten::add(%491, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%487, %492)
      -> ()
    block1():
      -> ()
  %493 : bool = prim::GetAttr[name="training"](%487)
  %494 : Tensor = prim::GetAttr[name="running_mean"](%487)
  %495 : Tensor = prim::GetAttr[name="running_var"](%487)
  %496 : Tensor = prim::GetAttr[name="weight"](%487)
  %497 : Tensor = prim::GetAttr[name="bias"](%487)
   = prim::If(%493) # torch/nn/functional.py:2011:4
    block0():
      %498 : int[] = aten::size(%out.227) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%498, %24) # torch/nn/functional.py:1991:17
      %500 : int = aten::len(%498) # torch/nn/functional.py:1992:19
      %501 : int = aten::sub(%500, %26) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%501, %25, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %505 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
          %506 : int = aten::__getitem__(%498, %505) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %506) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.359)
      %508 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
       = prim::If(%508) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.228 : Tensor = aten::batch_norm(%out.227, %496, %497, %494, %495, %493, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %510 : __torch__.torch.nn.modules.container.___torch_mangle_23.Sequential = prim::GetAttr[name="downsample"](%414)
  %511 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="0"](%510)
  %512 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="1"](%510)
  %513 : Tensor = prim::GetAttr[name="weight"](%511)
  %514 : Tensor? = prim::GetAttr[name="bias"](%511)
  %515 : int[] = prim::ListConstruct(%26, %26)
  %516 : int[] = prim::ListConstruct(%24, %24)
  %517 : int[] = prim::ListConstruct(%27, %27)
  %input.42 : Tensor = aten::conv2d(%x.15, %513, %514, %515, %516, %517, %27) # torch/nn/modules/conv.py:415:15
  %519 : int = aten::dim(%input.42) # torch/nn/modules/batchnorm.py:276:11
  %520 : bool = aten::ne(%519, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%520) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%512)
   = prim::If(%521) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %522 : Tensor = prim::GetAttr[name="num_batches_tracked"](%512)
      %523 : Tensor = aten::add(%522, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%512, %523)
      -> ()
    block1():
      -> ()
  %524 : bool = prim::GetAttr[name="training"](%512)
  %525 : Tensor = prim::GetAttr[name="running_mean"](%512)
  %526 : Tensor = prim::GetAttr[name="running_var"](%512)
  %527 : Tensor = prim::GetAttr[name="weight"](%512)
  %528 : Tensor = prim::GetAttr[name="bias"](%512)
   = prim::If(%524) # torch/nn/functional.py:2011:4
    block0():
      %529 : int[] = aten::size(%input.42) # torch/nn/functional.py:2012:27
      %size_prods.360 : int = aten::__getitem__(%529, %24) # torch/nn/functional.py:1991:17
      %531 : int = aten::len(%529) # torch/nn/functional.py:1992:19
      %532 : int = aten::sub(%531, %26) # torch/nn/functional.py:1992:19
      %size_prods.361 : int = prim::Loop(%532, %25, %size_prods.360) # torch/nn/functional.py:1992:4
        block0(%i.91 : int, %size_prods.362 : int):
          %536 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
          %537 : int = aten::__getitem__(%529, %536) # torch/nn/functional.py:1993:22
          %size_prods.363 : int = aten::mul(%size_prods.362, %537) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.363)
      %539 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
       = prim::If(%539) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.3 : Tensor = aten::batch_norm(%input.42, %527, %528, %525, %526, %524, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.229 : Tensor = aten::add_(%out.228, %identity.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.32 : Tensor = aten::relu_(%out.229) # torch/nn/functional.py:1117:17
  %543 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%415)
  %544 : Tensor = prim::GetAttr[name="weight"](%543)
  %545 : Tensor? = prim::GetAttr[name="bias"](%543)
  %546 : int[] = prim::ListConstruct(%27, %27)
  %547 : int[] = prim::ListConstruct(%24, %24)
  %548 : int[] = prim::ListConstruct(%27, %27)
  %out.239 : Tensor = aten::conv2d(%input.32, %544, %545, %546, %547, %548, %27) # torch/nn/modules/conv.py:415:15
  %550 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%415)
  %551 : int = aten::dim(%out.239) # torch/nn/modules/batchnorm.py:276:11
  %552 : bool = aten::ne(%551, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%552) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %553 : bool = prim::GetAttr[name="training"](%550)
   = prim::If(%553) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %554 : Tensor = prim::GetAttr[name="num_batches_tracked"](%550)
      %555 : Tensor = aten::add(%554, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%550, %555)
      -> ()
    block1():
      -> ()
  %556 : bool = prim::GetAttr[name="training"](%550)
  %557 : Tensor = prim::GetAttr[name="running_mean"](%550)
  %558 : Tensor = prim::GetAttr[name="running_var"](%550)
  %559 : Tensor = prim::GetAttr[name="weight"](%550)
  %560 : Tensor = prim::GetAttr[name="bias"](%550)
   = prim::If(%556) # torch/nn/functional.py:2011:4
    block0():
      %561 : int[] = aten::size(%out.239) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%561, %24) # torch/nn/functional.py:1991:17
      %563 : int = aten::len(%561) # torch/nn/functional.py:1992:19
      %564 : int = aten::sub(%563, %26) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%564, %25, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %568 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
          %569 : int = aten::__getitem__(%561, %568) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %569) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.283)
      %571 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
       = prim::If(%571) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.240 : Tensor = aten::batch_norm(%out.239, %559, %560, %557, %558, %556, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.241 : Tensor = aten::relu_(%out.240) # torch/nn/functional.py:1117:17
  %574 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%415)
  %575 : Tensor = prim::GetAttr[name="weight"](%574)
  %576 : Tensor? = prim::GetAttr[name="bias"](%574)
  %577 : int[] = prim::ListConstruct(%27, %27)
  %578 : int[] = prim::ListConstruct(%27, %27)
  %579 : int[] = prim::ListConstruct(%27, %27)
  %out.242 : Tensor = aten::conv2d(%out.241, %575, %576, %577, %578, %579, %27) # torch/nn/modules/conv.py:415:15
  %581 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%415)
  %582 : int = aten::dim(%out.242) # torch/nn/modules/batchnorm.py:276:11
  %583 : bool = aten::ne(%582, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%583) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %584 : bool = prim::GetAttr[name="training"](%581)
   = prim::If(%584) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %585 : Tensor = prim::GetAttr[name="num_batches_tracked"](%581)
      %586 : Tensor = aten::add(%585, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%581, %586)
      -> ()
    block1():
      -> ()
  %587 : bool = prim::GetAttr[name="training"](%581)
  %588 : Tensor = prim::GetAttr[name="running_mean"](%581)
  %589 : Tensor = prim::GetAttr[name="running_var"](%581)
  %590 : Tensor = prim::GetAttr[name="weight"](%581)
  %591 : Tensor = prim::GetAttr[name="bias"](%581)
   = prim::If(%587) # torch/nn/functional.py:2011:4
    block0():
      %592 : int[] = aten::size(%out.242) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%592, %24) # torch/nn/functional.py:1991:17
      %594 : int = aten::len(%592) # torch/nn/functional.py:1992:19
      %595 : int = aten::sub(%594, %26) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%595, %25, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %599 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
          %600 : int = aten::__getitem__(%592, %599) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %600) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.287)
      %602 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
       = prim::If(%602) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.243 : Tensor = aten::batch_norm(%out.242, %590, %591, %588, %589, %587, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.244 : Tensor = aten::relu_(%out.243) # torch/nn/functional.py:1117:17
  %605 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%415)
  %606 : Tensor = prim::GetAttr[name="weight"](%605)
  %607 : Tensor? = prim::GetAttr[name="bias"](%605)
  %608 : int[] = prim::ListConstruct(%27, %27)
  %609 : int[] = prim::ListConstruct(%24, %24)
  %610 : int[] = prim::ListConstruct(%27, %27)
  %out.236 : Tensor = aten::conv2d(%out.244, %606, %607, %608, %609, %610, %27) # torch/nn/modules/conv.py:415:15
  %612 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%415)
  %613 : int = aten::dim(%out.236) # torch/nn/modules/batchnorm.py:276:11
  %614 : bool = aten::ne(%613, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%614) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %615 : bool = prim::GetAttr[name="training"](%612)
   = prim::If(%615) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %616 : Tensor = prim::GetAttr[name="num_batches_tracked"](%612)
      %617 : Tensor = aten::add(%616, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%612, %617)
      -> ()
    block1():
      -> ()
  %618 : bool = prim::GetAttr[name="training"](%612)
  %619 : Tensor = prim::GetAttr[name="running_mean"](%612)
  %620 : Tensor = prim::GetAttr[name="running_var"](%612)
  %621 : Tensor = prim::GetAttr[name="weight"](%612)
  %622 : Tensor = prim::GetAttr[name="bias"](%612)
   = prim::If(%618) # torch/nn/functional.py:2011:4
    block0():
      %623 : int[] = aten::size(%out.236) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%623, %24) # torch/nn/functional.py:1991:17
      %625 : int = aten::len(%623) # torch/nn/functional.py:1992:19
      %626 : int = aten::sub(%625, %26) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%626, %25, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %630 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
          %631 : int = aten::__getitem__(%623, %630) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %631) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.291)
      %633 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
       = prim::If(%633) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.237 : Tensor = aten::batch_norm(%out.236, %621, %622, %619, %620, %618, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.238 : Tensor = aten::add_(%out.237, %input.32, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.38 : Tensor = aten::relu_(%out.238) # torch/nn/functional.py:1117:17
  %637 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%416)
  %638 : Tensor = prim::GetAttr[name="weight"](%637)
  %639 : Tensor? = prim::GetAttr[name="bias"](%637)
  %640 : int[] = prim::ListConstruct(%27, %27)
  %641 : int[] = prim::ListConstruct(%24, %24)
  %642 : int[] = prim::ListConstruct(%27, %27)
  %out.248 : Tensor = aten::conv2d(%input.38, %638, %639, %640, %641, %642, %27) # torch/nn/modules/conv.py:415:15
  %644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%416)
  %645 : int = aten::dim(%out.248) # torch/nn/modules/batchnorm.py:276:11
  %646 : bool = aten::ne(%645, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%646) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %647 : bool = prim::GetAttr[name="training"](%644)
   = prim::If(%647) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %648 : Tensor = prim::GetAttr[name="num_batches_tracked"](%644)
      %649 : Tensor = aten::add(%648, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%644, %649)
      -> ()
    block1():
      -> ()
  %650 : bool = prim::GetAttr[name="training"](%644)
  %651 : Tensor = prim::GetAttr[name="running_mean"](%644)
  %652 : Tensor = prim::GetAttr[name="running_var"](%644)
  %653 : Tensor = prim::GetAttr[name="weight"](%644)
  %654 : Tensor = prim::GetAttr[name="bias"](%644)
   = prim::If(%650) # torch/nn/functional.py:2011:4
    block0():
      %655 : int[] = aten::size(%out.248) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%655, %24) # torch/nn/functional.py:1991:17
      %657 : int = aten::len(%655) # torch/nn/functional.py:1992:19
      %658 : int = aten::sub(%657, %26) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%658, %25, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %662 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
          %663 : int = aten::__getitem__(%655, %662) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %663) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.295)
      %665 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
       = prim::If(%665) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.249 : Tensor = aten::batch_norm(%out.248, %653, %654, %651, %652, %650, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.250 : Tensor = aten::relu_(%out.249) # torch/nn/functional.py:1117:17
  %668 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%416)
  %669 : Tensor = prim::GetAttr[name="weight"](%668)
  %670 : Tensor? = prim::GetAttr[name="bias"](%668)
  %671 : int[] = prim::ListConstruct(%27, %27)
  %672 : int[] = prim::ListConstruct(%27, %27)
  %673 : int[] = prim::ListConstruct(%27, %27)
  %out.251 : Tensor = aten::conv2d(%out.250, %669, %670, %671, %672, %673, %27) # torch/nn/modules/conv.py:415:15
  %675 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%416)
  %676 : int = aten::dim(%out.251) # torch/nn/modules/batchnorm.py:276:11
  %677 : bool = aten::ne(%676, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%677) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %678 : bool = prim::GetAttr[name="training"](%675)
   = prim::If(%678) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %679 : Tensor = prim::GetAttr[name="num_batches_tracked"](%675)
      %680 : Tensor = aten::add(%679, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%675, %680)
      -> ()
    block1():
      -> ()
  %681 : bool = prim::GetAttr[name="training"](%675)
  %682 : Tensor = prim::GetAttr[name="running_mean"](%675)
  %683 : Tensor = prim::GetAttr[name="running_var"](%675)
  %684 : Tensor = prim::GetAttr[name="weight"](%675)
  %685 : Tensor = prim::GetAttr[name="bias"](%675)
   = prim::If(%681) # torch/nn/functional.py:2011:4
    block0():
      %686 : int[] = aten::size(%out.251) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%686, %24) # torch/nn/functional.py:1991:17
      %688 : int = aten::len(%686) # torch/nn/functional.py:1992:19
      %689 : int = aten::sub(%688, %26) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%689, %25, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %693 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
          %694 : int = aten::__getitem__(%686, %693) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %694) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.299)
      %696 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
       = prim::If(%696) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.252 : Tensor = aten::batch_norm(%out.251, %684, %685, %682, %683, %681, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.253 : Tensor = aten::relu_(%out.252) # torch/nn/functional.py:1117:17
  %699 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%416)
  %700 : Tensor = prim::GetAttr[name="weight"](%699)
  %701 : Tensor? = prim::GetAttr[name="bias"](%699)
  %702 : int[] = prim::ListConstruct(%27, %27)
  %703 : int[] = prim::ListConstruct(%24, %24)
  %704 : int[] = prim::ListConstruct(%27, %27)
  %out.245 : Tensor = aten::conv2d(%out.253, %700, %701, %702, %703, %704, %27) # torch/nn/modules/conv.py:415:15
  %706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%416)
  %707 : int = aten::dim(%out.245) # torch/nn/modules/batchnorm.py:276:11
  %708 : bool = aten::ne(%707, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%708) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %709 : bool = prim::GetAttr[name="training"](%706)
   = prim::If(%709) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %710 : Tensor = prim::GetAttr[name="num_batches_tracked"](%706)
      %711 : Tensor = aten::add(%710, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%706, %711)
      -> ()
    block1():
      -> ()
  %712 : bool = prim::GetAttr[name="training"](%706)
  %713 : Tensor = prim::GetAttr[name="running_mean"](%706)
  %714 : Tensor = prim::GetAttr[name="running_var"](%706)
  %715 : Tensor = prim::GetAttr[name="weight"](%706)
  %716 : Tensor = prim::GetAttr[name="bias"](%706)
   = prim::If(%712) # torch/nn/functional.py:2011:4
    block0():
      %717 : int[] = aten::size(%out.245) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%717, %24) # torch/nn/functional.py:1991:17
      %719 : int = aten::len(%717) # torch/nn/functional.py:1992:19
      %720 : int = aten::sub(%719, %26) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%720, %25, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %724 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
          %725 : int = aten::__getitem__(%717, %724) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %725) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.303)
      %727 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
       = prim::If(%727) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.246 : Tensor = aten::batch_norm(%out.245, %715, %716, %713, %714, %712, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.247 : Tensor = aten::add_(%out.246, %input.38, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.46 : Tensor = aten::relu_(%out.247) # torch/nn/functional.py:1117:17
  %731 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%417)
  %732 : Tensor = prim::GetAttr[name="weight"](%731)
  %733 : Tensor? = prim::GetAttr[name="bias"](%731)
  %734 : int[] = prim::ListConstruct(%27, %27)
  %735 : int[] = prim::ListConstruct(%24, %24)
  %736 : int[] = prim::ListConstruct(%27, %27)
  %out.263 : Tensor = aten::conv2d(%input.46, %732, %733, %734, %735, %736, %27) # torch/nn/modules/conv.py:415:15
  %738 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%417)
  %739 : int = aten::dim(%out.263) # torch/nn/modules/batchnorm.py:276:11
  %740 : bool = aten::ne(%739, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%740) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %741 : bool = prim::GetAttr[name="training"](%738)
   = prim::If(%741) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %742 : Tensor = prim::GetAttr[name="num_batches_tracked"](%738)
      %743 : Tensor = aten::add(%742, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%738, %743)
      -> ()
    block1():
      -> ()
  %744 : bool = prim::GetAttr[name="training"](%738)
  %745 : Tensor = prim::GetAttr[name="running_mean"](%738)
  %746 : Tensor = prim::GetAttr[name="running_var"](%738)
  %747 : Tensor = prim::GetAttr[name="weight"](%738)
  %748 : Tensor = prim::GetAttr[name="bias"](%738)
   = prim::If(%744) # torch/nn/functional.py:2011:4
    block0():
      %749 : int[] = aten::size(%out.263) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%749, %24) # torch/nn/functional.py:1991:17
      %751 : int = aten::len(%749) # torch/nn/functional.py:1992:19
      %752 : int = aten::sub(%751, %26) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%752, %25, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %756 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
          %757 : int = aten::__getitem__(%749, %756) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %757) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.367)
      %759 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
       = prim::If(%759) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.264 : Tensor = aten::batch_norm(%out.263, %747, %748, %745, %746, %744, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.265 : Tensor = aten::relu_(%out.264) # torch/nn/functional.py:1117:17
  %762 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%417)
  %763 : Tensor = prim::GetAttr[name="weight"](%762)
  %764 : Tensor? = prim::GetAttr[name="bias"](%762)
  %765 : int[] = prim::ListConstruct(%27, %27)
  %766 : int[] = prim::ListConstruct(%27, %27)
  %767 : int[] = prim::ListConstruct(%27, %27)
  %out.266 : Tensor = aten::conv2d(%out.265, %763, %764, %765, %766, %767, %27) # torch/nn/modules/conv.py:415:15
  %769 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%417)
  %770 : int = aten::dim(%out.266) # torch/nn/modules/batchnorm.py:276:11
  %771 : bool = aten::ne(%770, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%771) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %772 : bool = prim::GetAttr[name="training"](%769)
   = prim::If(%772) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %773 : Tensor = prim::GetAttr[name="num_batches_tracked"](%769)
      %774 : Tensor = aten::add(%773, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%769, %774)
      -> ()
    block1():
      -> ()
  %775 : bool = prim::GetAttr[name="training"](%769)
  %776 : Tensor = prim::GetAttr[name="running_mean"](%769)
  %777 : Tensor = prim::GetAttr[name="running_var"](%769)
  %778 : Tensor = prim::GetAttr[name="weight"](%769)
  %779 : Tensor = prim::GetAttr[name="bias"](%769)
   = prim::If(%775) # torch/nn/functional.py:2011:4
    block0():
      %780 : int[] = aten::size(%out.266) # torch/nn/functional.py:2012:27
      %size_prods.368 : int = aten::__getitem__(%780, %24) # torch/nn/functional.py:1991:17
      %782 : int = aten::len(%780) # torch/nn/functional.py:1992:19
      %783 : int = aten::sub(%782, %26) # torch/nn/functional.py:1992:19
      %size_prods.369 : int = prim::Loop(%783, %25, %size_prods.368) # torch/nn/functional.py:1992:4
        block0(%i.93 : int, %size_prods.370 : int):
          %787 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
          %788 : int = aten::__getitem__(%780, %787) # torch/nn/functional.py:1993:22
          %size_prods.371 : int = aten::mul(%size_prods.370, %788) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.371)
      %790 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
       = prim::If(%790) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.267 : Tensor = aten::batch_norm(%out.266, %778, %779, %776, %777, %775, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.268 : Tensor = aten::relu_(%out.267) # torch/nn/functional.py:1117:17
  %793 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%417)
  %794 : Tensor = prim::GetAttr[name="weight"](%793)
  %795 : Tensor? = prim::GetAttr[name="bias"](%793)
  %796 : int[] = prim::ListConstruct(%27, %27)
  %797 : int[] = prim::ListConstruct(%24, %24)
  %798 : int[] = prim::ListConstruct(%27, %27)
  %out.269 : Tensor = aten::conv2d(%out.268, %794, %795, %796, %797, %798, %27) # torch/nn/modules/conv.py:415:15
  %800 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%417)
  %801 : int = aten::dim(%out.269) # torch/nn/modules/batchnorm.py:276:11
  %802 : bool = aten::ne(%801, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%802) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %803 : bool = prim::GetAttr[name="training"](%800)
   = prim::If(%803) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %804 : Tensor = prim::GetAttr[name="num_batches_tracked"](%800)
      %805 : Tensor = aten::add(%804, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%800, %805)
      -> ()
    block1():
      -> ()
  %806 : bool = prim::GetAttr[name="training"](%800)
  %807 : Tensor = prim::GetAttr[name="running_mean"](%800)
  %808 : Tensor = prim::GetAttr[name="running_var"](%800)
  %809 : Tensor = prim::GetAttr[name="weight"](%800)
  %810 : Tensor = prim::GetAttr[name="bias"](%800)
   = prim::If(%806) # torch/nn/functional.py:2011:4
    block0():
      %811 : int[] = aten::size(%out.269) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%811, %24) # torch/nn/functional.py:1991:17
      %813 : int = aten::len(%811) # torch/nn/functional.py:1992:19
      %814 : int = aten::sub(%813, %26) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%814, %25, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %818 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
          %819 : int = aten::__getitem__(%811, %818) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %819) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.375)
      %821 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
       = prim::If(%821) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.270 : Tensor = aten::batch_norm(%out.269, %809, %810, %807, %808, %806, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.271 : Tensor = aten::add_(%out.270, %input.46, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.18 : Tensor = aten::relu_(%out.271) # torch/nn/functional.py:1117:17
  %825 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %826 : bool = aten::__contains__(%825, %name.16) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%826) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %827 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.11 : str = aten::__getitem__(%827, %name.16) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.11, %x.18) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %829 : __torch__.torchvision.models.resnet.___torch_mangle_34.Bottleneck = prim::GetAttr[name="0"](%42)
  %830 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="1"](%42)
  %831 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="2"](%42)
  %832 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="3"](%42)
  %833 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="4"](%42)
  %834 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="5"](%42)
  %835 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="6"](%42)
  %836 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="7"](%42)
  %837 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="8"](%42)
  %838 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="9"](%42)
  %839 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="10"](%42)
  %840 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="11"](%42)
  %841 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="12"](%42)
  %842 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="13"](%42)
  %843 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="14"](%42)
  %844 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="15"](%42)
  %845 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="16"](%42)
  %846 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="17"](%42)
  %847 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="18"](%42)
  %848 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="19"](%42)
  %849 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="20"](%42)
  %850 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="21"](%42)
  %851 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="22"](%42)
  %852 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%829)
  %853 : Tensor = prim::GetAttr[name="weight"](%852)
  %854 : Tensor? = prim::GetAttr[name="bias"](%852)
  %855 : int[] = prim::ListConstruct(%27, %27)
  %856 : int[] = prim::ListConstruct(%24, %24)
  %857 : int[] = prim::ListConstruct(%27, %27)
  %out.272 : Tensor = aten::conv2d(%x.18, %853, %854, %855, %856, %857, %27) # torch/nn/modules/conv.py:415:15
  %859 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%829)
  %860 : int = aten::dim(%out.272) # torch/nn/modules/batchnorm.py:276:11
  %861 : bool = aten::ne(%860, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%861) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %862 : bool = prim::GetAttr[name="training"](%859)
   = prim::If(%862) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %863 : Tensor = prim::GetAttr[name="num_batches_tracked"](%859)
      %864 : Tensor = aten::add(%863, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%859, %864)
      -> ()
    block1():
      -> ()
  %865 : bool = prim::GetAttr[name="training"](%859)
  %866 : Tensor = prim::GetAttr[name="running_mean"](%859)
  %867 : Tensor = prim::GetAttr[name="running_var"](%859)
  %868 : Tensor = prim::GetAttr[name="weight"](%859)
  %869 : Tensor = prim::GetAttr[name="bias"](%859)
   = prim::If(%865) # torch/nn/functional.py:2011:4
    block0():
      %870 : int[] = aten::size(%out.272) # torch/nn/functional.py:2012:27
      %size_prods.376 : int = aten::__getitem__(%870, %24) # torch/nn/functional.py:1991:17
      %872 : int = aten::len(%870) # torch/nn/functional.py:1992:19
      %873 : int = aten::sub(%872, %26) # torch/nn/functional.py:1992:19
      %size_prods.377 : int = prim::Loop(%873, %25, %size_prods.376) # torch/nn/functional.py:1992:4
        block0(%i.95 : int, %size_prods.378 : int):
          %877 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
          %878 : int = aten::__getitem__(%870, %877) # torch/nn/functional.py:1993:22
          %size_prods.379 : int = aten::mul(%size_prods.378, %878) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.379)
      %880 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
       = prim::If(%880) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.273 : Tensor = aten::batch_norm(%out.272, %868, %869, %866, %867, %865, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.274 : Tensor = aten::relu_(%out.273) # torch/nn/functional.py:1117:17
  %883 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%829)
  %884 : Tensor = prim::GetAttr[name="weight"](%883)
  %885 : Tensor? = prim::GetAttr[name="bias"](%883)
  %886 : int[] = prim::ListConstruct(%27, %27)
  %887 : int[] = prim::ListConstruct(%27, %27)
  %888 : int[] = prim::ListConstruct(%27, %27)
  %out.275 : Tensor = aten::conv2d(%out.274, %884, %885, %886, %887, %888, %27) # torch/nn/modules/conv.py:415:15
  %890 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%829)
  %891 : int = aten::dim(%out.275) # torch/nn/modules/batchnorm.py:276:11
  %892 : bool = aten::ne(%891, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%892) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %893 : bool = prim::GetAttr[name="training"](%890)
   = prim::If(%893) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %894 : Tensor = prim::GetAttr[name="num_batches_tracked"](%890)
      %895 : Tensor = aten::add(%894, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%890, %895)
      -> ()
    block1():
      -> ()
  %896 : bool = prim::GetAttr[name="training"](%890)
  %897 : Tensor = prim::GetAttr[name="running_mean"](%890)
  %898 : Tensor = prim::GetAttr[name="running_var"](%890)
  %899 : Tensor = prim::GetAttr[name="weight"](%890)
  %900 : Tensor = prim::GetAttr[name="bias"](%890)
   = prim::If(%896) # torch/nn/functional.py:2011:4
    block0():
      %901 : int[] = aten::size(%out.275) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%901, %24) # torch/nn/functional.py:1991:17
      %903 : int = aten::len(%901) # torch/nn/functional.py:1992:19
      %904 : int = aten::sub(%903, %26) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%904, %25, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %908 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
          %909 : int = aten::__getitem__(%901, %908) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %909) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.383)
      %911 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
       = prim::If(%911) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.276 : Tensor = aten::batch_norm(%out.275, %899, %900, %897, %898, %896, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.277 : Tensor = aten::relu_(%out.276) # torch/nn/functional.py:1117:17
  %914 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%829)
  %915 : Tensor = prim::GetAttr[name="weight"](%914)
  %916 : Tensor? = prim::GetAttr[name="bias"](%914)
  %917 : int[] = prim::ListConstruct(%27, %27)
  %918 : int[] = prim::ListConstruct(%24, %24)
  %919 : int[] = prim::ListConstruct(%27, %27)
  %out.278 : Tensor = aten::conv2d(%out.277, %915, %916, %917, %918, %919, %27) # torch/nn/modules/conv.py:415:15
  %921 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%829)
  %922 : int = aten::dim(%out.278) # torch/nn/modules/batchnorm.py:276:11
  %923 : bool = aten::ne(%922, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%923) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %924 : bool = prim::GetAttr[name="training"](%921)
   = prim::If(%924) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %925 : Tensor = prim::GetAttr[name="num_batches_tracked"](%921)
      %926 : Tensor = aten::add(%925, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%921, %926)
      -> ()
    block1():
      -> ()
  %927 : bool = prim::GetAttr[name="training"](%921)
  %928 : Tensor = prim::GetAttr[name="running_mean"](%921)
  %929 : Tensor = prim::GetAttr[name="running_var"](%921)
  %930 : Tensor = prim::GetAttr[name="weight"](%921)
  %931 : Tensor = prim::GetAttr[name="bias"](%921)
   = prim::If(%927) # torch/nn/functional.py:2011:4
    block0():
      %932 : int[] = aten::size(%out.278) # torch/nn/functional.py:2012:27
      %size_prods.384 : int = aten::__getitem__(%932, %24) # torch/nn/functional.py:1991:17
      %934 : int = aten::len(%932) # torch/nn/functional.py:1992:19
      %935 : int = aten::sub(%934, %26) # torch/nn/functional.py:1992:19
      %size_prods.385 : int = prim::Loop(%935, %25, %size_prods.384) # torch/nn/functional.py:1992:4
        block0(%i.97 : int, %size_prods.386 : int):
          %939 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
          %940 : int = aten::__getitem__(%932, %939) # torch/nn/functional.py:1993:22
          %size_prods.387 : int = aten::mul(%size_prods.386, %940) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.387)
      %942 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
       = prim::If(%942) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.279 : Tensor = aten::batch_norm(%out.278, %930, %931, %928, %929, %927, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %944 : __torch__.torch.nn.modules.container.___torch_mangle_33.Sequential = prim::GetAttr[name="downsample"](%829)
  %945 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="0"](%944)
  %946 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="1"](%944)
  %947 : Tensor = prim::GetAttr[name="weight"](%945)
  %948 : Tensor? = prim::GetAttr[name="bias"](%945)
  %949 : int[] = prim::ListConstruct(%27, %27)
  %950 : int[] = prim::ListConstruct(%24, %24)
  %951 : int[] = prim::ListConstruct(%27, %27)
  %input.51 : Tensor = aten::conv2d(%x.18, %947, %948, %949, %950, %951, %27) # torch/nn/modules/conv.py:415:15
  %953 : int = aten::dim(%input.51) # torch/nn/modules/batchnorm.py:276:11
  %954 : bool = aten::ne(%953, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%954) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %955 : bool = prim::GetAttr[name="training"](%946)
   = prim::If(%955) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %956 : Tensor = prim::GetAttr[name="num_batches_tracked"](%946)
      %957 : Tensor = aten::add(%956, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%946, %957)
      -> ()
    block1():
      -> ()
  %958 : bool = prim::GetAttr[name="training"](%946)
  %959 : Tensor = prim::GetAttr[name="running_mean"](%946)
  %960 : Tensor = prim::GetAttr[name="running_var"](%946)
  %961 : Tensor = prim::GetAttr[name="weight"](%946)
  %962 : Tensor = prim::GetAttr[name="bias"](%946)
   = prim::If(%958) # torch/nn/functional.py:2011:4
    block0():
      %963 : int[] = aten::size(%input.51) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%963, %24) # torch/nn/functional.py:1991:17
      %965 : int = aten::len(%963) # torch/nn/functional.py:1992:19
      %966 : int = aten::sub(%965, %26) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%966, %25, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %970 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
          %971 : int = aten::__getitem__(%963, %970) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %971) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.391)
      %973 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
       = prim::If(%973) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.4 : Tensor = aten::batch_norm(%input.51, %961, %962, %959, %960, %958, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.280 : Tensor = aten::add_(%out.279, %identity.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.53 : Tensor = aten::relu_(%out.280) # torch/nn/functional.py:1117:17
  %977 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%830)
  %978 : Tensor = prim::GetAttr[name="weight"](%977)
  %979 : Tensor? = prim::GetAttr[name="bias"](%977)
  %980 : int[] = prim::ListConstruct(%27, %27)
  %981 : int[] = prim::ListConstruct(%24, %24)
  %982 : int[] = prim::ListConstruct(%27, %27)
  %out.281 : Tensor = aten::conv2d(%input.53, %978, %979, %980, %981, %982, %27) # torch/nn/modules/conv.py:415:15
  %984 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%830)
  %985 : int = aten::dim(%out.281) # torch/nn/modules/batchnorm.py:276:11
  %986 : bool = aten::ne(%985, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%986) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %987 : bool = prim::GetAttr[name="training"](%984)
   = prim::If(%987) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %988 : Tensor = prim::GetAttr[name="num_batches_tracked"](%984)
      %989 : Tensor = aten::add(%988, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%984, %989)
      -> ()
    block1():
      -> ()
  %990 : bool = prim::GetAttr[name="training"](%984)
  %991 : Tensor = prim::GetAttr[name="running_mean"](%984)
  %992 : Tensor = prim::GetAttr[name="running_var"](%984)
  %993 : Tensor = prim::GetAttr[name="weight"](%984)
  %994 : Tensor = prim::GetAttr[name="bias"](%984)
   = prim::If(%990) # torch/nn/functional.py:2011:4
    block0():
      %995 : int[] = aten::size(%out.281) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%995, %24) # torch/nn/functional.py:1991:17
      %997 : int = aten::len(%995) # torch/nn/functional.py:1992:19
      %998 : int = aten::sub(%997, %26) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%998, %25, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %1002 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
          %1003 : int = aten::__getitem__(%995, %1002) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %1003) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.395)
      %1005 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1005) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.282 : Tensor = aten::batch_norm(%out.281, %993, %994, %991, %992, %990, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.283 : Tensor = aten::relu_(%out.282) # torch/nn/functional.py:1117:17
  %1008 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%830)
  %1009 : Tensor = prim::GetAttr[name="weight"](%1008)
  %1010 : Tensor? = prim::GetAttr[name="bias"](%1008)
  %1011 : int[] = prim::ListConstruct(%27, %27)
  %1012 : int[] = prim::ListConstruct(%26, %26)
  %1013 : int[] = prim::ListConstruct(%26, %26)
  %out.284 : Tensor = aten::conv2d(%out.283, %1009, %1010, %1011, %1012, %1013, %27) # torch/nn/modules/conv.py:415:15
  %1015 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%830)
  %1016 : int = aten::dim(%out.284) # torch/nn/modules/batchnorm.py:276:11
  %1017 : bool = aten::ne(%1016, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1017) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1018 : bool = prim::GetAttr[name="training"](%1015)
   = prim::If(%1018) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1019 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1015)
      %1020 : Tensor = aten::add(%1019, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1015, %1020)
      -> ()
    block1():
      -> ()
  %1021 : bool = prim::GetAttr[name="training"](%1015)
  %1022 : Tensor = prim::GetAttr[name="running_mean"](%1015)
  %1023 : Tensor = prim::GetAttr[name="running_var"](%1015)
  %1024 : Tensor = prim::GetAttr[name="weight"](%1015)
  %1025 : Tensor = prim::GetAttr[name="bias"](%1015)
   = prim::If(%1021) # torch/nn/functional.py:2011:4
    block0():
      %1026 : int[] = aten::size(%out.284) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%1026, %24) # torch/nn/functional.py:1991:17
      %1028 : int = aten::len(%1026) # torch/nn/functional.py:1992:19
      %1029 : int = aten::sub(%1028, %26) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%1029, %25, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %1033 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
          %1034 : int = aten::__getitem__(%1026, %1033) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %1034) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.399)
      %1036 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1036) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.285 : Tensor = aten::batch_norm(%out.284, %1024, %1025, %1022, %1023, %1021, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.286 : Tensor = aten::relu_(%out.285) # torch/nn/functional.py:1117:17
  %1039 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%830)
  %1040 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1041 : Tensor? = prim::GetAttr[name="bias"](%1039)
  %1042 : int[] = prim::ListConstruct(%27, %27)
  %1043 : int[] = prim::ListConstruct(%24, %24)
  %1044 : int[] = prim::ListConstruct(%27, %27)
  %out.287 : Tensor = aten::conv2d(%out.286, %1040, %1041, %1042, %1043, %1044, %27) # torch/nn/modules/conv.py:415:15
  %1046 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%830)
  %1047 : int = aten::dim(%out.287) # torch/nn/modules/batchnorm.py:276:11
  %1048 : bool = aten::ne(%1047, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1048) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1049 : bool = prim::GetAttr[name="training"](%1046)
   = prim::If(%1049) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1050 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1046)
      %1051 : Tensor = aten::add(%1050, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1046, %1051)
      -> ()
    block1():
      -> ()
  %1052 : bool = prim::GetAttr[name="training"](%1046)
  %1053 : Tensor = prim::GetAttr[name="running_mean"](%1046)
  %1054 : Tensor = prim::GetAttr[name="running_var"](%1046)
  %1055 : Tensor = prim::GetAttr[name="weight"](%1046)
  %1056 : Tensor = prim::GetAttr[name="bias"](%1046)
   = prim::If(%1052) # torch/nn/functional.py:2011:4
    block0():
      %1057 : int[] = aten::size(%out.287) # torch/nn/functional.py:2012:27
      %size_prods.400 : int = aten::__getitem__(%1057, %24) # torch/nn/functional.py:1991:17
      %1059 : int = aten::len(%1057) # torch/nn/functional.py:1992:19
      %1060 : int = aten::sub(%1059, %26) # torch/nn/functional.py:1992:19
      %size_prods.401 : int = prim::Loop(%1060, %25, %size_prods.400) # torch/nn/functional.py:1992:4
        block0(%i.101 : int, %size_prods.402 : int):
          %1064 : int = aten::add(%i.101, %26) # torch/nn/functional.py:1993:27
          %1065 : int = aten::__getitem__(%1057, %1064) # torch/nn/functional.py:1993:22
          %size_prods.403 : int = aten::mul(%size_prods.402, %1065) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.403)
      %1067 : bool = aten::eq(%size_prods.401, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1067) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.288 : Tensor = aten::batch_norm(%out.287, %1055, %1056, %1053, %1054, %1052, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.289 : Tensor = aten::add_(%out.288, %input.53, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.24 : Tensor = aten::relu_(%out.289) # torch/nn/functional.py:1117:17
  %1071 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%831)
  %1072 : Tensor = prim::GetAttr[name="weight"](%1071)
  %1073 : Tensor? = prim::GetAttr[name="bias"](%1071)
  %1074 : int[] = prim::ListConstruct(%27, %27)
  %1075 : int[] = prim::ListConstruct(%24, %24)
  %1076 : int[] = prim::ListConstruct(%27, %27)
  %out.37 : Tensor = aten::conv2d(%input.24, %1072, %1073, %1074, %1075, %1076, %27) # torch/nn/modules/conv.py:415:15
  %1078 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%831)
  %1079 : int = aten::dim(%out.37) # torch/nn/modules/batchnorm.py:276:11
  %1080 : bool = aten::ne(%1079, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1080) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1081 : bool = prim::GetAttr[name="training"](%1078)
   = prim::If(%1081) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1082 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1078)
      %1083 : Tensor = aten::add(%1082, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1078, %1083)
      -> ()
    block1():
      -> ()
  %1084 : bool = prim::GetAttr[name="training"](%1078)
  %1085 : Tensor = prim::GetAttr[name="running_mean"](%1078)
  %1086 : Tensor = prim::GetAttr[name="running_var"](%1078)
  %1087 : Tensor = prim::GetAttr[name="weight"](%1078)
  %1088 : Tensor = prim::GetAttr[name="bias"](%1078)
   = prim::If(%1084) # torch/nn/functional.py:2011:4
    block0():
      %1089 : int[] = aten::size(%out.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%1089, %24) # torch/nn/functional.py:1991:17
      %1091 : int = aten::len(%1089) # torch/nn/functional.py:1992:19
      %1092 : int = aten::sub(%1091, %26) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%1092, %25, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %1096 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
          %1097 : int = aten::__getitem__(%1089, %1096) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %1097) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.43)
      %1099 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1099) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.29 : Tensor = aten::batch_norm(%out.37, %1087, %1088, %1085, %1086, %1084, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.30 : Tensor = aten::relu_(%out.29) # torch/nn/functional.py:1117:17
  %1102 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%831)
  %1103 : Tensor = prim::GetAttr[name="weight"](%1102)
  %1104 : Tensor? = prim::GetAttr[name="bias"](%1102)
  %1105 : int[] = prim::ListConstruct(%27, %27)
  %1106 : int[] = prim::ListConstruct(%26, %26)
  %1107 : int[] = prim::ListConstruct(%26, %26)
  %out.31 : Tensor = aten::conv2d(%out.30, %1103, %1104, %1105, %1106, %1107, %27) # torch/nn/modules/conv.py:415:15
  %1109 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%831)
  %1110 : int = aten::dim(%out.31) # torch/nn/modules/batchnorm.py:276:11
  %1111 : bool = aten::ne(%1110, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1111) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1112 : bool = prim::GetAttr[name="training"](%1109)
   = prim::If(%1112) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1113 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1109)
      %1114 : Tensor = aten::add(%1113, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1109, %1114)
      -> ()
    block1():
      -> ()
  %1115 : bool = prim::GetAttr[name="training"](%1109)
  %1116 : Tensor = prim::GetAttr[name="running_mean"](%1109)
  %1117 : Tensor = prim::GetAttr[name="running_var"](%1109)
  %1118 : Tensor = prim::GetAttr[name="weight"](%1109)
  %1119 : Tensor = prim::GetAttr[name="bias"](%1109)
   = prim::If(%1115) # torch/nn/functional.py:2011:4
    block0():
      %1120 : int[] = aten::size(%out.31) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%1120, %24) # torch/nn/functional.py:1991:17
      %1122 : int = aten::len(%1120) # torch/nn/functional.py:1992:19
      %1123 : int = aten::sub(%1122, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%1123, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %1127 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %1128 : int = aten::__getitem__(%1120, %1127) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %1128) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %1130 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1130) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.32 : Tensor = aten::batch_norm(%out.31, %1118, %1119, %1116, %1117, %1115, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.33 : Tensor = aten::relu_(%out.32) # torch/nn/functional.py:1117:17
  %1133 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%831)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1135 : Tensor? = prim::GetAttr[name="bias"](%1133)
  %1136 : int[] = prim::ListConstruct(%27, %27)
  %1137 : int[] = prim::ListConstruct(%24, %24)
  %1138 : int[] = prim::ListConstruct(%27, %27)
  %out.34 : Tensor = aten::conv2d(%out.33, %1134, %1135, %1136, %1137, %1138, %27) # torch/nn/modules/conv.py:415:15
  %1140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%831)
  %1141 : int = aten::dim(%out.34) # torch/nn/modules/batchnorm.py:276:11
  %1142 : bool = aten::ne(%1141, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1142) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1143 : bool = prim::GetAttr[name="training"](%1140)
   = prim::If(%1143) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1144 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1140)
      %1145 : Tensor = aten::add(%1144, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1140, %1145)
      -> ()
    block1():
      -> ()
  %1146 : bool = prim::GetAttr[name="training"](%1140)
  %1147 : Tensor = prim::GetAttr[name="running_mean"](%1140)
  %1148 : Tensor = prim::GetAttr[name="running_var"](%1140)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1140)
  %1150 : Tensor = prim::GetAttr[name="bias"](%1140)
   = prim::If(%1146) # torch/nn/functional.py:2011:4
    block0():
      %1151 : int[] = aten::size(%out.34) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%1151, %24) # torch/nn/functional.py:1991:17
      %1153 : int = aten::len(%1151) # torch/nn/functional.py:1992:19
      %1154 : int = aten::sub(%1153, %26) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%1154, %25, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %1158 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
          %1159 : int = aten::__getitem__(%1151, %1158) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %1159) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.51)
      %1161 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1161) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.35 : Tensor = aten::batch_norm(%out.34, %1149, %1150, %1147, %1148, %1146, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.36 : Tensor = aten::add_(%out.35, %input.24, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.26 : Tensor = aten::relu_(%out.36) # torch/nn/functional.py:1117:17
  %1165 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%832)
  %1166 : Tensor = prim::GetAttr[name="weight"](%1165)
  %1167 : Tensor? = prim::GetAttr[name="bias"](%1165)
  %1168 : int[] = prim::ListConstruct(%27, %27)
  %1169 : int[] = prim::ListConstruct(%24, %24)
  %1170 : int[] = prim::ListConstruct(%27, %27)
  %out.46 : Tensor = aten::conv2d(%input.26, %1166, %1167, %1168, %1169, %1170, %27) # torch/nn/modules/conv.py:415:15
  %1172 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%832)
  %1173 : int = aten::dim(%out.46) # torch/nn/modules/batchnorm.py:276:11
  %1174 : bool = aten::ne(%1173, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1174) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1175 : bool = prim::GetAttr[name="training"](%1172)
   = prim::If(%1175) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1176 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1172)
      %1177 : Tensor = aten::add(%1176, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1172, %1177)
      -> ()
    block1():
      -> ()
  %1178 : bool = prim::GetAttr[name="training"](%1172)
  %1179 : Tensor = prim::GetAttr[name="running_mean"](%1172)
  %1180 : Tensor = prim::GetAttr[name="running_var"](%1172)
  %1181 : Tensor = prim::GetAttr[name="weight"](%1172)
  %1182 : Tensor = prim::GetAttr[name="bias"](%1172)
   = prim::If(%1178) # torch/nn/functional.py:2011:4
    block0():
      %1183 : int[] = aten::size(%out.46) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%1183, %24) # torch/nn/functional.py:1991:17
      %1185 : int = aten::len(%1183) # torch/nn/functional.py:1992:19
      %1186 : int = aten::sub(%1185, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%1186, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %1190 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %1191 : int = aten::__getitem__(%1183, %1190) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %1191) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %1193 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1193) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.38 : Tensor = aten::batch_norm(%out.46, %1181, %1182, %1179, %1180, %1178, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.39 : Tensor = aten::relu_(%out.38) # torch/nn/functional.py:1117:17
  %1196 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%832)
  %1197 : Tensor = prim::GetAttr[name="weight"](%1196)
  %1198 : Tensor? = prim::GetAttr[name="bias"](%1196)
  %1199 : int[] = prim::ListConstruct(%27, %27)
  %1200 : int[] = prim::ListConstruct(%26, %26)
  %1201 : int[] = prim::ListConstruct(%26, %26)
  %out.40 : Tensor = aten::conv2d(%out.39, %1197, %1198, %1199, %1200, %1201, %27) # torch/nn/modules/conv.py:415:15
  %1203 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%832)
  %1204 : int = aten::dim(%out.40) # torch/nn/modules/batchnorm.py:276:11
  %1205 : bool = aten::ne(%1204, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1205) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1206 : bool = prim::GetAttr[name="training"](%1203)
   = prim::If(%1206) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1207 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1203)
      %1208 : Tensor = aten::add(%1207, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1203, %1208)
      -> ()
    block1():
      -> ()
  %1209 : bool = prim::GetAttr[name="training"](%1203)
  %1210 : Tensor = prim::GetAttr[name="running_mean"](%1203)
  %1211 : Tensor = prim::GetAttr[name="running_var"](%1203)
  %1212 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1213 : Tensor = prim::GetAttr[name="bias"](%1203)
   = prim::If(%1209) # torch/nn/functional.py:2011:4
    block0():
      %1214 : int[] = aten::size(%out.40) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%1214, %24) # torch/nn/functional.py:1991:17
      %1216 : int = aten::len(%1214) # torch/nn/functional.py:1992:19
      %1217 : int = aten::sub(%1216, %26) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%1217, %25, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %1221 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
          %1222 : int = aten::__getitem__(%1214, %1221) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %1222) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.59)
      %1224 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1224) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.41 : Tensor = aten::batch_norm(%out.40, %1212, %1213, %1210, %1211, %1209, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.42 : Tensor = aten::relu_(%out.41) # torch/nn/functional.py:1117:17
  %1227 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%832)
  %1228 : Tensor = prim::GetAttr[name="weight"](%1227)
  %1229 : Tensor? = prim::GetAttr[name="bias"](%1227)
  %1230 : int[] = prim::ListConstruct(%27, %27)
  %1231 : int[] = prim::ListConstruct(%24, %24)
  %1232 : int[] = prim::ListConstruct(%27, %27)
  %out.43 : Tensor = aten::conv2d(%out.42, %1228, %1229, %1230, %1231, %1232, %27) # torch/nn/modules/conv.py:415:15
  %1234 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%832)
  %1235 : int = aten::dim(%out.43) # torch/nn/modules/batchnorm.py:276:11
  %1236 : bool = aten::ne(%1235, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1236) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1237 : bool = prim::GetAttr[name="training"](%1234)
   = prim::If(%1237) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1238 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1234)
      %1239 : Tensor = aten::add(%1238, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1234, %1239)
      -> ()
    block1():
      -> ()
  %1240 : bool = prim::GetAttr[name="training"](%1234)
  %1241 : Tensor = prim::GetAttr[name="running_mean"](%1234)
  %1242 : Tensor = prim::GetAttr[name="running_var"](%1234)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1234)
  %1244 : Tensor = prim::GetAttr[name="bias"](%1234)
   = prim::If(%1240) # torch/nn/functional.py:2011:4
    block0():
      %1245 : int[] = aten::size(%out.43) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%1245, %24) # torch/nn/functional.py:1991:17
      %1247 : int = aten::len(%1245) # torch/nn/functional.py:1992:19
      %1248 : int = aten::sub(%1247, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%1248, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %1252 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %1253 : int = aten::__getitem__(%1245, %1252) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %1253) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %1255 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1255) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.44 : Tensor = aten::batch_norm(%out.43, %1243, %1244, %1241, %1242, %1240, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.45 : Tensor = aten::add_(%out.44, %input.26, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.9 : Tensor = aten::relu_(%out.45) # torch/nn/functional.py:1117:17
  %1259 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%833)
  %1260 : Tensor = prim::GetAttr[name="weight"](%1259)
  %1261 : Tensor? = prim::GetAttr[name="bias"](%1259)
  %1262 : int[] = prim::ListConstruct(%27, %27)
  %1263 : int[] = prim::ListConstruct(%24, %24)
  %1264 : int[] = prim::ListConstruct(%27, %27)
  %out.55 : Tensor = aten::conv2d(%input.9, %1260, %1261, %1262, %1263, %1264, %27) # torch/nn/modules/conv.py:415:15
  %1266 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%833)
  %1267 : int = aten::dim(%out.55) # torch/nn/modules/batchnorm.py:276:11
  %1268 : bool = aten::ne(%1267, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1268) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1269 : bool = prim::GetAttr[name="training"](%1266)
   = prim::If(%1269) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1270 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1266)
      %1271 : Tensor = aten::add(%1270, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1266, %1271)
      -> ()
    block1():
      -> ()
  %1272 : bool = prim::GetAttr[name="training"](%1266)
  %1273 : Tensor = prim::GetAttr[name="running_mean"](%1266)
  %1274 : Tensor = prim::GetAttr[name="running_var"](%1266)
  %1275 : Tensor = prim::GetAttr[name="weight"](%1266)
  %1276 : Tensor = prim::GetAttr[name="bias"](%1266)
   = prim::If(%1272) # torch/nn/functional.py:2011:4
    block0():
      %1277 : int[] = aten::size(%out.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%1277, %24) # torch/nn/functional.py:1991:17
      %1279 : int = aten::len(%1277) # torch/nn/functional.py:1992:19
      %1280 : int = aten::sub(%1279, %26) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%1280, %25, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %1284 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
          %1285 : int = aten::__getitem__(%1277, %1284) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %1285) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.67)
      %1287 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1287) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.47 : Tensor = aten::batch_norm(%out.55, %1275, %1276, %1273, %1274, %1272, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.48 : Tensor = aten::relu_(%out.47) # torch/nn/functional.py:1117:17
  %1290 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%833)
  %1291 : Tensor = prim::GetAttr[name="weight"](%1290)
  %1292 : Tensor? = prim::GetAttr[name="bias"](%1290)
  %1293 : int[] = prim::ListConstruct(%27, %27)
  %1294 : int[] = prim::ListConstruct(%26, %26)
  %1295 : int[] = prim::ListConstruct(%26, %26)
  %out.49 : Tensor = aten::conv2d(%out.48, %1291, %1292, %1293, %1294, %1295, %27) # torch/nn/modules/conv.py:415:15
  %1297 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%833)
  %1298 : int = aten::dim(%out.49) # torch/nn/modules/batchnorm.py:276:11
  %1299 : bool = aten::ne(%1298, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1299) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1300 : bool = prim::GetAttr[name="training"](%1297)
   = prim::If(%1300) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1301 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1297)
      %1302 : Tensor = aten::add(%1301, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1297, %1302)
      -> ()
    block1():
      -> ()
  %1303 : bool = prim::GetAttr[name="training"](%1297)
  %1304 : Tensor = prim::GetAttr[name="running_mean"](%1297)
  %1305 : Tensor = prim::GetAttr[name="running_var"](%1297)
  %1306 : Tensor = prim::GetAttr[name="weight"](%1297)
  %1307 : Tensor = prim::GetAttr[name="bias"](%1297)
   = prim::If(%1303) # torch/nn/functional.py:2011:4
    block0():
      %1308 : int[] = aten::size(%out.49) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%1308, %24) # torch/nn/functional.py:1991:17
      %1310 : int = aten::len(%1308) # torch/nn/functional.py:1992:19
      %1311 : int = aten::sub(%1310, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%1311, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %1315 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %1316 : int = aten::__getitem__(%1308, %1315) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %1316) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %1318 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1318) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.50 : Tensor = aten::batch_norm(%out.49, %1306, %1307, %1304, %1305, %1303, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.51 : Tensor = aten::relu_(%out.50) # torch/nn/functional.py:1117:17
  %1321 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%833)
  %1322 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1323 : Tensor? = prim::GetAttr[name="bias"](%1321)
  %1324 : int[] = prim::ListConstruct(%27, %27)
  %1325 : int[] = prim::ListConstruct(%24, %24)
  %1326 : int[] = prim::ListConstruct(%27, %27)
  %out.52 : Tensor = aten::conv2d(%out.51, %1322, %1323, %1324, %1325, %1326, %27) # torch/nn/modules/conv.py:415:15
  %1328 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%833)
  %1329 : int = aten::dim(%out.52) # torch/nn/modules/batchnorm.py:276:11
  %1330 : bool = aten::ne(%1329, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1330) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1331 : bool = prim::GetAttr[name="training"](%1328)
   = prim::If(%1331) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1332 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1328)
      %1333 : Tensor = aten::add(%1332, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1328, %1333)
      -> ()
    block1():
      -> ()
  %1334 : bool = prim::GetAttr[name="training"](%1328)
  %1335 : Tensor = prim::GetAttr[name="running_mean"](%1328)
  %1336 : Tensor = prim::GetAttr[name="running_var"](%1328)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1328)
  %1338 : Tensor = prim::GetAttr[name="bias"](%1328)
   = prim::If(%1334) # torch/nn/functional.py:2011:4
    block0():
      %1339 : int[] = aten::size(%out.52) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%1339, %24) # torch/nn/functional.py:1991:17
      %1341 : int = aten::len(%1339) # torch/nn/functional.py:1992:19
      %1342 : int = aten::sub(%1341, %26) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%1342, %25, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %1346 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
          %1347 : int = aten::__getitem__(%1339, %1346) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %1347) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.75)
      %1349 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1349) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.53 : Tensor = aten::batch_norm(%out.52, %1337, %1338, %1335, %1336, %1334, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.54 : Tensor = aten::add_(%out.53, %input.9, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.36 : Tensor = aten::relu_(%out.54) # torch/nn/functional.py:1117:17
  %1353 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%834)
  %1354 : Tensor = prim::GetAttr[name="weight"](%1353)
  %1355 : Tensor? = prim::GetAttr[name="bias"](%1353)
  %1356 : int[] = prim::ListConstruct(%27, %27)
  %1357 : int[] = prim::ListConstruct(%24, %24)
  %1358 : int[] = prim::ListConstruct(%27, %27)
  %out.64 : Tensor = aten::conv2d(%input.36, %1354, %1355, %1356, %1357, %1358, %27) # torch/nn/modules/conv.py:415:15
  %1360 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%834)
  %1361 : int = aten::dim(%out.64) # torch/nn/modules/batchnorm.py:276:11
  %1362 : bool = aten::ne(%1361, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1362) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1363 : bool = prim::GetAttr[name="training"](%1360)
   = prim::If(%1363) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1364 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1360)
      %1365 : Tensor = aten::add(%1364, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1360, %1365)
      -> ()
    block1():
      -> ()
  %1366 : bool = prim::GetAttr[name="training"](%1360)
  %1367 : Tensor = prim::GetAttr[name="running_mean"](%1360)
  %1368 : Tensor = prim::GetAttr[name="running_var"](%1360)
  %1369 : Tensor = prim::GetAttr[name="weight"](%1360)
  %1370 : Tensor = prim::GetAttr[name="bias"](%1360)
   = prim::If(%1366) # torch/nn/functional.py:2011:4
    block0():
      %1371 : int[] = aten::size(%out.64) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%1371, %24) # torch/nn/functional.py:1991:17
      %1373 : int = aten::len(%1371) # torch/nn/functional.py:1992:19
      %1374 : int = aten::sub(%1373, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%1374, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %1378 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %1379 : int = aten::__getitem__(%1371, %1378) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %1379) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %1381 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1381) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.56 : Tensor = aten::batch_norm(%out.64, %1369, %1370, %1367, %1368, %1366, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.57 : Tensor = aten::relu_(%out.56) # torch/nn/functional.py:1117:17
  %1384 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%834)
  %1385 : Tensor = prim::GetAttr[name="weight"](%1384)
  %1386 : Tensor? = prim::GetAttr[name="bias"](%1384)
  %1387 : int[] = prim::ListConstruct(%27, %27)
  %1388 : int[] = prim::ListConstruct(%26, %26)
  %1389 : int[] = prim::ListConstruct(%26, %26)
  %out.58 : Tensor = aten::conv2d(%out.57, %1385, %1386, %1387, %1388, %1389, %27) # torch/nn/modules/conv.py:415:15
  %1391 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%834)
  %1392 : int = aten::dim(%out.58) # torch/nn/modules/batchnorm.py:276:11
  %1393 : bool = aten::ne(%1392, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1393) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1394 : bool = prim::GetAttr[name="training"](%1391)
   = prim::If(%1394) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1395 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1391)
      %1396 : Tensor = aten::add(%1395, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1391, %1396)
      -> ()
    block1():
      -> ()
  %1397 : bool = prim::GetAttr[name="training"](%1391)
  %1398 : Tensor = prim::GetAttr[name="running_mean"](%1391)
  %1399 : Tensor = prim::GetAttr[name="running_var"](%1391)
  %1400 : Tensor = prim::GetAttr[name="weight"](%1391)
  %1401 : Tensor = prim::GetAttr[name="bias"](%1391)
   = prim::If(%1397) # torch/nn/functional.py:2011:4
    block0():
      %1402 : int[] = aten::size(%out.58) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%1402, %24) # torch/nn/functional.py:1991:17
      %1404 : int = aten::len(%1402) # torch/nn/functional.py:1992:19
      %1405 : int = aten::sub(%1404, %26) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%1405, %25, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %1409 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
          %1410 : int = aten::__getitem__(%1402, %1409) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %1410) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.83)
      %1412 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1412) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.59 : Tensor = aten::batch_norm(%out.58, %1400, %1401, %1398, %1399, %1397, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.60 : Tensor = aten::relu_(%out.59) # torch/nn/functional.py:1117:17
  %1415 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%834)
  %1416 : Tensor = prim::GetAttr[name="weight"](%1415)
  %1417 : Tensor? = prim::GetAttr[name="bias"](%1415)
  %1418 : int[] = prim::ListConstruct(%27, %27)
  %1419 : int[] = prim::ListConstruct(%24, %24)
  %1420 : int[] = prim::ListConstruct(%27, %27)
  %out.61 : Tensor = aten::conv2d(%out.60, %1416, %1417, %1418, %1419, %1420, %27) # torch/nn/modules/conv.py:415:15
  %1422 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%834)
  %1423 : int = aten::dim(%out.61) # torch/nn/modules/batchnorm.py:276:11
  %1424 : bool = aten::ne(%1423, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1424) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1422)
   = prim::If(%1425) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1426 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1422)
      %1427 : Tensor = aten::add(%1426, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1422, %1427)
      -> ()
    block1():
      -> ()
  %1428 : bool = prim::GetAttr[name="training"](%1422)
  %1429 : Tensor = prim::GetAttr[name="running_mean"](%1422)
  %1430 : Tensor = prim::GetAttr[name="running_var"](%1422)
  %1431 : Tensor = prim::GetAttr[name="weight"](%1422)
  %1432 : Tensor = prim::GetAttr[name="bias"](%1422)
   = prim::If(%1428) # torch/nn/functional.py:2011:4
    block0():
      %1433 : int[] = aten::size(%out.61) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%1433, %24) # torch/nn/functional.py:1991:17
      %1435 : int = aten::len(%1433) # torch/nn/functional.py:1992:19
      %1436 : int = aten::sub(%1435, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%1436, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %1440 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %1441 : int = aten::__getitem__(%1433, %1440) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %1441) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %1443 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1443) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.62 : Tensor = aten::batch_norm(%out.61, %1431, %1432, %1429, %1430, %1428, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.63 : Tensor = aten::add_(%out.62, %input.36, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.40 : Tensor = aten::relu_(%out.63) # torch/nn/functional.py:1117:17
  %1447 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%835)
  %1448 : Tensor = prim::GetAttr[name="weight"](%1447)
  %1449 : Tensor? = prim::GetAttr[name="bias"](%1447)
  %1450 : int[] = prim::ListConstruct(%27, %27)
  %1451 : int[] = prim::ListConstruct(%24, %24)
  %1452 : int[] = prim::ListConstruct(%27, %27)
  %out.73 : Tensor = aten::conv2d(%input.40, %1448, %1449, %1450, %1451, %1452, %27) # torch/nn/modules/conv.py:415:15
  %1454 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%835)
  %1455 : int = aten::dim(%out.73) # torch/nn/modules/batchnorm.py:276:11
  %1456 : bool = aten::ne(%1455, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1456) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1457 : bool = prim::GetAttr[name="training"](%1454)
   = prim::If(%1457) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1458 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1454)
      %1459 : Tensor = aten::add(%1458, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1454, %1459)
      -> ()
    block1():
      -> ()
  %1460 : bool = prim::GetAttr[name="training"](%1454)
  %1461 : Tensor = prim::GetAttr[name="running_mean"](%1454)
  %1462 : Tensor = prim::GetAttr[name="running_var"](%1454)
  %1463 : Tensor = prim::GetAttr[name="weight"](%1454)
  %1464 : Tensor = prim::GetAttr[name="bias"](%1454)
   = prim::If(%1460) # torch/nn/functional.py:2011:4
    block0():
      %1465 : int[] = aten::size(%out.73) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%1465, %24) # torch/nn/functional.py:1991:17
      %1467 : int = aten::len(%1465) # torch/nn/functional.py:1992:19
      %1468 : int = aten::sub(%1467, %26) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%1468, %25, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %1472 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
          %1473 : int = aten::__getitem__(%1465, %1472) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %1473) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.91)
      %1475 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1475) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.65 : Tensor = aten::batch_norm(%out.73, %1463, %1464, %1461, %1462, %1460, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.66 : Tensor = aten::relu_(%out.65) # torch/nn/functional.py:1117:17
  %1478 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%835)
  %1479 : Tensor = prim::GetAttr[name="weight"](%1478)
  %1480 : Tensor? = prim::GetAttr[name="bias"](%1478)
  %1481 : int[] = prim::ListConstruct(%27, %27)
  %1482 : int[] = prim::ListConstruct(%26, %26)
  %1483 : int[] = prim::ListConstruct(%26, %26)
  %out.67 : Tensor = aten::conv2d(%out.66, %1479, %1480, %1481, %1482, %1483, %27) # torch/nn/modules/conv.py:415:15
  %1485 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%835)
  %1486 : int = aten::dim(%out.67) # torch/nn/modules/batchnorm.py:276:11
  %1487 : bool = aten::ne(%1486, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1487) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1488 : bool = prim::GetAttr[name="training"](%1485)
   = prim::If(%1488) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1489 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1485)
      %1490 : Tensor = aten::add(%1489, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1485, %1490)
      -> ()
    block1():
      -> ()
  %1491 : bool = prim::GetAttr[name="training"](%1485)
  %1492 : Tensor = prim::GetAttr[name="running_mean"](%1485)
  %1493 : Tensor = prim::GetAttr[name="running_var"](%1485)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1485)
  %1495 : Tensor = prim::GetAttr[name="bias"](%1485)
   = prim::If(%1491) # torch/nn/functional.py:2011:4
    block0():
      %1496 : int[] = aten::size(%out.67) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%1496, %24) # torch/nn/functional.py:1991:17
      %1498 : int = aten::len(%1496) # torch/nn/functional.py:1992:19
      %1499 : int = aten::sub(%1498, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%1499, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %1503 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %1504 : int = aten::__getitem__(%1496, %1503) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %1504) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %1506 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1506) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.68 : Tensor = aten::batch_norm(%out.67, %1494, %1495, %1492, %1493, %1491, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.69 : Tensor = aten::relu_(%out.68) # torch/nn/functional.py:1117:17
  %1509 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%835)
  %1510 : Tensor = prim::GetAttr[name="weight"](%1509)
  %1511 : Tensor? = prim::GetAttr[name="bias"](%1509)
  %1512 : int[] = prim::ListConstruct(%27, %27)
  %1513 : int[] = prim::ListConstruct(%24, %24)
  %1514 : int[] = prim::ListConstruct(%27, %27)
  %out.70 : Tensor = aten::conv2d(%out.69, %1510, %1511, %1512, %1513, %1514, %27) # torch/nn/modules/conv.py:415:15
  %1516 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%835)
  %1517 : int = aten::dim(%out.70) # torch/nn/modules/batchnorm.py:276:11
  %1518 : bool = aten::ne(%1517, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1518) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1519 : bool = prim::GetAttr[name="training"](%1516)
   = prim::If(%1519) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1520 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1516)
      %1521 : Tensor = aten::add(%1520, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1516, %1521)
      -> ()
    block1():
      -> ()
  %1522 : bool = prim::GetAttr[name="training"](%1516)
  %1523 : Tensor = prim::GetAttr[name="running_mean"](%1516)
  %1524 : Tensor = prim::GetAttr[name="running_var"](%1516)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1516)
  %1526 : Tensor = prim::GetAttr[name="bias"](%1516)
   = prim::If(%1522) # torch/nn/functional.py:2011:4
    block0():
      %1527 : int[] = aten::size(%out.70) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%1527, %24) # torch/nn/functional.py:1991:17
      %1529 : int = aten::len(%1527) # torch/nn/functional.py:1992:19
      %1530 : int = aten::sub(%1529, %26) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%1530, %25, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %1534 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
          %1535 : int = aten::__getitem__(%1527, %1534) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %1535) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.99)
      %1537 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1537) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.71 : Tensor = aten::batch_norm(%out.70, %1525, %1526, %1523, %1524, %1522, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.72 : Tensor = aten::add_(%out.71, %input.40, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.44 : Tensor = aten::relu_(%out.72) # torch/nn/functional.py:1117:17
  %1541 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%836)
  %1542 : Tensor = prim::GetAttr[name="weight"](%1541)
  %1543 : Tensor? = prim::GetAttr[name="bias"](%1541)
  %1544 : int[] = prim::ListConstruct(%27, %27)
  %1545 : int[] = prim::ListConstruct(%24, %24)
  %1546 : int[] = prim::ListConstruct(%27, %27)
  %out.82 : Tensor = aten::conv2d(%input.44, %1542, %1543, %1544, %1545, %1546, %27) # torch/nn/modules/conv.py:415:15
  %1548 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%836)
  %1549 : int = aten::dim(%out.82) # torch/nn/modules/batchnorm.py:276:11
  %1550 : bool = aten::ne(%1549, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1550) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1551 : bool = prim::GetAttr[name="training"](%1548)
   = prim::If(%1551) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1552 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1548)
      %1553 : Tensor = aten::add(%1552, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1548, %1553)
      -> ()
    block1():
      -> ()
  %1554 : bool = prim::GetAttr[name="training"](%1548)
  %1555 : Tensor = prim::GetAttr[name="running_mean"](%1548)
  %1556 : Tensor = prim::GetAttr[name="running_var"](%1548)
  %1557 : Tensor = prim::GetAttr[name="weight"](%1548)
  %1558 : Tensor = prim::GetAttr[name="bias"](%1548)
   = prim::If(%1554) # torch/nn/functional.py:2011:4
    block0():
      %1559 : int[] = aten::size(%out.82) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%1559, %24) # torch/nn/functional.py:1991:17
      %1561 : int = aten::len(%1559) # torch/nn/functional.py:1992:19
      %1562 : int = aten::sub(%1561, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%1562, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %1566 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %1567 : int = aten::__getitem__(%1559, %1566) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %1567) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %1569 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1569) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.74 : Tensor = aten::batch_norm(%out.82, %1557, %1558, %1555, %1556, %1554, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.75 : Tensor = aten::relu_(%out.74) # torch/nn/functional.py:1117:17
  %1572 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%836)
  %1573 : Tensor = prim::GetAttr[name="weight"](%1572)
  %1574 : Tensor? = prim::GetAttr[name="bias"](%1572)
  %1575 : int[] = prim::ListConstruct(%27, %27)
  %1576 : int[] = prim::ListConstruct(%26, %26)
  %1577 : int[] = prim::ListConstruct(%26, %26)
  %out.76 : Tensor = aten::conv2d(%out.75, %1573, %1574, %1575, %1576, %1577, %27) # torch/nn/modules/conv.py:415:15
  %1579 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%836)
  %1580 : int = aten::dim(%out.76) # torch/nn/modules/batchnorm.py:276:11
  %1581 : bool = aten::ne(%1580, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1581) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1582 : bool = prim::GetAttr[name="training"](%1579)
   = prim::If(%1582) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1583 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1579)
      %1584 : Tensor = aten::add(%1583, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1579, %1584)
      -> ()
    block1():
      -> ()
  %1585 : bool = prim::GetAttr[name="training"](%1579)
  %1586 : Tensor = prim::GetAttr[name="running_mean"](%1579)
  %1587 : Tensor = prim::GetAttr[name="running_var"](%1579)
  %1588 : Tensor = prim::GetAttr[name="weight"](%1579)
  %1589 : Tensor = prim::GetAttr[name="bias"](%1579)
   = prim::If(%1585) # torch/nn/functional.py:2011:4
    block0():
      %1590 : int[] = aten::size(%out.76) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%1590, %24) # torch/nn/functional.py:1991:17
      %1592 : int = aten::len(%1590) # torch/nn/functional.py:1992:19
      %1593 : int = aten::sub(%1592, %26) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%1593, %25, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %1597 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
          %1598 : int = aten::__getitem__(%1590, %1597) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %1598) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.107)
      %1600 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1600) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.77 : Tensor = aten::batch_norm(%out.76, %1588, %1589, %1586, %1587, %1585, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.78 : Tensor = aten::relu_(%out.77) # torch/nn/functional.py:1117:17
  %1603 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%836)
  %1604 : Tensor = prim::GetAttr[name="weight"](%1603)
  %1605 : Tensor? = prim::GetAttr[name="bias"](%1603)
  %1606 : int[] = prim::ListConstruct(%27, %27)
  %1607 : int[] = prim::ListConstruct(%24, %24)
  %1608 : int[] = prim::ListConstruct(%27, %27)
  %out.79 : Tensor = aten::conv2d(%out.78, %1604, %1605, %1606, %1607, %1608, %27) # torch/nn/modules/conv.py:415:15
  %1610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%836)
  %1611 : int = aten::dim(%out.79) # torch/nn/modules/batchnorm.py:276:11
  %1612 : bool = aten::ne(%1611, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1613 : bool = prim::GetAttr[name="training"](%1610)
   = prim::If(%1613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1610)
      %1615 : Tensor = aten::add(%1614, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1610, %1615)
      -> ()
    block1():
      -> ()
  %1616 : bool = prim::GetAttr[name="training"](%1610)
  %1617 : Tensor = prim::GetAttr[name="running_mean"](%1610)
  %1618 : Tensor = prim::GetAttr[name="running_var"](%1610)
  %1619 : Tensor = prim::GetAttr[name="weight"](%1610)
  %1620 : Tensor = prim::GetAttr[name="bias"](%1610)
   = prim::If(%1616) # torch/nn/functional.py:2011:4
    block0():
      %1621 : int[] = aten::size(%out.79) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%1621, %24) # torch/nn/functional.py:1991:17
      %1623 : int = aten::len(%1621) # torch/nn/functional.py:1992:19
      %1624 : int = aten::sub(%1623, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%1624, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %1628 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %1629 : int = aten::__getitem__(%1621, %1628) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %1629) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %1631 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.80 : Tensor = aten::batch_norm(%out.79, %1619, %1620, %1617, %1618, %1616, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.81 : Tensor = aten::add_(%out.80, %input.44, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.47 : Tensor = aten::relu_(%out.81) # torch/nn/functional.py:1117:17
  %1635 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%837)
  %1636 : Tensor = prim::GetAttr[name="weight"](%1635)
  %1637 : Tensor? = prim::GetAttr[name="bias"](%1635)
  %1638 : int[] = prim::ListConstruct(%27, %27)
  %1639 : int[] = prim::ListConstruct(%24, %24)
  %1640 : int[] = prim::ListConstruct(%27, %27)
  %out.91 : Tensor = aten::conv2d(%input.47, %1636, %1637, %1638, %1639, %1640, %27) # torch/nn/modules/conv.py:415:15
  %1642 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%837)
  %1643 : int = aten::dim(%out.91) # torch/nn/modules/batchnorm.py:276:11
  %1644 : bool = aten::ne(%1643, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1644) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1645 : bool = prim::GetAttr[name="training"](%1642)
   = prim::If(%1645) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1646 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1642)
      %1647 : Tensor = aten::add(%1646, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1642, %1647)
      -> ()
    block1():
      -> ()
  %1648 : bool = prim::GetAttr[name="training"](%1642)
  %1649 : Tensor = prim::GetAttr[name="running_mean"](%1642)
  %1650 : Tensor = prim::GetAttr[name="running_var"](%1642)
  %1651 : Tensor = prim::GetAttr[name="weight"](%1642)
  %1652 : Tensor = prim::GetAttr[name="bias"](%1642)
   = prim::If(%1648) # torch/nn/functional.py:2011:4
    block0():
      %1653 : int[] = aten::size(%out.91) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%1653, %24) # torch/nn/functional.py:1991:17
      %1655 : int = aten::len(%1653) # torch/nn/functional.py:1992:19
      %1656 : int = aten::sub(%1655, %26) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%1656, %25, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %1660 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
          %1661 : int = aten::__getitem__(%1653, %1660) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %1661) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.115)
      %1663 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1663) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.83 : Tensor = aten::batch_norm(%out.91, %1651, %1652, %1649, %1650, %1648, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.84 : Tensor = aten::relu_(%out.83) # torch/nn/functional.py:1117:17
  %1666 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%837)
  %1667 : Tensor = prim::GetAttr[name="weight"](%1666)
  %1668 : Tensor? = prim::GetAttr[name="bias"](%1666)
  %1669 : int[] = prim::ListConstruct(%27, %27)
  %1670 : int[] = prim::ListConstruct(%26, %26)
  %1671 : int[] = prim::ListConstruct(%26, %26)
  %out.85 : Tensor = aten::conv2d(%out.84, %1667, %1668, %1669, %1670, %1671, %27) # torch/nn/modules/conv.py:415:15
  %1673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%837)
  %1674 : int = aten::dim(%out.85) # torch/nn/modules/batchnorm.py:276:11
  %1675 : bool = aten::ne(%1674, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1675) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1676 : bool = prim::GetAttr[name="training"](%1673)
   = prim::If(%1676) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1673)
      %1678 : Tensor = aten::add(%1677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1673, %1678)
      -> ()
    block1():
      -> ()
  %1679 : bool = prim::GetAttr[name="training"](%1673)
  %1680 : Tensor = prim::GetAttr[name="running_mean"](%1673)
  %1681 : Tensor = prim::GetAttr[name="running_var"](%1673)
  %1682 : Tensor = prim::GetAttr[name="weight"](%1673)
  %1683 : Tensor = prim::GetAttr[name="bias"](%1673)
   = prim::If(%1679) # torch/nn/functional.py:2011:4
    block0():
      %1684 : int[] = aten::size(%out.85) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%1684, %24) # torch/nn/functional.py:1991:17
      %1686 : int = aten::len(%1684) # torch/nn/functional.py:1992:19
      %1687 : int = aten::sub(%1686, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%1687, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %1691 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %1692 : int = aten::__getitem__(%1684, %1691) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %1692) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %1694 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1694) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.86 : Tensor = aten::batch_norm(%out.85, %1682, %1683, %1680, %1681, %1679, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.87 : Tensor = aten::relu_(%out.86) # torch/nn/functional.py:1117:17
  %1697 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%837)
  %1698 : Tensor = prim::GetAttr[name="weight"](%1697)
  %1699 : Tensor? = prim::GetAttr[name="bias"](%1697)
  %1700 : int[] = prim::ListConstruct(%27, %27)
  %1701 : int[] = prim::ListConstruct(%24, %24)
  %1702 : int[] = prim::ListConstruct(%27, %27)
  %out.88 : Tensor = aten::conv2d(%out.87, %1698, %1699, %1700, %1701, %1702, %27) # torch/nn/modules/conv.py:415:15
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%837)
  %1705 : int = aten::dim(%out.88) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%out.88) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%1715, %24) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %26) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%1718, %25, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %1722 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %1723) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.123)
      %1725 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.89 : Tensor = aten::batch_norm(%out.88, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.90 : Tensor = aten::add_(%out.89, %input.47, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.55 : Tensor = aten::relu_(%out.90) # torch/nn/functional.py:1117:17
  %1729 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%838)
  %1730 : Tensor = prim::GetAttr[name="weight"](%1729)
  %1731 : Tensor? = prim::GetAttr[name="bias"](%1729)
  %1732 : int[] = prim::ListConstruct(%27, %27)
  %1733 : int[] = prim::ListConstruct(%24, %24)
  %1734 : int[] = prim::ListConstruct(%27, %27)
  %out.100 : Tensor = aten::conv2d(%input.55, %1730, %1731, %1732, %1733, %1734, %27) # torch/nn/modules/conv.py:415:15
  %1736 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%838)
  %1737 : int = aten::dim(%out.100) # torch/nn/modules/batchnorm.py:276:11
  %1738 : bool = aten::ne(%1737, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1738) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1739 : bool = prim::GetAttr[name="training"](%1736)
   = prim::If(%1739) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1740 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1736)
      %1741 : Tensor = aten::add(%1740, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1736, %1741)
      -> ()
    block1():
      -> ()
  %1742 : bool = prim::GetAttr[name="training"](%1736)
  %1743 : Tensor = prim::GetAttr[name="running_mean"](%1736)
  %1744 : Tensor = prim::GetAttr[name="running_var"](%1736)
  %1745 : Tensor = prim::GetAttr[name="weight"](%1736)
  %1746 : Tensor = prim::GetAttr[name="bias"](%1736)
   = prim::If(%1742) # torch/nn/functional.py:2011:4
    block0():
      %1747 : int[] = aten::size(%out.100) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%1747, %24) # torch/nn/functional.py:1991:17
      %1749 : int = aten::len(%1747) # torch/nn/functional.py:1992:19
      %1750 : int = aten::sub(%1749, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%1750, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %1754 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %1755 : int = aten::__getitem__(%1747, %1754) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %1755) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %1757 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1757) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.92 : Tensor = aten::batch_norm(%out.100, %1745, %1746, %1743, %1744, %1742, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.93 : Tensor = aten::relu_(%out.92) # torch/nn/functional.py:1117:17
  %1760 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%838)
  %1761 : Tensor = prim::GetAttr[name="weight"](%1760)
  %1762 : Tensor? = prim::GetAttr[name="bias"](%1760)
  %1763 : int[] = prim::ListConstruct(%27, %27)
  %1764 : int[] = prim::ListConstruct(%26, %26)
  %1765 : int[] = prim::ListConstruct(%26, %26)
  %out.94 : Tensor = aten::conv2d(%out.93, %1761, %1762, %1763, %1764, %1765, %27) # torch/nn/modules/conv.py:415:15
  %1767 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%838)
  %1768 : int = aten::dim(%out.94) # torch/nn/modules/batchnorm.py:276:11
  %1769 : bool = aten::ne(%1768, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1769) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1770 : bool = prim::GetAttr[name="training"](%1767)
   = prim::If(%1770) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1771 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1767)
      %1772 : Tensor = aten::add(%1771, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1767, %1772)
      -> ()
    block1():
      -> ()
  %1773 : bool = prim::GetAttr[name="training"](%1767)
  %1774 : Tensor = prim::GetAttr[name="running_mean"](%1767)
  %1775 : Tensor = prim::GetAttr[name="running_var"](%1767)
  %1776 : Tensor = prim::GetAttr[name="weight"](%1767)
  %1777 : Tensor = prim::GetAttr[name="bias"](%1767)
   = prim::If(%1773) # torch/nn/functional.py:2011:4
    block0():
      %1778 : int[] = aten::size(%out.94) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%1778, %24) # torch/nn/functional.py:1991:17
      %1780 : int = aten::len(%1778) # torch/nn/functional.py:1992:19
      %1781 : int = aten::sub(%1780, %26) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%1781, %25, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %1785 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
          %1786 : int = aten::__getitem__(%1778, %1785) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %1786) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.131)
      %1788 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1788) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.95 : Tensor = aten::batch_norm(%out.94, %1776, %1777, %1774, %1775, %1773, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.96 : Tensor = aten::relu_(%out.95) # torch/nn/functional.py:1117:17
  %1791 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%838)
  %1792 : Tensor = prim::GetAttr[name="weight"](%1791)
  %1793 : Tensor? = prim::GetAttr[name="bias"](%1791)
  %1794 : int[] = prim::ListConstruct(%27, %27)
  %1795 : int[] = prim::ListConstruct(%24, %24)
  %1796 : int[] = prim::ListConstruct(%27, %27)
  %out.97 : Tensor = aten::conv2d(%out.96, %1792, %1793, %1794, %1795, %1796, %27) # torch/nn/modules/conv.py:415:15
  %1798 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%838)
  %1799 : int = aten::dim(%out.97) # torch/nn/modules/batchnorm.py:276:11
  %1800 : bool = aten::ne(%1799, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1800) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1801 : bool = prim::GetAttr[name="training"](%1798)
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1802 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1798)
      %1803 : Tensor = aten::add(%1802, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1798, %1803)
      -> ()
    block1():
      -> ()
  %1804 : bool = prim::GetAttr[name="training"](%1798)
  %1805 : Tensor = prim::GetAttr[name="running_mean"](%1798)
  %1806 : Tensor = prim::GetAttr[name="running_var"](%1798)
  %1807 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1808 : Tensor = prim::GetAttr[name="bias"](%1798)
   = prim::If(%1804) # torch/nn/functional.py:2011:4
    block0():
      %1809 : int[] = aten::size(%out.97) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%1809, %24) # torch/nn/functional.py:1991:17
      %1811 : int = aten::len(%1809) # torch/nn/functional.py:1992:19
      %1812 : int = aten::sub(%1811, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%1812, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %1816 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %1817 : int = aten::__getitem__(%1809, %1816) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %1817) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %1819 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1819) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.98 : Tensor = aten::batch_norm(%out.97, %1807, %1808, %1805, %1806, %1804, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.99 : Tensor = aten::add_(%out.98, %input.55, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.52 : Tensor = aten::relu_(%out.99) # torch/nn/functional.py:1117:17
  %1823 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%839)
  %1824 : Tensor = prim::GetAttr[name="weight"](%1823)
  %1825 : Tensor? = prim::GetAttr[name="bias"](%1823)
  %1826 : int[] = prim::ListConstruct(%27, %27)
  %1827 : int[] = prim::ListConstruct(%24, %24)
  %1828 : int[] = prim::ListConstruct(%27, %27)
  %out.109 : Tensor = aten::conv2d(%input.52, %1824, %1825, %1826, %1827, %1828, %27) # torch/nn/modules/conv.py:415:15
  %1830 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%839)
  %1831 : int = aten::dim(%out.109) # torch/nn/modules/batchnorm.py:276:11
  %1832 : bool = aten::ne(%1831, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1832) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1833 : bool = prim::GetAttr[name="training"](%1830)
   = prim::If(%1833) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1834 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1830)
      %1835 : Tensor = aten::add(%1834, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1830, %1835)
      -> ()
    block1():
      -> ()
  %1836 : bool = prim::GetAttr[name="training"](%1830)
  %1837 : Tensor = prim::GetAttr[name="running_mean"](%1830)
  %1838 : Tensor = prim::GetAttr[name="running_var"](%1830)
  %1839 : Tensor = prim::GetAttr[name="weight"](%1830)
  %1840 : Tensor = prim::GetAttr[name="bias"](%1830)
   = prim::If(%1836) # torch/nn/functional.py:2011:4
    block0():
      %1841 : int[] = aten::size(%out.109) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%1841, %24) # torch/nn/functional.py:1991:17
      %1843 : int = aten::len(%1841) # torch/nn/functional.py:1992:19
      %1844 : int = aten::sub(%1843, %26) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%1844, %25, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %1848 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
          %1849 : int = aten::__getitem__(%1841, %1848) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %1849) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.139)
      %1851 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1851) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.101 : Tensor = aten::batch_norm(%out.109, %1839, %1840, %1837, %1838, %1836, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.102 : Tensor = aten::relu_(%out.101) # torch/nn/functional.py:1117:17
  %1854 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%839)
  %1855 : Tensor = prim::GetAttr[name="weight"](%1854)
  %1856 : Tensor? = prim::GetAttr[name="bias"](%1854)
  %1857 : int[] = prim::ListConstruct(%27, %27)
  %1858 : int[] = prim::ListConstruct(%26, %26)
  %1859 : int[] = prim::ListConstruct(%26, %26)
  %out.103 : Tensor = aten::conv2d(%out.102, %1855, %1856, %1857, %1858, %1859, %27) # torch/nn/modules/conv.py:415:15
  %1861 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%839)
  %1862 : int = aten::dim(%out.103) # torch/nn/modules/batchnorm.py:276:11
  %1863 : bool = aten::ne(%1862, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1863) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1864 : bool = prim::GetAttr[name="training"](%1861)
   = prim::If(%1864) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1865 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1861)
      %1866 : Tensor = aten::add(%1865, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1861, %1866)
      -> ()
    block1():
      -> ()
  %1867 : bool = prim::GetAttr[name="training"](%1861)
  %1868 : Tensor = prim::GetAttr[name="running_mean"](%1861)
  %1869 : Tensor = prim::GetAttr[name="running_var"](%1861)
  %1870 : Tensor = prim::GetAttr[name="weight"](%1861)
  %1871 : Tensor = prim::GetAttr[name="bias"](%1861)
   = prim::If(%1867) # torch/nn/functional.py:2011:4
    block0():
      %1872 : int[] = aten::size(%out.103) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%1872, %24) # torch/nn/functional.py:1991:17
      %1874 : int = aten::len(%1872) # torch/nn/functional.py:1992:19
      %1875 : int = aten::sub(%1874, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%1875, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %1879 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %1880 : int = aten::__getitem__(%1872, %1879) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %1880) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %1882 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1882) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.104 : Tensor = aten::batch_norm(%out.103, %1870, %1871, %1868, %1869, %1867, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.105 : Tensor = aten::relu_(%out.104) # torch/nn/functional.py:1117:17
  %1885 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%839)
  %1886 : Tensor = prim::GetAttr[name="weight"](%1885)
  %1887 : Tensor? = prim::GetAttr[name="bias"](%1885)
  %1888 : int[] = prim::ListConstruct(%27, %27)
  %1889 : int[] = prim::ListConstruct(%24, %24)
  %1890 : int[] = prim::ListConstruct(%27, %27)
  %out.106 : Tensor = aten::conv2d(%out.105, %1886, %1887, %1888, %1889, %1890, %27) # torch/nn/modules/conv.py:415:15
  %1892 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%839)
  %1893 : int = aten::dim(%out.106) # torch/nn/modules/batchnorm.py:276:11
  %1894 : bool = aten::ne(%1893, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1894) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1895 : bool = prim::GetAttr[name="training"](%1892)
   = prim::If(%1895) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1896 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1892)
      %1897 : Tensor = aten::add(%1896, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1892, %1897)
      -> ()
    block1():
      -> ()
  %1898 : bool = prim::GetAttr[name="training"](%1892)
  %1899 : Tensor = prim::GetAttr[name="running_mean"](%1892)
  %1900 : Tensor = prim::GetAttr[name="running_var"](%1892)
  %1901 : Tensor = prim::GetAttr[name="weight"](%1892)
  %1902 : Tensor = prim::GetAttr[name="bias"](%1892)
   = prim::If(%1898) # torch/nn/functional.py:2011:4
    block0():
      %1903 : int[] = aten::size(%out.106) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%1903, %24) # torch/nn/functional.py:1991:17
      %1905 : int = aten::len(%1903) # torch/nn/functional.py:1992:19
      %1906 : int = aten::sub(%1905, %26) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%1906, %25, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %1910 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
          %1911 : int = aten::__getitem__(%1903, %1910) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %1911) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.147)
      %1913 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1913) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.107 : Tensor = aten::batch_norm(%out.106, %1901, %1902, %1899, %1900, %1898, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.108 : Tensor = aten::add_(%out.107, %input.52, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.54 : Tensor = aten::relu_(%out.108) # torch/nn/functional.py:1117:17
  %1917 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%840)
  %1918 : Tensor = prim::GetAttr[name="weight"](%1917)
  %1919 : Tensor? = prim::GetAttr[name="bias"](%1917)
  %1920 : int[] = prim::ListConstruct(%27, %27)
  %1921 : int[] = prim::ListConstruct(%24, %24)
  %1922 : int[] = prim::ListConstruct(%27, %27)
  %out.118 : Tensor = aten::conv2d(%input.54, %1918, %1919, %1920, %1921, %1922, %27) # torch/nn/modules/conv.py:415:15
  %1924 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%840)
  %1925 : int = aten::dim(%out.118) # torch/nn/modules/batchnorm.py:276:11
  %1926 : bool = aten::ne(%1925, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1926) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1927 : bool = prim::GetAttr[name="training"](%1924)
   = prim::If(%1927) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1928 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1924)
      %1929 : Tensor = aten::add(%1928, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1924, %1929)
      -> ()
    block1():
      -> ()
  %1930 : bool = prim::GetAttr[name="training"](%1924)
  %1931 : Tensor = prim::GetAttr[name="running_mean"](%1924)
  %1932 : Tensor = prim::GetAttr[name="running_var"](%1924)
  %1933 : Tensor = prim::GetAttr[name="weight"](%1924)
  %1934 : Tensor = prim::GetAttr[name="bias"](%1924)
   = prim::If(%1930) # torch/nn/functional.py:2011:4
    block0():
      %1935 : int[] = aten::size(%out.118) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%1935, %24) # torch/nn/functional.py:1991:17
      %1937 : int = aten::len(%1935) # torch/nn/functional.py:1992:19
      %1938 : int = aten::sub(%1937, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%1938, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %1942 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %1943 : int = aten::__getitem__(%1935, %1942) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %1943) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %1945 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1945) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.110 : Tensor = aten::batch_norm(%out.118, %1933, %1934, %1931, %1932, %1930, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.111 : Tensor = aten::relu_(%out.110) # torch/nn/functional.py:1117:17
  %1948 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%840)
  %1949 : Tensor = prim::GetAttr[name="weight"](%1948)
  %1950 : Tensor? = prim::GetAttr[name="bias"](%1948)
  %1951 : int[] = prim::ListConstruct(%27, %27)
  %1952 : int[] = prim::ListConstruct(%26, %26)
  %1953 : int[] = prim::ListConstruct(%26, %26)
  %out.112 : Tensor = aten::conv2d(%out.111, %1949, %1950, %1951, %1952, %1953, %27) # torch/nn/modules/conv.py:415:15
  %1955 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%840)
  %1956 : int = aten::dim(%out.112) # torch/nn/modules/batchnorm.py:276:11
  %1957 : bool = aten::ne(%1956, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1957) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1958 : bool = prim::GetAttr[name="training"](%1955)
   = prim::If(%1958) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1959 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1955)
      %1960 : Tensor = aten::add(%1959, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1955, %1960)
      -> ()
    block1():
      -> ()
  %1961 : bool = prim::GetAttr[name="training"](%1955)
  %1962 : Tensor = prim::GetAttr[name="running_mean"](%1955)
  %1963 : Tensor = prim::GetAttr[name="running_var"](%1955)
  %1964 : Tensor = prim::GetAttr[name="weight"](%1955)
  %1965 : Tensor = prim::GetAttr[name="bias"](%1955)
   = prim::If(%1961) # torch/nn/functional.py:2011:4
    block0():
      %1966 : int[] = aten::size(%out.112) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%1966, %24) # torch/nn/functional.py:1991:17
      %1968 : int = aten::len(%1966) # torch/nn/functional.py:1992:19
      %1969 : int = aten::sub(%1968, %26) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%1969, %25, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %1973 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
          %1974 : int = aten::__getitem__(%1966, %1973) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %1974) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.155)
      %1976 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1976) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.113 : Tensor = aten::batch_norm(%out.112, %1964, %1965, %1962, %1963, %1961, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.114 : Tensor = aten::relu_(%out.113) # torch/nn/functional.py:1117:17
  %1979 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%840)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1979)
  %1981 : Tensor? = prim::GetAttr[name="bias"](%1979)
  %1982 : int[] = prim::ListConstruct(%27, %27)
  %1983 : int[] = prim::ListConstruct(%24, %24)
  %1984 : int[] = prim::ListConstruct(%27, %27)
  %out.115 : Tensor = aten::conv2d(%out.114, %1980, %1981, %1982, %1983, %1984, %27) # torch/nn/modules/conv.py:415:15
  %1986 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%840)
  %1987 : int = aten::dim(%out.115) # torch/nn/modules/batchnorm.py:276:11
  %1988 : bool = aten::ne(%1987, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1988) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1989 : bool = prim::GetAttr[name="training"](%1986)
   = prim::If(%1989) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1990 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1986)
      %1991 : Tensor = aten::add(%1990, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1986, %1991)
      -> ()
    block1():
      -> ()
  %1992 : bool = prim::GetAttr[name="training"](%1986)
  %1993 : Tensor = prim::GetAttr[name="running_mean"](%1986)
  %1994 : Tensor = prim::GetAttr[name="running_var"](%1986)
  %1995 : Tensor = prim::GetAttr[name="weight"](%1986)
  %1996 : Tensor = prim::GetAttr[name="bias"](%1986)
   = prim::If(%1992) # torch/nn/functional.py:2011:4
    block0():
      %1997 : int[] = aten::size(%out.115) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%1997, %24) # torch/nn/functional.py:1991:17
      %1999 : int = aten::len(%1997) # torch/nn/functional.py:1992:19
      %2000 : int = aten::sub(%1999, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%2000, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %2004 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %2005 : int = aten::__getitem__(%1997, %2004) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %2005) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %2007 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2007) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.116 : Tensor = aten::batch_norm(%out.115, %1995, %1996, %1993, %1994, %1992, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.117 : Tensor = aten::add_(%out.116, %input.54, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.25 : Tensor = aten::relu_(%out.117) # torch/nn/functional.py:1117:17
  %2011 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%841)
  %2012 : Tensor = prim::GetAttr[name="weight"](%2011)
  %2013 : Tensor? = prim::GetAttr[name="bias"](%2011)
  %2014 : int[] = prim::ListConstruct(%27, %27)
  %2015 : int[] = prim::ListConstruct(%24, %24)
  %2016 : int[] = prim::ListConstruct(%27, %27)
  %out.127 : Tensor = aten::conv2d(%input.25, %2012, %2013, %2014, %2015, %2016, %27) # torch/nn/modules/conv.py:415:15
  %2018 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%841)
  %2019 : int = aten::dim(%out.127) # torch/nn/modules/batchnorm.py:276:11
  %2020 : bool = aten::ne(%2019, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2020) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2021 : bool = prim::GetAttr[name="training"](%2018)
   = prim::If(%2021) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2022 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2018)
      %2023 : Tensor = aten::add(%2022, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2018, %2023)
      -> ()
    block1():
      -> ()
  %2024 : bool = prim::GetAttr[name="training"](%2018)
  %2025 : Tensor = prim::GetAttr[name="running_mean"](%2018)
  %2026 : Tensor = prim::GetAttr[name="running_var"](%2018)
  %2027 : Tensor = prim::GetAttr[name="weight"](%2018)
  %2028 : Tensor = prim::GetAttr[name="bias"](%2018)
   = prim::If(%2024) # torch/nn/functional.py:2011:4
    block0():
      %2029 : int[] = aten::size(%out.127) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%2029, %24) # torch/nn/functional.py:1991:17
      %2031 : int = aten::len(%2029) # torch/nn/functional.py:1992:19
      %2032 : int = aten::sub(%2031, %26) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%2032, %25, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %2036 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
          %2037 : int = aten::__getitem__(%2029, %2036) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %2037) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.163)
      %2039 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2039) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.119 : Tensor = aten::batch_norm(%out.127, %2027, %2028, %2025, %2026, %2024, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.120 : Tensor = aten::relu_(%out.119) # torch/nn/functional.py:1117:17
  %2042 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%841)
  %2043 : Tensor = prim::GetAttr[name="weight"](%2042)
  %2044 : Tensor? = prim::GetAttr[name="bias"](%2042)
  %2045 : int[] = prim::ListConstruct(%27, %27)
  %2046 : int[] = prim::ListConstruct(%26, %26)
  %2047 : int[] = prim::ListConstruct(%26, %26)
  %out.121 : Tensor = aten::conv2d(%out.120, %2043, %2044, %2045, %2046, %2047, %27) # torch/nn/modules/conv.py:415:15
  %2049 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%841)
  %2050 : int = aten::dim(%out.121) # torch/nn/modules/batchnorm.py:276:11
  %2051 : bool = aten::ne(%2050, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2051) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2052 : bool = prim::GetAttr[name="training"](%2049)
   = prim::If(%2052) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2053 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2049)
      %2054 : Tensor = aten::add(%2053, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2049, %2054)
      -> ()
    block1():
      -> ()
  %2055 : bool = prim::GetAttr[name="training"](%2049)
  %2056 : Tensor = prim::GetAttr[name="running_mean"](%2049)
  %2057 : Tensor = prim::GetAttr[name="running_var"](%2049)
  %2058 : Tensor = prim::GetAttr[name="weight"](%2049)
  %2059 : Tensor = prim::GetAttr[name="bias"](%2049)
   = prim::If(%2055) # torch/nn/functional.py:2011:4
    block0():
      %2060 : int[] = aten::size(%out.121) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%2060, %24) # torch/nn/functional.py:1991:17
      %2062 : int = aten::len(%2060) # torch/nn/functional.py:1992:19
      %2063 : int = aten::sub(%2062, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%2063, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %2067 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %2068 : int = aten::__getitem__(%2060, %2067) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %2068) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %2070 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2070) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.122 : Tensor = aten::batch_norm(%out.121, %2058, %2059, %2056, %2057, %2055, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.123 : Tensor = aten::relu_(%out.122) # torch/nn/functional.py:1117:17
  %2073 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%841)
  %2074 : Tensor = prim::GetAttr[name="weight"](%2073)
  %2075 : Tensor? = prim::GetAttr[name="bias"](%2073)
  %2076 : int[] = prim::ListConstruct(%27, %27)
  %2077 : int[] = prim::ListConstruct(%24, %24)
  %2078 : int[] = prim::ListConstruct(%27, %27)
  %out.124 : Tensor = aten::conv2d(%out.123, %2074, %2075, %2076, %2077, %2078, %27) # torch/nn/modules/conv.py:415:15
  %2080 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%841)
  %2081 : int = aten::dim(%out.124) # torch/nn/modules/batchnorm.py:276:11
  %2082 : bool = aten::ne(%2081, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2082) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2083 : bool = prim::GetAttr[name="training"](%2080)
   = prim::If(%2083) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2084 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2080)
      %2085 : Tensor = aten::add(%2084, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2080, %2085)
      -> ()
    block1():
      -> ()
  %2086 : bool = prim::GetAttr[name="training"](%2080)
  %2087 : Tensor = prim::GetAttr[name="running_mean"](%2080)
  %2088 : Tensor = prim::GetAttr[name="running_var"](%2080)
  %2089 : Tensor = prim::GetAttr[name="weight"](%2080)
  %2090 : Tensor = prim::GetAttr[name="bias"](%2080)
   = prim::If(%2086) # torch/nn/functional.py:2011:4
    block0():
      %2091 : int[] = aten::size(%out.124) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%2091, %24) # torch/nn/functional.py:1991:17
      %2093 : int = aten::len(%2091) # torch/nn/functional.py:1992:19
      %2094 : int = aten::sub(%2093, %26) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%2094, %25, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %2098 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
          %2099 : int = aten::__getitem__(%2091, %2098) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %2099) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.171)
      %2101 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2101) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.125 : Tensor = aten::batch_norm(%out.124, %2089, %2090, %2087, %2088, %2086, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.126 : Tensor = aten::add_(%out.125, %input.25, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.27 : Tensor = aten::relu_(%out.126) # torch/nn/functional.py:1117:17
  %2105 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%842)
  %2106 : Tensor = prim::GetAttr[name="weight"](%2105)
  %2107 : Tensor? = prim::GetAttr[name="bias"](%2105)
  %2108 : int[] = prim::ListConstruct(%27, %27)
  %2109 : int[] = prim::ListConstruct(%24, %24)
  %2110 : int[] = prim::ListConstruct(%27, %27)
  %out.136 : Tensor = aten::conv2d(%input.27, %2106, %2107, %2108, %2109, %2110, %27) # torch/nn/modules/conv.py:415:15
  %2112 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%842)
  %2113 : int = aten::dim(%out.136) # torch/nn/modules/batchnorm.py:276:11
  %2114 : bool = aten::ne(%2113, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2114) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2115 : bool = prim::GetAttr[name="training"](%2112)
   = prim::If(%2115) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2116 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2112)
      %2117 : Tensor = aten::add(%2116, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2112, %2117)
      -> ()
    block1():
      -> ()
  %2118 : bool = prim::GetAttr[name="training"](%2112)
  %2119 : Tensor = prim::GetAttr[name="running_mean"](%2112)
  %2120 : Tensor = prim::GetAttr[name="running_var"](%2112)
  %2121 : Tensor = prim::GetAttr[name="weight"](%2112)
  %2122 : Tensor = prim::GetAttr[name="bias"](%2112)
   = prim::If(%2118) # torch/nn/functional.py:2011:4
    block0():
      %2123 : int[] = aten::size(%out.136) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%2123, %24) # torch/nn/functional.py:1991:17
      %2125 : int = aten::len(%2123) # torch/nn/functional.py:1992:19
      %2126 : int = aten::sub(%2125, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%2126, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %2130 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %2131 : int = aten::__getitem__(%2123, %2130) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %2131) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %2133 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2133) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.128 : Tensor = aten::batch_norm(%out.136, %2121, %2122, %2119, %2120, %2118, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.129 : Tensor = aten::relu_(%out.128) # torch/nn/functional.py:1117:17
  %2136 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%842)
  %2137 : Tensor = prim::GetAttr[name="weight"](%2136)
  %2138 : Tensor? = prim::GetAttr[name="bias"](%2136)
  %2139 : int[] = prim::ListConstruct(%27, %27)
  %2140 : int[] = prim::ListConstruct(%26, %26)
  %2141 : int[] = prim::ListConstruct(%26, %26)
  %out.130 : Tensor = aten::conv2d(%out.129, %2137, %2138, %2139, %2140, %2141, %27) # torch/nn/modules/conv.py:415:15
  %2143 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%842)
  %2144 : int = aten::dim(%out.130) # torch/nn/modules/batchnorm.py:276:11
  %2145 : bool = aten::ne(%2144, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2145) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2146 : bool = prim::GetAttr[name="training"](%2143)
   = prim::If(%2146) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2147 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2143)
      %2148 : Tensor = aten::add(%2147, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2143, %2148)
      -> ()
    block1():
      -> ()
  %2149 : bool = prim::GetAttr[name="training"](%2143)
  %2150 : Tensor = prim::GetAttr[name="running_mean"](%2143)
  %2151 : Tensor = prim::GetAttr[name="running_var"](%2143)
  %2152 : Tensor = prim::GetAttr[name="weight"](%2143)
  %2153 : Tensor = prim::GetAttr[name="bias"](%2143)
   = prim::If(%2149) # torch/nn/functional.py:2011:4
    block0():
      %2154 : int[] = aten::size(%out.130) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%2154, %24) # torch/nn/functional.py:1991:17
      %2156 : int = aten::len(%2154) # torch/nn/functional.py:1992:19
      %2157 : int = aten::sub(%2156, %26) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%2157, %25, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %2161 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
          %2162 : int = aten::__getitem__(%2154, %2161) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %2162) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.179)
      %2164 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2164) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.131 : Tensor = aten::batch_norm(%out.130, %2152, %2153, %2150, %2151, %2149, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.132 : Tensor = aten::relu_(%out.131) # torch/nn/functional.py:1117:17
  %2167 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%842)
  %2168 : Tensor = prim::GetAttr[name="weight"](%2167)
  %2169 : Tensor? = prim::GetAttr[name="bias"](%2167)
  %2170 : int[] = prim::ListConstruct(%27, %27)
  %2171 : int[] = prim::ListConstruct(%24, %24)
  %2172 : int[] = prim::ListConstruct(%27, %27)
  %out.133 : Tensor = aten::conv2d(%out.132, %2168, %2169, %2170, %2171, %2172, %27) # torch/nn/modules/conv.py:415:15
  %2174 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%842)
  %2175 : int = aten::dim(%out.133) # torch/nn/modules/batchnorm.py:276:11
  %2176 : bool = aten::ne(%2175, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2176) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2177 : bool = prim::GetAttr[name="training"](%2174)
   = prim::If(%2177) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2178 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2174)
      %2179 : Tensor = aten::add(%2178, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2174, %2179)
      -> ()
    block1():
      -> ()
  %2180 : bool = prim::GetAttr[name="training"](%2174)
  %2181 : Tensor = prim::GetAttr[name="running_mean"](%2174)
  %2182 : Tensor = prim::GetAttr[name="running_var"](%2174)
  %2183 : Tensor = prim::GetAttr[name="weight"](%2174)
  %2184 : Tensor = prim::GetAttr[name="bias"](%2174)
   = prim::If(%2180) # torch/nn/functional.py:2011:4
    block0():
      %2185 : int[] = aten::size(%out.133) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%2185, %24) # torch/nn/functional.py:1991:17
      %2187 : int = aten::len(%2185) # torch/nn/functional.py:1992:19
      %2188 : int = aten::sub(%2187, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%2188, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %2192 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %2193 : int = aten::__getitem__(%2185, %2192) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %2193) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %2195 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2195) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.134 : Tensor = aten::batch_norm(%out.133, %2183, %2184, %2181, %2182, %2180, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.135 : Tensor = aten::add_(%out.134, %input.27, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.29 : Tensor = aten::relu_(%out.135) # torch/nn/functional.py:1117:17
  %2199 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%843)
  %2200 : Tensor = prim::GetAttr[name="weight"](%2199)
  %2201 : Tensor? = prim::GetAttr[name="bias"](%2199)
  %2202 : int[] = prim::ListConstruct(%27, %27)
  %2203 : int[] = prim::ListConstruct(%24, %24)
  %2204 : int[] = prim::ListConstruct(%27, %27)
  %out.145 : Tensor = aten::conv2d(%input.29, %2200, %2201, %2202, %2203, %2204, %27) # torch/nn/modules/conv.py:415:15
  %2206 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%843)
  %2207 : int = aten::dim(%out.145) # torch/nn/modules/batchnorm.py:276:11
  %2208 : bool = aten::ne(%2207, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2208) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2209 : bool = prim::GetAttr[name="training"](%2206)
   = prim::If(%2209) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2210 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2206)
      %2211 : Tensor = aten::add(%2210, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2206, %2211)
      -> ()
    block1():
      -> ()
  %2212 : bool = prim::GetAttr[name="training"](%2206)
  %2213 : Tensor = prim::GetAttr[name="running_mean"](%2206)
  %2214 : Tensor = prim::GetAttr[name="running_var"](%2206)
  %2215 : Tensor = prim::GetAttr[name="weight"](%2206)
  %2216 : Tensor = prim::GetAttr[name="bias"](%2206)
   = prim::If(%2212) # torch/nn/functional.py:2011:4
    block0():
      %2217 : int[] = aten::size(%out.145) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%2217, %24) # torch/nn/functional.py:1991:17
      %2219 : int = aten::len(%2217) # torch/nn/functional.py:1992:19
      %2220 : int = aten::sub(%2219, %26) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%2220, %25, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %2224 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
          %2225 : int = aten::__getitem__(%2217, %2224) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %2225) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.187)
      %2227 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2227) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.137 : Tensor = aten::batch_norm(%out.145, %2215, %2216, %2213, %2214, %2212, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.138 : Tensor = aten::relu_(%out.137) # torch/nn/functional.py:1117:17
  %2230 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%843)
  %2231 : Tensor = prim::GetAttr[name="weight"](%2230)
  %2232 : Tensor? = prim::GetAttr[name="bias"](%2230)
  %2233 : int[] = prim::ListConstruct(%27, %27)
  %2234 : int[] = prim::ListConstruct(%26, %26)
  %2235 : int[] = prim::ListConstruct(%26, %26)
  %out.139 : Tensor = aten::conv2d(%out.138, %2231, %2232, %2233, %2234, %2235, %27) # torch/nn/modules/conv.py:415:15
  %2237 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%843)
  %2238 : int = aten::dim(%out.139) # torch/nn/modules/batchnorm.py:276:11
  %2239 : bool = aten::ne(%2238, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2239) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2240 : bool = prim::GetAttr[name="training"](%2237)
   = prim::If(%2240) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2241 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2237)
      %2242 : Tensor = aten::add(%2241, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2237, %2242)
      -> ()
    block1():
      -> ()
  %2243 : bool = prim::GetAttr[name="training"](%2237)
  %2244 : Tensor = prim::GetAttr[name="running_mean"](%2237)
  %2245 : Tensor = prim::GetAttr[name="running_var"](%2237)
  %2246 : Tensor = prim::GetAttr[name="weight"](%2237)
  %2247 : Tensor = prim::GetAttr[name="bias"](%2237)
   = prim::If(%2243) # torch/nn/functional.py:2011:4
    block0():
      %2248 : int[] = aten::size(%out.139) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%2248, %24) # torch/nn/functional.py:1991:17
      %2250 : int = aten::len(%2248) # torch/nn/functional.py:1992:19
      %2251 : int = aten::sub(%2250, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%2251, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %2255 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %2256 : int = aten::__getitem__(%2248, %2255) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %2256) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %2258 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2258) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.140 : Tensor = aten::batch_norm(%out.139, %2246, %2247, %2244, %2245, %2243, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.141 : Tensor = aten::relu_(%out.140) # torch/nn/functional.py:1117:17
  %2261 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%843)
  %2262 : Tensor = prim::GetAttr[name="weight"](%2261)
  %2263 : Tensor? = prim::GetAttr[name="bias"](%2261)
  %2264 : int[] = prim::ListConstruct(%27, %27)
  %2265 : int[] = prim::ListConstruct(%24, %24)
  %2266 : int[] = prim::ListConstruct(%27, %27)
  %out.142 : Tensor = aten::conv2d(%out.141, %2262, %2263, %2264, %2265, %2266, %27) # torch/nn/modules/conv.py:415:15
  %2268 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%843)
  %2269 : int = aten::dim(%out.142) # torch/nn/modules/batchnorm.py:276:11
  %2270 : bool = aten::ne(%2269, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2270) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2271 : bool = prim::GetAttr[name="training"](%2268)
   = prim::If(%2271) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2272 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2268)
      %2273 : Tensor = aten::add(%2272, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2268, %2273)
      -> ()
    block1():
      -> ()
  %2274 : bool = prim::GetAttr[name="training"](%2268)
  %2275 : Tensor = prim::GetAttr[name="running_mean"](%2268)
  %2276 : Tensor = prim::GetAttr[name="running_var"](%2268)
  %2277 : Tensor = prim::GetAttr[name="weight"](%2268)
  %2278 : Tensor = prim::GetAttr[name="bias"](%2268)
   = prim::If(%2274) # torch/nn/functional.py:2011:4
    block0():
      %2279 : int[] = aten::size(%out.142) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%2279, %24) # torch/nn/functional.py:1991:17
      %2281 : int = aten::len(%2279) # torch/nn/functional.py:1992:19
      %2282 : int = aten::sub(%2281, %26) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%2282, %25, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %2286 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
          %2287 : int = aten::__getitem__(%2279, %2286) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %2287) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.195)
      %2289 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2289) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.143 : Tensor = aten::batch_norm(%out.142, %2277, %2278, %2275, %2276, %2274, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.144 : Tensor = aten::add_(%out.143, %input.29, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.31 : Tensor = aten::relu_(%out.144) # torch/nn/functional.py:1117:17
  %2293 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%844)
  %2294 : Tensor = prim::GetAttr[name="weight"](%2293)
  %2295 : Tensor? = prim::GetAttr[name="bias"](%2293)
  %2296 : int[] = prim::ListConstruct(%27, %27)
  %2297 : int[] = prim::ListConstruct(%24, %24)
  %2298 : int[] = prim::ListConstruct(%27, %27)
  %out.154 : Tensor = aten::conv2d(%input.31, %2294, %2295, %2296, %2297, %2298, %27) # torch/nn/modules/conv.py:415:15
  %2300 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%844)
  %2301 : int = aten::dim(%out.154) # torch/nn/modules/batchnorm.py:276:11
  %2302 : bool = aten::ne(%2301, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2302) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2303 : bool = prim::GetAttr[name="training"](%2300)
   = prim::If(%2303) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2304 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2300)
      %2305 : Tensor = aten::add(%2304, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2300, %2305)
      -> ()
    block1():
      -> ()
  %2306 : bool = prim::GetAttr[name="training"](%2300)
  %2307 : Tensor = prim::GetAttr[name="running_mean"](%2300)
  %2308 : Tensor = prim::GetAttr[name="running_var"](%2300)
  %2309 : Tensor = prim::GetAttr[name="weight"](%2300)
  %2310 : Tensor = prim::GetAttr[name="bias"](%2300)
   = prim::If(%2306) # torch/nn/functional.py:2011:4
    block0():
      %2311 : int[] = aten::size(%out.154) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%2311, %24) # torch/nn/functional.py:1991:17
      %2313 : int = aten::len(%2311) # torch/nn/functional.py:1992:19
      %2314 : int = aten::sub(%2313, %26) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%2314, %25, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %2318 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
          %2319 : int = aten::__getitem__(%2311, %2318) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %2319) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.199)
      %2321 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2321) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.146 : Tensor = aten::batch_norm(%out.154, %2309, %2310, %2307, %2308, %2306, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.147 : Tensor = aten::relu_(%out.146) # torch/nn/functional.py:1117:17
  %2324 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%844)
  %2325 : Tensor = prim::GetAttr[name="weight"](%2324)
  %2326 : Tensor? = prim::GetAttr[name="bias"](%2324)
  %2327 : int[] = prim::ListConstruct(%27, %27)
  %2328 : int[] = prim::ListConstruct(%26, %26)
  %2329 : int[] = prim::ListConstruct(%26, %26)
  %out.148 : Tensor = aten::conv2d(%out.147, %2325, %2326, %2327, %2328, %2329, %27) # torch/nn/modules/conv.py:415:15
  %2331 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%844)
  %2332 : int = aten::dim(%out.148) # torch/nn/modules/batchnorm.py:276:11
  %2333 : bool = aten::ne(%2332, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2333) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2334 : bool = prim::GetAttr[name="training"](%2331)
   = prim::If(%2334) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2335 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2331)
      %2336 : Tensor = aten::add(%2335, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2331, %2336)
      -> ()
    block1():
      -> ()
  %2337 : bool = prim::GetAttr[name="training"](%2331)
  %2338 : Tensor = prim::GetAttr[name="running_mean"](%2331)
  %2339 : Tensor = prim::GetAttr[name="running_var"](%2331)
  %2340 : Tensor = prim::GetAttr[name="weight"](%2331)
  %2341 : Tensor = prim::GetAttr[name="bias"](%2331)
   = prim::If(%2337) # torch/nn/functional.py:2011:4
    block0():
      %2342 : int[] = aten::size(%out.148) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%2342, %24) # torch/nn/functional.py:1991:17
      %2344 : int = aten::len(%2342) # torch/nn/functional.py:1992:19
      %2345 : int = aten::sub(%2344, %26) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%2345, %25, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %2349 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
          %2350 : int = aten::__getitem__(%2342, %2349) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %2350) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.203)
      %2352 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2352) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.149 : Tensor = aten::batch_norm(%out.148, %2340, %2341, %2338, %2339, %2337, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.150 : Tensor = aten::relu_(%out.149) # torch/nn/functional.py:1117:17
  %2355 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%844)
  %2356 : Tensor = prim::GetAttr[name="weight"](%2355)
  %2357 : Tensor? = prim::GetAttr[name="bias"](%2355)
  %2358 : int[] = prim::ListConstruct(%27, %27)
  %2359 : int[] = prim::ListConstruct(%24, %24)
  %2360 : int[] = prim::ListConstruct(%27, %27)
  %out.151 : Tensor = aten::conv2d(%out.150, %2356, %2357, %2358, %2359, %2360, %27) # torch/nn/modules/conv.py:415:15
  %2362 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%844)
  %2363 : int = aten::dim(%out.151) # torch/nn/modules/batchnorm.py:276:11
  %2364 : bool = aten::ne(%2363, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2364) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2365 : bool = prim::GetAttr[name="training"](%2362)
   = prim::If(%2365) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2366 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2362)
      %2367 : Tensor = aten::add(%2366, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2362, %2367)
      -> ()
    block1():
      -> ()
  %2368 : bool = prim::GetAttr[name="training"](%2362)
  %2369 : Tensor = prim::GetAttr[name="running_mean"](%2362)
  %2370 : Tensor = prim::GetAttr[name="running_var"](%2362)
  %2371 : Tensor = prim::GetAttr[name="weight"](%2362)
  %2372 : Tensor = prim::GetAttr[name="bias"](%2362)
   = prim::If(%2368) # torch/nn/functional.py:2011:4
    block0():
      %2373 : int[] = aten::size(%out.151) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%2373, %24) # torch/nn/functional.py:1991:17
      %2375 : int = aten::len(%2373) # torch/nn/functional.py:1992:19
      %2376 : int = aten::sub(%2375, %26) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%2376, %25, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %2380 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
          %2381 : int = aten::__getitem__(%2373, %2380) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %2381) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.207)
      %2383 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2383) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.152 : Tensor = aten::batch_norm(%out.151, %2371, %2372, %2369, %2370, %2368, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.153 : Tensor = aten::add_(%out.152, %input.31, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.33 : Tensor = aten::relu_(%out.153) # torch/nn/functional.py:1117:17
  %2387 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%845)
  %2388 : Tensor = prim::GetAttr[name="weight"](%2387)
  %2389 : Tensor? = prim::GetAttr[name="bias"](%2387)
  %2390 : int[] = prim::ListConstruct(%27, %27)
  %2391 : int[] = prim::ListConstruct(%24, %24)
  %2392 : int[] = prim::ListConstruct(%27, %27)
  %out.163 : Tensor = aten::conv2d(%input.33, %2388, %2389, %2390, %2391, %2392, %27) # torch/nn/modules/conv.py:415:15
  %2394 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%845)
  %2395 : int = aten::dim(%out.163) # torch/nn/modules/batchnorm.py:276:11
  %2396 : bool = aten::ne(%2395, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2396) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2397 : bool = prim::GetAttr[name="training"](%2394)
   = prim::If(%2397) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2398 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2394)
      %2399 : Tensor = aten::add(%2398, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2394, %2399)
      -> ()
    block1():
      -> ()
  %2400 : bool = prim::GetAttr[name="training"](%2394)
  %2401 : Tensor = prim::GetAttr[name="running_mean"](%2394)
  %2402 : Tensor = prim::GetAttr[name="running_var"](%2394)
  %2403 : Tensor = prim::GetAttr[name="weight"](%2394)
  %2404 : Tensor = prim::GetAttr[name="bias"](%2394)
   = prim::If(%2400) # torch/nn/functional.py:2011:4
    block0():
      %2405 : int[] = aten::size(%out.163) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%2405, %24) # torch/nn/functional.py:1991:17
      %2407 : int = aten::len(%2405) # torch/nn/functional.py:1992:19
      %2408 : int = aten::sub(%2407, %26) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%2408, %25, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %2412 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
          %2413 : int = aten::__getitem__(%2405, %2412) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %2413) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.211)
      %2415 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2415) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.155 : Tensor = aten::batch_norm(%out.163, %2403, %2404, %2401, %2402, %2400, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.156 : Tensor = aten::relu_(%out.155) # torch/nn/functional.py:1117:17
  %2418 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%845)
  %2419 : Tensor = prim::GetAttr[name="weight"](%2418)
  %2420 : Tensor? = prim::GetAttr[name="bias"](%2418)
  %2421 : int[] = prim::ListConstruct(%27, %27)
  %2422 : int[] = prim::ListConstruct(%26, %26)
  %2423 : int[] = prim::ListConstruct(%26, %26)
  %out.157 : Tensor = aten::conv2d(%out.156, %2419, %2420, %2421, %2422, %2423, %27) # torch/nn/modules/conv.py:415:15
  %2425 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%845)
  %2426 : int = aten::dim(%out.157) # torch/nn/modules/batchnorm.py:276:11
  %2427 : bool = aten::ne(%2426, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2427) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2428 : bool = prim::GetAttr[name="training"](%2425)
   = prim::If(%2428) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2429 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2425)
      %2430 : Tensor = aten::add(%2429, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2425, %2430)
      -> ()
    block1():
      -> ()
  %2431 : bool = prim::GetAttr[name="training"](%2425)
  %2432 : Tensor = prim::GetAttr[name="running_mean"](%2425)
  %2433 : Tensor = prim::GetAttr[name="running_var"](%2425)
  %2434 : Tensor = prim::GetAttr[name="weight"](%2425)
  %2435 : Tensor = prim::GetAttr[name="bias"](%2425)
   = prim::If(%2431) # torch/nn/functional.py:2011:4
    block0():
      %2436 : int[] = aten::size(%out.157) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%2436, %24) # torch/nn/functional.py:1991:17
      %2438 : int = aten::len(%2436) # torch/nn/functional.py:1992:19
      %2439 : int = aten::sub(%2438, %26) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%2439, %25, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %2443 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
          %2444 : int = aten::__getitem__(%2436, %2443) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %2444) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.215)
      %2446 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2446) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.158 : Tensor = aten::batch_norm(%out.157, %2434, %2435, %2432, %2433, %2431, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.159 : Tensor = aten::relu_(%out.158) # torch/nn/functional.py:1117:17
  %2449 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%845)
  %2450 : Tensor = prim::GetAttr[name="weight"](%2449)
  %2451 : Tensor? = prim::GetAttr[name="bias"](%2449)
  %2452 : int[] = prim::ListConstruct(%27, %27)
  %2453 : int[] = prim::ListConstruct(%24, %24)
  %2454 : int[] = prim::ListConstruct(%27, %27)
  %out.160 : Tensor = aten::conv2d(%out.159, %2450, %2451, %2452, %2453, %2454, %27) # torch/nn/modules/conv.py:415:15
  %2456 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%845)
  %2457 : int = aten::dim(%out.160) # torch/nn/modules/batchnorm.py:276:11
  %2458 : bool = aten::ne(%2457, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2458) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2459 : bool = prim::GetAttr[name="training"](%2456)
   = prim::If(%2459) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2460 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2456)
      %2461 : Tensor = aten::add(%2460, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2456, %2461)
      -> ()
    block1():
      -> ()
  %2462 : bool = prim::GetAttr[name="training"](%2456)
  %2463 : Tensor = prim::GetAttr[name="running_mean"](%2456)
  %2464 : Tensor = prim::GetAttr[name="running_var"](%2456)
  %2465 : Tensor = prim::GetAttr[name="weight"](%2456)
  %2466 : Tensor = prim::GetAttr[name="bias"](%2456)
   = prim::If(%2462) # torch/nn/functional.py:2011:4
    block0():
      %2467 : int[] = aten::size(%out.160) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%2467, %24) # torch/nn/functional.py:1991:17
      %2469 : int = aten::len(%2467) # torch/nn/functional.py:1992:19
      %2470 : int = aten::sub(%2469, %26) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%2470, %25, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %2474 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
          %2475 : int = aten::__getitem__(%2467, %2474) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %2475) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.219)
      %2477 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2477) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.161 : Tensor = aten::batch_norm(%out.160, %2465, %2466, %2463, %2464, %2462, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.162 : Tensor = aten::add_(%out.161, %input.33, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.35 : Tensor = aten::relu_(%out.162) # torch/nn/functional.py:1117:17
  %2481 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%846)
  %2482 : Tensor = prim::GetAttr[name="weight"](%2481)
  %2483 : Tensor? = prim::GetAttr[name="bias"](%2481)
  %2484 : int[] = prim::ListConstruct(%27, %27)
  %2485 : int[] = prim::ListConstruct(%24, %24)
  %2486 : int[] = prim::ListConstruct(%27, %27)
  %out.172 : Tensor = aten::conv2d(%input.35, %2482, %2483, %2484, %2485, %2486, %27) # torch/nn/modules/conv.py:415:15
  %2488 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%846)
  %2489 : int = aten::dim(%out.172) # torch/nn/modules/batchnorm.py:276:11
  %2490 : bool = aten::ne(%2489, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2490) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2491 : bool = prim::GetAttr[name="training"](%2488)
   = prim::If(%2491) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2492 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2488)
      %2493 : Tensor = aten::add(%2492, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2488, %2493)
      -> ()
    block1():
      -> ()
  %2494 : bool = prim::GetAttr[name="training"](%2488)
  %2495 : Tensor = prim::GetAttr[name="running_mean"](%2488)
  %2496 : Tensor = prim::GetAttr[name="running_var"](%2488)
  %2497 : Tensor = prim::GetAttr[name="weight"](%2488)
  %2498 : Tensor = prim::GetAttr[name="bias"](%2488)
   = prim::If(%2494) # torch/nn/functional.py:2011:4
    block0():
      %2499 : int[] = aten::size(%out.172) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%2499, %24) # torch/nn/functional.py:1991:17
      %2501 : int = aten::len(%2499) # torch/nn/functional.py:1992:19
      %2502 : int = aten::sub(%2501, %26) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%2502, %25, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %2506 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
          %2507 : int = aten::__getitem__(%2499, %2506) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %2507) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.223)
      %2509 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2509) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.164 : Tensor = aten::batch_norm(%out.172, %2497, %2498, %2495, %2496, %2494, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.165 : Tensor = aten::relu_(%out.164) # torch/nn/functional.py:1117:17
  %2512 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%846)
  %2513 : Tensor = prim::GetAttr[name="weight"](%2512)
  %2514 : Tensor? = prim::GetAttr[name="bias"](%2512)
  %2515 : int[] = prim::ListConstruct(%27, %27)
  %2516 : int[] = prim::ListConstruct(%26, %26)
  %2517 : int[] = prim::ListConstruct(%26, %26)
  %out.166 : Tensor = aten::conv2d(%out.165, %2513, %2514, %2515, %2516, %2517, %27) # torch/nn/modules/conv.py:415:15
  %2519 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%846)
  %2520 : int = aten::dim(%out.166) # torch/nn/modules/batchnorm.py:276:11
  %2521 : bool = aten::ne(%2520, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2521) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2522 : bool = prim::GetAttr[name="training"](%2519)
   = prim::If(%2522) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2523 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2519)
      %2524 : Tensor = aten::add(%2523, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2519, %2524)
      -> ()
    block1():
      -> ()
  %2525 : bool = prim::GetAttr[name="training"](%2519)
  %2526 : Tensor = prim::GetAttr[name="running_mean"](%2519)
  %2527 : Tensor = prim::GetAttr[name="running_var"](%2519)
  %2528 : Tensor = prim::GetAttr[name="weight"](%2519)
  %2529 : Tensor = prim::GetAttr[name="bias"](%2519)
   = prim::If(%2525) # torch/nn/functional.py:2011:4
    block0():
      %2530 : int[] = aten::size(%out.166) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%2530, %24) # torch/nn/functional.py:1991:17
      %2532 : int = aten::len(%2530) # torch/nn/functional.py:1992:19
      %2533 : int = aten::sub(%2532, %26) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%2533, %25, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %2537 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
          %2538 : int = aten::__getitem__(%2530, %2537) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %2538) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.227)
      %2540 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2540) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.167 : Tensor = aten::batch_norm(%out.166, %2528, %2529, %2526, %2527, %2525, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.168 : Tensor = aten::relu_(%out.167) # torch/nn/functional.py:1117:17
  %2543 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%846)
  %2544 : Tensor = prim::GetAttr[name="weight"](%2543)
  %2545 : Tensor? = prim::GetAttr[name="bias"](%2543)
  %2546 : int[] = prim::ListConstruct(%27, %27)
  %2547 : int[] = prim::ListConstruct(%24, %24)
  %2548 : int[] = prim::ListConstruct(%27, %27)
  %out.169 : Tensor = aten::conv2d(%out.168, %2544, %2545, %2546, %2547, %2548, %27) # torch/nn/modules/conv.py:415:15
  %2550 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%846)
  %2551 : int = aten::dim(%out.169) # torch/nn/modules/batchnorm.py:276:11
  %2552 : bool = aten::ne(%2551, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2552) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2553 : bool = prim::GetAttr[name="training"](%2550)
   = prim::If(%2553) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2554 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2550)
      %2555 : Tensor = aten::add(%2554, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2550, %2555)
      -> ()
    block1():
      -> ()
  %2556 : bool = prim::GetAttr[name="training"](%2550)
  %2557 : Tensor = prim::GetAttr[name="running_mean"](%2550)
  %2558 : Tensor = prim::GetAttr[name="running_var"](%2550)
  %2559 : Tensor = prim::GetAttr[name="weight"](%2550)
  %2560 : Tensor = prim::GetAttr[name="bias"](%2550)
   = prim::If(%2556) # torch/nn/functional.py:2011:4
    block0():
      %2561 : int[] = aten::size(%out.169) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%2561, %24) # torch/nn/functional.py:1991:17
      %2563 : int = aten::len(%2561) # torch/nn/functional.py:1992:19
      %2564 : int = aten::sub(%2563, %26) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%2564, %25, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %2568 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
          %2569 : int = aten::__getitem__(%2561, %2568) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %2569) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.231)
      %2571 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2571) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.170 : Tensor = aten::batch_norm(%out.169, %2559, %2560, %2557, %2558, %2556, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.171 : Tensor = aten::add_(%out.170, %input.35, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.37 : Tensor = aten::relu_(%out.171) # torch/nn/functional.py:1117:17
  %2575 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%847)
  %2576 : Tensor = prim::GetAttr[name="weight"](%2575)
  %2577 : Tensor? = prim::GetAttr[name="bias"](%2575)
  %2578 : int[] = prim::ListConstruct(%27, %27)
  %2579 : int[] = prim::ListConstruct(%24, %24)
  %2580 : int[] = prim::ListConstruct(%27, %27)
  %out.181 : Tensor = aten::conv2d(%input.37, %2576, %2577, %2578, %2579, %2580, %27) # torch/nn/modules/conv.py:415:15
  %2582 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%847)
  %2583 : int = aten::dim(%out.181) # torch/nn/modules/batchnorm.py:276:11
  %2584 : bool = aten::ne(%2583, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2584) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2585 : bool = prim::GetAttr[name="training"](%2582)
   = prim::If(%2585) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2586 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2582)
      %2587 : Tensor = aten::add(%2586, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2582, %2587)
      -> ()
    block1():
      -> ()
  %2588 : bool = prim::GetAttr[name="training"](%2582)
  %2589 : Tensor = prim::GetAttr[name="running_mean"](%2582)
  %2590 : Tensor = prim::GetAttr[name="running_var"](%2582)
  %2591 : Tensor = prim::GetAttr[name="weight"](%2582)
  %2592 : Tensor = prim::GetAttr[name="bias"](%2582)
   = prim::If(%2588) # torch/nn/functional.py:2011:4
    block0():
      %2593 : int[] = aten::size(%out.181) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%2593, %24) # torch/nn/functional.py:1991:17
      %2595 : int = aten::len(%2593) # torch/nn/functional.py:1992:19
      %2596 : int = aten::sub(%2595, %26) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%2596, %25, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %2600 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
          %2601 : int = aten::__getitem__(%2593, %2600) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %2601) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.235)
      %2603 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2603) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.173 : Tensor = aten::batch_norm(%out.181, %2591, %2592, %2589, %2590, %2588, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.174 : Tensor = aten::relu_(%out.173) # torch/nn/functional.py:1117:17
  %2606 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%847)
  %2607 : Tensor = prim::GetAttr[name="weight"](%2606)
  %2608 : Tensor? = prim::GetAttr[name="bias"](%2606)
  %2609 : int[] = prim::ListConstruct(%27, %27)
  %2610 : int[] = prim::ListConstruct(%26, %26)
  %2611 : int[] = prim::ListConstruct(%26, %26)
  %out.175 : Tensor = aten::conv2d(%out.174, %2607, %2608, %2609, %2610, %2611, %27) # torch/nn/modules/conv.py:415:15
  %2613 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%847)
  %2614 : int = aten::dim(%out.175) # torch/nn/modules/batchnorm.py:276:11
  %2615 : bool = aten::ne(%2614, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2615) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2616 : bool = prim::GetAttr[name="training"](%2613)
   = prim::If(%2616) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2617 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2613)
      %2618 : Tensor = aten::add(%2617, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2613, %2618)
      -> ()
    block1():
      -> ()
  %2619 : bool = prim::GetAttr[name="training"](%2613)
  %2620 : Tensor = prim::GetAttr[name="running_mean"](%2613)
  %2621 : Tensor = prim::GetAttr[name="running_var"](%2613)
  %2622 : Tensor = prim::GetAttr[name="weight"](%2613)
  %2623 : Tensor = prim::GetAttr[name="bias"](%2613)
   = prim::If(%2619) # torch/nn/functional.py:2011:4
    block0():
      %2624 : int[] = aten::size(%out.175) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%2624, %24) # torch/nn/functional.py:1991:17
      %2626 : int = aten::len(%2624) # torch/nn/functional.py:1992:19
      %2627 : int = aten::sub(%2626, %26) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%2627, %25, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %2631 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
          %2632 : int = aten::__getitem__(%2624, %2631) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %2632) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.239)
      %2634 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2634) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.176 : Tensor = aten::batch_norm(%out.175, %2622, %2623, %2620, %2621, %2619, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.177 : Tensor = aten::relu_(%out.176) # torch/nn/functional.py:1117:17
  %2637 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%847)
  %2638 : Tensor = prim::GetAttr[name="weight"](%2637)
  %2639 : Tensor? = prim::GetAttr[name="bias"](%2637)
  %2640 : int[] = prim::ListConstruct(%27, %27)
  %2641 : int[] = prim::ListConstruct(%24, %24)
  %2642 : int[] = prim::ListConstruct(%27, %27)
  %out.178 : Tensor = aten::conv2d(%out.177, %2638, %2639, %2640, %2641, %2642, %27) # torch/nn/modules/conv.py:415:15
  %2644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%847)
  %2645 : int = aten::dim(%out.178) # torch/nn/modules/batchnorm.py:276:11
  %2646 : bool = aten::ne(%2645, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2646) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2647 : bool = prim::GetAttr[name="training"](%2644)
   = prim::If(%2647) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2648 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2644)
      %2649 : Tensor = aten::add(%2648, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2644, %2649)
      -> ()
    block1():
      -> ()
  %2650 : bool = prim::GetAttr[name="training"](%2644)
  %2651 : Tensor = prim::GetAttr[name="running_mean"](%2644)
  %2652 : Tensor = prim::GetAttr[name="running_var"](%2644)
  %2653 : Tensor = prim::GetAttr[name="weight"](%2644)
  %2654 : Tensor = prim::GetAttr[name="bias"](%2644)
   = prim::If(%2650) # torch/nn/functional.py:2011:4
    block0():
      %2655 : int[] = aten::size(%out.178) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%2655, %24) # torch/nn/functional.py:1991:17
      %2657 : int = aten::len(%2655) # torch/nn/functional.py:1992:19
      %2658 : int = aten::sub(%2657, %26) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%2658, %25, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %2662 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
          %2663 : int = aten::__getitem__(%2655, %2662) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %2663) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.243)
      %2665 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2665) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.179 : Tensor = aten::batch_norm(%out.178, %2653, %2654, %2651, %2652, %2650, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.180 : Tensor = aten::add_(%out.179, %input.37, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.39 : Tensor = aten::relu_(%out.180) # torch/nn/functional.py:1117:17
  %2669 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%848)
  %2670 : Tensor = prim::GetAttr[name="weight"](%2669)
  %2671 : Tensor? = prim::GetAttr[name="bias"](%2669)
  %2672 : int[] = prim::ListConstruct(%27, %27)
  %2673 : int[] = prim::ListConstruct(%24, %24)
  %2674 : int[] = prim::ListConstruct(%27, %27)
  %out.190 : Tensor = aten::conv2d(%input.39, %2670, %2671, %2672, %2673, %2674, %27) # torch/nn/modules/conv.py:415:15
  %2676 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%848)
  %2677 : int = aten::dim(%out.190) # torch/nn/modules/batchnorm.py:276:11
  %2678 : bool = aten::ne(%2677, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2678) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2679 : bool = prim::GetAttr[name="training"](%2676)
   = prim::If(%2679) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2680 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2676)
      %2681 : Tensor = aten::add(%2680, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2676, %2681)
      -> ()
    block1():
      -> ()
  %2682 : bool = prim::GetAttr[name="training"](%2676)
  %2683 : Tensor = prim::GetAttr[name="running_mean"](%2676)
  %2684 : Tensor = prim::GetAttr[name="running_var"](%2676)
  %2685 : Tensor = prim::GetAttr[name="weight"](%2676)
  %2686 : Tensor = prim::GetAttr[name="bias"](%2676)
   = prim::If(%2682) # torch/nn/functional.py:2011:4
    block0():
      %2687 : int[] = aten::size(%out.190) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%2687, %24) # torch/nn/functional.py:1991:17
      %2689 : int = aten::len(%2687) # torch/nn/functional.py:1992:19
      %2690 : int = aten::sub(%2689, %26) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%2690, %25, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %2694 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
          %2695 : int = aten::__getitem__(%2687, %2694) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %2695) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.247)
      %2697 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2697) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.182 : Tensor = aten::batch_norm(%out.190, %2685, %2686, %2683, %2684, %2682, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.183 : Tensor = aten::relu_(%out.182) # torch/nn/functional.py:1117:17
  %2700 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%848)
  %2701 : Tensor = prim::GetAttr[name="weight"](%2700)
  %2702 : Tensor? = prim::GetAttr[name="bias"](%2700)
  %2703 : int[] = prim::ListConstruct(%27, %27)
  %2704 : int[] = prim::ListConstruct(%26, %26)
  %2705 : int[] = prim::ListConstruct(%26, %26)
  %out.184 : Tensor = aten::conv2d(%out.183, %2701, %2702, %2703, %2704, %2705, %27) # torch/nn/modules/conv.py:415:15
  %2707 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%848)
  %2708 : int = aten::dim(%out.184) # torch/nn/modules/batchnorm.py:276:11
  %2709 : bool = aten::ne(%2708, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2709) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2710 : bool = prim::GetAttr[name="training"](%2707)
   = prim::If(%2710) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2711 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2707)
      %2712 : Tensor = aten::add(%2711, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2707, %2712)
      -> ()
    block1():
      -> ()
  %2713 : bool = prim::GetAttr[name="training"](%2707)
  %2714 : Tensor = prim::GetAttr[name="running_mean"](%2707)
  %2715 : Tensor = prim::GetAttr[name="running_var"](%2707)
  %2716 : Tensor = prim::GetAttr[name="weight"](%2707)
  %2717 : Tensor = prim::GetAttr[name="bias"](%2707)
   = prim::If(%2713) # torch/nn/functional.py:2011:4
    block0():
      %2718 : int[] = aten::size(%out.184) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%2718, %24) # torch/nn/functional.py:1991:17
      %2720 : int = aten::len(%2718) # torch/nn/functional.py:1992:19
      %2721 : int = aten::sub(%2720, %26) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%2721, %25, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %2725 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
          %2726 : int = aten::__getitem__(%2718, %2725) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %2726) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.251)
      %2728 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2728) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.185 : Tensor = aten::batch_norm(%out.184, %2716, %2717, %2714, %2715, %2713, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.186 : Tensor = aten::relu_(%out.185) # torch/nn/functional.py:1117:17
  %2731 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%848)
  %2732 : Tensor = prim::GetAttr[name="weight"](%2731)
  %2733 : Tensor? = prim::GetAttr[name="bias"](%2731)
  %2734 : int[] = prim::ListConstruct(%27, %27)
  %2735 : int[] = prim::ListConstruct(%24, %24)
  %2736 : int[] = prim::ListConstruct(%27, %27)
  %out.187 : Tensor = aten::conv2d(%out.186, %2732, %2733, %2734, %2735, %2736, %27) # torch/nn/modules/conv.py:415:15
  %2738 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%848)
  %2739 : int = aten::dim(%out.187) # torch/nn/modules/batchnorm.py:276:11
  %2740 : bool = aten::ne(%2739, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2740) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2741 : bool = prim::GetAttr[name="training"](%2738)
   = prim::If(%2741) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2742 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2738)
      %2743 : Tensor = aten::add(%2742, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2738, %2743)
      -> ()
    block1():
      -> ()
  %2744 : bool = prim::GetAttr[name="training"](%2738)
  %2745 : Tensor = prim::GetAttr[name="running_mean"](%2738)
  %2746 : Tensor = prim::GetAttr[name="running_var"](%2738)
  %2747 : Tensor = prim::GetAttr[name="weight"](%2738)
  %2748 : Tensor = prim::GetAttr[name="bias"](%2738)
   = prim::If(%2744) # torch/nn/functional.py:2011:4
    block0():
      %2749 : int[] = aten::size(%out.187) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%2749, %24) # torch/nn/functional.py:1991:17
      %2751 : int = aten::len(%2749) # torch/nn/functional.py:1992:19
      %2752 : int = aten::sub(%2751, %26) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%2752, %25, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %2756 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
          %2757 : int = aten::__getitem__(%2749, %2756) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %2757) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.255)
      %2759 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2759) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.188 : Tensor = aten::batch_norm(%out.187, %2747, %2748, %2745, %2746, %2744, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.189 : Tensor = aten::add_(%out.188, %input.39, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.41 : Tensor = aten::relu_(%out.189) # torch/nn/functional.py:1117:17
  %2763 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%849)
  %2764 : Tensor = prim::GetAttr[name="weight"](%2763)
  %2765 : Tensor? = prim::GetAttr[name="bias"](%2763)
  %2766 : int[] = prim::ListConstruct(%27, %27)
  %2767 : int[] = prim::ListConstruct(%24, %24)
  %2768 : int[] = prim::ListConstruct(%27, %27)
  %out.199 : Tensor = aten::conv2d(%input.41, %2764, %2765, %2766, %2767, %2768, %27) # torch/nn/modules/conv.py:415:15
  %2770 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%849)
  %2771 : int = aten::dim(%out.199) # torch/nn/modules/batchnorm.py:276:11
  %2772 : bool = aten::ne(%2771, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2772) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2773 : bool = prim::GetAttr[name="training"](%2770)
   = prim::If(%2773) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2774 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2770)
      %2775 : Tensor = aten::add(%2774, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2770, %2775)
      -> ()
    block1():
      -> ()
  %2776 : bool = prim::GetAttr[name="training"](%2770)
  %2777 : Tensor = prim::GetAttr[name="running_mean"](%2770)
  %2778 : Tensor = prim::GetAttr[name="running_var"](%2770)
  %2779 : Tensor = prim::GetAttr[name="weight"](%2770)
  %2780 : Tensor = prim::GetAttr[name="bias"](%2770)
   = prim::If(%2776) # torch/nn/functional.py:2011:4
    block0():
      %2781 : int[] = aten::size(%out.199) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%2781, %24) # torch/nn/functional.py:1991:17
      %2783 : int = aten::len(%2781) # torch/nn/functional.py:1992:19
      %2784 : int = aten::sub(%2783, %26) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%2784, %25, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %2788 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
          %2789 : int = aten::__getitem__(%2781, %2788) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %2789) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.259)
      %2791 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2791) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.191 : Tensor = aten::batch_norm(%out.199, %2779, %2780, %2777, %2778, %2776, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.192 : Tensor = aten::relu_(%out.191) # torch/nn/functional.py:1117:17
  %2794 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%849)
  %2795 : Tensor = prim::GetAttr[name="weight"](%2794)
  %2796 : Tensor? = prim::GetAttr[name="bias"](%2794)
  %2797 : int[] = prim::ListConstruct(%27, %27)
  %2798 : int[] = prim::ListConstruct(%26, %26)
  %2799 : int[] = prim::ListConstruct(%26, %26)
  %out.193 : Tensor = aten::conv2d(%out.192, %2795, %2796, %2797, %2798, %2799, %27) # torch/nn/modules/conv.py:415:15
  %2801 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%849)
  %2802 : int = aten::dim(%out.193) # torch/nn/modules/batchnorm.py:276:11
  %2803 : bool = aten::ne(%2802, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2803) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2804 : bool = prim::GetAttr[name="training"](%2801)
   = prim::If(%2804) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2805 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2801)
      %2806 : Tensor = aten::add(%2805, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2801, %2806)
      -> ()
    block1():
      -> ()
  %2807 : bool = prim::GetAttr[name="training"](%2801)
  %2808 : Tensor = prim::GetAttr[name="running_mean"](%2801)
  %2809 : Tensor = prim::GetAttr[name="running_var"](%2801)
  %2810 : Tensor = prim::GetAttr[name="weight"](%2801)
  %2811 : Tensor = prim::GetAttr[name="bias"](%2801)
   = prim::If(%2807) # torch/nn/functional.py:2011:4
    block0():
      %2812 : int[] = aten::size(%out.193) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%2812, %24) # torch/nn/functional.py:1991:17
      %2814 : int = aten::len(%2812) # torch/nn/functional.py:1992:19
      %2815 : int = aten::sub(%2814, %26) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%2815, %25, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %2819 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
          %2820 : int = aten::__getitem__(%2812, %2819) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %2820) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.263)
      %2822 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2822) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.194 : Tensor = aten::batch_norm(%out.193, %2810, %2811, %2808, %2809, %2807, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.195 : Tensor = aten::relu_(%out.194) # torch/nn/functional.py:1117:17
  %2825 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%849)
  %2826 : Tensor = prim::GetAttr[name="weight"](%2825)
  %2827 : Tensor? = prim::GetAttr[name="bias"](%2825)
  %2828 : int[] = prim::ListConstruct(%27, %27)
  %2829 : int[] = prim::ListConstruct(%24, %24)
  %2830 : int[] = prim::ListConstruct(%27, %27)
  %out.196 : Tensor = aten::conv2d(%out.195, %2826, %2827, %2828, %2829, %2830, %27) # torch/nn/modules/conv.py:415:15
  %2832 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%849)
  %2833 : int = aten::dim(%out.196) # torch/nn/modules/batchnorm.py:276:11
  %2834 : bool = aten::ne(%2833, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2834) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2835 : bool = prim::GetAttr[name="training"](%2832)
   = prim::If(%2835) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2836 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2832)
      %2837 : Tensor = aten::add(%2836, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2832, %2837)
      -> ()
    block1():
      -> ()
  %2838 : bool = prim::GetAttr[name="training"](%2832)
  %2839 : Tensor = prim::GetAttr[name="running_mean"](%2832)
  %2840 : Tensor = prim::GetAttr[name="running_var"](%2832)
  %2841 : Tensor = prim::GetAttr[name="weight"](%2832)
  %2842 : Tensor = prim::GetAttr[name="bias"](%2832)
   = prim::If(%2838) # torch/nn/functional.py:2011:4
    block0():
      %2843 : int[] = aten::size(%out.196) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%2843, %24) # torch/nn/functional.py:1991:17
      %2845 : int = aten::len(%2843) # torch/nn/functional.py:1992:19
      %2846 : int = aten::sub(%2845, %26) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%2846, %25, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %2850 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
          %2851 : int = aten::__getitem__(%2843, %2850) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %2851) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.267)
      %2853 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2853) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.197 : Tensor = aten::batch_norm(%out.196, %2841, %2842, %2839, %2840, %2838, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.198 : Tensor = aten::add_(%out.197, %input.41, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.43 : Tensor = aten::relu_(%out.198) # torch/nn/functional.py:1117:17
  %2857 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%850)
  %2858 : Tensor = prim::GetAttr[name="weight"](%2857)
  %2859 : Tensor? = prim::GetAttr[name="bias"](%2857)
  %2860 : int[] = prim::ListConstruct(%27, %27)
  %2861 : int[] = prim::ListConstruct(%24, %24)
  %2862 : int[] = prim::ListConstruct(%27, %27)
  %out.208 : Tensor = aten::conv2d(%input.43, %2858, %2859, %2860, %2861, %2862, %27) # torch/nn/modules/conv.py:415:15
  %2864 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%850)
  %2865 : int = aten::dim(%out.208) # torch/nn/modules/batchnorm.py:276:11
  %2866 : bool = aten::ne(%2865, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2866) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2867 : bool = prim::GetAttr[name="training"](%2864)
   = prim::If(%2867) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2868 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2864)
      %2869 : Tensor = aten::add(%2868, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2864, %2869)
      -> ()
    block1():
      -> ()
  %2870 : bool = prim::GetAttr[name="training"](%2864)
  %2871 : Tensor = prim::GetAttr[name="running_mean"](%2864)
  %2872 : Tensor = prim::GetAttr[name="running_var"](%2864)
  %2873 : Tensor = prim::GetAttr[name="weight"](%2864)
  %2874 : Tensor = prim::GetAttr[name="bias"](%2864)
   = prim::If(%2870) # torch/nn/functional.py:2011:4
    block0():
      %2875 : int[] = aten::size(%out.208) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%2875, %24) # torch/nn/functional.py:1991:17
      %2877 : int = aten::len(%2875) # torch/nn/functional.py:1992:19
      %2878 : int = aten::sub(%2877, %26) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%2878, %25, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %2882 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
          %2883 : int = aten::__getitem__(%2875, %2882) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %2883) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.271)
      %2885 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2885) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.200 : Tensor = aten::batch_norm(%out.208, %2873, %2874, %2871, %2872, %2870, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.201 : Tensor = aten::relu_(%out.200) # torch/nn/functional.py:1117:17
  %2888 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%850)
  %2889 : Tensor = prim::GetAttr[name="weight"](%2888)
  %2890 : Tensor? = prim::GetAttr[name="bias"](%2888)
  %2891 : int[] = prim::ListConstruct(%27, %27)
  %2892 : int[] = prim::ListConstruct(%26, %26)
  %2893 : int[] = prim::ListConstruct(%26, %26)
  %out.202 : Tensor = aten::conv2d(%out.201, %2889, %2890, %2891, %2892, %2893, %27) # torch/nn/modules/conv.py:415:15
  %2895 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%850)
  %2896 : int = aten::dim(%out.202) # torch/nn/modules/batchnorm.py:276:11
  %2897 : bool = aten::ne(%2896, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2897) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2898 : bool = prim::GetAttr[name="training"](%2895)
   = prim::If(%2898) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2899 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2895)
      %2900 : Tensor = aten::add(%2899, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2895, %2900)
      -> ()
    block1():
      -> ()
  %2901 : bool = prim::GetAttr[name="training"](%2895)
  %2902 : Tensor = prim::GetAttr[name="running_mean"](%2895)
  %2903 : Tensor = prim::GetAttr[name="running_var"](%2895)
  %2904 : Tensor = prim::GetAttr[name="weight"](%2895)
  %2905 : Tensor = prim::GetAttr[name="bias"](%2895)
   = prim::If(%2901) # torch/nn/functional.py:2011:4
    block0():
      %2906 : int[] = aten::size(%out.202) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%2906, %24) # torch/nn/functional.py:1991:17
      %2908 : int = aten::len(%2906) # torch/nn/functional.py:1992:19
      %2909 : int = aten::sub(%2908, %26) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%2909, %25, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %2913 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
          %2914 : int = aten::__getitem__(%2906, %2913) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %2914) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.275)
      %2916 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2916) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.203 : Tensor = aten::batch_norm(%out.202, %2904, %2905, %2902, %2903, %2901, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.204 : Tensor = aten::relu_(%out.203) # torch/nn/functional.py:1117:17
  %2919 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%850)
  %2920 : Tensor = prim::GetAttr[name="weight"](%2919)
  %2921 : Tensor? = prim::GetAttr[name="bias"](%2919)
  %2922 : int[] = prim::ListConstruct(%27, %27)
  %2923 : int[] = prim::ListConstruct(%24, %24)
  %2924 : int[] = prim::ListConstruct(%27, %27)
  %out.205 : Tensor = aten::conv2d(%out.204, %2920, %2921, %2922, %2923, %2924, %27) # torch/nn/modules/conv.py:415:15
  %2926 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%850)
  %2927 : int = aten::dim(%out.205) # torch/nn/modules/batchnorm.py:276:11
  %2928 : bool = aten::ne(%2927, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2928) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2929 : bool = prim::GetAttr[name="training"](%2926)
   = prim::If(%2929) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2930 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2926)
      %2931 : Tensor = aten::add(%2930, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2926, %2931)
      -> ()
    block1():
      -> ()
  %2932 : bool = prim::GetAttr[name="training"](%2926)
  %2933 : Tensor = prim::GetAttr[name="running_mean"](%2926)
  %2934 : Tensor = prim::GetAttr[name="running_var"](%2926)
  %2935 : Tensor = prim::GetAttr[name="weight"](%2926)
  %2936 : Tensor = prim::GetAttr[name="bias"](%2926)
   = prim::If(%2932) # torch/nn/functional.py:2011:4
    block0():
      %2937 : int[] = aten::size(%out.205) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%2937, %24) # torch/nn/functional.py:1991:17
      %2939 : int = aten::len(%2937) # torch/nn/functional.py:1992:19
      %2940 : int = aten::sub(%2939, %26) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%2940, %25, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %2944 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
          %2945 : int = aten::__getitem__(%2937, %2944) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %2945) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.279)
      %2947 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2947) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.206 : Tensor = aten::batch_norm(%out.205, %2935, %2936, %2933, %2934, %2932, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.207 : Tensor = aten::add_(%out.206, %input.43, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.45 : Tensor = aten::relu_(%out.207) # torch/nn/functional.py:1117:17
  %2951 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%851)
  %2952 : Tensor = prim::GetAttr[name="weight"](%2951)
  %2953 : Tensor? = prim::GetAttr[name="bias"](%2951)
  %2954 : int[] = prim::ListConstruct(%27, %27)
  %2955 : int[] = prim::ListConstruct(%24, %24)
  %2956 : int[] = prim::ListConstruct(%27, %27)
  %out.290 : Tensor = aten::conv2d(%input.45, %2952, %2953, %2954, %2955, %2956, %27) # torch/nn/modules/conv.py:415:15
  %2958 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%851)
  %2959 : int = aten::dim(%out.290) # torch/nn/modules/batchnorm.py:276:11
  %2960 : bool = aten::ne(%2959, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2960) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2961 : bool = prim::GetAttr[name="training"](%2958)
   = prim::If(%2961) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2962 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2958)
      %2963 : Tensor = aten::add(%2962, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2958, %2963)
      -> ()
    block1():
      -> ()
  %2964 : bool = prim::GetAttr[name="training"](%2958)
  %2965 : Tensor = prim::GetAttr[name="running_mean"](%2958)
  %2966 : Tensor = prim::GetAttr[name="running_var"](%2958)
  %2967 : Tensor = prim::GetAttr[name="weight"](%2958)
  %2968 : Tensor = prim::GetAttr[name="bias"](%2958)
   = prim::If(%2964) # torch/nn/functional.py:2011:4
    block0():
      %2969 : int[] = aten::size(%out.290) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%2969, %24) # torch/nn/functional.py:1991:17
      %2971 : int = aten::len(%2969) # torch/nn/functional.py:1992:19
      %2972 : int = aten::sub(%2971, %26) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%2972, %25, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %2976 : int = aten::add(%i.102, %26) # torch/nn/functional.py:1993:27
          %2977 : int = aten::__getitem__(%2969, %2976) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %2977) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.407)
      %2979 : bool = aten::eq(%size_prods.405, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2979) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.291 : Tensor = aten::batch_norm(%out.290, %2967, %2968, %2965, %2966, %2964, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.292 : Tensor = aten::relu_(%out.291) # torch/nn/functional.py:1117:17
  %2982 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%851)
  %2983 : Tensor = prim::GetAttr[name="weight"](%2982)
  %2984 : Tensor? = prim::GetAttr[name="bias"](%2982)
  %2985 : int[] = prim::ListConstruct(%27, %27)
  %2986 : int[] = prim::ListConstruct(%26, %26)
  %2987 : int[] = prim::ListConstruct(%26, %26)
  %out.293 : Tensor = aten::conv2d(%out.292, %2983, %2984, %2985, %2986, %2987, %27) # torch/nn/modules/conv.py:415:15
  %2989 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%851)
  %2990 : int = aten::dim(%out.293) # torch/nn/modules/batchnorm.py:276:11
  %2991 : bool = aten::ne(%2990, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2991) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2992 : bool = prim::GetAttr[name="training"](%2989)
   = prim::If(%2992) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2993 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2989)
      %2994 : Tensor = aten::add(%2993, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2989, %2994)
      -> ()
    block1():
      -> ()
  %2995 : bool = prim::GetAttr[name="training"](%2989)
  %2996 : Tensor = prim::GetAttr[name="running_mean"](%2989)
  %2997 : Tensor = prim::GetAttr[name="running_var"](%2989)
  %2998 : Tensor = prim::GetAttr[name="weight"](%2989)
  %2999 : Tensor = prim::GetAttr[name="bias"](%2989)
   = prim::If(%2995) # torch/nn/functional.py:2011:4
    block0():
      %3000 : int[] = aten::size(%out.293) # torch/nn/functional.py:2012:27
      %size_prods.408 : int = aten::__getitem__(%3000, %24) # torch/nn/functional.py:1991:17
      %3002 : int = aten::len(%3000) # torch/nn/functional.py:1992:19
      %3003 : int = aten::sub(%3002, %26) # torch/nn/functional.py:1992:19
      %size_prods.409 : int = prim::Loop(%3003, %25, %size_prods.408) # torch/nn/functional.py:1992:4
        block0(%i.103 : int, %size_prods.410 : int):
          %3007 : int = aten::add(%i.103, %26) # torch/nn/functional.py:1993:27
          %3008 : int = aten::__getitem__(%3000, %3007) # torch/nn/functional.py:1993:22
          %size_prods.411 : int = aten::mul(%size_prods.410, %3008) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.411)
      %3010 : bool = aten::eq(%size_prods.409, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3010) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.294 : Tensor = aten::batch_norm(%out.293, %2998, %2999, %2996, %2997, %2995, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.295 : Tensor = aten::relu_(%out.294) # torch/nn/functional.py:1117:17
  %3013 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%851)
  %3014 : Tensor = prim::GetAttr[name="weight"](%3013)
  %3015 : Tensor? = prim::GetAttr[name="bias"](%3013)
  %3016 : int[] = prim::ListConstruct(%27, %27)
  %3017 : int[] = prim::ListConstruct(%24, %24)
  %3018 : int[] = prim::ListConstruct(%27, %27)
  %out.296 : Tensor = aten::conv2d(%out.295, %3014, %3015, %3016, %3017, %3018, %27) # torch/nn/modules/conv.py:415:15
  %3020 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%851)
  %3021 : int = aten::dim(%out.296) # torch/nn/modules/batchnorm.py:276:11
  %3022 : bool = aten::ne(%3021, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3022) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3023 : bool = prim::GetAttr[name="training"](%3020)
   = prim::If(%3023) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3024 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3020)
      %3025 : Tensor = aten::add(%3024, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3020, %3025)
      -> ()
    block1():
      -> ()
  %3026 : bool = prim::GetAttr[name="training"](%3020)
  %3027 : Tensor = prim::GetAttr[name="running_mean"](%3020)
  %3028 : Tensor = prim::GetAttr[name="running_var"](%3020)
  %3029 : Tensor = prim::GetAttr[name="weight"](%3020)
  %3030 : Tensor = prim::GetAttr[name="bias"](%3020)
   = prim::If(%3026) # torch/nn/functional.py:2011:4
    block0():
      %3031 : int[] = aten::size(%out.296) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%3031, %24) # torch/nn/functional.py:1991:17
      %3033 : int = aten::len(%3031) # torch/nn/functional.py:1992:19
      %3034 : int = aten::sub(%3033, %26) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%3034, %25, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %3038 : int = aten::add(%i.104, %26) # torch/nn/functional.py:1993:27
          %3039 : int = aten::__getitem__(%3031, %3038) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %3039) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.415)
      %3041 : bool = aten::eq(%size_prods.413, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3041) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.297 : Tensor = aten::batch_norm(%out.296, %3029, %3030, %3027, %3028, %3026, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.298 : Tensor = aten::add_(%out.297, %input.45, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.21 : Tensor = aten::relu_(%out.298) # torch/nn/functional.py:1117:17
  %3045 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %3046 : bool = aten::__contains__(%3045, %name.19) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%3046) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %3047 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.13 : str = aten::__getitem__(%3047, %name.19) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.13, %x.21) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %3049 : __torch__.torchvision.models.resnet.___torch_mangle_45.Bottleneck = prim::GetAttr[name="0"](%43)
  %3050 : __torch__.torchvision.models.resnet.___torch_mangle_48.Bottleneck = prim::GetAttr[name="1"](%43)
  %3051 : __torch__.torchvision.models.resnet.___torch_mangle_48.Bottleneck = prim::GetAttr[name="2"](%43)
  %3052 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%3049)
  %3053 : Tensor = prim::GetAttr[name="weight"](%3052)
  %3054 : Tensor? = prim::GetAttr[name="bias"](%3052)
  %3055 : int[] = prim::ListConstruct(%27, %27)
  %3056 : int[] = prim::ListConstruct(%24, %24)
  %3057 : int[] = prim::ListConstruct(%27, %27)
  %out.2 : Tensor = aten::conv2d(%x.21, %3053, %3054, %3055, %3056, %3057, %27) # torch/nn/modules/conv.py:415:15
  %3059 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%3049)
  %3060 : int = aten::dim(%out.2) # torch/nn/modules/batchnorm.py:276:11
  %3061 : bool = aten::ne(%3060, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3061) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3062 : bool = prim::GetAttr[name="training"](%3059)
   = prim::If(%3062) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3063 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3059)
      %3064 : Tensor = aten::add(%3063, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3059, %3064)
      -> ()
    block1():
      -> ()
  %3065 : bool = prim::GetAttr[name="training"](%3059)
  %3066 : Tensor = prim::GetAttr[name="running_mean"](%3059)
  %3067 : Tensor = prim::GetAttr[name="running_var"](%3059)
  %3068 : Tensor = prim::GetAttr[name="weight"](%3059)
  %3069 : Tensor = prim::GetAttr[name="bias"](%3059)
   = prim::If(%3065) # torch/nn/functional.py:2011:4
    block0():
      %3070 : int[] = aten::size(%out.2) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%3070, %24) # torch/nn/functional.py:1991:17
      %3072 : int = aten::len(%3070) # torch/nn/functional.py:1992:19
      %3073 : int = aten::sub(%3072, %26) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%3073, %25, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.105 : int, %size_prods.430 : int):
          %3077 : int = aten::add(%i.105, %26) # torch/nn/functional.py:1993:27
          %3078 : int = aten::__getitem__(%3070, %3077) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %3078) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.431)
      %3080 : bool = aten::eq(%size_prods.429, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3080) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.4 : Tensor = aten::batch_norm(%out.2, %3068, %3069, %3066, %3067, %3065, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.6 : Tensor = aten::relu_(%out.4) # torch/nn/functional.py:1117:17
  %3083 : __torch__.torch.nn.modules.conv.___torch_mangle_40.Conv2d = prim::GetAttr[name="conv2"](%3049)
  %3084 : Tensor = prim::GetAttr[name="weight"](%3083)
  %3085 : Tensor? = prim::GetAttr[name="bias"](%3083)
  %3086 : int[] = prim::ListConstruct(%27, %27)
  %3087 : int[] = prim::ListConstruct(%26, %26)
  %3088 : int[] = prim::ListConstruct(%26, %26)
  %out.8 : Tensor = aten::conv2d(%out.6, %3084, %3085, %3086, %3087, %3088, %27) # torch/nn/modules/conv.py:415:15
  %3090 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%3049)
  %3091 : int = aten::dim(%out.8) # torch/nn/modules/batchnorm.py:276:11
  %3092 : bool = aten::ne(%3091, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3092) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3093 : bool = prim::GetAttr[name="training"](%3090)
   = prim::If(%3093) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3094 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3090)
      %3095 : Tensor = aten::add(%3094, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3090, %3095)
      -> ()
    block1():
      -> ()
  %3096 : bool = prim::GetAttr[name="training"](%3090)
  %3097 : Tensor = prim::GetAttr[name="running_mean"](%3090)
  %3098 : Tensor = prim::GetAttr[name="running_var"](%3090)
  %3099 : Tensor = prim::GetAttr[name="weight"](%3090)
  %3100 : Tensor = prim::GetAttr[name="bias"](%3090)
   = prim::If(%3096) # torch/nn/functional.py:2011:4
    block0():
      %3101 : int[] = aten::size(%out.8) # torch/nn/functional.py:2012:27
      %size_prods.432 : int = aten::__getitem__(%3101, %24) # torch/nn/functional.py:1991:17
      %3103 : int = aten::len(%3101) # torch/nn/functional.py:1992:19
      %3104 : int = aten::sub(%3103, %26) # torch/nn/functional.py:1992:19
      %size_prods.433 : int = prim::Loop(%3104, %25, %size_prods.432) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.434 : int):
          %3108 : int = aten::add(%i.108, %26) # torch/nn/functional.py:1993:27
          %3109 : int = aten::__getitem__(%3101, %3108) # torch/nn/functional.py:1993:22
          %size_prods.435 : int = aten::mul(%size_prods.434, %3109) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.435)
      %3111 : bool = aten::eq(%size_prods.433, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3111) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.10 : Tensor = aten::batch_norm(%out.8, %3099, %3100, %3097, %3098, %3096, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.12 : Tensor = aten::relu_(%out.10) # torch/nn/functional.py:1117:17
  %3114 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%3049)
  %3115 : Tensor = prim::GetAttr[name="weight"](%3114)
  %3116 : Tensor? = prim::GetAttr[name="bias"](%3114)
  %3117 : int[] = prim::ListConstruct(%27, %27)
  %3118 : int[] = prim::ListConstruct(%24, %24)
  %3119 : int[] = prim::ListConstruct(%27, %27)
  %out.14 : Tensor = aten::conv2d(%out.12, %3115, %3116, %3117, %3118, %3119, %27) # torch/nn/modules/conv.py:415:15
  %3121 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%3049)
  %3122 : int = aten::dim(%out.14) # torch/nn/modules/batchnorm.py:276:11
  %3123 : bool = aten::ne(%3122, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3123) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3124 : bool = prim::GetAttr[name="training"](%3121)
   = prim::If(%3124) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3125 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3121)
      %3126 : Tensor = aten::add(%3125, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3121, %3126)
      -> ()
    block1():
      -> ()
  %3127 : bool = prim::GetAttr[name="training"](%3121)
  %3128 : Tensor = prim::GetAttr[name="running_mean"](%3121)
  %3129 : Tensor = prim::GetAttr[name="running_var"](%3121)
  %3130 : Tensor = prim::GetAttr[name="weight"](%3121)
  %3131 : Tensor = prim::GetAttr[name="bias"](%3121)
   = prim::If(%3127) # torch/nn/functional.py:2011:4
    block0():
      %3132 : int[] = aten::size(%out.14) # torch/nn/functional.py:2012:27
      %size_prods.424 : int = aten::__getitem__(%3132, %24) # torch/nn/functional.py:1991:17
      %3134 : int = aten::len(%3132) # torch/nn/functional.py:1992:19
      %3135 : int = aten::sub(%3134, %26) # torch/nn/functional.py:1992:19
      %size_prods.425 : int = prim::Loop(%3135, %25, %size_prods.424) # torch/nn/functional.py:1992:4
        block0(%i.107 : int, %size_prods.426 : int):
          %3139 : int = aten::add(%i.107, %26) # torch/nn/functional.py:1993:27
          %3140 : int = aten::__getitem__(%3132, %3139) # torch/nn/functional.py:1993:22
          %size_prods.427 : int = aten::mul(%size_prods.426, %3140) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.427)
      %3142 : bool = aten::eq(%size_prods.425, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3142) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.16 : Tensor = aten::batch_norm(%out.14, %3130, %3131, %3128, %3129, %3127, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %3144 : __torch__.torch.nn.modules.container.___torch_mangle_44.Sequential = prim::GetAttr[name="downsample"](%3049)
  %3145 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="0"](%3144)
  %3146 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="1"](%3144)
  %3147 : Tensor = prim::GetAttr[name="weight"](%3145)
  %3148 : Tensor? = prim::GetAttr[name="bias"](%3145)
  %3149 : int[] = prim::ListConstruct(%27, %27)
  %3150 : int[] = prim::ListConstruct(%24, %24)
  %3151 : int[] = prim::ListConstruct(%27, %27)
  %input.48 : Tensor = aten::conv2d(%x.21, %3147, %3148, %3149, %3150, %3151, %27) # torch/nn/modules/conv.py:415:15
  %3153 : int = aten::dim(%input.48) # torch/nn/modules/batchnorm.py:276:11
  %3154 : bool = aten::ne(%3153, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3154) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3155 : bool = prim::GetAttr[name="training"](%3146)
   = prim::If(%3155) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3156 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3146)
      %3157 : Tensor = aten::add(%3156, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3146, %3157)
      -> ()
    block1():
      -> ()
  %3158 : bool = prim::GetAttr[name="training"](%3146)
  %3159 : Tensor = prim::GetAttr[name="running_mean"](%3146)
  %3160 : Tensor = prim::GetAttr[name="running_var"](%3146)
  %3161 : Tensor = prim::GetAttr[name="weight"](%3146)
  %3162 : Tensor = prim::GetAttr[name="bias"](%3146)
   = prim::If(%3158) # torch/nn/functional.py:2011:4
    block0():
      %3163 : int[] = aten::size(%input.48) # torch/nn/functional.py:2012:27
      %size_prods.436 : int = aten::__getitem__(%3163, %24) # torch/nn/functional.py:1991:17
      %3165 : int = aten::len(%3163) # torch/nn/functional.py:1992:19
      %3166 : int = aten::sub(%3165, %26) # torch/nn/functional.py:1992:19
      %size_prods.437 : int = prim::Loop(%3166, %25, %size_prods.436) # torch/nn/functional.py:1992:4
        block0(%i.110 : int, %size_prods.438 : int):
          %3170 : int = aten::add(%i.110, %26) # torch/nn/functional.py:1993:27
          %3171 : int = aten::__getitem__(%3163, %3170) # torch/nn/functional.py:1993:22
          %size_prods.439 : int = aten::mul(%size_prods.438, %3171) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.439)
      %3173 : bool = aten::eq(%size_prods.437, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3173) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.1 : Tensor = aten::batch_norm(%input.48, %3161, %3162, %3159, %3160, %3158, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.18 : Tensor = aten::add_(%out.16, %identity.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.50 : Tensor = aten::relu_(%out.18) # torch/nn/functional.py:1117:17
  %3177 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%3050)
  %3178 : Tensor = prim::GetAttr[name="weight"](%3177)
  %3179 : Tensor? = prim::GetAttr[name="bias"](%3177)
  %3180 : int[] = prim::ListConstruct(%27, %27)
  %3181 : int[] = prim::ListConstruct(%24, %24)
  %3182 : int[] = prim::ListConstruct(%27, %27)
  %out.28 : Tensor = aten::conv2d(%input.50, %3178, %3179, %3180, %3181, %3182, %27) # torch/nn/modules/conv.py:415:15
  %3184 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%3050)
  %3185 : int = aten::dim(%out.28) # torch/nn/modules/batchnorm.py:276:11
  %3186 : bool = aten::ne(%3185, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3186) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3187 : bool = prim::GetAttr[name="training"](%3184)
   = prim::If(%3187) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3188 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3184)
      %3189 : Tensor = aten::add(%3188, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3184, %3189)
      -> ()
    block1():
      -> ()
  %3190 : bool = prim::GetAttr[name="training"](%3184)
  %3191 : Tensor = prim::GetAttr[name="running_mean"](%3184)
  %3192 : Tensor = prim::GetAttr[name="running_var"](%3184)
  %3193 : Tensor = prim::GetAttr[name="weight"](%3184)
  %3194 : Tensor = prim::GetAttr[name="bias"](%3184)
   = prim::If(%3190) # torch/nn/functional.py:2011:4
    block0():
      %3195 : int[] = aten::size(%out.28) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%3195, %24) # torch/nn/functional.py:1991:17
      %3197 : int = aten::len(%3195) # torch/nn/functional.py:1992:19
      %3198 : int = aten::sub(%3197, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%3198, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.111 : int, %size_prods.30 : int):
          %3202 : int = aten::add(%i.111, %26) # torch/nn/functional.py:1993:27
          %3203 : int = aten::__getitem__(%3195, %3202) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %3203) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %3205 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3205) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.20 : Tensor = aten::batch_norm(%out.28, %3193, %3194, %3191, %3192, %3190, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.21 : Tensor = aten::relu_(%out.20) # torch/nn/functional.py:1117:17
  %3208 : __torch__.torch.nn.modules.conv.___torch_mangle_47.Conv2d = prim::GetAttr[name="conv2"](%3050)
  %3209 : Tensor = prim::GetAttr[name="weight"](%3208)
  %3210 : Tensor? = prim::GetAttr[name="bias"](%3208)
  %3211 : int[] = prim::ListConstruct(%27, %27)
  %3212 : int[] = prim::ListConstruct(%22, %22)
  %3213 : int[] = prim::ListConstruct(%22, %22)
  %out.22 : Tensor = aten::conv2d(%out.21, %3209, %3210, %3211, %3212, %3213, %27) # torch/nn/modules/conv.py:415:15
  %3215 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%3050)
  %3216 : int = aten::dim(%out.22) # torch/nn/modules/batchnorm.py:276:11
  %3217 : bool = aten::ne(%3216, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3217) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3218 : bool = prim::GetAttr[name="training"](%3215)
   = prim::If(%3218) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3219 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3215)
      %3220 : Tensor = aten::add(%3219, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3215, %3220)
      -> ()
    block1():
      -> ()
  %3221 : bool = prim::GetAttr[name="training"](%3215)
  %3222 : Tensor = prim::GetAttr[name="running_mean"](%3215)
  %3223 : Tensor = prim::GetAttr[name="running_var"](%3215)
  %3224 : Tensor = prim::GetAttr[name="weight"](%3215)
  %3225 : Tensor = prim::GetAttr[name="bias"](%3215)
   = prim::If(%3221) # torch/nn/functional.py:2011:4
    block0():
      %3226 : int[] = aten::size(%out.22) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%3226, %24) # torch/nn/functional.py:1991:17
      %3228 : int = aten::len(%3226) # torch/nn/functional.py:1992:19
      %3229 : int = aten::sub(%3228, %26) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%3229, %25, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %3233 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
          %3234 : int = aten::__getitem__(%3226, %3233) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %3234) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.35)
      %3236 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3236) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.23 : Tensor = aten::batch_norm(%out.22, %3224, %3225, %3222, %3223, %3221, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.24 : Tensor = aten::relu_(%out.23) # torch/nn/functional.py:1117:17
  %3239 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%3050)
  %3240 : Tensor = prim::GetAttr[name="weight"](%3239)
  %3241 : Tensor? = prim::GetAttr[name="bias"](%3239)
  %3242 : int[] = prim::ListConstruct(%27, %27)
  %3243 : int[] = prim::ListConstruct(%24, %24)
  %3244 : int[] = prim::ListConstruct(%27, %27)
  %out.25 : Tensor = aten::conv2d(%out.24, %3240, %3241, %3242, %3243, %3244, %27) # torch/nn/modules/conv.py:415:15
  %3246 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%3050)
  %3247 : int = aten::dim(%out.25) # torch/nn/modules/batchnorm.py:276:11
  %3248 : bool = aten::ne(%3247, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3248) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3249 : bool = prim::GetAttr[name="training"](%3246)
   = prim::If(%3249) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3250 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3246)
      %3251 : Tensor = aten::add(%3250, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3246, %3251)
      -> ()
    block1():
      -> ()
  %3252 : bool = prim::GetAttr[name="training"](%3246)
  %3253 : Tensor = prim::GetAttr[name="running_mean"](%3246)
  %3254 : Tensor = prim::GetAttr[name="running_var"](%3246)
  %3255 : Tensor = prim::GetAttr[name="weight"](%3246)
  %3256 : Tensor = prim::GetAttr[name="bias"](%3246)
   = prim::If(%3252) # torch/nn/functional.py:2011:4
    block0():
      %3257 : int[] = aten::size(%out.25) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%3257, %24) # torch/nn/functional.py:1991:17
      %3259 : int = aten::len(%3257) # torch/nn/functional.py:1992:19
      %3260 : int = aten::sub(%3259, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%3260, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %3264 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %3265 : int = aten::__getitem__(%3257, %3264) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %3265) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %3267 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3267) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.26 : Tensor = aten::batch_norm(%out.25, %3255, %3256, %3253, %3254, %3252, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.27 : Tensor = aten::add_(%out.26, %input.50, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.49 : Tensor = aten::relu_(%out.27) # torch/nn/functional.py:1117:17
  %3271 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%3051)
  %3272 : Tensor = prim::GetAttr[name="weight"](%3271)
  %3273 : Tensor? = prim::GetAttr[name="bias"](%3271)
  %3274 : int[] = prim::ListConstruct(%27, %27)
  %3275 : int[] = prim::ListConstruct(%24, %24)
  %3276 : int[] = prim::ListConstruct(%27, %27)
  %out.1 : Tensor = aten::conv2d(%input.49, %3272, %3273, %3274, %3275, %3276, %27) # torch/nn/modules/conv.py:415:15
  %3278 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%3051)
  %3279 : int = aten::dim(%out.1) # torch/nn/modules/batchnorm.py:276:11
  %3280 : bool = aten::ne(%3279, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3280) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3281 : bool = prim::GetAttr[name="training"](%3278)
   = prim::If(%3281) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3282 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3278)
      %3283 : Tensor = aten::add(%3282, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3278, %3283)
      -> ()
    block1():
      -> ()
  %3284 : bool = prim::GetAttr[name="training"](%3278)
  %3285 : Tensor = prim::GetAttr[name="running_mean"](%3278)
  %3286 : Tensor = prim::GetAttr[name="running_var"](%3278)
  %3287 : Tensor = prim::GetAttr[name="weight"](%3278)
  %3288 : Tensor = prim::GetAttr[name="bias"](%3278)
   = prim::If(%3284) # torch/nn/functional.py:2011:4
    block0():
      %3289 : int[] = aten::size(%out.1) # torch/nn/functional.py:2012:27
      %size_prods.416 : int = aten::__getitem__(%3289, %24) # torch/nn/functional.py:1991:17
      %3291 : int = aten::len(%3289) # torch/nn/functional.py:1992:19
      %3292 : int = aten::sub(%3291, %26) # torch/nn/functional.py:1992:19
      %size_prods.417 : int = prim::Loop(%3292, %25, %size_prods.416) # torch/nn/functional.py:1992:4
        block0(%i.109 : int, %size_prods.418 : int):
          %3296 : int = aten::add(%i.109, %26) # torch/nn/functional.py:1993:27
          %3297 : int = aten::__getitem__(%3289, %3296) # torch/nn/functional.py:1993:22
          %size_prods.419 : int = aten::mul(%size_prods.418, %3297) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.419)
      %3299 : bool = aten::eq(%size_prods.417, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3299) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.3 : Tensor = aten::batch_norm(%out.1, %3287, %3288, %3285, %3286, %3284, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.5 : Tensor = aten::relu_(%out.3) # torch/nn/functional.py:1117:17
  %3302 : __torch__.torch.nn.modules.conv.___torch_mangle_47.Conv2d = prim::GetAttr[name="conv2"](%3051)
  %3303 : Tensor = prim::GetAttr[name="weight"](%3302)
  %3304 : Tensor? = prim::GetAttr[name="bias"](%3302)
  %3305 : int[] = prim::ListConstruct(%27, %27)
  %3306 : int[] = prim::ListConstruct(%22, %22)
  %3307 : int[] = prim::ListConstruct(%22, %22)
  %out.7 : Tensor = aten::conv2d(%out.5, %3303, %3304, %3305, %3306, %3307, %27) # torch/nn/modules/conv.py:415:15
  %3309 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%3051)
  %3310 : int = aten::dim(%out.7) # torch/nn/modules/batchnorm.py:276:11
  %3311 : bool = aten::ne(%3310, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3311) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3312 : bool = prim::GetAttr[name="training"](%3309)
   = prim::If(%3312) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3313 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3309)
      %3314 : Tensor = aten::add(%3313, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3309, %3314)
      -> ()
    block1():
      -> ()
  %3315 : bool = prim::GetAttr[name="training"](%3309)
  %3316 : Tensor = prim::GetAttr[name="running_mean"](%3309)
  %3317 : Tensor = prim::GetAttr[name="running_var"](%3309)
  %3318 : Tensor = prim::GetAttr[name="weight"](%3309)
  %3319 : Tensor = prim::GetAttr[name="bias"](%3309)
   = prim::If(%3315) # torch/nn/functional.py:2011:4
    block0():
      %3320 : int[] = aten::size(%out.7) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%3320, %24) # torch/nn/functional.py:1991:17
      %3322 : int = aten::len(%3320) # torch/nn/functional.py:1992:19
      %3323 : int = aten::sub(%3322, %26) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%3323, %25, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %3327 : int = aten::add(%i.106, %26) # torch/nn/functional.py:1993:27
          %3328 : int = aten::__getitem__(%3320, %3327) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %3328) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.423)
      %3330 : bool = aten::eq(%size_prods.421, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3330) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.9 : Tensor = aten::batch_norm(%out.7, %3318, %3319, %3316, %3317, %3315, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.11 : Tensor = aten::relu_(%out.9) # torch/nn/functional.py:1117:17
  %3333 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%3051)
  %3334 : Tensor = prim::GetAttr[name="weight"](%3333)
  %3335 : Tensor? = prim::GetAttr[name="bias"](%3333)
  %3336 : int[] = prim::ListConstruct(%27, %27)
  %3337 : int[] = prim::ListConstruct(%24, %24)
  %3338 : int[] = prim::ListConstruct(%27, %27)
  %out.13 : Tensor = aten::conv2d(%out.11, %3334, %3335, %3336, %3337, %3338, %27) # torch/nn/modules/conv.py:415:15
  %3340 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%3051)
  %3341 : int = aten::dim(%out.13) # torch/nn/modules/batchnorm.py:276:11
  %3342 : bool = aten::ne(%3341, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3342) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3343 : bool = prim::GetAttr[name="training"](%3340)
   = prim::If(%3343) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3344 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3340)
      %3345 : Tensor = aten::add(%3344, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3340, %3345)
      -> ()
    block1():
      -> ()
  %3346 : bool = prim::GetAttr[name="training"](%3340)
  %3347 : Tensor = prim::GetAttr[name="running_mean"](%3340)
  %3348 : Tensor = prim::GetAttr[name="running_var"](%3340)
  %3349 : Tensor = prim::GetAttr[name="weight"](%3340)
  %3350 : Tensor = prim::GetAttr[name="bias"](%3340)
   = prim::If(%3346) # torch/nn/functional.py:2011:4
    block0():
      %3351 : int[] = aten::size(%out.13) # torch/nn/functional.py:2012:27
      %size_prods.440 : int = aten::__getitem__(%3351, %24) # torch/nn/functional.py:1991:17
      %3353 : int = aten::len(%3351) # torch/nn/functional.py:1992:19
      %3354 : int = aten::sub(%3353, %26) # torch/nn/functional.py:1992:19
      %size_prods.441 : int = prim::Loop(%3354, %25, %size_prods.440) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.442 : int):
          %3358 : int = aten::add(%i.112, %26) # torch/nn/functional.py:1993:27
          %3359 : int = aten::__getitem__(%3351, %3358) # torch/nn/functional.py:1993:22
          %size_prods.443 : int = aten::mul(%size_prods.442, %3359) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.443)
      %3361 : bool = aten::eq(%size_prods.441, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3361) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.15 : Tensor = aten::batch_norm(%out.13, %3349, %3350, %3347, %3348, %3346, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.17 : Tensor = aten::add_(%out.15, %input.49, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.24 : Tensor = aten::relu_(%out.17) # torch/nn/functional.py:1117:17
  %3365 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %3366 : bool = aten::__contains__(%3365, %name.22) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%3366) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %3367 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.15 : str = aten::__getitem__(%3367, %name.22) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.15, %x.24) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %result.1 : Dict(str, Tensor) = aten::dict() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:22:17
  %x.5 : Tensor = aten::__getitem__(%features.1, %6) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:23:12
  %16 : __torch__.torchvision.models.segmentation.deeplabv3.DeepLabHead = prim::GetAttr[name="classifier"](%self)
  %3369 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:90:8
  %3370 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %3372 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %3373 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %3374 : int = prim::Constant[value=2]() # torch/nn/functional.py:1992:31
  %3375 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %3376 : int = prim::Constant[value=0]() # torch/nn/modules/conv.py:414:34
  %3377 : int = prim::Constant[value=12]() # torch/nn/modules/conv.py:414:38
  %3378 : int = prim::Constant[value=24]() # torch/nn/modules/conv.py:414:38
  %3379 : int = prim::Constant[value=36]() # torch/nn/modules/conv.py:414:38
  %3380 : str = prim::Constant[value="area"]() # torch/nn/functional.py:3112:27
  %3381 : str = prim::Constant[value="nearest"]() # torch/nn/functional.py:3112:16
  %3382 : int = prim::Constant[value=3]() # torch/nn/functional.py:3140:22
  %3383 : int = prim::Constant[value=5]() # torch/nn/functional.py:3144:24
  %3384 : str = prim::Constant[value="The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. "]() # torch/nn/functional.py:3000:26
  %3385 : int = prim::Constant[value=-2]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:59:23
  %3386 : int = prim::Constant[value=9223372036854775807]()
  %3387 : str = prim::Constant[value="bilinear"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:62:48
  %3388 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:62:74
  %3389 : None = prim::Constant()
  %3390 : float = prim::Constant[value=0.5]() # torch/nn/modules/dropout.py:58:32
  %3391 : __torch__.torchvision.models.segmentation.deeplabv3.ASPP = prim::GetAttr[name="0"](%16)
  %3392 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="1"](%16)
  %3393 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="2"](%16)
  %3394 : __torch__.torch.nn.modules.conv.___torch_mangle_61.Conv2d = prim::GetAttr[name="4"](%16)
  %res.1 : Tensor[] = prim::ListConstruct()
  %3396 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="convs"](%3391)
  %3397 : __torch__.torch.nn.modules.container.___torch_mangle_52.Sequential = prim::GetAttr[name="0"](%3396)
  %3398 : __torch__.torchvision.models.segmentation.deeplabv3.ASPPConv = prim::GetAttr[name="1"](%3396)
  %3399 : __torch__.torchvision.models.segmentation.deeplabv3.___torch_mangle_55.ASPPConv = prim::GetAttr[name="2"](%3396)
  %3400 : __torch__.torchvision.models.segmentation.deeplabv3.___torch_mangle_57.ASPPConv = prim::GetAttr[name="3"](%3396)
  %3401 : __torch__.torchvision.models.segmentation.deeplabv3.ASPPPooling = prim::GetAttr[name="4"](%3396)
  %3402 : __torch__.torch.nn.modules.conv.___torch_mangle_50.Conv2d = prim::GetAttr[name="0"](%3397)
  %3403 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%3397)
  %3404 : Tensor = prim::GetAttr[name="weight"](%3402)
  %3405 : Tensor? = prim::GetAttr[name="bias"](%3402)
  %3406 : int[] = prim::ListConstruct(%3369, %3369)
  %3407 : int[] = prim::ListConstruct(%3376, %3376)
  %3408 : int[] = prim::ListConstruct(%3369, %3369)
  %input.4 : Tensor = aten::conv2d(%x.5, %3404, %3405, %3406, %3407, %3408, %3369) # torch/nn/modules/conv.py:415:15
  %3410 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %3411 : bool = aten::ne(%3410, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3411) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3412 : bool = prim::GetAttr[name="training"](%3403)
   = prim::If(%3412) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3413 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3403)
      %3414 : Tensor = aten::add(%3413, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3403, %3414)
      -> ()
    block1():
      -> ()
  %3415 : bool = prim::GetAttr[name="training"](%3403)
  %3416 : Tensor = prim::GetAttr[name="running_mean"](%3403)
  %3417 : Tensor = prim::GetAttr[name="running_var"](%3403)
  %3418 : Tensor = prim::GetAttr[name="weight"](%3403)
  %3419 : Tensor = prim::GetAttr[name="bias"](%3403)
   = prim::If(%3415) # torch/nn/functional.py:2011:4
    block0():
      %3420 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%3420, %3376) # torch/nn/functional.py:1991:17
      %3422 : int = aten::len(%3420) # torch/nn/functional.py:1992:19
      %3423 : int = aten::sub(%3422, %3374) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%3423, %3375, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.7 : int):
          %3427 : int = aten::add(%i.5, %3374) # torch/nn/functional.py:1993:27
          %3428 : int = aten::__getitem__(%3420, %3427) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %3428) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.5)
      %3430 : bool = aten::eq(%size_prods.4, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3430) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %3418, %3419, %3416, %3417, %3415, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu(%input.6) # torch/nn/functional.py:1119:17
  %3433 : Tensor[] = aten::append(%res.1, %input.8) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %3434 : __torch__.torch.nn.modules.conv.___torch_mangle_53.Conv2d = prim::GetAttr[name="0"](%3398)
  %3435 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%3398)
  %3436 : Tensor = prim::GetAttr[name="weight"](%3434)
  %3437 : Tensor? = prim::GetAttr[name="bias"](%3434)
  %3438 : int[] = prim::ListConstruct(%3369, %3369)
  %3439 : int[] = prim::ListConstruct(%3377, %3377)
  %3440 : int[] = prim::ListConstruct(%3377, %3377)
  %input.18 : Tensor = aten::conv2d(%x.5, %3436, %3437, %3438, %3439, %3440, %3369) # torch/nn/modules/conv.py:415:15
  %3442 : int = aten::dim(%input.18) # torch/nn/modules/batchnorm.py:276:11
  %3443 : bool = aten::ne(%3442, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3443) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3444 : bool = prim::GetAttr[name="training"](%3435)
   = prim::If(%3444) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3445 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3435)
      %3446 : Tensor = aten::add(%3445, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3435, %3446)
      -> ()
    block1():
      -> ()
  %3447 : bool = prim::GetAttr[name="training"](%3435)
  %3448 : Tensor = prim::GetAttr[name="running_mean"](%3435)
  %3449 : Tensor = prim::GetAttr[name="running_var"](%3435)
  %3450 : Tensor = prim::GetAttr[name="weight"](%3435)
  %3451 : Tensor = prim::GetAttr[name="bias"](%3435)
   = prim::If(%3447) # torch/nn/functional.py:2011:4
    block0():
      %3452 : int[] = aten::size(%input.18) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%3452, %3376) # torch/nn/functional.py:1991:17
      %3454 : int = aten::len(%3452) # torch/nn/functional.py:1992:19
      %3455 : int = aten::sub(%3454, %3374) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%3455, %3375, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %3459 : int = aten::add(%i.3, %3374) # torch/nn/functional.py:1993:27
          %3460 : int = aten::__getitem__(%3452, %3459) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %3460) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.11)
      %3462 : bool = aten::eq(%size_prods.9, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3462) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.10 : Tensor = aten::batch_norm(%input.18, %3450, %3451, %3448, %3449, %3447, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.11 : Tensor = aten::relu(%input.10) # torch/nn/functional.py:1119:17
  %3465 : Tensor[] = aten::append(%res.1, %input.11) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %3466 : __torch__.torch.nn.modules.conv.___torch_mangle_54.Conv2d = prim::GetAttr[name="0"](%3399)
  %3467 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%3399)
  %3468 : Tensor = prim::GetAttr[name="weight"](%3466)
  %3469 : Tensor? = prim::GetAttr[name="bias"](%3466)
  %3470 : int[] = prim::ListConstruct(%3369, %3369)
  %3471 : int[] = prim::ListConstruct(%3378, %3378)
  %3472 : int[] = prim::ListConstruct(%3378, %3378)
  %input.12 : Tensor = aten::conv2d(%x.5, %3468, %3469, %3470, %3471, %3472, %3369) # torch/nn/modules/conv.py:415:15
  %3474 : int = aten::dim(%input.12) # torch/nn/modules/batchnorm.py:276:11
  %3475 : bool = aten::ne(%3474, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3475) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3476 : bool = prim::GetAttr[name="training"](%3467)
   = prim::If(%3476) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3477 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3467)
      %3478 : Tensor = aten::add(%3477, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3467, %3478)
      -> ()
    block1():
      -> ()
  %3479 : bool = prim::GetAttr[name="training"](%3467)
  %3480 : Tensor = prim::GetAttr[name="running_mean"](%3467)
  %3481 : Tensor = prim::GetAttr[name="running_var"](%3467)
  %3482 : Tensor = prim::GetAttr[name="weight"](%3467)
  %3483 : Tensor = prim::GetAttr[name="bias"](%3467)
   = prim::If(%3479) # torch/nn/functional.py:2011:4
    block0():
      %3484 : int[] = aten::size(%input.12) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%3484, %3376) # torch/nn/functional.py:1991:17
      %3486 : int = aten::len(%3484) # torch/nn/functional.py:1992:19
      %3487 : int = aten::sub(%3486, %3374) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%3487, %3375, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %3491 : int = aten::add(%i.4, %3374) # torch/nn/functional.py:1993:27
          %3492 : int = aten::__getitem__(%3484, %3491) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %3492) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.15)
      %3494 : bool = aten::eq(%size_prods.13, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3494) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.13 : Tensor = aten::batch_norm(%input.12, %3482, %3483, %3480, %3481, %3479, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.14 : Tensor = aten::relu(%input.13) # torch/nn/functional.py:1119:17
  %3497 : Tensor[] = aten::append(%res.1, %input.14) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %3498 : __torch__.torch.nn.modules.conv.___torch_mangle_56.Conv2d = prim::GetAttr[name="0"](%3400)
  %3499 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%3400)
  %3500 : Tensor = prim::GetAttr[name="weight"](%3498)
  %3501 : Tensor? = prim::GetAttr[name="bias"](%3498)
  %3502 : int[] = prim::ListConstruct(%3369, %3369)
  %3503 : int[] = prim::ListConstruct(%3379, %3379)
  %3504 : int[] = prim::ListConstruct(%3379, %3379)
  %input.15 : Tensor = aten::conv2d(%x.5, %3500, %3501, %3502, %3503, %3504, %3369) # torch/nn/modules/conv.py:415:15
  %3506 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %3507 : bool = aten::ne(%3506, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3507) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3508 : bool = prim::GetAttr[name="training"](%3499)
   = prim::If(%3508) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3509 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3499)
      %3510 : Tensor = aten::add(%3509, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3499, %3510)
      -> ()
    block1():
      -> ()
  %3511 : bool = prim::GetAttr[name="training"](%3499)
  %3512 : Tensor = prim::GetAttr[name="running_mean"](%3499)
  %3513 : Tensor = prim::GetAttr[name="running_var"](%3499)
  %3514 : Tensor = prim::GetAttr[name="weight"](%3499)
  %3515 : Tensor = prim::GetAttr[name="bias"](%3499)
   = prim::If(%3511) # torch/nn/functional.py:2011:4
    block0():
      %3516 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%3516, %3376) # torch/nn/functional.py:1991:17
      %3518 : int = aten::len(%3516) # torch/nn/functional.py:1992:19
      %3519 : int = aten::sub(%3518, %3374) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%3519, %3375, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.18 : int):
          %3523 : int = aten::add(%i.6, %3374) # torch/nn/functional.py:1993:27
          %3524 : int = aten::__getitem__(%3516, %3523) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %3524) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.19)
      %3526 : bool = aten::eq(%size_prods.17, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3526) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.15, %3514, %3515, %3512, %3513, %3511, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.17 : Tensor = aten::relu(%input.16) # torch/nn/functional.py:1119:17
  %3529 : Tensor[] = aten::append(%res.1, %input.17) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %3530 : int[] = aten::size(%x.5) # <string>:7:9
  %size.2 : int[] = aten::slice(%3530, %3385, %3386, %3369) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:59:15
  %3532 : __torch__.torch.nn.modules.conv.___torch_mangle_50.Conv2d = prim::GetAttr[name="1"](%3401)
  %3533 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="2"](%3401)
  %3534 : int[] = prim::ListConstruct(%3369, %3369)
  %3535 : int[] = aten::size(%x.5) # torch/nn/functional.py:925:51
  %3536 : int = aten::len(%3535) # <string>:5:9
  %3537 : bool = aten::gt(%3536, %3374) # <string>:5:9
   = prim::If(%3537) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%3373) # <string>:5:2
      -> ()
  %x.4 : Tensor = aten::adaptive_avg_pool2d(%x.5, %3534) # torch/nn/functional.py:926:11
  %3539 : Tensor = prim::GetAttr[name="weight"](%3532)
  %3540 : Tensor? = prim::GetAttr[name="bias"](%3532)
  %3541 : int[] = prim::ListConstruct(%3369, %3369)
  %3542 : int[] = prim::ListConstruct(%3376, %3376)
  %3543 : int[] = prim::ListConstruct(%3369, %3369)
  %x.6 : Tensor = aten::conv2d(%x.4, %3539, %3540, %3541, %3542, %3543, %3369) # torch/nn/modules/conv.py:415:15
  %3545 : int = aten::dim(%x.6) # torch/nn/modules/batchnorm.py:276:11
  %3546 : bool = aten::ne(%3545, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3546) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3547 : bool = prim::GetAttr[name="training"](%3533)
   = prim::If(%3547) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3548 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3533)
      %3549 : Tensor = aten::add(%3548, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3533, %3549)
      -> ()
    block1():
      -> ()
  %3550 : bool = prim::GetAttr[name="training"](%3533)
  %3551 : Tensor = prim::GetAttr[name="running_mean"](%3533)
  %3552 : Tensor = prim::GetAttr[name="running_var"](%3533)
  %3553 : Tensor = prim::GetAttr[name="weight"](%3533)
  %3554 : Tensor = prim::GetAttr[name="bias"](%3533)
   = prim::If(%3550) # torch/nn/functional.py:2011:4
    block0():
      %3555 : int[] = aten::size(%x.6) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%3555, %3376) # torch/nn/functional.py:1991:17
      %3557 : int = aten::len(%3555) # torch/nn/functional.py:1992:19
      %3558 : int = aten::sub(%3557, %3374) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%3558, %3375, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.22 : int):
          %3562 : int = aten::add(%i.2, %3374) # torch/nn/functional.py:1993:27
          %3563 : int = aten::__getitem__(%3555, %3562) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %3563) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.23)
      %3565 : bool = aten::eq(%size_prods.21, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3565) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.8 : Tensor = aten::batch_norm(%x.6, %3553, %3554, %3551, %3552, %3550, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %x.10 : Tensor = aten::relu(%x.8) # torch/nn/functional.py:1119:17
  %3568 : Tensor = prim::Uninitialized()
  %3569 : bool? = prim::Uninitialized()
  %3570 : bool = prim::Uninitialized()
  %3571 : str[] = prim::ListConstruct(%3381, %3380)
  %3572 : bool = aten::__contains__(%3571, %3387) # torch/nn/functional.py:3112:7
  %align_corners.61 : bool? = prim::If(%3572) # torch/nn/functional.py:3112:4
    block0():
       = prim::RaiseException(%3373) # torch/nn/functional.py:3114:12
      -> (%3569)
    block1():
      -> (%3388)
  %3574 : int = aten::dim(%x.10) # torch/nn/functional.py:3124:23
  %scale_factor_len.2 : int = aten::sub(%3574, %3374) # torch/nn/functional.py:3124:23
  %scale_factor_list.3 : float?[] = prim::ListConstruct()
   = prim::Loop(%scale_factor_len.2, %3375) # torch/nn/functional.py:3125:66
    block0(%3577 : int):
      %3578 : float?[] = aten::append(%scale_factor_list.3, %3389) # torch/nn/functional.py:3125:66
      -> (%3375)
  %closed_over_args.2 : (Tensor, int[]?, float[]?, bool?) = prim::TupleConstruct(%x.10, %size.2, %3389, %3389)
  %3580 : float[] = prim::Uninitialized()
  %3581 : float[]? = prim::Uninitialized()
  %3582 : int[]? = prim::Uninitialized()
  %input.2 : Tensor, %size.3 : int[]?, %scale_factor.3 : float[]?, %recompute_scale_factor.2 : bool? = prim::TupleUnpack(%closed_over_args.2)
  %3587 : int = aten::dim(%input.2) # torch/nn/functional.py:2966:10
  %dim.2 : int = aten::sub(%3587, %3374) # torch/nn/functional.py:2966:10
  %3589 : bool = aten::__is__(%size.3, %3389) # torch/nn/functional.py:2967:7
  %3590 : bool, %size.26 : int[]? = prim::If(%3589) # torch/nn/functional.py:2967:7
    block0():
      %3592 : bool = aten::__is__(%scale_factor.3, %3389) # torch/nn/functional.py:2967:24
      -> (%3592, %size.3)
    block1():
      %size.5 : int[] = prim::unchecked_cast(%size.3)
      -> (%3388, %size.5)
   = prim::If(%3590) # torch/nn/functional.py:2967:4
    block0():
       = prim::RaiseException(%3373) # torch/nn/functional.py:2968:8
      -> ()
    block1():
      -> ()
  %3594 : bool = aten::__isnot__(%size.26, %3389) # torch/nn/functional.py:2969:7
  %3595 : bool, %size.27 : int[]? = prim::If(%3594) # torch/nn/functional.py:2969:7
    block0():
      %size.9 : int[] = prim::unchecked_cast(%size.26)
      %3598 : bool = aten::__isnot__(%scale_factor.3, %3389) # torch/nn/functional.py:2969:28
      -> (%3598, %size.9)
    block1():
      -> (%3388, %size.26)
  %scale_factor.28 : float[]?, %size.28 : int[]? = prim::If(%3595) # torch/nn/functional.py:2969:4
    block0():
       = prim::RaiseException(%3373) # torch/nn/functional.py:2970:8
      -> (%3581, %3582)
    block1():
      -> (%scale_factor.3, %size.27)
  %3601 : bool = aten::__isnot__(%scale_factor.28, %3389) # torch/nn/functional.py:2971:7
  %scale_factor.29 : float[]? = prim::If(%3601) # torch/nn/functional.py:2971:4
    block0():
      %scale_factor.12 : float[] = prim::unchecked_cast(%scale_factor.28)
      %3604 : int = aten::len(%scale_factor.12) # torch/nn/functional.py:2973:15
      %3605 : bool = aten::ne(%3604, %dim.2) # torch/nn/functional.py:2973:15
       = prim::If(%3605) # torch/nn/functional.py:2973:12
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:2974:16
          -> ()
        block1():
          -> ()
      -> (%scale_factor.12)
    block1():
      -> (%scale_factor.28)
  %3606 : bool = aten::__isnot__(%size.28, %3389) # torch/nn/functional.py:2977:7
  %output_size.2 : int[] = prim::If(%3606) # torch/nn/functional.py:2977:4
    block0():
      %size.18 : int[] = prim::unchecked_cast(%size.28)
      -> (%size.18)
    block1():
      %3609 : bool = aten::__isnot__(%scale_factor.29, %3389) # torch/nn/functional.py:2983:11
      %scale_factor.5 : float[] = prim::If(%3609) # torch/nn/functional.py:2983:4
        block0():
          %scale_factor.22 : float[] = prim::unchecked_cast(%scale_factor.29)
          -> (%scale_factor.22)
        block1():
           = prim::RaiseException(%3373) # torch/nn/functional.py:2983:4
          -> (%3580)
      %3612 : bool = aten::__is__(%recompute_scale_factor.2, %3389) # torch/nn/functional.py:2989:7
       = prim::If(%3612) # torch/nn/functional.py:2989:4
        block0():
          %3613 : int = aten::len(%scale_factor.5) # torch/nn/functional.py:2994:8
          %3614 : bool = aten::gt(%3613, %3376)
          %is_float_scale_factor.2 : bool, %3616 : int = prim::Loop(%3386, %3614, %3388, %3376) # torch/nn/functional.py:2994:8
            block0(%3617 : int, %is_float_scale_factor.7 : bool, %3619 : int):
              %scale.3 : float = aten::__getitem__(%scale_factor.5, %3619) # torch/nn/functional.py:2994:8
              %3621 : int = aten::floor(%scale.3) # torch/nn/functional.py:2995:36
              %is_float_scale_factor.8 : bool = aten::ne(%3621, %scale.3) # torch/nn/functional.py:2995:36
              %3623 : bool = prim::If(%is_float_scale_factor.8) # torch/nn/functional.py:2996:12
                block0():
                  -> (%3388)
                block1():
                  -> (%3375)
              %3624 : int = aten::add(%3619, %3369)
              %3625 : bool = aten::lt(%3624, %3613)
              %3626 : bool = aten::__and__(%3625, %3623)
              -> (%3626, %is_float_scale_factor.8, %3624)
           = prim::If(%is_float_scale_factor.2) # torch/nn/functional.py:2999:8
            block0():
               = aten::warn(%3384, %3374) # torch/nn/functional.py:3000:12
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3627 : int[] = prim::ListConstruct()
       = prim::Loop(%dim.2, %3375) # torch/nn/functional.py:3011:11
        block0(%i.7 : int):
          %3629 : int = aten::add(%i.7, %3374) # torch/nn/functional.py:3011:44
          %3630 : int = aten::size(%input.2, %3629) # torch/nn/functional.py:3011:33
          %3631 : float = aten::Float(%3630) # torch/nn/functional.py:3011:27
          %3632 : float = aten::__getitem__(%scale_factor.5, %i.7) # torch/nn/functional.py:3011:54
          %3633 : float = aten::mul(%3631, %3632) # torch/nn/functional.py:3011:27
          %3634 : int = aten::floor(%3633) # torch/nn/functional.py:3011:16
          %3635 : int[] = aten::append(%3627, %3634) # torch/nn/functional.py:3011:11
          -> (%3375)
      -> (%3627)
  %3636 : int = aten::dim(%x.10) # torch/nn/functional.py:3155:9
  %3637 : bool = aten::eq(%3636, %3382) # torch/nn/functional.py:3155:9
  %3638 : Tensor = prim::If(%3637) # torch/nn/functional.py:3155:4
    block0():
       = prim::RaiseException(%3373) # torch/nn/functional.py:3156:8
      -> (%3568)
    block1():
      %3639 : int = aten::dim(%x.10) # torch/nn/functional.py:3161:9
      %3640 : bool = aten::eq(%3639, %3372) # torch/nn/functional.py:3161:9
      %3641 : Tensor = prim::If(%3640) # torch/nn/functional.py:3161:4
        block0():
          %3642 : bool = aten::__isnot__(%align_corners.61, %3389) # torch/nn/functional.py:3162:15
          %align_corners.62 : bool = prim::If(%3642) # torch/nn/functional.py:3162:8
            block0():
              %align_corners.32 : bool = prim::unchecked_cast(%align_corners.61)
              -> (%align_corners.32)
            block1():
               = prim::RaiseException(%3373) # torch/nn/functional.py:3162:8
              -> (%3570)
          %3645 : float? = aten::__getitem__(%scale_factor_list.3, %3376) # torch/nn/functional.py:3163:83
          %3646 : float? = aten::__getitem__(%scale_factor_list.3, %3369) # torch/nn/functional.py:3163:91
          %3647 : Tensor = aten::upsample_bilinear2d(%x.10, %output_size.2, %align_corners.62, %3645, %3646) # torch/nn/functional.py:3163:15
          -> (%3647)
        block1():
          %3648 : int = aten::dim(%x.10) # torch/nn/functional.py:3168:9
          %3649 : bool = aten::eq(%3648, %3383) # torch/nn/functional.py:3168:9
           = prim::If(%3649) # torch/nn/functional.py:3168:4
            block0():
               = prim::RaiseException(%3373) # torch/nn/functional.py:3169:8
              -> ()
            block1():
               = prim::RaiseException(%3373) # torch/nn/functional.py:3177:8
              -> ()
          -> (%3568)
      -> (%3641)
  %3650 : Tensor[] = aten::append(%res.1, %3638) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %res.8 : Tensor = aten::cat(%res.1, %3369) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:92:14
  %3652 : __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential = prim::GetAttr[name="project"](%3391)
  %3653 : __torch__.torch.nn.modules.conv.___torch_mangle_59.Conv2d = prim::GetAttr[name="0"](%3652)
  %3654 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%3652)
  %3655 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="3"](%3652)
  %3656 : Tensor = prim::GetAttr[name="weight"](%3653)
  %3657 : Tensor? = prim::GetAttr[name="bias"](%3653)
  %3658 : int[] = prim::ListConstruct(%3369, %3369)
  %3659 : int[] = prim::ListConstruct(%3376, %3376)
  %3660 : int[] = prim::ListConstruct(%3369, %3369)
  %input.3 : Tensor = aten::conv2d(%res.8, %3656, %3657, %3658, %3659, %3660, %3369) # torch/nn/modules/conv.py:415:15
  %3662 : int = aten::dim(%input.3) # torch/nn/modules/batchnorm.py:276:11
  %3663 : bool = aten::ne(%3662, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3663) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3664 : bool = prim::GetAttr[name="training"](%3654)
   = prim::If(%3664) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3665 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3654)
      %3666 : Tensor = aten::add(%3665, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3654, %3666)
      -> ()
    block1():
      -> ()
  %3667 : bool = prim::GetAttr[name="training"](%3654)
  %3668 : Tensor = prim::GetAttr[name="running_mean"](%3654)
  %3669 : Tensor = prim::GetAttr[name="running_var"](%3654)
  %3670 : Tensor = prim::GetAttr[name="weight"](%3654)
  %3671 : Tensor = prim::GetAttr[name="bias"](%3654)
   = prim::If(%3667) # torch/nn/functional.py:2011:4
    block0():
      %3672 : int[] = aten::size(%input.3) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%3672, %3376) # torch/nn/functional.py:1991:17
      %3674 : int = aten::len(%3672) # torch/nn/functional.py:1992:19
      %3675 : int = aten::sub(%3674, %3374) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%3675, %3375, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.26 : int):
          %3679 : int = aten::add(%i.8, %3374) # torch/nn/functional.py:1993:27
          %3680 : int = aten::__getitem__(%3672, %3679) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %3680) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.27)
      %3682 : bool = aten::eq(%size_prods.25, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3682) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.5 : Tensor = aten::batch_norm(%input.3, %3670, %3671, %3668, %3669, %3667, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.7 : Tensor = aten::relu(%input.5) # torch/nn/functional.py:1119:17
  %3685 : bool = prim::GetAttr[name="training"](%3655)
  %input.20 : Tensor = aten::dropout(%input.7, %3390, %3685) # torch/nn/functional.py:973:17
  %3687 : Tensor = prim::GetAttr[name="weight"](%3392)
  %3688 : Tensor? = prim::GetAttr[name="bias"](%3392)
  %3689 : int[] = prim::ListConstruct(%3369, %3369)
  %3690 : int[] = prim::ListConstruct(%3369, %3369)
  %3691 : int[] = prim::ListConstruct(%3369, %3369)
  %input.21 : Tensor = aten::conv2d(%input.20, %3687, %3688, %3689, %3690, %3691, %3369) # torch/nn/modules/conv.py:415:15
  %3693 : int = aten::dim(%input.21) # torch/nn/modules/batchnorm.py:276:11
  %3694 : bool = aten::ne(%3693, %3372) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3694) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%3373) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3695 : bool = prim::GetAttr[name="training"](%3393)
   = prim::If(%3695) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3696 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3393)
      %3697 : Tensor = aten::add(%3696, %3369, %3369) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3393, %3697)
      -> ()
    block1():
      -> ()
  %3698 : bool = prim::GetAttr[name="training"](%3393)
  %3699 : Tensor = prim::GetAttr[name="running_mean"](%3393)
  %3700 : Tensor = prim::GetAttr[name="running_var"](%3393)
  %3701 : Tensor = prim::GetAttr[name="weight"](%3393)
  %3702 : Tensor = prim::GetAttr[name="bias"](%3393)
   = prim::If(%3698) # torch/nn/functional.py:2011:4
    block0():
      %3703 : int[] = aten::size(%input.21) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3703, %3376) # torch/nn/functional.py:1991:17
      %3705 : int = aten::len(%3703) # torch/nn/functional.py:1992:19
      %3706 : int = aten::sub(%3705, %3374) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3706, %3375, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.113 : int, %size_prods.6 : int):
          %3710 : int = aten::add(%i.113, %3374) # torch/nn/functional.py:1993:27
          %3711 : int = aten::__getitem__(%3703, %3710) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3711) # torch/nn/functional.py:1993:8
          -> (%3375, %size_prods.3)
      %3713 : bool = aten::eq(%size_prods, %3369) # torch/nn/functional.py:1994:7
       = prim::If(%3713) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%3373) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.21, %3701, %3702, %3699, %3700, %3698, %exponential_average_factor.2, %3370, %3375) # torch/nn/functional.py:2014:11
  %input.23 : Tensor = aten::relu(%input.22) # torch/nn/functional.py:1119:17
  %3716 : Tensor = prim::GetAttr[name="weight"](%3394)
  %3717 : Tensor? = prim::GetAttr[name="bias"](%3394)
  %3718 : int[] = prim::ListConstruct(%3369, %3369)
  %3719 : int[] = prim::ListConstruct(%3376, %3376)
  %3720 : int[] = prim::ListConstruct(%3369, %3369)
  %x.7 : Tensor = aten::conv2d(%input.23, %3716, %3717, %3718, %3719, %3720, %3369) # torch/nn/modules/conv.py:415:15
  %3722 : str = prim::Constant[value="The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. "]() # torch/nn/functional.py:3000:26
  %3723 : int = prim::Constant[value=9223372036854775807]()
  %3724 : int = prim::Constant[value=5]() # torch/nn/functional.py:3144:24
  %3725 : int = prim::Constant[value=1]() # torch/nn/functional.py:3143:79
  %3726 : int = prim::Constant[value=4]() # torch/nn/functional.py:3142:24
  %3727 : int = prim::Constant[value=0]() # torch/nn/functional.py:3141:71
  %3728 : int = prim::Constant[value=3]() # torch/nn/functional.py:3140:22
  %3729 : int = prim::Constant[value=2]() # torch/nn/functional.py:3124:37
  %3730 : str = prim::Constant[value="nearest"]() # torch/nn/functional.py:3112:16
  %3731 : str = prim::Constant[value="area"]() # torch/nn/functional.py:3112:27
  %3732 : None = prim::Constant() # torch/nn/functional.py:3113:32
  %3733 : str = prim::Constant[value="Exception"]() # torch/nn/functional.py:3114:12
  %3734 : str = prim::Constant[value="Default upsampling behavior when mode={} is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details."]() # torch/nn/functional.py:3118:26
  %align_corners.9 : bool = prim::Constant[value=0]() # torch/nn/functional.py:3122:28
  %3736 : bool = prim::Constant[value=1]() # torch/nn/functional.py:3125:66
  %3737 : str = prim::Constant[value="linear"]() # torch/nn/functional.py:3152:38
  %3738 : str = prim::Constant[value="bilinear"]() # torch/nn/functional.py:3155:38
  %3739 : str = prim::Constant[value="trilinear"]() # torch/nn/functional.py:3157:38
  %3740 : str = prim::Constant[value="bicubic"]() # torch/nn/functional.py:3173:38
  %3741 : Tensor = prim::Uninitialized()
  %3742 : bool? = prim::Uninitialized()
  %3743 : bool = prim::Uninitialized()
  %3744 : str[] = prim::ListConstruct(%3730, %3731)
  %3745 : bool = aten::__contains__(%3744, %5) # torch/nn/functional.py:3112:7
  %align_corners.56 : bool? = prim::If(%3745) # torch/nn/functional.py:3112:4
    block0():
      %3747 : bool = aten::__isnot__(%4, %3732) # torch/nn/functional.py:3113:11
      %align_corners.54 : bool? = prim::If(%3747) # torch/nn/functional.py:3113:8
        block0():
           = prim::RaiseException(%3733) # torch/nn/functional.py:3114:12
          -> (%3742)
        block1():
          -> (%4)
      -> (%align_corners.54)
    block1():
      %3749 : bool = aten::__is__(%4, %3732) # torch/nn/functional.py:3117:11
      %align_corners.55 : bool = prim::If(%3749) # torch/nn/functional.py:3117:8
        block0():
          %3751 : str = aten::format(%3734, %5) # torch/nn/functional.py:3118:26
           = aten::warn(%3751, %3729) # torch/nn/functional.py:3118:12
          -> (%align_corners.9)
        block1():
          %align_corners.12 : bool = prim::unchecked_cast(%4)
          -> (%align_corners.12)
      -> (%align_corners.55)
  %3753 : int = aten::dim(%x.7) # torch/nn/functional.py:3124:23
  %scale_factor_len.1 : int = aten::sub(%3753, %3729) # torch/nn/functional.py:3124:23
  %scale_factor_list.1 : float?[] = prim::ListConstruct()
   = prim::Loop(%scale_factor_len.1, %3736) # torch/nn/functional.py:3125:66
    block0(%3756 : int):
      %3757 : float?[] = aten::append(%scale_factor_list.1, %3732) # torch/nn/functional.py:3125:66
      -> (%3736)
  %3758 : bool = aten::__isnot__(%3, %3732) # torch/nn/functional.py:3127:7
  %3759 : bool, %scale_factor.15 : float[]? = prim::If(%3758) # torch/nn/functional.py:3127:7
    block0():
      %scale_factor.4 : float[] = prim::unchecked_cast(%3)
      %3762 : bool = aten::__is__(%3, %align_corners.9) # torch/nn/functional.py:3127:37
      %3763 : bool = prim::If(%3762) # torch/nn/functional.py:3127:37
        block0():
          -> (%3736)
        block1():
          %3764 : bool = aten::__is__(%3, %3732) # torch/nn/functional.py:3127:72
          -> (%3764)
      -> (%3763, %scale_factor.4)
    block1():
      -> (%align_corners.9, %3)
  %scale_factor : float[]?, %scale_factor_list : float?[] = prim::If(%3759) # torch/nn/functional.py:3127:4
    block0():
      %scale_factor.7 : float[] = prim::unchecked_cast(%scale_factor.15)
      %scale_factor_list.2 : float?[] = prim::ListConstruct()
      %3769 : int = aten::len(%scale_factor.7) # torch/nn/functional.py:3132:70
       = prim::Loop(%3769, %3736) # torch/nn/functional.py:3132:70
        block0(%3770 : int):
          %elem.1 : float = aten::__getitem__(%scale_factor.7, %3770) # torch/nn/functional.py:3132:70
          %3772 : float?[] = aten::append(%scale_factor_list.2, %elem.1) # torch/nn/functional.py:3132:70
          -> (%3736)
      -> (%scale_factor.7, %scale_factor_list.2)
    block1():
      -> (%scale_factor.15, %scale_factor_list.1)
  %closed_over_args.1 : (Tensor, int[]?, float[]?, bool?) = prim::TupleConstruct(%x.7, %input_shape.1, %scale_factor, %3)
  %3774 : float[] = prim::Uninitialized()
  %3775 : float[]? = prim::Uninitialized()
  %3776 : int[]? = prim::Uninitialized()
  %input.1 : Tensor, %size.1 : int[]?, %scale_factor.1 : float[]?, %recompute_scale_factor.1 : bool? = prim::TupleUnpack(%closed_over_args.1)
  %3781 : int = aten::dim(%input.1) # torch/nn/functional.py:2966:10
  %dim.1 : int = aten::sub(%3781, %3729) # torch/nn/functional.py:2966:10
  %3783 : bool = aten::__is__(%size.1, %3732) # torch/nn/functional.py:2967:7
  %3784 : bool, %size.23 : int[]? = prim::If(%3783) # torch/nn/functional.py:2967:7
    block0():
      %3786 : bool = aten::__is__(%scale_factor.1, %3732) # torch/nn/functional.py:2967:24
      -> (%3786, %size.1)
    block1():
      %size.4 : int[] = prim::unchecked_cast(%size.1)
      -> (%align_corners.9, %size.4)
   = prim::If(%3784) # torch/nn/functional.py:2967:4
    block0():
       = prim::RaiseException(%3733) # torch/nn/functional.py:2968:8
      -> ()
    block1():
      -> ()
  %3788 : bool = aten::__isnot__(%size.23, %3732) # torch/nn/functional.py:2969:7
  %3789 : bool, %size.24 : int[]? = prim::If(%3788) # torch/nn/functional.py:2969:7
    block0():
      %size.8 : int[] = prim::unchecked_cast(%size.23)
      %3792 : bool = aten::__isnot__(%scale_factor.1, %3732) # torch/nn/functional.py:2969:28
      -> (%3792, %size.8)
    block1():
      -> (%align_corners.9, %size.23)
  %scale_factor.26 : float[]?, %size.25 : int[]? = prim::If(%3789) # torch/nn/functional.py:2969:4
    block0():
       = prim::RaiseException(%3733) # torch/nn/functional.py:2970:8
      -> (%3775, %3776)
    block1():
      -> (%scale_factor.1, %size.24)
  %3795 : bool = aten::__isnot__(%scale_factor.26, %3732) # torch/nn/functional.py:2971:7
  %scale_factor.27 : float[]? = prim::If(%3795) # torch/nn/functional.py:2971:4
    block0():
      %scale_factor.11 : float[] = prim::unchecked_cast(%scale_factor.26)
      %3798 : int = aten::len(%scale_factor.11) # torch/nn/functional.py:2973:15
      %3799 : bool = aten::ne(%3798, %dim.1) # torch/nn/functional.py:2973:15
       = prim::If(%3799) # torch/nn/functional.py:2973:12
        block0():
           = prim::RaiseException(%3733) # torch/nn/functional.py:2974:16
          -> ()
        block1():
          -> ()
      -> (%scale_factor.11)
    block1():
      -> (%scale_factor.26)
  %3800 : bool = aten::__isnot__(%size.25, %3732) # torch/nn/functional.py:2977:7
  %output_size.1 : int[] = prim::If(%3800) # torch/nn/functional.py:2977:4
    block0():
      %size.17 : int[] = prim::unchecked_cast(%size.25)
      -> (%size.17)
    block1():
      %3803 : bool = aten::__isnot__(%scale_factor.27, %3732) # torch/nn/functional.py:2983:11
      %scale_factor.2 : float[] = prim::If(%3803) # torch/nn/functional.py:2983:4
        block0():
          %scale_factor.21 : float[] = prim::unchecked_cast(%scale_factor.27)
          -> (%scale_factor.21)
        block1():
           = prim::RaiseException(%3733) # torch/nn/functional.py:2983:4
          -> (%3774)
      %3806 : bool = aten::__is__(%recompute_scale_factor.1, %3732) # torch/nn/functional.py:2989:7
       = prim::If(%3806) # torch/nn/functional.py:2989:4
        block0():
          %3807 : int = aten::len(%scale_factor.2) # torch/nn/functional.py:2994:8
          %3808 : bool = aten::gt(%3807, %3727)
          %is_float_scale_factor.1 : bool, %3810 : int = prim::Loop(%3723, %3808, %align_corners.9, %3727) # torch/nn/functional.py:2994:8
            block0(%3811 : int, %is_float_scale_factor.6 : bool, %3813 : int):
              %scale.2 : float = aten::__getitem__(%scale_factor.2, %3813) # torch/nn/functional.py:2994:8
              %3815 : int = aten::floor(%scale.2) # torch/nn/functional.py:2995:36
              %is_float_scale_factor.5 : bool = aten::ne(%3815, %scale.2) # torch/nn/functional.py:2995:36
              %3817 : bool = prim::If(%is_float_scale_factor.5) # torch/nn/functional.py:2996:12
                block0():
                  -> (%align_corners.9)
                block1():
                  -> (%3736)
              %3818 : int = aten::add(%3813, %3725)
              %3819 : bool = aten::lt(%3818, %3807)
              %3820 : bool = aten::__and__(%3819, %3817)
              -> (%3820, %is_float_scale_factor.5, %3818)
           = prim::If(%is_float_scale_factor.1) # torch/nn/functional.py:2999:8
            block0():
               = aten::warn(%3722, %3729) # torch/nn/functional.py:3000:12
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3821 : int[] = prim::ListConstruct()
       = prim::Loop(%dim.1, %3736) # torch/nn/functional.py:3011:11
        block0(%i.1 : int):
          %3823 : int = aten::add(%i.1, %3729) # torch/nn/functional.py:3011:44
          %3824 : int = aten::size(%input.1, %3823) # torch/nn/functional.py:3011:33
          %3825 : float = aten::Float(%3824) # torch/nn/functional.py:3011:27
          %3826 : float = aten::__getitem__(%scale_factor.2, %i.1) # torch/nn/functional.py:3011:54
          %3827 : float = aten::mul(%3825, %3826) # torch/nn/functional.py:3011:27
          %3828 : int = aten::floor(%3827) # torch/nn/functional.py:3011:16
          %3829 : int[] = aten::append(%3821, %3828) # torch/nn/functional.py:3011:11
          -> (%3736)
      -> (%3821)
  %3830 : int = aten::dim(%x.7) # torch/nn/functional.py:3140:7
  %3831 : bool = aten::eq(%3830, %3728) # torch/nn/functional.py:3140:7
  %3832 : bool = prim::If(%3831) # torch/nn/functional.py:3140:7
    block0():
      %3833 : bool = aten::eq(%5, %3730) # torch/nn/functional.py:3140:28
      -> (%3833)
    block1():
      -> (%align_corners.9)
  %x.11 : Tensor = prim::If(%3832) # torch/nn/functional.py:3140:4
    block0():
      %3835 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3141:67
      %3836 : Tensor = aten::upsample_nearest1d(%x.7, %output_size.1, %3835) # torch/nn/functional.py:3141:15
      -> (%3836)
    block1():
      %3837 : int = aten::dim(%x.7) # torch/nn/functional.py:3142:9
      %3838 : bool = aten::eq(%3837, %3726) # torch/nn/functional.py:3142:9
      %3839 : bool = prim::If(%3838) # torch/nn/functional.py:3142:9
        block0():
          %3840 : bool = aten::eq(%5, %3730) # torch/nn/functional.py:3142:30
          -> (%3840)
        block1():
          -> (%align_corners.9)
      %3841 : Tensor = prim::If(%3839) # torch/nn/functional.py:3142:4
        block0():
          %3842 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3143:67
          %3843 : float? = aten::__getitem__(%scale_factor_list, %3725) # torch/nn/functional.py:3143:75
          %3844 : Tensor = aten::upsample_nearest2d(%x.7, %output_size.1, %3842, %3843) # torch/nn/functional.py:3143:15
          -> (%3844)
        block1():
          %3845 : int = aten::dim(%x.7) # torch/nn/functional.py:3144:9
          %3846 : bool = aten::eq(%3845, %3724) # torch/nn/functional.py:3144:9
          %3847 : bool = prim::If(%3846) # torch/nn/functional.py:3144:9
            block0():
              %3848 : bool = aten::eq(%5, %3730) # torch/nn/functional.py:3144:30
              -> (%3848)
            block1():
              -> (%align_corners.9)
          %3849 : Tensor = prim::If(%3847) # torch/nn/functional.py:3144:4
            block0():
              %3850 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3145:67
              %3851 : float? = aten::__getitem__(%scale_factor_list, %3725) # torch/nn/functional.py:3145:75
              %3852 : float? = aten::__getitem__(%scale_factor_list, %3729) # torch/nn/functional.py:3145:83
              %3853 : Tensor = aten::upsample_nearest3d(%x.7, %output_size.1, %3850, %3851, %3852) # torch/nn/functional.py:3145:15
              -> (%3853)
            block1():
              %3854 : int = aten::dim(%x.7) # torch/nn/functional.py:3146:9
              %3855 : bool = aten::eq(%3854, %3728) # torch/nn/functional.py:3146:9
              %3856 : bool = prim::If(%3855) # torch/nn/functional.py:3146:9
                block0():
                  %3857 : bool = aten::eq(%5, %3731) # torch/nn/functional.py:3146:30
                  -> (%3857)
                block1():
                  -> (%align_corners.9)
              %3858 : Tensor = prim::If(%3856) # torch/nn/functional.py:3146:4
                block0():
                  %3859 : Tensor = aten::adaptive_avg_pool1d(%x.7, %output_size.1) # torch/nn/functional.py:3147:15
                  -> (%3859)
                block1():
                  %3860 : int = aten::dim(%x.7) # torch/nn/functional.py:3148:9
                  %3861 : bool = aten::eq(%3860, %3726) # torch/nn/functional.py:3148:9
                  %3862 : bool = prim::If(%3861) # torch/nn/functional.py:3148:9
                    block0():
                      %3863 : bool = aten::eq(%5, %3731) # torch/nn/functional.py:3148:30
                      -> (%3863)
                    block1():
                      -> (%align_corners.9)
                  %3864 : Tensor = prim::If(%3862) # torch/nn/functional.py:3148:4
                    block0():
                      %3865 : int[] = aten::size(%x.7) # torch/nn/functional.py:925:51
                      %3866 : int = aten::len(%3865) # <string>:5:9
                      %3867 : int = aten::len(%output_size.1) # <string>:5:25
                      %3868 : bool = aten::gt(%3866, %3867) # <string>:5:9
                       = prim::If(%3868) # <string>:5:2
                        block0():
                          -> ()
                        block1():
                           = prim::RaiseException(%3733) # <string>:5:2
                          -> ()
                      %3869 : Tensor = aten::adaptive_avg_pool2d(%x.7, %output_size.1) # torch/nn/functional.py:926:11
                      -> (%3869)
                    block1():
                      %3870 : int = aten::dim(%x.7) # torch/nn/functional.py:3150:9
                      %3871 : bool = aten::eq(%3870, %3724) # torch/nn/functional.py:3150:9
                      %3872 : bool = prim::If(%3871) # torch/nn/functional.py:3150:9
                        block0():
                          %3873 : bool = aten::eq(%5, %3731) # torch/nn/functional.py:3150:30
                          -> (%3873)
                        block1():
                          -> (%align_corners.9)
                      %3874 : Tensor = prim::If(%3872) # torch/nn/functional.py:3150:4
                        block0():
                          %3875 : int[] = aten::size(%x.7) # torch/nn/functional.py:945:51
                          %3876 : int = aten::len(%3875) # <string>:5:9
                          %3877 : int = aten::len(%output_size.1) # <string>:5:25
                          %3878 : bool = aten::gt(%3876, %3877) # <string>:5:9
                           = prim::If(%3878) # <string>:5:2
                            block0():
                              -> ()
                            block1():
                               = prim::RaiseException(%3733) # <string>:5:2
                              -> ()
                          %3879 : Tensor = aten::adaptive_avg_pool3d(%x.7, %output_size.1) # torch/nn/functional.py:946:11
                          -> (%3879)
                        block1():
                          %3880 : int = aten::dim(%x.7) # torch/nn/functional.py:3152:9
                          %3881 : bool = aten::eq(%3880, %3728) # torch/nn/functional.py:3152:9
                          %3882 : bool = prim::If(%3881) # torch/nn/functional.py:3152:9
                            block0():
                              %3883 : bool = aten::eq(%5, %3737) # torch/nn/functional.py:3152:30
                              -> (%3883)
                            block1():
                              -> (%align_corners.9)
                          %3884 : Tensor = prim::If(%3882) # torch/nn/functional.py:3152:4
                            block0():
                              %3885 : bool = aten::__isnot__(%align_corners.56, %3732) # torch/nn/functional.py:3153:15
                              %align_corners.57 : bool = prim::If(%3885) # torch/nn/functional.py:3153:8
                                block0():
                                  %align_corners.24 : bool = prim::unchecked_cast(%align_corners.56)
                                  -> (%align_corners.24)
                                block1():
                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3153:8
                                  -> (%3743)
                              %3888 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3154:81
                              %3889 : Tensor = aten::upsample_linear1d(%x.7, %output_size.1, %align_corners.57, %3888) # torch/nn/functional.py:3154:15
                              -> (%3889)
                            block1():
                              %3890 : int = aten::dim(%x.7) # torch/nn/functional.py:3155:9
                              %3891 : bool = aten::eq(%3890, %3728) # torch/nn/functional.py:3155:9
                              %3892 : bool = prim::If(%3891) # torch/nn/functional.py:3155:9
                                block0():
                                  %3893 : bool = aten::eq(%5, %3738) # torch/nn/functional.py:3155:30
                                  -> (%3893)
                                block1():
                                  -> (%align_corners.9)
                              %3894 : Tensor = prim::If(%3892) # torch/nn/functional.py:3155:4
                                block0():
                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3156:8
                                  -> (%3741)
                                block1():
                                  %3895 : int = aten::dim(%x.7) # torch/nn/functional.py:3157:9
                                  %3896 : bool = aten::eq(%3895, %3728) # torch/nn/functional.py:3157:9
                                  %3897 : bool = prim::If(%3896) # torch/nn/functional.py:3157:9
                                    block0():
                                      %3898 : bool = aten::eq(%5, %3739) # torch/nn/functional.py:3157:30
                                      -> (%3898)
                                    block1():
                                      -> (%align_corners.9)
                                  %3899 : Tensor = prim::If(%3897) # torch/nn/functional.py:3157:4
                                    block0():
                                       = prim::RaiseException(%3733) # torch/nn/functional.py:3158:8
                                      -> (%3741)
                                    block1():
                                      %3900 : int = aten::dim(%x.7) # torch/nn/functional.py:3159:9
                                      %3901 : bool = aten::eq(%3900, %3726) # torch/nn/functional.py:3159:9
                                      %3902 : bool = prim::If(%3901) # torch/nn/functional.py:3159:9
                                        block0():
                                          %3903 : bool = aten::eq(%5, %3737) # torch/nn/functional.py:3159:30
                                          -> (%3903)
                                        block1():
                                          -> (%align_corners.9)
                                      %3904 : Tensor = prim::If(%3902) # torch/nn/functional.py:3159:4
                                        block0():
                                           = prim::RaiseException(%3733) # torch/nn/functional.py:3160:8
                                          -> (%3741)
                                        block1():
                                          %3905 : int = aten::dim(%x.7) # torch/nn/functional.py:3161:9
                                          %3906 : bool = aten::eq(%3905, %3726) # torch/nn/functional.py:3161:9
                                          %3907 : bool = prim::If(%3906) # torch/nn/functional.py:3161:9
                                            block0():
                                              %3908 : bool = aten::eq(%5, %3738) # torch/nn/functional.py:3161:30
                                              -> (%3908)
                                            block1():
                                              -> (%align_corners.9)
                                          %3909 : Tensor = prim::If(%3907) # torch/nn/functional.py:3161:4
                                            block0():
                                              %3910 : bool = aten::__isnot__(%align_corners.56, %3732) # torch/nn/functional.py:3162:15
                                              %align_corners.58 : bool = prim::If(%3910) # torch/nn/functional.py:3162:8
                                                block0():
                                                  %align_corners.31 : bool = prim::unchecked_cast(%align_corners.56)
                                                  -> (%align_corners.31)
                                                block1():
                                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3162:8
                                                  -> (%3743)
                                              %3913 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3163:83
                                              %3914 : float? = aten::__getitem__(%scale_factor_list, %3725) # torch/nn/functional.py:3163:91
                                              %3915 : Tensor = aten::upsample_bilinear2d(%x.7, %output_size.1, %align_corners.58, %3913, %3914) # torch/nn/functional.py:3163:15
                                              -> (%3915)
                                            block1():
                                              %3916 : int = aten::dim(%x.7) # torch/nn/functional.py:3164:9
                                              %3917 : bool = aten::eq(%3916, %3726) # torch/nn/functional.py:3164:9
                                              %3918 : bool = prim::If(%3917) # torch/nn/functional.py:3164:9
                                                block0():
                                                  %3919 : bool = aten::eq(%5, %3739) # torch/nn/functional.py:3164:30
                                                  -> (%3919)
                                                block1():
                                                  -> (%align_corners.9)
                                              %3920 : Tensor = prim::If(%3918) # torch/nn/functional.py:3164:4
                                                block0():
                                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3165:8
                                                  -> (%3741)
                                                block1():
                                                  %3921 : int = aten::dim(%x.7) # torch/nn/functional.py:3166:9
                                                  %3922 : bool = aten::eq(%3921, %3724) # torch/nn/functional.py:3166:9
                                                  %3923 : bool = prim::If(%3922) # torch/nn/functional.py:3166:9
                                                    block0():
                                                      %3924 : bool = aten::eq(%5, %3737) # torch/nn/functional.py:3166:30
                                                      -> (%3924)
                                                    block1():
                                                      -> (%align_corners.9)
                                                  %3925 : Tensor = prim::If(%3923) # torch/nn/functional.py:3166:4
                                                    block0():
                                                       = prim::RaiseException(%3733) # torch/nn/functional.py:3167:8
                                                      -> (%3741)
                                                    block1():
                                                      %3926 : int = aten::dim(%x.7) # torch/nn/functional.py:3168:9
                                                      %3927 : bool = aten::eq(%3926, %3724) # torch/nn/functional.py:3168:9
                                                      %3928 : bool = prim::If(%3927) # torch/nn/functional.py:3168:9
                                                        block0():
                                                          %3929 : bool = aten::eq(%5, %3738) # torch/nn/functional.py:3168:30
                                                          -> (%3929)
                                                        block1():
                                                          -> (%align_corners.9)
                                                      %3930 : Tensor = prim::If(%3928) # torch/nn/functional.py:3168:4
                                                        block0():
                                                           = prim::RaiseException(%3733) # torch/nn/functional.py:3169:8
                                                          -> (%3741)
                                                        block1():
                                                          %3931 : int = aten::dim(%x.7) # torch/nn/functional.py:3170:9
                                                          %3932 : bool = aten::eq(%3931, %3724) # torch/nn/functional.py:3170:9
                                                          %3933 : bool = prim::If(%3932) # torch/nn/functional.py:3170:9
                                                            block0():
                                                              %3934 : bool = aten::eq(%5, %3739) # torch/nn/functional.py:3170:30
                                                              -> (%3934)
                                                            block1():
                                                              -> (%align_corners.9)
                                                          %3935 : Tensor = prim::If(%3933) # torch/nn/functional.py:3170:4
                                                            block0():
                                                              %3936 : bool = aten::__isnot__(%align_corners.56, %3732) # torch/nn/functional.py:3171:15
                                                              %align_corners.59 : bool = prim::If(%3936) # torch/nn/functional.py:3171:8
                                                                block0():
                                                                  %align_corners.38 : bool = prim::unchecked_cast(%align_corners.56)
                                                                  -> (%align_corners.38)
                                                                block1():
                                                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3171:8
                                                                  -> (%3743)
                                                              %3939 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3172:84
                                                              %3940 : float? = aten::__getitem__(%scale_factor_list, %3725) # torch/nn/functional.py:3172:92
                                                              %3941 : float? = aten::__getitem__(%scale_factor_list, %3729) # torch/nn/functional.py:3172:100
                                                              %3942 : Tensor = aten::upsample_trilinear3d(%x.7, %output_size.1, %align_corners.59, %3939, %3940, %3941) # torch/nn/functional.py:3172:15
                                                              -> (%3942)
                                                            block1():
                                                              %3943 : int = aten::dim(%x.7) # torch/nn/functional.py:3173:9
                                                              %3944 : bool = aten::eq(%3943, %3726) # torch/nn/functional.py:3173:9
                                                              %3945 : bool = prim::If(%3944) # torch/nn/functional.py:3173:9
                                                                block0():
                                                                  %3946 : bool = aten::eq(%5, %3740) # torch/nn/functional.py:3173:30
                                                                  -> (%3946)
                                                                block1():
                                                                  -> (%align_corners.9)
                                                              %3947 : Tensor = prim::If(%3945) # torch/nn/functional.py:3173:4
                                                                block0():
                                                                  %3948 : bool = aten::__isnot__(%align_corners.56, %3732) # torch/nn/functional.py:3174:15
                                                                  %align_corners.60 : bool = prim::If(%3948) # torch/nn/functional.py:3174:8
                                                                    block0():
                                                                      %align_corners.45 : bool = prim::unchecked_cast(%align_corners.56)
                                                                      -> (%align_corners.45)
                                                                    block1():
                                                                       = prim::RaiseException(%3733) # torch/nn/functional.py:3174:8
                                                                      -> (%3743)
                                                                  %3951 : float? = aten::__getitem__(%scale_factor_list, %3727) # torch/nn/functional.py:3175:82
                                                                  %3952 : float? = aten::__getitem__(%scale_factor_list, %3725) # torch/nn/functional.py:3175:90
                                                                  %3953 : Tensor = aten::upsample_bicubic2d(%x.7, %output_size.1, %align_corners.60, %3951, %3952) # torch/nn/functional.py:3175:15
                                                                  -> (%3953)
                                                                block1():
                                                                   = prim::RaiseException(%3733) # torch/nn/functional.py:3177:8
                                                                  -> (%3741)
                                                              -> (%3947)
                                                          -> (%3935)
                                                      -> (%3930)
                                                  -> (%3925)
                                              -> (%3920)
                                          -> (%3909)
                                      -> (%3904)
                                  -> (%3899)
                              -> (%3894)
                          -> (%3884)
                      -> (%3874)
                  -> (%3864)
              -> (%3858)
          -> (%3849)
      -> (%3841)
   = aten::_set_item(%result.1, %6, %x.11) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:26:8
  return (%result.1)
