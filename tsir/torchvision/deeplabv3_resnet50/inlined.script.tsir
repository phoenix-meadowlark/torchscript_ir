graph(%self : __torch__.torchvision.models.segmentation.deeplabv3.___torch_mangle_70.DeepLabV3,
      %x.1 : Tensor):
  %2 : Function = prim::Constant[name="interpolate"]()
  %3 : None = prim::Constant()
  %4 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:25:78
  %5 : str = prim::Constant[value="bilinear"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:25:52
  %6 : str = prim::Constant[value="out"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:23:21
  %7 : int = prim::Constant[value=9223372036854775807]()
  %8 : int = prim::Constant[value=-2]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:30
  %9 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:22
  %10 : int[] = aten::size(%x.1) # <string>:7:9
  %input_shape.1 : int[] = aten::slice(%10, %8, %7, %9) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:18:22
  %12 : __torch__.torchvision.models._utils.___torch_mangle_69.IntermediateLayerGetter = prim::GetAttr[name="backbone"](%self)
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.1 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %name.1 : str = prim::Constant[value="conv1"]()
  %name.4 : str = prim::Constant[value="bn1"]()
  %name.7 : str = prim::Constant[value="relu"]()
  %name.10 : str = prim::Constant[value="maxpool"]()
  %name.13 : str = prim::Constant[value="layer1"]()
  %name.16 : str = prim::Constant[value="layer2"]()
  %name.19 : str = prim::Constant[value="layer3"]()
  %name.22 : str = prim::Constant[value="layer4"]()
  %features.1 : Dict(str, Tensor) = aten::dict() # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:61:14
  %38 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv1"](%12)
  %39 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%12)
  %40 : __torch__.torch.nn.modules.container.___torch_mangle_16.Sequential = prim::GetAttr[name="layer1"](%12)
  %41 : __torch__.torch.nn.modules.container.___torch_mangle_28.Sequential = prim::GetAttr[name="layer2"](%12)
  %42 : __torch__.torch.nn.modules.container.___torch_mangle_68.Sequential = prim::GetAttr[name="layer3"](%12)
  %43 : __torch__.torch.nn.modules.container.___torch_mangle_49.Sequential = prim::GetAttr[name="layer4"](%12)
  %44 : Tensor = prim::GetAttr[name="weight"](%38)
  %45 : Tensor? = prim::GetAttr[name="bias"](%38)
  %46 : int[] = prim::ListConstruct(%26, %26)
  %47 : int[] = prim::ListConstruct(%28, %28)
  %48 : int[] = prim::ListConstruct(%27, %27)
  %x.3 : Tensor = aten::conv2d(%x.1, %44, %45, %46, %47, %48, %27) # torch/nn/modules/conv.py:415:15
  %50 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %51 : bool = aten::__contains__(%50, %name.1) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%51) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %52 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.1 : str = aten::__getitem__(%52, %name.1) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.1, %x.3) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %54 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %55 : bool = aten::ne(%54, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%55) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %56 : bool = prim::GetAttr[name="training"](%39)
   = prim::If(%56) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %57 : Tensor = prim::GetAttr[name="num_batches_tracked"](%39)
      %58 : Tensor = aten::add(%57, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%39, %58)
      -> ()
    block1():
      -> ()
  %59 : bool = prim::GetAttr[name="training"](%39)
  %60 : Tensor = prim::GetAttr[name="running_mean"](%39)
  %61 : Tensor = prim::GetAttr[name="running_var"](%39)
  %62 : Tensor = prim::GetAttr[name="weight"](%39)
  %63 : Tensor = prim::GetAttr[name="bias"](%39)
   = prim::If(%59) # torch/nn/functional.py:2011:4
    block0():
      %64 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%64, %24) # torch/nn/functional.py:1991:17
      %66 : int = aten::len(%64) # torch/nn/functional.py:1992:19
      %67 : int = aten::sub(%66, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%67, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %71 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %72 : int = aten::__getitem__(%64, %71) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %72) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %74 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%74) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.13 : Tensor = aten::batch_norm(%x.3, %62, %63, %60, %61, %59, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %76 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %77 : bool = aten::__contains__(%76, %name.4) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%77) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %78 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.3 : str = aten::__getitem__(%78, %name.4) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.3, %x.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %x.9 : Tensor = aten::relu_(%x.13) # torch/nn/functional.py:1117:17
  %81 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %82 : bool = aten::__contains__(%81, %name.7) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %83 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.5 : str = aten::__getitem__(%83, %name.7) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.5, %x.9) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %85 : int[] = prim::ListConstruct(%28, %28)
  %86 : int[] = prim::ListConstruct(%26, %26)
  %87 : int[] = prim::ListConstruct(%27, %27)
  %88 : int[] = prim::ListConstruct(%27, %27)
  %x.12 : Tensor = aten::max_pool2d(%x.9, %85, %86, %87, %88, %19) # torch/nn/functional.py:575:11
  %90 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %91 : bool = aten::__contains__(%90, %name.10) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%91) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %92 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.7 : str = aten::__getitem__(%92, %name.10) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.7, %x.12) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %94 : __torch__.torchvision.models.resnet.Bottleneck = prim::GetAttr[name="0"](%40)
  %95 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="1"](%40)
  %96 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="2"](%40)
  %97 : __torch__.torch.nn.modules.conv.___torch_mangle_9.Conv2d = prim::GetAttr[name="conv1"](%94)
  %98 : Tensor = prim::GetAttr[name="weight"](%97)
  %99 : Tensor? = prim::GetAttr[name="bias"](%97)
  %100 : int[] = prim::ListConstruct(%27, %27)
  %101 : int[] = prim::ListConstruct(%24, %24)
  %102 : int[] = prim::ListConstruct(%27, %27)
  %out.103 : Tensor = aten::conv2d(%x.12, %98, %99, %100, %101, %102, %27) # torch/nn/modules/conv.py:415:15
  %104 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%94)
  %105 : int = aten::dim(%out.103) # torch/nn/modules/batchnorm.py:276:11
  %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %107 : bool = prim::GetAttr[name="training"](%104)
   = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
      %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%104, %109)
      -> ()
    block1():
      -> ()
  %110 : bool = prim::GetAttr[name="training"](%104)
  %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
  %112 : Tensor = prim::GetAttr[name="running_var"](%104)
  %113 : Tensor = prim::GetAttr[name="weight"](%104)
  %114 : Tensor = prim::GetAttr[name="bias"](%104)
   = prim::If(%110) # torch/nn/functional.py:2011:4
    block0():
      %115 : int[] = aten::size(%out.103) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
      %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
      %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%118, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %122 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %123) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %125 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%125) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.104 : Tensor = aten::batch_norm(%out.103, %113, %114, %111, %112, %110, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.105 : Tensor = aten::relu_(%out.104) # torch/nn/functional.py:1117:17
  %128 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%94)
  %129 : Tensor = prim::GetAttr[name="weight"](%128)
  %130 : Tensor? = prim::GetAttr[name="bias"](%128)
  %131 : int[] = prim::ListConstruct(%27, %27)
  %132 : int[] = prim::ListConstruct(%27, %27)
  %133 : int[] = prim::ListConstruct(%27, %27)
  %out.106 : Tensor = aten::conv2d(%out.105, %129, %130, %131, %132, %133, %27) # torch/nn/modules/conv.py:415:15
  %135 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%94)
  %136 : int = aten::dim(%out.106) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%out.106) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%149, %25, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %153 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.83)
      %156 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.107 : Tensor = aten::batch_norm(%out.106, %144, %145, %142, %143, %141, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.108 : Tensor = aten::relu_(%out.107) # torch/nn/functional.py:1117:17
  %159 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%94)
  %160 : Tensor = prim::GetAttr[name="weight"](%159)
  %161 : Tensor? = prim::GetAttr[name="bias"](%159)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%24, %24)
  %164 : int[] = prim::ListConstruct(%27, %27)
  %out.109 : Tensor = aten::conv2d(%out.108, %160, %161, %162, %163, %164, %27) # torch/nn/modules/conv.py:415:15
  %166 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%94)
  %167 : int = aten::dim(%out.109) # torch/nn/modules/batchnorm.py:276:11
  %168 : bool = aten::ne(%167, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%168) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %169 : bool = prim::GetAttr[name="training"](%166)
   = prim::If(%169) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %170 : Tensor = prim::GetAttr[name="num_batches_tracked"](%166)
      %171 : Tensor = aten::add(%170, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%166, %171)
      -> ()
    block1():
      -> ()
  %172 : bool = prim::GetAttr[name="training"](%166)
  %173 : Tensor = prim::GetAttr[name="running_mean"](%166)
  %174 : Tensor = prim::GetAttr[name="running_var"](%166)
  %175 : Tensor = prim::GetAttr[name="weight"](%166)
  %176 : Tensor = prim::GetAttr[name="bias"](%166)
   = prim::If(%172) # torch/nn/functional.py:2011:4
    block0():
      %177 : int[] = aten::size(%out.109) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%177, %24) # torch/nn/functional.py:1991:17
      %179 : int = aten::len(%177) # torch/nn/functional.py:1992:19
      %180 : int = aten::sub(%179, %26) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%180, %25, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %184 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
          %185 : int = aten::__getitem__(%177, %184) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %185) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.139)
      %187 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
       = prim::If(%187) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.101 : Tensor = aten::batch_norm(%out.109, %175, %176, %173, %174, %172, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %189 : __torch__.torch.nn.modules.container.___torch_mangle_13.Sequential = prim::GetAttr[name="downsample"](%94)
  %190 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="0"](%189)
  %191 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%189)
  %192 : Tensor = prim::GetAttr[name="weight"](%190)
  %193 : Tensor? = prim::GetAttr[name="bias"](%190)
  %194 : int[] = prim::ListConstruct(%27, %27)
  %195 : int[] = prim::ListConstruct(%24, %24)
  %196 : int[] = prim::ListConstruct(%27, %27)
  %input.34 : Tensor = aten::conv2d(%x.12, %192, %193, %194, %195, %196, %27) # torch/nn/modules/conv.py:415:15
  %198 : int = aten::dim(%input.34) # torch/nn/modules/batchnorm.py:276:11
  %199 : bool = aten::ne(%198, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%199) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %200 : bool = prim::GetAttr[name="training"](%191)
   = prim::If(%200) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %201 : Tensor = prim::GetAttr[name="num_batches_tracked"](%191)
      %202 : Tensor = aten::add(%201, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%191, %202)
      -> ()
    block1():
      -> ()
  %203 : bool = prim::GetAttr[name="training"](%191)
  %204 : Tensor = prim::GetAttr[name="running_mean"](%191)
  %205 : Tensor = prim::GetAttr[name="running_var"](%191)
  %206 : Tensor = prim::GetAttr[name="weight"](%191)
  %207 : Tensor = prim::GetAttr[name="bias"](%191)
   = prim::If(%203) # torch/nn/functional.py:2011:4
    block0():
      %208 : int[] = aten::size(%input.34) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%208, %24) # torch/nn/functional.py:1991:17
      %210 : int = aten::len(%208) # torch/nn/functional.py:1992:19
      %211 : int = aten::sub(%210, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%211, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %215 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %216 : int = aten::__getitem__(%208, %215) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %216) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %218 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%218) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.2 : Tensor = aten::batch_norm(%input.34, %206, %207, %204, %205, %203, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.102 : Tensor = aten::add_(%out.101, %identity.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.37 : Tensor = aten::relu_(%out.102) # torch/nn/functional.py:1117:17
  %222 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%95)
  %223 : Tensor = prim::GetAttr[name="weight"](%222)
  %224 : Tensor? = prim::GetAttr[name="bias"](%222)
  %225 : int[] = prim::ListConstruct(%27, %27)
  %226 : int[] = prim::ListConstruct(%24, %24)
  %227 : int[] = prim::ListConstruct(%27, %27)
  %out.93 : Tensor = aten::conv2d(%input.37, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
  %229 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%95)
  %230 : int = aten::dim(%out.93) # torch/nn/modules/batchnorm.py:276:11
  %231 : bool = aten::ne(%230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %232 : bool = prim::GetAttr[name="training"](%229)
   = prim::If(%232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%229)
      %234 : Tensor = aten::add(%233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%229, %234)
      -> ()
    block1():
      -> ()
  %235 : bool = prim::GetAttr[name="training"](%229)
  %236 : Tensor = prim::GetAttr[name="running_mean"](%229)
  %237 : Tensor = prim::GetAttr[name="running_var"](%229)
  %238 : Tensor = prim::GetAttr[name="weight"](%229)
  %239 : Tensor = prim::GetAttr[name="bias"](%229)
   = prim::If(%235) # torch/nn/functional.py:2011:4
    block0():
      %240 : int[] = aten::size(%out.93) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%240, %24) # torch/nn/functional.py:1991:17
      %242 : int = aten::len(%240) # torch/nn/functional.py:1992:19
      %243 : int = aten::sub(%242, %26) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%243, %25, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %247 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
          %248 : int = aten::__getitem__(%240, %247) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.147)
      %250 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
       = prim::If(%250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.94 : Tensor = aten::batch_norm(%out.93, %238, %239, %236, %237, %235, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.95 : Tensor = aten::relu_(%out.94) # torch/nn/functional.py:1117:17
  %253 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%95)
  %254 : Tensor = prim::GetAttr[name="weight"](%253)
  %255 : Tensor? = prim::GetAttr[name="bias"](%253)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %out.96 : Tensor = aten::conv2d(%out.95, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%95)
  %261 : int = aten::dim(%out.96) # torch/nn/modules/batchnorm.py:276:11
  %262 : bool = aten::ne(%261, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%262) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %263 : bool = prim::GetAttr[name="training"](%260)
   = prim::If(%263) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %264 : Tensor = prim::GetAttr[name="num_batches_tracked"](%260)
      %265 : Tensor = aten::add(%264, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%260, %265)
      -> ()
    block1():
      -> ()
  %266 : bool = prim::GetAttr[name="training"](%260)
  %267 : Tensor = prim::GetAttr[name="running_mean"](%260)
  %268 : Tensor = prim::GetAttr[name="running_var"](%260)
  %269 : Tensor = prim::GetAttr[name="weight"](%260)
  %270 : Tensor = prim::GetAttr[name="bias"](%260)
   = prim::If(%266) # torch/nn/functional.py:2011:4
    block0():
      %271 : int[] = aten::size(%out.96) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%271, %24) # torch/nn/functional.py:1991:17
      %273 : int = aten::len(%271) # torch/nn/functional.py:1992:19
      %274 : int = aten::sub(%273, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%274, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %278 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %279 : int = aten::__getitem__(%271, %278) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %279) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %281 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%281) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.97 : Tensor = aten::batch_norm(%out.96, %269, %270, %267, %268, %266, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.98 : Tensor = aten::relu_(%out.97) # torch/nn/functional.py:1117:17
  %284 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%95)
  %285 : Tensor = prim::GetAttr[name="weight"](%284)
  %286 : Tensor? = prim::GetAttr[name="bias"](%284)
  %287 : int[] = prim::ListConstruct(%27, %27)
  %288 : int[] = prim::ListConstruct(%24, %24)
  %289 : int[] = prim::ListConstruct(%27, %27)
  %out.99 : Tensor = aten::conv2d(%out.98, %285, %286, %287, %288, %289, %27) # torch/nn/modules/conv.py:415:15
  %291 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%95)
  %292 : int = aten::dim(%out.99) # torch/nn/modules/batchnorm.py:276:11
  %293 : bool = aten::ne(%292, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%293) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %294 : bool = prim::GetAttr[name="training"](%291)
   = prim::If(%294) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %295 : Tensor = prim::GetAttr[name="num_batches_tracked"](%291)
      %296 : Tensor = aten::add(%295, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%291, %296)
      -> ()
    block1():
      -> ()
  %297 : bool = prim::GetAttr[name="training"](%291)
  %298 : Tensor = prim::GetAttr[name="running_mean"](%291)
  %299 : Tensor = prim::GetAttr[name="running_var"](%291)
  %300 : Tensor = prim::GetAttr[name="weight"](%291)
  %301 : Tensor = prim::GetAttr[name="bias"](%291)
   = prim::If(%297) # torch/nn/functional.py:2011:4
    block0():
      %302 : int[] = aten::size(%out.99) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%302, %24) # torch/nn/functional.py:1991:17
      %304 : int = aten::len(%302) # torch/nn/functional.py:1992:19
      %305 : int = aten::sub(%304, %26) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%305, %25, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %309 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
          %310 : int = aten::__getitem__(%302, %309) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %310) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.155)
      %312 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
       = prim::If(%312) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.100 : Tensor = aten::batch_norm(%out.99, %300, %301, %298, %299, %297, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.92 : Tensor = aten::add_(%out.100, %input.37, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.35 : Tensor = aten::relu_(%out.92) # torch/nn/functional.py:1117:17
  %316 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%96)
  %317 : Tensor = prim::GetAttr[name="weight"](%316)
  %318 : Tensor? = prim::GetAttr[name="bias"](%316)
  %319 : int[] = prim::ListConstruct(%27, %27)
  %320 : int[] = prim::ListConstruct(%24, %24)
  %321 : int[] = prim::ListConstruct(%27, %27)
  %out.56 : Tensor = aten::conv2d(%input.35, %317, %318, %319, %320, %321, %27) # torch/nn/modules/conv.py:415:15
  %323 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%96)
  %324 : int = aten::dim(%out.56) # torch/nn/modules/batchnorm.py:276:11
  %325 : bool = aten::ne(%324, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%325) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %326 : bool = prim::GetAttr[name="training"](%323)
   = prim::If(%326) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %327 : Tensor = prim::GetAttr[name="num_batches_tracked"](%323)
      %328 : Tensor = aten::add(%327, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%323, %328)
      -> ()
    block1():
      -> ()
  %329 : bool = prim::GetAttr[name="training"](%323)
  %330 : Tensor = prim::GetAttr[name="running_mean"](%323)
  %331 : Tensor = prim::GetAttr[name="running_var"](%323)
  %332 : Tensor = prim::GetAttr[name="weight"](%323)
  %333 : Tensor = prim::GetAttr[name="bias"](%323)
   = prim::If(%329) # torch/nn/functional.py:2011:4
    block0():
      %334 : int[] = aten::size(%out.56) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%334, %24) # torch/nn/functional.py:1991:17
      %336 : int = aten::len(%334) # torch/nn/functional.py:1992:19
      %337 : int = aten::sub(%336, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%337, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %341 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %342 : int = aten::__getitem__(%334, %341) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %342) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %344 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%344) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.57 : Tensor = aten::batch_norm(%out.56, %332, %333, %330, %331, %329, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.58 : Tensor = aten::relu_(%out.57) # torch/nn/functional.py:1117:17
  %347 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%96)
  %348 : Tensor = prim::GetAttr[name="weight"](%347)
  %349 : Tensor? = prim::GetAttr[name="bias"](%347)
  %350 : int[] = prim::ListConstruct(%27, %27)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %out.59 : Tensor = aten::conv2d(%out.58, %348, %349, %350, %351, %352, %27) # torch/nn/modules/conv.py:415:15
  %354 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%96)
  %355 : int = aten::dim(%out.59) # torch/nn/modules/batchnorm.py:276:11
  %356 : bool = aten::ne(%355, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%356) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %357 : bool = prim::GetAttr[name="training"](%354)
   = prim::If(%357) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %358 : Tensor = prim::GetAttr[name="num_batches_tracked"](%354)
      %359 : Tensor = aten::add(%358, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%354, %359)
      -> ()
    block1():
      -> ()
  %360 : bool = prim::GetAttr[name="training"](%354)
  %361 : Tensor = prim::GetAttr[name="running_mean"](%354)
  %362 : Tensor = prim::GetAttr[name="running_var"](%354)
  %363 : Tensor = prim::GetAttr[name="weight"](%354)
  %364 : Tensor = prim::GetAttr[name="bias"](%354)
   = prim::If(%360) # torch/nn/functional.py:2011:4
    block0():
      %365 : int[] = aten::size(%out.59) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%365, %24) # torch/nn/functional.py:1991:17
      %367 : int = aten::len(%365) # torch/nn/functional.py:1992:19
      %368 : int = aten::sub(%367, %26) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%368, %25, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %372 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
          %373 : int = aten::__getitem__(%365, %372) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %373) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.91)
      %375 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
       = prim::If(%375) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.60 : Tensor = aten::batch_norm(%out.59, %363, %364, %361, %362, %360, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.61 : Tensor = aten::relu_(%out.60) # torch/nn/functional.py:1117:17
  %378 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%96)
  %379 : Tensor = prim::GetAttr[name="weight"](%378)
  %380 : Tensor? = prim::GetAttr[name="bias"](%378)
  %381 : int[] = prim::ListConstruct(%27, %27)
  %382 : int[] = prim::ListConstruct(%24, %24)
  %383 : int[] = prim::ListConstruct(%27, %27)
  %out.62 : Tensor = aten::conv2d(%out.61, %379, %380, %381, %382, %383, %27) # torch/nn/modules/conv.py:415:15
  %385 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%96)
  %386 : int = aten::dim(%out.62) # torch/nn/modules/batchnorm.py:276:11
  %387 : bool = aten::ne(%386, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%387) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %388 : bool = prim::GetAttr[name="training"](%385)
   = prim::If(%388) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %389 : Tensor = prim::GetAttr[name="num_batches_tracked"](%385)
      %390 : Tensor = aten::add(%389, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%385, %390)
      -> ()
    block1():
      -> ()
  %391 : bool = prim::GetAttr[name="training"](%385)
  %392 : Tensor = prim::GetAttr[name="running_mean"](%385)
  %393 : Tensor = prim::GetAttr[name="running_var"](%385)
  %394 : Tensor = prim::GetAttr[name="weight"](%385)
  %395 : Tensor = prim::GetAttr[name="bias"](%385)
   = prim::If(%391) # torch/nn/functional.py:2011:4
    block0():
      %396 : int[] = aten::size(%out.62) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%396, %24) # torch/nn/functional.py:1991:17
      %398 : int = aten::len(%396) # torch/nn/functional.py:1992:19
      %399 : int = aten::sub(%398, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%399, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %403 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %404 : int = aten::__getitem__(%396, %403) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %404) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %406 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%406) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.63 : Tensor = aten::batch_norm(%out.62, %394, %395, %392, %393, %391, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.64 : Tensor = aten::add_(%out.63, %input.35, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.15 : Tensor = aten::relu_(%out.64) # torch/nn/functional.py:1117:17
  %410 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %411 : bool = aten::__contains__(%410, %name.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%411) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %412 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.9 : str = aten::__getitem__(%412, %name.13) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.9, %x.15) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %414 : __torch__.torchvision.models.resnet.___torch_mangle_24.Bottleneck = prim::GetAttr[name="0"](%41)
  %415 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="1"](%41)
  %416 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="2"](%41)
  %417 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="3"](%41)
  %418 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%414)
  %419 : Tensor = prim::GetAttr[name="weight"](%418)
  %420 : Tensor? = prim::GetAttr[name="bias"](%418)
  %421 : int[] = prim::ListConstruct(%27, %27)
  %422 : int[] = prim::ListConstruct(%24, %24)
  %423 : int[] = prim::ListConstruct(%27, %27)
  %out.65 : Tensor = aten::conv2d(%x.15, %419, %420, %421, %422, %423, %27) # torch/nn/modules/conv.py:415:15
  %425 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%414)
  %426 : int = aten::dim(%out.65) # torch/nn/modules/batchnorm.py:276:11
  %427 : bool = aten::ne(%426, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%427) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %428 : bool = prim::GetAttr[name="training"](%425)
   = prim::If(%428) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %429 : Tensor = prim::GetAttr[name="num_batches_tracked"](%425)
      %430 : Tensor = aten::add(%429, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%425, %430)
      -> ()
    block1():
      -> ()
  %431 : bool = prim::GetAttr[name="training"](%425)
  %432 : Tensor = prim::GetAttr[name="running_mean"](%425)
  %433 : Tensor = prim::GetAttr[name="running_var"](%425)
  %434 : Tensor = prim::GetAttr[name="weight"](%425)
  %435 : Tensor = prim::GetAttr[name="bias"](%425)
   = prim::If(%431) # torch/nn/functional.py:2011:4
    block0():
      %436 : int[] = aten::size(%out.65) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%436, %24) # torch/nn/functional.py:1991:17
      %438 : int = aten::len(%436) # torch/nn/functional.py:1992:19
      %439 : int = aten::sub(%438, %26) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%439, %25, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %443 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
          %444 : int = aten::__getitem__(%436, %443) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %444) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.99)
      %446 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
       = prim::If(%446) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.66 : Tensor = aten::batch_norm(%out.65, %434, %435, %432, %433, %431, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.67 : Tensor = aten::relu_(%out.66) # torch/nn/functional.py:1117:17
  %449 : __torch__.torch.nn.modules.conv.___torch_mangle_19.Conv2d = prim::GetAttr[name="conv2"](%414)
  %450 : Tensor = prim::GetAttr[name="weight"](%449)
  %451 : Tensor? = prim::GetAttr[name="bias"](%449)
  %452 : int[] = prim::ListConstruct(%26, %26)
  %453 : int[] = prim::ListConstruct(%27, %27)
  %454 : int[] = prim::ListConstruct(%27, %27)
  %out.68 : Tensor = aten::conv2d(%out.67, %450, %451, %452, %453, %454, %27) # torch/nn/modules/conv.py:415:15
  %456 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%414)
  %457 : int = aten::dim(%out.68) # torch/nn/modules/batchnorm.py:276:11
  %458 : bool = aten::ne(%457, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%458) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %459 : bool = prim::GetAttr[name="training"](%456)
   = prim::If(%459) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %460 : Tensor = prim::GetAttr[name="num_batches_tracked"](%456)
      %461 : Tensor = aten::add(%460, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%456, %461)
      -> ()
    block1():
      -> ()
  %462 : bool = prim::GetAttr[name="training"](%456)
  %463 : Tensor = prim::GetAttr[name="running_mean"](%456)
  %464 : Tensor = prim::GetAttr[name="running_var"](%456)
  %465 : Tensor = prim::GetAttr[name="weight"](%456)
  %466 : Tensor = prim::GetAttr[name="bias"](%456)
   = prim::If(%462) # torch/nn/functional.py:2011:4
    block0():
      %467 : int[] = aten::size(%out.68) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%467, %24) # torch/nn/functional.py:1991:17
      %469 : int = aten::len(%467) # torch/nn/functional.py:1992:19
      %470 : int = aten::sub(%469, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%470, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %474 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %475 : int = aten::__getitem__(%467, %474) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %475) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %477 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%477) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.69 : Tensor = aten::batch_norm(%out.68, %465, %466, %463, %464, %462, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.70 : Tensor = aten::relu_(%out.69) # torch/nn/functional.py:1117:17
  %480 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%414)
  %481 : Tensor = prim::GetAttr[name="weight"](%480)
  %482 : Tensor? = prim::GetAttr[name="bias"](%480)
  %483 : int[] = prim::ListConstruct(%27, %27)
  %484 : int[] = prim::ListConstruct(%24, %24)
  %485 : int[] = prim::ListConstruct(%27, %27)
  %out.71 : Tensor = aten::conv2d(%out.70, %481, %482, %483, %484, %485, %27) # torch/nn/modules/conv.py:415:15
  %487 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%414)
  %488 : int = aten::dim(%out.71) # torch/nn/modules/batchnorm.py:276:11
  %489 : bool = aten::ne(%488, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%489) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %490 : bool = prim::GetAttr[name="training"](%487)
   = prim::If(%490) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %491 : Tensor = prim::GetAttr[name="num_batches_tracked"](%487)
      %492 : Tensor = aten::add(%491, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%487, %492)
      -> ()
    block1():
      -> ()
  %493 : bool = prim::GetAttr[name="training"](%487)
  %494 : Tensor = prim::GetAttr[name="running_mean"](%487)
  %495 : Tensor = prim::GetAttr[name="running_var"](%487)
  %496 : Tensor = prim::GetAttr[name="weight"](%487)
  %497 : Tensor = prim::GetAttr[name="bias"](%487)
   = prim::If(%493) # torch/nn/functional.py:2011:4
    block0():
      %498 : int[] = aten::size(%out.71) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%498, %24) # torch/nn/functional.py:1991:17
      %500 : int = aten::len(%498) # torch/nn/functional.py:1992:19
      %501 : int = aten::sub(%500, %26) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%501, %25, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %505 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
          %506 : int = aten::__getitem__(%498, %505) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %506) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.107)
      %508 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
       = prim::If(%508) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.72 : Tensor = aten::batch_norm(%out.71, %496, %497, %494, %495, %493, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %510 : __torch__.torch.nn.modules.container.___torch_mangle_23.Sequential = prim::GetAttr[name="downsample"](%414)
  %511 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="0"](%510)
  %512 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="1"](%510)
  %513 : Tensor = prim::GetAttr[name="weight"](%511)
  %514 : Tensor? = prim::GetAttr[name="bias"](%511)
  %515 : int[] = prim::ListConstruct(%26, %26)
  %516 : int[] = prim::ListConstruct(%24, %24)
  %517 : int[] = prim::ListConstruct(%27, %27)
  %input.27 : Tensor = aten::conv2d(%x.15, %513, %514, %515, %516, %517, %27) # torch/nn/modules/conv.py:415:15
  %519 : int = aten::dim(%input.27) # torch/nn/modules/batchnorm.py:276:11
  %520 : bool = aten::ne(%519, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%520) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%512)
   = prim::If(%521) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %522 : Tensor = prim::GetAttr[name="num_batches_tracked"](%512)
      %523 : Tensor = aten::add(%522, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%512, %523)
      -> ()
    block1():
      -> ()
  %524 : bool = prim::GetAttr[name="training"](%512)
  %525 : Tensor = prim::GetAttr[name="running_mean"](%512)
  %526 : Tensor = prim::GetAttr[name="running_var"](%512)
  %527 : Tensor = prim::GetAttr[name="weight"](%512)
  %528 : Tensor = prim::GetAttr[name="bias"](%512)
   = prim::If(%524) # torch/nn/functional.py:2011:4
    block0():
      %529 : int[] = aten::size(%input.27) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%529, %24) # torch/nn/functional.py:1991:17
      %531 : int = aten::len(%529) # torch/nn/functional.py:1992:19
      %532 : int = aten::sub(%531, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%532, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %536 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %537 : int = aten::__getitem__(%529, %536) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %537) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %539 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%539) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.3 : Tensor = aten::batch_norm(%input.27, %527, %528, %525, %526, %524, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.73 : Tensor = aten::add_(%out.72, %identity.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.28 : Tensor = aten::relu_(%out.73) # torch/nn/functional.py:1117:17
  %543 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%415)
  %544 : Tensor = prim::GetAttr[name="weight"](%543)
  %545 : Tensor? = prim::GetAttr[name="bias"](%543)
  %546 : int[] = prim::ListConstruct(%27, %27)
  %547 : int[] = prim::ListConstruct(%24, %24)
  %548 : int[] = prim::ListConstruct(%27, %27)
  %out.74 : Tensor = aten::conv2d(%input.28, %544, %545, %546, %547, %548, %27) # torch/nn/modules/conv.py:415:15
  %550 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%415)
  %551 : int = aten::dim(%out.74) # torch/nn/modules/batchnorm.py:276:11
  %552 : bool = aten::ne(%551, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%552) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %553 : bool = prim::GetAttr[name="training"](%550)
   = prim::If(%553) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %554 : Tensor = prim::GetAttr[name="num_batches_tracked"](%550)
      %555 : Tensor = aten::add(%554, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%550, %555)
      -> ()
    block1():
      -> ()
  %556 : bool = prim::GetAttr[name="training"](%550)
  %557 : Tensor = prim::GetAttr[name="running_mean"](%550)
  %558 : Tensor = prim::GetAttr[name="running_var"](%550)
  %559 : Tensor = prim::GetAttr[name="weight"](%550)
  %560 : Tensor = prim::GetAttr[name="bias"](%550)
   = prim::If(%556) # torch/nn/functional.py:2011:4
    block0():
      %561 : int[] = aten::size(%out.74) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%561, %24) # torch/nn/functional.py:1991:17
      %563 : int = aten::len(%561) # torch/nn/functional.py:1992:19
      %564 : int = aten::sub(%563, %26) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%564, %25, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %568 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
          %569 : int = aten::__getitem__(%561, %568) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %569) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.115)
      %571 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
       = prim::If(%571) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.75 : Tensor = aten::batch_norm(%out.74, %559, %560, %557, %558, %556, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.76 : Tensor = aten::relu_(%out.75) # torch/nn/functional.py:1117:17
  %574 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%415)
  %575 : Tensor = prim::GetAttr[name="weight"](%574)
  %576 : Tensor? = prim::GetAttr[name="bias"](%574)
  %577 : int[] = prim::ListConstruct(%27, %27)
  %578 : int[] = prim::ListConstruct(%27, %27)
  %579 : int[] = prim::ListConstruct(%27, %27)
  %out.77 : Tensor = aten::conv2d(%out.76, %575, %576, %577, %578, %579, %27) # torch/nn/modules/conv.py:415:15
  %581 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%415)
  %582 : int = aten::dim(%out.77) # torch/nn/modules/batchnorm.py:276:11
  %583 : bool = aten::ne(%582, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%583) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %584 : bool = prim::GetAttr[name="training"](%581)
   = prim::If(%584) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %585 : Tensor = prim::GetAttr[name="num_batches_tracked"](%581)
      %586 : Tensor = aten::add(%585, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%581, %586)
      -> ()
    block1():
      -> ()
  %587 : bool = prim::GetAttr[name="training"](%581)
  %588 : Tensor = prim::GetAttr[name="running_mean"](%581)
  %589 : Tensor = prim::GetAttr[name="running_var"](%581)
  %590 : Tensor = prim::GetAttr[name="weight"](%581)
  %591 : Tensor = prim::GetAttr[name="bias"](%581)
   = prim::If(%587) # torch/nn/functional.py:2011:4
    block0():
      %592 : int[] = aten::size(%out.77) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%592, %24) # torch/nn/functional.py:1991:17
      %594 : int = aten::len(%592) # torch/nn/functional.py:1992:19
      %595 : int = aten::sub(%594, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%595, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %599 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %600 : int = aten::__getitem__(%592, %599) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %600) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %602 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%602) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.78 : Tensor = aten::batch_norm(%out.77, %590, %591, %588, %589, %587, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.79 : Tensor = aten::relu_(%out.78) # torch/nn/functional.py:1117:17
  %605 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%415)
  %606 : Tensor = prim::GetAttr[name="weight"](%605)
  %607 : Tensor? = prim::GetAttr[name="bias"](%605)
  %608 : int[] = prim::ListConstruct(%27, %27)
  %609 : int[] = prim::ListConstruct(%24, %24)
  %610 : int[] = prim::ListConstruct(%27, %27)
  %out.80 : Tensor = aten::conv2d(%out.79, %606, %607, %608, %609, %610, %27) # torch/nn/modules/conv.py:415:15
  %612 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%415)
  %613 : int = aten::dim(%out.80) # torch/nn/modules/batchnorm.py:276:11
  %614 : bool = aten::ne(%613, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%614) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %615 : bool = prim::GetAttr[name="training"](%612)
   = prim::If(%615) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %616 : Tensor = prim::GetAttr[name="num_batches_tracked"](%612)
      %617 : Tensor = aten::add(%616, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%612, %617)
      -> ()
    block1():
      -> ()
  %618 : bool = prim::GetAttr[name="training"](%612)
  %619 : Tensor = prim::GetAttr[name="running_mean"](%612)
  %620 : Tensor = prim::GetAttr[name="running_var"](%612)
  %621 : Tensor = prim::GetAttr[name="weight"](%612)
  %622 : Tensor = prim::GetAttr[name="bias"](%612)
   = prim::If(%618) # torch/nn/functional.py:2011:4
    block0():
      %623 : int[] = aten::size(%out.80) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%623, %24) # torch/nn/functional.py:1991:17
      %625 : int = aten::len(%623) # torch/nn/functional.py:1992:19
      %626 : int = aten::sub(%625, %26) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%626, %25, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %630 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
          %631 : int = aten::__getitem__(%623, %630) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %631) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.123)
      %633 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
       = prim::If(%633) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.81 : Tensor = aten::batch_norm(%out.80, %621, %622, %619, %620, %618, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.82 : Tensor = aten::add_(%out.81, %input.28, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.25 : Tensor = aten::relu_(%out.82) # torch/nn/functional.py:1117:17
  %637 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%416)
  %638 : Tensor = prim::GetAttr[name="weight"](%637)
  %639 : Tensor? = prim::GetAttr[name="bias"](%637)
  %640 : int[] = prim::ListConstruct(%27, %27)
  %641 : int[] = prim::ListConstruct(%24, %24)
  %642 : int[] = prim::ListConstruct(%27, %27)
  %out.83 : Tensor = aten::conv2d(%input.25, %638, %639, %640, %641, %642, %27) # torch/nn/modules/conv.py:415:15
  %644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%416)
  %645 : int = aten::dim(%out.83) # torch/nn/modules/batchnorm.py:276:11
  %646 : bool = aten::ne(%645, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%646) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %647 : bool = prim::GetAttr[name="training"](%644)
   = prim::If(%647) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %648 : Tensor = prim::GetAttr[name="num_batches_tracked"](%644)
      %649 : Tensor = aten::add(%648, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%644, %649)
      -> ()
    block1():
      -> ()
  %650 : bool = prim::GetAttr[name="training"](%644)
  %651 : Tensor = prim::GetAttr[name="running_mean"](%644)
  %652 : Tensor = prim::GetAttr[name="running_var"](%644)
  %653 : Tensor = prim::GetAttr[name="weight"](%644)
  %654 : Tensor = prim::GetAttr[name="bias"](%644)
   = prim::If(%650) # torch/nn/functional.py:2011:4
    block0():
      %655 : int[] = aten::size(%out.83) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%655, %24) # torch/nn/functional.py:1991:17
      %657 : int = aten::len(%655) # torch/nn/functional.py:1992:19
      %658 : int = aten::sub(%657, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%658, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %662 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %663 : int = aten::__getitem__(%655, %662) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %663) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %665 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%665) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.84 : Tensor = aten::batch_norm(%out.83, %653, %654, %651, %652, %650, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.85 : Tensor = aten::relu_(%out.84) # torch/nn/functional.py:1117:17
  %668 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%416)
  %669 : Tensor = prim::GetAttr[name="weight"](%668)
  %670 : Tensor? = prim::GetAttr[name="bias"](%668)
  %671 : int[] = prim::ListConstruct(%27, %27)
  %672 : int[] = prim::ListConstruct(%27, %27)
  %673 : int[] = prim::ListConstruct(%27, %27)
  %out.86 : Tensor = aten::conv2d(%out.85, %669, %670, %671, %672, %673, %27) # torch/nn/modules/conv.py:415:15
  %675 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%416)
  %676 : int = aten::dim(%out.86) # torch/nn/modules/batchnorm.py:276:11
  %677 : bool = aten::ne(%676, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%677) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %678 : bool = prim::GetAttr[name="training"](%675)
   = prim::If(%678) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %679 : Tensor = prim::GetAttr[name="num_batches_tracked"](%675)
      %680 : Tensor = aten::add(%679, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%675, %680)
      -> ()
    block1():
      -> ()
  %681 : bool = prim::GetAttr[name="training"](%675)
  %682 : Tensor = prim::GetAttr[name="running_mean"](%675)
  %683 : Tensor = prim::GetAttr[name="running_var"](%675)
  %684 : Tensor = prim::GetAttr[name="weight"](%675)
  %685 : Tensor = prim::GetAttr[name="bias"](%675)
   = prim::If(%681) # torch/nn/functional.py:2011:4
    block0():
      %686 : int[] = aten::size(%out.86) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%686, %24) # torch/nn/functional.py:1991:17
      %688 : int = aten::len(%686) # torch/nn/functional.py:1992:19
      %689 : int = aten::sub(%688, %26) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%689, %25, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %693 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
          %694 : int = aten::__getitem__(%686, %693) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %694) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.131)
      %696 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
       = prim::If(%696) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.87 : Tensor = aten::batch_norm(%out.86, %684, %685, %682, %683, %681, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.88 : Tensor = aten::relu_(%out.87) # torch/nn/functional.py:1117:17
  %699 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%416)
  %700 : Tensor = prim::GetAttr[name="weight"](%699)
  %701 : Tensor? = prim::GetAttr[name="bias"](%699)
  %702 : int[] = prim::ListConstruct(%27, %27)
  %703 : int[] = prim::ListConstruct(%24, %24)
  %704 : int[] = prim::ListConstruct(%27, %27)
  %out.89 : Tensor = aten::conv2d(%out.88, %700, %701, %702, %703, %704, %27) # torch/nn/modules/conv.py:415:15
  %706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%416)
  %707 : int = aten::dim(%out.89) # torch/nn/modules/batchnorm.py:276:11
  %708 : bool = aten::ne(%707, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%708) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %709 : bool = prim::GetAttr[name="training"](%706)
   = prim::If(%709) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %710 : Tensor = prim::GetAttr[name="num_batches_tracked"](%706)
      %711 : Tensor = aten::add(%710, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%706, %711)
      -> ()
    block1():
      -> ()
  %712 : bool = prim::GetAttr[name="training"](%706)
  %713 : Tensor = prim::GetAttr[name="running_mean"](%706)
  %714 : Tensor = prim::GetAttr[name="running_var"](%706)
  %715 : Tensor = prim::GetAttr[name="weight"](%706)
  %716 : Tensor = prim::GetAttr[name="bias"](%706)
   = prim::If(%712) # torch/nn/functional.py:2011:4
    block0():
      %717 : int[] = aten::size(%out.89) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%717, %24) # torch/nn/functional.py:1991:17
      %719 : int = aten::len(%717) # torch/nn/functional.py:1992:19
      %720 : int = aten::sub(%719, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%720, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %724 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %725 : int = aten::__getitem__(%717, %724) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %725) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %727 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%727) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.90 : Tensor = aten::batch_norm(%out.89, %715, %716, %713, %714, %712, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.91 : Tensor = aten::add_(%out.90, %input.25, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.29 : Tensor = aten::relu_(%out.91) # torch/nn/functional.py:1117:17
  %731 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%417)
  %732 : Tensor = prim::GetAttr[name="weight"](%731)
  %733 : Tensor? = prim::GetAttr[name="bias"](%731)
  %734 : int[] = prim::ListConstruct(%27, %27)
  %735 : int[] = prim::ListConstruct(%24, %24)
  %736 : int[] = prim::ListConstruct(%27, %27)
  %out.110 : Tensor = aten::conv2d(%input.29, %732, %733, %734, %735, %736, %27) # torch/nn/modules/conv.py:415:15
  %738 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%417)
  %739 : int = aten::dim(%out.110) # torch/nn/modules/batchnorm.py:276:11
  %740 : bool = aten::ne(%739, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%740) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %741 : bool = prim::GetAttr[name="training"](%738)
   = prim::If(%741) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %742 : Tensor = prim::GetAttr[name="num_batches_tracked"](%738)
      %743 : Tensor = aten::add(%742, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%738, %743)
      -> ()
    block1():
      -> ()
  %744 : bool = prim::GetAttr[name="training"](%738)
  %745 : Tensor = prim::GetAttr[name="running_mean"](%738)
  %746 : Tensor = prim::GetAttr[name="running_var"](%738)
  %747 : Tensor = prim::GetAttr[name="weight"](%738)
  %748 : Tensor = prim::GetAttr[name="bias"](%738)
   = prim::If(%744) # torch/nn/functional.py:2011:4
    block0():
      %749 : int[] = aten::size(%out.110) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%749, %24) # torch/nn/functional.py:1991:17
      %751 : int = aten::len(%749) # torch/nn/functional.py:1992:19
      %752 : int = aten::sub(%751, %26) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%752, %25, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %756 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
          %757 : int = aten::__getitem__(%749, %756) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %757) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.163)
      %759 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
       = prim::If(%759) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.111 : Tensor = aten::batch_norm(%out.110, %747, %748, %745, %746, %744, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.112 : Tensor = aten::relu_(%out.111) # torch/nn/functional.py:1117:17
  %762 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%417)
  %763 : Tensor = prim::GetAttr[name="weight"](%762)
  %764 : Tensor? = prim::GetAttr[name="bias"](%762)
  %765 : int[] = prim::ListConstruct(%27, %27)
  %766 : int[] = prim::ListConstruct(%27, %27)
  %767 : int[] = prim::ListConstruct(%27, %27)
  %out.113 : Tensor = aten::conv2d(%out.112, %763, %764, %765, %766, %767, %27) # torch/nn/modules/conv.py:415:15
  %769 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%417)
  %770 : int = aten::dim(%out.113) # torch/nn/modules/batchnorm.py:276:11
  %771 : bool = aten::ne(%770, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%771) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %772 : bool = prim::GetAttr[name="training"](%769)
   = prim::If(%772) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %773 : Tensor = prim::GetAttr[name="num_batches_tracked"](%769)
      %774 : Tensor = aten::add(%773, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%769, %774)
      -> ()
    block1():
      -> ()
  %775 : bool = prim::GetAttr[name="training"](%769)
  %776 : Tensor = prim::GetAttr[name="running_mean"](%769)
  %777 : Tensor = prim::GetAttr[name="running_var"](%769)
  %778 : Tensor = prim::GetAttr[name="weight"](%769)
  %779 : Tensor = prim::GetAttr[name="bias"](%769)
   = prim::If(%775) # torch/nn/functional.py:2011:4
    block0():
      %780 : int[] = aten::size(%out.113) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%780, %24) # torch/nn/functional.py:1991:17
      %782 : int = aten::len(%780) # torch/nn/functional.py:1992:19
      %783 : int = aten::sub(%782, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%783, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %787 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %788 : int = aten::__getitem__(%780, %787) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %788) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %790 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%790) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.114 : Tensor = aten::batch_norm(%out.113, %778, %779, %776, %777, %775, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.115 : Tensor = aten::relu_(%out.114) # torch/nn/functional.py:1117:17
  %793 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%417)
  %794 : Tensor = prim::GetAttr[name="weight"](%793)
  %795 : Tensor? = prim::GetAttr[name="bias"](%793)
  %796 : int[] = prim::ListConstruct(%27, %27)
  %797 : int[] = prim::ListConstruct(%24, %24)
  %798 : int[] = prim::ListConstruct(%27, %27)
  %out.116 : Tensor = aten::conv2d(%out.115, %794, %795, %796, %797, %798, %27) # torch/nn/modules/conv.py:415:15
  %800 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%417)
  %801 : int = aten::dim(%out.116) # torch/nn/modules/batchnorm.py:276:11
  %802 : bool = aten::ne(%801, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%802) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %803 : bool = prim::GetAttr[name="training"](%800)
   = prim::If(%803) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %804 : Tensor = prim::GetAttr[name="num_batches_tracked"](%800)
      %805 : Tensor = aten::add(%804, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%800, %805)
      -> ()
    block1():
      -> ()
  %806 : bool = prim::GetAttr[name="training"](%800)
  %807 : Tensor = prim::GetAttr[name="running_mean"](%800)
  %808 : Tensor = prim::GetAttr[name="running_var"](%800)
  %809 : Tensor = prim::GetAttr[name="weight"](%800)
  %810 : Tensor = prim::GetAttr[name="bias"](%800)
   = prim::If(%806) # torch/nn/functional.py:2011:4
    block0():
      %811 : int[] = aten::size(%out.116) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%811, %24) # torch/nn/functional.py:1991:17
      %813 : int = aten::len(%811) # torch/nn/functional.py:1992:19
      %814 : int = aten::sub(%813, %26) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%814, %25, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %818 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
          %819 : int = aten::__getitem__(%811, %818) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %819) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.171)
      %821 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
       = prim::If(%821) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.117 : Tensor = aten::batch_norm(%out.116, %809, %810, %807, %808, %806, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.118 : Tensor = aten::add_(%out.117, %input.29, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.18 : Tensor = aten::relu_(%out.118) # torch/nn/functional.py:1117:17
  %825 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %826 : bool = aten::__contains__(%825, %name.16) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%826) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %827 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.11 : str = aten::__getitem__(%827, %name.16) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.11, %x.18) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %829 : __torch__.torchvision.models.resnet.___torch_mangle_34.Bottleneck = prim::GetAttr[name="0"](%42)
  %830 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="1"](%42)
  %831 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="2"](%42)
  %832 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="3"](%42)
  %833 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="4"](%42)
  %834 : __torch__.torchvision.models.resnet.___torch_mangle_37.Bottleneck = prim::GetAttr[name="5"](%42)
  %835 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%829)
  %836 : Tensor = prim::GetAttr[name="weight"](%835)
  %837 : Tensor? = prim::GetAttr[name="bias"](%835)
  %838 : int[] = prim::ListConstruct(%27, %27)
  %839 : int[] = prim::ListConstruct(%24, %24)
  %840 : int[] = prim::ListConstruct(%27, %27)
  %out.119 : Tensor = aten::conv2d(%x.18, %836, %837, %838, %839, %840, %27) # torch/nn/modules/conv.py:415:15
  %842 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%829)
  %843 : int = aten::dim(%out.119) # torch/nn/modules/batchnorm.py:276:11
  %844 : bool = aten::ne(%843, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%844) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %845 : bool = prim::GetAttr[name="training"](%842)
   = prim::If(%845) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %846 : Tensor = prim::GetAttr[name="num_batches_tracked"](%842)
      %847 : Tensor = aten::add(%846, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%842, %847)
      -> ()
    block1():
      -> ()
  %848 : bool = prim::GetAttr[name="training"](%842)
  %849 : Tensor = prim::GetAttr[name="running_mean"](%842)
  %850 : Tensor = prim::GetAttr[name="running_var"](%842)
  %851 : Tensor = prim::GetAttr[name="weight"](%842)
  %852 : Tensor = prim::GetAttr[name="bias"](%842)
   = prim::If(%848) # torch/nn/functional.py:2011:4
    block0():
      %853 : int[] = aten::size(%out.119) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%853, %24) # torch/nn/functional.py:1991:17
      %855 : int = aten::len(%853) # torch/nn/functional.py:1992:19
      %856 : int = aten::sub(%855, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%856, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %860 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %861 : int = aten::__getitem__(%853, %860) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %861) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %863 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%863) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.120 : Tensor = aten::batch_norm(%out.119, %851, %852, %849, %850, %848, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.121 : Tensor = aten::relu_(%out.120) # torch/nn/functional.py:1117:17
  %866 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%829)
  %867 : Tensor = prim::GetAttr[name="weight"](%866)
  %868 : Tensor? = prim::GetAttr[name="bias"](%866)
  %869 : int[] = prim::ListConstruct(%27, %27)
  %870 : int[] = prim::ListConstruct(%27, %27)
  %871 : int[] = prim::ListConstruct(%27, %27)
  %out.122 : Tensor = aten::conv2d(%out.121, %867, %868, %869, %870, %871, %27) # torch/nn/modules/conv.py:415:15
  %873 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%829)
  %874 : int = aten::dim(%out.122) # torch/nn/modules/batchnorm.py:276:11
  %875 : bool = aten::ne(%874, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%875) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %876 : bool = prim::GetAttr[name="training"](%873)
   = prim::If(%876) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %877 : Tensor = prim::GetAttr[name="num_batches_tracked"](%873)
      %878 : Tensor = aten::add(%877, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%873, %878)
      -> ()
    block1():
      -> ()
  %879 : bool = prim::GetAttr[name="training"](%873)
  %880 : Tensor = prim::GetAttr[name="running_mean"](%873)
  %881 : Tensor = prim::GetAttr[name="running_var"](%873)
  %882 : Tensor = prim::GetAttr[name="weight"](%873)
  %883 : Tensor = prim::GetAttr[name="bias"](%873)
   = prim::If(%879) # torch/nn/functional.py:2011:4
    block0():
      %884 : int[] = aten::size(%out.122) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%884, %24) # torch/nn/functional.py:1991:17
      %886 : int = aten::len(%884) # torch/nn/functional.py:1992:19
      %887 : int = aten::sub(%886, %26) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%887, %25, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %891 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
          %892 : int = aten::__getitem__(%884, %891) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %892) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.179)
      %894 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
       = prim::If(%894) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.123 : Tensor = aten::batch_norm(%out.122, %882, %883, %880, %881, %879, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.124 : Tensor = aten::relu_(%out.123) # torch/nn/functional.py:1117:17
  %897 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%829)
  %898 : Tensor = prim::GetAttr[name="weight"](%897)
  %899 : Tensor? = prim::GetAttr[name="bias"](%897)
  %900 : int[] = prim::ListConstruct(%27, %27)
  %901 : int[] = prim::ListConstruct(%24, %24)
  %902 : int[] = prim::ListConstruct(%27, %27)
  %out.125 : Tensor = aten::conv2d(%out.124, %898, %899, %900, %901, %902, %27) # torch/nn/modules/conv.py:415:15
  %904 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%829)
  %905 : int = aten::dim(%out.125) # torch/nn/modules/batchnorm.py:276:11
  %906 : bool = aten::ne(%905, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%906) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %907 : bool = prim::GetAttr[name="training"](%904)
   = prim::If(%907) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %908 : Tensor = prim::GetAttr[name="num_batches_tracked"](%904)
      %909 : Tensor = aten::add(%908, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%904, %909)
      -> ()
    block1():
      -> ()
  %910 : bool = prim::GetAttr[name="training"](%904)
  %911 : Tensor = prim::GetAttr[name="running_mean"](%904)
  %912 : Tensor = prim::GetAttr[name="running_var"](%904)
  %913 : Tensor = prim::GetAttr[name="weight"](%904)
  %914 : Tensor = prim::GetAttr[name="bias"](%904)
   = prim::If(%910) # torch/nn/functional.py:2011:4
    block0():
      %915 : int[] = aten::size(%out.125) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%915, %24) # torch/nn/functional.py:1991:17
      %917 : int = aten::len(%915) # torch/nn/functional.py:1992:19
      %918 : int = aten::sub(%917, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%918, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %922 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %923 : int = aten::__getitem__(%915, %922) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %923) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %925 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%925) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.126 : Tensor = aten::batch_norm(%out.125, %913, %914, %911, %912, %910, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %927 : __torch__.torch.nn.modules.container.___torch_mangle_33.Sequential = prim::GetAttr[name="downsample"](%829)
  %928 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="0"](%927)
  %929 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="1"](%927)
  %930 : Tensor = prim::GetAttr[name="weight"](%928)
  %931 : Tensor? = prim::GetAttr[name="bias"](%928)
  %932 : int[] = prim::ListConstruct(%27, %27)
  %933 : int[] = prim::ListConstruct(%24, %24)
  %934 : int[] = prim::ListConstruct(%27, %27)
  %input.26 : Tensor = aten::conv2d(%x.18, %930, %931, %932, %933, %934, %27) # torch/nn/modules/conv.py:415:15
  %936 : int = aten::dim(%input.26) # torch/nn/modules/batchnorm.py:276:11
  %937 : bool = aten::ne(%936, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%937) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %938 : bool = prim::GetAttr[name="training"](%929)
   = prim::If(%938) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %939 : Tensor = prim::GetAttr[name="num_batches_tracked"](%929)
      %940 : Tensor = aten::add(%939, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%929, %940)
      -> ()
    block1():
      -> ()
  %941 : bool = prim::GetAttr[name="training"](%929)
  %942 : Tensor = prim::GetAttr[name="running_mean"](%929)
  %943 : Tensor = prim::GetAttr[name="running_var"](%929)
  %944 : Tensor = prim::GetAttr[name="weight"](%929)
  %945 : Tensor = prim::GetAttr[name="bias"](%929)
   = prim::If(%941) # torch/nn/functional.py:2011:4
    block0():
      %946 : int[] = aten::size(%input.26) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%946, %24) # torch/nn/functional.py:1991:17
      %948 : int = aten::len(%946) # torch/nn/functional.py:1992:19
      %949 : int = aten::sub(%948, %26) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%949, %25, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %953 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
          %954 : int = aten::__getitem__(%946, %953) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %954) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.187)
      %956 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
       = prim::If(%956) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.4 : Tensor = aten::batch_norm(%input.26, %944, %945, %942, %943, %941, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.127 : Tensor = aten::add_(%out.126, %identity.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.30 : Tensor = aten::relu_(%out.127) # torch/nn/functional.py:1117:17
  %960 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%830)
  %961 : Tensor = prim::GetAttr[name="weight"](%960)
  %962 : Tensor? = prim::GetAttr[name="bias"](%960)
  %963 : int[] = prim::ListConstruct(%27, %27)
  %964 : int[] = prim::ListConstruct(%24, %24)
  %965 : int[] = prim::ListConstruct(%27, %27)
  %out.128 : Tensor = aten::conv2d(%input.30, %961, %962, %963, %964, %965, %27) # torch/nn/modules/conv.py:415:15
  %967 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%830)
  %968 : int = aten::dim(%out.128) # torch/nn/modules/batchnorm.py:276:11
  %969 : bool = aten::ne(%968, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%969) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %970 : bool = prim::GetAttr[name="training"](%967)
   = prim::If(%970) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %971 : Tensor = prim::GetAttr[name="num_batches_tracked"](%967)
      %972 : Tensor = aten::add(%971, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%967, %972)
      -> ()
    block1():
      -> ()
  %973 : bool = prim::GetAttr[name="training"](%967)
  %974 : Tensor = prim::GetAttr[name="running_mean"](%967)
  %975 : Tensor = prim::GetAttr[name="running_var"](%967)
  %976 : Tensor = prim::GetAttr[name="weight"](%967)
  %977 : Tensor = prim::GetAttr[name="bias"](%967)
   = prim::If(%973) # torch/nn/functional.py:2011:4
    block0():
      %978 : int[] = aten::size(%out.128) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%978, %24) # torch/nn/functional.py:1991:17
      %980 : int = aten::len(%978) # torch/nn/functional.py:1992:19
      %981 : int = aten::sub(%980, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%981, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %985 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %986 : int = aten::__getitem__(%978, %985) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %986) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %988 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%988) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.129 : Tensor = aten::batch_norm(%out.128, %976, %977, %974, %975, %973, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.130 : Tensor = aten::relu_(%out.129) # torch/nn/functional.py:1117:17
  %991 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%830)
  %992 : Tensor = prim::GetAttr[name="weight"](%991)
  %993 : Tensor? = prim::GetAttr[name="bias"](%991)
  %994 : int[] = prim::ListConstruct(%27, %27)
  %995 : int[] = prim::ListConstruct(%26, %26)
  %996 : int[] = prim::ListConstruct(%26, %26)
  %out.131 : Tensor = aten::conv2d(%out.130, %992, %993, %994, %995, %996, %27) # torch/nn/modules/conv.py:415:15
  %998 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%830)
  %999 : int = aten::dim(%out.131) # torch/nn/modules/batchnorm.py:276:11
  %1000 : bool = aten::ne(%999, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1000) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1001 : bool = prim::GetAttr[name="training"](%998)
   = prim::If(%1001) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1002 : Tensor = prim::GetAttr[name="num_batches_tracked"](%998)
      %1003 : Tensor = aten::add(%1002, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%998, %1003)
      -> ()
    block1():
      -> ()
  %1004 : bool = prim::GetAttr[name="training"](%998)
  %1005 : Tensor = prim::GetAttr[name="running_mean"](%998)
  %1006 : Tensor = prim::GetAttr[name="running_var"](%998)
  %1007 : Tensor = prim::GetAttr[name="weight"](%998)
  %1008 : Tensor = prim::GetAttr[name="bias"](%998)
   = prim::If(%1004) # torch/nn/functional.py:2011:4
    block0():
      %1009 : int[] = aten::size(%out.131) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%1009, %24) # torch/nn/functional.py:1991:17
      %1011 : int = aten::len(%1009) # torch/nn/functional.py:1992:19
      %1012 : int = aten::sub(%1011, %26) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%1012, %25, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %1016 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
          %1017 : int = aten::__getitem__(%1009, %1016) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %1017) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.195)
      %1019 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1019) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.132 : Tensor = aten::batch_norm(%out.131, %1007, %1008, %1005, %1006, %1004, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.133 : Tensor = aten::relu_(%out.132) # torch/nn/functional.py:1117:17
  %1022 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%830)
  %1023 : Tensor = prim::GetAttr[name="weight"](%1022)
  %1024 : Tensor? = prim::GetAttr[name="bias"](%1022)
  %1025 : int[] = prim::ListConstruct(%27, %27)
  %1026 : int[] = prim::ListConstruct(%24, %24)
  %1027 : int[] = prim::ListConstruct(%27, %27)
  %out.134 : Tensor = aten::conv2d(%out.133, %1023, %1024, %1025, %1026, %1027, %27) # torch/nn/modules/conv.py:415:15
  %1029 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%830)
  %1030 : int = aten::dim(%out.134) # torch/nn/modules/batchnorm.py:276:11
  %1031 : bool = aten::ne(%1030, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1031) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1032 : bool = prim::GetAttr[name="training"](%1029)
   = prim::If(%1032) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1033 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1029)
      %1034 : Tensor = aten::add(%1033, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1029, %1034)
      -> ()
    block1():
      -> ()
  %1035 : bool = prim::GetAttr[name="training"](%1029)
  %1036 : Tensor = prim::GetAttr[name="running_mean"](%1029)
  %1037 : Tensor = prim::GetAttr[name="running_var"](%1029)
  %1038 : Tensor = prim::GetAttr[name="weight"](%1029)
  %1039 : Tensor = prim::GetAttr[name="bias"](%1029)
   = prim::If(%1035) # torch/nn/functional.py:2011:4
    block0():
      %1040 : int[] = aten::size(%out.134) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%1040, %24) # torch/nn/functional.py:1991:17
      %1042 : int = aten::len(%1040) # torch/nn/functional.py:1992:19
      %1043 : int = aten::sub(%1042, %26) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%1043, %25, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %1047 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
          %1048 : int = aten::__getitem__(%1040, %1047) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %1048) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.199)
      %1050 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1050) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.135 : Tensor = aten::batch_norm(%out.134, %1038, %1039, %1036, %1037, %1035, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.136 : Tensor = aten::add_(%out.135, %input.30, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.36 : Tensor = aten::relu_(%out.136) # torch/nn/functional.py:1117:17
  %1054 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%831)
  %1055 : Tensor = prim::GetAttr[name="weight"](%1054)
  %1056 : Tensor? = prim::GetAttr[name="bias"](%1054)
  %1057 : int[] = prim::ListConstruct(%27, %27)
  %1058 : int[] = prim::ListConstruct(%24, %24)
  %1059 : int[] = prim::ListConstruct(%27, %27)
  %out.37 : Tensor = aten::conv2d(%input.36, %1055, %1056, %1057, %1058, %1059, %27) # torch/nn/modules/conv.py:415:15
  %1061 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%831)
  %1062 : int = aten::dim(%out.37) # torch/nn/modules/batchnorm.py:276:11
  %1063 : bool = aten::ne(%1062, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1063) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1064 : bool = prim::GetAttr[name="training"](%1061)
   = prim::If(%1064) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1065 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1061)
      %1066 : Tensor = aten::add(%1065, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1061, %1066)
      -> ()
    block1():
      -> ()
  %1067 : bool = prim::GetAttr[name="training"](%1061)
  %1068 : Tensor = prim::GetAttr[name="running_mean"](%1061)
  %1069 : Tensor = prim::GetAttr[name="running_var"](%1061)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1071 : Tensor = prim::GetAttr[name="bias"](%1061)
   = prim::If(%1067) # torch/nn/functional.py:2011:4
    block0():
      %1072 : int[] = aten::size(%out.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%1072, %24) # torch/nn/functional.py:1991:17
      %1074 : int = aten::len(%1072) # torch/nn/functional.py:1992:19
      %1075 : int = aten::sub(%1074, %26) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%1075, %25, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %1079 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
          %1080 : int = aten::__getitem__(%1072, %1079) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %1080) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.43)
      %1082 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1082) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.29 : Tensor = aten::batch_norm(%out.37, %1070, %1071, %1068, %1069, %1067, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.30 : Tensor = aten::relu_(%out.29) # torch/nn/functional.py:1117:17
  %1085 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%831)
  %1086 : Tensor = prim::GetAttr[name="weight"](%1085)
  %1087 : Tensor? = prim::GetAttr[name="bias"](%1085)
  %1088 : int[] = prim::ListConstruct(%27, %27)
  %1089 : int[] = prim::ListConstruct(%26, %26)
  %1090 : int[] = prim::ListConstruct(%26, %26)
  %out.31 : Tensor = aten::conv2d(%out.30, %1086, %1087, %1088, %1089, %1090, %27) # torch/nn/modules/conv.py:415:15
  %1092 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%831)
  %1093 : int = aten::dim(%out.31) # torch/nn/modules/batchnorm.py:276:11
  %1094 : bool = aten::ne(%1093, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1094) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1095 : bool = prim::GetAttr[name="training"](%1092)
   = prim::If(%1095) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1096 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1092)
      %1097 : Tensor = aten::add(%1096, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1092, %1097)
      -> ()
    block1():
      -> ()
  %1098 : bool = prim::GetAttr[name="training"](%1092)
  %1099 : Tensor = prim::GetAttr[name="running_mean"](%1092)
  %1100 : Tensor = prim::GetAttr[name="running_var"](%1092)
  %1101 : Tensor = prim::GetAttr[name="weight"](%1092)
  %1102 : Tensor = prim::GetAttr[name="bias"](%1092)
   = prim::If(%1098) # torch/nn/functional.py:2011:4
    block0():
      %1103 : int[] = aten::size(%out.31) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%1103, %24) # torch/nn/functional.py:1991:17
      %1105 : int = aten::len(%1103) # torch/nn/functional.py:1992:19
      %1106 : int = aten::sub(%1105, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%1106, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %1110 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %1111 : int = aten::__getitem__(%1103, %1110) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %1111) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %1113 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1113) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.32 : Tensor = aten::batch_norm(%out.31, %1101, %1102, %1099, %1100, %1098, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.33 : Tensor = aten::relu_(%out.32) # torch/nn/functional.py:1117:17
  %1116 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%831)
  %1117 : Tensor = prim::GetAttr[name="weight"](%1116)
  %1118 : Tensor? = prim::GetAttr[name="bias"](%1116)
  %1119 : int[] = prim::ListConstruct(%27, %27)
  %1120 : int[] = prim::ListConstruct(%24, %24)
  %1121 : int[] = prim::ListConstruct(%27, %27)
  %out.34 : Tensor = aten::conv2d(%out.33, %1117, %1118, %1119, %1120, %1121, %27) # torch/nn/modules/conv.py:415:15
  %1123 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%831)
  %1124 : int = aten::dim(%out.34) # torch/nn/modules/batchnorm.py:276:11
  %1125 : bool = aten::ne(%1124, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1125) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1126 : bool = prim::GetAttr[name="training"](%1123)
   = prim::If(%1126) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1127 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1123)
      %1128 : Tensor = aten::add(%1127, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1123, %1128)
      -> ()
    block1():
      -> ()
  %1129 : bool = prim::GetAttr[name="training"](%1123)
  %1130 : Tensor = prim::GetAttr[name="running_mean"](%1123)
  %1131 : Tensor = prim::GetAttr[name="running_var"](%1123)
  %1132 : Tensor = prim::GetAttr[name="weight"](%1123)
  %1133 : Tensor = prim::GetAttr[name="bias"](%1123)
   = prim::If(%1129) # torch/nn/functional.py:2011:4
    block0():
      %1134 : int[] = aten::size(%out.34) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%1134, %24) # torch/nn/functional.py:1991:17
      %1136 : int = aten::len(%1134) # torch/nn/functional.py:1992:19
      %1137 : int = aten::sub(%1136, %26) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%1137, %25, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %1141 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
          %1142 : int = aten::__getitem__(%1134, %1141) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %1142) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.51)
      %1144 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1144) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.35 : Tensor = aten::batch_norm(%out.34, %1132, %1133, %1130, %1131, %1129, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.36 : Tensor = aten::add_(%out.35, %input.36, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.38 : Tensor = aten::relu_(%out.36) # torch/nn/functional.py:1117:17
  %1148 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%832)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1148)
  %1150 : Tensor? = prim::GetAttr[name="bias"](%1148)
  %1151 : int[] = prim::ListConstruct(%27, %27)
  %1152 : int[] = prim::ListConstruct(%24, %24)
  %1153 : int[] = prim::ListConstruct(%27, %27)
  %out.46 : Tensor = aten::conv2d(%input.38, %1149, %1150, %1151, %1152, %1153, %27) # torch/nn/modules/conv.py:415:15
  %1155 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%832)
  %1156 : int = aten::dim(%out.46) # torch/nn/modules/batchnorm.py:276:11
  %1157 : bool = aten::ne(%1156, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1157) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1158 : bool = prim::GetAttr[name="training"](%1155)
   = prim::If(%1158) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1159 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1155)
      %1160 : Tensor = aten::add(%1159, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1155, %1160)
      -> ()
    block1():
      -> ()
  %1161 : bool = prim::GetAttr[name="training"](%1155)
  %1162 : Tensor = prim::GetAttr[name="running_mean"](%1155)
  %1163 : Tensor = prim::GetAttr[name="running_var"](%1155)
  %1164 : Tensor = prim::GetAttr[name="weight"](%1155)
  %1165 : Tensor = prim::GetAttr[name="bias"](%1155)
   = prim::If(%1161) # torch/nn/functional.py:2011:4
    block0():
      %1166 : int[] = aten::size(%out.46) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%1166, %24) # torch/nn/functional.py:1991:17
      %1168 : int = aten::len(%1166) # torch/nn/functional.py:1992:19
      %1169 : int = aten::sub(%1168, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%1169, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %1173 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %1174 : int = aten::__getitem__(%1166, %1173) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %1174) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %1176 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1176) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.38 : Tensor = aten::batch_norm(%out.46, %1164, %1165, %1162, %1163, %1161, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.39 : Tensor = aten::relu_(%out.38) # torch/nn/functional.py:1117:17
  %1179 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%832)
  %1180 : Tensor = prim::GetAttr[name="weight"](%1179)
  %1181 : Tensor? = prim::GetAttr[name="bias"](%1179)
  %1182 : int[] = prim::ListConstruct(%27, %27)
  %1183 : int[] = prim::ListConstruct(%26, %26)
  %1184 : int[] = prim::ListConstruct(%26, %26)
  %out.40 : Tensor = aten::conv2d(%out.39, %1180, %1181, %1182, %1183, %1184, %27) # torch/nn/modules/conv.py:415:15
  %1186 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%832)
  %1187 : int = aten::dim(%out.40) # torch/nn/modules/batchnorm.py:276:11
  %1188 : bool = aten::ne(%1187, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1188) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1189 : bool = prim::GetAttr[name="training"](%1186)
   = prim::If(%1189) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1190 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1186)
      %1191 : Tensor = aten::add(%1190, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1186, %1191)
      -> ()
    block1():
      -> ()
  %1192 : bool = prim::GetAttr[name="training"](%1186)
  %1193 : Tensor = prim::GetAttr[name="running_mean"](%1186)
  %1194 : Tensor = prim::GetAttr[name="running_var"](%1186)
  %1195 : Tensor = prim::GetAttr[name="weight"](%1186)
  %1196 : Tensor = prim::GetAttr[name="bias"](%1186)
   = prim::If(%1192) # torch/nn/functional.py:2011:4
    block0():
      %1197 : int[] = aten::size(%out.40) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%1197, %24) # torch/nn/functional.py:1991:17
      %1199 : int = aten::len(%1197) # torch/nn/functional.py:1992:19
      %1200 : int = aten::sub(%1199, %26) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%1200, %25, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %1204 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
          %1205 : int = aten::__getitem__(%1197, %1204) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %1205) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.59)
      %1207 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1207) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.41 : Tensor = aten::batch_norm(%out.40, %1195, %1196, %1193, %1194, %1192, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.42 : Tensor = aten::relu_(%out.41) # torch/nn/functional.py:1117:17
  %1210 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%832)
  %1211 : Tensor = prim::GetAttr[name="weight"](%1210)
  %1212 : Tensor? = prim::GetAttr[name="bias"](%1210)
  %1213 : int[] = prim::ListConstruct(%27, %27)
  %1214 : int[] = prim::ListConstruct(%24, %24)
  %1215 : int[] = prim::ListConstruct(%27, %27)
  %out.43 : Tensor = aten::conv2d(%out.42, %1211, %1212, %1213, %1214, %1215, %27) # torch/nn/modules/conv.py:415:15
  %1217 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%832)
  %1218 : int = aten::dim(%out.43) # torch/nn/modules/batchnorm.py:276:11
  %1219 : bool = aten::ne(%1218, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1219) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1220 : bool = prim::GetAttr[name="training"](%1217)
   = prim::If(%1220) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1221 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1217)
      %1222 : Tensor = aten::add(%1221, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1217, %1222)
      -> ()
    block1():
      -> ()
  %1223 : bool = prim::GetAttr[name="training"](%1217)
  %1224 : Tensor = prim::GetAttr[name="running_mean"](%1217)
  %1225 : Tensor = prim::GetAttr[name="running_var"](%1217)
  %1226 : Tensor = prim::GetAttr[name="weight"](%1217)
  %1227 : Tensor = prim::GetAttr[name="bias"](%1217)
   = prim::If(%1223) # torch/nn/functional.py:2011:4
    block0():
      %1228 : int[] = aten::size(%out.43) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%1228, %24) # torch/nn/functional.py:1991:17
      %1230 : int = aten::len(%1228) # torch/nn/functional.py:1992:19
      %1231 : int = aten::sub(%1230, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%1231, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %1235 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %1236 : int = aten::__getitem__(%1228, %1235) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %1236) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %1238 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1238) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.44 : Tensor = aten::batch_norm(%out.43, %1226, %1227, %1224, %1225, %1223, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.45 : Tensor = aten::add_(%out.44, %input.38, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.9 : Tensor = aten::relu_(%out.45) # torch/nn/functional.py:1117:17
  %1242 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%833)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1242)
  %1244 : Tensor? = prim::GetAttr[name="bias"](%1242)
  %1245 : int[] = prim::ListConstruct(%27, %27)
  %1246 : int[] = prim::ListConstruct(%24, %24)
  %1247 : int[] = prim::ListConstruct(%27, %27)
  %out.55 : Tensor = aten::conv2d(%input.9, %1243, %1244, %1245, %1246, %1247, %27) # torch/nn/modules/conv.py:415:15
  %1249 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%833)
  %1250 : int = aten::dim(%out.55) # torch/nn/modules/batchnorm.py:276:11
  %1251 : bool = aten::ne(%1250, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1251) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1252 : bool = prim::GetAttr[name="training"](%1249)
   = prim::If(%1252) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1253 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1249)
      %1254 : Tensor = aten::add(%1253, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1249, %1254)
      -> ()
    block1():
      -> ()
  %1255 : bool = prim::GetAttr[name="training"](%1249)
  %1256 : Tensor = prim::GetAttr[name="running_mean"](%1249)
  %1257 : Tensor = prim::GetAttr[name="running_var"](%1249)
  %1258 : Tensor = prim::GetAttr[name="weight"](%1249)
  %1259 : Tensor = prim::GetAttr[name="bias"](%1249)
   = prim::If(%1255) # torch/nn/functional.py:2011:4
    block0():
      %1260 : int[] = aten::size(%out.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%1260, %24) # torch/nn/functional.py:1991:17
      %1262 : int = aten::len(%1260) # torch/nn/functional.py:1992:19
      %1263 : int = aten::sub(%1262, %26) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%1263, %25, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %1267 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
          %1268 : int = aten::__getitem__(%1260, %1267) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %1268) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.67)
      %1270 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1270) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.47 : Tensor = aten::batch_norm(%out.55, %1258, %1259, %1256, %1257, %1255, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.48 : Tensor = aten::relu_(%out.47) # torch/nn/functional.py:1117:17
  %1273 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%833)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1273)
  %1275 : Tensor? = prim::GetAttr[name="bias"](%1273)
  %1276 : int[] = prim::ListConstruct(%27, %27)
  %1277 : int[] = prim::ListConstruct(%26, %26)
  %1278 : int[] = prim::ListConstruct(%26, %26)
  %out.49 : Tensor = aten::conv2d(%out.48, %1274, %1275, %1276, %1277, %1278, %27) # torch/nn/modules/conv.py:415:15
  %1280 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%833)
  %1281 : int = aten::dim(%out.49) # torch/nn/modules/batchnorm.py:276:11
  %1282 : bool = aten::ne(%1281, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1282) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1283 : bool = prim::GetAttr[name="training"](%1280)
   = prim::If(%1283) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1284 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1280)
      %1285 : Tensor = aten::add(%1284, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1280, %1285)
      -> ()
    block1():
      -> ()
  %1286 : bool = prim::GetAttr[name="training"](%1280)
  %1287 : Tensor = prim::GetAttr[name="running_mean"](%1280)
  %1288 : Tensor = prim::GetAttr[name="running_var"](%1280)
  %1289 : Tensor = prim::GetAttr[name="weight"](%1280)
  %1290 : Tensor = prim::GetAttr[name="bias"](%1280)
   = prim::If(%1286) # torch/nn/functional.py:2011:4
    block0():
      %1291 : int[] = aten::size(%out.49) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%1291, %24) # torch/nn/functional.py:1991:17
      %1293 : int = aten::len(%1291) # torch/nn/functional.py:1992:19
      %1294 : int = aten::sub(%1293, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%1294, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %1298 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %1299 : int = aten::__getitem__(%1291, %1298) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %1299) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %1301 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1301) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.50 : Tensor = aten::batch_norm(%out.49, %1289, %1290, %1287, %1288, %1286, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.51 : Tensor = aten::relu_(%out.50) # torch/nn/functional.py:1117:17
  %1304 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%833)
  %1305 : Tensor = prim::GetAttr[name="weight"](%1304)
  %1306 : Tensor? = prim::GetAttr[name="bias"](%1304)
  %1307 : int[] = prim::ListConstruct(%27, %27)
  %1308 : int[] = prim::ListConstruct(%24, %24)
  %1309 : int[] = prim::ListConstruct(%27, %27)
  %out.52 : Tensor = aten::conv2d(%out.51, %1305, %1306, %1307, %1308, %1309, %27) # torch/nn/modules/conv.py:415:15
  %1311 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%833)
  %1312 : int = aten::dim(%out.52) # torch/nn/modules/batchnorm.py:276:11
  %1313 : bool = aten::ne(%1312, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1313) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1314 : bool = prim::GetAttr[name="training"](%1311)
   = prim::If(%1314) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1311)
      %1316 : Tensor = aten::add(%1315, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1311, %1316)
      -> ()
    block1():
      -> ()
  %1317 : bool = prim::GetAttr[name="training"](%1311)
  %1318 : Tensor = prim::GetAttr[name="running_mean"](%1311)
  %1319 : Tensor = prim::GetAttr[name="running_var"](%1311)
  %1320 : Tensor = prim::GetAttr[name="weight"](%1311)
  %1321 : Tensor = prim::GetAttr[name="bias"](%1311)
   = prim::If(%1317) # torch/nn/functional.py:2011:4
    block0():
      %1322 : int[] = aten::size(%out.52) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%1322, %24) # torch/nn/functional.py:1991:17
      %1324 : int = aten::len(%1322) # torch/nn/functional.py:1992:19
      %1325 : int = aten::sub(%1324, %26) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%1325, %25, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %1329 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
          %1330 : int = aten::__getitem__(%1322, %1329) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %1330) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.75)
      %1332 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1332) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.53 : Tensor = aten::batch_norm(%out.52, %1320, %1321, %1318, %1319, %1317, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.54 : Tensor = aten::add_(%out.53, %input.9, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.24 : Tensor = aten::relu_(%out.54) # torch/nn/functional.py:1117:17
  %1336 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%834)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1336)
  %1338 : Tensor? = prim::GetAttr[name="bias"](%1336)
  %1339 : int[] = prim::ListConstruct(%27, %27)
  %1340 : int[] = prim::ListConstruct(%24, %24)
  %1341 : int[] = prim::ListConstruct(%27, %27)
  %out.137 : Tensor = aten::conv2d(%input.24, %1337, %1338, %1339, %1340, %1341, %27) # torch/nn/modules/conv.py:415:15
  %1343 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%834)
  %1344 : int = aten::dim(%out.137) # torch/nn/modules/batchnorm.py:276:11
  %1345 : bool = aten::ne(%1344, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1345) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1346 : bool = prim::GetAttr[name="training"](%1343)
   = prim::If(%1346) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1343)
      %1348 : Tensor = aten::add(%1347, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1343, %1348)
      -> ()
    block1():
      -> ()
  %1349 : bool = prim::GetAttr[name="training"](%1343)
  %1350 : Tensor = prim::GetAttr[name="running_mean"](%1343)
  %1351 : Tensor = prim::GetAttr[name="running_var"](%1343)
  %1352 : Tensor = prim::GetAttr[name="weight"](%1343)
  %1353 : Tensor = prim::GetAttr[name="bias"](%1343)
   = prim::If(%1349) # torch/nn/functional.py:2011:4
    block0():
      %1354 : int[] = aten::size(%out.137) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%1354, %24) # torch/nn/functional.py:1991:17
      %1356 : int = aten::len(%1354) # torch/nn/functional.py:1992:19
      %1357 : int = aten::sub(%1356, %26) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%1357, %25, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %1361 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
          %1362 : int = aten::__getitem__(%1354, %1361) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %1362) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.203)
      %1364 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1364) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.138 : Tensor = aten::batch_norm(%out.137, %1352, %1353, %1350, %1351, %1349, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.139 : Tensor = aten::relu_(%out.138) # torch/nn/functional.py:1117:17
  %1367 : __torch__.torch.nn.modules.conv.___torch_mangle_36.Conv2d = prim::GetAttr[name="conv2"](%834)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1367)
  %1369 : Tensor? = prim::GetAttr[name="bias"](%1367)
  %1370 : int[] = prim::ListConstruct(%27, %27)
  %1371 : int[] = prim::ListConstruct(%26, %26)
  %1372 : int[] = prim::ListConstruct(%26, %26)
  %out.140 : Tensor = aten::conv2d(%out.139, %1368, %1369, %1370, %1371, %1372, %27) # torch/nn/modules/conv.py:415:15
  %1374 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%834)
  %1375 : int = aten::dim(%out.140) # torch/nn/modules/batchnorm.py:276:11
  %1376 : bool = aten::ne(%1375, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1376) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1377 : bool = prim::GetAttr[name="training"](%1374)
   = prim::If(%1377) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1378 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1374)
      %1379 : Tensor = aten::add(%1378, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1374, %1379)
      -> ()
    block1():
      -> ()
  %1380 : bool = prim::GetAttr[name="training"](%1374)
  %1381 : Tensor = prim::GetAttr[name="running_mean"](%1374)
  %1382 : Tensor = prim::GetAttr[name="running_var"](%1374)
  %1383 : Tensor = prim::GetAttr[name="weight"](%1374)
  %1384 : Tensor = prim::GetAttr[name="bias"](%1374)
   = prim::If(%1380) # torch/nn/functional.py:2011:4
    block0():
      %1385 : int[] = aten::size(%out.140) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%1385, %24) # torch/nn/functional.py:1991:17
      %1387 : int = aten::len(%1385) # torch/nn/functional.py:1992:19
      %1388 : int = aten::sub(%1387, %26) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%1388, %25, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %1392 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
          %1393 : int = aten::__getitem__(%1385, %1392) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %1393) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.207)
      %1395 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1395) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.141 : Tensor = aten::batch_norm(%out.140, %1383, %1384, %1381, %1382, %1380, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.142 : Tensor = aten::relu_(%out.141) # torch/nn/functional.py:1117:17
  %1398 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%834)
  %1399 : Tensor = prim::GetAttr[name="weight"](%1398)
  %1400 : Tensor? = prim::GetAttr[name="bias"](%1398)
  %1401 : int[] = prim::ListConstruct(%27, %27)
  %1402 : int[] = prim::ListConstruct(%24, %24)
  %1403 : int[] = prim::ListConstruct(%27, %27)
  %out.143 : Tensor = aten::conv2d(%out.142, %1399, %1400, %1401, %1402, %1403, %27) # torch/nn/modules/conv.py:415:15
  %1405 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%834)
  %1406 : int = aten::dim(%out.143) # torch/nn/modules/batchnorm.py:276:11
  %1407 : bool = aten::ne(%1406, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1407) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1408 : bool = prim::GetAttr[name="training"](%1405)
   = prim::If(%1408) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1409 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1405)
      %1410 : Tensor = aten::add(%1409, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1405, %1410)
      -> ()
    block1():
      -> ()
  %1411 : bool = prim::GetAttr[name="training"](%1405)
  %1412 : Tensor = prim::GetAttr[name="running_mean"](%1405)
  %1413 : Tensor = prim::GetAttr[name="running_var"](%1405)
  %1414 : Tensor = prim::GetAttr[name="weight"](%1405)
  %1415 : Tensor = prim::GetAttr[name="bias"](%1405)
   = prim::If(%1411) # torch/nn/functional.py:2011:4
    block0():
      %1416 : int[] = aten::size(%out.143) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%1416, %24) # torch/nn/functional.py:1991:17
      %1418 : int = aten::len(%1416) # torch/nn/functional.py:1992:19
      %1419 : int = aten::sub(%1418, %26) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%1419, %25, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %1423 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
          %1424 : int = aten::__getitem__(%1416, %1423) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %1424) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.211)
      %1426 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1426) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.144 : Tensor = aten::batch_norm(%out.143, %1414, %1415, %1412, %1413, %1411, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.145 : Tensor = aten::add_(%out.144, %input.24, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.21 : Tensor = aten::relu_(%out.145) # torch/nn/functional.py:1117:17
  %1430 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %1431 : bool = aten::__contains__(%1430, %name.19) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%1431) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %1432 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.13 : str = aten::__getitem__(%1432, %name.19) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.13, %x.21) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %1434 : __torch__.torchvision.models.resnet.___torch_mangle_45.Bottleneck = prim::GetAttr[name="0"](%43)
  %1435 : __torch__.torchvision.models.resnet.___torch_mangle_48.Bottleneck = prim::GetAttr[name="1"](%43)
  %1436 : __torch__.torchvision.models.resnet.___torch_mangle_48.Bottleneck = prim::GetAttr[name="2"](%43)
  %1437 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%1434)
  %1438 : Tensor = prim::GetAttr[name="weight"](%1437)
  %1439 : Tensor? = prim::GetAttr[name="bias"](%1437)
  %1440 : int[] = prim::ListConstruct(%27, %27)
  %1441 : int[] = prim::ListConstruct(%24, %24)
  %1442 : int[] = prim::ListConstruct(%27, %27)
  %out.2 : Tensor = aten::conv2d(%x.21, %1438, %1439, %1440, %1441, %1442, %27) # torch/nn/modules/conv.py:415:15
  %1444 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%1434)
  %1445 : int = aten::dim(%out.2) # torch/nn/modules/batchnorm.py:276:11
  %1446 : bool = aten::ne(%1445, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1446) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1447 : bool = prim::GetAttr[name="training"](%1444)
   = prim::If(%1447) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1448 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1444)
      %1449 : Tensor = aten::add(%1448, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1444, %1449)
      -> ()
    block1():
      -> ()
  %1450 : bool = prim::GetAttr[name="training"](%1444)
  %1451 : Tensor = prim::GetAttr[name="running_mean"](%1444)
  %1452 : Tensor = prim::GetAttr[name="running_var"](%1444)
  %1453 : Tensor = prim::GetAttr[name="weight"](%1444)
  %1454 : Tensor = prim::GetAttr[name="bias"](%1444)
   = prim::If(%1450) # torch/nn/functional.py:2011:4
    block0():
      %1455 : int[] = aten::size(%out.2) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%1455, %24) # torch/nn/functional.py:1991:17
      %1457 : int = aten::len(%1455) # torch/nn/functional.py:1992:19
      %1458 : int = aten::sub(%1457, %26) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%1458, %25, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.226 : int):
          %1462 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
          %1463 : int = aten::__getitem__(%1455, %1462) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %1463) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.227)
      %1465 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1465) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.4 : Tensor = aten::batch_norm(%out.2, %1453, %1454, %1451, %1452, %1450, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.6 : Tensor = aten::relu_(%out.4) # torch/nn/functional.py:1117:17
  %1468 : __torch__.torch.nn.modules.conv.___torch_mangle_40.Conv2d = prim::GetAttr[name="conv2"](%1434)
  %1469 : Tensor = prim::GetAttr[name="weight"](%1468)
  %1470 : Tensor? = prim::GetAttr[name="bias"](%1468)
  %1471 : int[] = prim::ListConstruct(%27, %27)
  %1472 : int[] = prim::ListConstruct(%26, %26)
  %1473 : int[] = prim::ListConstruct(%26, %26)
  %out.8 : Tensor = aten::conv2d(%out.6, %1469, %1470, %1471, %1472, %1473, %27) # torch/nn/modules/conv.py:415:15
  %1475 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%1434)
  %1476 : int = aten::dim(%out.8) # torch/nn/modules/batchnorm.py:276:11
  %1477 : bool = aten::ne(%1476, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1477) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1478 : bool = prim::GetAttr[name="training"](%1475)
   = prim::If(%1478) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1479 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1475)
      %1480 : Tensor = aten::add(%1479, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1475, %1480)
      -> ()
    block1():
      -> ()
  %1481 : bool = prim::GetAttr[name="training"](%1475)
  %1482 : Tensor = prim::GetAttr[name="running_mean"](%1475)
  %1483 : Tensor = prim::GetAttr[name="running_var"](%1475)
  %1484 : Tensor = prim::GetAttr[name="weight"](%1475)
  %1485 : Tensor = prim::GetAttr[name="bias"](%1475)
   = prim::If(%1481) # torch/nn/functional.py:2011:4
    block0():
      %1486 : int[] = aten::size(%out.8) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%1486, %24) # torch/nn/functional.py:1991:17
      %1488 : int = aten::len(%1486) # torch/nn/functional.py:1992:19
      %1489 : int = aten::sub(%1488, %26) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%1489, %25, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.230 : int):
          %1493 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
          %1494 : int = aten::__getitem__(%1486, %1493) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %1494) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.231)
      %1496 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1496) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.10 : Tensor = aten::batch_norm(%out.8, %1484, %1485, %1482, %1483, %1481, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.12 : Tensor = aten::relu_(%out.10) # torch/nn/functional.py:1117:17
  %1499 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%1434)
  %1500 : Tensor = prim::GetAttr[name="weight"](%1499)
  %1501 : Tensor? = prim::GetAttr[name="bias"](%1499)
  %1502 : int[] = prim::ListConstruct(%27, %27)
  %1503 : int[] = prim::ListConstruct(%24, %24)
  %1504 : int[] = prim::ListConstruct(%27, %27)
  %out.14 : Tensor = aten::conv2d(%out.12, %1500, %1501, %1502, %1503, %1504, %27) # torch/nn/modules/conv.py:415:15
  %1506 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%1434)
  %1507 : int = aten::dim(%out.14) # torch/nn/modules/batchnorm.py:276:11
  %1508 : bool = aten::ne(%1507, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1508) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1509 : bool = prim::GetAttr[name="training"](%1506)
   = prim::If(%1509) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1510 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1506)
      %1511 : Tensor = aten::add(%1510, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1506, %1511)
      -> ()
    block1():
      -> ()
  %1512 : bool = prim::GetAttr[name="training"](%1506)
  %1513 : Tensor = prim::GetAttr[name="running_mean"](%1506)
  %1514 : Tensor = prim::GetAttr[name="running_var"](%1506)
  %1515 : Tensor = prim::GetAttr[name="weight"](%1506)
  %1516 : Tensor = prim::GetAttr[name="bias"](%1506)
   = prim::If(%1512) # torch/nn/functional.py:2011:4
    block0():
      %1517 : int[] = aten::size(%out.14) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%1517, %24) # torch/nn/functional.py:1991:17
      %1519 : int = aten::len(%1517) # torch/nn/functional.py:1992:19
      %1520 : int = aten::sub(%1519, %26) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%1520, %25, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %1524 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
          %1525 : int = aten::__getitem__(%1517, %1524) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %1525) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.223)
      %1527 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1527) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.16 : Tensor = aten::batch_norm(%out.14, %1515, %1516, %1513, %1514, %1512, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %1529 : __torch__.torch.nn.modules.container.___torch_mangle_44.Sequential = prim::GetAttr[name="downsample"](%1434)
  %1530 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="0"](%1529)
  %1531 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="1"](%1529)
  %1532 : Tensor = prim::GetAttr[name="weight"](%1530)
  %1533 : Tensor? = prim::GetAttr[name="bias"](%1530)
  %1534 : int[] = prim::ListConstruct(%27, %27)
  %1535 : int[] = prim::ListConstruct(%24, %24)
  %1536 : int[] = prim::ListConstruct(%27, %27)
  %input.31 : Tensor = aten::conv2d(%x.21, %1532, %1533, %1534, %1535, %1536, %27) # torch/nn/modules/conv.py:415:15
  %1538 : int = aten::dim(%input.31) # torch/nn/modules/batchnorm.py:276:11
  %1539 : bool = aten::ne(%1538, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1539) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1540 : bool = prim::GetAttr[name="training"](%1531)
   = prim::If(%1540) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1541 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1531)
      %1542 : Tensor = aten::add(%1541, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1531, %1542)
      -> ()
    block1():
      -> ()
  %1543 : bool = prim::GetAttr[name="training"](%1531)
  %1544 : Tensor = prim::GetAttr[name="running_mean"](%1531)
  %1545 : Tensor = prim::GetAttr[name="running_var"](%1531)
  %1546 : Tensor = prim::GetAttr[name="weight"](%1531)
  %1547 : Tensor = prim::GetAttr[name="bias"](%1531)
   = prim::If(%1543) # torch/nn/functional.py:2011:4
    block0():
      %1548 : int[] = aten::size(%input.31) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%1548, %24) # torch/nn/functional.py:1991:17
      %1550 : int = aten::len(%1548) # torch/nn/functional.py:1992:19
      %1551 : int = aten::sub(%1550, %26) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%1551, %25, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %1555 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
          %1556 : int = aten::__getitem__(%1548, %1555) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %1556) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.235)
      %1558 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1558) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.1 : Tensor = aten::batch_norm(%input.31, %1546, %1547, %1544, %1545, %1543, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.18 : Tensor = aten::add_(%out.16, %identity.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.33 : Tensor = aten::relu_(%out.18) # torch/nn/functional.py:1117:17
  %1562 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%1435)
  %1563 : Tensor = prim::GetAttr[name="weight"](%1562)
  %1564 : Tensor? = prim::GetAttr[name="bias"](%1562)
  %1565 : int[] = prim::ListConstruct(%27, %27)
  %1566 : int[] = prim::ListConstruct(%24, %24)
  %1567 : int[] = prim::ListConstruct(%27, %27)
  %out.28 : Tensor = aten::conv2d(%input.33, %1563, %1564, %1565, %1566, %1567, %27) # torch/nn/modules/conv.py:415:15
  %1569 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%1435)
  %1570 : int = aten::dim(%out.28) # torch/nn/modules/batchnorm.py:276:11
  %1571 : bool = aten::ne(%1570, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1571) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1572 : bool = prim::GetAttr[name="training"](%1569)
   = prim::If(%1572) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1573 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1569)
      %1574 : Tensor = aten::add(%1573, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1569, %1574)
      -> ()
    block1():
      -> ()
  %1575 : bool = prim::GetAttr[name="training"](%1569)
  %1576 : Tensor = prim::GetAttr[name="running_mean"](%1569)
  %1577 : Tensor = prim::GetAttr[name="running_var"](%1569)
  %1578 : Tensor = prim::GetAttr[name="weight"](%1569)
  %1579 : Tensor = prim::GetAttr[name="bias"](%1569)
   = prim::If(%1575) # torch/nn/functional.py:2011:4
    block0():
      %1580 : int[] = aten::size(%out.28) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%1580, %24) # torch/nn/functional.py:1991:17
      %1582 : int = aten::len(%1580) # torch/nn/functional.py:1992:19
      %1583 : int = aten::sub(%1582, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%1583, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.30 : int):
          %1587 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
          %1588 : int = aten::__getitem__(%1580, %1587) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %1588) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %1590 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1590) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.20 : Tensor = aten::batch_norm(%out.28, %1578, %1579, %1576, %1577, %1575, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.21 : Tensor = aten::relu_(%out.20) # torch/nn/functional.py:1117:17
  %1593 : __torch__.torch.nn.modules.conv.___torch_mangle_47.Conv2d = prim::GetAttr[name="conv2"](%1435)
  %1594 : Tensor = prim::GetAttr[name="weight"](%1593)
  %1595 : Tensor? = prim::GetAttr[name="bias"](%1593)
  %1596 : int[] = prim::ListConstruct(%27, %27)
  %1597 : int[] = prim::ListConstruct(%22, %22)
  %1598 : int[] = prim::ListConstruct(%22, %22)
  %out.22 : Tensor = aten::conv2d(%out.21, %1594, %1595, %1596, %1597, %1598, %27) # torch/nn/modules/conv.py:415:15
  %1600 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%1435)
  %1601 : int = aten::dim(%out.22) # torch/nn/modules/batchnorm.py:276:11
  %1602 : bool = aten::ne(%1601, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1602) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1603 : bool = prim::GetAttr[name="training"](%1600)
   = prim::If(%1603) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1604 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1600)
      %1605 : Tensor = aten::add(%1604, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1600, %1605)
      -> ()
    block1():
      -> ()
  %1606 : bool = prim::GetAttr[name="training"](%1600)
  %1607 : Tensor = prim::GetAttr[name="running_mean"](%1600)
  %1608 : Tensor = prim::GetAttr[name="running_var"](%1600)
  %1609 : Tensor = prim::GetAttr[name="weight"](%1600)
  %1610 : Tensor = prim::GetAttr[name="bias"](%1600)
   = prim::If(%1606) # torch/nn/functional.py:2011:4
    block0():
      %1611 : int[] = aten::size(%out.22) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%1611, %24) # torch/nn/functional.py:1991:17
      %1613 : int = aten::len(%1611) # torch/nn/functional.py:1992:19
      %1614 : int = aten::sub(%1613, %26) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%1614, %25, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %1618 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
          %1619 : int = aten::__getitem__(%1611, %1618) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %1619) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.35)
      %1621 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1621) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.23 : Tensor = aten::batch_norm(%out.22, %1609, %1610, %1607, %1608, %1606, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.24 : Tensor = aten::relu_(%out.23) # torch/nn/functional.py:1117:17
  %1624 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%1435)
  %1625 : Tensor = prim::GetAttr[name="weight"](%1624)
  %1626 : Tensor? = prim::GetAttr[name="bias"](%1624)
  %1627 : int[] = prim::ListConstruct(%27, %27)
  %1628 : int[] = prim::ListConstruct(%24, %24)
  %1629 : int[] = prim::ListConstruct(%27, %27)
  %out.25 : Tensor = aten::conv2d(%out.24, %1625, %1626, %1627, %1628, %1629, %27) # torch/nn/modules/conv.py:415:15
  %1631 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%1435)
  %1632 : int = aten::dim(%out.25) # torch/nn/modules/batchnorm.py:276:11
  %1633 : bool = aten::ne(%1632, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1633) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1634 : bool = prim::GetAttr[name="training"](%1631)
   = prim::If(%1634) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1635 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1631)
      %1636 : Tensor = aten::add(%1635, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1631, %1636)
      -> ()
    block1():
      -> ()
  %1637 : bool = prim::GetAttr[name="training"](%1631)
  %1638 : Tensor = prim::GetAttr[name="running_mean"](%1631)
  %1639 : Tensor = prim::GetAttr[name="running_var"](%1631)
  %1640 : Tensor = prim::GetAttr[name="weight"](%1631)
  %1641 : Tensor = prim::GetAttr[name="bias"](%1631)
   = prim::If(%1637) # torch/nn/functional.py:2011:4
    block0():
      %1642 : int[] = aten::size(%out.25) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%1642, %24) # torch/nn/functional.py:1991:17
      %1644 : int = aten::len(%1642) # torch/nn/functional.py:1992:19
      %1645 : int = aten::sub(%1644, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%1645, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %1649 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %1650 : int = aten::__getitem__(%1642, %1649) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %1650) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %1652 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1652) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.26 : Tensor = aten::batch_norm(%out.25, %1640, %1641, %1638, %1639, %1637, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.27 : Tensor = aten::add_(%out.26, %input.33, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.32 : Tensor = aten::relu_(%out.27) # torch/nn/functional.py:1117:17
  %1656 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%1436)
  %1657 : Tensor = prim::GetAttr[name="weight"](%1656)
  %1658 : Tensor? = prim::GetAttr[name="bias"](%1656)
  %1659 : int[] = prim::ListConstruct(%27, %27)
  %1660 : int[] = prim::ListConstruct(%24, %24)
  %1661 : int[] = prim::ListConstruct(%27, %27)
  %out.1 : Tensor = aten::conv2d(%input.32, %1657, %1658, %1659, %1660, %1661, %27) # torch/nn/modules/conv.py:415:15
  %1663 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%1436)
  %1664 : int = aten::dim(%out.1) # torch/nn/modules/batchnorm.py:276:11
  %1665 : bool = aten::ne(%1664, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1665) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1666 : bool = prim::GetAttr[name="training"](%1663)
   = prim::If(%1666) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1667 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1663)
      %1668 : Tensor = aten::add(%1667, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1663, %1668)
      -> ()
    block1():
      -> ()
  %1669 : bool = prim::GetAttr[name="training"](%1663)
  %1670 : Tensor = prim::GetAttr[name="running_mean"](%1663)
  %1671 : Tensor = prim::GetAttr[name="running_var"](%1663)
  %1672 : Tensor = prim::GetAttr[name="weight"](%1663)
  %1673 : Tensor = prim::GetAttr[name="bias"](%1663)
   = prim::If(%1669) # torch/nn/functional.py:2011:4
    block0():
      %1674 : int[] = aten::size(%out.1) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%1674, %24) # torch/nn/functional.py:1991:17
      %1676 : int = aten::len(%1674) # torch/nn/functional.py:1992:19
      %1677 : int = aten::sub(%1676, %26) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%1677, %25, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.214 : int):
          %1681 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
          %1682 : int = aten::__getitem__(%1674, %1681) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %1682) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.215)
      %1684 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1684) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.3 : Tensor = aten::batch_norm(%out.1, %1672, %1673, %1670, %1671, %1669, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.5 : Tensor = aten::relu_(%out.3) # torch/nn/functional.py:1117:17
  %1687 : __torch__.torch.nn.modules.conv.___torch_mangle_47.Conv2d = prim::GetAttr[name="conv2"](%1436)
  %1688 : Tensor = prim::GetAttr[name="weight"](%1687)
  %1689 : Tensor? = prim::GetAttr[name="bias"](%1687)
  %1690 : int[] = prim::ListConstruct(%27, %27)
  %1691 : int[] = prim::ListConstruct(%22, %22)
  %1692 : int[] = prim::ListConstruct(%22, %22)
  %out.7 : Tensor = aten::conv2d(%out.5, %1688, %1689, %1690, %1691, %1692, %27) # torch/nn/modules/conv.py:415:15
  %1694 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%1436)
  %1695 : int = aten::dim(%out.7) # torch/nn/modules/batchnorm.py:276:11
  %1696 : bool = aten::ne(%1695, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1696) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1697 : bool = prim::GetAttr[name="training"](%1694)
   = prim::If(%1697) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1698 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1694)
      %1699 : Tensor = aten::add(%1698, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1694, %1699)
      -> ()
    block1():
      -> ()
  %1700 : bool = prim::GetAttr[name="training"](%1694)
  %1701 : Tensor = prim::GetAttr[name="running_mean"](%1694)
  %1702 : Tensor = prim::GetAttr[name="running_var"](%1694)
  %1703 : Tensor = prim::GetAttr[name="weight"](%1694)
  %1704 : Tensor = prim::GetAttr[name="bias"](%1694)
   = prim::If(%1700) # torch/nn/functional.py:2011:4
    block0():
      %1705 : int[] = aten::size(%out.7) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%1705, %24) # torch/nn/functional.py:1991:17
      %1707 : int = aten::len(%1705) # torch/nn/functional.py:1992:19
      %1708 : int = aten::sub(%1707, %26) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%1708, %25, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %1712 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
          %1713 : int = aten::__getitem__(%1705, %1712) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %1713) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.219)
      %1715 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1715) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.9 : Tensor = aten::batch_norm(%out.7, %1703, %1704, %1701, %1702, %1700, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.11 : Tensor = aten::relu_(%out.9) # torch/nn/functional.py:1117:17
  %1718 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%1436)
  %1719 : Tensor = prim::GetAttr[name="weight"](%1718)
  %1720 : Tensor? = prim::GetAttr[name="bias"](%1718)
  %1721 : int[] = prim::ListConstruct(%27, %27)
  %1722 : int[] = prim::ListConstruct(%24, %24)
  %1723 : int[] = prim::ListConstruct(%27, %27)
  %out.13 : Tensor = aten::conv2d(%out.11, %1719, %1720, %1721, %1722, %1723, %27) # torch/nn/modules/conv.py:415:15
  %1725 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%1436)
  %1726 : int = aten::dim(%out.13) # torch/nn/modules/batchnorm.py:276:11
  %1727 : bool = aten::ne(%1726, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1727) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1728 : bool = prim::GetAttr[name="training"](%1725)
   = prim::If(%1728) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1729 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1725)
      %1730 : Tensor = aten::add(%1729, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1725, %1730)
      -> ()
    block1():
      -> ()
  %1731 : bool = prim::GetAttr[name="training"](%1725)
  %1732 : Tensor = prim::GetAttr[name="running_mean"](%1725)
  %1733 : Tensor = prim::GetAttr[name="running_var"](%1725)
  %1734 : Tensor = prim::GetAttr[name="weight"](%1725)
  %1735 : Tensor = prim::GetAttr[name="bias"](%1725)
   = prim::If(%1731) # torch/nn/functional.py:2011:4
    block0():
      %1736 : int[] = aten::size(%out.13) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%1736, %24) # torch/nn/functional.py:1991:17
      %1738 : int = aten::len(%1736) # torch/nn/functional.py:1992:19
      %1739 : int = aten::sub(%1738, %26) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%1739, %25, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.238 : int):
          %1743 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
          %1744 : int = aten::__getitem__(%1736, %1743) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %1744) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.239)
      %1746 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1746) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.15 : Tensor = aten::batch_norm(%out.13, %1734, %1735, %1732, %1733, %1731, %exponential_average_factor.1, %20, %25) # torch/nn/functional.py:2014:11
  %out.17 : Tensor = aten::add_(%out.15, %input.32, %27) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.24 : Tensor = aten::relu_(%out.17) # torch/nn/functional.py:1117:17
  %1750 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
  %1751 : bool = aten::__contains__(%1750, %name.22) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:15
   = prim::If(%1751) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:64:12
    block0():
      %1752 : Dict(str, str) = prim::GetAttr[name="return_layers"](%12)
      %out_name.15 : str = aten::__getitem__(%1752, %name.22) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:65:27
       = aten::_set_item(%features.1, %out_name.15, %x.24) # torch/hub/pytorch_vision_master/torchvision/models/_utils.py:66:16
      -> ()
    block1():
      -> ()
  %result.1 : Dict(str, Tensor) = aten::dict() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:22:17
  %x.5 : Tensor = aten::__getitem__(%features.1, %6) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:23:12
  %16 : __torch__.torchvision.models.segmentation.deeplabv3.DeepLabHead = prim::GetAttr[name="classifier"](%self)
  %1754 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:90:8
  %1755 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %1757 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %1758 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %1759 : int = prim::Constant[value=2]() # torch/nn/functional.py:1992:31
  %1760 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %1761 : int = prim::Constant[value=0]() # torch/nn/modules/conv.py:414:34
  %1762 : int = prim::Constant[value=12]() # torch/nn/modules/conv.py:414:38
  %1763 : int = prim::Constant[value=24]() # torch/nn/modules/conv.py:414:38
  %1764 : int = prim::Constant[value=36]() # torch/nn/modules/conv.py:414:38
  %1765 : str = prim::Constant[value="area"]() # torch/nn/functional.py:3112:27
  %1766 : str = prim::Constant[value="nearest"]() # torch/nn/functional.py:3112:16
  %1767 : int = prim::Constant[value=3]() # torch/nn/functional.py:3140:22
  %1768 : int = prim::Constant[value=5]() # torch/nn/functional.py:3144:24
  %1769 : str = prim::Constant[value="The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. "]() # torch/nn/functional.py:3000:26
  %1770 : int = prim::Constant[value=-2]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:59:23
  %1771 : int = prim::Constant[value=9223372036854775807]()
  %1772 : str = prim::Constant[value="bilinear"]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:62:48
  %1773 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:62:74
  %1774 : None = prim::Constant()
  %1775 : float = prim::Constant[value=0.5]() # torch/nn/modules/dropout.py:58:32
  %1776 : __torch__.torchvision.models.segmentation.deeplabv3.ASPP = prim::GetAttr[name="0"](%16)
  %1777 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="1"](%16)
  %1778 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="2"](%16)
  %1779 : __torch__.torch.nn.modules.conv.___torch_mangle_61.Conv2d = prim::GetAttr[name="4"](%16)
  %res.1 : Tensor[] = prim::ListConstruct()
  %1781 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name="convs"](%1776)
  %1782 : __torch__.torch.nn.modules.container.___torch_mangle_52.Sequential = prim::GetAttr[name="0"](%1781)
  %1783 : __torch__.torchvision.models.segmentation.deeplabv3.ASPPConv = prim::GetAttr[name="1"](%1781)
  %1784 : __torch__.torchvision.models.segmentation.deeplabv3.___torch_mangle_55.ASPPConv = prim::GetAttr[name="2"](%1781)
  %1785 : __torch__.torchvision.models.segmentation.deeplabv3.___torch_mangle_57.ASPPConv = prim::GetAttr[name="3"](%1781)
  %1786 : __torch__.torchvision.models.segmentation.deeplabv3.ASPPPooling = prim::GetAttr[name="4"](%1781)
  %1787 : __torch__.torch.nn.modules.conv.___torch_mangle_50.Conv2d = prim::GetAttr[name="0"](%1782)
  %1788 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%1782)
  %1789 : Tensor = prim::GetAttr[name="weight"](%1787)
  %1790 : Tensor? = prim::GetAttr[name="bias"](%1787)
  %1791 : int[] = prim::ListConstruct(%1754, %1754)
  %1792 : int[] = prim::ListConstruct(%1761, %1761)
  %1793 : int[] = prim::ListConstruct(%1754, %1754)
  %input.4 : Tensor = aten::conv2d(%x.5, %1789, %1790, %1791, %1792, %1793, %1754) # torch/nn/modules/conv.py:415:15
  %1795 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %1796 : bool = aten::ne(%1795, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1796) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1797 : bool = prim::GetAttr[name="training"](%1788)
   = prim::If(%1797) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1798 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1788)
      %1799 : Tensor = aten::add(%1798, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1788, %1799)
      -> ()
    block1():
      -> ()
  %1800 : bool = prim::GetAttr[name="training"](%1788)
  %1801 : Tensor = prim::GetAttr[name="running_mean"](%1788)
  %1802 : Tensor = prim::GetAttr[name="running_var"](%1788)
  %1803 : Tensor = prim::GetAttr[name="weight"](%1788)
  %1804 : Tensor = prim::GetAttr[name="bias"](%1788)
   = prim::If(%1800) # torch/nn/functional.py:2011:4
    block0():
      %1805 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%1805, %1761) # torch/nn/functional.py:1991:17
      %1807 : int = aten::len(%1805) # torch/nn/functional.py:1992:19
      %1808 : int = aten::sub(%1807, %1759) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%1808, %1760, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.7 : int):
          %1812 : int = aten::add(%i.5, %1759) # torch/nn/functional.py:1993:27
          %1813 : int = aten::__getitem__(%1805, %1812) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %1813) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.5)
      %1815 : bool = aten::eq(%size_prods.4, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%1815) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %1803, %1804, %1801, %1802, %1800, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu(%input.6) # torch/nn/functional.py:1119:17
  %1818 : Tensor[] = aten::append(%res.1, %input.8) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %1819 : __torch__.torch.nn.modules.conv.___torch_mangle_53.Conv2d = prim::GetAttr[name="0"](%1783)
  %1820 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%1783)
  %1821 : Tensor = prim::GetAttr[name="weight"](%1819)
  %1822 : Tensor? = prim::GetAttr[name="bias"](%1819)
  %1823 : int[] = prim::ListConstruct(%1754, %1754)
  %1824 : int[] = prim::ListConstruct(%1762, %1762)
  %1825 : int[] = prim::ListConstruct(%1762, %1762)
  %input.18 : Tensor = aten::conv2d(%x.5, %1821, %1822, %1823, %1824, %1825, %1754) # torch/nn/modules/conv.py:415:15
  %1827 : int = aten::dim(%input.18) # torch/nn/modules/batchnorm.py:276:11
  %1828 : bool = aten::ne(%1827, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1828) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1829 : bool = prim::GetAttr[name="training"](%1820)
   = prim::If(%1829) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1830 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1820)
      %1831 : Tensor = aten::add(%1830, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1820, %1831)
      -> ()
    block1():
      -> ()
  %1832 : bool = prim::GetAttr[name="training"](%1820)
  %1833 : Tensor = prim::GetAttr[name="running_mean"](%1820)
  %1834 : Tensor = prim::GetAttr[name="running_var"](%1820)
  %1835 : Tensor = prim::GetAttr[name="weight"](%1820)
  %1836 : Tensor = prim::GetAttr[name="bias"](%1820)
   = prim::If(%1832) # torch/nn/functional.py:2011:4
    block0():
      %1837 : int[] = aten::size(%input.18) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%1837, %1761) # torch/nn/functional.py:1991:17
      %1839 : int = aten::len(%1837) # torch/nn/functional.py:1992:19
      %1840 : int = aten::sub(%1839, %1759) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%1840, %1760, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %1844 : int = aten::add(%i.3, %1759) # torch/nn/functional.py:1993:27
          %1845 : int = aten::__getitem__(%1837, %1844) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %1845) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.11)
      %1847 : bool = aten::eq(%size_prods.9, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%1847) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.10 : Tensor = aten::batch_norm(%input.18, %1835, %1836, %1833, %1834, %1832, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.11 : Tensor = aten::relu(%input.10) # torch/nn/functional.py:1119:17
  %1850 : Tensor[] = aten::append(%res.1, %input.11) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %1851 : __torch__.torch.nn.modules.conv.___torch_mangle_54.Conv2d = prim::GetAttr[name="0"](%1784)
  %1852 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%1784)
  %1853 : Tensor = prim::GetAttr[name="weight"](%1851)
  %1854 : Tensor? = prim::GetAttr[name="bias"](%1851)
  %1855 : int[] = prim::ListConstruct(%1754, %1754)
  %1856 : int[] = prim::ListConstruct(%1763, %1763)
  %1857 : int[] = prim::ListConstruct(%1763, %1763)
  %input.12 : Tensor = aten::conv2d(%x.5, %1853, %1854, %1855, %1856, %1857, %1754) # torch/nn/modules/conv.py:415:15
  %1859 : int = aten::dim(%input.12) # torch/nn/modules/batchnorm.py:276:11
  %1860 : bool = aten::ne(%1859, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1860) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1861 : bool = prim::GetAttr[name="training"](%1852)
   = prim::If(%1861) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1862 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1852)
      %1863 : Tensor = aten::add(%1862, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1852, %1863)
      -> ()
    block1():
      -> ()
  %1864 : bool = prim::GetAttr[name="training"](%1852)
  %1865 : Tensor = prim::GetAttr[name="running_mean"](%1852)
  %1866 : Tensor = prim::GetAttr[name="running_var"](%1852)
  %1867 : Tensor = prim::GetAttr[name="weight"](%1852)
  %1868 : Tensor = prim::GetAttr[name="bias"](%1852)
   = prim::If(%1864) # torch/nn/functional.py:2011:4
    block0():
      %1869 : int[] = aten::size(%input.12) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%1869, %1761) # torch/nn/functional.py:1991:17
      %1871 : int = aten::len(%1869) # torch/nn/functional.py:1992:19
      %1872 : int = aten::sub(%1871, %1759) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%1872, %1760, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %1876 : int = aten::add(%i.4, %1759) # torch/nn/functional.py:1993:27
          %1877 : int = aten::__getitem__(%1869, %1876) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %1877) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.15)
      %1879 : bool = aten::eq(%size_prods.13, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%1879) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.13 : Tensor = aten::batch_norm(%input.12, %1867, %1868, %1865, %1866, %1864, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.14 : Tensor = aten::relu(%input.13) # torch/nn/functional.py:1119:17
  %1882 : Tensor[] = aten::append(%res.1, %input.14) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %1883 : __torch__.torch.nn.modules.conv.___torch_mangle_56.Conv2d = prim::GetAttr[name="0"](%1785)
  %1884 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%1785)
  %1885 : Tensor = prim::GetAttr[name="weight"](%1883)
  %1886 : Tensor? = prim::GetAttr[name="bias"](%1883)
  %1887 : int[] = prim::ListConstruct(%1754, %1754)
  %1888 : int[] = prim::ListConstruct(%1764, %1764)
  %1889 : int[] = prim::ListConstruct(%1764, %1764)
  %input.15 : Tensor = aten::conv2d(%x.5, %1885, %1886, %1887, %1888, %1889, %1754) # torch/nn/modules/conv.py:415:15
  %1891 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %1892 : bool = aten::ne(%1891, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1892) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1893 : bool = prim::GetAttr[name="training"](%1884)
   = prim::If(%1893) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1894 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1884)
      %1895 : Tensor = aten::add(%1894, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1884, %1895)
      -> ()
    block1():
      -> ()
  %1896 : bool = prim::GetAttr[name="training"](%1884)
  %1897 : Tensor = prim::GetAttr[name="running_mean"](%1884)
  %1898 : Tensor = prim::GetAttr[name="running_var"](%1884)
  %1899 : Tensor = prim::GetAttr[name="weight"](%1884)
  %1900 : Tensor = prim::GetAttr[name="bias"](%1884)
   = prim::If(%1896) # torch/nn/functional.py:2011:4
    block0():
      %1901 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%1901, %1761) # torch/nn/functional.py:1991:17
      %1903 : int = aten::len(%1901) # torch/nn/functional.py:1992:19
      %1904 : int = aten::sub(%1903, %1759) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%1904, %1760, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.18 : int):
          %1908 : int = aten::add(%i.6, %1759) # torch/nn/functional.py:1993:27
          %1909 : int = aten::__getitem__(%1901, %1908) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %1909) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.19)
      %1911 : bool = aten::eq(%size_prods.17, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%1911) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.15, %1899, %1900, %1897, %1898, %1896, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.17 : Tensor = aten::relu(%input.16) # torch/nn/functional.py:1119:17
  %1914 : Tensor[] = aten::append(%res.1, %input.17) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %1915 : int[] = aten::size(%x.5) # <string>:7:9
  %size.2 : int[] = aten::slice(%1915, %1770, %1771, %1754) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:59:15
  %1917 : __torch__.torch.nn.modules.conv.___torch_mangle_50.Conv2d = prim::GetAttr[name="1"](%1786)
  %1918 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="2"](%1786)
  %1919 : int[] = prim::ListConstruct(%1754, %1754)
  %1920 : int[] = aten::size(%x.5) # torch/nn/functional.py:925:51
  %1921 : int = aten::len(%1920) # <string>:5:9
  %1922 : bool = aten::gt(%1921, %1759) # <string>:5:9
   = prim::If(%1922) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%1758) # <string>:5:2
      -> ()
  %x.4 : Tensor = aten::adaptive_avg_pool2d(%x.5, %1919) # torch/nn/functional.py:926:11
  %1924 : Tensor = prim::GetAttr[name="weight"](%1917)
  %1925 : Tensor? = prim::GetAttr[name="bias"](%1917)
  %1926 : int[] = prim::ListConstruct(%1754, %1754)
  %1927 : int[] = prim::ListConstruct(%1761, %1761)
  %1928 : int[] = prim::ListConstruct(%1754, %1754)
  %x.6 : Tensor = aten::conv2d(%x.4, %1924, %1925, %1926, %1927, %1928, %1754) # torch/nn/modules/conv.py:415:15
  %1930 : int = aten::dim(%x.6) # torch/nn/modules/batchnorm.py:276:11
  %1931 : bool = aten::ne(%1930, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1931) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1932 : bool = prim::GetAttr[name="training"](%1918)
   = prim::If(%1932) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1933 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1918)
      %1934 : Tensor = aten::add(%1933, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1918, %1934)
      -> ()
    block1():
      -> ()
  %1935 : bool = prim::GetAttr[name="training"](%1918)
  %1936 : Tensor = prim::GetAttr[name="running_mean"](%1918)
  %1937 : Tensor = prim::GetAttr[name="running_var"](%1918)
  %1938 : Tensor = prim::GetAttr[name="weight"](%1918)
  %1939 : Tensor = prim::GetAttr[name="bias"](%1918)
   = prim::If(%1935) # torch/nn/functional.py:2011:4
    block0():
      %1940 : int[] = aten::size(%x.6) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%1940, %1761) # torch/nn/functional.py:1991:17
      %1942 : int = aten::len(%1940) # torch/nn/functional.py:1992:19
      %1943 : int = aten::sub(%1942, %1759) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%1943, %1760, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.22 : int):
          %1947 : int = aten::add(%i.2, %1759) # torch/nn/functional.py:1993:27
          %1948 : int = aten::__getitem__(%1940, %1947) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %1948) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.23)
      %1950 : bool = aten::eq(%size_prods.21, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%1950) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.8 : Tensor = aten::batch_norm(%x.6, %1938, %1939, %1936, %1937, %1935, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %x.10 : Tensor = aten::relu(%x.8) # torch/nn/functional.py:1119:17
  %1953 : Tensor = prim::Uninitialized()
  %1954 : bool? = prim::Uninitialized()
  %1955 : bool = prim::Uninitialized()
  %1956 : str[] = prim::ListConstruct(%1766, %1765)
  %1957 : bool = aten::__contains__(%1956, %1772) # torch/nn/functional.py:3112:7
  %align_corners.61 : bool? = prim::If(%1957) # torch/nn/functional.py:3112:4
    block0():
       = prim::RaiseException(%1758) # torch/nn/functional.py:3114:12
      -> (%1954)
    block1():
      -> (%1773)
  %1959 : int = aten::dim(%x.10) # torch/nn/functional.py:3124:23
  %scale_factor_len.2 : int = aten::sub(%1959, %1759) # torch/nn/functional.py:3124:23
  %scale_factor_list.3 : float?[] = prim::ListConstruct()
   = prim::Loop(%scale_factor_len.2, %1760) # torch/nn/functional.py:3125:66
    block0(%1962 : int):
      %1963 : float?[] = aten::append(%scale_factor_list.3, %1774) # torch/nn/functional.py:3125:66
      -> (%1760)
  %closed_over_args.2 : (Tensor, int[]?, float[]?, bool?) = prim::TupleConstruct(%x.10, %size.2, %1774, %1774)
  %1965 : float[] = prim::Uninitialized()
  %1966 : float[]? = prim::Uninitialized()
  %1967 : int[]? = prim::Uninitialized()
  %input.2 : Tensor, %size.3 : int[]?, %scale_factor.3 : float[]?, %recompute_scale_factor.2 : bool? = prim::TupleUnpack(%closed_over_args.2)
  %1972 : int = aten::dim(%input.2) # torch/nn/functional.py:2966:10
  %dim.2 : int = aten::sub(%1972, %1759) # torch/nn/functional.py:2966:10
  %1974 : bool = aten::__is__(%size.3, %1774) # torch/nn/functional.py:2967:7
  %1975 : bool, %size.26 : int[]? = prim::If(%1974) # torch/nn/functional.py:2967:7
    block0():
      %1977 : bool = aten::__is__(%scale_factor.3, %1774) # torch/nn/functional.py:2967:24
      -> (%1977, %size.3)
    block1():
      %size.5 : int[] = prim::unchecked_cast(%size.3)
      -> (%1773, %size.5)
   = prim::If(%1975) # torch/nn/functional.py:2967:4
    block0():
       = prim::RaiseException(%1758) # torch/nn/functional.py:2968:8
      -> ()
    block1():
      -> ()
  %1979 : bool = aten::__isnot__(%size.26, %1774) # torch/nn/functional.py:2969:7
  %1980 : bool, %size.27 : int[]? = prim::If(%1979) # torch/nn/functional.py:2969:7
    block0():
      %size.9 : int[] = prim::unchecked_cast(%size.26)
      %1983 : bool = aten::__isnot__(%scale_factor.3, %1774) # torch/nn/functional.py:2969:28
      -> (%1983, %size.9)
    block1():
      -> (%1773, %size.26)
  %scale_factor.28 : float[]?, %size.28 : int[]? = prim::If(%1980) # torch/nn/functional.py:2969:4
    block0():
       = prim::RaiseException(%1758) # torch/nn/functional.py:2970:8
      -> (%1966, %1967)
    block1():
      -> (%scale_factor.3, %size.27)
  %1986 : bool = aten::__isnot__(%scale_factor.28, %1774) # torch/nn/functional.py:2971:7
  %scale_factor.29 : float[]? = prim::If(%1986) # torch/nn/functional.py:2971:4
    block0():
      %scale_factor.12 : float[] = prim::unchecked_cast(%scale_factor.28)
      %1989 : int = aten::len(%scale_factor.12) # torch/nn/functional.py:2973:15
      %1990 : bool = aten::ne(%1989, %dim.2) # torch/nn/functional.py:2973:15
       = prim::If(%1990) # torch/nn/functional.py:2973:12
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:2974:16
          -> ()
        block1():
          -> ()
      -> (%scale_factor.12)
    block1():
      -> (%scale_factor.28)
  %1991 : bool = aten::__isnot__(%size.28, %1774) # torch/nn/functional.py:2977:7
  %output_size.2 : int[] = prim::If(%1991) # torch/nn/functional.py:2977:4
    block0():
      %size.18 : int[] = prim::unchecked_cast(%size.28)
      -> (%size.18)
    block1():
      %1994 : bool = aten::__isnot__(%scale_factor.29, %1774) # torch/nn/functional.py:2983:11
      %scale_factor.5 : float[] = prim::If(%1994) # torch/nn/functional.py:2983:4
        block0():
          %scale_factor.22 : float[] = prim::unchecked_cast(%scale_factor.29)
          -> (%scale_factor.22)
        block1():
           = prim::RaiseException(%1758) # torch/nn/functional.py:2983:4
          -> (%1965)
      %1997 : bool = aten::__is__(%recompute_scale_factor.2, %1774) # torch/nn/functional.py:2989:7
       = prim::If(%1997) # torch/nn/functional.py:2989:4
        block0():
          %1998 : int = aten::len(%scale_factor.5) # torch/nn/functional.py:2994:8
          %1999 : bool = aten::gt(%1998, %1761)
          %is_float_scale_factor.2 : bool, %2001 : int = prim::Loop(%1771, %1999, %1773, %1761) # torch/nn/functional.py:2994:8
            block0(%2002 : int, %is_float_scale_factor.7 : bool, %2004 : int):
              %scale.3 : float = aten::__getitem__(%scale_factor.5, %2004) # torch/nn/functional.py:2994:8
              %2006 : int = aten::floor(%scale.3) # torch/nn/functional.py:2995:36
              %is_float_scale_factor.8 : bool = aten::ne(%2006, %scale.3) # torch/nn/functional.py:2995:36
              %2008 : bool = prim::If(%is_float_scale_factor.8) # torch/nn/functional.py:2996:12
                block0():
                  -> (%1773)
                block1():
                  -> (%1760)
              %2009 : int = aten::add(%2004, %1754)
              %2010 : bool = aten::lt(%2009, %1998)
              %2011 : bool = aten::__and__(%2010, %2008)
              -> (%2011, %is_float_scale_factor.8, %2009)
           = prim::If(%is_float_scale_factor.2) # torch/nn/functional.py:2999:8
            block0():
               = aten::warn(%1769, %1759) # torch/nn/functional.py:3000:12
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2012 : int[] = prim::ListConstruct()
       = prim::Loop(%dim.2, %1760) # torch/nn/functional.py:3011:11
        block0(%i.7 : int):
          %2014 : int = aten::add(%i.7, %1759) # torch/nn/functional.py:3011:44
          %2015 : int = aten::size(%input.2, %2014) # torch/nn/functional.py:3011:33
          %2016 : float = aten::Float(%2015) # torch/nn/functional.py:3011:27
          %2017 : float = aten::__getitem__(%scale_factor.5, %i.7) # torch/nn/functional.py:3011:54
          %2018 : float = aten::mul(%2016, %2017) # torch/nn/functional.py:3011:27
          %2019 : int = aten::floor(%2018) # torch/nn/functional.py:3011:16
          %2020 : int[] = aten::append(%2012, %2019) # torch/nn/functional.py:3011:11
          -> (%1760)
      -> (%2012)
  %2021 : int = aten::dim(%x.10) # torch/nn/functional.py:3155:9
  %2022 : bool = aten::eq(%2021, %1767) # torch/nn/functional.py:3155:9
  %2023 : Tensor = prim::If(%2022) # torch/nn/functional.py:3155:4
    block0():
       = prim::RaiseException(%1758) # torch/nn/functional.py:3156:8
      -> (%1953)
    block1():
      %2024 : int = aten::dim(%x.10) # torch/nn/functional.py:3161:9
      %2025 : bool = aten::eq(%2024, %1757) # torch/nn/functional.py:3161:9
      %2026 : Tensor = prim::If(%2025) # torch/nn/functional.py:3161:4
        block0():
          %2027 : bool = aten::__isnot__(%align_corners.61, %1774) # torch/nn/functional.py:3162:15
          %align_corners.62 : bool = prim::If(%2027) # torch/nn/functional.py:3162:8
            block0():
              %align_corners.32 : bool = prim::unchecked_cast(%align_corners.61)
              -> (%align_corners.32)
            block1():
               = prim::RaiseException(%1758) # torch/nn/functional.py:3162:8
              -> (%1955)
          %2030 : float? = aten::__getitem__(%scale_factor_list.3, %1761) # torch/nn/functional.py:3163:83
          %2031 : float? = aten::__getitem__(%scale_factor_list.3, %1754) # torch/nn/functional.py:3163:91
          %2032 : Tensor = aten::upsample_bilinear2d(%x.10, %output_size.2, %align_corners.62, %2030, %2031) # torch/nn/functional.py:3163:15
          -> (%2032)
        block1():
          %2033 : int = aten::dim(%x.10) # torch/nn/functional.py:3168:9
          %2034 : bool = aten::eq(%2033, %1768) # torch/nn/functional.py:3168:9
           = prim::If(%2034) # torch/nn/functional.py:3168:4
            block0():
               = prim::RaiseException(%1758) # torch/nn/functional.py:3169:8
              -> ()
            block1():
               = prim::RaiseException(%1758) # torch/nn/functional.py:3177:8
              -> ()
          -> (%1953)
      -> (%2026)
  %2035 : Tensor[] = aten::append(%res.1, %2023) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:91:12
  %res.8 : Tensor = aten::cat(%res.1, %1754) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/deeplabv3.py:92:14
  %2037 : __torch__.torch.nn.modules.container.___torch_mangle_60.Sequential = prim::GetAttr[name="project"](%1776)
  %2038 : __torch__.torch.nn.modules.conv.___torch_mangle_59.Conv2d = prim::GetAttr[name="0"](%2037)
  %2039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%2037)
  %2040 : __torch__.torch.nn.modules.dropout.Dropout = prim::GetAttr[name="3"](%2037)
  %2041 : Tensor = prim::GetAttr[name="weight"](%2038)
  %2042 : Tensor? = prim::GetAttr[name="bias"](%2038)
  %2043 : int[] = prim::ListConstruct(%1754, %1754)
  %2044 : int[] = prim::ListConstruct(%1761, %1761)
  %2045 : int[] = prim::ListConstruct(%1754, %1754)
  %input.3 : Tensor = aten::conv2d(%res.8, %2041, %2042, %2043, %2044, %2045, %1754) # torch/nn/modules/conv.py:415:15
  %2047 : int = aten::dim(%input.3) # torch/nn/modules/batchnorm.py:276:11
  %2048 : bool = aten::ne(%2047, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2048) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2049 : bool = prim::GetAttr[name="training"](%2039)
   = prim::If(%2049) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2050 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2039)
      %2051 : Tensor = aten::add(%2050, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2039, %2051)
      -> ()
    block1():
      -> ()
  %2052 : bool = prim::GetAttr[name="training"](%2039)
  %2053 : Tensor = prim::GetAttr[name="running_mean"](%2039)
  %2054 : Tensor = prim::GetAttr[name="running_var"](%2039)
  %2055 : Tensor = prim::GetAttr[name="weight"](%2039)
  %2056 : Tensor = prim::GetAttr[name="bias"](%2039)
   = prim::If(%2052) # torch/nn/functional.py:2011:4
    block0():
      %2057 : int[] = aten::size(%input.3) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%2057, %1761) # torch/nn/functional.py:1991:17
      %2059 : int = aten::len(%2057) # torch/nn/functional.py:1992:19
      %2060 : int = aten::sub(%2059, %1759) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%2060, %1760, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.26 : int):
          %2064 : int = aten::add(%i.8, %1759) # torch/nn/functional.py:1993:27
          %2065 : int = aten::__getitem__(%2057, %2064) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %2065) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.27)
      %2067 : bool = aten::eq(%size_prods.25, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%2067) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.5 : Tensor = aten::batch_norm(%input.3, %2055, %2056, %2053, %2054, %2052, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.7 : Tensor = aten::relu(%input.5) # torch/nn/functional.py:1119:17
  %2070 : bool = prim::GetAttr[name="training"](%2040)
  %input.20 : Tensor = aten::dropout(%input.7, %1775, %2070) # torch/nn/functional.py:973:17
  %2072 : Tensor = prim::GetAttr[name="weight"](%1777)
  %2073 : Tensor? = prim::GetAttr[name="bias"](%1777)
  %2074 : int[] = prim::ListConstruct(%1754, %1754)
  %2075 : int[] = prim::ListConstruct(%1754, %1754)
  %2076 : int[] = prim::ListConstruct(%1754, %1754)
  %input.21 : Tensor = aten::conv2d(%input.20, %2072, %2073, %2074, %2075, %2076, %1754) # torch/nn/modules/conv.py:415:15
  %2078 : int = aten::dim(%input.21) # torch/nn/modules/batchnorm.py:276:11
  %2079 : bool = aten::ne(%2078, %1757) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2079) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%1758) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2080 : bool = prim::GetAttr[name="training"](%1778)
   = prim::If(%2080) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2081 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1778)
      %2082 : Tensor = aten::add(%2081, %1754, %1754) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1778, %2082)
      -> ()
    block1():
      -> ()
  %2083 : bool = prim::GetAttr[name="training"](%1778)
  %2084 : Tensor = prim::GetAttr[name="running_mean"](%1778)
  %2085 : Tensor = prim::GetAttr[name="running_var"](%1778)
  %2086 : Tensor = prim::GetAttr[name="weight"](%1778)
  %2087 : Tensor = prim::GetAttr[name="bias"](%1778)
   = prim::If(%2083) # torch/nn/functional.py:2011:4
    block0():
      %2088 : int[] = aten::size(%input.21) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%2088, %1761) # torch/nn/functional.py:1991:17
      %2090 : int = aten::len(%2088) # torch/nn/functional.py:1992:19
      %2091 : int = aten::sub(%2090, %1759) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%2091, %1760, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.6 : int):
          %2095 : int = aten::add(%i.62, %1759) # torch/nn/functional.py:1993:27
          %2096 : int = aten::__getitem__(%2088, %2095) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %2096) # torch/nn/functional.py:1993:8
          -> (%1760, %size_prods.3)
      %2098 : bool = aten::eq(%size_prods, %1754) # torch/nn/functional.py:1994:7
       = prim::If(%2098) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%1758) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.21, %2086, %2087, %2084, %2085, %2083, %exponential_average_factor.2, %1755, %1760) # torch/nn/functional.py:2014:11
  %input.23 : Tensor = aten::relu(%input.22) # torch/nn/functional.py:1119:17
  %2101 : Tensor = prim::GetAttr[name="weight"](%1779)
  %2102 : Tensor? = prim::GetAttr[name="bias"](%1779)
  %2103 : int[] = prim::ListConstruct(%1754, %1754)
  %2104 : int[] = prim::ListConstruct(%1761, %1761)
  %2105 : int[] = prim::ListConstruct(%1754, %1754)
  %x.7 : Tensor = aten::conv2d(%input.23, %2101, %2102, %2103, %2104, %2105, %1754) # torch/nn/modules/conv.py:415:15
  %2107 : str = prim::Constant[value="The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and uses scale_factor directly, instead of relying on the computed output size. If you wish to keep the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. "]() # torch/nn/functional.py:3000:26
  %2108 : int = prim::Constant[value=9223372036854775807]()
  %2109 : int = prim::Constant[value=5]() # torch/nn/functional.py:3144:24
  %2110 : int = prim::Constant[value=1]() # torch/nn/functional.py:3143:79
  %2111 : int = prim::Constant[value=4]() # torch/nn/functional.py:3142:24
  %2112 : int = prim::Constant[value=0]() # torch/nn/functional.py:3141:71
  %2113 : int = prim::Constant[value=3]() # torch/nn/functional.py:3140:22
  %2114 : int = prim::Constant[value=2]() # torch/nn/functional.py:3124:37
  %2115 : str = prim::Constant[value="nearest"]() # torch/nn/functional.py:3112:16
  %2116 : str = prim::Constant[value="area"]() # torch/nn/functional.py:3112:27
  %2117 : None = prim::Constant() # torch/nn/functional.py:3113:32
  %2118 : str = prim::Constant[value="Exception"]() # torch/nn/functional.py:3114:12
  %2119 : str = prim::Constant[value="Default upsampling behavior when mode={} is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details."]() # torch/nn/functional.py:3118:26
  %align_corners.9 : bool = prim::Constant[value=0]() # torch/nn/functional.py:3122:28
  %2121 : bool = prim::Constant[value=1]() # torch/nn/functional.py:3125:66
  %2122 : str = prim::Constant[value="linear"]() # torch/nn/functional.py:3152:38
  %2123 : str = prim::Constant[value="bilinear"]() # torch/nn/functional.py:3155:38
  %2124 : str = prim::Constant[value="trilinear"]() # torch/nn/functional.py:3157:38
  %2125 : str = prim::Constant[value="bicubic"]() # torch/nn/functional.py:3173:38
  %2126 : Tensor = prim::Uninitialized()
  %2127 : bool? = prim::Uninitialized()
  %2128 : bool = prim::Uninitialized()
  %2129 : str[] = prim::ListConstruct(%2115, %2116)
  %2130 : bool = aten::__contains__(%2129, %5) # torch/nn/functional.py:3112:7
  %align_corners.56 : bool? = prim::If(%2130) # torch/nn/functional.py:3112:4
    block0():
      %2132 : bool = aten::__isnot__(%4, %2117) # torch/nn/functional.py:3113:11
      %align_corners.54 : bool? = prim::If(%2132) # torch/nn/functional.py:3113:8
        block0():
           = prim::RaiseException(%2118) # torch/nn/functional.py:3114:12
          -> (%2127)
        block1():
          -> (%4)
      -> (%align_corners.54)
    block1():
      %2134 : bool = aten::__is__(%4, %2117) # torch/nn/functional.py:3117:11
      %align_corners.55 : bool = prim::If(%2134) # torch/nn/functional.py:3117:8
        block0():
          %2136 : str = aten::format(%2119, %5) # torch/nn/functional.py:3118:26
           = aten::warn(%2136, %2114) # torch/nn/functional.py:3118:12
          -> (%align_corners.9)
        block1():
          %align_corners.12 : bool = prim::unchecked_cast(%4)
          -> (%align_corners.12)
      -> (%align_corners.55)
  %2138 : int = aten::dim(%x.7) # torch/nn/functional.py:3124:23
  %scale_factor_len.1 : int = aten::sub(%2138, %2114) # torch/nn/functional.py:3124:23
  %scale_factor_list.1 : float?[] = prim::ListConstruct()
   = prim::Loop(%scale_factor_len.1, %2121) # torch/nn/functional.py:3125:66
    block0(%2141 : int):
      %2142 : float?[] = aten::append(%scale_factor_list.1, %2117) # torch/nn/functional.py:3125:66
      -> (%2121)
  %2143 : bool = aten::__isnot__(%3, %2117) # torch/nn/functional.py:3127:7
  %2144 : bool, %scale_factor.15 : float[]? = prim::If(%2143) # torch/nn/functional.py:3127:7
    block0():
      %scale_factor.4 : float[] = prim::unchecked_cast(%3)
      %2147 : bool = aten::__is__(%3, %align_corners.9) # torch/nn/functional.py:3127:37
      %2148 : bool = prim::If(%2147) # torch/nn/functional.py:3127:37
        block0():
          -> (%2121)
        block1():
          %2149 : bool = aten::__is__(%3, %2117) # torch/nn/functional.py:3127:72
          -> (%2149)
      -> (%2148, %scale_factor.4)
    block1():
      -> (%align_corners.9, %3)
  %scale_factor : float[]?, %scale_factor_list : float?[] = prim::If(%2144) # torch/nn/functional.py:3127:4
    block0():
      %scale_factor.7 : float[] = prim::unchecked_cast(%scale_factor.15)
      %scale_factor_list.2 : float?[] = prim::ListConstruct()
      %2154 : int = aten::len(%scale_factor.7) # torch/nn/functional.py:3132:70
       = prim::Loop(%2154, %2121) # torch/nn/functional.py:3132:70
        block0(%2155 : int):
          %elem.1 : float = aten::__getitem__(%scale_factor.7, %2155) # torch/nn/functional.py:3132:70
          %2157 : float?[] = aten::append(%scale_factor_list.2, %elem.1) # torch/nn/functional.py:3132:70
          -> (%2121)
      -> (%scale_factor.7, %scale_factor_list.2)
    block1():
      -> (%scale_factor.15, %scale_factor_list.1)
  %closed_over_args.1 : (Tensor, int[]?, float[]?, bool?) = prim::TupleConstruct(%x.7, %input_shape.1, %scale_factor, %3)
  %2159 : float[] = prim::Uninitialized()
  %2160 : float[]? = prim::Uninitialized()
  %2161 : int[]? = prim::Uninitialized()
  %input.1 : Tensor, %size.1 : int[]?, %scale_factor.1 : float[]?, %recompute_scale_factor.1 : bool? = prim::TupleUnpack(%closed_over_args.1)
  %2166 : int = aten::dim(%input.1) # torch/nn/functional.py:2966:10
  %dim.1 : int = aten::sub(%2166, %2114) # torch/nn/functional.py:2966:10
  %2168 : bool = aten::__is__(%size.1, %2117) # torch/nn/functional.py:2967:7
  %2169 : bool, %size.23 : int[]? = prim::If(%2168) # torch/nn/functional.py:2967:7
    block0():
      %2171 : bool = aten::__is__(%scale_factor.1, %2117) # torch/nn/functional.py:2967:24
      -> (%2171, %size.1)
    block1():
      %size.4 : int[] = prim::unchecked_cast(%size.1)
      -> (%align_corners.9, %size.4)
   = prim::If(%2169) # torch/nn/functional.py:2967:4
    block0():
       = prim::RaiseException(%2118) # torch/nn/functional.py:2968:8
      -> ()
    block1():
      -> ()
  %2173 : bool = aten::__isnot__(%size.23, %2117) # torch/nn/functional.py:2969:7
  %2174 : bool, %size.24 : int[]? = prim::If(%2173) # torch/nn/functional.py:2969:7
    block0():
      %size.8 : int[] = prim::unchecked_cast(%size.23)
      %2177 : bool = aten::__isnot__(%scale_factor.1, %2117) # torch/nn/functional.py:2969:28
      -> (%2177, %size.8)
    block1():
      -> (%align_corners.9, %size.23)
  %scale_factor.26 : float[]?, %size.25 : int[]? = prim::If(%2174) # torch/nn/functional.py:2969:4
    block0():
       = prim::RaiseException(%2118) # torch/nn/functional.py:2970:8
      -> (%2160, %2161)
    block1():
      -> (%scale_factor.1, %size.24)
  %2180 : bool = aten::__isnot__(%scale_factor.26, %2117) # torch/nn/functional.py:2971:7
  %scale_factor.27 : float[]? = prim::If(%2180) # torch/nn/functional.py:2971:4
    block0():
      %scale_factor.11 : float[] = prim::unchecked_cast(%scale_factor.26)
      %2183 : int = aten::len(%scale_factor.11) # torch/nn/functional.py:2973:15
      %2184 : bool = aten::ne(%2183, %dim.1) # torch/nn/functional.py:2973:15
       = prim::If(%2184) # torch/nn/functional.py:2973:12
        block0():
           = prim::RaiseException(%2118) # torch/nn/functional.py:2974:16
          -> ()
        block1():
          -> ()
      -> (%scale_factor.11)
    block1():
      -> (%scale_factor.26)
  %2185 : bool = aten::__isnot__(%size.25, %2117) # torch/nn/functional.py:2977:7
  %output_size.1 : int[] = prim::If(%2185) # torch/nn/functional.py:2977:4
    block0():
      %size.17 : int[] = prim::unchecked_cast(%size.25)
      -> (%size.17)
    block1():
      %2188 : bool = aten::__isnot__(%scale_factor.27, %2117) # torch/nn/functional.py:2983:11
      %scale_factor.2 : float[] = prim::If(%2188) # torch/nn/functional.py:2983:4
        block0():
          %scale_factor.21 : float[] = prim::unchecked_cast(%scale_factor.27)
          -> (%scale_factor.21)
        block1():
           = prim::RaiseException(%2118) # torch/nn/functional.py:2983:4
          -> (%2159)
      %2191 : bool = aten::__is__(%recompute_scale_factor.1, %2117) # torch/nn/functional.py:2989:7
       = prim::If(%2191) # torch/nn/functional.py:2989:4
        block0():
          %2192 : int = aten::len(%scale_factor.2) # torch/nn/functional.py:2994:8
          %2193 : bool = aten::gt(%2192, %2112)
          %is_float_scale_factor.1 : bool, %2195 : int = prim::Loop(%2108, %2193, %align_corners.9, %2112) # torch/nn/functional.py:2994:8
            block0(%2196 : int, %is_float_scale_factor.6 : bool, %2198 : int):
              %scale.2 : float = aten::__getitem__(%scale_factor.2, %2198) # torch/nn/functional.py:2994:8
              %2200 : int = aten::floor(%scale.2) # torch/nn/functional.py:2995:36
              %is_float_scale_factor.5 : bool = aten::ne(%2200, %scale.2) # torch/nn/functional.py:2995:36
              %2202 : bool = prim::If(%is_float_scale_factor.5) # torch/nn/functional.py:2996:12
                block0():
                  -> (%align_corners.9)
                block1():
                  -> (%2121)
              %2203 : int = aten::add(%2198, %2110)
              %2204 : bool = aten::lt(%2203, %2192)
              %2205 : bool = aten::__and__(%2204, %2202)
              -> (%2205, %is_float_scale_factor.5, %2203)
           = prim::If(%is_float_scale_factor.1) # torch/nn/functional.py:2999:8
            block0():
               = aten::warn(%2107, %2114) # torch/nn/functional.py:3000:12
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2206 : int[] = prim::ListConstruct()
       = prim::Loop(%dim.1, %2121) # torch/nn/functional.py:3011:11
        block0(%i.1 : int):
          %2208 : int = aten::add(%i.1, %2114) # torch/nn/functional.py:3011:44
          %2209 : int = aten::size(%input.1, %2208) # torch/nn/functional.py:3011:33
          %2210 : float = aten::Float(%2209) # torch/nn/functional.py:3011:27
          %2211 : float = aten::__getitem__(%scale_factor.2, %i.1) # torch/nn/functional.py:3011:54
          %2212 : float = aten::mul(%2210, %2211) # torch/nn/functional.py:3011:27
          %2213 : int = aten::floor(%2212) # torch/nn/functional.py:3011:16
          %2214 : int[] = aten::append(%2206, %2213) # torch/nn/functional.py:3011:11
          -> (%2121)
      -> (%2206)
  %2215 : int = aten::dim(%x.7) # torch/nn/functional.py:3140:7
  %2216 : bool = aten::eq(%2215, %2113) # torch/nn/functional.py:3140:7
  %2217 : bool = prim::If(%2216) # torch/nn/functional.py:3140:7
    block0():
      %2218 : bool = aten::eq(%5, %2115) # torch/nn/functional.py:3140:28
      -> (%2218)
    block1():
      -> (%align_corners.9)
  %x.11 : Tensor = prim::If(%2217) # torch/nn/functional.py:3140:4
    block0():
      %2220 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3141:67
      %2221 : Tensor = aten::upsample_nearest1d(%x.7, %output_size.1, %2220) # torch/nn/functional.py:3141:15
      -> (%2221)
    block1():
      %2222 : int = aten::dim(%x.7) # torch/nn/functional.py:3142:9
      %2223 : bool = aten::eq(%2222, %2111) # torch/nn/functional.py:3142:9
      %2224 : bool = prim::If(%2223) # torch/nn/functional.py:3142:9
        block0():
          %2225 : bool = aten::eq(%5, %2115) # torch/nn/functional.py:3142:30
          -> (%2225)
        block1():
          -> (%align_corners.9)
      %2226 : Tensor = prim::If(%2224) # torch/nn/functional.py:3142:4
        block0():
          %2227 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3143:67
          %2228 : float? = aten::__getitem__(%scale_factor_list, %2110) # torch/nn/functional.py:3143:75
          %2229 : Tensor = aten::upsample_nearest2d(%x.7, %output_size.1, %2227, %2228) # torch/nn/functional.py:3143:15
          -> (%2229)
        block1():
          %2230 : int = aten::dim(%x.7) # torch/nn/functional.py:3144:9
          %2231 : bool = aten::eq(%2230, %2109) # torch/nn/functional.py:3144:9
          %2232 : bool = prim::If(%2231) # torch/nn/functional.py:3144:9
            block0():
              %2233 : bool = aten::eq(%5, %2115) # torch/nn/functional.py:3144:30
              -> (%2233)
            block1():
              -> (%align_corners.9)
          %2234 : Tensor = prim::If(%2232) # torch/nn/functional.py:3144:4
            block0():
              %2235 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3145:67
              %2236 : float? = aten::__getitem__(%scale_factor_list, %2110) # torch/nn/functional.py:3145:75
              %2237 : float? = aten::__getitem__(%scale_factor_list, %2114) # torch/nn/functional.py:3145:83
              %2238 : Tensor = aten::upsample_nearest3d(%x.7, %output_size.1, %2235, %2236, %2237) # torch/nn/functional.py:3145:15
              -> (%2238)
            block1():
              %2239 : int = aten::dim(%x.7) # torch/nn/functional.py:3146:9
              %2240 : bool = aten::eq(%2239, %2113) # torch/nn/functional.py:3146:9
              %2241 : bool = prim::If(%2240) # torch/nn/functional.py:3146:9
                block0():
                  %2242 : bool = aten::eq(%5, %2116) # torch/nn/functional.py:3146:30
                  -> (%2242)
                block1():
                  -> (%align_corners.9)
              %2243 : Tensor = prim::If(%2241) # torch/nn/functional.py:3146:4
                block0():
                  %2244 : Tensor = aten::adaptive_avg_pool1d(%x.7, %output_size.1) # torch/nn/functional.py:3147:15
                  -> (%2244)
                block1():
                  %2245 : int = aten::dim(%x.7) # torch/nn/functional.py:3148:9
                  %2246 : bool = aten::eq(%2245, %2111) # torch/nn/functional.py:3148:9
                  %2247 : bool = prim::If(%2246) # torch/nn/functional.py:3148:9
                    block0():
                      %2248 : bool = aten::eq(%5, %2116) # torch/nn/functional.py:3148:30
                      -> (%2248)
                    block1():
                      -> (%align_corners.9)
                  %2249 : Tensor = prim::If(%2247) # torch/nn/functional.py:3148:4
                    block0():
                      %2250 : int[] = aten::size(%x.7) # torch/nn/functional.py:925:51
                      %2251 : int = aten::len(%2250) # <string>:5:9
                      %2252 : int = aten::len(%output_size.1) # <string>:5:25
                      %2253 : bool = aten::gt(%2251, %2252) # <string>:5:9
                       = prim::If(%2253) # <string>:5:2
                        block0():
                          -> ()
                        block1():
                           = prim::RaiseException(%2118) # <string>:5:2
                          -> ()
                      %2254 : Tensor = aten::adaptive_avg_pool2d(%x.7, %output_size.1) # torch/nn/functional.py:926:11
                      -> (%2254)
                    block1():
                      %2255 : int = aten::dim(%x.7) # torch/nn/functional.py:3150:9
                      %2256 : bool = aten::eq(%2255, %2109) # torch/nn/functional.py:3150:9
                      %2257 : bool = prim::If(%2256) # torch/nn/functional.py:3150:9
                        block0():
                          %2258 : bool = aten::eq(%5, %2116) # torch/nn/functional.py:3150:30
                          -> (%2258)
                        block1():
                          -> (%align_corners.9)
                      %2259 : Tensor = prim::If(%2257) # torch/nn/functional.py:3150:4
                        block0():
                          %2260 : int[] = aten::size(%x.7) # torch/nn/functional.py:945:51
                          %2261 : int = aten::len(%2260) # <string>:5:9
                          %2262 : int = aten::len(%output_size.1) # <string>:5:25
                          %2263 : bool = aten::gt(%2261, %2262) # <string>:5:9
                           = prim::If(%2263) # <string>:5:2
                            block0():
                              -> ()
                            block1():
                               = prim::RaiseException(%2118) # <string>:5:2
                              -> ()
                          %2264 : Tensor = aten::adaptive_avg_pool3d(%x.7, %output_size.1) # torch/nn/functional.py:946:11
                          -> (%2264)
                        block1():
                          %2265 : int = aten::dim(%x.7) # torch/nn/functional.py:3152:9
                          %2266 : bool = aten::eq(%2265, %2113) # torch/nn/functional.py:3152:9
                          %2267 : bool = prim::If(%2266) # torch/nn/functional.py:3152:9
                            block0():
                              %2268 : bool = aten::eq(%5, %2122) # torch/nn/functional.py:3152:30
                              -> (%2268)
                            block1():
                              -> (%align_corners.9)
                          %2269 : Tensor = prim::If(%2267) # torch/nn/functional.py:3152:4
                            block0():
                              %2270 : bool = aten::__isnot__(%align_corners.56, %2117) # torch/nn/functional.py:3153:15
                              %align_corners.57 : bool = prim::If(%2270) # torch/nn/functional.py:3153:8
                                block0():
                                  %align_corners.24 : bool = prim::unchecked_cast(%align_corners.56)
                                  -> (%align_corners.24)
                                block1():
                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3153:8
                                  -> (%2128)
                              %2273 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3154:81
                              %2274 : Tensor = aten::upsample_linear1d(%x.7, %output_size.1, %align_corners.57, %2273) # torch/nn/functional.py:3154:15
                              -> (%2274)
                            block1():
                              %2275 : int = aten::dim(%x.7) # torch/nn/functional.py:3155:9
                              %2276 : bool = aten::eq(%2275, %2113) # torch/nn/functional.py:3155:9
                              %2277 : bool = prim::If(%2276) # torch/nn/functional.py:3155:9
                                block0():
                                  %2278 : bool = aten::eq(%5, %2123) # torch/nn/functional.py:3155:30
                                  -> (%2278)
                                block1():
                                  -> (%align_corners.9)
                              %2279 : Tensor = prim::If(%2277) # torch/nn/functional.py:3155:4
                                block0():
                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3156:8
                                  -> (%2126)
                                block1():
                                  %2280 : int = aten::dim(%x.7) # torch/nn/functional.py:3157:9
                                  %2281 : bool = aten::eq(%2280, %2113) # torch/nn/functional.py:3157:9
                                  %2282 : bool = prim::If(%2281) # torch/nn/functional.py:3157:9
                                    block0():
                                      %2283 : bool = aten::eq(%5, %2124) # torch/nn/functional.py:3157:30
                                      -> (%2283)
                                    block1():
                                      -> (%align_corners.9)
                                  %2284 : Tensor = prim::If(%2282) # torch/nn/functional.py:3157:4
                                    block0():
                                       = prim::RaiseException(%2118) # torch/nn/functional.py:3158:8
                                      -> (%2126)
                                    block1():
                                      %2285 : int = aten::dim(%x.7) # torch/nn/functional.py:3159:9
                                      %2286 : bool = aten::eq(%2285, %2111) # torch/nn/functional.py:3159:9
                                      %2287 : bool = prim::If(%2286) # torch/nn/functional.py:3159:9
                                        block0():
                                          %2288 : bool = aten::eq(%5, %2122) # torch/nn/functional.py:3159:30
                                          -> (%2288)
                                        block1():
                                          -> (%align_corners.9)
                                      %2289 : Tensor = prim::If(%2287) # torch/nn/functional.py:3159:4
                                        block0():
                                           = prim::RaiseException(%2118) # torch/nn/functional.py:3160:8
                                          -> (%2126)
                                        block1():
                                          %2290 : int = aten::dim(%x.7) # torch/nn/functional.py:3161:9
                                          %2291 : bool = aten::eq(%2290, %2111) # torch/nn/functional.py:3161:9
                                          %2292 : bool = prim::If(%2291) # torch/nn/functional.py:3161:9
                                            block0():
                                              %2293 : bool = aten::eq(%5, %2123) # torch/nn/functional.py:3161:30
                                              -> (%2293)
                                            block1():
                                              -> (%align_corners.9)
                                          %2294 : Tensor = prim::If(%2292) # torch/nn/functional.py:3161:4
                                            block0():
                                              %2295 : bool = aten::__isnot__(%align_corners.56, %2117) # torch/nn/functional.py:3162:15
                                              %align_corners.58 : bool = prim::If(%2295) # torch/nn/functional.py:3162:8
                                                block0():
                                                  %align_corners.31 : bool = prim::unchecked_cast(%align_corners.56)
                                                  -> (%align_corners.31)
                                                block1():
                                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3162:8
                                                  -> (%2128)
                                              %2298 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3163:83
                                              %2299 : float? = aten::__getitem__(%scale_factor_list, %2110) # torch/nn/functional.py:3163:91
                                              %2300 : Tensor = aten::upsample_bilinear2d(%x.7, %output_size.1, %align_corners.58, %2298, %2299) # torch/nn/functional.py:3163:15
                                              -> (%2300)
                                            block1():
                                              %2301 : int = aten::dim(%x.7) # torch/nn/functional.py:3164:9
                                              %2302 : bool = aten::eq(%2301, %2111) # torch/nn/functional.py:3164:9
                                              %2303 : bool = prim::If(%2302) # torch/nn/functional.py:3164:9
                                                block0():
                                                  %2304 : bool = aten::eq(%5, %2124) # torch/nn/functional.py:3164:30
                                                  -> (%2304)
                                                block1():
                                                  -> (%align_corners.9)
                                              %2305 : Tensor = prim::If(%2303) # torch/nn/functional.py:3164:4
                                                block0():
                                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3165:8
                                                  -> (%2126)
                                                block1():
                                                  %2306 : int = aten::dim(%x.7) # torch/nn/functional.py:3166:9
                                                  %2307 : bool = aten::eq(%2306, %2109) # torch/nn/functional.py:3166:9
                                                  %2308 : bool = prim::If(%2307) # torch/nn/functional.py:3166:9
                                                    block0():
                                                      %2309 : bool = aten::eq(%5, %2122) # torch/nn/functional.py:3166:30
                                                      -> (%2309)
                                                    block1():
                                                      -> (%align_corners.9)
                                                  %2310 : Tensor = prim::If(%2308) # torch/nn/functional.py:3166:4
                                                    block0():
                                                       = prim::RaiseException(%2118) # torch/nn/functional.py:3167:8
                                                      -> (%2126)
                                                    block1():
                                                      %2311 : int = aten::dim(%x.7) # torch/nn/functional.py:3168:9
                                                      %2312 : bool = aten::eq(%2311, %2109) # torch/nn/functional.py:3168:9
                                                      %2313 : bool = prim::If(%2312) # torch/nn/functional.py:3168:9
                                                        block0():
                                                          %2314 : bool = aten::eq(%5, %2123) # torch/nn/functional.py:3168:30
                                                          -> (%2314)
                                                        block1():
                                                          -> (%align_corners.9)
                                                      %2315 : Tensor = prim::If(%2313) # torch/nn/functional.py:3168:4
                                                        block0():
                                                           = prim::RaiseException(%2118) # torch/nn/functional.py:3169:8
                                                          -> (%2126)
                                                        block1():
                                                          %2316 : int = aten::dim(%x.7) # torch/nn/functional.py:3170:9
                                                          %2317 : bool = aten::eq(%2316, %2109) # torch/nn/functional.py:3170:9
                                                          %2318 : bool = prim::If(%2317) # torch/nn/functional.py:3170:9
                                                            block0():
                                                              %2319 : bool = aten::eq(%5, %2124) # torch/nn/functional.py:3170:30
                                                              -> (%2319)
                                                            block1():
                                                              -> (%align_corners.9)
                                                          %2320 : Tensor = prim::If(%2318) # torch/nn/functional.py:3170:4
                                                            block0():
                                                              %2321 : bool = aten::__isnot__(%align_corners.56, %2117) # torch/nn/functional.py:3171:15
                                                              %align_corners.59 : bool = prim::If(%2321) # torch/nn/functional.py:3171:8
                                                                block0():
                                                                  %align_corners.38 : bool = prim::unchecked_cast(%align_corners.56)
                                                                  -> (%align_corners.38)
                                                                block1():
                                                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3171:8
                                                                  -> (%2128)
                                                              %2324 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3172:84
                                                              %2325 : float? = aten::__getitem__(%scale_factor_list, %2110) # torch/nn/functional.py:3172:92
                                                              %2326 : float? = aten::__getitem__(%scale_factor_list, %2114) # torch/nn/functional.py:3172:100
                                                              %2327 : Tensor = aten::upsample_trilinear3d(%x.7, %output_size.1, %align_corners.59, %2324, %2325, %2326) # torch/nn/functional.py:3172:15
                                                              -> (%2327)
                                                            block1():
                                                              %2328 : int = aten::dim(%x.7) # torch/nn/functional.py:3173:9
                                                              %2329 : bool = aten::eq(%2328, %2111) # torch/nn/functional.py:3173:9
                                                              %2330 : bool = prim::If(%2329) # torch/nn/functional.py:3173:9
                                                                block0():
                                                                  %2331 : bool = aten::eq(%5, %2125) # torch/nn/functional.py:3173:30
                                                                  -> (%2331)
                                                                block1():
                                                                  -> (%align_corners.9)
                                                              %2332 : Tensor = prim::If(%2330) # torch/nn/functional.py:3173:4
                                                                block0():
                                                                  %2333 : bool = aten::__isnot__(%align_corners.56, %2117) # torch/nn/functional.py:3174:15
                                                                  %align_corners.60 : bool = prim::If(%2333) # torch/nn/functional.py:3174:8
                                                                    block0():
                                                                      %align_corners.45 : bool = prim::unchecked_cast(%align_corners.56)
                                                                      -> (%align_corners.45)
                                                                    block1():
                                                                       = prim::RaiseException(%2118) # torch/nn/functional.py:3174:8
                                                                      -> (%2128)
                                                                  %2336 : float? = aten::__getitem__(%scale_factor_list, %2112) # torch/nn/functional.py:3175:82
                                                                  %2337 : float? = aten::__getitem__(%scale_factor_list, %2110) # torch/nn/functional.py:3175:90
                                                                  %2338 : Tensor = aten::upsample_bicubic2d(%x.7, %output_size.1, %align_corners.60, %2336, %2337) # torch/nn/functional.py:3175:15
                                                                  -> (%2338)
                                                                block1():
                                                                   = prim::RaiseException(%2118) # torch/nn/functional.py:3177:8
                                                                  -> (%2126)
                                                              -> (%2332)
                                                          -> (%2320)
                                                      -> (%2315)
                                                  -> (%2310)
                                              -> (%2305)
                                          -> (%2294)
                                      -> (%2289)
                                  -> (%2284)
                              -> (%2279)
                          -> (%2269)
                      -> (%2259)
                  -> (%2249)
              -> (%2243)
          -> (%2234)
      -> (%2226)
   = aten::_set_item(%result.1, %6, %x.11) # torch/hub/pytorch_vision_master/torchvision/models/segmentation/_utils.py:26:8
  return (%result.1)
