graph(%self : __torch__.torchvision.models.googlenet.GoogLeNet,
      %x.148 : Tensor):
  %2 : int = prim::Constant[value=2]()
  %3 : str = prim::Constant[value="Scripted GoogleNet always returns GoogleNetOutputs Tuple"]() # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:209:30
  %4 : bool = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:206:22
  %5 : bool = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:206:40
  %15 : float = prim::Constant[value=0.20000000000000001]() # torch/nn/modules/dropout.py:58:32
  %16 : float = prim::Constant[value=0.69999999999999996]() # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:276:25
  %17 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %18 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %19 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %20 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %21 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %24 : float = prim::Constant[value=0.001]() # torch/nn/modules/batchnorm.py:136:77
  %25 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:188:29
  %aux1.1 : Tensor? = prim::Constant()
  %27 : int = prim::Constant[value=-1]()
  %28 : __torch__.torchvision.models.googlenet.BasicConv2d = prim::GetAttr[name="conv1"](%self)
  %29 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv"](%28)
  %30 : Tensor = prim::GetAttr[name="weight"](%29)
  %31 : Tensor? = prim::GetAttr[name="bias"](%29)
  %32 : int[] = prim::ListConstruct(%18, %18)
  %33 : int[] = prim::ListConstruct(%17, %17)
  %34 : int[] = prim::ListConstruct(%25, %25)
  %x.18 : Tensor = aten::conv2d(%x.148, %30, %31, %32, %33, %34, %25) # torch/nn/modules/conv.py:415:15
  %36 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%28)
  %37 : int = aten::dim(%x.18) # torch/nn/modules/batchnorm.py:276:11
  %38 : bool = aten::ne(%37, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%38) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %39 : bool = prim::GetAttr[name="training"](%36)
   = prim::If(%39) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %40 : Tensor = prim::GetAttr[name="num_batches_tracked"](%36)
      %41 : Tensor = aten::add(%40, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%36, %41)
      -> ()
    block1():
      -> ()
  %42 : bool = prim::GetAttr[name="training"](%36)
  %43 : Tensor = prim::GetAttr[name="running_mean"](%36)
  %44 : Tensor = prim::GetAttr[name="running_var"](%36)
  %45 : Tensor = prim::GetAttr[name="weight"](%36)
  %46 : Tensor = prim::GetAttr[name="bias"](%36)
   = prim::If(%42) # torch/nn/functional.py:2011:4
    block0():
      %47 : int[] = aten::size(%x.18) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%47, %20) # torch/nn/functional.py:1991:17
      %49 : int = aten::len(%47) # torch/nn/functional.py:1992:19
      %50 : int = aten::sub(%49, %18) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%50, %19, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %54 : int = aten::add(%i.7, %18) # torch/nn/functional.py:1993:27
          %55 : int = aten::__getitem__(%47, %54) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %55) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.27)
      %57 : bool = aten::eq(%size_prods.25, %25) # torch/nn/functional.py:1994:7
       = prim::If(%57) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.21 : Tensor = aten::batch_norm(%x.18, %45, %46, %43, %44, %42, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %x.27 : Tensor = aten::relu_(%x.21) # torch/nn/functional.py:1117:17
  %60 : int[] = prim::ListConstruct(%17, %17)
  %61 : int[] = prim::ListConstruct(%18, %18)
  %62 : int[] = prim::ListConstruct(%20, %20)
  %63 : int[] = prim::ListConstruct(%25, %25)
  %x.28 : Tensor = aten::max_pool2d(%x.27, %60, %61, %62, %63, %19) # torch/nn/functional.py:575:11
  %65 : __torch__.torchvision.models.googlenet.___torch_mangle_393.BasicConv2d = prim::GetAttr[name="conv2"](%self)
  %66 : __torch__.torch.nn.modules.conv.___torch_mangle_9.Conv2d = prim::GetAttr[name="conv"](%65)
  %67 : Tensor = prim::GetAttr[name="weight"](%66)
  %68 : Tensor? = prim::GetAttr[name="bias"](%66)
  %69 : int[] = prim::ListConstruct(%25, %25)
  %70 : int[] = prim::ListConstruct(%20, %20)
  %71 : int[] = prim::ListConstruct(%25, %25)
  %x.25 : Tensor = aten::conv2d(%x.28, %67, %68, %69, %70, %71, %25) # torch/nn/modules/conv.py:415:15
  %73 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%65)
  %74 : int = aten::dim(%x.25) # torch/nn/modules/batchnorm.py:276:11
  %75 : bool = aten::ne(%74, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%75) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %76 : bool = prim::GetAttr[name="training"](%73)
   = prim::If(%76) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %77 : Tensor = prim::GetAttr[name="num_batches_tracked"](%73)
      %78 : Tensor = aten::add(%77, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%73, %78)
      -> ()
    block1():
      -> ()
  %79 : bool = prim::GetAttr[name="training"](%73)
  %80 : Tensor = prim::GetAttr[name="running_mean"](%73)
  %81 : Tensor = prim::GetAttr[name="running_var"](%73)
  %82 : Tensor = prim::GetAttr[name="weight"](%73)
  %83 : Tensor = prim::GetAttr[name="bias"](%73)
   = prim::If(%79) # torch/nn/functional.py:2011:4
    block0():
      %84 : int[] = aten::size(%x.25) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%84, %20) # torch/nn/functional.py:1991:17
      %86 : int = aten::len(%84) # torch/nn/functional.py:1992:19
      %87 : int = aten::sub(%86, %18) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%87, %19, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %91 : int = aten::add(%i.8, %18) # torch/nn/functional.py:1993:27
          %92 : int = aten::__getitem__(%84, %91) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %92) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.31)
      %94 : bool = aten::eq(%size_prods.29, %25) # torch/nn/functional.py:1994:7
       = prim::If(%94) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.32 : Tensor = aten::batch_norm(%x.25, %82, %83, %80, %81, %79, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %x.16 : Tensor = aten::relu_(%x.32) # torch/nn/functional.py:1117:17
  %97 : __torch__.torchvision.models.googlenet.___torch_mangle_396.BasicConv2d = prim::GetAttr[name="conv3"](%self)
  %98 : __torch__.torch.nn.modules.conv.___torch_mangle_394.Conv2d = prim::GetAttr[name="conv"](%97)
  %99 : Tensor = prim::GetAttr[name="weight"](%98)
  %100 : Tensor? = prim::GetAttr[name="bias"](%98)
  %101 : int[] = prim::ListConstruct(%25, %25)
  %102 : int[] = prim::ListConstruct(%25, %25)
  %103 : int[] = prim::ListConstruct(%25, %25)
  %x.34 : Tensor = aten::conv2d(%x.16, %99, %100, %101, %102, %103, %25) # torch/nn/modules/conv.py:415:15
  %105 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%97)
  %106 : int = aten::dim(%x.34) # torch/nn/modules/batchnorm.py:276:11
  %107 : bool = aten::ne(%106, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%107) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %108 : bool = prim::GetAttr[name="training"](%105)
   = prim::If(%108) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %109 : Tensor = prim::GetAttr[name="num_batches_tracked"](%105)
      %110 : Tensor = aten::add(%109, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%105, %110)
      -> ()
    block1():
      -> ()
  %111 : bool = prim::GetAttr[name="training"](%105)
  %112 : Tensor = prim::GetAttr[name="running_mean"](%105)
  %113 : Tensor = prim::GetAttr[name="running_var"](%105)
  %114 : Tensor = prim::GetAttr[name="weight"](%105)
  %115 : Tensor = prim::GetAttr[name="bias"](%105)
   = prim::If(%111) # torch/nn/functional.py:2011:4
    block0():
      %116 : int[] = aten::size(%x.34) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%116, %20) # torch/nn/functional.py:1991:17
      %118 : int = aten::len(%116) # torch/nn/functional.py:1992:19
      %119 : int = aten::sub(%118, %18) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%119, %19, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %123 : int = aten::add(%i.9, %18) # torch/nn/functional.py:1993:27
          %124 : int = aten::__getitem__(%116, %123) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %124) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.35)
      %126 : bool = aten::eq(%size_prods.33, %25) # torch/nn/functional.py:1994:7
       = prim::If(%126) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.36 : Tensor = aten::batch_norm(%x.34, %114, %115, %112, %113, %111, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %x.20 : Tensor = aten::relu_(%x.36) # torch/nn/functional.py:1117:17
  %129 : int[] = prim::ListConstruct(%17, %17)
  %130 : int[] = prim::ListConstruct(%18, %18)
  %131 : int[] = prim::ListConstruct(%20, %20)
  %132 : int[] = prim::ListConstruct(%25, %25)
  %x.23 : Tensor = aten::max_pool2d(%x.20, %129, %130, %131, %132, %19) # torch/nn/functional.py:575:11
  %134 : __torch__.torchvision.models.googlenet.Inception = prim::GetAttr[name="inception3a"](%self)
  %135 : __torch__.torchvision.models.googlenet.___torch_mangle_398.BasicConv2d = prim::GetAttr[name="branch1"](%134)
  %136 : __torch__.torch.nn.modules.conv.___torch_mangle_397.Conv2d = prim::GetAttr[name="conv"](%135)
  %137 : Tensor = prim::GetAttr[name="weight"](%136)
  %138 : Tensor? = prim::GetAttr[name="bias"](%136)
  %139 : int[] = prim::ListConstruct(%25, %25)
  %140 : int[] = prim::ListConstruct(%20, %20)
  %141 : int[] = prim::ListConstruct(%25, %25)
  %x.38 : Tensor = aten::conv2d(%x.23, %137, %138, %139, %140, %141, %25) # torch/nn/modules/conv.py:415:15
  %143 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%135)
  %144 : int = aten::dim(%x.38) # torch/nn/modules/batchnorm.py:276:11
  %145 : bool = aten::ne(%144, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%145) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %146 : bool = prim::GetAttr[name="training"](%143)
   = prim::If(%146) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %147 : Tensor = prim::GetAttr[name="num_batches_tracked"](%143)
      %148 : Tensor = aten::add(%147, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%143, %148)
      -> ()
    block1():
      -> ()
  %149 : bool = prim::GetAttr[name="training"](%143)
  %150 : Tensor = prim::GetAttr[name="running_mean"](%143)
  %151 : Tensor = prim::GetAttr[name="running_var"](%143)
  %152 : Tensor = prim::GetAttr[name="weight"](%143)
  %153 : Tensor = prim::GetAttr[name="bias"](%143)
   = prim::If(%149) # torch/nn/functional.py:2011:4
    block0():
      %154 : int[] = aten::size(%x.38) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%154, %20) # torch/nn/functional.py:1991:17
      %156 : int = aten::len(%154) # torch/nn/functional.py:1992:19
      %157 : int = aten::sub(%156, %18) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%157, %19, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %161 : int = aten::add(%i.10, %18) # torch/nn/functional.py:1993:27
          %162 : int = aten::__getitem__(%154, %161) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %162) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.39)
      %164 : bool = aten::eq(%size_prods.37, %25) # torch/nn/functional.py:1994:7
       = prim::If(%164) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.40 : Tensor = aten::batch_norm(%x.38, %152, %153, %150, %151, %149, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.2 : Tensor = aten::relu_(%x.40) # torch/nn/functional.py:1117:17
  %167 : __torch__.torch.nn.modules.container.___torch_mangle_405.Sequential = prim::GetAttr[name="branch2"](%134)
  %168 : __torch__.torchvision.models.googlenet.___torch_mangle_401.BasicConv2d = prim::GetAttr[name="0"](%167)
  %169 : __torch__.torchvision.models.googlenet.___torch_mangle_404.BasicConv2d = prim::GetAttr[name="1"](%167)
  %170 : __torch__.torch.nn.modules.conv.___torch_mangle_399.Conv2d = prim::GetAttr[name="conv"](%168)
  %171 : Tensor = prim::GetAttr[name="weight"](%170)
  %172 : Tensor? = prim::GetAttr[name="bias"](%170)
  %173 : int[] = prim::ListConstruct(%25, %25)
  %174 : int[] = prim::ListConstruct(%20, %20)
  %175 : int[] = prim::ListConstruct(%25, %25)
  %x.42 : Tensor = aten::conv2d(%x.23, %171, %172, %173, %174, %175, %25) # torch/nn/modules/conv.py:415:15
  %177 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%168)
  %178 : int = aten::dim(%x.42) # torch/nn/modules/batchnorm.py:276:11
  %179 : bool = aten::ne(%178, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%179) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %180 : bool = prim::GetAttr[name="training"](%177)
   = prim::If(%180) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %181 : Tensor = prim::GetAttr[name="num_batches_tracked"](%177)
      %182 : Tensor = aten::add(%181, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%177, %182)
      -> ()
    block1():
      -> ()
  %183 : bool = prim::GetAttr[name="training"](%177)
  %184 : Tensor = prim::GetAttr[name="running_mean"](%177)
  %185 : Tensor = prim::GetAttr[name="running_var"](%177)
  %186 : Tensor = prim::GetAttr[name="weight"](%177)
  %187 : Tensor = prim::GetAttr[name="bias"](%177)
   = prim::If(%183) # torch/nn/functional.py:2011:4
    block0():
      %188 : int[] = aten::size(%x.42) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%188, %20) # torch/nn/functional.py:1991:17
      %190 : int = aten::len(%188) # torch/nn/functional.py:1992:19
      %191 : int = aten::sub(%190, %18) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%191, %19, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %195 : int = aten::add(%i.11, %18) # torch/nn/functional.py:1993:27
          %196 : int = aten::__getitem__(%188, %195) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %196) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.43)
      %198 : bool = aten::eq(%size_prods.41, %25) # torch/nn/functional.py:1994:7
       = prim::If(%198) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.44 : Tensor = aten::batch_norm(%x.42, %186, %187, %184, %185, %183, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.5 : Tensor = aten::relu_(%x.44) # torch/nn/functional.py:1117:17
  %201 : __torch__.torch.nn.modules.conv.___torch_mangle_402.Conv2d = prim::GetAttr[name="conv"](%169)
  %202 : Tensor = prim::GetAttr[name="weight"](%201)
  %203 : Tensor? = prim::GetAttr[name="bias"](%201)
  %204 : int[] = prim::ListConstruct(%25, %25)
  %205 : int[] = prim::ListConstruct(%25, %25)
  %206 : int[] = prim::ListConstruct(%25, %25)
  %x.45 : Tensor = aten::conv2d(%input.5, %202, %203, %204, %205, %206, %25) # torch/nn/modules/conv.py:415:15
  %208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%169)
  %209 : int = aten::dim(%x.45) # torch/nn/modules/batchnorm.py:276:11
  %210 : bool = aten::ne(%209, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%210) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %211 : bool = prim::GetAttr[name="training"](%208)
   = prim::If(%211) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %212 : Tensor = prim::GetAttr[name="num_batches_tracked"](%208)
      %213 : Tensor = aten::add(%212, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%208, %213)
      -> ()
    block1():
      -> ()
  %214 : bool = prim::GetAttr[name="training"](%208)
  %215 : Tensor = prim::GetAttr[name="running_mean"](%208)
  %216 : Tensor = prim::GetAttr[name="running_var"](%208)
  %217 : Tensor = prim::GetAttr[name="weight"](%208)
  %218 : Tensor = prim::GetAttr[name="bias"](%208)
   = prim::If(%214) # torch/nn/functional.py:2011:4
    block0():
      %219 : int[] = aten::size(%x.45) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%219, %20) # torch/nn/functional.py:1991:17
      %221 : int = aten::len(%219) # torch/nn/functional.py:1992:19
      %222 : int = aten::sub(%221, %18) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%222, %19, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %226 : int = aten::add(%i.12, %18) # torch/nn/functional.py:1993:27
          %227 : int = aten::__getitem__(%219, %226) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %227) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.47)
      %229 : bool = aten::eq(%size_prods.45, %25) # torch/nn/functional.py:1994:7
       = prim::If(%229) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.46 : Tensor = aten::batch_norm(%x.45, %217, %218, %215, %216, %214, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.2 : Tensor = aten::relu_(%x.46) # torch/nn/functional.py:1117:17
  %232 : __torch__.torch.nn.modules.container.___torch_mangle_412.Sequential = prim::GetAttr[name="branch3"](%134)
  %233 : __torch__.torchvision.models.googlenet.___torch_mangle_408.BasicConv2d = prim::GetAttr[name="0"](%232)
  %234 : __torch__.torchvision.models.googlenet.___torch_mangle_411.BasicConv2d = prim::GetAttr[name="1"](%232)
  %235 : __torch__.torch.nn.modules.conv.___torch_mangle_406.Conv2d = prim::GetAttr[name="conv"](%233)
  %236 : Tensor = prim::GetAttr[name="weight"](%235)
  %237 : Tensor? = prim::GetAttr[name="bias"](%235)
  %238 : int[] = prim::ListConstruct(%25, %25)
  %239 : int[] = prim::ListConstruct(%20, %20)
  %240 : int[] = prim::ListConstruct(%25, %25)
  %x.47 : Tensor = aten::conv2d(%x.23, %236, %237, %238, %239, %240, %25) # torch/nn/modules/conv.py:415:15
  %242 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_407.BatchNorm2d = prim::GetAttr[name="bn"](%233)
  %243 : int = aten::dim(%x.47) # torch/nn/modules/batchnorm.py:276:11
  %244 : bool = aten::ne(%243, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%244) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %245 : bool = prim::GetAttr[name="training"](%242)
   = prim::If(%245) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %246 : Tensor = prim::GetAttr[name="num_batches_tracked"](%242)
      %247 : Tensor = aten::add(%246, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%242, %247)
      -> ()
    block1():
      -> ()
  %248 : bool = prim::GetAttr[name="training"](%242)
  %249 : Tensor = prim::GetAttr[name="running_mean"](%242)
  %250 : Tensor = prim::GetAttr[name="running_var"](%242)
  %251 : Tensor = prim::GetAttr[name="weight"](%242)
  %252 : Tensor = prim::GetAttr[name="bias"](%242)
   = prim::If(%248) # torch/nn/functional.py:2011:4
    block0():
      %253 : int[] = aten::size(%x.47) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%253, %20) # torch/nn/functional.py:1991:17
      %255 : int = aten::len(%253) # torch/nn/functional.py:1992:19
      %256 : int = aten::sub(%255, %18) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%256, %19, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %260 : int = aten::add(%i.13, %18) # torch/nn/functional.py:1993:27
          %261 : int = aten::__getitem__(%253, %260) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %261) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.51)
      %263 : bool = aten::eq(%size_prods.49, %25) # torch/nn/functional.py:1994:7
       = prim::If(%263) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.48 : Tensor = aten::batch_norm(%x.47, %251, %252, %249, %250, %248, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.7 : Tensor = aten::relu_(%x.48) # torch/nn/functional.py:1117:17
  %266 : __torch__.torch.nn.modules.conv.___torch_mangle_409.Conv2d = prim::GetAttr[name="conv"](%234)
  %267 : Tensor = prim::GetAttr[name="weight"](%266)
  %268 : Tensor? = prim::GetAttr[name="bias"](%266)
  %269 : int[] = prim::ListConstruct(%25, %25)
  %270 : int[] = prim::ListConstruct(%25, %25)
  %271 : int[] = prim::ListConstruct(%25, %25)
  %x.49 : Tensor = aten::conv2d(%input.7, %267, %268, %269, %270, %271, %25) # torch/nn/modules/conv.py:415:15
  %273 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%234)
  %274 : int = aten::dim(%x.49) # torch/nn/modules/batchnorm.py:276:11
  %275 : bool = aten::ne(%274, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%275) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %276 : bool = prim::GetAttr[name="training"](%273)
   = prim::If(%276) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %277 : Tensor = prim::GetAttr[name="num_batches_tracked"](%273)
      %278 : Tensor = aten::add(%277, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%273, %278)
      -> ()
    block1():
      -> ()
  %279 : bool = prim::GetAttr[name="training"](%273)
  %280 : Tensor = prim::GetAttr[name="running_mean"](%273)
  %281 : Tensor = prim::GetAttr[name="running_var"](%273)
  %282 : Tensor = prim::GetAttr[name="weight"](%273)
  %283 : Tensor = prim::GetAttr[name="bias"](%273)
   = prim::If(%279) # torch/nn/functional.py:2011:4
    block0():
      %284 : int[] = aten::size(%x.49) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%284, %20) # torch/nn/functional.py:1991:17
      %286 : int = aten::len(%284) # torch/nn/functional.py:1992:19
      %287 : int = aten::sub(%286, %18) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%287, %19, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %291 : int = aten::add(%i.14, %18) # torch/nn/functional.py:1993:27
          %292 : int = aten::__getitem__(%284, %291) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %292) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.55)
      %294 : bool = aten::eq(%size_prods.53, %25) # torch/nn/functional.py:1994:7
       = prim::If(%294) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.50 : Tensor = aten::batch_norm(%x.49, %282, %283, %280, %281, %279, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.2 : Tensor = aten::relu_(%x.50) # torch/nn/functional.py:1117:17
  %297 : __torch__.torch.nn.modules.container.___torch_mangle_416.Sequential = prim::GetAttr[name="branch4"](%134)
  %298 : __torch__.torchvision.models.googlenet.___torch_mangle_415.BasicConv2d = prim::GetAttr[name="1"](%297)
  %299 : int[] = prim::ListConstruct(%17, %17)
  %300 : int[] = prim::ListConstruct(%25, %25)
  %301 : int[] = prim::ListConstruct(%25, %25)
  %302 : int[] = prim::ListConstruct(%25, %25)
  %input.8 : Tensor = aten::max_pool2d(%x.23, %299, %300, %301, %302, %19) # torch/nn/functional.py:575:11
  %304 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="conv"](%298)
  %305 : Tensor = prim::GetAttr[name="weight"](%304)
  %306 : Tensor? = prim::GetAttr[name="bias"](%304)
  %307 : int[] = prim::ListConstruct(%25, %25)
  %308 : int[] = prim::ListConstruct(%20, %20)
  %309 : int[] = prim::ListConstruct(%25, %25)
  %x.51 : Tensor = aten::conv2d(%input.8, %305, %306, %307, %308, %309, %25) # torch/nn/modules/conv.py:415:15
  %311 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%298)
  %312 : int = aten::dim(%x.51) # torch/nn/modules/batchnorm.py:276:11
  %313 : bool = aten::ne(%312, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%313) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %314 : bool = prim::GetAttr[name="training"](%311)
   = prim::If(%314) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%311)
      %316 : Tensor = aten::add(%315, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%311, %316)
      -> ()
    block1():
      -> ()
  %317 : bool = prim::GetAttr[name="training"](%311)
  %318 : Tensor = prim::GetAttr[name="running_mean"](%311)
  %319 : Tensor = prim::GetAttr[name="running_var"](%311)
  %320 : Tensor = prim::GetAttr[name="weight"](%311)
  %321 : Tensor = prim::GetAttr[name="bias"](%311)
   = prim::If(%317) # torch/nn/functional.py:2011:4
    block0():
      %322 : int[] = aten::size(%x.51) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%322, %20) # torch/nn/functional.py:1991:17
      %324 : int = aten::len(%322) # torch/nn/functional.py:1992:19
      %325 : int = aten::sub(%324, %18) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%325, %19, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %329 : int = aten::add(%i.15, %18) # torch/nn/functional.py:1993:27
          %330 : int = aten::__getitem__(%322, %329) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %330) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.59)
      %332 : bool = aten::eq(%size_prods.57, %25) # torch/nn/functional.py:1994:7
       = prim::If(%332) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.52 : Tensor = aten::batch_norm(%x.51, %320, %321, %318, %319, %317, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.2 : Tensor = aten::relu_(%x.52) # torch/nn/functional.py:1117:17
  %outputs.3 : Tensor[] = prim::ListConstruct(%branch1.2, %branch2.2, %branch3.2, %branch4.2)
  %x.30 : Tensor = aten::cat(%outputs.3, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %337 : __torch__.torchvision.models.googlenet.___torch_mangle_428.Inception = prim::GetAttr[name="inception3b"](%self)
  %338 : __torch__.torchvision.models.googlenet.___torch_mangle_417.BasicConv2d = prim::GetAttr[name="branch1"](%337)
  %339 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv"](%338)
  %340 : Tensor = prim::GetAttr[name="weight"](%339)
  %341 : Tensor? = prim::GetAttr[name="bias"](%339)
  %342 : int[] = prim::ListConstruct(%25, %25)
  %343 : int[] = prim::ListConstruct(%20, %20)
  %344 : int[] = prim::ListConstruct(%25, %25)
  %x.53 : Tensor = aten::conv2d(%x.30, %340, %341, %342, %343, %344, %25) # torch/nn/modules/conv.py:415:15
  %346 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%338)
  %347 : int = aten::dim(%x.53) # torch/nn/modules/batchnorm.py:276:11
  %348 : bool = aten::ne(%347, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%348) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %349 : bool = prim::GetAttr[name="training"](%346)
   = prim::If(%349) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %350 : Tensor = prim::GetAttr[name="num_batches_tracked"](%346)
      %351 : Tensor = aten::add(%350, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%346, %351)
      -> ()
    block1():
      -> ()
  %352 : bool = prim::GetAttr[name="training"](%346)
  %353 : Tensor = prim::GetAttr[name="running_mean"](%346)
  %354 : Tensor = prim::GetAttr[name="running_var"](%346)
  %355 : Tensor = prim::GetAttr[name="weight"](%346)
  %356 : Tensor = prim::GetAttr[name="bias"](%346)
   = prim::If(%352) # torch/nn/functional.py:2011:4
    block0():
      %357 : int[] = aten::size(%x.53) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%357, %20) # torch/nn/functional.py:1991:17
      %359 : int = aten::len(%357) # torch/nn/functional.py:1992:19
      %360 : int = aten::sub(%359, %18) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%360, %19, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %364 : int = aten::add(%i.16, %18) # torch/nn/functional.py:1993:27
          %365 : int = aten::__getitem__(%357, %364) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %365) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.63)
      %367 : bool = aten::eq(%size_prods.61, %25) # torch/nn/functional.py:1994:7
       = prim::If(%367) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.54 : Tensor = aten::batch_norm(%x.53, %355, %356, %353, %354, %352, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.3 : Tensor = aten::relu_(%x.54) # torch/nn/functional.py:1117:17
  %370 : __torch__.torch.nn.modules.container.___torch_mangle_420.Sequential = prim::GetAttr[name="branch2"](%337)
  %371 : __torch__.torchvision.models.googlenet.___torch_mangle_417.BasicConv2d = prim::GetAttr[name="0"](%370)
  %372 : __torch__.torchvision.models.googlenet.___torch_mangle_419.BasicConv2d = prim::GetAttr[name="1"](%370)
  %373 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv"](%371)
  %374 : Tensor = prim::GetAttr[name="weight"](%373)
  %375 : Tensor? = prim::GetAttr[name="bias"](%373)
  %376 : int[] = prim::ListConstruct(%25, %25)
  %377 : int[] = prim::ListConstruct(%20, %20)
  %378 : int[] = prim::ListConstruct(%25, %25)
  %x.55 : Tensor = aten::conv2d(%x.30, %374, %375, %376, %377, %378, %25) # torch/nn/modules/conv.py:415:15
  %380 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%371)
  %381 : int = aten::dim(%x.55) # torch/nn/modules/batchnorm.py:276:11
  %382 : bool = aten::ne(%381, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%382) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %383 : bool = prim::GetAttr[name="training"](%380)
   = prim::If(%383) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %384 : Tensor = prim::GetAttr[name="num_batches_tracked"](%380)
      %385 : Tensor = aten::add(%384, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%380, %385)
      -> ()
    block1():
      -> ()
  %386 : bool = prim::GetAttr[name="training"](%380)
  %387 : Tensor = prim::GetAttr[name="running_mean"](%380)
  %388 : Tensor = prim::GetAttr[name="running_var"](%380)
  %389 : Tensor = prim::GetAttr[name="weight"](%380)
  %390 : Tensor = prim::GetAttr[name="bias"](%380)
   = prim::If(%386) # torch/nn/functional.py:2011:4
    block0():
      %391 : int[] = aten::size(%x.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%391, %20) # torch/nn/functional.py:1991:17
      %393 : int = aten::len(%391) # torch/nn/functional.py:1992:19
      %394 : int = aten::sub(%393, %18) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%394, %19, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %398 : int = aten::add(%i.17, %18) # torch/nn/functional.py:1993:27
          %399 : int = aten::__getitem__(%391, %398) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %399) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.67)
      %401 : bool = aten::eq(%size_prods.65, %25) # torch/nn/functional.py:1994:7
       = prim::If(%401) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.56 : Tensor = aten::batch_norm(%x.55, %389, %390, %387, %388, %386, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.9 : Tensor = aten::relu_(%x.56) # torch/nn/functional.py:1117:17
  %404 : __torch__.torch.nn.modules.conv.___torch_mangle_418.Conv2d = prim::GetAttr[name="conv"](%372)
  %405 : Tensor = prim::GetAttr[name="weight"](%404)
  %406 : Tensor? = prim::GetAttr[name="bias"](%404)
  %407 : int[] = prim::ListConstruct(%25, %25)
  %408 : int[] = prim::ListConstruct(%25, %25)
  %409 : int[] = prim::ListConstruct(%25, %25)
  %x.57 : Tensor = aten::conv2d(%input.9, %405, %406, %407, %408, %409, %25) # torch/nn/modules/conv.py:415:15
  %411 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%372)
  %412 : int = aten::dim(%x.57) # torch/nn/modules/batchnorm.py:276:11
  %413 : bool = aten::ne(%412, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%413) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %414 : bool = prim::GetAttr[name="training"](%411)
   = prim::If(%414) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %415 : Tensor = prim::GetAttr[name="num_batches_tracked"](%411)
      %416 : Tensor = aten::add(%415, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%411, %416)
      -> ()
    block1():
      -> ()
  %417 : bool = prim::GetAttr[name="training"](%411)
  %418 : Tensor = prim::GetAttr[name="running_mean"](%411)
  %419 : Tensor = prim::GetAttr[name="running_var"](%411)
  %420 : Tensor = prim::GetAttr[name="weight"](%411)
  %421 : Tensor = prim::GetAttr[name="bias"](%411)
   = prim::If(%417) # torch/nn/functional.py:2011:4
    block0():
      %422 : int[] = aten::size(%x.57) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%422, %20) # torch/nn/functional.py:1991:17
      %424 : int = aten::len(%422) # torch/nn/functional.py:1992:19
      %425 : int = aten::sub(%424, %18) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%425, %19, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %429 : int = aten::add(%i.18, %18) # torch/nn/functional.py:1993:27
          %430 : int = aten::__getitem__(%422, %429) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %430) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.71)
      %432 : bool = aten::eq(%size_prods.69, %25) # torch/nn/functional.py:1994:7
       = prim::If(%432) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.58 : Tensor = aten::batch_norm(%x.57, %420, %421, %418, %419, %417, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.3 : Tensor = aten::relu_(%x.58) # torch/nn/functional.py:1117:17
  %435 : __torch__.torch.nn.modules.container.___torch_mangle_425.Sequential = prim::GetAttr[name="branch3"](%337)
  %436 : __torch__.torchvision.models.googlenet.___torch_mangle_422.BasicConv2d = prim::GetAttr[name="0"](%435)
  %437 : __torch__.torchvision.models.googlenet.___torch_mangle_424.BasicConv2d = prim::GetAttr[name="1"](%435)
  %438 : __torch__.torch.nn.modules.conv.___torch_mangle_421.Conv2d = prim::GetAttr[name="conv"](%436)
  %439 : Tensor = prim::GetAttr[name="weight"](%438)
  %440 : Tensor? = prim::GetAttr[name="bias"](%438)
  %441 : int[] = prim::ListConstruct(%25, %25)
  %442 : int[] = prim::ListConstruct(%20, %20)
  %443 : int[] = prim::ListConstruct(%25, %25)
  %x.59 : Tensor = aten::conv2d(%x.30, %439, %440, %441, %442, %443, %25) # torch/nn/modules/conv.py:415:15
  %445 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%436)
  %446 : int = aten::dim(%x.59) # torch/nn/modules/batchnorm.py:276:11
  %447 : bool = aten::ne(%446, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%447) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %448 : bool = prim::GetAttr[name="training"](%445)
   = prim::If(%448) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %449 : Tensor = prim::GetAttr[name="num_batches_tracked"](%445)
      %450 : Tensor = aten::add(%449, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%445, %450)
      -> ()
    block1():
      -> ()
  %451 : bool = prim::GetAttr[name="training"](%445)
  %452 : Tensor = prim::GetAttr[name="running_mean"](%445)
  %453 : Tensor = prim::GetAttr[name="running_var"](%445)
  %454 : Tensor = prim::GetAttr[name="weight"](%445)
  %455 : Tensor = prim::GetAttr[name="bias"](%445)
   = prim::If(%451) # torch/nn/functional.py:2011:4
    block0():
      %456 : int[] = aten::size(%x.59) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%456, %20) # torch/nn/functional.py:1991:17
      %458 : int = aten::len(%456) # torch/nn/functional.py:1992:19
      %459 : int = aten::sub(%458, %18) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%459, %19, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %463 : int = aten::add(%i.19, %18) # torch/nn/functional.py:1993:27
          %464 : int = aten::__getitem__(%456, %463) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %464) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.75)
      %466 : bool = aten::eq(%size_prods.73, %25) # torch/nn/functional.py:1994:7
       = prim::If(%466) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.60 : Tensor = aten::batch_norm(%x.59, %454, %455, %452, %453, %451, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.10 : Tensor = aten::relu_(%x.60) # torch/nn/functional.py:1117:17
  %469 : __torch__.torch.nn.modules.conv.___torch_mangle_423.Conv2d = prim::GetAttr[name="conv"](%437)
  %470 : Tensor = prim::GetAttr[name="weight"](%469)
  %471 : Tensor? = prim::GetAttr[name="bias"](%469)
  %472 : int[] = prim::ListConstruct(%25, %25)
  %473 : int[] = prim::ListConstruct(%25, %25)
  %474 : int[] = prim::ListConstruct(%25, %25)
  %x.61 : Tensor = aten::conv2d(%input.10, %470, %471, %472, %473, %474, %25) # torch/nn/modules/conv.py:415:15
  %476 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%437)
  %477 : int = aten::dim(%x.61) # torch/nn/modules/batchnorm.py:276:11
  %478 : bool = aten::ne(%477, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%478) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %479 : bool = prim::GetAttr[name="training"](%476)
   = prim::If(%479) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %480 : Tensor = prim::GetAttr[name="num_batches_tracked"](%476)
      %481 : Tensor = aten::add(%480, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%476, %481)
      -> ()
    block1():
      -> ()
  %482 : bool = prim::GetAttr[name="training"](%476)
  %483 : Tensor = prim::GetAttr[name="running_mean"](%476)
  %484 : Tensor = prim::GetAttr[name="running_var"](%476)
  %485 : Tensor = prim::GetAttr[name="weight"](%476)
  %486 : Tensor = prim::GetAttr[name="bias"](%476)
   = prim::If(%482) # torch/nn/functional.py:2011:4
    block0():
      %487 : int[] = aten::size(%x.61) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%487, %20) # torch/nn/functional.py:1991:17
      %489 : int = aten::len(%487) # torch/nn/functional.py:1992:19
      %490 : int = aten::sub(%489, %18) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%490, %19, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %494 : int = aten::add(%i.20, %18) # torch/nn/functional.py:1993:27
          %495 : int = aten::__getitem__(%487, %494) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %495) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.79)
      %497 : bool = aten::eq(%size_prods.77, %25) # torch/nn/functional.py:1994:7
       = prim::If(%497) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.62 : Tensor = aten::batch_norm(%x.61, %485, %486, %483, %484, %482, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.3 : Tensor = aten::relu_(%x.62) # torch/nn/functional.py:1117:17
  %500 : __torch__.torch.nn.modules.container.___torch_mangle_427.Sequential = prim::GetAttr[name="branch4"](%337)
  %501 : __torch__.torchvision.models.googlenet.___torch_mangle_426.BasicConv2d = prim::GetAttr[name="1"](%500)
  %502 : int[] = prim::ListConstruct(%17, %17)
  %503 : int[] = prim::ListConstruct(%25, %25)
  %504 : int[] = prim::ListConstruct(%25, %25)
  %505 : int[] = prim::ListConstruct(%25, %25)
  %input.11 : Tensor = aten::max_pool2d(%x.30, %502, %503, %504, %505, %19) # torch/nn/functional.py:575:11
  %507 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv"](%501)
  %508 : Tensor = prim::GetAttr[name="weight"](%507)
  %509 : Tensor? = prim::GetAttr[name="bias"](%507)
  %510 : int[] = prim::ListConstruct(%25, %25)
  %511 : int[] = prim::ListConstruct(%20, %20)
  %512 : int[] = prim::ListConstruct(%25, %25)
  %x.63 : Tensor = aten::conv2d(%input.11, %508, %509, %510, %511, %512, %25) # torch/nn/modules/conv.py:415:15
  %514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%501)
  %515 : int = aten::dim(%x.63) # torch/nn/modules/batchnorm.py:276:11
  %516 : bool = aten::ne(%515, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%516) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %517 : bool = prim::GetAttr[name="training"](%514)
   = prim::If(%517) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %518 : Tensor = prim::GetAttr[name="num_batches_tracked"](%514)
      %519 : Tensor = aten::add(%518, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%514, %519)
      -> ()
    block1():
      -> ()
  %520 : bool = prim::GetAttr[name="training"](%514)
  %521 : Tensor = prim::GetAttr[name="running_mean"](%514)
  %522 : Tensor = prim::GetAttr[name="running_var"](%514)
  %523 : Tensor = prim::GetAttr[name="weight"](%514)
  %524 : Tensor = prim::GetAttr[name="bias"](%514)
   = prim::If(%520) # torch/nn/functional.py:2011:4
    block0():
      %525 : int[] = aten::size(%x.63) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%525, %20) # torch/nn/functional.py:1991:17
      %527 : int = aten::len(%525) # torch/nn/functional.py:1992:19
      %528 : int = aten::sub(%527, %18) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%528, %19, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %532 : int = aten::add(%i.21, %18) # torch/nn/functional.py:1993:27
          %533 : int = aten::__getitem__(%525, %532) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %533) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.83)
      %535 : bool = aten::eq(%size_prods.81, %25) # torch/nn/functional.py:1994:7
       = prim::If(%535) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.64 : Tensor = aten::batch_norm(%x.63, %523, %524, %521, %522, %520, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.3 : Tensor = aten::relu_(%x.64) # torch/nn/functional.py:1117:17
  %outputs.4 : Tensor[] = prim::ListConstruct(%branch1.3, %branch2.3, %branch3.3, %branch4.3)
  %x.15 : Tensor = aten::cat(%outputs.4, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %540 : int[] = prim::ListConstruct(%17, %17)
  %541 : int[] = prim::ListConstruct(%18, %18)
  %542 : int[] = prim::ListConstruct(%20, %20)
  %543 : int[] = prim::ListConstruct(%25, %25)
  %x.17 : Tensor = aten::max_pool2d(%x.15, %540, %541, %542, %543, %19) # torch/nn/functional.py:575:11
  %545 : __torch__.torchvision.models.googlenet.___torch_mangle_445.Inception = prim::GetAttr[name="inception4a"](%self)
  %546 : __torch__.torchvision.models.googlenet.___torch_mangle_429.BasicConv2d = prim::GetAttr[name="branch1"](%545)
  %547 : __torch__.torch.nn.modules.conv.___torch_mangle_186.Conv2d = prim::GetAttr[name="conv"](%546)
  %548 : Tensor = prim::GetAttr[name="weight"](%547)
  %549 : Tensor? = prim::GetAttr[name="bias"](%547)
  %550 : int[] = prim::ListConstruct(%25, %25)
  %551 : int[] = prim::ListConstruct(%20, %20)
  %552 : int[] = prim::ListConstruct(%25, %25)
  %x.69 : Tensor = aten::conv2d(%x.17, %548, %549, %550, %551, %552, %25) # torch/nn/modules/conv.py:415:15
  %554 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%546)
  %555 : int = aten::dim(%x.69) # torch/nn/modules/batchnorm.py:276:11
  %556 : bool = aten::ne(%555, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%556) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %557 : bool = prim::GetAttr[name="training"](%554)
   = prim::If(%557) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %558 : Tensor = prim::GetAttr[name="num_batches_tracked"](%554)
      %559 : Tensor = aten::add(%558, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%554, %559)
      -> ()
    block1():
      -> ()
  %560 : bool = prim::GetAttr[name="training"](%554)
  %561 : Tensor = prim::GetAttr[name="running_mean"](%554)
  %562 : Tensor = prim::GetAttr[name="running_var"](%554)
  %563 : Tensor = prim::GetAttr[name="weight"](%554)
  %564 : Tensor = prim::GetAttr[name="bias"](%554)
   = prim::If(%560) # torch/nn/functional.py:2011:4
    block0():
      %565 : int[] = aten::size(%x.69) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%565, %20) # torch/nn/functional.py:1991:17
      %567 : int = aten::len(%565) # torch/nn/functional.py:1992:19
      %568 : int = aten::sub(%567, %18) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%568, %19, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %572 : int = aten::add(%i.23, %18) # torch/nn/functional.py:1993:27
          %573 : int = aten::__getitem__(%565, %572) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %573) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.91)
      %575 : bool = aten::eq(%size_prods.89, %25) # torch/nn/functional.py:1994:7
       = prim::If(%575) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.71 : Tensor = aten::batch_norm(%x.69, %563, %564, %561, %562, %560, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.4 : Tensor = aten::relu_(%x.71) # torch/nn/functional.py:1117:17
  %578 : __torch__.torch.nn.modules.container.___torch_mangle_435.Sequential = prim::GetAttr[name="branch2"](%545)
  %579 : __torch__.torchvision.models.googlenet.___torch_mangle_431.BasicConv2d = prim::GetAttr[name="0"](%578)
  %580 : __torch__.torchvision.models.googlenet.___torch_mangle_434.BasicConv2d = prim::GetAttr[name="1"](%578)
  %581 : __torch__.torch.nn.modules.conv.___torch_mangle_430.Conv2d = prim::GetAttr[name="conv"](%579)
  %582 : Tensor = prim::GetAttr[name="weight"](%581)
  %583 : Tensor? = prim::GetAttr[name="bias"](%581)
  %584 : int[] = prim::ListConstruct(%25, %25)
  %585 : int[] = prim::ListConstruct(%20, %20)
  %586 : int[] = prim::ListConstruct(%25, %25)
  %x.72 : Tensor = aten::conv2d(%x.17, %582, %583, %584, %585, %586, %25) # torch/nn/modules/conv.py:415:15
  %588 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_400.BatchNorm2d = prim::GetAttr[name="bn"](%579)
  %589 : int = aten::dim(%x.72) # torch/nn/modules/batchnorm.py:276:11
  %590 : bool = aten::ne(%589, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%590) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %591 : bool = prim::GetAttr[name="training"](%588)
   = prim::If(%591) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %592 : Tensor = prim::GetAttr[name="num_batches_tracked"](%588)
      %593 : Tensor = aten::add(%592, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%588, %593)
      -> ()
    block1():
      -> ()
  %594 : bool = prim::GetAttr[name="training"](%588)
  %595 : Tensor = prim::GetAttr[name="running_mean"](%588)
  %596 : Tensor = prim::GetAttr[name="running_var"](%588)
  %597 : Tensor = prim::GetAttr[name="weight"](%588)
  %598 : Tensor = prim::GetAttr[name="bias"](%588)
   = prim::If(%594) # torch/nn/functional.py:2011:4
    block0():
      %599 : int[] = aten::size(%x.72) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%599, %20) # torch/nn/functional.py:1991:17
      %601 : int = aten::len(%599) # torch/nn/functional.py:1992:19
      %602 : int = aten::sub(%601, %18) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%602, %19, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %606 : int = aten::add(%i.24, %18) # torch/nn/functional.py:1993:27
          %607 : int = aten::__getitem__(%599, %606) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %607) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.95)
      %609 : bool = aten::eq(%size_prods.93, %25) # torch/nn/functional.py:1994:7
       = prim::If(%609) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.73 : Tensor = aten::batch_norm(%x.72, %597, %598, %595, %596, %594, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.12 : Tensor = aten::relu_(%x.73) # torch/nn/functional.py:1117:17
  %612 : __torch__.torch.nn.modules.conv.___torch_mangle_432.Conv2d = prim::GetAttr[name="conv"](%580)
  %613 : Tensor = prim::GetAttr[name="weight"](%612)
  %614 : Tensor? = prim::GetAttr[name="bias"](%612)
  %615 : int[] = prim::ListConstruct(%25, %25)
  %616 : int[] = prim::ListConstruct(%25, %25)
  %617 : int[] = prim::ListConstruct(%25, %25)
  %x.74 : Tensor = aten::conv2d(%input.12, %613, %614, %615, %616, %617, %25) # torch/nn/modules/conv.py:415:15
  %619 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_433.BatchNorm2d = prim::GetAttr[name="bn"](%580)
  %620 : int = aten::dim(%x.74) # torch/nn/modules/batchnorm.py:276:11
  %621 : bool = aten::ne(%620, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%621) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %622 : bool = prim::GetAttr[name="training"](%619)
   = prim::If(%622) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %623 : Tensor = prim::GetAttr[name="num_batches_tracked"](%619)
      %624 : Tensor = aten::add(%623, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%619, %624)
      -> ()
    block1():
      -> ()
  %625 : bool = prim::GetAttr[name="training"](%619)
  %626 : Tensor = prim::GetAttr[name="running_mean"](%619)
  %627 : Tensor = prim::GetAttr[name="running_var"](%619)
  %628 : Tensor = prim::GetAttr[name="weight"](%619)
  %629 : Tensor = prim::GetAttr[name="bias"](%619)
   = prim::If(%625) # torch/nn/functional.py:2011:4
    block0():
      %630 : int[] = aten::size(%x.74) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%630, %20) # torch/nn/functional.py:1991:17
      %632 : int = aten::len(%630) # torch/nn/functional.py:1992:19
      %633 : int = aten::sub(%632, %18) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%633, %19, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %637 : int = aten::add(%i.25, %18) # torch/nn/functional.py:1993:27
          %638 : int = aten::__getitem__(%630, %637) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %638) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.99)
      %640 : bool = aten::eq(%size_prods.97, %25) # torch/nn/functional.py:1994:7
       = prim::If(%640) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.75 : Tensor = aten::batch_norm(%x.74, %628, %629, %626, %627, %625, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.4 : Tensor = aten::relu_(%x.75) # torch/nn/functional.py:1117:17
  %643 : __torch__.torch.nn.modules.container.___torch_mangle_441.Sequential = prim::GetAttr[name="branch3"](%545)
  %644 : __torch__.torchvision.models.googlenet.___torch_mangle_437.BasicConv2d = prim::GetAttr[name="0"](%643)
  %645 : __torch__.torchvision.models.googlenet.___torch_mangle_440.BasicConv2d = prim::GetAttr[name="1"](%643)
  %646 : __torch__.torch.nn.modules.conv.___torch_mangle_436.Conv2d = prim::GetAttr[name="conv"](%644)
  %647 : Tensor = prim::GetAttr[name="weight"](%646)
  %648 : Tensor? = prim::GetAttr[name="bias"](%646)
  %649 : int[] = prim::ListConstruct(%25, %25)
  %650 : int[] = prim::ListConstruct(%20, %20)
  %651 : int[] = prim::ListConstruct(%25, %25)
  %x.65 : Tensor = aten::conv2d(%x.17, %647, %648, %649, %650, %651, %25) # torch/nn/modules/conv.py:415:15
  %653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_407.BatchNorm2d = prim::GetAttr[name="bn"](%644)
  %654 : int = aten::dim(%x.65) # torch/nn/modules/batchnorm.py:276:11
  %655 : bool = aten::ne(%654, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%655) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %656 : bool = prim::GetAttr[name="training"](%653)
   = prim::If(%656) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %657 : Tensor = prim::GetAttr[name="num_batches_tracked"](%653)
      %658 : Tensor = aten::add(%657, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%653, %658)
      -> ()
    block1():
      -> ()
  %659 : bool = prim::GetAttr[name="training"](%653)
  %660 : Tensor = prim::GetAttr[name="running_mean"](%653)
  %661 : Tensor = prim::GetAttr[name="running_var"](%653)
  %662 : Tensor = prim::GetAttr[name="weight"](%653)
  %663 : Tensor = prim::GetAttr[name="bias"](%653)
   = prim::If(%659) # torch/nn/functional.py:2011:4
    block0():
      %664 : int[] = aten::size(%x.65) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%664, %20) # torch/nn/functional.py:1991:17
      %666 : int = aten::len(%664) # torch/nn/functional.py:1992:19
      %667 : int = aten::sub(%666, %18) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%667, %19, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %671 : int = aten::add(%i.26, %18) # torch/nn/functional.py:1993:27
          %672 : int = aten::__getitem__(%664, %671) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %672) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.103)
      %674 : bool = aten::eq(%size_prods.101, %25) # torch/nn/functional.py:1994:7
       = prim::If(%674) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.68 : Tensor = aten::batch_norm(%x.65, %662, %663, %660, %661, %659, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.13 : Tensor = aten::relu_(%x.68) # torch/nn/functional.py:1117:17
  %677 : __torch__.torch.nn.modules.conv.___torch_mangle_438.Conv2d = prim::GetAttr[name="conv"](%645)
  %678 : Tensor = prim::GetAttr[name="weight"](%677)
  %679 : Tensor? = prim::GetAttr[name="bias"](%677)
  %680 : int[] = prim::ListConstruct(%25, %25)
  %681 : int[] = prim::ListConstruct(%25, %25)
  %682 : int[] = prim::ListConstruct(%25, %25)
  %x.70 : Tensor = aten::conv2d(%input.13, %678, %679, %680, %681, %682, %25) # torch/nn/modules/conv.py:415:15
  %684 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_439.BatchNorm2d = prim::GetAttr[name="bn"](%645)
  %685 : int = aten::dim(%x.70) # torch/nn/modules/batchnorm.py:276:11
  %686 : bool = aten::ne(%685, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%686) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %687 : bool = prim::GetAttr[name="training"](%684)
   = prim::If(%687) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %688 : Tensor = prim::GetAttr[name="num_batches_tracked"](%684)
      %689 : Tensor = aten::add(%688, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%684, %689)
      -> ()
    block1():
      -> ()
  %690 : bool = prim::GetAttr[name="training"](%684)
  %691 : Tensor = prim::GetAttr[name="running_mean"](%684)
  %692 : Tensor = prim::GetAttr[name="running_var"](%684)
  %693 : Tensor = prim::GetAttr[name="weight"](%684)
  %694 : Tensor = prim::GetAttr[name="bias"](%684)
   = prim::If(%690) # torch/nn/functional.py:2011:4
    block0():
      %695 : int[] = aten::size(%x.70) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%695, %20) # torch/nn/functional.py:1991:17
      %697 : int = aten::len(%695) # torch/nn/functional.py:1992:19
      %698 : int = aten::sub(%697, %18) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%698, %19, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %702 : int = aten::add(%i.27, %18) # torch/nn/functional.py:1993:27
          %703 : int = aten::__getitem__(%695, %702) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %703) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.107)
      %705 : bool = aten::eq(%size_prods.105, %25) # torch/nn/functional.py:1994:7
       = prim::If(%705) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.79 : Tensor = aten::batch_norm(%x.70, %693, %694, %691, %692, %690, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.4 : Tensor = aten::relu_(%x.79) # torch/nn/functional.py:1117:17
  %708 : __torch__.torch.nn.modules.container.___torch_mangle_444.Sequential = prim::GetAttr[name="branch4"](%545)
  %709 : __torch__.torchvision.models.googlenet.___torch_mangle_443.BasicConv2d = prim::GetAttr[name="1"](%708)
  %710 : int[] = prim::ListConstruct(%17, %17)
  %711 : int[] = prim::ListConstruct(%25, %25)
  %712 : int[] = prim::ListConstruct(%25, %25)
  %713 : int[] = prim::ListConstruct(%25, %25)
  %input.14 : Tensor = aten::max_pool2d(%x.17, %710, %711, %712, %713, %19) # torch/nn/functional.py:575:11
  %715 : __torch__.torch.nn.modules.conv.___torch_mangle_442.Conv2d = prim::GetAttr[name="conv"](%709)
  %716 : Tensor = prim::GetAttr[name="weight"](%715)
  %717 : Tensor? = prim::GetAttr[name="bias"](%715)
  %718 : int[] = prim::ListConstruct(%25, %25)
  %719 : int[] = prim::ListConstruct(%20, %20)
  %720 : int[] = prim::ListConstruct(%25, %25)
  %x.66 : Tensor = aten::conv2d(%input.14, %716, %717, %718, %719, %720, %25) # torch/nn/modules/conv.py:415:15
  %722 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%709)
  %723 : int = aten::dim(%x.66) # torch/nn/modules/batchnorm.py:276:11
  %724 : bool = aten::ne(%723, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%724) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %725 : bool = prim::GetAttr[name="training"](%722)
   = prim::If(%725) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %726 : Tensor = prim::GetAttr[name="num_batches_tracked"](%722)
      %727 : Tensor = aten::add(%726, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%722, %727)
      -> ()
    block1():
      -> ()
  %728 : bool = prim::GetAttr[name="training"](%722)
  %729 : Tensor = prim::GetAttr[name="running_mean"](%722)
  %730 : Tensor = prim::GetAttr[name="running_var"](%722)
  %731 : Tensor = prim::GetAttr[name="weight"](%722)
  %732 : Tensor = prim::GetAttr[name="bias"](%722)
   = prim::If(%728) # torch/nn/functional.py:2011:4
    block0():
      %733 : int[] = aten::size(%x.66) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%733, %20) # torch/nn/functional.py:1991:17
      %735 : int = aten::len(%733) # torch/nn/functional.py:1992:19
      %736 : int = aten::sub(%735, %18) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%736, %19, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %740 : int = aten::add(%i.22, %18) # torch/nn/functional.py:1993:27
          %741 : int = aten::__getitem__(%733, %740) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %741) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.87)
      %743 : bool = aten::eq(%size_prods.85, %25) # torch/nn/functional.py:1994:7
       = prim::If(%743) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.67 : Tensor = aten::batch_norm(%x.66, %731, %732, %729, %730, %728, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.4 : Tensor = aten::relu_(%x.67) # torch/nn/functional.py:1117:17
  %outputs.5 : Tensor[] = prim::ListConstruct(%branch1.4, %branch2.4, %branch3.4, %branch4.4)
  %x.19 : Tensor = aten::cat(%outputs.5, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %748 : bool = prim::GetAttr[name="training"](%self)
  %aux1 : Tensor? = prim::If(%748) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:163:12
    block0():
      %750 : __torch__.torchvision.models.googlenet.InceptionAux = prim::GetAttr[name="aux1"](%self)
      %751 : int[] = prim::ListConstruct(%22, %22)
      %752 : int[] = aten::size(%x.19) # torch/nn/functional.py:925:51
      %753 : int = aten::len(%752) # <string>:5:9
      %754 : bool = aten::gt(%753, %18) # <string>:5:9
       = prim::If(%754) # <string>:5:2
        block0():
          -> ()
        block1():
           = prim::RaiseException(%21) # <string>:5:2
          -> ()
      %x.76 : Tensor = aten::adaptive_avg_pool2d(%x.19, %751) # torch/nn/functional.py:926:11
      %756 : __torch__.torchvision.models.googlenet.___torch_mangle_466.BasicConv2d = prim::GetAttr[name="conv"](%750)
      %757 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv"](%756)
      %758 : Tensor = prim::GetAttr[name="weight"](%757)
      %759 : Tensor? = prim::GetAttr[name="bias"](%757)
      %760 : int[] = prim::ListConstruct(%25, %25)
      %761 : int[] = prim::ListConstruct(%20, %20)
      %762 : int[] = prim::ListConstruct(%25, %25)
      %x.80 : Tensor = aten::conv2d(%x.76, %758, %759, %760, %761, %762, %25) # torch/nn/modules/conv.py:415:15
      %764 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%756)
      %765 : int = aten::dim(%x.80) # torch/nn/modules/batchnorm.py:276:11
      %766 : bool = aten::ne(%765, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%766) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %767 : bool = prim::GetAttr[name="training"](%764)
       = prim::If(%767) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %768 : Tensor = prim::GetAttr[name="num_batches_tracked"](%764)
          %769 : Tensor = aten::add(%768, %25, %25) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%764, %769)
          -> ()
        block1():
          -> ()
      %770 : bool = prim::GetAttr[name="training"](%764)
      %771 : Tensor = prim::GetAttr[name="running_mean"](%764)
      %772 : Tensor = prim::GetAttr[name="running_var"](%764)
      %773 : Tensor = prim::GetAttr[name="weight"](%764)
      %774 : Tensor = prim::GetAttr[name="bias"](%764)
       = prim::If(%770) # torch/nn/functional.py:2011:4
        block0():
          %775 : int[] = aten::size(%x.80) # torch/nn/functional.py:2012:27
          %size_prods.108 : int = aten::__getitem__(%775, %20) # torch/nn/functional.py:1991:17
          %777 : int = aten::len(%775) # torch/nn/functional.py:1992:19
          %778 : int = aten::sub(%777, %18) # torch/nn/functional.py:1992:19
          %size_prods.109 : int = prim::Loop(%778, %19, %size_prods.108) # torch/nn/functional.py:1992:4
            block0(%i.28 : int, %size_prods.110 : int):
              %782 : int = aten::add(%i.28, %18) # torch/nn/functional.py:1993:27
              %783 : int = aten::__getitem__(%775, %782) # torch/nn/functional.py:1993:22
              %size_prods.111 : int = aten::mul(%size_prods.110, %783) # torch/nn/functional.py:1993:8
              -> (%19, %size_prods.111)
          %785 : bool = aten::eq(%size_prods.109, %25) # torch/nn/functional.py:1994:7
           = prim::If(%785) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %x.81 : Tensor = aten::batch_norm(%x.80, %773, %774, %771, %772, %770, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
      %x.77 : Tensor = aten::relu_(%x.81) # torch/nn/functional.py:1117:17
      %x.78 : Tensor = aten::flatten(%x.77, %25, %27) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:272:12
      %789 : __torch__.torch.nn.modules.linear.___torch_mangle_527.Linear = prim::GetAttr[name="fc1"](%750)
      %790 : Tensor = prim::GetAttr[name="weight"](%789)
      %791 : Tensor = prim::GetAttr[name="bias"](%789)
      %792 : int = aten::dim(%x.78) # torch/nn/functional.py:1672:7
      %793 : bool = aten::eq(%792, %18) # torch/nn/functional.py:1672:7
      %ret.4 : Tensor = prim::If(%793) # torch/nn/functional.py:1672:4
        block0():
          %795 : Tensor = aten::t(%790) # torch/nn/functional.py:1674:39
          %ret.5 : Tensor = aten::addmm(%791, %x.78, %795, %25, %25) # torch/nn/functional.py:1674:14
          -> (%ret.5)
        block1():
          %797 : Tensor = aten::t(%790) # torch/nn/functional.py:1676:30
          %output.5 : Tensor = aten::matmul(%x.78, %797) # torch/nn/functional.py:1676:17
          %output.6 : Tensor = aten::add_(%output.5, %791, %25) # torch/nn/functional.py:1678:12
          -> (%output.6)
      %result.2 : Tensor = aten::relu_(%ret.4) # torch/nn/functional.py:1117:17
      %801 : bool = prim::GetAttr[name="training"](%750)
      %802 : Tensor = aten::dropout(%result.2, %16, %801) # torch/nn/functional.py:973:17
      %803 : __torch__.torch.nn.modules.linear.___torch_mangle_161.Linear = prim::GetAttr[name="fc2"](%750)
      %804 : Tensor = prim::GetAttr[name="weight"](%803)
      %805 : Tensor = prim::GetAttr[name="bias"](%803)
      %806 : int = aten::dim(%802) # torch/nn/functional.py:1672:7
      %807 : bool = aten::eq(%806, %18) # torch/nn/functional.py:1672:7
      %aux1.2 : Tensor = prim::If(%807) # torch/nn/functional.py:1672:4
        block0():
          %809 : Tensor = aten::t(%804) # torch/nn/functional.py:1674:39
          %ret.6 : Tensor = aten::addmm(%805, %802, %809, %25, %25) # torch/nn/functional.py:1674:14
          -> (%ret.6)
        block1():
          %811 : Tensor = aten::t(%804) # torch/nn/functional.py:1676:30
          %output.7 : Tensor = aten::matmul(%802, %811) # torch/nn/functional.py:1676:17
          %output.8 : Tensor = aten::add_(%output.7, %805, %25) # torch/nn/functional.py:1678:12
          -> (%output.8)
      -> (%aux1.2)
    block1():
      -> (%aux1.1)
  %814 : __torch__.torchvision.models.googlenet.___torch_mangle_465.Inception = prim::GetAttr[name="inception4b"](%self)
  %815 : __torch__.torchvision.models.googlenet.___torch_mangle_448.BasicConv2d = prim::GetAttr[name="branch1"](%814)
  %816 : __torch__.torch.nn.modules.conv.___torch_mangle_446.Conv2d = prim::GetAttr[name="conv"](%815)
  %817 : Tensor = prim::GetAttr[name="weight"](%816)
  %818 : Tensor? = prim::GetAttr[name="bias"](%816)
  %819 : int[] = prim::ListConstruct(%25, %25)
  %820 : int[] = prim::ListConstruct(%20, %20)
  %821 : int[] = prim::ListConstruct(%25, %25)
  %x.82 : Tensor = aten::conv2d(%x.19, %817, %818, %819, %820, %821, %25) # torch/nn/modules/conv.py:415:15
  %823 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%815)
  %824 : int = aten::dim(%x.82) # torch/nn/modules/batchnorm.py:276:11
  %825 : bool = aten::ne(%824, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%825) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %826 : bool = prim::GetAttr[name="training"](%823)
   = prim::If(%826) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %827 : Tensor = prim::GetAttr[name="num_batches_tracked"](%823)
      %828 : Tensor = aten::add(%827, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%823, %828)
      -> ()
    block1():
      -> ()
  %829 : bool = prim::GetAttr[name="training"](%823)
  %830 : Tensor = prim::GetAttr[name="running_mean"](%823)
  %831 : Tensor = prim::GetAttr[name="running_var"](%823)
  %832 : Tensor = prim::GetAttr[name="weight"](%823)
  %833 : Tensor = prim::GetAttr[name="bias"](%823)
   = prim::If(%829) # torch/nn/functional.py:2011:4
    block0():
      %834 : int[] = aten::size(%x.82) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%834, %20) # torch/nn/functional.py:1991:17
      %836 : int = aten::len(%834) # torch/nn/functional.py:1992:19
      %837 : int = aten::sub(%836, %18) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%837, %19, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %841 : int = aten::add(%i.29, %18) # torch/nn/functional.py:1993:27
          %842 : int = aten::__getitem__(%834, %841) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %842) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.115)
      %844 : bool = aten::eq(%size_prods.113, %25) # torch/nn/functional.py:1994:7
       = prim::If(%844) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.83 : Tensor = aten::batch_norm(%x.82, %832, %833, %830, %831, %829, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.5 : Tensor = aten::relu_(%x.83) # torch/nn/functional.py:1117:17
  %847 : __torch__.torch.nn.modules.container.___torch_mangle_455.Sequential = prim::GetAttr[name="branch2"](%814)
  %848 : __torch__.torchvision.models.googlenet.___torch_mangle_451.BasicConv2d = prim::GetAttr[name="0"](%847)
  %849 : __torch__.torchvision.models.googlenet.___torch_mangle_454.BasicConv2d = prim::GetAttr[name="1"](%847)
  %850 : __torch__.torch.nn.modules.conv.___torch_mangle_449.Conv2d = prim::GetAttr[name="conv"](%848)
  %851 : Tensor = prim::GetAttr[name="weight"](%850)
  %852 : Tensor? = prim::GetAttr[name="bias"](%850)
  %853 : int[] = prim::ListConstruct(%25, %25)
  %854 : int[] = prim::ListConstruct(%20, %20)
  %855 : int[] = prim::ListConstruct(%25, %25)
  %x.84 : Tensor = aten::conv2d(%x.19, %851, %852, %853, %854, %855, %25) # torch/nn/modules/conv.py:415:15
  %857 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_450.BatchNorm2d = prim::GetAttr[name="bn"](%848)
  %858 : int = aten::dim(%x.84) # torch/nn/modules/batchnorm.py:276:11
  %859 : bool = aten::ne(%858, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%859) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %860 : bool = prim::GetAttr[name="training"](%857)
   = prim::If(%860) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %861 : Tensor = prim::GetAttr[name="num_batches_tracked"](%857)
      %862 : Tensor = aten::add(%861, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%857, %862)
      -> ()
    block1():
      -> ()
  %863 : bool = prim::GetAttr[name="training"](%857)
  %864 : Tensor = prim::GetAttr[name="running_mean"](%857)
  %865 : Tensor = prim::GetAttr[name="running_var"](%857)
  %866 : Tensor = prim::GetAttr[name="weight"](%857)
  %867 : Tensor = prim::GetAttr[name="bias"](%857)
   = prim::If(%863) # torch/nn/functional.py:2011:4
    block0():
      %868 : int[] = aten::size(%x.84) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%868, %20) # torch/nn/functional.py:1991:17
      %870 : int = aten::len(%868) # torch/nn/functional.py:1992:19
      %871 : int = aten::sub(%870, %18) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%871, %19, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %875 : int = aten::add(%i.30, %18) # torch/nn/functional.py:1993:27
          %876 : int = aten::__getitem__(%868, %875) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %876) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.119)
      %878 : bool = aten::eq(%size_prods.117, %25) # torch/nn/functional.py:1994:7
       = prim::If(%878) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.85 : Tensor = aten::batch_norm(%x.84, %866, %867, %864, %865, %863, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.15 : Tensor = aten::relu_(%x.85) # torch/nn/functional.py:1117:17
  %881 : __torch__.torch.nn.modules.conv.___torch_mangle_452.Conv2d = prim::GetAttr[name="conv"](%849)
  %882 : Tensor = prim::GetAttr[name="weight"](%881)
  %883 : Tensor? = prim::GetAttr[name="bias"](%881)
  %884 : int[] = prim::ListConstruct(%25, %25)
  %885 : int[] = prim::ListConstruct(%25, %25)
  %886 : int[] = prim::ListConstruct(%25, %25)
  %x.86 : Tensor = aten::conv2d(%input.15, %882, %883, %884, %885, %886, %25) # torch/nn/modules/conv.py:415:15
  %888 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_453.BatchNorm2d = prim::GetAttr[name="bn"](%849)
  %889 : int = aten::dim(%x.86) # torch/nn/modules/batchnorm.py:276:11
  %890 : bool = aten::ne(%889, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%890) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %891 : bool = prim::GetAttr[name="training"](%888)
   = prim::If(%891) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %892 : Tensor = prim::GetAttr[name="num_batches_tracked"](%888)
      %893 : Tensor = aten::add(%892, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%888, %893)
      -> ()
    block1():
      -> ()
  %894 : bool = prim::GetAttr[name="training"](%888)
  %895 : Tensor = prim::GetAttr[name="running_mean"](%888)
  %896 : Tensor = prim::GetAttr[name="running_var"](%888)
  %897 : Tensor = prim::GetAttr[name="weight"](%888)
  %898 : Tensor = prim::GetAttr[name="bias"](%888)
   = prim::If(%894) # torch/nn/functional.py:2011:4
    block0():
      %899 : int[] = aten::size(%x.86) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%899, %20) # torch/nn/functional.py:1991:17
      %901 : int = aten::len(%899) # torch/nn/functional.py:1992:19
      %902 : int = aten::sub(%901, %18) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%902, %19, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %906 : int = aten::add(%i.31, %18) # torch/nn/functional.py:1993:27
          %907 : int = aten::__getitem__(%899, %906) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %907) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.123)
      %909 : bool = aten::eq(%size_prods.121, %25) # torch/nn/functional.py:1994:7
       = prim::If(%909) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.87 : Tensor = aten::batch_norm(%x.86, %897, %898, %895, %896, %894, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.5 : Tensor = aten::relu_(%x.87) # torch/nn/functional.py:1117:17
  %912 : __torch__.torch.nn.modules.container.___torch_mangle_461.Sequential = prim::GetAttr[name="branch3"](%814)
  %913 : __torch__.torchvision.models.googlenet.___torch_mangle_458.BasicConv2d = prim::GetAttr[name="0"](%912)
  %914 : __torch__.torchvision.models.googlenet.___torch_mangle_460.BasicConv2d = prim::GetAttr[name="1"](%912)
  %915 : __torch__.torch.nn.modules.conv.___torch_mangle_456.Conv2d = prim::GetAttr[name="conv"](%913)
  %916 : Tensor = prim::GetAttr[name="weight"](%915)
  %917 : Tensor? = prim::GetAttr[name="bias"](%915)
  %918 : int[] = prim::ListConstruct(%25, %25)
  %919 : int[] = prim::ListConstruct(%20, %20)
  %920 : int[] = prim::ListConstruct(%25, %25)
  %x.88 : Tensor = aten::conv2d(%x.19, %916, %917, %918, %919, %920, %25) # torch/nn/modules/conv.py:415:15
  %922 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_457.BatchNorm2d = prim::GetAttr[name="bn"](%913)
  %923 : int = aten::dim(%x.88) # torch/nn/modules/batchnorm.py:276:11
  %924 : bool = aten::ne(%923, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%924) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %925 : bool = prim::GetAttr[name="training"](%922)
   = prim::If(%925) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %926 : Tensor = prim::GetAttr[name="num_batches_tracked"](%922)
      %927 : Tensor = aten::add(%926, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%922, %927)
      -> ()
    block1():
      -> ()
  %928 : bool = prim::GetAttr[name="training"](%922)
  %929 : Tensor = prim::GetAttr[name="running_mean"](%922)
  %930 : Tensor = prim::GetAttr[name="running_var"](%922)
  %931 : Tensor = prim::GetAttr[name="weight"](%922)
  %932 : Tensor = prim::GetAttr[name="bias"](%922)
   = prim::If(%928) # torch/nn/functional.py:2011:4
    block0():
      %933 : int[] = aten::size(%x.88) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%933, %20) # torch/nn/functional.py:1991:17
      %935 : int = aten::len(%933) # torch/nn/functional.py:1992:19
      %936 : int = aten::sub(%935, %18) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%936, %19, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %940 : int = aten::add(%i.32, %18) # torch/nn/functional.py:1993:27
          %941 : int = aten::__getitem__(%933, %940) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %941) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.127)
      %943 : bool = aten::eq(%size_prods.125, %25) # torch/nn/functional.py:1994:7
       = prim::If(%943) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.89 : Tensor = aten::batch_norm(%x.88, %931, %932, %929, %930, %928, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.16 : Tensor = aten::relu_(%x.89) # torch/nn/functional.py:1117:17
  %946 : __torch__.torch.nn.modules.conv.___torch_mangle_459.Conv2d = prim::GetAttr[name="conv"](%914)
  %947 : Tensor = prim::GetAttr[name="weight"](%946)
  %948 : Tensor? = prim::GetAttr[name="bias"](%946)
  %949 : int[] = prim::ListConstruct(%25, %25)
  %950 : int[] = prim::ListConstruct(%25, %25)
  %951 : int[] = prim::ListConstruct(%25, %25)
  %x.90 : Tensor = aten::conv2d(%input.16, %947, %948, %949, %950, %951, %25) # torch/nn/modules/conv.py:415:15
  %953 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%914)
  %954 : int = aten::dim(%x.90) # torch/nn/modules/batchnorm.py:276:11
  %955 : bool = aten::ne(%954, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%955) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %956 : bool = prim::GetAttr[name="training"](%953)
   = prim::If(%956) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %957 : Tensor = prim::GetAttr[name="num_batches_tracked"](%953)
      %958 : Tensor = aten::add(%957, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%953, %958)
      -> ()
    block1():
      -> ()
  %959 : bool = prim::GetAttr[name="training"](%953)
  %960 : Tensor = prim::GetAttr[name="running_mean"](%953)
  %961 : Tensor = prim::GetAttr[name="running_var"](%953)
  %962 : Tensor = prim::GetAttr[name="weight"](%953)
  %963 : Tensor = prim::GetAttr[name="bias"](%953)
   = prim::If(%959) # torch/nn/functional.py:2011:4
    block0():
      %964 : int[] = aten::size(%x.90) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%964, %20) # torch/nn/functional.py:1991:17
      %966 : int = aten::len(%964) # torch/nn/functional.py:1992:19
      %967 : int = aten::sub(%966, %18) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%967, %19, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %971 : int = aten::add(%i.33, %18) # torch/nn/functional.py:1993:27
          %972 : int = aten::__getitem__(%964, %971) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %972) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.131)
      %974 : bool = aten::eq(%size_prods.129, %25) # torch/nn/functional.py:1994:7
       = prim::If(%974) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.91 : Tensor = aten::batch_norm(%x.90, %962, %963, %960, %961, %959, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.5 : Tensor = aten::relu_(%x.91) # torch/nn/functional.py:1117:17
  %977 : __torch__.torch.nn.modules.container.___torch_mangle_464.Sequential = prim::GetAttr[name="branch4"](%814)
  %978 : __torch__.torchvision.models.googlenet.___torch_mangle_463.BasicConv2d = prim::GetAttr[name="1"](%977)
  %979 : int[] = prim::ListConstruct(%17, %17)
  %980 : int[] = prim::ListConstruct(%25, %25)
  %981 : int[] = prim::ListConstruct(%25, %25)
  %982 : int[] = prim::ListConstruct(%25, %25)
  %input.17 : Tensor = aten::max_pool2d(%x.19, %979, %980, %981, %982, %19) # torch/nn/functional.py:575:11
  %984 : __torch__.torch.nn.modules.conv.___torch_mangle_462.Conv2d = prim::GetAttr[name="conv"](%978)
  %985 : Tensor = prim::GetAttr[name="weight"](%984)
  %986 : Tensor? = prim::GetAttr[name="bias"](%984)
  %987 : int[] = prim::ListConstruct(%25, %25)
  %988 : int[] = prim::ListConstruct(%20, %20)
  %989 : int[] = prim::ListConstruct(%25, %25)
  %x.92 : Tensor = aten::conv2d(%input.17, %985, %986, %987, %988, %989, %25) # torch/nn/modules/conv.py:415:15
  %991 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%978)
  %992 : int = aten::dim(%x.92) # torch/nn/modules/batchnorm.py:276:11
  %993 : bool = aten::ne(%992, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%993) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %994 : bool = prim::GetAttr[name="training"](%991)
   = prim::If(%994) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %995 : Tensor = prim::GetAttr[name="num_batches_tracked"](%991)
      %996 : Tensor = aten::add(%995, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%991, %996)
      -> ()
    block1():
      -> ()
  %997 : bool = prim::GetAttr[name="training"](%991)
  %998 : Tensor = prim::GetAttr[name="running_mean"](%991)
  %999 : Tensor = prim::GetAttr[name="running_var"](%991)
  %1000 : Tensor = prim::GetAttr[name="weight"](%991)
  %1001 : Tensor = prim::GetAttr[name="bias"](%991)
   = prim::If(%997) # torch/nn/functional.py:2011:4
    block0():
      %1002 : int[] = aten::size(%x.92) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%1002, %20) # torch/nn/functional.py:1991:17
      %1004 : int = aten::len(%1002) # torch/nn/functional.py:1992:19
      %1005 : int = aten::sub(%1004, %18) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%1005, %19, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %1009 : int = aten::add(%i.34, %18) # torch/nn/functional.py:1993:27
          %1010 : int = aten::__getitem__(%1002, %1009) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %1010) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.135)
      %1012 : bool = aten::eq(%size_prods.133, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1012) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.93 : Tensor = aten::batch_norm(%x.92, %1000, %1001, %998, %999, %997, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.5 : Tensor = aten::relu_(%x.93) # torch/nn/functional.py:1117:17
  %outputs.6 : Tensor[] = prim::ListConstruct(%branch1.5, %branch2.5, %branch3.5, %branch4.5)
  %x.22 : Tensor = aten::cat(%outputs.6, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %1017 : __torch__.torchvision.models.googlenet.___torch_mangle_471.Inception = prim::GetAttr[name="inception4c"](%self)
  %1018 : __torch__.torchvision.models.googlenet.___torch_mangle_466.BasicConv2d = prim::GetAttr[name="branch1"](%1017)
  %1019 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv"](%1018)
  %1020 : Tensor = prim::GetAttr[name="weight"](%1019)
  %1021 : Tensor? = prim::GetAttr[name="bias"](%1019)
  %1022 : int[] = prim::ListConstruct(%25, %25)
  %1023 : int[] = prim::ListConstruct(%20, %20)
  %1024 : int[] = prim::ListConstruct(%25, %25)
  %x.94 : Tensor = aten::conv2d(%x.22, %1020, %1021, %1022, %1023, %1024, %25) # torch/nn/modules/conv.py:415:15
  %1026 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1018)
  %1027 : int = aten::dim(%x.94) # torch/nn/modules/batchnorm.py:276:11
  %1028 : bool = aten::ne(%1027, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1028) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1029 : bool = prim::GetAttr[name="training"](%1026)
   = prim::If(%1029) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1030 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1026)
      %1031 : Tensor = aten::add(%1030, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1026, %1031)
      -> ()
    block1():
      -> ()
  %1032 : bool = prim::GetAttr[name="training"](%1026)
  %1033 : Tensor = prim::GetAttr[name="running_mean"](%1026)
  %1034 : Tensor = prim::GetAttr[name="running_var"](%1026)
  %1035 : Tensor = prim::GetAttr[name="weight"](%1026)
  %1036 : Tensor = prim::GetAttr[name="bias"](%1026)
   = prim::If(%1032) # torch/nn/functional.py:2011:4
    block0():
      %1037 : int[] = aten::size(%x.94) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%1037, %20) # torch/nn/functional.py:1991:17
      %1039 : int = aten::len(%1037) # torch/nn/functional.py:1992:19
      %1040 : int = aten::sub(%1039, %18) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%1040, %19, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %1044 : int = aten::add(%i.35, %18) # torch/nn/functional.py:1993:27
          %1045 : int = aten::__getitem__(%1037, %1044) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %1045) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.139)
      %1047 : bool = aten::eq(%size_prods.137, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1047) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.95 : Tensor = aten::batch_norm(%x.94, %1035, %1036, %1033, %1034, %1032, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.6 : Tensor = aten::relu_(%x.95) # torch/nn/functional.py:1117:17
  %1050 : __torch__.torch.nn.modules.container.___torch_mangle_470.Sequential = prim::GetAttr[name="branch2"](%1017)
  %1051 : __torch__.torchvision.models.googlenet.___torch_mangle_466.BasicConv2d = prim::GetAttr[name="0"](%1050)
  %1052 : __torch__.torchvision.models.googlenet.___torch_mangle_469.BasicConv2d = prim::GetAttr[name="1"](%1050)
  %1053 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv"](%1051)
  %1054 : Tensor = prim::GetAttr[name="weight"](%1053)
  %1055 : Tensor? = prim::GetAttr[name="bias"](%1053)
  %1056 : int[] = prim::ListConstruct(%25, %25)
  %1057 : int[] = prim::ListConstruct(%20, %20)
  %1058 : int[] = prim::ListConstruct(%25, %25)
  %x.96 : Tensor = aten::conv2d(%x.22, %1054, %1055, %1056, %1057, %1058, %25) # torch/nn/modules/conv.py:415:15
  %1060 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1051)
  %1061 : int = aten::dim(%x.96) # torch/nn/modules/batchnorm.py:276:11
  %1062 : bool = aten::ne(%1061, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1062) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1063 : bool = prim::GetAttr[name="training"](%1060)
   = prim::If(%1063) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1064 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1060)
      %1065 : Tensor = aten::add(%1064, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1060, %1065)
      -> ()
    block1():
      -> ()
  %1066 : bool = prim::GetAttr[name="training"](%1060)
  %1067 : Tensor = prim::GetAttr[name="running_mean"](%1060)
  %1068 : Tensor = prim::GetAttr[name="running_var"](%1060)
  %1069 : Tensor = prim::GetAttr[name="weight"](%1060)
  %1070 : Tensor = prim::GetAttr[name="bias"](%1060)
   = prim::If(%1066) # torch/nn/functional.py:2011:4
    block0():
      %1071 : int[] = aten::size(%x.96) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%1071, %20) # torch/nn/functional.py:1991:17
      %1073 : int = aten::len(%1071) # torch/nn/functional.py:1992:19
      %1074 : int = aten::sub(%1073, %18) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%1074, %19, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %1078 : int = aten::add(%i.36, %18) # torch/nn/functional.py:1993:27
          %1079 : int = aten::__getitem__(%1071, %1078) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %1079) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.143)
      %1081 : bool = aten::eq(%size_prods.141, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1081) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.97 : Tensor = aten::batch_norm(%x.96, %1069, %1070, %1067, %1068, %1066, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.18 : Tensor = aten::relu_(%x.97) # torch/nn/functional.py:1117:17
  %1084 : __torch__.torch.nn.modules.conv.___torch_mangle_467.Conv2d = prim::GetAttr[name="conv"](%1052)
  %1085 : Tensor = prim::GetAttr[name="weight"](%1084)
  %1086 : Tensor? = prim::GetAttr[name="bias"](%1084)
  %1087 : int[] = prim::ListConstruct(%25, %25)
  %1088 : int[] = prim::ListConstruct(%25, %25)
  %1089 : int[] = prim::ListConstruct(%25, %25)
  %x.98 : Tensor = aten::conv2d(%input.18, %1085, %1086, %1087, %1088, %1089, %25) # torch/nn/modules/conv.py:415:15
  %1091 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_468.BatchNorm2d = prim::GetAttr[name="bn"](%1052)
  %1092 : int = aten::dim(%x.98) # torch/nn/modules/batchnorm.py:276:11
  %1093 : bool = aten::ne(%1092, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1093) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1094 : bool = prim::GetAttr[name="training"](%1091)
   = prim::If(%1094) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1095 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1091)
      %1096 : Tensor = aten::add(%1095, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1091, %1096)
      -> ()
    block1():
      -> ()
  %1097 : bool = prim::GetAttr[name="training"](%1091)
  %1098 : Tensor = prim::GetAttr[name="running_mean"](%1091)
  %1099 : Tensor = prim::GetAttr[name="running_var"](%1091)
  %1100 : Tensor = prim::GetAttr[name="weight"](%1091)
  %1101 : Tensor = prim::GetAttr[name="bias"](%1091)
   = prim::If(%1097) # torch/nn/functional.py:2011:4
    block0():
      %1102 : int[] = aten::size(%x.98) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%1102, %20) # torch/nn/functional.py:1991:17
      %1104 : int = aten::len(%1102) # torch/nn/functional.py:1992:19
      %1105 : int = aten::sub(%1104, %18) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%1105, %19, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %1109 : int = aten::add(%i.37, %18) # torch/nn/functional.py:1993:27
          %1110 : int = aten::__getitem__(%1102, %1109) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %1110) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.147)
      %1112 : bool = aten::eq(%size_prods.145, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1112) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.99 : Tensor = aten::batch_norm(%x.98, %1100, %1101, %1098, %1099, %1097, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.6 : Tensor = aten::relu_(%x.99) # torch/nn/functional.py:1117:17
  %1115 : __torch__.torch.nn.modules.container.___torch_mangle_461.Sequential = prim::GetAttr[name="branch3"](%1017)
  %1116 : __torch__.torchvision.models.googlenet.___torch_mangle_458.BasicConv2d = prim::GetAttr[name="0"](%1115)
  %1117 : __torch__.torchvision.models.googlenet.___torch_mangle_460.BasicConv2d = prim::GetAttr[name="1"](%1115)
  %1118 : __torch__.torch.nn.modules.conv.___torch_mangle_456.Conv2d = prim::GetAttr[name="conv"](%1116)
  %1119 : Tensor = prim::GetAttr[name="weight"](%1118)
  %1120 : Tensor? = prim::GetAttr[name="bias"](%1118)
  %1121 : int[] = prim::ListConstruct(%25, %25)
  %1122 : int[] = prim::ListConstruct(%20, %20)
  %1123 : int[] = prim::ListConstruct(%25, %25)
  %x.100 : Tensor = aten::conv2d(%x.22, %1119, %1120, %1121, %1122, %1123, %25) # torch/nn/modules/conv.py:415:15
  %1125 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_457.BatchNorm2d = prim::GetAttr[name="bn"](%1116)
  %1126 : int = aten::dim(%x.100) # torch/nn/modules/batchnorm.py:276:11
  %1127 : bool = aten::ne(%1126, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1127) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1128 : bool = prim::GetAttr[name="training"](%1125)
   = prim::If(%1128) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1129 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1125)
      %1130 : Tensor = aten::add(%1129, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1125, %1130)
      -> ()
    block1():
      -> ()
  %1131 : bool = prim::GetAttr[name="training"](%1125)
  %1132 : Tensor = prim::GetAttr[name="running_mean"](%1125)
  %1133 : Tensor = prim::GetAttr[name="running_var"](%1125)
  %1134 : Tensor = prim::GetAttr[name="weight"](%1125)
  %1135 : Tensor = prim::GetAttr[name="bias"](%1125)
   = prim::If(%1131) # torch/nn/functional.py:2011:4
    block0():
      %1136 : int[] = aten::size(%x.100) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%1136, %20) # torch/nn/functional.py:1991:17
      %1138 : int = aten::len(%1136) # torch/nn/functional.py:1992:19
      %1139 : int = aten::sub(%1138, %18) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%1139, %19, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %1143 : int = aten::add(%i.38, %18) # torch/nn/functional.py:1993:27
          %1144 : int = aten::__getitem__(%1136, %1143) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %1144) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.151)
      %1146 : bool = aten::eq(%size_prods.149, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1146) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.101 : Tensor = aten::batch_norm(%x.100, %1134, %1135, %1132, %1133, %1131, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.19 : Tensor = aten::relu_(%x.101) # torch/nn/functional.py:1117:17
  %1149 : __torch__.torch.nn.modules.conv.___torch_mangle_459.Conv2d = prim::GetAttr[name="conv"](%1117)
  %1150 : Tensor = prim::GetAttr[name="weight"](%1149)
  %1151 : Tensor? = prim::GetAttr[name="bias"](%1149)
  %1152 : int[] = prim::ListConstruct(%25, %25)
  %1153 : int[] = prim::ListConstruct(%25, %25)
  %1154 : int[] = prim::ListConstruct(%25, %25)
  %x.102 : Tensor = aten::conv2d(%input.19, %1150, %1151, %1152, %1153, %1154, %25) # torch/nn/modules/conv.py:415:15
  %1156 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%1117)
  %1157 : int = aten::dim(%x.102) # torch/nn/modules/batchnorm.py:276:11
  %1158 : bool = aten::ne(%1157, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1158) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1159 : bool = prim::GetAttr[name="training"](%1156)
   = prim::If(%1159) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1160 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1156)
      %1161 : Tensor = aten::add(%1160, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1156, %1161)
      -> ()
    block1():
      -> ()
  %1162 : bool = prim::GetAttr[name="training"](%1156)
  %1163 : Tensor = prim::GetAttr[name="running_mean"](%1156)
  %1164 : Tensor = prim::GetAttr[name="running_var"](%1156)
  %1165 : Tensor = prim::GetAttr[name="weight"](%1156)
  %1166 : Tensor = prim::GetAttr[name="bias"](%1156)
   = prim::If(%1162) # torch/nn/functional.py:2011:4
    block0():
      %1167 : int[] = aten::size(%x.102) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%1167, %20) # torch/nn/functional.py:1991:17
      %1169 : int = aten::len(%1167) # torch/nn/functional.py:1992:19
      %1170 : int = aten::sub(%1169, %18) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%1170, %19, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %1174 : int = aten::add(%i.39, %18) # torch/nn/functional.py:1993:27
          %1175 : int = aten::__getitem__(%1167, %1174) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %1175) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.155)
      %1177 : bool = aten::eq(%size_prods.153, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1177) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.103 : Tensor = aten::batch_norm(%x.102, %1165, %1166, %1163, %1164, %1162, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.6 : Tensor = aten::relu_(%x.103) # torch/nn/functional.py:1117:17
  %1180 : __torch__.torch.nn.modules.container.___torch_mangle_464.Sequential = prim::GetAttr[name="branch4"](%1017)
  %1181 : __torch__.torchvision.models.googlenet.___torch_mangle_463.BasicConv2d = prim::GetAttr[name="1"](%1180)
  %1182 : int[] = prim::ListConstruct(%17, %17)
  %1183 : int[] = prim::ListConstruct(%25, %25)
  %1184 : int[] = prim::ListConstruct(%25, %25)
  %1185 : int[] = prim::ListConstruct(%25, %25)
  %input.20 : Tensor = aten::max_pool2d(%x.22, %1182, %1183, %1184, %1185, %19) # torch/nn/functional.py:575:11
  %1187 : __torch__.torch.nn.modules.conv.___torch_mangle_462.Conv2d = prim::GetAttr[name="conv"](%1181)
  %1188 : Tensor = prim::GetAttr[name="weight"](%1187)
  %1189 : Tensor? = prim::GetAttr[name="bias"](%1187)
  %1190 : int[] = prim::ListConstruct(%25, %25)
  %1191 : int[] = prim::ListConstruct(%20, %20)
  %1192 : int[] = prim::ListConstruct(%25, %25)
  %x.104 : Tensor = aten::conv2d(%input.20, %1188, %1189, %1190, %1191, %1192, %25) # torch/nn/modules/conv.py:415:15
  %1194 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%1181)
  %1195 : int = aten::dim(%x.104) # torch/nn/modules/batchnorm.py:276:11
  %1196 : bool = aten::ne(%1195, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1196) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1197 : bool = prim::GetAttr[name="training"](%1194)
   = prim::If(%1197) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1198 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1194)
      %1199 : Tensor = aten::add(%1198, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1194, %1199)
      -> ()
    block1():
      -> ()
  %1200 : bool = prim::GetAttr[name="training"](%1194)
  %1201 : Tensor = prim::GetAttr[name="running_mean"](%1194)
  %1202 : Tensor = prim::GetAttr[name="running_var"](%1194)
  %1203 : Tensor = prim::GetAttr[name="weight"](%1194)
  %1204 : Tensor = prim::GetAttr[name="bias"](%1194)
   = prim::If(%1200) # torch/nn/functional.py:2011:4
    block0():
      %1205 : int[] = aten::size(%x.104) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%1205, %20) # torch/nn/functional.py:1991:17
      %1207 : int = aten::len(%1205) # torch/nn/functional.py:1992:19
      %1208 : int = aten::sub(%1207, %18) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%1208, %19, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %1212 : int = aten::add(%i.40, %18) # torch/nn/functional.py:1993:27
          %1213 : int = aten::__getitem__(%1205, %1212) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %1213) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.159)
      %1215 : bool = aten::eq(%size_prods.157, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1215) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.105 : Tensor = aten::batch_norm(%x.104, %1203, %1204, %1201, %1202, %1200, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.6 : Tensor = aten::relu_(%x.105) # torch/nn/functional.py:1117:17
  %outputs.7 : Tensor[] = prim::ListConstruct(%branch1.6, %branch2.6, %branch3.6, %branch4.6)
  %x.24 : Tensor = aten::cat(%outputs.7, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %1220 : __torch__.torchvision.models.googlenet.___torch_mangle_484.Inception = prim::GetAttr[name="inception4d"](%self)
  %1221 : __torch__.torchvision.models.googlenet.___torch_mangle_451.BasicConv2d = prim::GetAttr[name="branch1"](%1220)
  %1222 : __torch__.torch.nn.modules.conv.___torch_mangle_449.Conv2d = prim::GetAttr[name="conv"](%1221)
  %1223 : Tensor = prim::GetAttr[name="weight"](%1222)
  %1224 : Tensor? = prim::GetAttr[name="bias"](%1222)
  %1225 : int[] = prim::ListConstruct(%25, %25)
  %1226 : int[] = prim::ListConstruct(%20, %20)
  %1227 : int[] = prim::ListConstruct(%25, %25)
  %x.110 : Tensor = aten::conv2d(%x.24, %1223, %1224, %1225, %1226, %1227, %25) # torch/nn/modules/conv.py:415:15
  %1229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_450.BatchNorm2d = prim::GetAttr[name="bn"](%1221)
  %1230 : int = aten::dim(%x.110) # torch/nn/modules/batchnorm.py:276:11
  %1231 : bool = aten::ne(%1230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1229)
   = prim::If(%1232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1229)
      %1234 : Tensor = aten::add(%1233, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1229, %1234)
      -> ()
    block1():
      -> ()
  %1235 : bool = prim::GetAttr[name="training"](%1229)
  %1236 : Tensor = prim::GetAttr[name="running_mean"](%1229)
  %1237 : Tensor = prim::GetAttr[name="running_var"](%1229)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1239 : Tensor = prim::GetAttr[name="bias"](%1229)
   = prim::If(%1235) # torch/nn/functional.py:2011:4
    block0():
      %1240 : int[] = aten::size(%x.110) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%1240, %20) # torch/nn/functional.py:1991:17
      %1242 : int = aten::len(%1240) # torch/nn/functional.py:1992:19
      %1243 : int = aten::sub(%1242, %18) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%1243, %19, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %1247 : int = aten::add(%i.42, %18) # torch/nn/functional.py:1993:27
          %1248 : int = aten::__getitem__(%1240, %1247) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %1248) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.167)
      %1250 : bool = aten::eq(%size_prods.165, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.112 : Tensor = aten::batch_norm(%x.110, %1238, %1239, %1236, %1237, %1235, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.7 : Tensor = aten::relu_(%x.112) # torch/nn/functional.py:1117:17
  %1253 : __torch__.torch.nn.modules.container.___torch_mangle_478.Sequential = prim::GetAttr[name="branch2"](%1220)
  %1254 : __torch__.torchvision.models.googlenet.___torch_mangle_474.BasicConv2d = prim::GetAttr[name="0"](%1253)
  %1255 : __torch__.torchvision.models.googlenet.___torch_mangle_477.BasicConv2d = prim::GetAttr[name="1"](%1253)
  %1256 : __torch__.torch.nn.modules.conv.___torch_mangle_472.Conv2d = prim::GetAttr[name="conv"](%1254)
  %1257 : Tensor = prim::GetAttr[name="weight"](%1256)
  %1258 : Tensor? = prim::GetAttr[name="bias"](%1256)
  %1259 : int[] = prim::ListConstruct(%25, %25)
  %1260 : int[] = prim::ListConstruct(%20, %20)
  %1261 : int[] = prim::ListConstruct(%25, %25)
  %x.113 : Tensor = aten::conv2d(%x.24, %1257, %1258, %1259, %1260, %1261, %25) # torch/nn/modules/conv.py:415:15
  %1263 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_473.BatchNorm2d = prim::GetAttr[name="bn"](%1254)
  %1264 : int = aten::dim(%x.113) # torch/nn/modules/batchnorm.py:276:11
  %1265 : bool = aten::ne(%1264, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1265) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1266 : bool = prim::GetAttr[name="training"](%1263)
   = prim::If(%1266) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1267 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1263)
      %1268 : Tensor = aten::add(%1267, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1263, %1268)
      -> ()
    block1():
      -> ()
  %1269 : bool = prim::GetAttr[name="training"](%1263)
  %1270 : Tensor = prim::GetAttr[name="running_mean"](%1263)
  %1271 : Tensor = prim::GetAttr[name="running_var"](%1263)
  %1272 : Tensor = prim::GetAttr[name="weight"](%1263)
  %1273 : Tensor = prim::GetAttr[name="bias"](%1263)
   = prim::If(%1269) # torch/nn/functional.py:2011:4
    block0():
      %1274 : int[] = aten::size(%x.113) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%1274, %20) # torch/nn/functional.py:1991:17
      %1276 : int = aten::len(%1274) # torch/nn/functional.py:1992:19
      %1277 : int = aten::sub(%1276, %18) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%1277, %19, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %1281 : int = aten::add(%i.43, %18) # torch/nn/functional.py:1993:27
          %1282 : int = aten::__getitem__(%1274, %1281) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %1282) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.171)
      %1284 : bool = aten::eq(%size_prods.169, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1284) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.114 : Tensor = aten::batch_norm(%x.113, %1272, %1273, %1270, %1271, %1269, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.21 : Tensor = aten::relu_(%x.114) # torch/nn/functional.py:1117:17
  %1287 : __torch__.torch.nn.modules.conv.___torch_mangle_475.Conv2d = prim::GetAttr[name="conv"](%1255)
  %1288 : Tensor = prim::GetAttr[name="weight"](%1287)
  %1289 : Tensor? = prim::GetAttr[name="bias"](%1287)
  %1290 : int[] = prim::ListConstruct(%25, %25)
  %1291 : int[] = prim::ListConstruct(%25, %25)
  %1292 : int[] = prim::ListConstruct(%25, %25)
  %x.115 : Tensor = aten::conv2d(%input.21, %1288, %1289, %1290, %1291, %1292, %25) # torch/nn/modules/conv.py:415:15
  %1294 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_476.BatchNorm2d = prim::GetAttr[name="bn"](%1255)
  %1295 : int = aten::dim(%x.115) # torch/nn/modules/batchnorm.py:276:11
  %1296 : bool = aten::ne(%1295, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1296) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1297 : bool = prim::GetAttr[name="training"](%1294)
   = prim::If(%1297) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1294)
      %1299 : Tensor = aten::add(%1298, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1294, %1299)
      -> ()
    block1():
      -> ()
  %1300 : bool = prim::GetAttr[name="training"](%1294)
  %1301 : Tensor = prim::GetAttr[name="running_mean"](%1294)
  %1302 : Tensor = prim::GetAttr[name="running_var"](%1294)
  %1303 : Tensor = prim::GetAttr[name="weight"](%1294)
  %1304 : Tensor = prim::GetAttr[name="bias"](%1294)
   = prim::If(%1300) # torch/nn/functional.py:2011:4
    block0():
      %1305 : int[] = aten::size(%x.115) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%1305, %20) # torch/nn/functional.py:1991:17
      %1307 : int = aten::len(%1305) # torch/nn/functional.py:1992:19
      %1308 : int = aten::sub(%1307, %18) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%1308, %19, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %1312 : int = aten::add(%i.44, %18) # torch/nn/functional.py:1993:27
          %1313 : int = aten::__getitem__(%1305, %1312) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %1313) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.175)
      %1315 : bool = aten::eq(%size_prods.173, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1315) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.116 : Tensor = aten::batch_norm(%x.115, %1303, %1304, %1301, %1302, %1300, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.7 : Tensor = aten::relu_(%x.116) # torch/nn/functional.py:1117:17
  %1318 : __torch__.torch.nn.modules.container.___torch_mangle_483.Sequential = prim::GetAttr[name="branch3"](%1220)
  %1319 : __torch__.torchvision.models.googlenet.___torch_mangle_480.BasicConv2d = prim::GetAttr[name="0"](%1318)
  %1320 : __torch__.torchvision.models.googlenet.___torch_mangle_482.BasicConv2d = prim::GetAttr[name="1"](%1318)
  %1321 : __torch__.torch.nn.modules.conv.___torch_mangle_479.Conv2d = prim::GetAttr[name="conv"](%1319)
  %1322 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1323 : Tensor? = prim::GetAttr[name="bias"](%1321)
  %1324 : int[] = prim::ListConstruct(%25, %25)
  %1325 : int[] = prim::ListConstruct(%20, %20)
  %1326 : int[] = prim::ListConstruct(%25, %25)
  %x.106 : Tensor = aten::conv2d(%x.24, %1322, %1323, %1324, %1325, %1326, %25) # torch/nn/modules/conv.py:415:15
  %1328 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%1319)
  %1329 : int = aten::dim(%x.106) # torch/nn/modules/batchnorm.py:276:11
  %1330 : bool = aten::ne(%1329, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1330) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1331 : bool = prim::GetAttr[name="training"](%1328)
   = prim::If(%1331) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1332 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1328)
      %1333 : Tensor = aten::add(%1332, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1328, %1333)
      -> ()
    block1():
      -> ()
  %1334 : bool = prim::GetAttr[name="training"](%1328)
  %1335 : Tensor = prim::GetAttr[name="running_mean"](%1328)
  %1336 : Tensor = prim::GetAttr[name="running_var"](%1328)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1328)
  %1338 : Tensor = prim::GetAttr[name="bias"](%1328)
   = prim::If(%1334) # torch/nn/functional.py:2011:4
    block0():
      %1339 : int[] = aten::size(%x.106) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%1339, %20) # torch/nn/functional.py:1991:17
      %1341 : int = aten::len(%1339) # torch/nn/functional.py:1992:19
      %1342 : int = aten::sub(%1341, %18) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%1342, %19, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %1346 : int = aten::add(%i.45, %18) # torch/nn/functional.py:1993:27
          %1347 : int = aten::__getitem__(%1339, %1346) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %1347) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.179)
      %1349 : bool = aten::eq(%size_prods.177, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1349) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.109 : Tensor = aten::batch_norm(%x.106, %1337, %1338, %1335, %1336, %1334, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.22 : Tensor = aten::relu_(%x.109) # torch/nn/functional.py:1117:17
  %1352 : __torch__.torch.nn.modules.conv.___torch_mangle_481.Conv2d = prim::GetAttr[name="conv"](%1320)
  %1353 : Tensor = prim::GetAttr[name="weight"](%1352)
  %1354 : Tensor? = prim::GetAttr[name="bias"](%1352)
  %1355 : int[] = prim::ListConstruct(%25, %25)
  %1356 : int[] = prim::ListConstruct(%25, %25)
  %1357 : int[] = prim::ListConstruct(%25, %25)
  %x.111 : Tensor = aten::conv2d(%input.22, %1353, %1354, %1355, %1356, %1357, %25) # torch/nn/modules/conv.py:415:15
  %1359 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%1320)
  %1360 : int = aten::dim(%x.111) # torch/nn/modules/batchnorm.py:276:11
  %1361 : bool = aten::ne(%1360, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1361) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1362 : bool = prim::GetAttr[name="training"](%1359)
   = prim::If(%1362) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1363 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1359)
      %1364 : Tensor = aten::add(%1363, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1359, %1364)
      -> ()
    block1():
      -> ()
  %1365 : bool = prim::GetAttr[name="training"](%1359)
  %1366 : Tensor = prim::GetAttr[name="running_mean"](%1359)
  %1367 : Tensor = prim::GetAttr[name="running_var"](%1359)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1359)
  %1369 : Tensor = prim::GetAttr[name="bias"](%1359)
   = prim::If(%1365) # torch/nn/functional.py:2011:4
    block0():
      %1370 : int[] = aten::size(%x.111) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%1370, %20) # torch/nn/functional.py:1991:17
      %1372 : int = aten::len(%1370) # torch/nn/functional.py:1992:19
      %1373 : int = aten::sub(%1372, %18) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%1373, %19, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %1377 : int = aten::add(%i.46, %18) # torch/nn/functional.py:1993:27
          %1378 : int = aten::__getitem__(%1370, %1377) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %1378) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.183)
      %1380 : bool = aten::eq(%size_prods.181, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1380) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.120 : Tensor = aten::batch_norm(%x.111, %1368, %1369, %1366, %1367, %1365, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.7 : Tensor = aten::relu_(%x.120) # torch/nn/functional.py:1117:17
  %1383 : __torch__.torch.nn.modules.container.___torch_mangle_464.Sequential = prim::GetAttr[name="branch4"](%1220)
  %1384 : __torch__.torchvision.models.googlenet.___torch_mangle_463.BasicConv2d = prim::GetAttr[name="1"](%1383)
  %1385 : int[] = prim::ListConstruct(%17, %17)
  %1386 : int[] = prim::ListConstruct(%25, %25)
  %1387 : int[] = prim::ListConstruct(%25, %25)
  %1388 : int[] = prim::ListConstruct(%25, %25)
  %input.23 : Tensor = aten::max_pool2d(%x.24, %1385, %1386, %1387, %1388, %19) # torch/nn/functional.py:575:11
  %1390 : __torch__.torch.nn.modules.conv.___torch_mangle_462.Conv2d = prim::GetAttr[name="conv"](%1384)
  %1391 : Tensor = prim::GetAttr[name="weight"](%1390)
  %1392 : Tensor? = prim::GetAttr[name="bias"](%1390)
  %1393 : int[] = prim::ListConstruct(%25, %25)
  %1394 : int[] = prim::ListConstruct(%20, %20)
  %1395 : int[] = prim::ListConstruct(%25, %25)
  %x.107 : Tensor = aten::conv2d(%input.23, %1391, %1392, %1393, %1394, %1395, %25) # torch/nn/modules/conv.py:415:15
  %1397 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_391.BatchNorm2d = prim::GetAttr[name="bn"](%1384)
  %1398 : int = aten::dim(%x.107) # torch/nn/modules/batchnorm.py:276:11
  %1399 : bool = aten::ne(%1398, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1399) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1400 : bool = prim::GetAttr[name="training"](%1397)
   = prim::If(%1400) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1401 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1397)
      %1402 : Tensor = aten::add(%1401, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1397, %1402)
      -> ()
    block1():
      -> ()
  %1403 : bool = prim::GetAttr[name="training"](%1397)
  %1404 : Tensor = prim::GetAttr[name="running_mean"](%1397)
  %1405 : Tensor = prim::GetAttr[name="running_var"](%1397)
  %1406 : Tensor = prim::GetAttr[name="weight"](%1397)
  %1407 : Tensor = prim::GetAttr[name="bias"](%1397)
   = prim::If(%1403) # torch/nn/functional.py:2011:4
    block0():
      %1408 : int[] = aten::size(%x.107) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%1408, %20) # torch/nn/functional.py:1991:17
      %1410 : int = aten::len(%1408) # torch/nn/functional.py:1992:19
      %1411 : int = aten::sub(%1410, %18) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%1411, %19, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %1415 : int = aten::add(%i.41, %18) # torch/nn/functional.py:1993:27
          %1416 : int = aten::__getitem__(%1408, %1415) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %1416) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.163)
      %1418 : bool = aten::eq(%size_prods.161, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1418) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.108 : Tensor = aten::batch_norm(%x.107, %1406, %1407, %1404, %1405, %1403, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.7 : Tensor = aten::relu_(%x.108) # torch/nn/functional.py:1117:17
  %outputs.8 : Tensor[] = prim::ListConstruct(%branch1.7, %branch2.7, %branch3.7, %branch4.7)
  %x.26 : Tensor = aten::cat(%outputs.8, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %1423 : bool = prim::GetAttr[name="training"](%self)
  %aux2 : Tensor? = prim::If(%1423) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:174:12
    block0():
      %1425 : __torch__.torchvision.models.googlenet.___torch_mangle_528.InceptionAux = prim::GetAttr[name="aux2"](%self)
      %1426 : int[] = prim::ListConstruct(%22, %22)
      %1427 : int[] = aten::size(%x.26) # torch/nn/functional.py:925:51
      %1428 : int = aten::len(%1427) # <string>:5:9
      %1429 : bool = aten::gt(%1428, %18) # <string>:5:9
       = prim::If(%1429) # <string>:5:2
        block0():
          -> ()
        block1():
           = prim::RaiseException(%21) # <string>:5:2
          -> ()
      %x.117 : Tensor = aten::adaptive_avg_pool2d(%x.26, %1426) # torch/nn/functional.py:926:11
      %1431 : __torch__.torchvision.models.googlenet.___torch_mangle_499.BasicConv2d = prim::GetAttr[name="conv"](%1425)
      %1432 : __torch__.torch.nn.modules.conv.___torch_mangle_498.Conv2d = prim::GetAttr[name="conv"](%1431)
      %1433 : Tensor = prim::GetAttr[name="weight"](%1432)
      %1434 : Tensor? = prim::GetAttr[name="bias"](%1432)
      %1435 : int[] = prim::ListConstruct(%25, %25)
      %1436 : int[] = prim::ListConstruct(%20, %20)
      %1437 : int[] = prim::ListConstruct(%25, %25)
      %x.121 : Tensor = aten::conv2d(%x.117, %1433, %1434, %1435, %1436, %1437, %25) # torch/nn/modules/conv.py:415:15
      %1439 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1431)
      %1440 : int = aten::dim(%x.121) # torch/nn/modules/batchnorm.py:276:11
      %1441 : bool = aten::ne(%1440, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1441) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1442 : bool = prim::GetAttr[name="training"](%1439)
       = prim::If(%1442) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1443 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1439)
          %1444 : Tensor = aten::add(%1443, %25, %25) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1439, %1444)
          -> ()
        block1():
          -> ()
      %1445 : bool = prim::GetAttr[name="training"](%1439)
      %1446 : Tensor = prim::GetAttr[name="running_mean"](%1439)
      %1447 : Tensor = prim::GetAttr[name="running_var"](%1439)
      %1448 : Tensor = prim::GetAttr[name="weight"](%1439)
      %1449 : Tensor = prim::GetAttr[name="bias"](%1439)
       = prim::If(%1445) # torch/nn/functional.py:2011:4
        block0():
          %1450 : int[] = aten::size(%x.121) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%1450, %20) # torch/nn/functional.py:1991:17
          %1452 : int = aten::len(%1450) # torch/nn/functional.py:1992:19
          %1453 : int = aten::sub(%1452, %18) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%1453, %19, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %1457 : int = aten::add(%i.47, %18) # torch/nn/functional.py:1993:27
              %1458 : int = aten::__getitem__(%1450, %1457) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %1458) # torch/nn/functional.py:1993:8
              -> (%19, %size_prods.187)
          %1460 : bool = aten::eq(%size_prods.185, %25) # torch/nn/functional.py:1994:7
           = prim::If(%1460) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %x.122 : Tensor = aten::batch_norm(%x.121, %1448, %1449, %1446, %1447, %1445, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
      %x.118 : Tensor = aten::relu_(%x.122) # torch/nn/functional.py:1117:17
      %x.119 : Tensor = aten::flatten(%x.118, %25, %27) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:272:12
      %1464 : __torch__.torch.nn.modules.linear.___torch_mangle_527.Linear = prim::GetAttr[name="fc1"](%1425)
      %1465 : Tensor = prim::GetAttr[name="weight"](%1464)
      %1466 : Tensor = prim::GetAttr[name="bias"](%1464)
      %1467 : int = aten::dim(%x.119) # torch/nn/functional.py:1672:7
      %1468 : bool = aten::eq(%1467, %18) # torch/nn/functional.py:1672:7
      %ret.2 : Tensor = prim::If(%1468) # torch/nn/functional.py:1672:4
        block0():
          %1470 : Tensor = aten::t(%1465) # torch/nn/functional.py:1674:39
          %ret.3 : Tensor = aten::addmm(%1466, %x.119, %1470, %25, %25) # torch/nn/functional.py:1674:14
          -> (%ret.3)
        block1():
          %1472 : Tensor = aten::t(%1465) # torch/nn/functional.py:1676:30
          %output.2 : Tensor = aten::matmul(%x.119, %1472) # torch/nn/functional.py:1676:17
          %output.4 : Tensor = aten::add_(%output.2, %1466, %25) # torch/nn/functional.py:1678:12
          -> (%output.4)
      %result.1 : Tensor = aten::relu_(%ret.2) # torch/nn/functional.py:1117:17
      %1476 : bool = prim::GetAttr[name="training"](%1425)
      %1477 : Tensor = aten::dropout(%result.1, %16, %1476) # torch/nn/functional.py:973:17
      %1478 : __torch__.torch.nn.modules.linear.___torch_mangle_161.Linear = prim::GetAttr[name="fc2"](%1425)
      %1479 : Tensor = prim::GetAttr[name="weight"](%1478)
      %1480 : Tensor = prim::GetAttr[name="bias"](%1478)
      %1481 : int = aten::dim(%1477) # torch/nn/functional.py:1672:7
      %1482 : bool = aten::eq(%1481, %18) # torch/nn/functional.py:1672:7
      %aux2.2 : Tensor = prim::If(%1482) # torch/nn/functional.py:1672:4
        block0():
          %1484 : Tensor = aten::t(%1479) # torch/nn/functional.py:1674:39
          %ret.7 : Tensor = aten::addmm(%1480, %1477, %1484, %25, %25) # torch/nn/functional.py:1674:14
          -> (%ret.7)
        block1():
          %1486 : Tensor = aten::t(%1479) # torch/nn/functional.py:1676:30
          %output.9 : Tensor = aten::matmul(%1477, %1486) # torch/nn/functional.py:1676:17
          %output.10 : Tensor = aten::add_(%output.9, %1480, %25) # torch/nn/functional.py:1678:12
          -> (%output.10)
      -> (%aux2.2)
    block1():
      -> (%aux1.1)
  %1489 : __torch__.torchvision.models.googlenet.___torch_mangle_501.Inception = prim::GetAttr[name="inception4e"](%self)
  %1490 : __torch__.torchvision.models.googlenet.___torch_mangle_486.BasicConv2d = prim::GetAttr[name="branch1"](%1489)
  %1491 : __torch__.torch.nn.modules.conv.___torch_mangle_485.Conv2d = prim::GetAttr[name="conv"](%1490)
  %1492 : Tensor = prim::GetAttr[name="weight"](%1491)
  %1493 : Tensor? = prim::GetAttr[name="bias"](%1491)
  %1494 : int[] = prim::ListConstruct(%25, %25)
  %1495 : int[] = prim::ListConstruct(%20, %20)
  %1496 : int[] = prim::ListConstruct(%25, %25)
  %x.123 : Tensor = aten::conv2d(%x.26, %1492, %1493, %1494, %1495, %1496, %25) # torch/nn/modules/conv.py:415:15
  %1498 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_468.BatchNorm2d = prim::GetAttr[name="bn"](%1490)
  %1499 : int = aten::dim(%x.123) # torch/nn/modules/batchnorm.py:276:11
  %1500 : bool = aten::ne(%1499, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1500) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1501 : bool = prim::GetAttr[name="training"](%1498)
   = prim::If(%1501) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1502 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1498)
      %1503 : Tensor = aten::add(%1502, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1498, %1503)
      -> ()
    block1():
      -> ()
  %1504 : bool = prim::GetAttr[name="training"](%1498)
  %1505 : Tensor = prim::GetAttr[name="running_mean"](%1498)
  %1506 : Tensor = prim::GetAttr[name="running_var"](%1498)
  %1507 : Tensor = prim::GetAttr[name="weight"](%1498)
  %1508 : Tensor = prim::GetAttr[name="bias"](%1498)
   = prim::If(%1504) # torch/nn/functional.py:2011:4
    block0():
      %1509 : int[] = aten::size(%x.123) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%1509, %20) # torch/nn/functional.py:1991:17
      %1511 : int = aten::len(%1509) # torch/nn/functional.py:1992:19
      %1512 : int = aten::sub(%1511, %18) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%1512, %19, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %1516 : int = aten::add(%i.48, %18) # torch/nn/functional.py:1993:27
          %1517 : int = aten::__getitem__(%1509, %1516) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %1517) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.191)
      %1519 : bool = aten::eq(%size_prods.189, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1519) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.124 : Tensor = aten::batch_norm(%x.123, %1507, %1508, %1505, %1506, %1504, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.8 : Tensor = aten::relu_(%x.124) # torch/nn/functional.py:1117:17
  %1522 : __torch__.torch.nn.modules.container.___torch_mangle_492.Sequential = prim::GetAttr[name="branch2"](%1489)
  %1523 : __torch__.torchvision.models.googlenet.___torch_mangle_488.BasicConv2d = prim::GetAttr[name="0"](%1522)
  %1524 : __torch__.torchvision.models.googlenet.___torch_mangle_491.BasicConv2d = prim::GetAttr[name="1"](%1522)
  %1525 : __torch__.torch.nn.modules.conv.___torch_mangle_487.Conv2d = prim::GetAttr[name="conv"](%1523)
  %1526 : Tensor = prim::GetAttr[name="weight"](%1525)
  %1527 : Tensor? = prim::GetAttr[name="bias"](%1525)
  %1528 : int[] = prim::ListConstruct(%25, %25)
  %1529 : int[] = prim::ListConstruct(%20, %20)
  %1530 : int[] = prim::ListConstruct(%25, %25)
  %x.125 : Tensor = aten::conv2d(%x.26, %1526, %1527, %1528, %1529, %1530, %25) # torch/nn/modules/conv.py:415:15
  %1532 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1523)
  %1533 : int = aten::dim(%x.125) # torch/nn/modules/batchnorm.py:276:11
  %1534 : bool = aten::ne(%1533, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1534) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1535 : bool = prim::GetAttr[name="training"](%1532)
   = prim::If(%1535) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1536 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1532)
      %1537 : Tensor = aten::add(%1536, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1532, %1537)
      -> ()
    block1():
      -> ()
  %1538 : bool = prim::GetAttr[name="training"](%1532)
  %1539 : Tensor = prim::GetAttr[name="running_mean"](%1532)
  %1540 : Tensor = prim::GetAttr[name="running_var"](%1532)
  %1541 : Tensor = prim::GetAttr[name="weight"](%1532)
  %1542 : Tensor = prim::GetAttr[name="bias"](%1532)
   = prim::If(%1538) # torch/nn/functional.py:2011:4
    block0():
      %1543 : int[] = aten::size(%x.125) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%1543, %20) # torch/nn/functional.py:1991:17
      %1545 : int = aten::len(%1543) # torch/nn/functional.py:1992:19
      %1546 : int = aten::sub(%1545, %18) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%1546, %19, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %1550 : int = aten::add(%i.49, %18) # torch/nn/functional.py:1993:27
          %1551 : int = aten::__getitem__(%1543, %1550) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %1551) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.195)
      %1553 : bool = aten::eq(%size_prods.193, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1553) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.126 : Tensor = aten::batch_norm(%x.125, %1541, %1542, %1539, %1540, %1538, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.24 : Tensor = aten::relu_(%x.126) # torch/nn/functional.py:1117:17
  %1556 : __torch__.torch.nn.modules.conv.___torch_mangle_489.Conv2d = prim::GetAttr[name="conv"](%1524)
  %1557 : Tensor = prim::GetAttr[name="weight"](%1556)
  %1558 : Tensor? = prim::GetAttr[name="bias"](%1556)
  %1559 : int[] = prim::ListConstruct(%25, %25)
  %1560 : int[] = prim::ListConstruct(%25, %25)
  %1561 : int[] = prim::ListConstruct(%25, %25)
  %x.127 : Tensor = aten::conv2d(%input.24, %1557, %1558, %1559, %1560, %1561, %25) # torch/nn/modules/conv.py:415:15
  %1563 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_490.BatchNorm2d = prim::GetAttr[name="bn"](%1524)
  %1564 : int = aten::dim(%x.127) # torch/nn/modules/batchnorm.py:276:11
  %1565 : bool = aten::ne(%1564, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1565) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1566 : bool = prim::GetAttr[name="training"](%1563)
   = prim::If(%1566) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1567 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1563)
      %1568 : Tensor = aten::add(%1567, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1563, %1568)
      -> ()
    block1():
      -> ()
  %1569 : bool = prim::GetAttr[name="training"](%1563)
  %1570 : Tensor = prim::GetAttr[name="running_mean"](%1563)
  %1571 : Tensor = prim::GetAttr[name="running_var"](%1563)
  %1572 : Tensor = prim::GetAttr[name="weight"](%1563)
  %1573 : Tensor = prim::GetAttr[name="bias"](%1563)
   = prim::If(%1569) # torch/nn/functional.py:2011:4
    block0():
      %1574 : int[] = aten::size(%x.127) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%1574, %20) # torch/nn/functional.py:1991:17
      %1576 : int = aten::len(%1574) # torch/nn/functional.py:1992:19
      %1577 : int = aten::sub(%1576, %18) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%1577, %19, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %1581 : int = aten::add(%i.50, %18) # torch/nn/functional.py:1993:27
          %1582 : int = aten::__getitem__(%1574, %1581) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %1582) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.199)
      %1584 : bool = aten::eq(%size_prods.197, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1584) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.128 : Tensor = aten::batch_norm(%x.127, %1572, %1573, %1570, %1571, %1569, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.8 : Tensor = aten::relu_(%x.128) # torch/nn/functional.py:1117:17
  %1587 : __torch__.torch.nn.modules.container.___torch_mangle_497.Sequential = prim::GetAttr[name="branch3"](%1489)
  %1588 : __torch__.torchvision.models.googlenet.___torch_mangle_494.BasicConv2d = prim::GetAttr[name="0"](%1587)
  %1589 : __torch__.torchvision.models.googlenet.___torch_mangle_496.BasicConv2d = prim::GetAttr[name="1"](%1587)
  %1590 : __torch__.torch.nn.modules.conv.___torch_mangle_493.Conv2d = prim::GetAttr[name="conv"](%1588)
  %1591 : Tensor = prim::GetAttr[name="weight"](%1590)
  %1592 : Tensor? = prim::GetAttr[name="bias"](%1590)
  %1593 : int[] = prim::ListConstruct(%25, %25)
  %1594 : int[] = prim::ListConstruct(%20, %20)
  %1595 : int[] = prim::ListConstruct(%25, %25)
  %x.129 : Tensor = aten::conv2d(%x.26, %1591, %1592, %1593, %1594, %1595, %25) # torch/nn/modules/conv.py:415:15
  %1597 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%1588)
  %1598 : int = aten::dim(%x.129) # torch/nn/modules/batchnorm.py:276:11
  %1599 : bool = aten::ne(%1598, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1599) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1600 : bool = prim::GetAttr[name="training"](%1597)
   = prim::If(%1600) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1601 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1597)
      %1602 : Tensor = aten::add(%1601, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1597, %1602)
      -> ()
    block1():
      -> ()
  %1603 : bool = prim::GetAttr[name="training"](%1597)
  %1604 : Tensor = prim::GetAttr[name="running_mean"](%1597)
  %1605 : Tensor = prim::GetAttr[name="running_var"](%1597)
  %1606 : Tensor = prim::GetAttr[name="weight"](%1597)
  %1607 : Tensor = prim::GetAttr[name="bias"](%1597)
   = prim::If(%1603) # torch/nn/functional.py:2011:4
    block0():
      %1608 : int[] = aten::size(%x.129) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%1608, %20) # torch/nn/functional.py:1991:17
      %1610 : int = aten::len(%1608) # torch/nn/functional.py:1992:19
      %1611 : int = aten::sub(%1610, %18) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%1611, %19, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %1615 : int = aten::add(%i.51, %18) # torch/nn/functional.py:1993:27
          %1616 : int = aten::__getitem__(%1608, %1615) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %1616) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.203)
      %1618 : bool = aten::eq(%size_prods.201, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1618) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.130 : Tensor = aten::batch_norm(%x.129, %1606, %1607, %1604, %1605, %1603, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.25 : Tensor = aten::relu_(%x.130) # torch/nn/functional.py:1117:17
  %1621 : __torch__.torch.nn.modules.conv.___torch_mangle_495.Conv2d = prim::GetAttr[name="conv"](%1589)
  %1622 : Tensor = prim::GetAttr[name="weight"](%1621)
  %1623 : Tensor? = prim::GetAttr[name="bias"](%1621)
  %1624 : int[] = prim::ListConstruct(%25, %25)
  %1625 : int[] = prim::ListConstruct(%25, %25)
  %1626 : int[] = prim::ListConstruct(%25, %25)
  %x.131 : Tensor = aten::conv2d(%input.25, %1622, %1623, %1624, %1625, %1626, %25) # torch/nn/modules/conv.py:415:15
  %1628 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1589)
  %1629 : int = aten::dim(%x.131) # torch/nn/modules/batchnorm.py:276:11
  %1630 : bool = aten::ne(%1629, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1630) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1631 : bool = prim::GetAttr[name="training"](%1628)
   = prim::If(%1631) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1632 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1628)
      %1633 : Tensor = aten::add(%1632, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1628, %1633)
      -> ()
    block1():
      -> ()
  %1634 : bool = prim::GetAttr[name="training"](%1628)
  %1635 : Tensor = prim::GetAttr[name="running_mean"](%1628)
  %1636 : Tensor = prim::GetAttr[name="running_var"](%1628)
  %1637 : Tensor = prim::GetAttr[name="weight"](%1628)
  %1638 : Tensor = prim::GetAttr[name="bias"](%1628)
   = prim::If(%1634) # torch/nn/functional.py:2011:4
    block0():
      %1639 : int[] = aten::size(%x.131) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%1639, %20) # torch/nn/functional.py:1991:17
      %1641 : int = aten::len(%1639) # torch/nn/functional.py:1992:19
      %1642 : int = aten::sub(%1641, %18) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%1642, %19, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %1646 : int = aten::add(%i.52, %18) # torch/nn/functional.py:1993:27
          %1647 : int = aten::__getitem__(%1639, %1646) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %1647) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.207)
      %1649 : bool = aten::eq(%size_prods.205, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1649) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.132 : Tensor = aten::batch_norm(%x.131, %1637, %1638, %1635, %1636, %1634, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.8 : Tensor = aten::relu_(%x.132) # torch/nn/functional.py:1117:17
  %1652 : __torch__.torch.nn.modules.container.___torch_mangle_500.Sequential = prim::GetAttr[name="branch4"](%1489)
  %1653 : __torch__.torchvision.models.googlenet.___torch_mangle_499.BasicConv2d = prim::GetAttr[name="1"](%1652)
  %1654 : int[] = prim::ListConstruct(%17, %17)
  %1655 : int[] = prim::ListConstruct(%25, %25)
  %1656 : int[] = prim::ListConstruct(%25, %25)
  %1657 : int[] = prim::ListConstruct(%25, %25)
  %input.26 : Tensor = aten::max_pool2d(%x.26, %1654, %1655, %1656, %1657, %19) # torch/nn/functional.py:575:11
  %1659 : __torch__.torch.nn.modules.conv.___torch_mangle_498.Conv2d = prim::GetAttr[name="conv"](%1653)
  %1660 : Tensor = prim::GetAttr[name="weight"](%1659)
  %1661 : Tensor? = prim::GetAttr[name="bias"](%1659)
  %1662 : int[] = prim::ListConstruct(%25, %25)
  %1663 : int[] = prim::ListConstruct(%20, %20)
  %1664 : int[] = prim::ListConstruct(%25, %25)
  %x.133 : Tensor = aten::conv2d(%input.26, %1660, %1661, %1662, %1663, %1664, %25) # torch/nn/modules/conv.py:415:15
  %1666 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1653)
  %1667 : int = aten::dim(%x.133) # torch/nn/modules/batchnorm.py:276:11
  %1668 : bool = aten::ne(%1667, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1668) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1669 : bool = prim::GetAttr[name="training"](%1666)
   = prim::If(%1669) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1670 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1666)
      %1671 : Tensor = aten::add(%1670, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1666, %1671)
      -> ()
    block1():
      -> ()
  %1672 : bool = prim::GetAttr[name="training"](%1666)
  %1673 : Tensor = prim::GetAttr[name="running_mean"](%1666)
  %1674 : Tensor = prim::GetAttr[name="running_var"](%1666)
  %1675 : Tensor = prim::GetAttr[name="weight"](%1666)
  %1676 : Tensor = prim::GetAttr[name="bias"](%1666)
   = prim::If(%1672) # torch/nn/functional.py:2011:4
    block0():
      %1677 : int[] = aten::size(%x.133) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%1677, %20) # torch/nn/functional.py:1991:17
      %1679 : int = aten::len(%1677) # torch/nn/functional.py:1992:19
      %1680 : int = aten::sub(%1679, %18) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%1680, %19, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %1684 : int = aten::add(%i.53, %18) # torch/nn/functional.py:1993:27
          %1685 : int = aten::__getitem__(%1677, %1684) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %1685) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.211)
      %1687 : bool = aten::eq(%size_prods.209, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1687) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.134 : Tensor = aten::batch_norm(%x.133, %1675, %1676, %1673, %1674, %1672, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.8 : Tensor = aten::relu_(%x.134) # torch/nn/functional.py:1117:17
  %outputs.9 : Tensor[] = prim::ListConstruct(%branch1.8, %branch2.8, %branch3.8, %branch4.8)
  %x.29 : Tensor = aten::cat(%outputs.9, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %1692 : int[] = prim::ListConstruct(%18, %18)
  %1693 : int[] = prim::ListConstruct(%18, %18)
  %1694 : int[] = prim::ListConstruct(%20, %20)
  %1695 : int[] = prim::ListConstruct(%25, %25)
  %x.31 : Tensor = aten::max_pool2d(%x.29, %1692, %1693, %1694, %1695, %19) # torch/nn/functional.py:575:11
  %1697 : __torch__.torchvision.models.googlenet.___torch_mangle_513.Inception = prim::GetAttr[name="inception5a"](%self)
  %1698 : __torch__.torchvision.models.googlenet.___torch_mangle_504.BasicConv2d = prim::GetAttr[name="branch1"](%1697)
  %1699 : __torch__.torch.nn.modules.conv.___torch_mangle_503.Conv2d = prim::GetAttr[name="conv"](%1698)
  %1700 : Tensor = prim::GetAttr[name="weight"](%1699)
  %1701 : Tensor? = prim::GetAttr[name="bias"](%1699)
  %1702 : int[] = prim::ListConstruct(%25, %25)
  %1703 : int[] = prim::ListConstruct(%20, %20)
  %1704 : int[] = prim::ListConstruct(%25, %25)
  %x.135 : Tensor = aten::conv2d(%x.31, %1700, %1701, %1702, %1703, %1704, %25) # torch/nn/modules/conv.py:415:15
  %1706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_468.BatchNorm2d = prim::GetAttr[name="bn"](%1698)
  %1707 : int = aten::dim(%x.135) # torch/nn/modules/batchnorm.py:276:11
  %1708 : bool = aten::ne(%1707, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1708) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1709 : bool = prim::GetAttr[name="training"](%1706)
   = prim::If(%1709) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1710 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1706)
      %1711 : Tensor = aten::add(%1710, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1706, %1711)
      -> ()
    block1():
      -> ()
  %1712 : bool = prim::GetAttr[name="training"](%1706)
  %1713 : Tensor = prim::GetAttr[name="running_mean"](%1706)
  %1714 : Tensor = prim::GetAttr[name="running_var"](%1706)
  %1715 : Tensor = prim::GetAttr[name="weight"](%1706)
  %1716 : Tensor = prim::GetAttr[name="bias"](%1706)
   = prim::If(%1712) # torch/nn/functional.py:2011:4
    block0():
      %1717 : int[] = aten::size(%x.135) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%1717, %20) # torch/nn/functional.py:1991:17
      %1719 : int = aten::len(%1717) # torch/nn/functional.py:1992:19
      %1720 : int = aten::sub(%1719, %18) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%1720, %19, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %1724 : int = aten::add(%i.54, %18) # torch/nn/functional.py:1993:27
          %1725 : int = aten::__getitem__(%1717, %1724) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %1725) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.215)
      %1727 : bool = aten::eq(%size_prods.213, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1727) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.136 : Tensor = aten::batch_norm(%x.135, %1715, %1716, %1713, %1714, %1712, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.9 : Tensor = aten::relu_(%x.136) # torch/nn/functional.py:1117:17
  %1730 : __torch__.torch.nn.modules.container.___torch_mangle_507.Sequential = prim::GetAttr[name="branch2"](%1697)
  %1731 : __torch__.torchvision.models.googlenet.___torch_mangle_506.BasicConv2d = prim::GetAttr[name="0"](%1730)
  %1732 : __torch__.torchvision.models.googlenet.___torch_mangle_491.BasicConv2d = prim::GetAttr[name="1"](%1730)
  %1733 : __torch__.torch.nn.modules.conv.___torch_mangle_505.Conv2d = prim::GetAttr[name="conv"](%1731)
  %1734 : Tensor = prim::GetAttr[name="weight"](%1733)
  %1735 : Tensor? = prim::GetAttr[name="bias"](%1733)
  %1736 : int[] = prim::ListConstruct(%25, %25)
  %1737 : int[] = prim::ListConstruct(%20, %20)
  %1738 : int[] = prim::ListConstruct(%25, %25)
  %x.137 : Tensor = aten::conv2d(%x.31, %1734, %1735, %1736, %1737, %1738, %25) # torch/nn/modules/conv.py:415:15
  %1740 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_447.BatchNorm2d = prim::GetAttr[name="bn"](%1731)
  %1741 : int = aten::dim(%x.137) # torch/nn/modules/batchnorm.py:276:11
  %1742 : bool = aten::ne(%1741, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1742) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1743 : bool = prim::GetAttr[name="training"](%1740)
   = prim::If(%1743) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1744 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1740)
      %1745 : Tensor = aten::add(%1744, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1740, %1745)
      -> ()
    block1():
      -> ()
  %1746 : bool = prim::GetAttr[name="training"](%1740)
  %1747 : Tensor = prim::GetAttr[name="running_mean"](%1740)
  %1748 : Tensor = prim::GetAttr[name="running_var"](%1740)
  %1749 : Tensor = prim::GetAttr[name="weight"](%1740)
  %1750 : Tensor = prim::GetAttr[name="bias"](%1740)
   = prim::If(%1746) # torch/nn/functional.py:2011:4
    block0():
      %1751 : int[] = aten::size(%x.137) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%1751, %20) # torch/nn/functional.py:1991:17
      %1753 : int = aten::len(%1751) # torch/nn/functional.py:1992:19
      %1754 : int = aten::sub(%1753, %18) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%1754, %19, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %1758 : int = aten::add(%i.55, %18) # torch/nn/functional.py:1993:27
          %1759 : int = aten::__getitem__(%1751, %1758) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %1759) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.219)
      %1761 : bool = aten::eq(%size_prods.217, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1761) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.138 : Tensor = aten::batch_norm(%x.137, %1749, %1750, %1747, %1748, %1746, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.27 : Tensor = aten::relu_(%x.138) # torch/nn/functional.py:1117:17
  %1764 : __torch__.torch.nn.modules.conv.___torch_mangle_489.Conv2d = prim::GetAttr[name="conv"](%1732)
  %1765 : Tensor = prim::GetAttr[name="weight"](%1764)
  %1766 : Tensor? = prim::GetAttr[name="bias"](%1764)
  %1767 : int[] = prim::ListConstruct(%25, %25)
  %1768 : int[] = prim::ListConstruct(%25, %25)
  %1769 : int[] = prim::ListConstruct(%25, %25)
  %x.139 : Tensor = aten::conv2d(%input.27, %1765, %1766, %1767, %1768, %1769, %25) # torch/nn/modules/conv.py:415:15
  %1771 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_490.BatchNorm2d = prim::GetAttr[name="bn"](%1732)
  %1772 : int = aten::dim(%x.139) # torch/nn/modules/batchnorm.py:276:11
  %1773 : bool = aten::ne(%1772, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1773) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1774 : bool = prim::GetAttr[name="training"](%1771)
   = prim::If(%1774) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1775 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1771)
      %1776 : Tensor = aten::add(%1775, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1771, %1776)
      -> ()
    block1():
      -> ()
  %1777 : bool = prim::GetAttr[name="training"](%1771)
  %1778 : Tensor = prim::GetAttr[name="running_mean"](%1771)
  %1779 : Tensor = prim::GetAttr[name="running_var"](%1771)
  %1780 : Tensor = prim::GetAttr[name="weight"](%1771)
  %1781 : Tensor = prim::GetAttr[name="bias"](%1771)
   = prim::If(%1777) # torch/nn/functional.py:2011:4
    block0():
      %1782 : int[] = aten::size(%x.139) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%1782, %20) # torch/nn/functional.py:1991:17
      %1784 : int = aten::len(%1782) # torch/nn/functional.py:1992:19
      %1785 : int = aten::sub(%1784, %18) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%1785, %19, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %1789 : int = aten::add(%i.56, %18) # torch/nn/functional.py:1993:27
          %1790 : int = aten::__getitem__(%1782, %1789) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %1790) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.223)
      %1792 : bool = aten::eq(%size_prods.221, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1792) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.140 : Tensor = aten::batch_norm(%x.139, %1780, %1781, %1778, %1779, %1777, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.9 : Tensor = aten::relu_(%x.140) # torch/nn/functional.py:1117:17
  %1795 : __torch__.torch.nn.modules.container.___torch_mangle_510.Sequential = prim::GetAttr[name="branch3"](%1697)
  %1796 : __torch__.torchvision.models.googlenet.___torch_mangle_509.BasicConv2d = prim::GetAttr[name="0"](%1795)
  %1797 : __torch__.torchvision.models.googlenet.___torch_mangle_496.BasicConv2d = prim::GetAttr[name="1"](%1795)
  %1798 : __torch__.torch.nn.modules.conv.___torch_mangle_508.Conv2d = prim::GetAttr[name="conv"](%1796)
  %1799 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1800 : Tensor? = prim::GetAttr[name="bias"](%1798)
  %1801 : int[] = prim::ListConstruct(%25, %25)
  %1802 : int[] = prim::ListConstruct(%20, %20)
  %1803 : int[] = prim::ListConstruct(%25, %25)
  %x.141 : Tensor = aten::conv2d(%x.31, %1799, %1800, %1801, %1802, %1803, %25) # torch/nn/modules/conv.py:415:15
  %1805 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_410.BatchNorm2d = prim::GetAttr[name="bn"](%1796)
  %1806 : int = aten::dim(%x.141) # torch/nn/modules/batchnorm.py:276:11
  %1807 : bool = aten::ne(%1806, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1807) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1808 : bool = prim::GetAttr[name="training"](%1805)
   = prim::If(%1808) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1809 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1805)
      %1810 : Tensor = aten::add(%1809, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1805, %1810)
      -> ()
    block1():
      -> ()
  %1811 : bool = prim::GetAttr[name="training"](%1805)
  %1812 : Tensor = prim::GetAttr[name="running_mean"](%1805)
  %1813 : Tensor = prim::GetAttr[name="running_var"](%1805)
  %1814 : Tensor = prim::GetAttr[name="weight"](%1805)
  %1815 : Tensor = prim::GetAttr[name="bias"](%1805)
   = prim::If(%1811) # torch/nn/functional.py:2011:4
    block0():
      %1816 : int[] = aten::size(%x.141) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%1816, %20) # torch/nn/functional.py:1991:17
      %1818 : int = aten::len(%1816) # torch/nn/functional.py:1992:19
      %1819 : int = aten::sub(%1818, %18) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%1819, %19, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %1823 : int = aten::add(%i.57, %18) # torch/nn/functional.py:1993:27
          %1824 : int = aten::__getitem__(%1816, %1823) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %1824) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.227)
      %1826 : bool = aten::eq(%size_prods.225, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1826) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.142 : Tensor = aten::batch_norm(%x.141, %1814, %1815, %1812, %1813, %1811, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.28 : Tensor = aten::relu_(%x.142) # torch/nn/functional.py:1117:17
  %1829 : __torch__.torch.nn.modules.conv.___torch_mangle_495.Conv2d = prim::GetAttr[name="conv"](%1797)
  %1830 : Tensor = prim::GetAttr[name="weight"](%1829)
  %1831 : Tensor? = prim::GetAttr[name="bias"](%1829)
  %1832 : int[] = prim::ListConstruct(%25, %25)
  %1833 : int[] = prim::ListConstruct(%25, %25)
  %1834 : int[] = prim::ListConstruct(%25, %25)
  %x.143 : Tensor = aten::conv2d(%input.28, %1830, %1831, %1832, %1833, %1834, %25) # torch/nn/modules/conv.py:415:15
  %1836 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1797)
  %1837 : int = aten::dim(%x.143) # torch/nn/modules/batchnorm.py:276:11
  %1838 : bool = aten::ne(%1837, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1838) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1839 : bool = prim::GetAttr[name="training"](%1836)
   = prim::If(%1839) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1840 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1836)
      %1841 : Tensor = aten::add(%1840, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1836, %1841)
      -> ()
    block1():
      -> ()
  %1842 : bool = prim::GetAttr[name="training"](%1836)
  %1843 : Tensor = prim::GetAttr[name="running_mean"](%1836)
  %1844 : Tensor = prim::GetAttr[name="running_var"](%1836)
  %1845 : Tensor = prim::GetAttr[name="weight"](%1836)
  %1846 : Tensor = prim::GetAttr[name="bias"](%1836)
   = prim::If(%1842) # torch/nn/functional.py:2011:4
    block0():
      %1847 : int[] = aten::size(%x.143) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%1847, %20) # torch/nn/functional.py:1991:17
      %1849 : int = aten::len(%1847) # torch/nn/functional.py:1992:19
      %1850 : int = aten::sub(%1849, %18) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%1850, %19, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %1854 : int = aten::add(%i.58, %18) # torch/nn/functional.py:1993:27
          %1855 : int = aten::__getitem__(%1847, %1854) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %1855) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.231)
      %1857 : bool = aten::eq(%size_prods.229, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1857) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.144 : Tensor = aten::batch_norm(%x.143, %1845, %1846, %1843, %1844, %1842, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.9 : Tensor = aten::relu_(%x.144) # torch/nn/functional.py:1117:17
  %1860 : __torch__.torch.nn.modules.container.___torch_mangle_512.Sequential = prim::GetAttr[name="branch4"](%1697)
  %1861 : __torch__.torchvision.models.googlenet.___torch_mangle_511.BasicConv2d = prim::GetAttr[name="1"](%1860)
  %1862 : int[] = prim::ListConstruct(%17, %17)
  %1863 : int[] = prim::ListConstruct(%25, %25)
  %1864 : int[] = prim::ListConstruct(%25, %25)
  %1865 : int[] = prim::ListConstruct(%25, %25)
  %input.29 : Tensor = aten::max_pool2d(%x.31, %1862, %1863, %1864, %1865, %19) # torch/nn/functional.py:575:11
  %1867 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv"](%1861)
  %1868 : Tensor = prim::GetAttr[name="weight"](%1867)
  %1869 : Tensor? = prim::GetAttr[name="bias"](%1867)
  %1870 : int[] = prim::ListConstruct(%25, %25)
  %1871 : int[] = prim::ListConstruct(%20, %20)
  %1872 : int[] = prim::ListConstruct(%25, %25)
  %x.145 : Tensor = aten::conv2d(%input.29, %1868, %1869, %1870, %1871, %1872, %25) # torch/nn/modules/conv.py:415:15
  %1874 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%1861)
  %1875 : int = aten::dim(%x.145) # torch/nn/modules/batchnorm.py:276:11
  %1876 : bool = aten::ne(%1875, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1876) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1877 : bool = prim::GetAttr[name="training"](%1874)
   = prim::If(%1877) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1878 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1874)
      %1879 : Tensor = aten::add(%1878, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1874, %1879)
      -> ()
    block1():
      -> ()
  %1880 : bool = prim::GetAttr[name="training"](%1874)
  %1881 : Tensor = prim::GetAttr[name="running_mean"](%1874)
  %1882 : Tensor = prim::GetAttr[name="running_var"](%1874)
  %1883 : Tensor = prim::GetAttr[name="weight"](%1874)
  %1884 : Tensor = prim::GetAttr[name="bias"](%1874)
   = prim::If(%1880) # torch/nn/functional.py:2011:4
    block0():
      %1885 : int[] = aten::size(%x.145) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%1885, %20) # torch/nn/functional.py:1991:17
      %1887 : int = aten::len(%1885) # torch/nn/functional.py:1992:19
      %1888 : int = aten::sub(%1887, %18) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%1888, %19, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %1892 : int = aten::add(%i.59, %18) # torch/nn/functional.py:1993:27
          %1893 : int = aten::__getitem__(%1885, %1892) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %1893) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.235)
      %1895 : bool = aten::eq(%size_prods.233, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1895) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.146 : Tensor = aten::batch_norm(%x.145, %1883, %1884, %1881, %1882, %1880, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.9 : Tensor = aten::relu_(%x.146) # torch/nn/functional.py:1117:17
  %outputs.10 : Tensor[] = prim::ListConstruct(%branch1.9, %branch2.9, %branch3.9, %branch4.9)
  %x.33 : Tensor = aten::cat(%outputs.10, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %1900 : __torch__.torchvision.models.googlenet.___torch_mangle_526.Inception = prim::GetAttr[name="inception5b"](%self)
  %1901 : __torch__.torchvision.models.googlenet.___torch_mangle_516.BasicConv2d = prim::GetAttr[name="branch1"](%1900)
  %1902 : __torch__.torch.nn.modules.conv.___torch_mangle_514.Conv2d = prim::GetAttr[name="conv"](%1901)
  %1903 : Tensor = prim::GetAttr[name="weight"](%1902)
  %1904 : Tensor? = prim::GetAttr[name="bias"](%1902)
  %1905 : int[] = prim::ListConstruct(%25, %25)
  %1906 : int[] = prim::ListConstruct(%20, %20)
  %1907 : int[] = prim::ListConstruct(%25, %25)
  %x.7 : Tensor = aten::conv2d(%x.33, %1903, %1904, %1905, %1906, %1907, %25) # torch/nn/modules/conv.py:415:15
  %1909 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%1901)
  %1910 : int = aten::dim(%x.7) # torch/nn/modules/batchnorm.py:276:11
  %1911 : bool = aten::ne(%1910, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1911) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1912 : bool = prim::GetAttr[name="training"](%1909)
   = prim::If(%1912) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1913 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1909)
      %1914 : Tensor = aten::add(%1913, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1909, %1914)
      -> ()
    block1():
      -> ()
  %1915 : bool = prim::GetAttr[name="training"](%1909)
  %1916 : Tensor = prim::GetAttr[name="running_mean"](%1909)
  %1917 : Tensor = prim::GetAttr[name="running_var"](%1909)
  %1918 : Tensor = prim::GetAttr[name="weight"](%1909)
  %1919 : Tensor = prim::GetAttr[name="bias"](%1909)
   = prim::If(%1915) # torch/nn/functional.py:2011:4
    block0():
      %1920 : int[] = aten::size(%x.7) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%1920, %20) # torch/nn/functional.py:1991:17
      %1922 : int = aten::len(%1920) # torch/nn/functional.py:1992:19
      %1923 : int = aten::sub(%1922, %18) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%1923, %19, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %1927 : int = aten::add(%i.3, %18) # torch/nn/functional.py:1993:27
          %1928 : int = aten::__getitem__(%1920, %1927) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %1928) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.11)
      %1930 : bool = aten::eq(%size_prods.9, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1930) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.8 : Tensor = aten::batch_norm(%x.7, %1918, %1919, %1916, %1917, %1915, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch1.1 : Tensor = aten::relu_(%x.8) # torch/nn/functional.py:1117:17
  %1933 : __torch__.torch.nn.modules.container.___torch_mangle_520.Sequential = prim::GetAttr[name="branch2"](%1900)
  %1934 : __torch__.torchvision.models.googlenet.___torch_mangle_518.BasicConv2d = prim::GetAttr[name="0"](%1933)
  %1935 : __torch__.torchvision.models.googlenet.___torch_mangle_519.BasicConv2d = prim::GetAttr[name="1"](%1933)
  %1936 : __torch__.torch.nn.modules.conv.___torch_mangle_517.Conv2d = prim::GetAttr[name="conv"](%1934)
  %1937 : Tensor = prim::GetAttr[name="weight"](%1936)
  %1938 : Tensor? = prim::GetAttr[name="bias"](%1936)
  %1939 : int[] = prim::ListConstruct(%25, %25)
  %1940 : int[] = prim::ListConstruct(%20, %20)
  %1941 : int[] = prim::ListConstruct(%25, %25)
  %x.9 : Tensor = aten::conv2d(%x.33, %1937, %1938, %1939, %1940, %1941, %25) # torch/nn/modules/conv.py:415:15
  %1943 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_395.BatchNorm2d = prim::GetAttr[name="bn"](%1934)
  %1944 : int = aten::dim(%x.9) # torch/nn/modules/batchnorm.py:276:11
  %1945 : bool = aten::ne(%1944, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1945) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1946 : bool = prim::GetAttr[name="training"](%1943)
   = prim::If(%1946) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1947 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1943)
      %1948 : Tensor = aten::add(%1947, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1943, %1948)
      -> ()
    block1():
      -> ()
  %1949 : bool = prim::GetAttr[name="training"](%1943)
  %1950 : Tensor = prim::GetAttr[name="running_mean"](%1943)
  %1951 : Tensor = prim::GetAttr[name="running_var"](%1943)
  %1952 : Tensor = prim::GetAttr[name="weight"](%1943)
  %1953 : Tensor = prim::GetAttr[name="bias"](%1943)
   = prim::If(%1949) # torch/nn/functional.py:2011:4
    block0():
      %1954 : int[] = aten::size(%x.9) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%1954, %20) # torch/nn/functional.py:1991:17
      %1956 : int = aten::len(%1954) # torch/nn/functional.py:1992:19
      %1957 : int = aten::sub(%1956, %18) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%1957, %19, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %1961 : int = aten::add(%i.4, %18) # torch/nn/functional.py:1993:27
          %1962 : int = aten::__getitem__(%1954, %1961) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %1962) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.15)
      %1964 : bool = aten::eq(%size_prods.13, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1964) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.10 : Tensor = aten::batch_norm(%x.9, %1952, %1953, %1950, %1951, %1949, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.4 : Tensor = aten::relu_(%x.10) # torch/nn/functional.py:1117:17
  %1967 : __torch__.torch.nn.modules.conv.___torch_mangle_1.Conv2d = prim::GetAttr[name="conv"](%1935)
  %1968 : Tensor = prim::GetAttr[name="weight"](%1967)
  %1969 : Tensor? = prim::GetAttr[name="bias"](%1967)
  %1970 : int[] = prim::ListConstruct(%25, %25)
  %1971 : int[] = prim::ListConstruct(%25, %25)
  %1972 : int[] = prim::ListConstruct(%25, %25)
  %x.11 : Tensor = aten::conv2d(%input.4, %1968, %1969, %1970, %1971, %1972, %25) # torch/nn/modules/conv.py:415:15
  %1974 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_515.BatchNorm2d = prim::GetAttr[name="bn"](%1935)
  %1975 : int = aten::dim(%x.11) # torch/nn/modules/batchnorm.py:276:11
  %1976 : bool = aten::ne(%1975, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1976) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1977 : bool = prim::GetAttr[name="training"](%1974)
   = prim::If(%1977) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1978 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1974)
      %1979 : Tensor = aten::add(%1978, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1974, %1979)
      -> ()
    block1():
      -> ()
  %1980 : bool = prim::GetAttr[name="training"](%1974)
  %1981 : Tensor = prim::GetAttr[name="running_mean"](%1974)
  %1982 : Tensor = prim::GetAttr[name="running_var"](%1974)
  %1983 : Tensor = prim::GetAttr[name="weight"](%1974)
  %1984 : Tensor = prim::GetAttr[name="bias"](%1974)
   = prim::If(%1980) # torch/nn/functional.py:2011:4
    block0():
      %1985 : int[] = aten::size(%x.11) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%1985, %20) # torch/nn/functional.py:1991:17
      %1987 : int = aten::len(%1985) # torch/nn/functional.py:1992:19
      %1988 : int = aten::sub(%1987, %18) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%1988, %19, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.18 : int):
          %1992 : int = aten::add(%i.5, %18) # torch/nn/functional.py:1993:27
          %1993 : int = aten::__getitem__(%1985, %1992) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %1993) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.19)
      %1995 : bool = aten::eq(%size_prods.17, %25) # torch/nn/functional.py:1994:7
       = prim::If(%1995) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.12 : Tensor = aten::batch_norm(%x.11, %1983, %1984, %1981, %1982, %1980, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch2.1 : Tensor = aten::relu_(%x.12) # torch/nn/functional.py:1117:17
  %1998 : __torch__.torch.nn.modules.container.___torch_mangle_525.Sequential = prim::GetAttr[name="branch3"](%1900)
  %1999 : __torch__.torchvision.models.googlenet.___torch_mangle_522.BasicConv2d = prim::GetAttr[name="0"](%1998)
  %2000 : __torch__.torchvision.models.googlenet.___torch_mangle_524.BasicConv2d = prim::GetAttr[name="1"](%1998)
  %2001 : __torch__.torch.nn.modules.conv.___torch_mangle_521.Conv2d = prim::GetAttr[name="conv"](%1999)
  %2002 : Tensor = prim::GetAttr[name="weight"](%2001)
  %2003 : Tensor? = prim::GetAttr[name="bias"](%2001)
  %2004 : int[] = prim::ListConstruct(%25, %25)
  %2005 : int[] = prim::ListConstruct(%20, %20)
  %2006 : int[] = prim::ListConstruct(%25, %25)
  %x.4 : Tensor = aten::conv2d(%x.33, %2002, %2003, %2004, %2005, %2006, %25) # torch/nn/modules/conv.py:415:15
  %2008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_439.BatchNorm2d = prim::GetAttr[name="bn"](%1999)
  %2009 : int = aten::dim(%x.4) # torch/nn/modules/batchnorm.py:276:11
  %2010 : bool = aten::ne(%2009, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2010) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2011 : bool = prim::GetAttr[name="training"](%2008)
   = prim::If(%2011) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2008)
      %2013 : Tensor = aten::add(%2012, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2008, %2013)
      -> ()
    block1():
      -> ()
  %2014 : bool = prim::GetAttr[name="training"](%2008)
  %2015 : Tensor = prim::GetAttr[name="running_mean"](%2008)
  %2016 : Tensor = prim::GetAttr[name="running_var"](%2008)
  %2017 : Tensor = prim::GetAttr[name="weight"](%2008)
  %2018 : Tensor = prim::GetAttr[name="bias"](%2008)
   = prim::If(%2014) # torch/nn/functional.py:2011:4
    block0():
      %2019 : int[] = aten::size(%x.4) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%2019, %20) # torch/nn/functional.py:1991:17
      %2021 : int = aten::len(%2019) # torch/nn/functional.py:1992:19
      %2022 : int = aten::sub(%2021, %18) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%2022, %19, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.7 : int):
          %2026 : int = aten::add(%i.2, %18) # torch/nn/functional.py:1993:27
          %2027 : int = aten::__getitem__(%2019, %2026) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %2027) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.5)
      %2029 : bool = aten::eq(%size_prods.4, %25) # torch/nn/functional.py:1994:7
       = prim::If(%2029) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.6 : Tensor = aten::batch_norm(%x.4, %2017, %2018, %2015, %2016, %2014, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %input.6 : Tensor = aten::relu_(%x.6) # torch/nn/functional.py:1117:17
  %2032 : __torch__.torch.nn.modules.conv.___torch_mangle_523.Conv2d = prim::GetAttr[name="conv"](%2000)
  %2033 : Tensor = prim::GetAttr[name="weight"](%2032)
  %2034 : Tensor? = prim::GetAttr[name="bias"](%2032)
  %2035 : int[] = prim::ListConstruct(%25, %25)
  %2036 : int[] = prim::ListConstruct(%25, %25)
  %2037 : int[] = prim::ListConstruct(%25, %25)
  %x.13 : Tensor = aten::conv2d(%input.6, %2033, %2034, %2035, %2036, %2037, %25) # torch/nn/modules/conv.py:415:15
  %2039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%2000)
  %2040 : int = aten::dim(%x.13) # torch/nn/modules/batchnorm.py:276:11
  %2041 : bool = aten::ne(%2040, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2042 : bool = prim::GetAttr[name="training"](%2039)
   = prim::If(%2042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2039)
      %2044 : Tensor = aten::add(%2043, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2039, %2044)
      -> ()
    block1():
      -> ()
  %2045 : bool = prim::GetAttr[name="training"](%2039)
  %2046 : Tensor = prim::GetAttr[name="running_mean"](%2039)
  %2047 : Tensor = prim::GetAttr[name="running_var"](%2039)
  %2048 : Tensor = prim::GetAttr[name="weight"](%2039)
  %2049 : Tensor = prim::GetAttr[name="bias"](%2039)
   = prim::If(%2045) # torch/nn/functional.py:2011:4
    block0():
      %2050 : int[] = aten::size(%x.13) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%2050, %20) # torch/nn/functional.py:1991:17
      %2052 : int = aten::len(%2050) # torch/nn/functional.py:1992:19
      %2053 : int = aten::sub(%2052, %18) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%2053, %19, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %2057 : int = aten::add(%i.6, %18) # torch/nn/functional.py:1993:27
          %2058 : int = aten::__getitem__(%2050, %2057) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %2058) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.23)
      %2060 : bool = aten::eq(%size_prods.21, %25) # torch/nn/functional.py:1994:7
       = prim::If(%2060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.14 : Tensor = aten::batch_norm(%x.13, %2048, %2049, %2046, %2047, %2045, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch3.1 : Tensor = aten::relu_(%x.14) # torch/nn/functional.py:1117:17
  %2063 : __torch__.torch.nn.modules.container.___torch_mangle_512.Sequential = prim::GetAttr[name="branch4"](%1900)
  %2064 : __torch__.torchvision.models.googlenet.___torch_mangle_511.BasicConv2d = prim::GetAttr[name="1"](%2063)
  %2065 : int[] = prim::ListConstruct(%17, %17)
  %2066 : int[] = prim::ListConstruct(%25, %25)
  %2067 : int[] = prim::ListConstruct(%25, %25)
  %2068 : int[] = prim::ListConstruct(%25, %25)
  %input.3 : Tensor = aten::max_pool2d(%x.33, %2065, %2066, %2067, %2068, %19) # torch/nn/functional.py:575:11
  %2070 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv"](%2064)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2070)
  %2072 : Tensor? = prim::GetAttr[name="bias"](%2070)
  %2073 : int[] = prim::ListConstruct(%25, %25)
  %2074 : int[] = prim::ListConstruct(%20, %20)
  %2075 : int[] = prim::ListConstruct(%25, %25)
  %x.3 : Tensor = aten::conv2d(%input.3, %2071, %2072, %2073, %2074, %2075, %25) # torch/nn/modules/conv.py:415:15
  %2077 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_403.BatchNorm2d = prim::GetAttr[name="bn"](%2064)
  %2078 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %2079 : bool = aten::ne(%2078, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2079) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%21) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2080 : bool = prim::GetAttr[name="training"](%2077)
   = prim::If(%2080) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2081 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2077)
      %2082 : Tensor = aten::add(%2081, %25, %25) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2077, %2082)
      -> ()
    block1():
      -> ()
  %2083 : bool = prim::GetAttr[name="training"](%2077)
  %2084 : Tensor = prim::GetAttr[name="running_mean"](%2077)
  %2085 : Tensor = prim::GetAttr[name="running_var"](%2077)
  %2086 : Tensor = prim::GetAttr[name="weight"](%2077)
  %2087 : Tensor = prim::GetAttr[name="bias"](%2077)
   = prim::If(%2083) # torch/nn/functional.py:2011:4
    block0():
      %2088 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%2088, %20) # torch/nn/functional.py:1991:17
      %2090 : int = aten::len(%2088) # torch/nn/functional.py:1992:19
      %2091 : int = aten::sub(%2090, %18) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%2091, %19, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %2095 : int = aten::add(%i.1, %18) # torch/nn/functional.py:1993:27
          %2096 : int = aten::__getitem__(%2088, %2095) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %2096) # torch/nn/functional.py:1993:8
          -> (%19, %size_prods.3)
      %2098 : bool = aten::eq(%size_prods, %25) # torch/nn/functional.py:1994:7
       = prim::If(%2098) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%21) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.5 : Tensor = aten::batch_norm(%x.3, %2086, %2087, %2084, %2085, %2083, %exponential_average_factor.2, %24, %19) # torch/nn/functional.py:2014:11
  %branch4.1 : Tensor = aten::relu_(%x.5) # torch/nn/functional.py:1117:17
  %outputs.2 : Tensor[] = prim::ListConstruct(%branch1.1, %branch2.1, %branch3.1, %branch4.1)
  %x.35 : Tensor = aten::cat(%outputs.2, %25) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:252:15
  %2103 : int[] = prim::ListConstruct(%25, %25)
  %2104 : int[] = aten::size(%x.35) # torch/nn/functional.py:925:51
  %2105 : int = aten::len(%2104) # <string>:5:9
  %2106 : bool = aten::gt(%2105, %18) # <string>:5:9
   = prim::If(%2106) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%21) # <string>:5:2
      -> ()
  %x.37 : Tensor = aten::adaptive_avg_pool2d(%x.35, %2103) # torch/nn/functional.py:926:11
  %x.39 : Tensor = aten::flatten(%x.37, %25, %27) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:188:12
  %2109 : __torch__.torch.nn.modules.dropout.___torch_mangle_530.Dropout = prim::GetAttr[name="dropout"](%self)
  %2110 : bool = prim::GetAttr[name="training"](%2109)
  %x.41 : Tensor = aten::dropout(%x.39, %15, %2110) # torch/nn/functional.py:973:17
  %2112 : __torch__.torch.nn.modules.linear.___torch_mangle_161.Linear = prim::GetAttr[name="fc"](%self)
  %2113 : Tensor = prim::GetAttr[name="weight"](%2112)
  %2114 : Tensor = prim::GetAttr[name="bias"](%2112)
  %2115 : int = aten::dim(%x.41) # torch/nn/functional.py:1672:7
  %2116 : bool = aten::eq(%2115, %18) # torch/nn/functional.py:1672:7
  %x.43 : Tensor = prim::If(%2116) # torch/nn/functional.py:1672:4
    block0():
      %2118 : Tensor = aten::t(%2113) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%2114, %x.41, %2118, %25, %25) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %2120 : Tensor = aten::t(%2113) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%x.41, %2120) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %2114, %25) # torch/nn/functional.py:1678:12
      -> (%output.3)
  %2123 : (Tensor, Tensor?, Tensor?) = prim::TupleConstruct(%x.43, %aux2, %aux1)
  %x.147 : Tensor, %aux1.3 : Tensor?, %aux2.1 : Tensor? = prim::TupleUnpack(%2123)
  %11 : bool = prim::GetAttr[name="training"](%self)
  %aux_defined.1 : bool = prim::If(%11) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:206:22
    block0():
      -> (%5)
    block1():
      -> (%4)
  %13 : bool = aten::__not__(%aux_defined.1) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:208:15
   = prim::If(%13) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:208:12
    block0():
       = aten::warn(%3, %2) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:209:16
      -> ()
    block1():
      -> ()
  %14 : NamedTuple(logits : Tensor, aux_logits2 : Tensor?, aux_logits1 : Tensor?) = prim::TupleConstruct(%x.147, %aux2.1, %aux1.3) # torch/hub/pytorch_vision_master/torchvision/models/googlenet.py:210:19
  return (%14)
