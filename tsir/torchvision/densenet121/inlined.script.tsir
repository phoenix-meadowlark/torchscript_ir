graph(%self : __torch__.torchvision.models.densenet.DenseNet,
      %x.1 : Tensor):
  %2 : int = prim::Constant[value=-1]()
  %3 : Function = prim::Constant[name="adaptive_avg_pool2d"]()
  %4 : Function = prim::Constant[name="relu"]()
  %5 : bool = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:193:39
  %6 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:194:42
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_160.Sequential = prim::GetAttr[name="features"](%self)
  %15 : None = prim::Constant() # torch/nn/modules/pooling.py:599:82
  %16 : float = prim::Constant[value=0.]() # torch/nn/functional.py:968:11
  %17 : float = prim::Constant[value=1.]() # torch/nn/functional.py:968:21
  %18 : int = prim::Constant[value=9223372036854775807]()
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %29 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv0"](%7)
  %30 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm0"](%7)
  %31 : __torch__.torchvision.models.densenet._DenseBlock = prim::GetAttr[name="denseblock1"](%7)
  %32 : __torch__.torchvision.models.densenet._Transition = prim::GetAttr[name="transition1"](%7)
  %33 : __torch__.torchvision.models.densenet.___torch_mangle_109._DenseBlock = prim::GetAttr[name="denseblock2"](%7)
  %34 : __torch__.torchvision.models.densenet.___torch_mangle_110._Transition = prim::GetAttr[name="transition2"](%7)
  %35 : __torch__.torchvision.models.densenet.___torch_mangle_157._DenseBlock = prim::GetAttr[name="denseblock3"](%7)
  %36 : __torch__.torchvision.models.densenet.___torch_mangle_158._Transition = prim::GetAttr[name="transition3"](%7)
  %37 : __torch__.torchvision.models.densenet.___torch_mangle_159._DenseBlock = prim::GetAttr[name="denseblock4"](%7)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm5"](%7)
  %39 : Tensor = prim::GetAttr[name="weight"](%29)
  %40 : Tensor? = prim::GetAttr[name="bias"](%29)
  %41 : int[] = prim::ListConstruct(%26, %26)
  %42 : int[] = prim::ListConstruct(%28, %28)
  %43 : int[] = prim::ListConstruct(%27, %27)
  %input.4 : Tensor = aten::conv2d(%x.1, %39, %40, %41, %42, %43, %27) # torch/nn/modules/conv.py:415:15
  %45 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %46 : bool = aten::ne(%45, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%46) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %47 : bool = prim::GetAttr[name="training"](%30)
   = prim::If(%47) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %48 : Tensor = prim::GetAttr[name="num_batches_tracked"](%30)
      %49 : Tensor = aten::add(%48, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%30, %49)
      -> ()
    block1():
      -> ()
  %50 : bool = prim::GetAttr[name="training"](%30)
  %51 : Tensor = prim::GetAttr[name="running_mean"](%30)
  %52 : Tensor = prim::GetAttr[name="running_var"](%30)
  %53 : Tensor = prim::GetAttr[name="weight"](%30)
  %54 : Tensor = prim::GetAttr[name="bias"](%30)
   = prim::If(%50) # torch/nn/functional.py:2011:4
    block0():
      %55 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%55, %24) # torch/nn/functional.py:1991:17
      %57 : int = aten::len(%55) # torch/nn/functional.py:1992:19
      %58 : int = aten::sub(%57, %26) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%58, %25, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %62 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
          %63 : int = aten::__getitem__(%55, %62) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %63) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.247)
      %65 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
       = prim::If(%65) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %53, %54, %51, %52, %50, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu_(%input.6) # torch/nn/functional.py:1117:17
  %68 : int[] = prim::ListConstruct(%28, %28)
  %69 : int[] = prim::ListConstruct(%26, %26)
  %70 : int[] = prim::ListConstruct(%27, %27)
  %71 : int[] = prim::ListConstruct(%27, %27)
  %input.10 : Tensor = aten::max_pool2d(%input.8, %68, %69, %70, %71, %19) # torch/nn/functional.py:575:11
  %features.2 : Tensor[] = prim::ListConstruct(%input.10)
  %74 : __torch__.torchvision.models.densenet._DenseLayer = prim::GetAttr[name="denselayer1"](%31)
  %75 : __torch__.torchvision.models.densenet.___torch_mangle_75._DenseLayer = prim::GetAttr[name="denselayer2"](%31)
  %76 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer3"](%31)
  %77 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer4"](%31)
  %78 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer5"](%31)
  %79 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer6"](%31)
  %80 : Tensor = prim::Uninitialized()
  %81 : bool = prim::GetAttr[name="memory_efficient"](%74)
  %82 : bool = prim::If(%81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %83 : bool = prim::Uninitialized()
      %84 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %85 : bool = aten::gt(%84, %24)
      %86 : bool, %87 : bool, %88 : int = prim::Loop(%18, %85, %19, %83, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%89 : int, %90 : bool, %91 : bool, %92 : int):
          %tensor.34 : Tensor = aten::__getitem__(%features.2, %92) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %94 : bool = prim::requires_grad(%tensor.34)
          %95 : bool, %96 : bool = prim::If(%94) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %83)
          %97 : int = aten::add(%92, %27)
          %98 : bool = aten::lt(%97, %84)
          %99 : bool = aten::__and__(%98, %95)
          -> (%99, %94, %96, %97)
      %100 : bool = prim::If(%86)
        block0():
          -> (%87)
        block1():
          -> (%19)
      -> (%100)
    block1():
      -> (%19)
  %bottleneck_output.66 : Tensor = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%80)
    block1():
      %concated_features.34 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %103 : __torch__.torch.nn.modules.conv.___torch_mangle_71.Conv2d = prim::GetAttr[name="conv1"](%74)
      %104 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm1"](%74)
      %105 : int = aten::dim(%concated_features.34) # torch/nn/modules/batchnorm.py:276:11
      %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %107 : bool = prim::GetAttr[name="training"](%104)
       = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
          %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%104, %109)
          -> ()
        block1():
          -> ()
      %110 : bool = prim::GetAttr[name="training"](%104)
      %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
      %112 : Tensor = prim::GetAttr[name="running_var"](%104)
      %113 : Tensor = prim::GetAttr[name="weight"](%104)
      %114 : Tensor = prim::GetAttr[name="bias"](%104)
       = prim::If(%110) # torch/nn/functional.py:2011:4
        block0():
          %115 : int[] = aten::size(%concated_features.34) # torch/nn/functional.py:2012:27
          %size_prods.252 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
          %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
          %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
          %size_prods.253 : int = prim::Loop(%118, %25, %size_prods.252) # torch/nn/functional.py:1992:4
            block0(%i.64 : int, %size_prods.254 : int):
              %122 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
              %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
              %size_prods.255 : int = aten::mul(%size_prods.254, %123) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.255)
          %125 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
           = prim::If(%125) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %126 : Tensor = aten::batch_norm(%concated_features.34, %113, %114, %111, %112, %110, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.67 : Tensor = aten::relu_(%126) # torch/nn/functional.py:1117:17
      %128 : Tensor = prim::GetAttr[name="weight"](%103)
      %129 : Tensor? = prim::GetAttr[name="bias"](%103)
      %130 : int[] = prim::ListConstruct(%27, %27)
      %131 : int[] = prim::ListConstruct(%24, %24)
      %132 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.67 : Tensor = aten::conv2d(%result.67, %128, %129, %130, %131, %132, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.67)
  %134 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%74)
  %135 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%74)
  %136 : int = aten::dim(%bottleneck_output.66) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%bottleneck_output.66) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%149, %25, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %153 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.259)
      %156 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %157 : Tensor = aten::batch_norm(%bottleneck_output.66, %144, %145, %142, %143, %141, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.68 : Tensor = aten::relu_(%157) # torch/nn/functional.py:1117:17
  %159 : Tensor = prim::GetAttr[name="weight"](%134)
  %160 : Tensor? = prim::GetAttr[name="bias"](%134)
  %161 : int[] = prim::ListConstruct(%27, %27)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%27, %27)
  %new_features.59 : Tensor = aten::conv2d(%result.68, %159, %160, %161, %162, %163, %27) # torch/nn/modules/conv.py:415:15
  %165 : float = prim::GetAttr[name="drop_rate"](%74)
  %166 : bool = aten::gt(%165, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.69 : Tensor = prim::If(%166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %168 : float = prim::GetAttr[name="drop_rate"](%74)
      %169 : bool = prim::GetAttr[name="training"](%74)
      %170 : bool = aten::lt(%168, %16) # torch/nn/functional.py:968:7
      %171 : bool = prim::If(%170) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %172 : bool = aten::gt(%168, %17) # torch/nn/functional.py:968:17
          -> (%172)
       = prim::If(%171) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %173 : Tensor = aten::dropout(%new_features.59, %168, %169) # torch/nn/functional.py:973:17
      -> (%173)
    block1():
      -> (%new_features.59)
  %174 : Tensor[] = aten::append(%features.2, %new_features.69) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %175 : Tensor = prim::Uninitialized()
  %176 : bool = prim::GetAttr[name="memory_efficient"](%75)
  %177 : bool = prim::If(%176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %178 : bool = prim::Uninitialized()
      %179 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %180 : bool = aten::gt(%179, %24)
      %181 : bool, %182 : bool, %183 : int = prim::Loop(%18, %180, %19, %178, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%184 : int, %185 : bool, %186 : bool, %187 : int):
          %tensor.30 : Tensor = aten::__getitem__(%features.2, %187) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %189 : bool = prim::requires_grad(%tensor.30)
          %190 : bool, %191 : bool = prim::If(%189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %178)
          %192 : int = aten::add(%187, %27)
          %193 : bool = aten::lt(%192, %179)
          %194 : bool = aten::__and__(%193, %190)
          -> (%194, %189, %191, %192)
      %195 : bool = prim::If(%181)
        block0():
          -> (%182)
        block1():
          -> (%19)
      -> (%195)
    block1():
      -> (%19)
  %bottleneck_output.58 : Tensor = prim::If(%177) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%175)
    block1():
      %concated_features.30 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %198 : __torch__.torch.nn.modules.conv.___torch_mangle_74.Conv2d = prim::GetAttr[name="conv1"](%75)
      %199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="norm1"](%75)
      %200 : int = aten::dim(%concated_features.30) # torch/nn/modules/batchnorm.py:276:11
      %201 : bool = aten::ne(%200, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%201) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %202 : bool = prim::GetAttr[name="training"](%199)
       = prim::If(%202) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %203 : Tensor = prim::GetAttr[name="num_batches_tracked"](%199)
          %204 : Tensor = aten::add(%203, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%199, %204)
          -> ()
        block1():
          -> ()
      %205 : bool = prim::GetAttr[name="training"](%199)
      %206 : Tensor = prim::GetAttr[name="running_mean"](%199)
      %207 : Tensor = prim::GetAttr[name="running_var"](%199)
      %208 : Tensor = prim::GetAttr[name="weight"](%199)
      %209 : Tensor = prim::GetAttr[name="bias"](%199)
       = prim::If(%205) # torch/nn/functional.py:2011:4
        block0():
          %210 : int[] = aten::size(%concated_features.30) # torch/nn/functional.py:2012:27
          %size_prods.260 : int = aten::__getitem__(%210, %24) # torch/nn/functional.py:1991:17
          %212 : int = aten::len(%210) # torch/nn/functional.py:1992:19
          %213 : int = aten::sub(%212, %26) # torch/nn/functional.py:1992:19
          %size_prods.261 : int = prim::Loop(%213, %25, %size_prods.260) # torch/nn/functional.py:1992:4
            block0(%i.66 : int, %size_prods.262 : int):
              %217 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
              %218 : int = aten::__getitem__(%210, %217) # torch/nn/functional.py:1993:22
              %size_prods.263 : int = aten::mul(%size_prods.262, %218) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.263)
          %220 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
           = prim::If(%220) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %221 : Tensor = aten::batch_norm(%concated_features.30, %208, %209, %206, %207, %205, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.59 : Tensor = aten::relu_(%221) # torch/nn/functional.py:1117:17
      %223 : Tensor = prim::GetAttr[name="weight"](%198)
      %224 : Tensor? = prim::GetAttr[name="bias"](%198)
      %225 : int[] = prim::ListConstruct(%27, %27)
      %226 : int[] = prim::ListConstruct(%24, %24)
      %227 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.59 : Tensor = aten::conv2d(%result.59, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.59)
  %229 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%75)
  %230 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%75)
  %231 : int = aten::dim(%bottleneck_output.58) # torch/nn/modules/batchnorm.py:276:11
  %232 : bool = aten::ne(%231, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%232) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %233 : bool = prim::GetAttr[name="training"](%230)
   = prim::If(%233) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %234 : Tensor = prim::GetAttr[name="num_batches_tracked"](%230)
      %235 : Tensor = aten::add(%234, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%230, %235)
      -> ()
    block1():
      -> ()
  %236 : bool = prim::GetAttr[name="training"](%230)
  %237 : Tensor = prim::GetAttr[name="running_mean"](%230)
  %238 : Tensor = prim::GetAttr[name="running_var"](%230)
  %239 : Tensor = prim::GetAttr[name="weight"](%230)
  %240 : Tensor = prim::GetAttr[name="bias"](%230)
   = prim::If(%236) # torch/nn/functional.py:2011:4
    block0():
      %241 : int[] = aten::size(%bottleneck_output.58) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%241, %24) # torch/nn/functional.py:1991:17
      %243 : int = aten::len(%241) # torch/nn/functional.py:1992:19
      %244 : int = aten::sub(%243, %26) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%244, %25, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %248 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
          %249 : int = aten::__getitem__(%241, %248) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %249) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.267)
      %251 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
       = prim::If(%251) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %252 : Tensor = aten::batch_norm(%bottleneck_output.58, %239, %240, %237, %238, %236, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.60 : Tensor = aten::relu_(%252) # torch/nn/functional.py:1117:17
  %254 : Tensor = prim::GetAttr[name="weight"](%229)
  %255 : Tensor? = prim::GetAttr[name="bias"](%229)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %new_features.61 : Tensor = aten::conv2d(%result.60, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : float = prim::GetAttr[name="drop_rate"](%75)
  %261 : bool = aten::gt(%260, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.62 : Tensor = prim::If(%261) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %263 : float = prim::GetAttr[name="drop_rate"](%75)
      %264 : bool = prim::GetAttr[name="training"](%75)
      %265 : bool = aten::lt(%263, %16) # torch/nn/functional.py:968:7
      %266 : bool = prim::If(%265) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %267 : bool = aten::gt(%263, %17) # torch/nn/functional.py:968:17
          -> (%267)
       = prim::If(%266) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %268 : Tensor = aten::dropout(%new_features.61, %263, %264) # torch/nn/functional.py:973:17
      -> (%268)
    block1():
      -> (%new_features.61)
  %269 : Tensor[] = aten::append(%features.2, %new_features.62) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %270 : Tensor = prim::Uninitialized()
  %271 : bool = prim::GetAttr[name="memory_efficient"](%76)
  %272 : bool = prim::If(%271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %273 : bool = prim::Uninitialized()
      %274 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %275 : bool = aten::gt(%274, %24)
      %276 : bool, %277 : bool, %278 : int = prim::Loop(%18, %275, %19, %273, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%279 : int, %280 : bool, %281 : bool, %282 : int):
          %tensor.31 : Tensor = aten::__getitem__(%features.2, %282) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %284 : bool = prim::requires_grad(%tensor.31)
          %285 : bool, %286 : bool = prim::If(%284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %273)
          %287 : int = aten::add(%282, %27)
          %288 : bool = aten::lt(%287, %274)
          %289 : bool = aten::__and__(%288, %285)
          -> (%289, %284, %286, %287)
      %290 : bool = prim::If(%276)
        block0():
          -> (%277)
        block1():
          -> (%19)
      -> (%290)
    block1():
      -> (%19)
  %bottleneck_output.60 : Tensor = prim::If(%272) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%270)
    block1():
      %concated_features.31 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %293 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%76)
      %294 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%76)
      %295 : int = aten::dim(%concated_features.31) # torch/nn/modules/batchnorm.py:276:11
      %296 : bool = aten::ne(%295, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%296) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %297 : bool = prim::GetAttr[name="training"](%294)
       = prim::If(%297) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%294)
          %299 : Tensor = aten::add(%298, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%294, %299)
          -> ()
        block1():
          -> ()
      %300 : bool = prim::GetAttr[name="training"](%294)
      %301 : Tensor = prim::GetAttr[name="running_mean"](%294)
      %302 : Tensor = prim::GetAttr[name="running_var"](%294)
      %303 : Tensor = prim::GetAttr[name="weight"](%294)
      %304 : Tensor = prim::GetAttr[name="bias"](%294)
       = prim::If(%300) # torch/nn/functional.py:2011:4
        block0():
          %305 : int[] = aten::size(%concated_features.31) # torch/nn/functional.py:2012:27
          %size_prods.268 : int = aten::__getitem__(%305, %24) # torch/nn/functional.py:1991:17
          %307 : int = aten::len(%305) # torch/nn/functional.py:1992:19
          %308 : int = aten::sub(%307, %26) # torch/nn/functional.py:1992:19
          %size_prods.269 : int = prim::Loop(%308, %25, %size_prods.268) # torch/nn/functional.py:1992:4
            block0(%i.68 : int, %size_prods.270 : int):
              %312 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
              %313 : int = aten::__getitem__(%305, %312) # torch/nn/functional.py:1993:22
              %size_prods.271 : int = aten::mul(%size_prods.270, %313) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.271)
          %315 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
           = prim::If(%315) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %316 : Tensor = aten::batch_norm(%concated_features.31, %303, %304, %301, %302, %300, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.61 : Tensor = aten::relu_(%316) # torch/nn/functional.py:1117:17
      %318 : Tensor = prim::GetAttr[name="weight"](%293)
      %319 : Tensor? = prim::GetAttr[name="bias"](%293)
      %320 : int[] = prim::ListConstruct(%27, %27)
      %321 : int[] = prim::ListConstruct(%24, %24)
      %322 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.61 : Tensor = aten::conv2d(%result.61, %318, %319, %320, %321, %322, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.61)
  %324 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%76)
  %325 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%76)
  %326 : int = aten::dim(%bottleneck_output.60) # torch/nn/modules/batchnorm.py:276:11
  %327 : bool = aten::ne(%326, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%327) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %328 : bool = prim::GetAttr[name="training"](%325)
   = prim::If(%328) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %329 : Tensor = prim::GetAttr[name="num_batches_tracked"](%325)
      %330 : Tensor = aten::add(%329, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%325, %330)
      -> ()
    block1():
      -> ()
  %331 : bool = prim::GetAttr[name="training"](%325)
  %332 : Tensor = prim::GetAttr[name="running_mean"](%325)
  %333 : Tensor = prim::GetAttr[name="running_var"](%325)
  %334 : Tensor = prim::GetAttr[name="weight"](%325)
  %335 : Tensor = prim::GetAttr[name="bias"](%325)
   = prim::If(%331) # torch/nn/functional.py:2011:4
    block0():
      %336 : int[] = aten::size(%bottleneck_output.60) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%336, %24) # torch/nn/functional.py:1991:17
      %338 : int = aten::len(%336) # torch/nn/functional.py:1992:19
      %339 : int = aten::sub(%338, %26) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%339, %25, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %343 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
          %344 : int = aten::__getitem__(%336, %343) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %344) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.275)
      %346 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
       = prim::If(%346) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %347 : Tensor = aten::batch_norm(%bottleneck_output.60, %334, %335, %332, %333, %331, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.62 : Tensor = aten::relu_(%347) # torch/nn/functional.py:1117:17
  %349 : Tensor = prim::GetAttr[name="weight"](%324)
  %350 : Tensor? = prim::GetAttr[name="bias"](%324)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %353 : int[] = prim::ListConstruct(%27, %27)
  %new_features.63 : Tensor = aten::conv2d(%result.62, %349, %350, %351, %352, %353, %27) # torch/nn/modules/conv.py:415:15
  %355 : float = prim::GetAttr[name="drop_rate"](%76)
  %356 : bool = aten::gt(%355, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.64 : Tensor = prim::If(%356) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %358 : float = prim::GetAttr[name="drop_rate"](%76)
      %359 : bool = prim::GetAttr[name="training"](%76)
      %360 : bool = aten::lt(%358, %16) # torch/nn/functional.py:968:7
      %361 : bool = prim::If(%360) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %362 : bool = aten::gt(%358, %17) # torch/nn/functional.py:968:17
          -> (%362)
       = prim::If(%361) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %363 : Tensor = aten::dropout(%new_features.63, %358, %359) # torch/nn/functional.py:973:17
      -> (%363)
    block1():
      -> (%new_features.63)
  %364 : Tensor[] = aten::append(%features.2, %new_features.64) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %365 : Tensor = prim::Uninitialized()
  %366 : bool = prim::GetAttr[name="memory_efficient"](%77)
  %367 : bool = prim::If(%366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %368 : bool = prim::Uninitialized()
      %369 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %370 : bool = aten::gt(%369, %24)
      %371 : bool, %372 : bool, %373 : int = prim::Loop(%18, %370, %19, %368, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%374 : int, %375 : bool, %376 : bool, %377 : int):
          %tensor.32 : Tensor = aten::__getitem__(%features.2, %377) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %379 : bool = prim::requires_grad(%tensor.32)
          %380 : bool, %381 : bool = prim::If(%379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %368)
          %382 : int = aten::add(%377, %27)
          %383 : bool = aten::lt(%382, %369)
          %384 : bool = aten::__and__(%383, %380)
          -> (%384, %379, %381, %382)
      %385 : bool = prim::If(%371)
        block0():
          -> (%372)
        block1():
          -> (%19)
      -> (%385)
    block1():
      -> (%19)
  %bottleneck_output.62 : Tensor = prim::If(%367) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%365)
    block1():
      %concated_features.32 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %388 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%77)
      %389 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%77)
      %390 : int = aten::dim(%concated_features.32) # torch/nn/modules/batchnorm.py:276:11
      %391 : bool = aten::ne(%390, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%391) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %392 : bool = prim::GetAttr[name="training"](%389)
       = prim::If(%392) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %393 : Tensor = prim::GetAttr[name="num_batches_tracked"](%389)
          %394 : Tensor = aten::add(%393, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%389, %394)
          -> ()
        block1():
          -> ()
      %395 : bool = prim::GetAttr[name="training"](%389)
      %396 : Tensor = prim::GetAttr[name="running_mean"](%389)
      %397 : Tensor = prim::GetAttr[name="running_var"](%389)
      %398 : Tensor = prim::GetAttr[name="weight"](%389)
      %399 : Tensor = prim::GetAttr[name="bias"](%389)
       = prim::If(%395) # torch/nn/functional.py:2011:4
        block0():
          %400 : int[] = aten::size(%concated_features.32) # torch/nn/functional.py:2012:27
          %size_prods.276 : int = aten::__getitem__(%400, %24) # torch/nn/functional.py:1991:17
          %402 : int = aten::len(%400) # torch/nn/functional.py:1992:19
          %403 : int = aten::sub(%402, %26) # torch/nn/functional.py:1992:19
          %size_prods.277 : int = prim::Loop(%403, %25, %size_prods.276) # torch/nn/functional.py:1992:4
            block0(%i.70 : int, %size_prods.278 : int):
              %407 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
              %408 : int = aten::__getitem__(%400, %407) # torch/nn/functional.py:1993:22
              %size_prods.279 : int = aten::mul(%size_prods.278, %408) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.279)
          %410 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
           = prim::If(%410) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %411 : Tensor = aten::batch_norm(%concated_features.32, %398, %399, %396, %397, %395, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.63 : Tensor = aten::relu_(%411) # torch/nn/functional.py:1117:17
      %413 : Tensor = prim::GetAttr[name="weight"](%388)
      %414 : Tensor? = prim::GetAttr[name="bias"](%388)
      %415 : int[] = prim::ListConstruct(%27, %27)
      %416 : int[] = prim::ListConstruct(%24, %24)
      %417 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.63 : Tensor = aten::conv2d(%result.63, %413, %414, %415, %416, %417, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.63)
  %419 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%77)
  %420 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%77)
  %421 : int = aten::dim(%bottleneck_output.62) # torch/nn/modules/batchnorm.py:276:11
  %422 : bool = aten::ne(%421, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%422) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %423 : bool = prim::GetAttr[name="training"](%420)
   = prim::If(%423) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %424 : Tensor = prim::GetAttr[name="num_batches_tracked"](%420)
      %425 : Tensor = aten::add(%424, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%420, %425)
      -> ()
    block1():
      -> ()
  %426 : bool = prim::GetAttr[name="training"](%420)
  %427 : Tensor = prim::GetAttr[name="running_mean"](%420)
  %428 : Tensor = prim::GetAttr[name="running_var"](%420)
  %429 : Tensor = prim::GetAttr[name="weight"](%420)
  %430 : Tensor = prim::GetAttr[name="bias"](%420)
   = prim::If(%426) # torch/nn/functional.py:2011:4
    block0():
      %431 : int[] = aten::size(%bottleneck_output.62) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%431, %24) # torch/nn/functional.py:1991:17
      %433 : int = aten::len(%431) # torch/nn/functional.py:1992:19
      %434 : int = aten::sub(%433, %26) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%434, %25, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %438 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
          %439 : int = aten::__getitem__(%431, %438) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %439) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.283)
      %441 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
       = prim::If(%441) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %442 : Tensor = aten::batch_norm(%bottleneck_output.62, %429, %430, %427, %428, %426, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.64 : Tensor = aten::relu_(%442) # torch/nn/functional.py:1117:17
  %444 : Tensor = prim::GetAttr[name="weight"](%419)
  %445 : Tensor? = prim::GetAttr[name="bias"](%419)
  %446 : int[] = prim::ListConstruct(%27, %27)
  %447 : int[] = prim::ListConstruct(%27, %27)
  %448 : int[] = prim::ListConstruct(%27, %27)
  %new_features.65 : Tensor = aten::conv2d(%result.64, %444, %445, %446, %447, %448, %27) # torch/nn/modules/conv.py:415:15
  %450 : float = prim::GetAttr[name="drop_rate"](%77)
  %451 : bool = aten::gt(%450, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.66 : Tensor = prim::If(%451) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %453 : float = prim::GetAttr[name="drop_rate"](%77)
      %454 : bool = prim::GetAttr[name="training"](%77)
      %455 : bool = aten::lt(%453, %16) # torch/nn/functional.py:968:7
      %456 : bool = prim::If(%455) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %457 : bool = aten::gt(%453, %17) # torch/nn/functional.py:968:17
          -> (%457)
       = prim::If(%456) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %458 : Tensor = aten::dropout(%new_features.65, %453, %454) # torch/nn/functional.py:973:17
      -> (%458)
    block1():
      -> (%new_features.65)
  %459 : Tensor[] = aten::append(%features.2, %new_features.66) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %460 : Tensor = prim::Uninitialized()
  %461 : bool = prim::GetAttr[name="memory_efficient"](%78)
  %462 : bool = prim::If(%461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %463 : bool = prim::Uninitialized()
      %464 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %465 : bool = aten::gt(%464, %24)
      %466 : bool, %467 : bool, %468 : int = prim::Loop(%18, %465, %19, %463, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%469 : int, %470 : bool, %471 : bool, %472 : int):
          %tensor.33 : Tensor = aten::__getitem__(%features.2, %472) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %474 : bool = prim::requires_grad(%tensor.33)
          %475 : bool, %476 : bool = prim::If(%474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %463)
          %477 : int = aten::add(%472, %27)
          %478 : bool = aten::lt(%477, %464)
          %479 : bool = aten::__and__(%478, %475)
          -> (%479, %474, %476, %477)
      %480 : bool = prim::If(%466)
        block0():
          -> (%467)
        block1():
          -> (%19)
      -> (%480)
    block1():
      -> (%19)
  %bottleneck_output.64 : Tensor = prim::If(%462) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%460)
    block1():
      %concated_features.33 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %483 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%78)
      %484 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%78)
      %485 : int = aten::dim(%concated_features.33) # torch/nn/modules/batchnorm.py:276:11
      %486 : bool = aten::ne(%485, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%486) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %487 : bool = prim::GetAttr[name="training"](%484)
       = prim::If(%487) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %488 : Tensor = prim::GetAttr[name="num_batches_tracked"](%484)
          %489 : Tensor = aten::add(%488, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%484, %489)
          -> ()
        block1():
          -> ()
      %490 : bool = prim::GetAttr[name="training"](%484)
      %491 : Tensor = prim::GetAttr[name="running_mean"](%484)
      %492 : Tensor = prim::GetAttr[name="running_var"](%484)
      %493 : Tensor = prim::GetAttr[name="weight"](%484)
      %494 : Tensor = prim::GetAttr[name="bias"](%484)
       = prim::If(%490) # torch/nn/functional.py:2011:4
        block0():
          %495 : int[] = aten::size(%concated_features.33) # torch/nn/functional.py:2012:27
          %size_prods.284 : int = aten::__getitem__(%495, %24) # torch/nn/functional.py:1991:17
          %497 : int = aten::len(%495) # torch/nn/functional.py:1992:19
          %498 : int = aten::sub(%497, %26) # torch/nn/functional.py:1992:19
          %size_prods.285 : int = prim::Loop(%498, %25, %size_prods.284) # torch/nn/functional.py:1992:4
            block0(%i.72 : int, %size_prods.286 : int):
              %502 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
              %503 : int = aten::__getitem__(%495, %502) # torch/nn/functional.py:1993:22
              %size_prods.287 : int = aten::mul(%size_prods.286, %503) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.287)
          %505 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
           = prim::If(%505) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %506 : Tensor = aten::batch_norm(%concated_features.33, %493, %494, %491, %492, %490, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.65 : Tensor = aten::relu_(%506) # torch/nn/functional.py:1117:17
      %508 : Tensor = prim::GetAttr[name="weight"](%483)
      %509 : Tensor? = prim::GetAttr[name="bias"](%483)
      %510 : int[] = prim::ListConstruct(%27, %27)
      %511 : int[] = prim::ListConstruct(%24, %24)
      %512 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.65 : Tensor = aten::conv2d(%result.65, %508, %509, %510, %511, %512, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.65)
  %514 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%78)
  %515 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%78)
  %516 : int = aten::dim(%bottleneck_output.64) # torch/nn/modules/batchnorm.py:276:11
  %517 : bool = aten::ne(%516, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%517) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %518 : bool = prim::GetAttr[name="training"](%515)
   = prim::If(%518) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %519 : Tensor = prim::GetAttr[name="num_batches_tracked"](%515)
      %520 : Tensor = aten::add(%519, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%515, %520)
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%515)
  %522 : Tensor = prim::GetAttr[name="running_mean"](%515)
  %523 : Tensor = prim::GetAttr[name="running_var"](%515)
  %524 : Tensor = prim::GetAttr[name="weight"](%515)
  %525 : Tensor = prim::GetAttr[name="bias"](%515)
   = prim::If(%521) # torch/nn/functional.py:2011:4
    block0():
      %526 : int[] = aten::size(%bottleneck_output.64) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%526, %24) # torch/nn/functional.py:1991:17
      %528 : int = aten::len(%526) # torch/nn/functional.py:1992:19
      %529 : int = aten::sub(%528, %26) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%529, %25, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %533 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
          %534 : int = aten::__getitem__(%526, %533) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %534) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.291)
      %536 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
       = prim::If(%536) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %537 : Tensor = aten::batch_norm(%bottleneck_output.64, %524, %525, %522, %523, %521, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.66 : Tensor = aten::relu_(%537) # torch/nn/functional.py:1117:17
  %539 : Tensor = prim::GetAttr[name="weight"](%514)
  %540 : Tensor? = prim::GetAttr[name="bias"](%514)
  %541 : int[] = prim::ListConstruct(%27, %27)
  %542 : int[] = prim::ListConstruct(%27, %27)
  %543 : int[] = prim::ListConstruct(%27, %27)
  %new_features.67 : Tensor = aten::conv2d(%result.66, %539, %540, %541, %542, %543, %27) # torch/nn/modules/conv.py:415:15
  %545 : float = prim::GetAttr[name="drop_rate"](%78)
  %546 : bool = aten::gt(%545, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.68 : Tensor = prim::If(%546) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %548 : float = prim::GetAttr[name="drop_rate"](%78)
      %549 : bool = prim::GetAttr[name="training"](%78)
      %550 : bool = aten::lt(%548, %16) # torch/nn/functional.py:968:7
      %551 : bool = prim::If(%550) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %552 : bool = aten::gt(%548, %17) # torch/nn/functional.py:968:17
          -> (%552)
       = prim::If(%551) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %553 : Tensor = aten::dropout(%new_features.67, %548, %549) # torch/nn/functional.py:973:17
      -> (%553)
    block1():
      -> (%new_features.67)
  %554 : Tensor[] = aten::append(%features.2, %new_features.68) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %555 : Tensor = prim::Uninitialized()
  %556 : bool = prim::GetAttr[name="memory_efficient"](%79)
  %557 : bool = prim::If(%556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %558 : bool = prim::Uninitialized()
      %559 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %560 : bool = aten::gt(%559, %24)
      %561 : bool, %562 : bool, %563 : int = prim::Loop(%18, %560, %19, %558, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%564 : int, %565 : bool, %566 : bool, %567 : int):
          %tensor.35 : Tensor = aten::__getitem__(%features.2, %567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %569 : bool = prim::requires_grad(%tensor.35)
          %570 : bool, %571 : bool = prim::If(%569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %558)
          %572 : int = aten::add(%567, %27)
          %573 : bool = aten::lt(%572, %559)
          %574 : bool = aten::__and__(%573, %570)
          -> (%574, %569, %571, %572)
      %575 : bool = prim::If(%561)
        block0():
          -> (%562)
        block1():
          -> (%19)
      -> (%575)
    block1():
      -> (%19)
  %bottleneck_output.68 : Tensor = prim::If(%557) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%555)
    block1():
      %concated_features.35 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %578 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%79)
      %579 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%79)
      %580 : int = aten::dim(%concated_features.35) # torch/nn/modules/batchnorm.py:276:11
      %581 : bool = aten::ne(%580, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%581) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %582 : bool = prim::GetAttr[name="training"](%579)
       = prim::If(%582) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %583 : Tensor = prim::GetAttr[name="num_batches_tracked"](%579)
          %584 : Tensor = aten::add(%583, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%579, %584)
          -> ()
        block1():
          -> ()
      %585 : bool = prim::GetAttr[name="training"](%579)
      %586 : Tensor = prim::GetAttr[name="running_mean"](%579)
      %587 : Tensor = prim::GetAttr[name="running_var"](%579)
      %588 : Tensor = prim::GetAttr[name="weight"](%579)
      %589 : Tensor = prim::GetAttr[name="bias"](%579)
       = prim::If(%585) # torch/nn/functional.py:2011:4
        block0():
          %590 : int[] = aten::size(%concated_features.35) # torch/nn/functional.py:2012:27
          %size_prods.292 : int = aten::__getitem__(%590, %24) # torch/nn/functional.py:1991:17
          %592 : int = aten::len(%590) # torch/nn/functional.py:1992:19
          %593 : int = aten::sub(%592, %26) # torch/nn/functional.py:1992:19
          %size_prods.293 : int = prim::Loop(%593, %25, %size_prods.292) # torch/nn/functional.py:1992:4
            block0(%i.74 : int, %size_prods.294 : int):
              %597 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
              %598 : int = aten::__getitem__(%590, %597) # torch/nn/functional.py:1993:22
              %size_prods.295 : int = aten::mul(%size_prods.294, %598) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.295)
          %600 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
           = prim::If(%600) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %601 : Tensor = aten::batch_norm(%concated_features.35, %588, %589, %586, %587, %585, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.69 : Tensor = aten::relu_(%601) # torch/nn/functional.py:1117:17
      %603 : Tensor = prim::GetAttr[name="weight"](%578)
      %604 : Tensor? = prim::GetAttr[name="bias"](%578)
      %605 : int[] = prim::ListConstruct(%27, %27)
      %606 : int[] = prim::ListConstruct(%24, %24)
      %607 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.69 : Tensor = aten::conv2d(%result.69, %603, %604, %605, %606, %607, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.69)
  %609 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%79)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%79)
  %611 : int = aten::dim(%bottleneck_output.68) # torch/nn/modules/batchnorm.py:276:11
  %612 : bool = aten::ne(%611, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %613 : bool = prim::GetAttr[name="training"](%610)
   = prim::If(%613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%610)
      %615 : Tensor = aten::add(%614, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%610, %615)
      -> ()
    block1():
      -> ()
  %616 : bool = prim::GetAttr[name="training"](%610)
  %617 : Tensor = prim::GetAttr[name="running_mean"](%610)
  %618 : Tensor = prim::GetAttr[name="running_var"](%610)
  %619 : Tensor = prim::GetAttr[name="weight"](%610)
  %620 : Tensor = prim::GetAttr[name="bias"](%610)
   = prim::If(%616) # torch/nn/functional.py:2011:4
    block0():
      %621 : int[] = aten::size(%bottleneck_output.68) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%621, %24) # torch/nn/functional.py:1991:17
      %623 : int = aten::len(%621) # torch/nn/functional.py:1992:19
      %624 : int = aten::sub(%623, %26) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%624, %25, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %628 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
          %629 : int = aten::__getitem__(%621, %628) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %629) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.251)
      %631 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
       = prim::If(%631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %632 : Tensor = aten::batch_norm(%bottleneck_output.68, %619, %620, %617, %618, %616, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.70 : Tensor = aten::relu_(%632) # torch/nn/functional.py:1117:17
  %634 : Tensor = prim::GetAttr[name="weight"](%609)
  %635 : Tensor? = prim::GetAttr[name="bias"](%609)
  %636 : int[] = prim::ListConstruct(%27, %27)
  %637 : int[] = prim::ListConstruct(%27, %27)
  %638 : int[] = prim::ListConstruct(%27, %27)
  %new_features.72 : Tensor = aten::conv2d(%result.70, %634, %635, %636, %637, %638, %27) # torch/nn/modules/conv.py:415:15
  %640 : float = prim::GetAttr[name="drop_rate"](%79)
  %641 : bool = aten::gt(%640, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.70 : Tensor = prim::If(%641) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %643 : float = prim::GetAttr[name="drop_rate"](%79)
      %644 : bool = prim::GetAttr[name="training"](%79)
      %645 : bool = aten::lt(%643, %16) # torch/nn/functional.py:968:7
      %646 : bool = prim::If(%645) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %647 : bool = aten::gt(%643, %17) # torch/nn/functional.py:968:17
          -> (%647)
       = prim::If(%646) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %648 : Tensor = aten::dropout(%new_features.72, %643, %644) # torch/nn/functional.py:973:17
      -> (%648)
    block1():
      -> (%new_features.72)
  %649 : Tensor[] = aten::append(%features.2, %new_features.70) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.11 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %651 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm"](%32)
  %652 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv"](%32)
  %653 : int = aten::dim(%input.11) # torch/nn/modules/batchnorm.py:276:11
  %654 : bool = aten::ne(%653, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%654) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %655 : bool = prim::GetAttr[name="training"](%651)
   = prim::If(%655) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %656 : Tensor = prim::GetAttr[name="num_batches_tracked"](%651)
      %657 : Tensor = aten::add(%656, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%651, %657)
      -> ()
    block1():
      -> ()
  %658 : bool = prim::GetAttr[name="training"](%651)
  %659 : Tensor = prim::GetAttr[name="running_mean"](%651)
  %660 : Tensor = prim::GetAttr[name="running_var"](%651)
  %661 : Tensor = prim::GetAttr[name="weight"](%651)
  %662 : Tensor = prim::GetAttr[name="bias"](%651)
   = prim::If(%658) # torch/nn/functional.py:2011:4
    block0():
      %663 : int[] = aten::size(%input.11) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%663, %24) # torch/nn/functional.py:1991:17
      %665 : int = aten::len(%663) # torch/nn/functional.py:1992:19
      %666 : int = aten::sub(%665, %26) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%666, %25, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %670 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
          %671 : int = aten::__getitem__(%663, %670) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %671) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.299)
      %673 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
       = prim::If(%673) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.11, %661, %662, %659, %660, %658, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.12 : Tensor = aten::relu_(%input.16) # torch/nn/functional.py:1117:17
  %676 : Tensor = prim::GetAttr[name="weight"](%652)
  %677 : Tensor? = prim::GetAttr[name="bias"](%652)
  %678 : int[] = prim::ListConstruct(%27, %27)
  %679 : int[] = prim::ListConstruct(%24, %24)
  %680 : int[] = prim::ListConstruct(%27, %27)
  %input.14 : Tensor = aten::conv2d(%input.12, %676, %677, %678, %679, %680, %27) # torch/nn/modules/conv.py:415:15
  %682 : int[] = prim::ListConstruct(%26, %26)
  %683 : int[] = prim::ListConstruct(%26, %26)
  %684 : int[] = prim::ListConstruct(%24, %24)
  %input.13 : Tensor = aten::avg_pool2d(%input.14, %682, %683, %684, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.3 : Tensor[] = prim::ListConstruct(%input.13)
  %687 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer1"](%33)
  %688 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer2"](%33)
  %689 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer3"](%33)
  %690 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer4"](%33)
  %691 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer5"](%33)
  %692 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer6"](%33)
  %693 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer7"](%33)
  %694 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer8"](%33)
  %695 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer9"](%33)
  %696 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer10"](%33)
  %697 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer11"](%33)
  %698 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer12"](%33)
  %699 : Tensor = prim::Uninitialized()
  %700 : bool = prim::GetAttr[name="memory_efficient"](%687)
  %701 : bool = prim::If(%700) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %702 : bool = prim::Uninitialized()
      %703 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %704 : bool = aten::gt(%703, %24)
      %705 : bool, %706 : bool, %707 : int = prim::Loop(%18, %704, %19, %702, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%708 : int, %709 : bool, %710 : bool, %711 : int):
          %tensor.36 : Tensor = aten::__getitem__(%features.3, %711) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %713 : bool = prim::requires_grad(%tensor.36)
          %714 : bool, %715 : bool = prim::If(%713) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %702)
          %716 : int = aten::add(%711, %27)
          %717 : bool = aten::lt(%716, %703)
          %718 : bool = aten::__and__(%717, %714)
          -> (%718, %713, %715, %716)
      %719 : bool = prim::If(%705)
        block0():
          -> (%706)
        block1():
          -> (%19)
      -> (%719)
    block1():
      -> (%19)
  %bottleneck_output.70 : Tensor = prim::If(%701) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%699)
    block1():
      %concated_features.36 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %722 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%687)
      %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%687)
      %724 : int = aten::dim(%concated_features.36) # torch/nn/modules/batchnorm.py:276:11
      %725 : bool = aten::ne(%724, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%725) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %726 : bool = prim::GetAttr[name="training"](%723)
       = prim::If(%726) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
          %728 : Tensor = aten::add(%727, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%723, %728)
          -> ()
        block1():
          -> ()
      %729 : bool = prim::GetAttr[name="training"](%723)
      %730 : Tensor = prim::GetAttr[name="running_mean"](%723)
      %731 : Tensor = prim::GetAttr[name="running_var"](%723)
      %732 : Tensor = prim::GetAttr[name="weight"](%723)
      %733 : Tensor = prim::GetAttr[name="bias"](%723)
       = prim::If(%729) # torch/nn/functional.py:2011:4
        block0():
          %734 : int[] = aten::size(%concated_features.36) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%734, %24) # torch/nn/functional.py:1991:17
          %736 : int = aten::len(%734) # torch/nn/functional.py:1992:19
          %737 : int = aten::sub(%736, %26) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%737, %25, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %741 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
              %742 : int = aten::__getitem__(%734, %741) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %742) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.307)
          %744 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
           = prim::If(%744) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %745 : Tensor = aten::batch_norm(%concated_features.36, %732, %733, %730, %731, %729, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.71 : Tensor = aten::relu_(%745) # torch/nn/functional.py:1117:17
      %747 : Tensor = prim::GetAttr[name="weight"](%722)
      %748 : Tensor? = prim::GetAttr[name="bias"](%722)
      %749 : int[] = prim::ListConstruct(%27, %27)
      %750 : int[] = prim::ListConstruct(%24, %24)
      %751 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.71 : Tensor = aten::conv2d(%result.71, %747, %748, %749, %750, %751, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.71)
  %753 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%687)
  %754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%687)
  %755 : int = aten::dim(%bottleneck_output.70) # torch/nn/modules/batchnorm.py:276:11
  %756 : bool = aten::ne(%755, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%756) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %757 : bool = prim::GetAttr[name="training"](%754)
   = prim::If(%757) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %758 : Tensor = prim::GetAttr[name="num_batches_tracked"](%754)
      %759 : Tensor = aten::add(%758, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%754, %759)
      -> ()
    block1():
      -> ()
  %760 : bool = prim::GetAttr[name="training"](%754)
  %761 : Tensor = prim::GetAttr[name="running_mean"](%754)
  %762 : Tensor = prim::GetAttr[name="running_var"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Tensor = prim::GetAttr[name="bias"](%754)
   = prim::If(%760) # torch/nn/functional.py:2011:4
    block0():
      %765 : int[] = aten::size(%bottleneck_output.70) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%765, %24) # torch/nn/functional.py:1991:17
      %767 : int = aten::len(%765) # torch/nn/functional.py:1992:19
      %768 : int = aten::sub(%767, %26) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%768, %25, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %772 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
          %773 : int = aten::__getitem__(%765, %772) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %773) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.311)
      %775 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
       = prim::If(%775) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %776 : Tensor = aten::batch_norm(%bottleneck_output.70, %763, %764, %761, %762, %760, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.72 : Tensor = aten::relu_(%776) # torch/nn/functional.py:1117:17
  %778 : Tensor = prim::GetAttr[name="weight"](%753)
  %779 : Tensor? = prim::GetAttr[name="bias"](%753)
  %780 : int[] = prim::ListConstruct(%27, %27)
  %781 : int[] = prim::ListConstruct(%27, %27)
  %782 : int[] = prim::ListConstruct(%27, %27)
  %new_features.74 : Tensor = aten::conv2d(%result.72, %778, %779, %780, %781, %782, %27) # torch/nn/modules/conv.py:415:15
  %784 : float = prim::GetAttr[name="drop_rate"](%687)
  %785 : bool = aten::gt(%784, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.71 : Tensor = prim::If(%785) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %787 : float = prim::GetAttr[name="drop_rate"](%687)
      %788 : bool = prim::GetAttr[name="training"](%687)
      %789 : bool = aten::lt(%787, %16) # torch/nn/functional.py:968:7
      %790 : bool = prim::If(%789) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %791 : bool = aten::gt(%787, %17) # torch/nn/functional.py:968:17
          -> (%791)
       = prim::If(%790) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %792 : Tensor = aten::dropout(%new_features.74, %787, %788) # torch/nn/functional.py:973:17
      -> (%792)
    block1():
      -> (%new_features.74)
  %793 : Tensor[] = aten::append(%features.3, %new_features.71) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %794 : Tensor = prim::Uninitialized()
  %795 : bool = prim::GetAttr[name="memory_efficient"](%688)
  %796 : bool = prim::If(%795) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %797 : bool = prim::Uninitialized()
      %798 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %799 : bool = aten::gt(%798, %24)
      %800 : bool, %801 : bool, %802 : int = prim::Loop(%18, %799, %19, %797, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%803 : int, %804 : bool, %805 : bool, %806 : int):
          %tensor.37 : Tensor = aten::__getitem__(%features.3, %806) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %808 : bool = prim::requires_grad(%tensor.37)
          %809 : bool, %810 : bool = prim::If(%808) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %797)
          %811 : int = aten::add(%806, %27)
          %812 : bool = aten::lt(%811, %798)
          %813 : bool = aten::__and__(%812, %809)
          -> (%813, %808, %810, %811)
      %814 : bool = prim::If(%800)
        block0():
          -> (%801)
        block1():
          -> (%19)
      -> (%814)
    block1():
      -> (%19)
  %bottleneck_output.72 : Tensor = prim::If(%796) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%794)
    block1():
      %concated_features.37 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %817 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%688)
      %818 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%688)
      %819 : int = aten::dim(%concated_features.37) # torch/nn/modules/batchnorm.py:276:11
      %820 : bool = aten::ne(%819, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%820) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %821 : bool = prim::GetAttr[name="training"](%818)
       = prim::If(%821) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %822 : Tensor = prim::GetAttr[name="num_batches_tracked"](%818)
          %823 : Tensor = aten::add(%822, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%818, %823)
          -> ()
        block1():
          -> ()
      %824 : bool = prim::GetAttr[name="training"](%818)
      %825 : Tensor = prim::GetAttr[name="running_mean"](%818)
      %826 : Tensor = prim::GetAttr[name="running_var"](%818)
      %827 : Tensor = prim::GetAttr[name="weight"](%818)
      %828 : Tensor = prim::GetAttr[name="bias"](%818)
       = prim::If(%824) # torch/nn/functional.py:2011:4
        block0():
          %829 : int[] = aten::size(%concated_features.37) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%829, %24) # torch/nn/functional.py:1991:17
          %831 : int = aten::len(%829) # torch/nn/functional.py:1992:19
          %832 : int = aten::sub(%831, %26) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%832, %25, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %836 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
              %837 : int = aten::__getitem__(%829, %836) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %837) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.315)
          %839 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
           = prim::If(%839) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %840 : Tensor = aten::batch_norm(%concated_features.37, %827, %828, %825, %826, %824, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.73 : Tensor = aten::relu_(%840) # torch/nn/functional.py:1117:17
      %842 : Tensor = prim::GetAttr[name="weight"](%817)
      %843 : Tensor? = prim::GetAttr[name="bias"](%817)
      %844 : int[] = prim::ListConstruct(%27, %27)
      %845 : int[] = prim::ListConstruct(%24, %24)
      %846 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.73 : Tensor = aten::conv2d(%result.73, %842, %843, %844, %845, %846, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.73)
  %848 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%688)
  %849 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%688)
  %850 : int = aten::dim(%bottleneck_output.72) # torch/nn/modules/batchnorm.py:276:11
  %851 : bool = aten::ne(%850, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%851) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %852 : bool = prim::GetAttr[name="training"](%849)
   = prim::If(%852) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %853 : Tensor = prim::GetAttr[name="num_batches_tracked"](%849)
      %854 : Tensor = aten::add(%853, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%849, %854)
      -> ()
    block1():
      -> ()
  %855 : bool = prim::GetAttr[name="training"](%849)
  %856 : Tensor = prim::GetAttr[name="running_mean"](%849)
  %857 : Tensor = prim::GetAttr[name="running_var"](%849)
  %858 : Tensor = prim::GetAttr[name="weight"](%849)
  %859 : Tensor = prim::GetAttr[name="bias"](%849)
   = prim::If(%855) # torch/nn/functional.py:2011:4
    block0():
      %860 : int[] = aten::size(%bottleneck_output.72) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%860, %24) # torch/nn/functional.py:1991:17
      %862 : int = aten::len(%860) # torch/nn/functional.py:1992:19
      %863 : int = aten::sub(%862, %26) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%863, %25, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %867 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
          %868 : int = aten::__getitem__(%860, %867) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %868) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.319)
      %870 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
       = prim::If(%870) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %871 : Tensor = aten::batch_norm(%bottleneck_output.72, %858, %859, %856, %857, %855, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.74 : Tensor = aten::relu_(%871) # torch/nn/functional.py:1117:17
  %873 : Tensor = prim::GetAttr[name="weight"](%848)
  %874 : Tensor? = prim::GetAttr[name="bias"](%848)
  %875 : int[] = prim::ListConstruct(%27, %27)
  %876 : int[] = prim::ListConstruct(%27, %27)
  %877 : int[] = prim::ListConstruct(%27, %27)
  %new_features.76 : Tensor = aten::conv2d(%result.74, %873, %874, %875, %876, %877, %27) # torch/nn/modules/conv.py:415:15
  %879 : float = prim::GetAttr[name="drop_rate"](%688)
  %880 : bool = aten::gt(%879, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.73 : Tensor = prim::If(%880) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %882 : float = prim::GetAttr[name="drop_rate"](%688)
      %883 : bool = prim::GetAttr[name="training"](%688)
      %884 : bool = aten::lt(%882, %16) # torch/nn/functional.py:968:7
      %885 : bool = prim::If(%884) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %886 : bool = aten::gt(%882, %17) # torch/nn/functional.py:968:17
          -> (%886)
       = prim::If(%885) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %887 : Tensor = aten::dropout(%new_features.76, %882, %883) # torch/nn/functional.py:973:17
      -> (%887)
    block1():
      -> (%new_features.76)
  %888 : Tensor[] = aten::append(%features.3, %new_features.73) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %889 : Tensor = prim::Uninitialized()
  %890 : bool = prim::GetAttr[name="memory_efficient"](%689)
  %891 : bool = prim::If(%890) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %892 : bool = prim::Uninitialized()
      %893 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %894 : bool = aten::gt(%893, %24)
      %895 : bool, %896 : bool, %897 : int = prim::Loop(%18, %894, %19, %892, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%898 : int, %899 : bool, %900 : bool, %901 : int):
          %tensor.38 : Tensor = aten::__getitem__(%features.3, %901) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %903 : bool = prim::requires_grad(%tensor.38)
          %904 : bool, %905 : bool = prim::If(%903) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %892)
          %906 : int = aten::add(%901, %27)
          %907 : bool = aten::lt(%906, %893)
          %908 : bool = aten::__and__(%907, %904)
          -> (%908, %903, %905, %906)
      %909 : bool = prim::If(%895)
        block0():
          -> (%896)
        block1():
          -> (%19)
      -> (%909)
    block1():
      -> (%19)
  %bottleneck_output.74 : Tensor = prim::If(%891) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%889)
    block1():
      %concated_features.38 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %912 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%689)
      %913 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%689)
      %914 : int = aten::dim(%concated_features.38) # torch/nn/modules/batchnorm.py:276:11
      %915 : bool = aten::ne(%914, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%915) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %916 : bool = prim::GetAttr[name="training"](%913)
       = prim::If(%916) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %917 : Tensor = prim::GetAttr[name="num_batches_tracked"](%913)
          %918 : Tensor = aten::add(%917, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%913, %918)
          -> ()
        block1():
          -> ()
      %919 : bool = prim::GetAttr[name="training"](%913)
      %920 : Tensor = prim::GetAttr[name="running_mean"](%913)
      %921 : Tensor = prim::GetAttr[name="running_var"](%913)
      %922 : Tensor = prim::GetAttr[name="weight"](%913)
      %923 : Tensor = prim::GetAttr[name="bias"](%913)
       = prim::If(%919) # torch/nn/functional.py:2011:4
        block0():
          %924 : int[] = aten::size(%concated_features.38) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%924, %24) # torch/nn/functional.py:1991:17
          %926 : int = aten::len(%924) # torch/nn/functional.py:1992:19
          %927 : int = aten::sub(%926, %26) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%927, %25, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %931 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
              %932 : int = aten::__getitem__(%924, %931) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %932) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.323)
          %934 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
           = prim::If(%934) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %935 : Tensor = aten::batch_norm(%concated_features.38, %922, %923, %920, %921, %919, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.75 : Tensor = aten::relu_(%935) # torch/nn/functional.py:1117:17
      %937 : Tensor = prim::GetAttr[name="weight"](%912)
      %938 : Tensor? = prim::GetAttr[name="bias"](%912)
      %939 : int[] = prim::ListConstruct(%27, %27)
      %940 : int[] = prim::ListConstruct(%24, %24)
      %941 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.75 : Tensor = aten::conv2d(%result.75, %937, %938, %939, %940, %941, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.75)
  %943 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%689)
  %944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%689)
  %945 : int = aten::dim(%bottleneck_output.74) # torch/nn/modules/batchnorm.py:276:11
  %946 : bool = aten::ne(%945, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%946) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %947 : bool = prim::GetAttr[name="training"](%944)
   = prim::If(%947) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %948 : Tensor = prim::GetAttr[name="num_batches_tracked"](%944)
      %949 : Tensor = aten::add(%948, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%944, %949)
      -> ()
    block1():
      -> ()
  %950 : bool = prim::GetAttr[name="training"](%944)
  %951 : Tensor = prim::GetAttr[name="running_mean"](%944)
  %952 : Tensor = prim::GetAttr[name="running_var"](%944)
  %953 : Tensor = prim::GetAttr[name="weight"](%944)
  %954 : Tensor = prim::GetAttr[name="bias"](%944)
   = prim::If(%950) # torch/nn/functional.py:2011:4
    block0():
      %955 : int[] = aten::size(%bottleneck_output.74) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%955, %24) # torch/nn/functional.py:1991:17
      %957 : int = aten::len(%955) # torch/nn/functional.py:1992:19
      %958 : int = aten::sub(%957, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%958, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %962 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %963 : int = aten::__getitem__(%955, %962) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %963) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %965 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%965) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %966 : Tensor = aten::batch_norm(%bottleneck_output.74, %953, %954, %951, %952, %950, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.76 : Tensor = aten::relu_(%966) # torch/nn/functional.py:1117:17
  %968 : Tensor = prim::GetAttr[name="weight"](%943)
  %969 : Tensor? = prim::GetAttr[name="bias"](%943)
  %970 : int[] = prim::ListConstruct(%27, %27)
  %971 : int[] = prim::ListConstruct(%27, %27)
  %972 : int[] = prim::ListConstruct(%27, %27)
  %new_features.78 : Tensor = aten::conv2d(%result.76, %968, %969, %970, %971, %972, %27) # torch/nn/modules/conv.py:415:15
  %974 : float = prim::GetAttr[name="drop_rate"](%689)
  %975 : bool = aten::gt(%974, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.75 : Tensor = prim::If(%975) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %977 : float = prim::GetAttr[name="drop_rate"](%689)
      %978 : bool = prim::GetAttr[name="training"](%689)
      %979 : bool = aten::lt(%977, %16) # torch/nn/functional.py:968:7
      %980 : bool = prim::If(%979) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %981 : bool = aten::gt(%977, %17) # torch/nn/functional.py:968:17
          -> (%981)
       = prim::If(%980) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %982 : Tensor = aten::dropout(%new_features.78, %977, %978) # torch/nn/functional.py:973:17
      -> (%982)
    block1():
      -> (%new_features.78)
  %983 : Tensor[] = aten::append(%features.3, %new_features.75) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %984 : Tensor = prim::Uninitialized()
  %985 : bool = prim::GetAttr[name="memory_efficient"](%690)
  %986 : bool = prim::If(%985) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %987 : bool = prim::Uninitialized()
      %988 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %989 : bool = aten::gt(%988, %24)
      %990 : bool, %991 : bool, %992 : int = prim::Loop(%18, %989, %19, %987, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%993 : int, %994 : bool, %995 : bool, %996 : int):
          %tensor.39 : Tensor = aten::__getitem__(%features.3, %996) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %998 : bool = prim::requires_grad(%tensor.39)
          %999 : bool, %1000 : bool = prim::If(%998) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %987)
          %1001 : int = aten::add(%996, %27)
          %1002 : bool = aten::lt(%1001, %988)
          %1003 : bool = aten::__and__(%1002, %999)
          -> (%1003, %998, %1000, %1001)
      %1004 : bool = prim::If(%990)
        block0():
          -> (%991)
        block1():
          -> (%19)
      -> (%1004)
    block1():
      -> (%19)
  %bottleneck_output.76 : Tensor = prim::If(%986) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%984)
    block1():
      %concated_features.39 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1007 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%690)
      %1008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%690)
      %1009 : int = aten::dim(%concated_features.39) # torch/nn/modules/batchnorm.py:276:11
      %1010 : bool = aten::ne(%1009, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1010) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1011 : bool = prim::GetAttr[name="training"](%1008)
       = prim::If(%1011) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1008)
          %1013 : Tensor = aten::add(%1012, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1008, %1013)
          -> ()
        block1():
          -> ()
      %1014 : bool = prim::GetAttr[name="training"](%1008)
      %1015 : Tensor = prim::GetAttr[name="running_mean"](%1008)
      %1016 : Tensor = prim::GetAttr[name="running_var"](%1008)
      %1017 : Tensor = prim::GetAttr[name="weight"](%1008)
      %1018 : Tensor = prim::GetAttr[name="bias"](%1008)
       = prim::If(%1014) # torch/nn/functional.py:2011:4
        block0():
          %1019 : int[] = aten::size(%concated_features.39) # torch/nn/functional.py:2012:27
          %size_prods.328 : int = aten::__getitem__(%1019, %24) # torch/nn/functional.py:1991:17
          %1021 : int = aten::len(%1019) # torch/nn/functional.py:1992:19
          %1022 : int = aten::sub(%1021, %26) # torch/nn/functional.py:1992:19
          %size_prods.329 : int = prim::Loop(%1022, %25, %size_prods.328) # torch/nn/functional.py:1992:4
            block0(%i.83 : int, %size_prods.330 : int):
              %1026 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
              %1027 : int = aten::__getitem__(%1019, %1026) # torch/nn/functional.py:1993:22
              %size_prods.331 : int = aten::mul(%size_prods.330, %1027) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.331)
          %1029 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1029) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1030 : Tensor = aten::batch_norm(%concated_features.39, %1017, %1018, %1015, %1016, %1014, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.77 : Tensor = aten::relu_(%1030) # torch/nn/functional.py:1117:17
      %1032 : Tensor = prim::GetAttr[name="weight"](%1007)
      %1033 : Tensor? = prim::GetAttr[name="bias"](%1007)
      %1034 : int[] = prim::ListConstruct(%27, %27)
      %1035 : int[] = prim::ListConstruct(%24, %24)
      %1036 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.77 : Tensor = aten::conv2d(%result.77, %1032, %1033, %1034, %1035, %1036, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.77)
  %1038 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%690)
  %1039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%690)
  %1040 : int = aten::dim(%bottleneck_output.76) # torch/nn/modules/batchnorm.py:276:11
  %1041 : bool = aten::ne(%1040, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1042 : bool = prim::GetAttr[name="training"](%1039)
   = prim::If(%1042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1039)
      %1044 : Tensor = aten::add(%1043, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1039, %1044)
      -> ()
    block1():
      -> ()
  %1045 : bool = prim::GetAttr[name="training"](%1039)
  %1046 : Tensor = prim::GetAttr[name="running_mean"](%1039)
  %1047 : Tensor = prim::GetAttr[name="running_var"](%1039)
  %1048 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1039)
   = prim::If(%1045) # torch/nn/functional.py:2011:4
    block0():
      %1050 : int[] = aten::size(%bottleneck_output.76) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%1050, %24) # torch/nn/functional.py:1991:17
      %1052 : int = aten::len(%1050) # torch/nn/functional.py:1992:19
      %1053 : int = aten::sub(%1052, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%1053, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %1057 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %1058 : int = aten::__getitem__(%1050, %1057) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %1058) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %1060 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1061 : Tensor = aten::batch_norm(%bottleneck_output.76, %1048, %1049, %1046, %1047, %1045, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.78 : Tensor = aten::relu_(%1061) # torch/nn/functional.py:1117:17
  %1063 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1064 : Tensor? = prim::GetAttr[name="bias"](%1038)
  %1065 : int[] = prim::ListConstruct(%27, %27)
  %1066 : int[] = prim::ListConstruct(%27, %27)
  %1067 : int[] = prim::ListConstruct(%27, %27)
  %new_features.80 : Tensor = aten::conv2d(%result.78, %1063, %1064, %1065, %1066, %1067, %27) # torch/nn/modules/conv.py:415:15
  %1069 : float = prim::GetAttr[name="drop_rate"](%690)
  %1070 : bool = aten::gt(%1069, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.77 : Tensor = prim::If(%1070) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1072 : float = prim::GetAttr[name="drop_rate"](%690)
      %1073 : bool = prim::GetAttr[name="training"](%690)
      %1074 : bool = aten::lt(%1072, %16) # torch/nn/functional.py:968:7
      %1075 : bool = prim::If(%1074) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1076 : bool = aten::gt(%1072, %17) # torch/nn/functional.py:968:17
          -> (%1076)
       = prim::If(%1075) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1077 : Tensor = aten::dropout(%new_features.80, %1072, %1073) # torch/nn/functional.py:973:17
      -> (%1077)
    block1():
      -> (%new_features.80)
  %1078 : Tensor[] = aten::append(%features.3, %new_features.77) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1079 : Tensor = prim::Uninitialized()
  %1080 : bool = prim::GetAttr[name="memory_efficient"](%691)
  %1081 : bool = prim::If(%1080) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1082 : bool = prim::Uninitialized()
      %1083 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1084 : bool = aten::gt(%1083, %24)
      %1085 : bool, %1086 : bool, %1087 : int = prim::Loop(%18, %1084, %19, %1082, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1088 : int, %1089 : bool, %1090 : bool, %1091 : int):
          %tensor.40 : Tensor = aten::__getitem__(%features.3, %1091) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1093 : bool = prim::requires_grad(%tensor.40)
          %1094 : bool, %1095 : bool = prim::If(%1093) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1082)
          %1096 : int = aten::add(%1091, %27)
          %1097 : bool = aten::lt(%1096, %1083)
          %1098 : bool = aten::__and__(%1097, %1094)
          -> (%1098, %1093, %1095, %1096)
      %1099 : bool = prim::If(%1085)
        block0():
          -> (%1086)
        block1():
          -> (%19)
      -> (%1099)
    block1():
      -> (%19)
  %bottleneck_output.78 : Tensor = prim::If(%1081) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1079)
    block1():
      %concated_features.40 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1102 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%691)
      %1103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%691)
      %1104 : int = aten::dim(%concated_features.40) # torch/nn/modules/batchnorm.py:276:11
      %1105 : bool = aten::ne(%1104, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1105) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1106 : bool = prim::GetAttr[name="training"](%1103)
       = prim::If(%1106) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1103)
          %1108 : Tensor = aten::add(%1107, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1103, %1108)
          -> ()
        block1():
          -> ()
      %1109 : bool = prim::GetAttr[name="training"](%1103)
      %1110 : Tensor = prim::GetAttr[name="running_mean"](%1103)
      %1111 : Tensor = prim::GetAttr[name="running_var"](%1103)
      %1112 : Tensor = prim::GetAttr[name="weight"](%1103)
      %1113 : Tensor = prim::GetAttr[name="bias"](%1103)
       = prim::If(%1109) # torch/nn/functional.py:2011:4
        block0():
          %1114 : int[] = aten::size(%concated_features.40) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%1114, %24) # torch/nn/functional.py:1991:17
          %1116 : int = aten::len(%1114) # torch/nn/functional.py:1992:19
          %1117 : int = aten::sub(%1116, %26) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%1117, %25, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %1121 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
              %1122 : int = aten::__getitem__(%1114, %1121) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %1122) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.339)
          %1124 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1124) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1125 : Tensor = aten::batch_norm(%concated_features.40, %1112, %1113, %1110, %1111, %1109, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.79 : Tensor = aten::relu_(%1125) # torch/nn/functional.py:1117:17
      %1127 : Tensor = prim::GetAttr[name="weight"](%1102)
      %1128 : Tensor? = prim::GetAttr[name="bias"](%1102)
      %1129 : int[] = prim::ListConstruct(%27, %27)
      %1130 : int[] = prim::ListConstruct(%24, %24)
      %1131 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.79 : Tensor = aten::conv2d(%result.79, %1127, %1128, %1129, %1130, %1131, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.79)
  %1133 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%691)
  %1134 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%691)
  %1135 : int = aten::dim(%bottleneck_output.78) # torch/nn/modules/batchnorm.py:276:11
  %1136 : bool = aten::ne(%1135, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1136) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1137 : bool = prim::GetAttr[name="training"](%1134)
   = prim::If(%1137) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1138 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1134)
      %1139 : Tensor = aten::add(%1138, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1134, %1139)
      -> ()
    block1():
      -> ()
  %1140 : bool = prim::GetAttr[name="training"](%1134)
  %1141 : Tensor = prim::GetAttr[name="running_mean"](%1134)
  %1142 : Tensor = prim::GetAttr[name="running_var"](%1134)
  %1143 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
   = prim::If(%1140) # torch/nn/functional.py:2011:4
    block0():
      %1145 : int[] = aten::size(%bottleneck_output.78) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%1145, %24) # torch/nn/functional.py:1991:17
      %1147 : int = aten::len(%1145) # torch/nn/functional.py:1992:19
      %1148 : int = aten::sub(%1147, %26) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%1148, %25, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %1152 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
          %1153 : int = aten::__getitem__(%1145, %1152) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %1153) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.195)
      %1155 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1155) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1156 : Tensor = aten::batch_norm(%bottleneck_output.78, %1143, %1144, %1141, %1142, %1140, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.80 : Tensor = aten::relu_(%1156) # torch/nn/functional.py:1117:17
  %1158 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1159 : Tensor? = prim::GetAttr[name="bias"](%1133)
  %1160 : int[] = prim::ListConstruct(%27, %27)
  %1161 : int[] = prim::ListConstruct(%27, %27)
  %1162 : int[] = prim::ListConstruct(%27, %27)
  %new_features.82 : Tensor = aten::conv2d(%result.80, %1158, %1159, %1160, %1161, %1162, %27) # torch/nn/modules/conv.py:415:15
  %1164 : float = prim::GetAttr[name="drop_rate"](%691)
  %1165 : bool = aten::gt(%1164, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.79 : Tensor = prim::If(%1165) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1167 : float = prim::GetAttr[name="drop_rate"](%691)
      %1168 : bool = prim::GetAttr[name="training"](%691)
      %1169 : bool = aten::lt(%1167, %16) # torch/nn/functional.py:968:7
      %1170 : bool = prim::If(%1169) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1171 : bool = aten::gt(%1167, %17) # torch/nn/functional.py:968:17
          -> (%1171)
       = prim::If(%1170) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1172 : Tensor = aten::dropout(%new_features.82, %1167, %1168) # torch/nn/functional.py:973:17
      -> (%1172)
    block1():
      -> (%new_features.82)
  %1173 : Tensor[] = aten::append(%features.3, %new_features.79) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1174 : Tensor = prim::Uninitialized()
  %1175 : bool = prim::GetAttr[name="memory_efficient"](%692)
  %1176 : bool = prim::If(%1175) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1177 : bool = prim::Uninitialized()
      %1178 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1179 : bool = aten::gt(%1178, %24)
      %1180 : bool, %1181 : bool, %1182 : int = prim::Loop(%18, %1179, %19, %1177, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1183 : int, %1184 : bool, %1185 : bool, %1186 : int):
          %tensor.41 : Tensor = aten::__getitem__(%features.3, %1186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1188 : bool = prim::requires_grad(%tensor.41)
          %1189 : bool, %1190 : bool = prim::If(%1188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1177)
          %1191 : int = aten::add(%1186, %27)
          %1192 : bool = aten::lt(%1191, %1178)
          %1193 : bool = aten::__and__(%1192, %1189)
          -> (%1193, %1188, %1190, %1191)
      %1194 : bool = prim::If(%1180)
        block0():
          -> (%1181)
        block1():
          -> (%19)
      -> (%1194)
    block1():
      -> (%19)
  %bottleneck_output.80 : Tensor = prim::If(%1176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1174)
    block1():
      %concated_features.41 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1197 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%692)
      %1198 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%692)
      %1199 : int = aten::dim(%concated_features.41) # torch/nn/modules/batchnorm.py:276:11
      %1200 : bool = aten::ne(%1199, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1200) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1201 : bool = prim::GetAttr[name="training"](%1198)
       = prim::If(%1201) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1202 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1198)
          %1203 : Tensor = aten::add(%1202, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1198, %1203)
          -> ()
        block1():
          -> ()
      %1204 : bool = prim::GetAttr[name="training"](%1198)
      %1205 : Tensor = prim::GetAttr[name="running_mean"](%1198)
      %1206 : Tensor = prim::GetAttr[name="running_var"](%1198)
      %1207 : Tensor = prim::GetAttr[name="weight"](%1198)
      %1208 : Tensor = prim::GetAttr[name="bias"](%1198)
       = prim::If(%1204) # torch/nn/functional.py:2011:4
        block0():
          %1209 : int[] = aten::size(%concated_features.41) # torch/nn/functional.py:2012:27
          %size_prods.196 : int = aten::__getitem__(%1209, %24) # torch/nn/functional.py:1991:17
          %1211 : int = aten::len(%1209) # torch/nn/functional.py:1992:19
          %1212 : int = aten::sub(%1211, %26) # torch/nn/functional.py:1992:19
          %size_prods.197 : int = prim::Loop(%1212, %25, %size_prods.196) # torch/nn/functional.py:1992:4
            block0(%i.50 : int, %size_prods.198 : int):
              %1216 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
              %1217 : int = aten::__getitem__(%1209, %1216) # torch/nn/functional.py:1993:22
              %size_prods.199 : int = aten::mul(%size_prods.198, %1217) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.199)
          %1219 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1219) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1220 : Tensor = aten::batch_norm(%concated_features.41, %1207, %1208, %1205, %1206, %1204, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.81 : Tensor = aten::relu_(%1220) # torch/nn/functional.py:1117:17
      %1222 : Tensor = prim::GetAttr[name="weight"](%1197)
      %1223 : Tensor? = prim::GetAttr[name="bias"](%1197)
      %1224 : int[] = prim::ListConstruct(%27, %27)
      %1225 : int[] = prim::ListConstruct(%24, %24)
      %1226 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.81 : Tensor = aten::conv2d(%result.81, %1222, %1223, %1224, %1225, %1226, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.81)
  %1228 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%692)
  %1229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%692)
  %1230 : int = aten::dim(%bottleneck_output.80) # torch/nn/modules/batchnorm.py:276:11
  %1231 : bool = aten::ne(%1230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1229)
   = prim::If(%1232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1229)
      %1234 : Tensor = aten::add(%1233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1229, %1234)
      -> ()
    block1():
      -> ()
  %1235 : bool = prim::GetAttr[name="training"](%1229)
  %1236 : Tensor = prim::GetAttr[name="running_mean"](%1229)
  %1237 : Tensor = prim::GetAttr[name="running_var"](%1229)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1239 : Tensor = prim::GetAttr[name="bias"](%1229)
   = prim::If(%1235) # torch/nn/functional.py:2011:4
    block0():
      %1240 : int[] = aten::size(%bottleneck_output.80) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%1240, %24) # torch/nn/functional.py:1991:17
      %1242 : int = aten::len(%1240) # torch/nn/functional.py:1992:19
      %1243 : int = aten::sub(%1242, %26) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%1243, %25, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %1247 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
          %1248 : int = aten::__getitem__(%1240, %1247) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %1248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.203)
      %1250 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1251 : Tensor = aten::batch_norm(%bottleneck_output.80, %1238, %1239, %1236, %1237, %1235, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.82 : Tensor = aten::relu_(%1251) # torch/nn/functional.py:1117:17
  %1253 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1254 : Tensor? = prim::GetAttr[name="bias"](%1228)
  %1255 : int[] = prim::ListConstruct(%27, %27)
  %1256 : int[] = prim::ListConstruct(%27, %27)
  %1257 : int[] = prim::ListConstruct(%27, %27)
  %new_features.84 : Tensor = aten::conv2d(%result.82, %1253, %1254, %1255, %1256, %1257, %27) # torch/nn/modules/conv.py:415:15
  %1259 : float = prim::GetAttr[name="drop_rate"](%692)
  %1260 : bool = aten::gt(%1259, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.81 : Tensor = prim::If(%1260) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1262 : float = prim::GetAttr[name="drop_rate"](%692)
      %1263 : bool = prim::GetAttr[name="training"](%692)
      %1264 : bool = aten::lt(%1262, %16) # torch/nn/functional.py:968:7
      %1265 : bool = prim::If(%1264) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1266 : bool = aten::gt(%1262, %17) # torch/nn/functional.py:968:17
          -> (%1266)
       = prim::If(%1265) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1267 : Tensor = aten::dropout(%new_features.84, %1262, %1263) # torch/nn/functional.py:973:17
      -> (%1267)
    block1():
      -> (%new_features.84)
  %1268 : Tensor[] = aten::append(%features.3, %new_features.81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1269 : Tensor = prim::Uninitialized()
  %1270 : bool = prim::GetAttr[name="memory_efficient"](%693)
  %1271 : bool = prim::If(%1270) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1272 : bool = prim::Uninitialized()
      %1273 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1274 : bool = aten::gt(%1273, %24)
      %1275 : bool, %1276 : bool, %1277 : int = prim::Loop(%18, %1274, %19, %1272, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1278 : int, %1279 : bool, %1280 : bool, %1281 : int):
          %tensor.25 : Tensor = aten::__getitem__(%features.3, %1281) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1283 : bool = prim::requires_grad(%tensor.25)
          %1284 : bool, %1285 : bool = prim::If(%1283) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1272)
          %1286 : int = aten::add(%1281, %27)
          %1287 : bool = aten::lt(%1286, %1273)
          %1288 : bool = aten::__and__(%1287, %1284)
          -> (%1288, %1283, %1285, %1286)
      %1289 : bool = prim::If(%1275)
        block0():
          -> (%1276)
        block1():
          -> (%19)
      -> (%1289)
    block1():
      -> (%19)
  %bottleneck_output.48 : Tensor = prim::If(%1271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1269)
    block1():
      %concated_features.25 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1292 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%693)
      %1293 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%693)
      %1294 : int = aten::dim(%concated_features.25) # torch/nn/modules/batchnorm.py:276:11
      %1295 : bool = aten::ne(%1294, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1295) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1296 : bool = prim::GetAttr[name="training"](%1293)
       = prim::If(%1296) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1297 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1293)
          %1298 : Tensor = aten::add(%1297, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1293, %1298)
          -> ()
        block1():
          -> ()
      %1299 : bool = prim::GetAttr[name="training"](%1293)
      %1300 : Tensor = prim::GetAttr[name="running_mean"](%1293)
      %1301 : Tensor = prim::GetAttr[name="running_var"](%1293)
      %1302 : Tensor = prim::GetAttr[name="weight"](%1293)
      %1303 : Tensor = prim::GetAttr[name="bias"](%1293)
       = prim::If(%1299) # torch/nn/functional.py:2011:4
        block0():
          %1304 : int[] = aten::size(%concated_features.25) # torch/nn/functional.py:2012:27
          %size_prods.204 : int = aten::__getitem__(%1304, %24) # torch/nn/functional.py:1991:17
          %1306 : int = aten::len(%1304) # torch/nn/functional.py:1992:19
          %1307 : int = aten::sub(%1306, %26) # torch/nn/functional.py:1992:19
          %size_prods.205 : int = prim::Loop(%1307, %25, %size_prods.204) # torch/nn/functional.py:1992:4
            block0(%i.52 : int, %size_prods.206 : int):
              %1311 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
              %1312 : int = aten::__getitem__(%1304, %1311) # torch/nn/functional.py:1993:22
              %size_prods.207 : int = aten::mul(%size_prods.206, %1312) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.207)
          %1314 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1314) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1315 : Tensor = aten::batch_norm(%concated_features.25, %1302, %1303, %1300, %1301, %1299, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.49 : Tensor = aten::relu_(%1315) # torch/nn/functional.py:1117:17
      %1317 : Tensor = prim::GetAttr[name="weight"](%1292)
      %1318 : Tensor? = prim::GetAttr[name="bias"](%1292)
      %1319 : int[] = prim::ListConstruct(%27, %27)
      %1320 : int[] = prim::ListConstruct(%24, %24)
      %1321 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.49 : Tensor = aten::conv2d(%result.49, %1317, %1318, %1319, %1320, %1321, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.49)
  %1323 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%693)
  %1324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%693)
  %1325 : int = aten::dim(%bottleneck_output.48) # torch/nn/modules/batchnorm.py:276:11
  %1326 : bool = aten::ne(%1325, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1326) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1327 : bool = prim::GetAttr[name="training"](%1324)
   = prim::If(%1327) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1328 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1324)
      %1329 : Tensor = aten::add(%1328, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1324, %1329)
      -> ()
    block1():
      -> ()
  %1330 : bool = prim::GetAttr[name="training"](%1324)
  %1331 : Tensor = prim::GetAttr[name="running_mean"](%1324)
  %1332 : Tensor = prim::GetAttr[name="running_var"](%1324)
  %1333 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1334 : Tensor = prim::GetAttr[name="bias"](%1324)
   = prim::If(%1330) # torch/nn/functional.py:2011:4
    block0():
      %1335 : int[] = aten::size(%bottleneck_output.48) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%1335, %24) # torch/nn/functional.py:1991:17
      %1337 : int = aten::len(%1335) # torch/nn/functional.py:1992:19
      %1338 : int = aten::sub(%1337, %26) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%1338, %25, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %1342 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
          %1343 : int = aten::__getitem__(%1335, %1342) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %1343) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.211)
      %1345 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1345) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1346 : Tensor = aten::batch_norm(%bottleneck_output.48, %1333, %1334, %1331, %1332, %1330, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.50 : Tensor = aten::relu_(%1346) # torch/nn/functional.py:1117:17
  %1348 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1349 : Tensor? = prim::GetAttr[name="bias"](%1323)
  %1350 : int[] = prim::ListConstruct(%27, %27)
  %1351 : int[] = prim::ListConstruct(%27, %27)
  %1352 : int[] = prim::ListConstruct(%27, %27)
  %new_features.49 : Tensor = aten::conv2d(%result.50, %1348, %1349, %1350, %1351, %1352, %27) # torch/nn/modules/conv.py:415:15
  %1354 : float = prim::GetAttr[name="drop_rate"](%693)
  %1355 : bool = aten::gt(%1354, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.50 : Tensor = prim::If(%1355) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1357 : float = prim::GetAttr[name="drop_rate"](%693)
      %1358 : bool = prim::GetAttr[name="training"](%693)
      %1359 : bool = aten::lt(%1357, %16) # torch/nn/functional.py:968:7
      %1360 : bool = prim::If(%1359) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1361 : bool = aten::gt(%1357, %17) # torch/nn/functional.py:968:17
          -> (%1361)
       = prim::If(%1360) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1362 : Tensor = aten::dropout(%new_features.49, %1357, %1358) # torch/nn/functional.py:973:17
      -> (%1362)
    block1():
      -> (%new_features.49)
  %1363 : Tensor[] = aten::append(%features.3, %new_features.50) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1364 : Tensor = prim::Uninitialized()
  %1365 : bool = prim::GetAttr[name="memory_efficient"](%694)
  %1366 : bool = prim::If(%1365) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1367 : bool = prim::Uninitialized()
      %1368 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1369 : bool = aten::gt(%1368, %24)
      %1370 : bool, %1371 : bool, %1372 : int = prim::Loop(%18, %1369, %19, %1367, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1373 : int, %1374 : bool, %1375 : bool, %1376 : int):
          %tensor.26 : Tensor = aten::__getitem__(%features.3, %1376) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1378 : bool = prim::requires_grad(%tensor.26)
          %1379 : bool, %1380 : bool = prim::If(%1378) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1367)
          %1381 : int = aten::add(%1376, %27)
          %1382 : bool = aten::lt(%1381, %1368)
          %1383 : bool = aten::__and__(%1382, %1379)
          -> (%1383, %1378, %1380, %1381)
      %1384 : bool = prim::If(%1370)
        block0():
          -> (%1371)
        block1():
          -> (%19)
      -> (%1384)
    block1():
      -> (%19)
  %bottleneck_output.50 : Tensor = prim::If(%1366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1364)
    block1():
      %concated_features.26 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1387 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%694)
      %1388 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%694)
      %1389 : int = aten::dim(%concated_features.26) # torch/nn/modules/batchnorm.py:276:11
      %1390 : bool = aten::ne(%1389, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1390) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1391 : bool = prim::GetAttr[name="training"](%1388)
       = prim::If(%1391) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1388)
          %1393 : Tensor = aten::add(%1392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1388, %1393)
          -> ()
        block1():
          -> ()
      %1394 : bool = prim::GetAttr[name="training"](%1388)
      %1395 : Tensor = prim::GetAttr[name="running_mean"](%1388)
      %1396 : Tensor = prim::GetAttr[name="running_var"](%1388)
      %1397 : Tensor = prim::GetAttr[name="weight"](%1388)
      %1398 : Tensor = prim::GetAttr[name="bias"](%1388)
       = prim::If(%1394) # torch/nn/functional.py:2011:4
        block0():
          %1399 : int[] = aten::size(%concated_features.26) # torch/nn/functional.py:2012:27
          %size_prods.212 : int = aten::__getitem__(%1399, %24) # torch/nn/functional.py:1991:17
          %1401 : int = aten::len(%1399) # torch/nn/functional.py:1992:19
          %1402 : int = aten::sub(%1401, %26) # torch/nn/functional.py:1992:19
          %size_prods.213 : int = prim::Loop(%1402, %25, %size_prods.212) # torch/nn/functional.py:1992:4
            block0(%i.54 : int, %size_prods.214 : int):
              %1406 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
              %1407 : int = aten::__getitem__(%1399, %1406) # torch/nn/functional.py:1993:22
              %size_prods.215 : int = aten::mul(%size_prods.214, %1407) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.215)
          %1409 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1409) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1410 : Tensor = aten::batch_norm(%concated_features.26, %1397, %1398, %1395, %1396, %1394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.51 : Tensor = aten::relu_(%1410) # torch/nn/functional.py:1117:17
      %1412 : Tensor = prim::GetAttr[name="weight"](%1387)
      %1413 : Tensor? = prim::GetAttr[name="bias"](%1387)
      %1414 : int[] = prim::ListConstruct(%27, %27)
      %1415 : int[] = prim::ListConstruct(%24, %24)
      %1416 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.51 : Tensor = aten::conv2d(%result.51, %1412, %1413, %1414, %1415, %1416, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.51)
  %1418 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%694)
  %1419 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%694)
  %1420 : int = aten::dim(%bottleneck_output.50) # torch/nn/modules/batchnorm.py:276:11
  %1421 : bool = aten::ne(%1420, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1421) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1422 : bool = prim::GetAttr[name="training"](%1419)
   = prim::If(%1422) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1423 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1419)
      %1424 : Tensor = aten::add(%1423, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1419, %1424)
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1419)
  %1426 : Tensor = prim::GetAttr[name="running_mean"](%1419)
  %1427 : Tensor = prim::GetAttr[name="running_var"](%1419)
  %1428 : Tensor = prim::GetAttr[name="weight"](%1419)
  %1429 : Tensor = prim::GetAttr[name="bias"](%1419)
   = prim::If(%1425) # torch/nn/functional.py:2011:4
    block0():
      %1430 : int[] = aten::size(%bottleneck_output.50) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%1430, %24) # torch/nn/functional.py:1991:17
      %1432 : int = aten::len(%1430) # torch/nn/functional.py:1992:19
      %1433 : int = aten::sub(%1432, %26) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%1433, %25, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %1437 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
          %1438 : int = aten::__getitem__(%1430, %1437) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %1438) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.219)
      %1440 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1440) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1441 : Tensor = aten::batch_norm(%bottleneck_output.50, %1428, %1429, %1426, %1427, %1425, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.52 : Tensor = aten::relu_(%1441) # torch/nn/functional.py:1117:17
  %1443 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1444 : Tensor? = prim::GetAttr[name="bias"](%1418)
  %1445 : int[] = prim::ListConstruct(%27, %27)
  %1446 : int[] = prim::ListConstruct(%27, %27)
  %1447 : int[] = prim::ListConstruct(%27, %27)
  %new_features.51 : Tensor = aten::conv2d(%result.52, %1443, %1444, %1445, %1446, %1447, %27) # torch/nn/modules/conv.py:415:15
  %1449 : float = prim::GetAttr[name="drop_rate"](%694)
  %1450 : bool = aten::gt(%1449, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.52 : Tensor = prim::If(%1450) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1452 : float = prim::GetAttr[name="drop_rate"](%694)
      %1453 : bool = prim::GetAttr[name="training"](%694)
      %1454 : bool = aten::lt(%1452, %16) # torch/nn/functional.py:968:7
      %1455 : bool = prim::If(%1454) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1456 : bool = aten::gt(%1452, %17) # torch/nn/functional.py:968:17
          -> (%1456)
       = prim::If(%1455) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1457 : Tensor = aten::dropout(%new_features.51, %1452, %1453) # torch/nn/functional.py:973:17
      -> (%1457)
    block1():
      -> (%new_features.51)
  %1458 : Tensor[] = aten::append(%features.3, %new_features.52) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1459 : Tensor = prim::Uninitialized()
  %1460 : bool = prim::GetAttr[name="memory_efficient"](%695)
  %1461 : bool = prim::If(%1460) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1462 : bool = prim::Uninitialized()
      %1463 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1464 : bool = aten::gt(%1463, %24)
      %1465 : bool, %1466 : bool, %1467 : int = prim::Loop(%18, %1464, %19, %1462, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1468 : int, %1469 : bool, %1470 : bool, %1471 : int):
          %tensor.27 : Tensor = aten::__getitem__(%features.3, %1471) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1473 : bool = prim::requires_grad(%tensor.27)
          %1474 : bool, %1475 : bool = prim::If(%1473) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1462)
          %1476 : int = aten::add(%1471, %27)
          %1477 : bool = aten::lt(%1476, %1463)
          %1478 : bool = aten::__and__(%1477, %1474)
          -> (%1478, %1473, %1475, %1476)
      %1479 : bool = prim::If(%1465)
        block0():
          -> (%1466)
        block1():
          -> (%19)
      -> (%1479)
    block1():
      -> (%19)
  %bottleneck_output.52 : Tensor = prim::If(%1461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1459)
    block1():
      %concated_features.27 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1482 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%695)
      %1483 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%695)
      %1484 : int = aten::dim(%concated_features.27) # torch/nn/modules/batchnorm.py:276:11
      %1485 : bool = aten::ne(%1484, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1485) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1486 : bool = prim::GetAttr[name="training"](%1483)
       = prim::If(%1486) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1483)
          %1488 : Tensor = aten::add(%1487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1483, %1488)
          -> ()
        block1():
          -> ()
      %1489 : bool = prim::GetAttr[name="training"](%1483)
      %1490 : Tensor = prim::GetAttr[name="running_mean"](%1483)
      %1491 : Tensor = prim::GetAttr[name="running_var"](%1483)
      %1492 : Tensor = prim::GetAttr[name="weight"](%1483)
      %1493 : Tensor = prim::GetAttr[name="bias"](%1483)
       = prim::If(%1489) # torch/nn/functional.py:2011:4
        block0():
          %1494 : int[] = aten::size(%concated_features.27) # torch/nn/functional.py:2012:27
          %size_prods.220 : int = aten::__getitem__(%1494, %24) # torch/nn/functional.py:1991:17
          %1496 : int = aten::len(%1494) # torch/nn/functional.py:1992:19
          %1497 : int = aten::sub(%1496, %26) # torch/nn/functional.py:1992:19
          %size_prods.221 : int = prim::Loop(%1497, %25, %size_prods.220) # torch/nn/functional.py:1992:4
            block0(%i.56 : int, %size_prods.222 : int):
              %1501 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
              %1502 : int = aten::__getitem__(%1494, %1501) # torch/nn/functional.py:1993:22
              %size_prods.223 : int = aten::mul(%size_prods.222, %1502) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.223)
          %1504 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1504) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1505 : Tensor = aten::batch_norm(%concated_features.27, %1492, %1493, %1490, %1491, %1489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.53 : Tensor = aten::relu_(%1505) # torch/nn/functional.py:1117:17
      %1507 : Tensor = prim::GetAttr[name="weight"](%1482)
      %1508 : Tensor? = prim::GetAttr[name="bias"](%1482)
      %1509 : int[] = prim::ListConstruct(%27, %27)
      %1510 : int[] = prim::ListConstruct(%24, %24)
      %1511 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.53 : Tensor = aten::conv2d(%result.53, %1507, %1508, %1509, %1510, %1511, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.53)
  %1513 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%695)
  %1514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%695)
  %1515 : int = aten::dim(%bottleneck_output.52) # torch/nn/modules/batchnorm.py:276:11
  %1516 : bool = aten::ne(%1515, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1516) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1517 : bool = prim::GetAttr[name="training"](%1514)
   = prim::If(%1517) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1518 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1514)
      %1519 : Tensor = aten::add(%1518, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1514, %1519)
      -> ()
    block1():
      -> ()
  %1520 : bool = prim::GetAttr[name="training"](%1514)
  %1521 : Tensor = prim::GetAttr[name="running_mean"](%1514)
  %1522 : Tensor = prim::GetAttr[name="running_var"](%1514)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1524 : Tensor = prim::GetAttr[name="bias"](%1514)
   = prim::If(%1520) # torch/nn/functional.py:2011:4
    block0():
      %1525 : int[] = aten::size(%bottleneck_output.52) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%1525, %24) # torch/nn/functional.py:1991:17
      %1527 : int = aten::len(%1525) # torch/nn/functional.py:1992:19
      %1528 : int = aten::sub(%1527, %26) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%1528, %25, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %1532 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
          %1533 : int = aten::__getitem__(%1525, %1532) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %1533) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.227)
      %1535 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1535) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1536 : Tensor = aten::batch_norm(%bottleneck_output.52, %1523, %1524, %1521, %1522, %1520, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.54 : Tensor = aten::relu_(%1536) # torch/nn/functional.py:1117:17
  %1538 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1539 : Tensor? = prim::GetAttr[name="bias"](%1513)
  %1540 : int[] = prim::ListConstruct(%27, %27)
  %1541 : int[] = prim::ListConstruct(%27, %27)
  %1542 : int[] = prim::ListConstruct(%27, %27)
  %new_features.53 : Tensor = aten::conv2d(%result.54, %1538, %1539, %1540, %1541, %1542, %27) # torch/nn/modules/conv.py:415:15
  %1544 : float = prim::GetAttr[name="drop_rate"](%695)
  %1545 : bool = aten::gt(%1544, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.54 : Tensor = prim::If(%1545) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1547 : float = prim::GetAttr[name="drop_rate"](%695)
      %1548 : bool = prim::GetAttr[name="training"](%695)
      %1549 : bool = aten::lt(%1547, %16) # torch/nn/functional.py:968:7
      %1550 : bool = prim::If(%1549) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1551 : bool = aten::gt(%1547, %17) # torch/nn/functional.py:968:17
          -> (%1551)
       = prim::If(%1550) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1552 : Tensor = aten::dropout(%new_features.53, %1547, %1548) # torch/nn/functional.py:973:17
      -> (%1552)
    block1():
      -> (%new_features.53)
  %1553 : Tensor[] = aten::append(%features.3, %new_features.54) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1554 : Tensor = prim::Uninitialized()
  %1555 : bool = prim::GetAttr[name="memory_efficient"](%696)
  %1556 : bool = prim::If(%1555) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1557 : bool = prim::Uninitialized()
      %1558 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1559 : bool = aten::gt(%1558, %24)
      %1560 : bool, %1561 : bool, %1562 : int = prim::Loop(%18, %1559, %19, %1557, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1563 : int, %1564 : bool, %1565 : bool, %1566 : int):
          %tensor.28 : Tensor = aten::__getitem__(%features.3, %1566) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1568 : bool = prim::requires_grad(%tensor.28)
          %1569 : bool, %1570 : bool = prim::If(%1568) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1557)
          %1571 : int = aten::add(%1566, %27)
          %1572 : bool = aten::lt(%1571, %1558)
          %1573 : bool = aten::__and__(%1572, %1569)
          -> (%1573, %1568, %1570, %1571)
      %1574 : bool = prim::If(%1560)
        block0():
          -> (%1561)
        block1():
          -> (%19)
      -> (%1574)
    block1():
      -> (%19)
  %bottleneck_output.54 : Tensor = prim::If(%1556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1554)
    block1():
      %concated_features.28 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1577 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%696)
      %1578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%696)
      %1579 : int = aten::dim(%concated_features.28) # torch/nn/modules/batchnorm.py:276:11
      %1580 : bool = aten::ne(%1579, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1580) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1581 : bool = prim::GetAttr[name="training"](%1578)
       = prim::If(%1581) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1578)
          %1583 : Tensor = aten::add(%1582, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1578, %1583)
          -> ()
        block1():
          -> ()
      %1584 : bool = prim::GetAttr[name="training"](%1578)
      %1585 : Tensor = prim::GetAttr[name="running_mean"](%1578)
      %1586 : Tensor = prim::GetAttr[name="running_var"](%1578)
      %1587 : Tensor = prim::GetAttr[name="weight"](%1578)
      %1588 : Tensor = prim::GetAttr[name="bias"](%1578)
       = prim::If(%1584) # torch/nn/functional.py:2011:4
        block0():
          %1589 : int[] = aten::size(%concated_features.28) # torch/nn/functional.py:2012:27
          %size_prods.228 : int = aten::__getitem__(%1589, %24) # torch/nn/functional.py:1991:17
          %1591 : int = aten::len(%1589) # torch/nn/functional.py:1992:19
          %1592 : int = aten::sub(%1591, %26) # torch/nn/functional.py:1992:19
          %size_prods.229 : int = prim::Loop(%1592, %25, %size_prods.228) # torch/nn/functional.py:1992:4
            block0(%i.58 : int, %size_prods.230 : int):
              %1596 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
              %1597 : int = aten::__getitem__(%1589, %1596) # torch/nn/functional.py:1993:22
              %size_prods.231 : int = aten::mul(%size_prods.230, %1597) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.231)
          %1599 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1599) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1600 : Tensor = aten::batch_norm(%concated_features.28, %1587, %1588, %1585, %1586, %1584, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.55 : Tensor = aten::relu_(%1600) # torch/nn/functional.py:1117:17
      %1602 : Tensor = prim::GetAttr[name="weight"](%1577)
      %1603 : Tensor? = prim::GetAttr[name="bias"](%1577)
      %1604 : int[] = prim::ListConstruct(%27, %27)
      %1605 : int[] = prim::ListConstruct(%24, %24)
      %1606 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.55 : Tensor = aten::conv2d(%result.55, %1602, %1603, %1604, %1605, %1606, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.55)
  %1608 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%696)
  %1609 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%696)
  %1610 : int = aten::dim(%bottleneck_output.54) # torch/nn/modules/batchnorm.py:276:11
  %1611 : bool = aten::ne(%1610, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1611) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1612 : bool = prim::GetAttr[name="training"](%1609)
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1613 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1609)
      %1614 : Tensor = aten::add(%1613, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1609, %1614)
      -> ()
    block1():
      -> ()
  %1615 : bool = prim::GetAttr[name="training"](%1609)
  %1616 : Tensor = prim::GetAttr[name="running_mean"](%1609)
  %1617 : Tensor = prim::GetAttr[name="running_var"](%1609)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1609)
  %1619 : Tensor = prim::GetAttr[name="bias"](%1609)
   = prim::If(%1615) # torch/nn/functional.py:2011:4
    block0():
      %1620 : int[] = aten::size(%bottleneck_output.54) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%1620, %24) # torch/nn/functional.py:1991:17
      %1622 : int = aten::len(%1620) # torch/nn/functional.py:1992:19
      %1623 : int = aten::sub(%1622, %26) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%1623, %25, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %1627 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
          %1628 : int = aten::__getitem__(%1620, %1627) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %1628) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.235)
      %1630 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1630) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1631 : Tensor = aten::batch_norm(%bottleneck_output.54, %1618, %1619, %1616, %1617, %1615, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.56 : Tensor = aten::relu_(%1631) # torch/nn/functional.py:1117:17
  %1633 : Tensor = prim::GetAttr[name="weight"](%1608)
  %1634 : Tensor? = prim::GetAttr[name="bias"](%1608)
  %1635 : int[] = prim::ListConstruct(%27, %27)
  %1636 : int[] = prim::ListConstruct(%27, %27)
  %1637 : int[] = prim::ListConstruct(%27, %27)
  %new_features.55 : Tensor = aten::conv2d(%result.56, %1633, %1634, %1635, %1636, %1637, %27) # torch/nn/modules/conv.py:415:15
  %1639 : float = prim::GetAttr[name="drop_rate"](%696)
  %1640 : bool = aten::gt(%1639, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.56 : Tensor = prim::If(%1640) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1642 : float = prim::GetAttr[name="drop_rate"](%696)
      %1643 : bool = prim::GetAttr[name="training"](%696)
      %1644 : bool = aten::lt(%1642, %16) # torch/nn/functional.py:968:7
      %1645 : bool = prim::If(%1644) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1646 : bool = aten::gt(%1642, %17) # torch/nn/functional.py:968:17
          -> (%1646)
       = prim::If(%1645) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1647 : Tensor = aten::dropout(%new_features.55, %1642, %1643) # torch/nn/functional.py:973:17
      -> (%1647)
    block1():
      -> (%new_features.55)
  %1648 : Tensor[] = aten::append(%features.3, %new_features.56) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1649 : Tensor = prim::Uninitialized()
  %1650 : bool = prim::GetAttr[name="memory_efficient"](%697)
  %1651 : bool = prim::If(%1650) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1652 : bool = prim::Uninitialized()
      %1653 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1654 : bool = aten::gt(%1653, %24)
      %1655 : bool, %1656 : bool, %1657 : int = prim::Loop(%18, %1654, %19, %1652, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1658 : int, %1659 : bool, %1660 : bool, %1661 : int):
          %tensor.29 : Tensor = aten::__getitem__(%features.3, %1661) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1663 : bool = prim::requires_grad(%tensor.29)
          %1664 : bool, %1665 : bool = prim::If(%1663) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1652)
          %1666 : int = aten::add(%1661, %27)
          %1667 : bool = aten::lt(%1666, %1653)
          %1668 : bool = aten::__and__(%1667, %1664)
          -> (%1668, %1663, %1665, %1666)
      %1669 : bool = prim::If(%1655)
        block0():
          -> (%1656)
        block1():
          -> (%19)
      -> (%1669)
    block1():
      -> (%19)
  %bottleneck_output.56 : Tensor = prim::If(%1651) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1649)
    block1():
      %concated_features.29 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1672 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%697)
      %1673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%697)
      %1674 : int = aten::dim(%concated_features.29) # torch/nn/modules/batchnorm.py:276:11
      %1675 : bool = aten::ne(%1674, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1675) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1676 : bool = prim::GetAttr[name="training"](%1673)
       = prim::If(%1676) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1673)
          %1678 : Tensor = aten::add(%1677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1673, %1678)
          -> ()
        block1():
          -> ()
      %1679 : bool = prim::GetAttr[name="training"](%1673)
      %1680 : Tensor = prim::GetAttr[name="running_mean"](%1673)
      %1681 : Tensor = prim::GetAttr[name="running_var"](%1673)
      %1682 : Tensor = prim::GetAttr[name="weight"](%1673)
      %1683 : Tensor = prim::GetAttr[name="bias"](%1673)
       = prim::If(%1679) # torch/nn/functional.py:2011:4
        block0():
          %1684 : int[] = aten::size(%concated_features.29) # torch/nn/functional.py:2012:27
          %size_prods.236 : int = aten::__getitem__(%1684, %24) # torch/nn/functional.py:1991:17
          %1686 : int = aten::len(%1684) # torch/nn/functional.py:1992:19
          %1687 : int = aten::sub(%1686, %26) # torch/nn/functional.py:1992:19
          %size_prods.237 : int = prim::Loop(%1687, %25, %size_prods.236) # torch/nn/functional.py:1992:4
            block0(%i.60 : int, %size_prods.238 : int):
              %1691 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
              %1692 : int = aten::__getitem__(%1684, %1691) # torch/nn/functional.py:1993:22
              %size_prods.239 : int = aten::mul(%size_prods.238, %1692) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.239)
          %1694 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1694) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1695 : Tensor = aten::batch_norm(%concated_features.29, %1682, %1683, %1680, %1681, %1679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.57 : Tensor = aten::relu_(%1695) # torch/nn/functional.py:1117:17
      %1697 : Tensor = prim::GetAttr[name="weight"](%1672)
      %1698 : Tensor? = prim::GetAttr[name="bias"](%1672)
      %1699 : int[] = prim::ListConstruct(%27, %27)
      %1700 : int[] = prim::ListConstruct(%24, %24)
      %1701 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.57 : Tensor = aten::conv2d(%result.57, %1697, %1698, %1699, %1700, %1701, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.57)
  %1703 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%697)
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%697)
  %1705 : int = aten::dim(%bottleneck_output.56) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%bottleneck_output.56) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%1715, %24) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %26) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%1718, %25, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %1722 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %1723) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.243)
      %1725 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1726 : Tensor = aten::batch_norm(%bottleneck_output.56, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.58 : Tensor = aten::relu_(%1726) # torch/nn/functional.py:1117:17
  %1728 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1729 : Tensor? = prim::GetAttr[name="bias"](%1703)
  %1730 : int[] = prim::ListConstruct(%27, %27)
  %1731 : int[] = prim::ListConstruct(%27, %27)
  %1732 : int[] = prim::ListConstruct(%27, %27)
  %new_features.57 : Tensor = aten::conv2d(%result.58, %1728, %1729, %1730, %1731, %1732, %27) # torch/nn/modules/conv.py:415:15
  %1734 : float = prim::GetAttr[name="drop_rate"](%697)
  %1735 : bool = aten::gt(%1734, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.58 : Tensor = prim::If(%1735) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1737 : float = prim::GetAttr[name="drop_rate"](%697)
      %1738 : bool = prim::GetAttr[name="training"](%697)
      %1739 : bool = aten::lt(%1737, %16) # torch/nn/functional.py:968:7
      %1740 : bool = prim::If(%1739) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1741 : bool = aten::gt(%1737, %17) # torch/nn/functional.py:968:17
          -> (%1741)
       = prim::If(%1740) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1742 : Tensor = aten::dropout(%new_features.57, %1737, %1738) # torch/nn/functional.py:973:17
      -> (%1742)
    block1():
      -> (%new_features.57)
  %1743 : Tensor[] = aten::append(%features.3, %new_features.58) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1744 : Tensor = prim::Uninitialized()
  %1745 : bool = prim::GetAttr[name="memory_efficient"](%698)
  %1746 : bool = prim::If(%1745) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1747 : bool = prim::Uninitialized()
      %1748 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1749 : bool = aten::gt(%1748, %24)
      %1750 : bool, %1751 : bool, %1752 : int = prim::Loop(%18, %1749, %19, %1747, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1753 : int, %1754 : bool, %1755 : bool, %1756 : int):
          %tensor.42 : Tensor = aten::__getitem__(%features.3, %1756) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1758 : bool = prim::requires_grad(%tensor.42)
          %1759 : bool, %1760 : bool = prim::If(%1758) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1747)
          %1761 : int = aten::add(%1756, %27)
          %1762 : bool = aten::lt(%1761, %1748)
          %1763 : bool = aten::__and__(%1762, %1759)
          -> (%1763, %1758, %1760, %1761)
      %1764 : bool = prim::If(%1750)
        block0():
          -> (%1751)
        block1():
          -> (%19)
      -> (%1764)
    block1():
      -> (%19)
  %bottleneck_output.82 : Tensor = prim::If(%1746) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1744)
    block1():
      %concated_features.42 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1767 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%698)
      %1768 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%698)
      %1769 : int = aten::dim(%concated_features.42) # torch/nn/modules/batchnorm.py:276:11
      %1770 : bool = aten::ne(%1769, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1770) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1771 : bool = prim::GetAttr[name="training"](%1768)
       = prim::If(%1771) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1772 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1768)
          %1773 : Tensor = aten::add(%1772, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1768, %1773)
          -> ()
        block1():
          -> ()
      %1774 : bool = prim::GetAttr[name="training"](%1768)
      %1775 : Tensor = prim::GetAttr[name="running_mean"](%1768)
      %1776 : Tensor = prim::GetAttr[name="running_var"](%1768)
      %1777 : Tensor = prim::GetAttr[name="weight"](%1768)
      %1778 : Tensor = prim::GetAttr[name="bias"](%1768)
       = prim::If(%1774) # torch/nn/functional.py:2011:4
        block0():
          %1779 : int[] = aten::size(%concated_features.42) # torch/nn/functional.py:2012:27
          %size_prods.340 : int = aten::__getitem__(%1779, %24) # torch/nn/functional.py:1991:17
          %1781 : int = aten::len(%1779) # torch/nn/functional.py:1992:19
          %1782 : int = aten::sub(%1781, %26) # torch/nn/functional.py:1992:19
          %size_prods.341 : int = prim::Loop(%1782, %25, %size_prods.340) # torch/nn/functional.py:1992:4
            block0(%i.86 : int, %size_prods.342 : int):
              %1786 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
              %1787 : int = aten::__getitem__(%1779, %1786) # torch/nn/functional.py:1993:22
              %size_prods.343 : int = aten::mul(%size_prods.342, %1787) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.343)
          %1789 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1789) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1790 : Tensor = aten::batch_norm(%concated_features.42, %1777, %1778, %1775, %1776, %1774, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.83 : Tensor = aten::relu_(%1790) # torch/nn/functional.py:1117:17
      %1792 : Tensor = prim::GetAttr[name="weight"](%1767)
      %1793 : Tensor? = prim::GetAttr[name="bias"](%1767)
      %1794 : int[] = prim::ListConstruct(%27, %27)
      %1795 : int[] = prim::ListConstruct(%24, %24)
      %1796 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.83 : Tensor = aten::conv2d(%result.83, %1792, %1793, %1794, %1795, %1796, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.83)
  %1798 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%698)
  %1799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%698)
  %1800 : int = aten::dim(%bottleneck_output.82) # torch/nn/modules/batchnorm.py:276:11
  %1801 : bool = aten::ne(%1800, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1802 : bool = prim::GetAttr[name="training"](%1799)
   = prim::If(%1802) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1803 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1799)
      %1804 : Tensor = aten::add(%1803, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1799, %1804)
      -> ()
    block1():
      -> ()
  %1805 : bool = prim::GetAttr[name="training"](%1799)
  %1806 : Tensor = prim::GetAttr[name="running_mean"](%1799)
  %1807 : Tensor = prim::GetAttr[name="running_var"](%1799)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1799)
  %1809 : Tensor = prim::GetAttr[name="bias"](%1799)
   = prim::If(%1805) # torch/nn/functional.py:2011:4
    block0():
      %1810 : int[] = aten::size(%bottleneck_output.82) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%1810, %24) # torch/nn/functional.py:1991:17
      %1812 : int = aten::len(%1810) # torch/nn/functional.py:1992:19
      %1813 : int = aten::sub(%1812, %26) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%1813, %25, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %1817 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
          %1818 : int = aten::__getitem__(%1810, %1817) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %1818) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.303)
      %1820 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1820) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1821 : Tensor = aten::batch_norm(%bottleneck_output.82, %1808, %1809, %1806, %1807, %1805, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.84 : Tensor = aten::relu_(%1821) # torch/nn/functional.py:1117:17
  %1823 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1824 : Tensor? = prim::GetAttr[name="bias"](%1798)
  %1825 : int[] = prim::ListConstruct(%27, %27)
  %1826 : int[] = prim::ListConstruct(%27, %27)
  %1827 : int[] = prim::ListConstruct(%27, %27)
  %new_features.83 : Tensor = aten::conv2d(%result.84, %1823, %1824, %1825, %1826, %1827, %27) # torch/nn/modules/conv.py:415:15
  %1829 : float = prim::GetAttr[name="drop_rate"](%698)
  %1830 : bool = aten::gt(%1829, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.60 : Tensor = prim::If(%1830) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1832 : float = prim::GetAttr[name="drop_rate"](%698)
      %1833 : bool = prim::GetAttr[name="training"](%698)
      %1834 : bool = aten::lt(%1832, %16) # torch/nn/functional.py:968:7
      %1835 : bool = prim::If(%1834) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1836 : bool = aten::gt(%1832, %17) # torch/nn/functional.py:968:17
          -> (%1836)
       = prim::If(%1835) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1837 : Tensor = aten::dropout(%new_features.83, %1832, %1833) # torch/nn/functional.py:973:17
      -> (%1837)
    block1():
      -> (%new_features.83)
  %1838 : Tensor[] = aten::append(%features.3, %new_features.60) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.15 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %1840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm"](%34)
  %1841 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv"](%34)
  %1842 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %1843 : bool = aten::ne(%1842, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1844 : bool = prim::GetAttr[name="training"](%1840)
   = prim::If(%1844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1840)
      %1846 : Tensor = aten::add(%1845, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1840, %1846)
      -> ()
    block1():
      -> ()
  %1847 : bool = prim::GetAttr[name="training"](%1840)
  %1848 : Tensor = prim::GetAttr[name="running_mean"](%1840)
  %1849 : Tensor = prim::GetAttr[name="running_var"](%1840)
  %1850 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1851 : Tensor = prim::GetAttr[name="bias"](%1840)
   = prim::If(%1847) # torch/nn/functional.py:2011:4
    block0():
      %1852 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%1852, %24) # torch/nn/functional.py:1991:17
      %1854 : int = aten::len(%1852) # torch/nn/functional.py:1992:19
      %1855 : int = aten::sub(%1854, %26) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%1855, %25, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %1859 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
          %1860 : int = aten::__getitem__(%1852, %1859) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %1860) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.347)
      %1862 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.15, %1850, %1851, %1848, %1849, %1847, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.18 : Tensor = aten::relu_(%input.22) # torch/nn/functional.py:1117:17
  %1865 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1866 : Tensor? = prim::GetAttr[name="bias"](%1841)
  %1867 : int[] = prim::ListConstruct(%27, %27)
  %1868 : int[] = prim::ListConstruct(%24, %24)
  %1869 : int[] = prim::ListConstruct(%27, %27)
  %input.20 : Tensor = aten::conv2d(%input.18, %1865, %1866, %1867, %1868, %1869, %27) # torch/nn/modules/conv.py:415:15
  %1871 : int[] = prim::ListConstruct(%26, %26)
  %1872 : int[] = prim::ListConstruct(%26, %26)
  %1873 : int[] = prim::ListConstruct(%24, %24)
  %input.17 : Tensor = aten::avg_pool2d(%input.20, %1871, %1872, %1873, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.4 : Tensor[] = prim::ListConstruct(%input.17)
  %1876 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer1"](%35)
  %1877 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer2"](%35)
  %1878 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer3"](%35)
  %1879 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer4"](%35)
  %1880 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer5"](%35)
  %1881 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer6"](%35)
  %1882 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer7"](%35)
  %1883 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer8"](%35)
  %1884 : __torch__.torchvision.models.densenet.___torch_mangle_111._DenseLayer = prim::GetAttr[name="denselayer9"](%35)
  %1885 : __torch__.torchvision.models.densenet.___torch_mangle_114._DenseLayer = prim::GetAttr[name="denselayer10"](%35)
  %1886 : __torch__.torchvision.models.densenet.___torch_mangle_117._DenseLayer = prim::GetAttr[name="denselayer11"](%35)
  %1887 : __torch__.torchvision.models.densenet.___torch_mangle_120._DenseLayer = prim::GetAttr[name="denselayer12"](%35)
  %1888 : __torch__.torchvision.models.densenet.___torch_mangle_123._DenseLayer = prim::GetAttr[name="denselayer13"](%35)
  %1889 : __torch__.torchvision.models.densenet.___torch_mangle_126._DenseLayer = prim::GetAttr[name="denselayer14"](%35)
  %1890 : __torch__.torchvision.models.densenet.___torch_mangle_129._DenseLayer = prim::GetAttr[name="denselayer15"](%35)
  %1891 : __torch__.torchvision.models.densenet.___torch_mangle_132._DenseLayer = prim::GetAttr[name="denselayer16"](%35)
  %1892 : __torch__.torchvision.models.densenet.___torch_mangle_135._DenseLayer = prim::GetAttr[name="denselayer17"](%35)
  %1893 : __torch__.torchvision.models.densenet.___torch_mangle_138._DenseLayer = prim::GetAttr[name="denselayer18"](%35)
  %1894 : __torch__.torchvision.models.densenet.___torch_mangle_141._DenseLayer = prim::GetAttr[name="denselayer19"](%35)
  %1895 : __torch__.torchvision.models.densenet.___torch_mangle_144._DenseLayer = prim::GetAttr[name="denselayer20"](%35)
  %1896 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer21"](%35)
  %1897 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer22"](%35)
  %1898 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer23"](%35)
  %1899 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer24"](%35)
  %1900 : Tensor = prim::Uninitialized()
  %1901 : bool = prim::GetAttr[name="memory_efficient"](%1876)
  %1902 : bool = prim::If(%1901) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1903 : bool = prim::Uninitialized()
      %1904 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1905 : bool = aten::gt(%1904, %24)
      %1906 : bool, %1907 : bool, %1908 : int = prim::Loop(%18, %1905, %19, %1903, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1909 : int, %1910 : bool, %1911 : bool, %1912 : int):
          %tensor.43 : Tensor = aten::__getitem__(%features.4, %1912) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1914 : bool = prim::requires_grad(%tensor.43)
          %1915 : bool, %1916 : bool = prim::If(%1914) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1903)
          %1917 : int = aten::add(%1912, %27)
          %1918 : bool = aten::lt(%1917, %1904)
          %1919 : bool = aten::__and__(%1918, %1915)
          -> (%1919, %1914, %1916, %1917)
      %1920 : bool = prim::If(%1906)
        block0():
          -> (%1907)
        block1():
          -> (%19)
      -> (%1920)
    block1():
      -> (%19)
  %bottleneck_output.84 : Tensor = prim::If(%1902) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1900)
    block1():
      %concated_features.43 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1923 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%1876)
      %1924 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%1876)
      %1925 : int = aten::dim(%concated_features.43) # torch/nn/modules/batchnorm.py:276:11
      %1926 : bool = aten::ne(%1925, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1926) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1927 : bool = prim::GetAttr[name="training"](%1924)
       = prim::If(%1927) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1928 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1924)
          %1929 : Tensor = aten::add(%1928, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1924, %1929)
          -> ()
        block1():
          -> ()
      %1930 : bool = prim::GetAttr[name="training"](%1924)
      %1931 : Tensor = prim::GetAttr[name="running_mean"](%1924)
      %1932 : Tensor = prim::GetAttr[name="running_var"](%1924)
      %1933 : Tensor = prim::GetAttr[name="weight"](%1924)
      %1934 : Tensor = prim::GetAttr[name="bias"](%1924)
       = prim::If(%1930) # torch/nn/functional.py:2011:4
        block0():
          %1935 : int[] = aten::size(%concated_features.43) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%1935, %24) # torch/nn/functional.py:1991:17
          %1937 : int = aten::len(%1935) # torch/nn/functional.py:1992:19
          %1938 : int = aten::sub(%1937, %26) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%1938, %25, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %1942 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
              %1943 : int = aten::__getitem__(%1935, %1942) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %1943) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.355)
          %1945 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1945) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1946 : Tensor = aten::batch_norm(%concated_features.43, %1933, %1934, %1931, %1932, %1930, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.85 : Tensor = aten::relu_(%1946) # torch/nn/functional.py:1117:17
      %1948 : Tensor = prim::GetAttr[name="weight"](%1923)
      %1949 : Tensor? = prim::GetAttr[name="bias"](%1923)
      %1950 : int[] = prim::ListConstruct(%27, %27)
      %1951 : int[] = prim::ListConstruct(%24, %24)
      %1952 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.85 : Tensor = aten::conv2d(%result.85, %1948, %1949, %1950, %1951, %1952, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.85)
  %1954 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1876)
  %1955 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1876)
  %1956 : int = aten::dim(%bottleneck_output.84) # torch/nn/modules/batchnorm.py:276:11
  %1957 : bool = aten::ne(%1956, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1957) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1958 : bool = prim::GetAttr[name="training"](%1955)
   = prim::If(%1958) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1959 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1955)
      %1960 : Tensor = aten::add(%1959, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1955, %1960)
      -> ()
    block1():
      -> ()
  %1961 : bool = prim::GetAttr[name="training"](%1955)
  %1962 : Tensor = prim::GetAttr[name="running_mean"](%1955)
  %1963 : Tensor = prim::GetAttr[name="running_var"](%1955)
  %1964 : Tensor = prim::GetAttr[name="weight"](%1955)
  %1965 : Tensor = prim::GetAttr[name="bias"](%1955)
   = prim::If(%1961) # torch/nn/functional.py:2011:4
    block0():
      %1966 : int[] = aten::size(%bottleneck_output.84) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%1966, %24) # torch/nn/functional.py:1991:17
      %1968 : int = aten::len(%1966) # torch/nn/functional.py:1992:19
      %1969 : int = aten::sub(%1968, %26) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%1969, %25, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %1973 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
          %1974 : int = aten::__getitem__(%1966, %1973) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %1974) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.359)
      %1976 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1976) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1977 : Tensor = aten::batch_norm(%bottleneck_output.84, %1964, %1965, %1962, %1963, %1961, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.86 : Tensor = aten::relu_(%1977) # torch/nn/functional.py:1117:17
  %1979 : Tensor = prim::GetAttr[name="weight"](%1954)
  %1980 : Tensor? = prim::GetAttr[name="bias"](%1954)
  %1981 : int[] = prim::ListConstruct(%27, %27)
  %1982 : int[] = prim::ListConstruct(%27, %27)
  %1983 : int[] = prim::ListConstruct(%27, %27)
  %new_features.85 : Tensor = aten::conv2d(%result.86, %1979, %1980, %1981, %1982, %1983, %27) # torch/nn/modules/conv.py:415:15
  %1985 : float = prim::GetAttr[name="drop_rate"](%1876)
  %1986 : bool = aten::gt(%1985, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.86 : Tensor = prim::If(%1986) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1988 : float = prim::GetAttr[name="drop_rate"](%1876)
      %1989 : bool = prim::GetAttr[name="training"](%1876)
      %1990 : bool = aten::lt(%1988, %16) # torch/nn/functional.py:968:7
      %1991 : bool = prim::If(%1990) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1992 : bool = aten::gt(%1988, %17) # torch/nn/functional.py:968:17
          -> (%1992)
       = prim::If(%1991) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1993 : Tensor = aten::dropout(%new_features.85, %1988, %1989) # torch/nn/functional.py:973:17
      -> (%1993)
    block1():
      -> (%new_features.85)
  %1994 : Tensor[] = aten::append(%features.4, %new_features.86) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1995 : Tensor = prim::Uninitialized()
  %1996 : bool = prim::GetAttr[name="memory_efficient"](%1877)
  %1997 : bool = prim::If(%1996) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1998 : bool = prim::Uninitialized()
      %1999 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2000 : bool = aten::gt(%1999, %24)
      %2001 : bool, %2002 : bool, %2003 : int = prim::Loop(%18, %2000, %19, %1998, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2004 : int, %2005 : bool, %2006 : bool, %2007 : int):
          %tensor.44 : Tensor = aten::__getitem__(%features.4, %2007) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2009 : bool = prim::requires_grad(%tensor.44)
          %2010 : bool, %2011 : bool = prim::If(%2009) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1998)
          %2012 : int = aten::add(%2007, %27)
          %2013 : bool = aten::lt(%2012, %1999)
          %2014 : bool = aten::__and__(%2013, %2010)
          -> (%2014, %2009, %2011, %2012)
      %2015 : bool = prim::If(%2001)
        block0():
          -> (%2002)
        block1():
          -> (%19)
      -> (%2015)
    block1():
      -> (%19)
  %bottleneck_output.86 : Tensor = prim::If(%1997) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1995)
    block1():
      %concated_features.44 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2018 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%1877)
      %2019 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%1877)
      %2020 : int = aten::dim(%concated_features.44) # torch/nn/modules/batchnorm.py:276:11
      %2021 : bool = aten::ne(%2020, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2021) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2022 : bool = prim::GetAttr[name="training"](%2019)
       = prim::If(%2022) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2023 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2019)
          %2024 : Tensor = aten::add(%2023, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2019, %2024)
          -> ()
        block1():
          -> ()
      %2025 : bool = prim::GetAttr[name="training"](%2019)
      %2026 : Tensor = prim::GetAttr[name="running_mean"](%2019)
      %2027 : Tensor = prim::GetAttr[name="running_var"](%2019)
      %2028 : Tensor = prim::GetAttr[name="weight"](%2019)
      %2029 : Tensor = prim::GetAttr[name="bias"](%2019)
       = prim::If(%2025) # torch/nn/functional.py:2011:4
        block0():
          %2030 : int[] = aten::size(%concated_features.44) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%2030, %24) # torch/nn/functional.py:1991:17
          %2032 : int = aten::len(%2030) # torch/nn/functional.py:1992:19
          %2033 : int = aten::sub(%2032, %26) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%2033, %25, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %2037 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
              %2038 : int = aten::__getitem__(%2030, %2037) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %2038) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.363)
          %2040 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2040) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2041 : Tensor = aten::batch_norm(%concated_features.44, %2028, %2029, %2026, %2027, %2025, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.87 : Tensor = aten::relu_(%2041) # torch/nn/functional.py:1117:17
      %2043 : Tensor = prim::GetAttr[name="weight"](%2018)
      %2044 : Tensor? = prim::GetAttr[name="bias"](%2018)
      %2045 : int[] = prim::ListConstruct(%27, %27)
      %2046 : int[] = prim::ListConstruct(%24, %24)
      %2047 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.87 : Tensor = aten::conv2d(%result.87, %2043, %2044, %2045, %2046, %2047, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.87)
  %2049 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1877)
  %2050 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1877)
  %2051 : int = aten::dim(%bottleneck_output.86) # torch/nn/modules/batchnorm.py:276:11
  %2052 : bool = aten::ne(%2051, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2052) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2053 : bool = prim::GetAttr[name="training"](%2050)
   = prim::If(%2053) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2054 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2050)
      %2055 : Tensor = aten::add(%2054, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2050, %2055)
      -> ()
    block1():
      -> ()
  %2056 : bool = prim::GetAttr[name="training"](%2050)
  %2057 : Tensor = prim::GetAttr[name="running_mean"](%2050)
  %2058 : Tensor = prim::GetAttr[name="running_var"](%2050)
  %2059 : Tensor = prim::GetAttr[name="weight"](%2050)
  %2060 : Tensor = prim::GetAttr[name="bias"](%2050)
   = prim::If(%2056) # torch/nn/functional.py:2011:4
    block0():
      %2061 : int[] = aten::size(%bottleneck_output.86) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%2061, %24) # torch/nn/functional.py:1991:17
      %2063 : int = aten::len(%2061) # torch/nn/functional.py:1992:19
      %2064 : int = aten::sub(%2063, %26) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%2064, %25, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %2068 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
          %2069 : int = aten::__getitem__(%2061, %2068) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %2069) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.367)
      %2071 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2071) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2072 : Tensor = aten::batch_norm(%bottleneck_output.86, %2059, %2060, %2057, %2058, %2056, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.88 : Tensor = aten::relu_(%2072) # torch/nn/functional.py:1117:17
  %2074 : Tensor = prim::GetAttr[name="weight"](%2049)
  %2075 : Tensor? = prim::GetAttr[name="bias"](%2049)
  %2076 : int[] = prim::ListConstruct(%27, %27)
  %2077 : int[] = prim::ListConstruct(%27, %27)
  %2078 : int[] = prim::ListConstruct(%27, %27)
  %new_features.87 : Tensor = aten::conv2d(%result.88, %2074, %2075, %2076, %2077, %2078, %27) # torch/nn/modules/conv.py:415:15
  %2080 : float = prim::GetAttr[name="drop_rate"](%1877)
  %2081 : bool = aten::gt(%2080, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.88 : Tensor = prim::If(%2081) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2083 : float = prim::GetAttr[name="drop_rate"](%1877)
      %2084 : bool = prim::GetAttr[name="training"](%1877)
      %2085 : bool = aten::lt(%2083, %16) # torch/nn/functional.py:968:7
      %2086 : bool = prim::If(%2085) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2087 : bool = aten::gt(%2083, %17) # torch/nn/functional.py:968:17
          -> (%2087)
       = prim::If(%2086) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2088 : Tensor = aten::dropout(%new_features.87, %2083, %2084) # torch/nn/functional.py:973:17
      -> (%2088)
    block1():
      -> (%new_features.87)
  %2089 : Tensor[] = aten::append(%features.4, %new_features.88) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2090 : Tensor = prim::Uninitialized()
  %2091 : bool = prim::GetAttr[name="memory_efficient"](%1878)
  %2092 : bool = prim::If(%2091) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2093 : bool = prim::Uninitialized()
      %2094 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2095 : bool = aten::gt(%2094, %24)
      %2096 : bool, %2097 : bool, %2098 : int = prim::Loop(%18, %2095, %19, %2093, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2099 : int, %2100 : bool, %2101 : bool, %2102 : int):
          %tensor.45 : Tensor = aten::__getitem__(%features.4, %2102) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2104 : bool = prim::requires_grad(%tensor.45)
          %2105 : bool, %2106 : bool = prim::If(%2104) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2093)
          %2107 : int = aten::add(%2102, %27)
          %2108 : bool = aten::lt(%2107, %2094)
          %2109 : bool = aten::__and__(%2108, %2105)
          -> (%2109, %2104, %2106, %2107)
      %2110 : bool = prim::If(%2096)
        block0():
          -> (%2097)
        block1():
          -> (%19)
      -> (%2110)
    block1():
      -> (%19)
  %bottleneck_output.88 : Tensor = prim::If(%2092) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2090)
    block1():
      %concated_features.45 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2113 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%1878)
      %2114 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%1878)
      %2115 : int = aten::dim(%concated_features.45) # torch/nn/modules/batchnorm.py:276:11
      %2116 : bool = aten::ne(%2115, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2116) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2117 : bool = prim::GetAttr[name="training"](%2114)
       = prim::If(%2117) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2118 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2114)
          %2119 : Tensor = aten::add(%2118, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2114, %2119)
          -> ()
        block1():
          -> ()
      %2120 : bool = prim::GetAttr[name="training"](%2114)
      %2121 : Tensor = prim::GetAttr[name="running_mean"](%2114)
      %2122 : Tensor = prim::GetAttr[name="running_var"](%2114)
      %2123 : Tensor = prim::GetAttr[name="weight"](%2114)
      %2124 : Tensor = prim::GetAttr[name="bias"](%2114)
       = prim::If(%2120) # torch/nn/functional.py:2011:4
        block0():
          %2125 : int[] = aten::size(%concated_features.45) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%2125, %24) # torch/nn/functional.py:1991:17
          %2127 : int = aten::len(%2125) # torch/nn/functional.py:1992:19
          %2128 : int = aten::sub(%2127, %26) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%2128, %25, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %2132 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
              %2133 : int = aten::__getitem__(%2125, %2132) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %2133) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.371)
          %2135 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2135) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2136 : Tensor = aten::batch_norm(%concated_features.45, %2123, %2124, %2121, %2122, %2120, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.89 : Tensor = aten::relu_(%2136) # torch/nn/functional.py:1117:17
      %2138 : Tensor = prim::GetAttr[name="weight"](%2113)
      %2139 : Tensor? = prim::GetAttr[name="bias"](%2113)
      %2140 : int[] = prim::ListConstruct(%27, %27)
      %2141 : int[] = prim::ListConstruct(%24, %24)
      %2142 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.89 : Tensor = aten::conv2d(%result.89, %2138, %2139, %2140, %2141, %2142, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.89)
  %2144 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1878)
  %2145 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1878)
  %2146 : int = aten::dim(%bottleneck_output.88) # torch/nn/modules/batchnorm.py:276:11
  %2147 : bool = aten::ne(%2146, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2147) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2148 : bool = prim::GetAttr[name="training"](%2145)
   = prim::If(%2148) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2149 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2145)
      %2150 : Tensor = aten::add(%2149, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2145, %2150)
      -> ()
    block1():
      -> ()
  %2151 : bool = prim::GetAttr[name="training"](%2145)
  %2152 : Tensor = prim::GetAttr[name="running_mean"](%2145)
  %2153 : Tensor = prim::GetAttr[name="running_var"](%2145)
  %2154 : Tensor = prim::GetAttr[name="weight"](%2145)
  %2155 : Tensor = prim::GetAttr[name="bias"](%2145)
   = prim::If(%2151) # torch/nn/functional.py:2011:4
    block0():
      %2156 : int[] = aten::size(%bottleneck_output.88) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%2156, %24) # torch/nn/functional.py:1991:17
      %2158 : int = aten::len(%2156) # torch/nn/functional.py:1992:19
      %2159 : int = aten::sub(%2158, %26) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%2159, %25, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %2163 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
          %2164 : int = aten::__getitem__(%2156, %2163) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %2164) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.375)
      %2166 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2166) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2167 : Tensor = aten::batch_norm(%bottleneck_output.88, %2154, %2155, %2152, %2153, %2151, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.90 : Tensor = aten::relu_(%2167) # torch/nn/functional.py:1117:17
  %2169 : Tensor = prim::GetAttr[name="weight"](%2144)
  %2170 : Tensor? = prim::GetAttr[name="bias"](%2144)
  %2171 : int[] = prim::ListConstruct(%27, %27)
  %2172 : int[] = prim::ListConstruct(%27, %27)
  %2173 : int[] = prim::ListConstruct(%27, %27)
  %new_features.89 : Tensor = aten::conv2d(%result.90, %2169, %2170, %2171, %2172, %2173, %27) # torch/nn/modules/conv.py:415:15
  %2175 : float = prim::GetAttr[name="drop_rate"](%1878)
  %2176 : bool = aten::gt(%2175, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.90 : Tensor = prim::If(%2176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2178 : float = prim::GetAttr[name="drop_rate"](%1878)
      %2179 : bool = prim::GetAttr[name="training"](%1878)
      %2180 : bool = aten::lt(%2178, %16) # torch/nn/functional.py:968:7
      %2181 : bool = prim::If(%2180) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2182 : bool = aten::gt(%2178, %17) # torch/nn/functional.py:968:17
          -> (%2182)
       = prim::If(%2181) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2183 : Tensor = aten::dropout(%new_features.89, %2178, %2179) # torch/nn/functional.py:973:17
      -> (%2183)
    block1():
      -> (%new_features.89)
  %2184 : Tensor[] = aten::append(%features.4, %new_features.90) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2185 : Tensor = prim::Uninitialized()
  %2186 : bool = prim::GetAttr[name="memory_efficient"](%1879)
  %2187 : bool = prim::If(%2186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2188 : bool = prim::Uninitialized()
      %2189 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2190 : bool = aten::gt(%2189, %24)
      %2191 : bool, %2192 : bool, %2193 : int = prim::Loop(%18, %2190, %19, %2188, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2194 : int, %2195 : bool, %2196 : bool, %2197 : int):
          %tensor.46 : Tensor = aten::__getitem__(%features.4, %2197) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2199 : bool = prim::requires_grad(%tensor.46)
          %2200 : bool, %2201 : bool = prim::If(%2199) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2188)
          %2202 : int = aten::add(%2197, %27)
          %2203 : bool = aten::lt(%2202, %2189)
          %2204 : bool = aten::__and__(%2203, %2200)
          -> (%2204, %2199, %2201, %2202)
      %2205 : bool = prim::If(%2191)
        block0():
          -> (%2192)
        block1():
          -> (%19)
      -> (%2205)
    block1():
      -> (%19)
  %bottleneck_output.90 : Tensor = prim::If(%2187) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2185)
    block1():
      %concated_features.46 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2208 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%1879)
      %2209 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%1879)
      %2210 : int = aten::dim(%concated_features.46) # torch/nn/modules/batchnorm.py:276:11
      %2211 : bool = aten::ne(%2210, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2211) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2212 : bool = prim::GetAttr[name="training"](%2209)
       = prim::If(%2212) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2213 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2209)
          %2214 : Tensor = aten::add(%2213, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2209, %2214)
          -> ()
        block1():
          -> ()
      %2215 : bool = prim::GetAttr[name="training"](%2209)
      %2216 : Tensor = prim::GetAttr[name="running_mean"](%2209)
      %2217 : Tensor = prim::GetAttr[name="running_var"](%2209)
      %2218 : Tensor = prim::GetAttr[name="weight"](%2209)
      %2219 : Tensor = prim::GetAttr[name="bias"](%2209)
       = prim::If(%2215) # torch/nn/functional.py:2011:4
        block0():
          %2220 : int[] = aten::size(%concated_features.46) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%2220, %24) # torch/nn/functional.py:1991:17
          %2222 : int = aten::len(%2220) # torch/nn/functional.py:1992:19
          %2223 : int = aten::sub(%2222, %26) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%2223, %25, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %2227 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
              %2228 : int = aten::__getitem__(%2220, %2227) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %2228) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.379)
          %2230 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2230) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2231 : Tensor = aten::batch_norm(%concated_features.46, %2218, %2219, %2216, %2217, %2215, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.91 : Tensor = aten::relu_(%2231) # torch/nn/functional.py:1117:17
      %2233 : Tensor = prim::GetAttr[name="weight"](%2208)
      %2234 : Tensor? = prim::GetAttr[name="bias"](%2208)
      %2235 : int[] = prim::ListConstruct(%27, %27)
      %2236 : int[] = prim::ListConstruct(%24, %24)
      %2237 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.91 : Tensor = aten::conv2d(%result.91, %2233, %2234, %2235, %2236, %2237, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.91)
  %2239 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1879)
  %2240 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1879)
  %2241 : int = aten::dim(%bottleneck_output.90) # torch/nn/modules/batchnorm.py:276:11
  %2242 : bool = aten::ne(%2241, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2242) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2243 : bool = prim::GetAttr[name="training"](%2240)
   = prim::If(%2243) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2244 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2240)
      %2245 : Tensor = aten::add(%2244, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2240, %2245)
      -> ()
    block1():
      -> ()
  %2246 : bool = prim::GetAttr[name="training"](%2240)
  %2247 : Tensor = prim::GetAttr[name="running_mean"](%2240)
  %2248 : Tensor = prim::GetAttr[name="running_var"](%2240)
  %2249 : Tensor = prim::GetAttr[name="weight"](%2240)
  %2250 : Tensor = prim::GetAttr[name="bias"](%2240)
   = prim::If(%2246) # torch/nn/functional.py:2011:4
    block0():
      %2251 : int[] = aten::size(%bottleneck_output.90) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%2251, %24) # torch/nn/functional.py:1991:17
      %2253 : int = aten::len(%2251) # torch/nn/functional.py:1992:19
      %2254 : int = aten::sub(%2253, %26) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%2254, %25, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %2258 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
          %2259 : int = aten::__getitem__(%2251, %2258) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %2259) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.383)
      %2261 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2261) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2262 : Tensor = aten::batch_norm(%bottleneck_output.90, %2249, %2250, %2247, %2248, %2246, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.92 : Tensor = aten::relu_(%2262) # torch/nn/functional.py:1117:17
  %2264 : Tensor = prim::GetAttr[name="weight"](%2239)
  %2265 : Tensor? = prim::GetAttr[name="bias"](%2239)
  %2266 : int[] = prim::ListConstruct(%27, %27)
  %2267 : int[] = prim::ListConstruct(%27, %27)
  %2268 : int[] = prim::ListConstruct(%27, %27)
  %new_features.91 : Tensor = aten::conv2d(%result.92, %2264, %2265, %2266, %2267, %2268, %27) # torch/nn/modules/conv.py:415:15
  %2270 : float = prim::GetAttr[name="drop_rate"](%1879)
  %2271 : bool = aten::gt(%2270, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.92 : Tensor = prim::If(%2271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2273 : float = prim::GetAttr[name="drop_rate"](%1879)
      %2274 : bool = prim::GetAttr[name="training"](%1879)
      %2275 : bool = aten::lt(%2273, %16) # torch/nn/functional.py:968:7
      %2276 : bool = prim::If(%2275) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2277 : bool = aten::gt(%2273, %17) # torch/nn/functional.py:968:17
          -> (%2277)
       = prim::If(%2276) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2278 : Tensor = aten::dropout(%new_features.91, %2273, %2274) # torch/nn/functional.py:973:17
      -> (%2278)
    block1():
      -> (%new_features.91)
  %2279 : Tensor[] = aten::append(%features.4, %new_features.92) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2280 : Tensor = prim::Uninitialized()
  %2281 : bool = prim::GetAttr[name="memory_efficient"](%1880)
  %2282 : bool = prim::If(%2281) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2283 : bool = prim::Uninitialized()
      %2284 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2285 : bool = aten::gt(%2284, %24)
      %2286 : bool, %2287 : bool, %2288 : int = prim::Loop(%18, %2285, %19, %2283, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2289 : int, %2290 : bool, %2291 : bool, %2292 : int):
          %tensor.47 : Tensor = aten::__getitem__(%features.4, %2292) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2294 : bool = prim::requires_grad(%tensor.47)
          %2295 : bool, %2296 : bool = prim::If(%2294) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2283)
          %2297 : int = aten::add(%2292, %27)
          %2298 : bool = aten::lt(%2297, %2284)
          %2299 : bool = aten::__and__(%2298, %2295)
          -> (%2299, %2294, %2296, %2297)
      %2300 : bool = prim::If(%2286)
        block0():
          -> (%2287)
        block1():
          -> (%19)
      -> (%2300)
    block1():
      -> (%19)
  %bottleneck_output.92 : Tensor = prim::If(%2282) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2280)
    block1():
      %concated_features.47 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2303 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%1880)
      %2304 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%1880)
      %2305 : int = aten::dim(%concated_features.47) # torch/nn/modules/batchnorm.py:276:11
      %2306 : bool = aten::ne(%2305, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2306) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2307 : bool = prim::GetAttr[name="training"](%2304)
       = prim::If(%2307) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2308 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2304)
          %2309 : Tensor = aten::add(%2308, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2304, %2309)
          -> ()
        block1():
          -> ()
      %2310 : bool = prim::GetAttr[name="training"](%2304)
      %2311 : Tensor = prim::GetAttr[name="running_mean"](%2304)
      %2312 : Tensor = prim::GetAttr[name="running_var"](%2304)
      %2313 : Tensor = prim::GetAttr[name="weight"](%2304)
      %2314 : Tensor = prim::GetAttr[name="bias"](%2304)
       = prim::If(%2310) # torch/nn/functional.py:2011:4
        block0():
          %2315 : int[] = aten::size(%concated_features.47) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%2315, %24) # torch/nn/functional.py:1991:17
          %2317 : int = aten::len(%2315) # torch/nn/functional.py:1992:19
          %2318 : int = aten::sub(%2317, %26) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%2318, %25, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %2322 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
              %2323 : int = aten::__getitem__(%2315, %2322) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %2323) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.387)
          %2325 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2325) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2326 : Tensor = aten::batch_norm(%concated_features.47, %2313, %2314, %2311, %2312, %2310, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.93 : Tensor = aten::relu_(%2326) # torch/nn/functional.py:1117:17
      %2328 : Tensor = prim::GetAttr[name="weight"](%2303)
      %2329 : Tensor? = prim::GetAttr[name="bias"](%2303)
      %2330 : int[] = prim::ListConstruct(%27, %27)
      %2331 : int[] = prim::ListConstruct(%24, %24)
      %2332 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.93 : Tensor = aten::conv2d(%result.93, %2328, %2329, %2330, %2331, %2332, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.93)
  %2334 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1880)
  %2335 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1880)
  %2336 : int = aten::dim(%bottleneck_output.92) # torch/nn/modules/batchnorm.py:276:11
  %2337 : bool = aten::ne(%2336, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2337) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2338 : bool = prim::GetAttr[name="training"](%2335)
   = prim::If(%2338) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2339 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2335)
      %2340 : Tensor = aten::add(%2339, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2335, %2340)
      -> ()
    block1():
      -> ()
  %2341 : bool = prim::GetAttr[name="training"](%2335)
  %2342 : Tensor = prim::GetAttr[name="running_mean"](%2335)
  %2343 : Tensor = prim::GetAttr[name="running_var"](%2335)
  %2344 : Tensor = prim::GetAttr[name="weight"](%2335)
  %2345 : Tensor = prim::GetAttr[name="bias"](%2335)
   = prim::If(%2341) # torch/nn/functional.py:2011:4
    block0():
      %2346 : int[] = aten::size(%bottleneck_output.92) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%2346, %24) # torch/nn/functional.py:1991:17
      %2348 : int = aten::len(%2346) # torch/nn/functional.py:1992:19
      %2349 : int = aten::sub(%2348, %26) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%2349, %25, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %2353 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
          %2354 : int = aten::__getitem__(%2346, %2353) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %2354) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.391)
      %2356 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2356) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2357 : Tensor = aten::batch_norm(%bottleneck_output.92, %2344, %2345, %2342, %2343, %2341, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.94 : Tensor = aten::relu_(%2357) # torch/nn/functional.py:1117:17
  %2359 : Tensor = prim::GetAttr[name="weight"](%2334)
  %2360 : Tensor? = prim::GetAttr[name="bias"](%2334)
  %2361 : int[] = prim::ListConstruct(%27, %27)
  %2362 : int[] = prim::ListConstruct(%27, %27)
  %2363 : int[] = prim::ListConstruct(%27, %27)
  %new_features.93 : Tensor = aten::conv2d(%result.94, %2359, %2360, %2361, %2362, %2363, %27) # torch/nn/modules/conv.py:415:15
  %2365 : float = prim::GetAttr[name="drop_rate"](%1880)
  %2366 : bool = aten::gt(%2365, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.94 : Tensor = prim::If(%2366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2368 : float = prim::GetAttr[name="drop_rate"](%1880)
      %2369 : bool = prim::GetAttr[name="training"](%1880)
      %2370 : bool = aten::lt(%2368, %16) # torch/nn/functional.py:968:7
      %2371 : bool = prim::If(%2370) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2372 : bool = aten::gt(%2368, %17) # torch/nn/functional.py:968:17
          -> (%2372)
       = prim::If(%2371) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2373 : Tensor = aten::dropout(%new_features.93, %2368, %2369) # torch/nn/functional.py:973:17
      -> (%2373)
    block1():
      -> (%new_features.93)
  %2374 : Tensor[] = aten::append(%features.4, %new_features.94) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2375 : Tensor = prim::Uninitialized()
  %2376 : bool = prim::GetAttr[name="memory_efficient"](%1881)
  %2377 : bool = prim::If(%2376) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2378 : bool = prim::Uninitialized()
      %2379 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2380 : bool = aten::gt(%2379, %24)
      %2381 : bool, %2382 : bool, %2383 : int = prim::Loop(%18, %2380, %19, %2378, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2384 : int, %2385 : bool, %2386 : bool, %2387 : int):
          %tensor.48 : Tensor = aten::__getitem__(%features.4, %2387) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2389 : bool = prim::requires_grad(%tensor.48)
          %2390 : bool, %2391 : bool = prim::If(%2389) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2378)
          %2392 : int = aten::add(%2387, %27)
          %2393 : bool = aten::lt(%2392, %2379)
          %2394 : bool = aten::__and__(%2393, %2390)
          -> (%2394, %2389, %2391, %2392)
      %2395 : bool = prim::If(%2381)
        block0():
          -> (%2382)
        block1():
          -> (%19)
      -> (%2395)
    block1():
      -> (%19)
  %bottleneck_output.94 : Tensor = prim::If(%2377) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2375)
    block1():
      %concated_features.48 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2398 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%1881)
      %2399 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%1881)
      %2400 : int = aten::dim(%concated_features.48) # torch/nn/modules/batchnorm.py:276:11
      %2401 : bool = aten::ne(%2400, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2401) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2402 : bool = prim::GetAttr[name="training"](%2399)
       = prim::If(%2402) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2403 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2399)
          %2404 : Tensor = aten::add(%2403, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2399, %2404)
          -> ()
        block1():
          -> ()
      %2405 : bool = prim::GetAttr[name="training"](%2399)
      %2406 : Tensor = prim::GetAttr[name="running_mean"](%2399)
      %2407 : Tensor = prim::GetAttr[name="running_var"](%2399)
      %2408 : Tensor = prim::GetAttr[name="weight"](%2399)
      %2409 : Tensor = prim::GetAttr[name="bias"](%2399)
       = prim::If(%2405) # torch/nn/functional.py:2011:4
        block0():
          %2410 : int[] = aten::size(%concated_features.48) # torch/nn/functional.py:2012:27
          %size_prods.392 : int = aten::__getitem__(%2410, %24) # torch/nn/functional.py:1991:17
          %2412 : int = aten::len(%2410) # torch/nn/functional.py:1992:19
          %2413 : int = aten::sub(%2412, %26) # torch/nn/functional.py:1992:19
          %size_prods.393 : int = prim::Loop(%2413, %25, %size_prods.392) # torch/nn/functional.py:1992:4
            block0(%i.99 : int, %size_prods.394 : int):
              %2417 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
              %2418 : int = aten::__getitem__(%2410, %2417) # torch/nn/functional.py:1993:22
              %size_prods.395 : int = aten::mul(%size_prods.394, %2418) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.395)
          %2420 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2420) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2421 : Tensor = aten::batch_norm(%concated_features.48, %2408, %2409, %2406, %2407, %2405, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.95 : Tensor = aten::relu_(%2421) # torch/nn/functional.py:1117:17
      %2423 : Tensor = prim::GetAttr[name="weight"](%2398)
      %2424 : Tensor? = prim::GetAttr[name="bias"](%2398)
      %2425 : int[] = prim::ListConstruct(%27, %27)
      %2426 : int[] = prim::ListConstruct(%24, %24)
      %2427 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.95 : Tensor = aten::conv2d(%result.95, %2423, %2424, %2425, %2426, %2427, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.95)
  %2429 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1881)
  %2430 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1881)
  %2431 : int = aten::dim(%bottleneck_output.94) # torch/nn/modules/batchnorm.py:276:11
  %2432 : bool = aten::ne(%2431, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2432) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2433 : bool = prim::GetAttr[name="training"](%2430)
   = prim::If(%2433) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2434 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2430)
      %2435 : Tensor = aten::add(%2434, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2430, %2435)
      -> ()
    block1():
      -> ()
  %2436 : bool = prim::GetAttr[name="training"](%2430)
  %2437 : Tensor = prim::GetAttr[name="running_mean"](%2430)
  %2438 : Tensor = prim::GetAttr[name="running_var"](%2430)
  %2439 : Tensor = prim::GetAttr[name="weight"](%2430)
  %2440 : Tensor = prim::GetAttr[name="bias"](%2430)
   = prim::If(%2436) # torch/nn/functional.py:2011:4
    block0():
      %2441 : int[] = aten::size(%bottleneck_output.94) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%2441, %24) # torch/nn/functional.py:1991:17
      %2443 : int = aten::len(%2441) # torch/nn/functional.py:1992:19
      %2444 : int = aten::sub(%2443, %26) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%2444, %25, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %2448 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
          %2449 : int = aten::__getitem__(%2441, %2448) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %2449) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.399)
      %2451 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2451) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2452 : Tensor = aten::batch_norm(%bottleneck_output.94, %2439, %2440, %2437, %2438, %2436, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.96 : Tensor = aten::relu_(%2452) # torch/nn/functional.py:1117:17
  %2454 : Tensor = prim::GetAttr[name="weight"](%2429)
  %2455 : Tensor? = prim::GetAttr[name="bias"](%2429)
  %2456 : int[] = prim::ListConstruct(%27, %27)
  %2457 : int[] = prim::ListConstruct(%27, %27)
  %2458 : int[] = prim::ListConstruct(%27, %27)
  %new_features.95 : Tensor = aten::conv2d(%result.96, %2454, %2455, %2456, %2457, %2458, %27) # torch/nn/modules/conv.py:415:15
  %2460 : float = prim::GetAttr[name="drop_rate"](%1881)
  %2461 : bool = aten::gt(%2460, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.96 : Tensor = prim::If(%2461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2463 : float = prim::GetAttr[name="drop_rate"](%1881)
      %2464 : bool = prim::GetAttr[name="training"](%1881)
      %2465 : bool = aten::lt(%2463, %16) # torch/nn/functional.py:968:7
      %2466 : bool = prim::If(%2465) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2467 : bool = aten::gt(%2463, %17) # torch/nn/functional.py:968:17
          -> (%2467)
       = prim::If(%2466) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2468 : Tensor = aten::dropout(%new_features.95, %2463, %2464) # torch/nn/functional.py:973:17
      -> (%2468)
    block1():
      -> (%new_features.95)
  %2469 : Tensor[] = aten::append(%features.4, %new_features.96) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2470 : Tensor = prim::Uninitialized()
  %2471 : bool = prim::GetAttr[name="memory_efficient"](%1882)
  %2472 : bool = prim::If(%2471) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2473 : bool = prim::Uninitialized()
      %2474 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2475 : bool = aten::gt(%2474, %24)
      %2476 : bool, %2477 : bool, %2478 : int = prim::Loop(%18, %2475, %19, %2473, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2479 : int, %2480 : bool, %2481 : bool, %2482 : int):
          %tensor.49 : Tensor = aten::__getitem__(%features.4, %2482) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2484 : bool = prim::requires_grad(%tensor.49)
          %2485 : bool, %2486 : bool = prim::If(%2484) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2473)
          %2487 : int = aten::add(%2482, %27)
          %2488 : bool = aten::lt(%2487, %2474)
          %2489 : bool = aten::__and__(%2488, %2485)
          -> (%2489, %2484, %2486, %2487)
      %2490 : bool = prim::If(%2476)
        block0():
          -> (%2477)
        block1():
          -> (%19)
      -> (%2490)
    block1():
      -> (%19)
  %bottleneck_output.96 : Tensor = prim::If(%2472) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2470)
    block1():
      %concated_features.49 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2493 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%1882)
      %2494 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%1882)
      %2495 : int = aten::dim(%concated_features.49) # torch/nn/modules/batchnorm.py:276:11
      %2496 : bool = aten::ne(%2495, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2496) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2497 : bool = prim::GetAttr[name="training"](%2494)
       = prim::If(%2497) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2498 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2494)
          %2499 : Tensor = aten::add(%2498, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2494, %2499)
          -> ()
        block1():
          -> ()
      %2500 : bool = prim::GetAttr[name="training"](%2494)
      %2501 : Tensor = prim::GetAttr[name="running_mean"](%2494)
      %2502 : Tensor = prim::GetAttr[name="running_var"](%2494)
      %2503 : Tensor = prim::GetAttr[name="weight"](%2494)
      %2504 : Tensor = prim::GetAttr[name="bias"](%2494)
       = prim::If(%2500) # torch/nn/functional.py:2011:4
        block0():
          %2505 : int[] = aten::size(%concated_features.49) # torch/nn/functional.py:2012:27
          %size_prods.400 : int = aten::__getitem__(%2505, %24) # torch/nn/functional.py:1991:17
          %2507 : int = aten::len(%2505) # torch/nn/functional.py:1992:19
          %2508 : int = aten::sub(%2507, %26) # torch/nn/functional.py:1992:19
          %size_prods.401 : int = prim::Loop(%2508, %25, %size_prods.400) # torch/nn/functional.py:1992:4
            block0(%i.101 : int, %size_prods.402 : int):
              %2512 : int = aten::add(%i.101, %26) # torch/nn/functional.py:1993:27
              %2513 : int = aten::__getitem__(%2505, %2512) # torch/nn/functional.py:1993:22
              %size_prods.403 : int = aten::mul(%size_prods.402, %2513) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.403)
          %2515 : bool = aten::eq(%size_prods.401, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2515) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2516 : Tensor = aten::batch_norm(%concated_features.49, %2503, %2504, %2501, %2502, %2500, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.97 : Tensor = aten::relu_(%2516) # torch/nn/functional.py:1117:17
      %2518 : Tensor = prim::GetAttr[name="weight"](%2493)
      %2519 : Tensor? = prim::GetAttr[name="bias"](%2493)
      %2520 : int[] = prim::ListConstruct(%27, %27)
      %2521 : int[] = prim::ListConstruct(%24, %24)
      %2522 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.97 : Tensor = aten::conv2d(%result.97, %2518, %2519, %2520, %2521, %2522, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.97)
  %2524 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1882)
  %2525 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1882)
  %2526 : int = aten::dim(%bottleneck_output.96) # torch/nn/modules/batchnorm.py:276:11
  %2527 : bool = aten::ne(%2526, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2527) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2528 : bool = prim::GetAttr[name="training"](%2525)
   = prim::If(%2528) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2529 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2525)
      %2530 : Tensor = aten::add(%2529, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2525, %2530)
      -> ()
    block1():
      -> ()
  %2531 : bool = prim::GetAttr[name="training"](%2525)
  %2532 : Tensor = prim::GetAttr[name="running_mean"](%2525)
  %2533 : Tensor = prim::GetAttr[name="running_var"](%2525)
  %2534 : Tensor = prim::GetAttr[name="weight"](%2525)
  %2535 : Tensor = prim::GetAttr[name="bias"](%2525)
   = prim::If(%2531) # torch/nn/functional.py:2011:4
    block0():
      %2536 : int[] = aten::size(%bottleneck_output.96) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%2536, %24) # torch/nn/functional.py:1991:17
      %2538 : int = aten::len(%2536) # torch/nn/functional.py:1992:19
      %2539 : int = aten::sub(%2538, %26) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%2539, %25, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %2543 : int = aten::add(%i.102, %26) # torch/nn/functional.py:1993:27
          %2544 : int = aten::__getitem__(%2536, %2543) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %2544) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.407)
      %2546 : bool = aten::eq(%size_prods.405, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2546) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2547 : Tensor = aten::batch_norm(%bottleneck_output.96, %2534, %2535, %2532, %2533, %2531, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.98 : Tensor = aten::relu_(%2547) # torch/nn/functional.py:1117:17
  %2549 : Tensor = prim::GetAttr[name="weight"](%2524)
  %2550 : Tensor? = prim::GetAttr[name="bias"](%2524)
  %2551 : int[] = prim::ListConstruct(%27, %27)
  %2552 : int[] = prim::ListConstruct(%27, %27)
  %2553 : int[] = prim::ListConstruct(%27, %27)
  %new_features.97 : Tensor = aten::conv2d(%result.98, %2549, %2550, %2551, %2552, %2553, %27) # torch/nn/modules/conv.py:415:15
  %2555 : float = prim::GetAttr[name="drop_rate"](%1882)
  %2556 : bool = aten::gt(%2555, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.98 : Tensor = prim::If(%2556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2558 : float = prim::GetAttr[name="drop_rate"](%1882)
      %2559 : bool = prim::GetAttr[name="training"](%1882)
      %2560 : bool = aten::lt(%2558, %16) # torch/nn/functional.py:968:7
      %2561 : bool = prim::If(%2560) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2562 : bool = aten::gt(%2558, %17) # torch/nn/functional.py:968:17
          -> (%2562)
       = prim::If(%2561) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2563 : Tensor = aten::dropout(%new_features.97, %2558, %2559) # torch/nn/functional.py:973:17
      -> (%2563)
    block1():
      -> (%new_features.97)
  %2564 : Tensor[] = aten::append(%features.4, %new_features.98) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2565 : Tensor = prim::Uninitialized()
  %2566 : bool = prim::GetAttr[name="memory_efficient"](%1883)
  %2567 : bool = prim::If(%2566) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2568 : bool = prim::Uninitialized()
      %2569 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2570 : bool = aten::gt(%2569, %24)
      %2571 : bool, %2572 : bool, %2573 : int = prim::Loop(%18, %2570, %19, %2568, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2574 : int, %2575 : bool, %2576 : bool, %2577 : int):
          %tensor.50 : Tensor = aten::__getitem__(%features.4, %2577) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2579 : bool = prim::requires_grad(%tensor.50)
          %2580 : bool, %2581 : bool = prim::If(%2579) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2568)
          %2582 : int = aten::add(%2577, %27)
          %2583 : bool = aten::lt(%2582, %2569)
          %2584 : bool = aten::__and__(%2583, %2580)
          -> (%2584, %2579, %2581, %2582)
      %2585 : bool = prim::If(%2571)
        block0():
          -> (%2572)
        block1():
          -> (%19)
      -> (%2585)
    block1():
      -> (%19)
  %bottleneck_output.98 : Tensor = prim::If(%2567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2565)
    block1():
      %concated_features.50 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2588 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%1883)
      %2589 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%1883)
      %2590 : int = aten::dim(%concated_features.50) # torch/nn/modules/batchnorm.py:276:11
      %2591 : bool = aten::ne(%2590, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2591) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2592 : bool = prim::GetAttr[name="training"](%2589)
       = prim::If(%2592) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2593 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2589)
          %2594 : Tensor = aten::add(%2593, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2589, %2594)
          -> ()
        block1():
          -> ()
      %2595 : bool = prim::GetAttr[name="training"](%2589)
      %2596 : Tensor = prim::GetAttr[name="running_mean"](%2589)
      %2597 : Tensor = prim::GetAttr[name="running_var"](%2589)
      %2598 : Tensor = prim::GetAttr[name="weight"](%2589)
      %2599 : Tensor = prim::GetAttr[name="bias"](%2589)
       = prim::If(%2595) # torch/nn/functional.py:2011:4
        block0():
          %2600 : int[] = aten::size(%concated_features.50) # torch/nn/functional.py:2012:27
          %size_prods.408 : int = aten::__getitem__(%2600, %24) # torch/nn/functional.py:1991:17
          %2602 : int = aten::len(%2600) # torch/nn/functional.py:1992:19
          %2603 : int = aten::sub(%2602, %26) # torch/nn/functional.py:1992:19
          %size_prods.409 : int = prim::Loop(%2603, %25, %size_prods.408) # torch/nn/functional.py:1992:4
            block0(%i.103 : int, %size_prods.410 : int):
              %2607 : int = aten::add(%i.103, %26) # torch/nn/functional.py:1993:27
              %2608 : int = aten::__getitem__(%2600, %2607) # torch/nn/functional.py:1993:22
              %size_prods.411 : int = aten::mul(%size_prods.410, %2608) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.411)
          %2610 : bool = aten::eq(%size_prods.409, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2610) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2611 : Tensor = aten::batch_norm(%concated_features.50, %2598, %2599, %2596, %2597, %2595, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.99 : Tensor = aten::relu_(%2611) # torch/nn/functional.py:1117:17
      %2613 : Tensor = prim::GetAttr[name="weight"](%2588)
      %2614 : Tensor? = prim::GetAttr[name="bias"](%2588)
      %2615 : int[] = prim::ListConstruct(%27, %27)
      %2616 : int[] = prim::ListConstruct(%24, %24)
      %2617 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.99 : Tensor = aten::conv2d(%result.99, %2613, %2614, %2615, %2616, %2617, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.99)
  %2619 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1883)
  %2620 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1883)
  %2621 : int = aten::dim(%bottleneck_output.98) # torch/nn/modules/batchnorm.py:276:11
  %2622 : bool = aten::ne(%2621, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2622) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2623 : bool = prim::GetAttr[name="training"](%2620)
   = prim::If(%2623) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2624 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2620)
      %2625 : Tensor = aten::add(%2624, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2620, %2625)
      -> ()
    block1():
      -> ()
  %2626 : bool = prim::GetAttr[name="training"](%2620)
  %2627 : Tensor = prim::GetAttr[name="running_mean"](%2620)
  %2628 : Tensor = prim::GetAttr[name="running_var"](%2620)
  %2629 : Tensor = prim::GetAttr[name="weight"](%2620)
  %2630 : Tensor = prim::GetAttr[name="bias"](%2620)
   = prim::If(%2626) # torch/nn/functional.py:2011:4
    block0():
      %2631 : int[] = aten::size(%bottleneck_output.98) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%2631, %24) # torch/nn/functional.py:1991:17
      %2633 : int = aten::len(%2631) # torch/nn/functional.py:1992:19
      %2634 : int = aten::sub(%2633, %26) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%2634, %25, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %2638 : int = aten::add(%i.104, %26) # torch/nn/functional.py:1993:27
          %2639 : int = aten::__getitem__(%2631, %2638) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %2639) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.415)
      %2641 : bool = aten::eq(%size_prods.413, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2641) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2642 : Tensor = aten::batch_norm(%bottleneck_output.98, %2629, %2630, %2627, %2628, %2626, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.100 : Tensor = aten::relu_(%2642) # torch/nn/functional.py:1117:17
  %2644 : Tensor = prim::GetAttr[name="weight"](%2619)
  %2645 : Tensor? = prim::GetAttr[name="bias"](%2619)
  %2646 : int[] = prim::ListConstruct(%27, %27)
  %2647 : int[] = prim::ListConstruct(%27, %27)
  %2648 : int[] = prim::ListConstruct(%27, %27)
  %new_features.99 : Tensor = aten::conv2d(%result.100, %2644, %2645, %2646, %2647, %2648, %27) # torch/nn/modules/conv.py:415:15
  %2650 : float = prim::GetAttr[name="drop_rate"](%1883)
  %2651 : bool = aten::gt(%2650, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.100 : Tensor = prim::If(%2651) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2653 : float = prim::GetAttr[name="drop_rate"](%1883)
      %2654 : bool = prim::GetAttr[name="training"](%1883)
      %2655 : bool = aten::lt(%2653, %16) # torch/nn/functional.py:968:7
      %2656 : bool = prim::If(%2655) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2657 : bool = aten::gt(%2653, %17) # torch/nn/functional.py:968:17
          -> (%2657)
       = prim::If(%2656) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2658 : Tensor = aten::dropout(%new_features.99, %2653, %2654) # torch/nn/functional.py:973:17
      -> (%2658)
    block1():
      -> (%new_features.99)
  %2659 : Tensor[] = aten::append(%features.4, %new_features.100) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2660 : Tensor = prim::Uninitialized()
  %2661 : bool = prim::GetAttr[name="memory_efficient"](%1884)
  %2662 : bool = prim::If(%2661) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2663 : bool = prim::Uninitialized()
      %2664 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2665 : bool = aten::gt(%2664, %24)
      %2666 : bool, %2667 : bool, %2668 : int = prim::Loop(%18, %2665, %19, %2663, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2669 : int, %2670 : bool, %2671 : bool, %2672 : int):
          %tensor.51 : Tensor = aten::__getitem__(%features.4, %2672) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2674 : bool = prim::requires_grad(%tensor.51)
          %2675 : bool, %2676 : bool = prim::If(%2674) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2663)
          %2677 : int = aten::add(%2672, %27)
          %2678 : bool = aten::lt(%2677, %2664)
          %2679 : bool = aten::__and__(%2678, %2675)
          -> (%2679, %2674, %2676, %2677)
      %2680 : bool = prim::If(%2666)
        block0():
          -> (%2667)
        block1():
          -> (%19)
      -> (%2680)
    block1():
      -> (%19)
  %bottleneck_output.100 : Tensor = prim::If(%2662) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2660)
    block1():
      %concated_features.51 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2683 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%1884)
      %2684 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm1"](%1884)
      %2685 : int = aten::dim(%concated_features.51) # torch/nn/modules/batchnorm.py:276:11
      %2686 : bool = aten::ne(%2685, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2686) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2687 : bool = prim::GetAttr[name="training"](%2684)
       = prim::If(%2687) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2688 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2684)
          %2689 : Tensor = aten::add(%2688, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2684, %2689)
          -> ()
        block1():
          -> ()
      %2690 : bool = prim::GetAttr[name="training"](%2684)
      %2691 : Tensor = prim::GetAttr[name="running_mean"](%2684)
      %2692 : Tensor = prim::GetAttr[name="running_var"](%2684)
      %2693 : Tensor = prim::GetAttr[name="weight"](%2684)
      %2694 : Tensor = prim::GetAttr[name="bias"](%2684)
       = prim::If(%2690) # torch/nn/functional.py:2011:4
        block0():
          %2695 : int[] = aten::size(%concated_features.51) # torch/nn/functional.py:2012:27
          %size_prods.416 : int = aten::__getitem__(%2695, %24) # torch/nn/functional.py:1991:17
          %2697 : int = aten::len(%2695) # torch/nn/functional.py:1992:19
          %2698 : int = aten::sub(%2697, %26) # torch/nn/functional.py:1992:19
          %size_prods.417 : int = prim::Loop(%2698, %25, %size_prods.416) # torch/nn/functional.py:1992:4
            block0(%i.105 : int, %size_prods.418 : int):
              %2702 : int = aten::add(%i.105, %26) # torch/nn/functional.py:1993:27
              %2703 : int = aten::__getitem__(%2695, %2702) # torch/nn/functional.py:1993:22
              %size_prods.419 : int = aten::mul(%size_prods.418, %2703) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.419)
          %2705 : bool = aten::eq(%size_prods.417, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2705) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2706 : Tensor = aten::batch_norm(%concated_features.51, %2693, %2694, %2691, %2692, %2690, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.101 : Tensor = aten::relu_(%2706) # torch/nn/functional.py:1117:17
      %2708 : Tensor = prim::GetAttr[name="weight"](%2683)
      %2709 : Tensor? = prim::GetAttr[name="bias"](%2683)
      %2710 : int[] = prim::ListConstruct(%27, %27)
      %2711 : int[] = prim::ListConstruct(%24, %24)
      %2712 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.101 : Tensor = aten::conv2d(%result.101, %2708, %2709, %2710, %2711, %2712, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.101)
  %2714 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1884)
  %2715 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1884)
  %2716 : int = aten::dim(%bottleneck_output.100) # torch/nn/modules/batchnorm.py:276:11
  %2717 : bool = aten::ne(%2716, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2717) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2718 : bool = prim::GetAttr[name="training"](%2715)
   = prim::If(%2718) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2719 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2715)
      %2720 : Tensor = aten::add(%2719, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2715, %2720)
      -> ()
    block1():
      -> ()
  %2721 : bool = prim::GetAttr[name="training"](%2715)
  %2722 : Tensor = prim::GetAttr[name="running_mean"](%2715)
  %2723 : Tensor = prim::GetAttr[name="running_var"](%2715)
  %2724 : Tensor = prim::GetAttr[name="weight"](%2715)
  %2725 : Tensor = prim::GetAttr[name="bias"](%2715)
   = prim::If(%2721) # torch/nn/functional.py:2011:4
    block0():
      %2726 : int[] = aten::size(%bottleneck_output.100) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%2726, %24) # torch/nn/functional.py:1991:17
      %2728 : int = aten::len(%2726) # torch/nn/functional.py:1992:19
      %2729 : int = aten::sub(%2728, %26) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%2729, %25, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %2733 : int = aten::add(%i.106, %26) # torch/nn/functional.py:1993:27
          %2734 : int = aten::__getitem__(%2726, %2733) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %2734) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.423)
      %2736 : bool = aten::eq(%size_prods.421, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2736) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2737 : Tensor = aten::batch_norm(%bottleneck_output.100, %2724, %2725, %2722, %2723, %2721, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.102 : Tensor = aten::relu_(%2737) # torch/nn/functional.py:1117:17
  %2739 : Tensor = prim::GetAttr[name="weight"](%2714)
  %2740 : Tensor? = prim::GetAttr[name="bias"](%2714)
  %2741 : int[] = prim::ListConstruct(%27, %27)
  %2742 : int[] = prim::ListConstruct(%27, %27)
  %2743 : int[] = prim::ListConstruct(%27, %27)
  %new_features.101 : Tensor = aten::conv2d(%result.102, %2739, %2740, %2741, %2742, %2743, %27) # torch/nn/modules/conv.py:415:15
  %2745 : float = prim::GetAttr[name="drop_rate"](%1884)
  %2746 : bool = aten::gt(%2745, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.102 : Tensor = prim::If(%2746) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2748 : float = prim::GetAttr[name="drop_rate"](%1884)
      %2749 : bool = prim::GetAttr[name="training"](%1884)
      %2750 : bool = aten::lt(%2748, %16) # torch/nn/functional.py:968:7
      %2751 : bool = prim::If(%2750) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2752 : bool = aten::gt(%2748, %17) # torch/nn/functional.py:968:17
          -> (%2752)
       = prim::If(%2751) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2753 : Tensor = aten::dropout(%new_features.101, %2748, %2749) # torch/nn/functional.py:973:17
      -> (%2753)
    block1():
      -> (%new_features.101)
  %2754 : Tensor[] = aten::append(%features.4, %new_features.102) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2755 : Tensor = prim::Uninitialized()
  %2756 : bool = prim::GetAttr[name="memory_efficient"](%1885)
  %2757 : bool = prim::If(%2756) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2758 : bool = prim::Uninitialized()
      %2759 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2760 : bool = aten::gt(%2759, %24)
      %2761 : bool, %2762 : bool, %2763 : int = prim::Loop(%18, %2760, %19, %2758, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2764 : int, %2765 : bool, %2766 : bool, %2767 : int):
          %tensor.52 : Tensor = aten::__getitem__(%features.4, %2767) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2769 : bool = prim::requires_grad(%tensor.52)
          %2770 : bool, %2771 : bool = prim::If(%2769) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2758)
          %2772 : int = aten::add(%2767, %27)
          %2773 : bool = aten::lt(%2772, %2759)
          %2774 : bool = aten::__and__(%2773, %2770)
          -> (%2774, %2769, %2771, %2772)
      %2775 : bool = prim::If(%2761)
        block0():
          -> (%2762)
        block1():
          -> (%19)
      -> (%2775)
    block1():
      -> (%19)
  %bottleneck_output.102 : Tensor = prim::If(%2757) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2755)
    block1():
      %concated_features.52 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2778 : __torch__.torch.nn.modules.conv.___torch_mangle_113.Conv2d = prim::GetAttr[name="conv1"](%1885)
      %2779 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_112.BatchNorm2d = prim::GetAttr[name="norm1"](%1885)
      %2780 : int = aten::dim(%concated_features.52) # torch/nn/modules/batchnorm.py:276:11
      %2781 : bool = aten::ne(%2780, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2781) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2782 : bool = prim::GetAttr[name="training"](%2779)
       = prim::If(%2782) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2783 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2779)
          %2784 : Tensor = aten::add(%2783, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2779, %2784)
          -> ()
        block1():
          -> ()
      %2785 : bool = prim::GetAttr[name="training"](%2779)
      %2786 : Tensor = prim::GetAttr[name="running_mean"](%2779)
      %2787 : Tensor = prim::GetAttr[name="running_var"](%2779)
      %2788 : Tensor = prim::GetAttr[name="weight"](%2779)
      %2789 : Tensor = prim::GetAttr[name="bias"](%2779)
       = prim::If(%2785) # torch/nn/functional.py:2011:4
        block0():
          %2790 : int[] = aten::size(%concated_features.52) # torch/nn/functional.py:2012:27
          %size_prods.424 : int = aten::__getitem__(%2790, %24) # torch/nn/functional.py:1991:17
          %2792 : int = aten::len(%2790) # torch/nn/functional.py:1992:19
          %2793 : int = aten::sub(%2792, %26) # torch/nn/functional.py:1992:19
          %size_prods.425 : int = prim::Loop(%2793, %25, %size_prods.424) # torch/nn/functional.py:1992:4
            block0(%i.107 : int, %size_prods.426 : int):
              %2797 : int = aten::add(%i.107, %26) # torch/nn/functional.py:1993:27
              %2798 : int = aten::__getitem__(%2790, %2797) # torch/nn/functional.py:1993:22
              %size_prods.427 : int = aten::mul(%size_prods.426, %2798) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.427)
          %2800 : bool = aten::eq(%size_prods.425, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2800) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2801 : Tensor = aten::batch_norm(%concated_features.52, %2788, %2789, %2786, %2787, %2785, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.103 : Tensor = aten::relu_(%2801) # torch/nn/functional.py:1117:17
      %2803 : Tensor = prim::GetAttr[name="weight"](%2778)
      %2804 : Tensor? = prim::GetAttr[name="bias"](%2778)
      %2805 : int[] = prim::ListConstruct(%27, %27)
      %2806 : int[] = prim::ListConstruct(%24, %24)
      %2807 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.103 : Tensor = aten::conv2d(%result.103, %2803, %2804, %2805, %2806, %2807, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.103)
  %2809 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1885)
  %2810 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1885)
  %2811 : int = aten::dim(%bottleneck_output.102) # torch/nn/modules/batchnorm.py:276:11
  %2812 : bool = aten::ne(%2811, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2812) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2813 : bool = prim::GetAttr[name="training"](%2810)
   = prim::If(%2813) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2814 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2810)
      %2815 : Tensor = aten::add(%2814, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2810, %2815)
      -> ()
    block1():
      -> ()
  %2816 : bool = prim::GetAttr[name="training"](%2810)
  %2817 : Tensor = prim::GetAttr[name="running_mean"](%2810)
  %2818 : Tensor = prim::GetAttr[name="running_var"](%2810)
  %2819 : Tensor = prim::GetAttr[name="weight"](%2810)
  %2820 : Tensor = prim::GetAttr[name="bias"](%2810)
   = prim::If(%2816) # torch/nn/functional.py:2011:4
    block0():
      %2821 : int[] = aten::size(%bottleneck_output.102) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%2821, %24) # torch/nn/functional.py:1991:17
      %2823 : int = aten::len(%2821) # torch/nn/functional.py:1992:19
      %2824 : int = aten::sub(%2823, %26) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%2824, %25, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.430 : int):
          %2828 : int = aten::add(%i.108, %26) # torch/nn/functional.py:1993:27
          %2829 : int = aten::__getitem__(%2821, %2828) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %2829) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.431)
      %2831 : bool = aten::eq(%size_prods.429, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2831) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2832 : Tensor = aten::batch_norm(%bottleneck_output.102, %2819, %2820, %2817, %2818, %2816, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.104 : Tensor = aten::relu_(%2832) # torch/nn/functional.py:1117:17
  %2834 : Tensor = prim::GetAttr[name="weight"](%2809)
  %2835 : Tensor? = prim::GetAttr[name="bias"](%2809)
  %2836 : int[] = prim::ListConstruct(%27, %27)
  %2837 : int[] = prim::ListConstruct(%27, %27)
  %2838 : int[] = prim::ListConstruct(%27, %27)
  %new_features.103 : Tensor = aten::conv2d(%result.104, %2834, %2835, %2836, %2837, %2838, %27) # torch/nn/modules/conv.py:415:15
  %2840 : float = prim::GetAttr[name="drop_rate"](%1885)
  %2841 : bool = aten::gt(%2840, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.104 : Tensor = prim::If(%2841) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2843 : float = prim::GetAttr[name="drop_rate"](%1885)
      %2844 : bool = prim::GetAttr[name="training"](%1885)
      %2845 : bool = aten::lt(%2843, %16) # torch/nn/functional.py:968:7
      %2846 : bool = prim::If(%2845) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2847 : bool = aten::gt(%2843, %17) # torch/nn/functional.py:968:17
          -> (%2847)
       = prim::If(%2846) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2848 : Tensor = aten::dropout(%new_features.103, %2843, %2844) # torch/nn/functional.py:973:17
      -> (%2848)
    block1():
      -> (%new_features.103)
  %2849 : Tensor[] = aten::append(%features.4, %new_features.104) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2850 : Tensor = prim::Uninitialized()
  %2851 : bool = prim::GetAttr[name="memory_efficient"](%1886)
  %2852 : bool = prim::If(%2851) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2853 : bool = prim::Uninitialized()
      %2854 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2855 : bool = aten::gt(%2854, %24)
      %2856 : bool, %2857 : bool, %2858 : int = prim::Loop(%18, %2855, %19, %2853, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2859 : int, %2860 : bool, %2861 : bool, %2862 : int):
          %tensor.53 : Tensor = aten::__getitem__(%features.4, %2862) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2864 : bool = prim::requires_grad(%tensor.53)
          %2865 : bool, %2866 : bool = prim::If(%2864) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2853)
          %2867 : int = aten::add(%2862, %27)
          %2868 : bool = aten::lt(%2867, %2854)
          %2869 : bool = aten::__and__(%2868, %2865)
          -> (%2869, %2864, %2866, %2867)
      %2870 : bool = prim::If(%2856)
        block0():
          -> (%2857)
        block1():
          -> (%19)
      -> (%2870)
    block1():
      -> (%19)
  %bottleneck_output.104 : Tensor = prim::If(%2852) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2850)
    block1():
      %concated_features.53 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2873 : __torch__.torch.nn.modules.conv.___torch_mangle_116.Conv2d = prim::GetAttr[name="conv1"](%1886)
      %2874 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%1886)
      %2875 : int = aten::dim(%concated_features.53) # torch/nn/modules/batchnorm.py:276:11
      %2876 : bool = aten::ne(%2875, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2876) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2877 : bool = prim::GetAttr[name="training"](%2874)
       = prim::If(%2877) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2878 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2874)
          %2879 : Tensor = aten::add(%2878, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2874, %2879)
          -> ()
        block1():
          -> ()
      %2880 : bool = prim::GetAttr[name="training"](%2874)
      %2881 : Tensor = prim::GetAttr[name="running_mean"](%2874)
      %2882 : Tensor = prim::GetAttr[name="running_var"](%2874)
      %2883 : Tensor = prim::GetAttr[name="weight"](%2874)
      %2884 : Tensor = prim::GetAttr[name="bias"](%2874)
       = prim::If(%2880) # torch/nn/functional.py:2011:4
        block0():
          %2885 : int[] = aten::size(%concated_features.53) # torch/nn/functional.py:2012:27
          %size_prods.432 : int = aten::__getitem__(%2885, %24) # torch/nn/functional.py:1991:17
          %2887 : int = aten::len(%2885) # torch/nn/functional.py:1992:19
          %2888 : int = aten::sub(%2887, %26) # torch/nn/functional.py:1992:19
          %size_prods.433 : int = prim::Loop(%2888, %25, %size_prods.432) # torch/nn/functional.py:1992:4
            block0(%i.109 : int, %size_prods.434 : int):
              %2892 : int = aten::add(%i.109, %26) # torch/nn/functional.py:1993:27
              %2893 : int = aten::__getitem__(%2885, %2892) # torch/nn/functional.py:1993:22
              %size_prods.435 : int = aten::mul(%size_prods.434, %2893) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.435)
          %2895 : bool = aten::eq(%size_prods.433, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2895) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2896 : Tensor = aten::batch_norm(%concated_features.53, %2883, %2884, %2881, %2882, %2880, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.105 : Tensor = aten::relu_(%2896) # torch/nn/functional.py:1117:17
      %2898 : Tensor = prim::GetAttr[name="weight"](%2873)
      %2899 : Tensor? = prim::GetAttr[name="bias"](%2873)
      %2900 : int[] = prim::ListConstruct(%27, %27)
      %2901 : int[] = prim::ListConstruct(%24, %24)
      %2902 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.105 : Tensor = aten::conv2d(%result.105, %2898, %2899, %2900, %2901, %2902, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.105)
  %2904 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1886)
  %2905 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1886)
  %2906 : int = aten::dim(%bottleneck_output.104) # torch/nn/modules/batchnorm.py:276:11
  %2907 : bool = aten::ne(%2906, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2907) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2908 : bool = prim::GetAttr[name="training"](%2905)
   = prim::If(%2908) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2909 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2905)
      %2910 : Tensor = aten::add(%2909, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2905, %2910)
      -> ()
    block1():
      -> ()
  %2911 : bool = prim::GetAttr[name="training"](%2905)
  %2912 : Tensor = prim::GetAttr[name="running_mean"](%2905)
  %2913 : Tensor = prim::GetAttr[name="running_var"](%2905)
  %2914 : Tensor = prim::GetAttr[name="weight"](%2905)
  %2915 : Tensor = prim::GetAttr[name="bias"](%2905)
   = prim::If(%2911) # torch/nn/functional.py:2011:4
    block0():
      %2916 : int[] = aten::size(%bottleneck_output.104) # torch/nn/functional.py:2012:27
      %size_prods.436 : int = aten::__getitem__(%2916, %24) # torch/nn/functional.py:1991:17
      %2918 : int = aten::len(%2916) # torch/nn/functional.py:1992:19
      %2919 : int = aten::sub(%2918, %26) # torch/nn/functional.py:1992:19
      %size_prods.437 : int = prim::Loop(%2919, %25, %size_prods.436) # torch/nn/functional.py:1992:4
        block0(%i.110 : int, %size_prods.438 : int):
          %2923 : int = aten::add(%i.110, %26) # torch/nn/functional.py:1993:27
          %2924 : int = aten::__getitem__(%2916, %2923) # torch/nn/functional.py:1993:22
          %size_prods.439 : int = aten::mul(%size_prods.438, %2924) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.439)
      %2926 : bool = aten::eq(%size_prods.437, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2926) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2927 : Tensor = aten::batch_norm(%bottleneck_output.104, %2914, %2915, %2912, %2913, %2911, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.106 : Tensor = aten::relu_(%2927) # torch/nn/functional.py:1117:17
  %2929 : Tensor = prim::GetAttr[name="weight"](%2904)
  %2930 : Tensor? = prim::GetAttr[name="bias"](%2904)
  %2931 : int[] = prim::ListConstruct(%27, %27)
  %2932 : int[] = prim::ListConstruct(%27, %27)
  %2933 : int[] = prim::ListConstruct(%27, %27)
  %new_features.105 : Tensor = aten::conv2d(%result.106, %2929, %2930, %2931, %2932, %2933, %27) # torch/nn/modules/conv.py:415:15
  %2935 : float = prim::GetAttr[name="drop_rate"](%1886)
  %2936 : bool = aten::gt(%2935, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.106 : Tensor = prim::If(%2936) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2938 : float = prim::GetAttr[name="drop_rate"](%1886)
      %2939 : bool = prim::GetAttr[name="training"](%1886)
      %2940 : bool = aten::lt(%2938, %16) # torch/nn/functional.py:968:7
      %2941 : bool = prim::If(%2940) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2942 : bool = aten::gt(%2938, %17) # torch/nn/functional.py:968:17
          -> (%2942)
       = prim::If(%2941) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2943 : Tensor = aten::dropout(%new_features.105, %2938, %2939) # torch/nn/functional.py:973:17
      -> (%2943)
    block1():
      -> (%new_features.105)
  %2944 : Tensor[] = aten::append(%features.4, %new_features.106) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2945 : Tensor = prim::Uninitialized()
  %2946 : bool = prim::GetAttr[name="memory_efficient"](%1887)
  %2947 : bool = prim::If(%2946) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2948 : bool = prim::Uninitialized()
      %2949 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2950 : bool = aten::gt(%2949, %24)
      %2951 : bool, %2952 : bool, %2953 : int = prim::Loop(%18, %2950, %19, %2948, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2954 : int, %2955 : bool, %2956 : bool, %2957 : int):
          %tensor.54 : Tensor = aten::__getitem__(%features.4, %2957) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2959 : bool = prim::requires_grad(%tensor.54)
          %2960 : bool, %2961 : bool = prim::If(%2959) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2948)
          %2962 : int = aten::add(%2957, %27)
          %2963 : bool = aten::lt(%2962, %2949)
          %2964 : bool = aten::__and__(%2963, %2960)
          -> (%2964, %2959, %2961, %2962)
      %2965 : bool = prim::If(%2951)
        block0():
          -> (%2952)
        block1():
          -> (%19)
      -> (%2965)
    block1():
      -> (%19)
  %bottleneck_output.106 : Tensor = prim::If(%2947) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2945)
    block1():
      %concated_features.54 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2968 : __torch__.torch.nn.modules.conv.___torch_mangle_119.Conv2d = prim::GetAttr[name="conv1"](%1887)
      %2969 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="norm1"](%1887)
      %2970 : int = aten::dim(%concated_features.54) # torch/nn/modules/batchnorm.py:276:11
      %2971 : bool = aten::ne(%2970, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2971) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2972 : bool = prim::GetAttr[name="training"](%2969)
       = prim::If(%2972) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2973 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2969)
          %2974 : Tensor = aten::add(%2973, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2969, %2974)
          -> ()
        block1():
          -> ()
      %2975 : bool = prim::GetAttr[name="training"](%2969)
      %2976 : Tensor = prim::GetAttr[name="running_mean"](%2969)
      %2977 : Tensor = prim::GetAttr[name="running_var"](%2969)
      %2978 : Tensor = prim::GetAttr[name="weight"](%2969)
      %2979 : Tensor = prim::GetAttr[name="bias"](%2969)
       = prim::If(%2975) # torch/nn/functional.py:2011:4
        block0():
          %2980 : int[] = aten::size(%concated_features.54) # torch/nn/functional.py:2012:27
          %size_prods.440 : int = aten::__getitem__(%2980, %24) # torch/nn/functional.py:1991:17
          %2982 : int = aten::len(%2980) # torch/nn/functional.py:1992:19
          %2983 : int = aten::sub(%2982, %26) # torch/nn/functional.py:1992:19
          %size_prods.441 : int = prim::Loop(%2983, %25, %size_prods.440) # torch/nn/functional.py:1992:4
            block0(%i.111 : int, %size_prods.442 : int):
              %2987 : int = aten::add(%i.111, %26) # torch/nn/functional.py:1993:27
              %2988 : int = aten::__getitem__(%2980, %2987) # torch/nn/functional.py:1993:22
              %size_prods.443 : int = aten::mul(%size_prods.442, %2988) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.443)
          %2990 : bool = aten::eq(%size_prods.441, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2990) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2991 : Tensor = aten::batch_norm(%concated_features.54, %2978, %2979, %2976, %2977, %2975, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.107 : Tensor = aten::relu_(%2991) # torch/nn/functional.py:1117:17
      %2993 : Tensor = prim::GetAttr[name="weight"](%2968)
      %2994 : Tensor? = prim::GetAttr[name="bias"](%2968)
      %2995 : int[] = prim::ListConstruct(%27, %27)
      %2996 : int[] = prim::ListConstruct(%24, %24)
      %2997 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.107 : Tensor = aten::conv2d(%result.107, %2993, %2994, %2995, %2996, %2997, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.107)
  %2999 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1887)
  %3000 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1887)
  %3001 : int = aten::dim(%bottleneck_output.106) # torch/nn/modules/batchnorm.py:276:11
  %3002 : bool = aten::ne(%3001, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3002) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3003 : bool = prim::GetAttr[name="training"](%3000)
   = prim::If(%3003) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3004 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3000)
      %3005 : Tensor = aten::add(%3004, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3000, %3005)
      -> ()
    block1():
      -> ()
  %3006 : bool = prim::GetAttr[name="training"](%3000)
  %3007 : Tensor = prim::GetAttr[name="running_mean"](%3000)
  %3008 : Tensor = prim::GetAttr[name="running_var"](%3000)
  %3009 : Tensor = prim::GetAttr[name="weight"](%3000)
  %3010 : Tensor = prim::GetAttr[name="bias"](%3000)
   = prim::If(%3006) # torch/nn/functional.py:2011:4
    block0():
      %3011 : int[] = aten::size(%bottleneck_output.106) # torch/nn/functional.py:2012:27
      %size_prods.444 : int = aten::__getitem__(%3011, %24) # torch/nn/functional.py:1991:17
      %3013 : int = aten::len(%3011) # torch/nn/functional.py:1992:19
      %3014 : int = aten::sub(%3013, %26) # torch/nn/functional.py:1992:19
      %size_prods.445 : int = prim::Loop(%3014, %25, %size_prods.444) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.446 : int):
          %3018 : int = aten::add(%i.112, %26) # torch/nn/functional.py:1993:27
          %3019 : int = aten::__getitem__(%3011, %3018) # torch/nn/functional.py:1993:22
          %size_prods.447 : int = aten::mul(%size_prods.446, %3019) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.447)
      %3021 : bool = aten::eq(%size_prods.445, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3021) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3022 : Tensor = aten::batch_norm(%bottleneck_output.106, %3009, %3010, %3007, %3008, %3006, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.108 : Tensor = aten::relu_(%3022) # torch/nn/functional.py:1117:17
  %3024 : Tensor = prim::GetAttr[name="weight"](%2999)
  %3025 : Tensor? = prim::GetAttr[name="bias"](%2999)
  %3026 : int[] = prim::ListConstruct(%27, %27)
  %3027 : int[] = prim::ListConstruct(%27, %27)
  %3028 : int[] = prim::ListConstruct(%27, %27)
  %new_features.107 : Tensor = aten::conv2d(%result.108, %3024, %3025, %3026, %3027, %3028, %27) # torch/nn/modules/conv.py:415:15
  %3030 : float = prim::GetAttr[name="drop_rate"](%1887)
  %3031 : bool = aten::gt(%3030, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.108 : Tensor = prim::If(%3031) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3033 : float = prim::GetAttr[name="drop_rate"](%1887)
      %3034 : bool = prim::GetAttr[name="training"](%1887)
      %3035 : bool = aten::lt(%3033, %16) # torch/nn/functional.py:968:7
      %3036 : bool = prim::If(%3035) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3037 : bool = aten::gt(%3033, %17) # torch/nn/functional.py:968:17
          -> (%3037)
       = prim::If(%3036) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3038 : Tensor = aten::dropout(%new_features.107, %3033, %3034) # torch/nn/functional.py:973:17
      -> (%3038)
    block1():
      -> (%new_features.107)
  %3039 : Tensor[] = aten::append(%features.4, %new_features.108) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3040 : Tensor = prim::Uninitialized()
  %3041 : bool = prim::GetAttr[name="memory_efficient"](%1888)
  %3042 : bool = prim::If(%3041) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3043 : bool = prim::Uninitialized()
      %3044 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3045 : bool = aten::gt(%3044, %24)
      %3046 : bool, %3047 : bool, %3048 : int = prim::Loop(%18, %3045, %19, %3043, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3049 : int, %3050 : bool, %3051 : bool, %3052 : int):
          %tensor.55 : Tensor = aten::__getitem__(%features.4, %3052) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3054 : bool = prim::requires_grad(%tensor.55)
          %3055 : bool, %3056 : bool = prim::If(%3054) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3043)
          %3057 : int = aten::add(%3052, %27)
          %3058 : bool = aten::lt(%3057, %3044)
          %3059 : bool = aten::__and__(%3058, %3055)
          -> (%3059, %3054, %3056, %3057)
      %3060 : bool = prim::If(%3046)
        block0():
          -> (%3047)
        block1():
          -> (%19)
      -> (%3060)
    block1():
      -> (%19)
  %bottleneck_output.108 : Tensor = prim::If(%3042) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3040)
    block1():
      %concated_features.55 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3063 : __torch__.torch.nn.modules.conv.___torch_mangle_122.Conv2d = prim::GetAttr[name="conv1"](%1888)
      %3064 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_121.BatchNorm2d = prim::GetAttr[name="norm1"](%1888)
      %3065 : int = aten::dim(%concated_features.55) # torch/nn/modules/batchnorm.py:276:11
      %3066 : bool = aten::ne(%3065, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3066) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3067 : bool = prim::GetAttr[name="training"](%3064)
       = prim::If(%3067) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3068 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3064)
          %3069 : Tensor = aten::add(%3068, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3064, %3069)
          -> ()
        block1():
          -> ()
      %3070 : bool = prim::GetAttr[name="training"](%3064)
      %3071 : Tensor = prim::GetAttr[name="running_mean"](%3064)
      %3072 : Tensor = prim::GetAttr[name="running_var"](%3064)
      %3073 : Tensor = prim::GetAttr[name="weight"](%3064)
      %3074 : Tensor = prim::GetAttr[name="bias"](%3064)
       = prim::If(%3070) # torch/nn/functional.py:2011:4
        block0():
          %3075 : int[] = aten::size(%concated_features.55) # torch/nn/functional.py:2012:27
          %size_prods.448 : int = aten::__getitem__(%3075, %24) # torch/nn/functional.py:1991:17
          %3077 : int = aten::len(%3075) # torch/nn/functional.py:1992:19
          %3078 : int = aten::sub(%3077, %26) # torch/nn/functional.py:1992:19
          %size_prods.449 : int = prim::Loop(%3078, %25, %size_prods.448) # torch/nn/functional.py:1992:4
            block0(%i.113 : int, %size_prods.450 : int):
              %3082 : int = aten::add(%i.113, %26) # torch/nn/functional.py:1993:27
              %3083 : int = aten::__getitem__(%3075, %3082) # torch/nn/functional.py:1993:22
              %size_prods.451 : int = aten::mul(%size_prods.450, %3083) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.451)
          %3085 : bool = aten::eq(%size_prods.449, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3085) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3086 : Tensor = aten::batch_norm(%concated_features.55, %3073, %3074, %3071, %3072, %3070, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.109 : Tensor = aten::relu_(%3086) # torch/nn/functional.py:1117:17
      %3088 : Tensor = prim::GetAttr[name="weight"](%3063)
      %3089 : Tensor? = prim::GetAttr[name="bias"](%3063)
      %3090 : int[] = prim::ListConstruct(%27, %27)
      %3091 : int[] = prim::ListConstruct(%24, %24)
      %3092 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.109 : Tensor = aten::conv2d(%result.109, %3088, %3089, %3090, %3091, %3092, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.109)
  %3094 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1888)
  %3095 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1888)
  %3096 : int = aten::dim(%bottleneck_output.108) # torch/nn/modules/batchnorm.py:276:11
  %3097 : bool = aten::ne(%3096, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3097) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3098 : bool = prim::GetAttr[name="training"](%3095)
   = prim::If(%3098) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3099 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3095)
      %3100 : Tensor = aten::add(%3099, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3095, %3100)
      -> ()
    block1():
      -> ()
  %3101 : bool = prim::GetAttr[name="training"](%3095)
  %3102 : Tensor = prim::GetAttr[name="running_mean"](%3095)
  %3103 : Tensor = prim::GetAttr[name="running_var"](%3095)
  %3104 : Tensor = prim::GetAttr[name="weight"](%3095)
  %3105 : Tensor = prim::GetAttr[name="bias"](%3095)
   = prim::If(%3101) # torch/nn/functional.py:2011:4
    block0():
      %3106 : int[] = aten::size(%bottleneck_output.108) # torch/nn/functional.py:2012:27
      %size_prods.452 : int = aten::__getitem__(%3106, %24) # torch/nn/functional.py:1991:17
      %3108 : int = aten::len(%3106) # torch/nn/functional.py:1992:19
      %3109 : int = aten::sub(%3108, %26) # torch/nn/functional.py:1992:19
      %size_prods.453 : int = prim::Loop(%3109, %25, %size_prods.452) # torch/nn/functional.py:1992:4
        block0(%i.114 : int, %size_prods.454 : int):
          %3113 : int = aten::add(%i.114, %26) # torch/nn/functional.py:1993:27
          %3114 : int = aten::__getitem__(%3106, %3113) # torch/nn/functional.py:1993:22
          %size_prods.455 : int = aten::mul(%size_prods.454, %3114) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.455)
      %3116 : bool = aten::eq(%size_prods.453, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3116) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3117 : Tensor = aten::batch_norm(%bottleneck_output.108, %3104, %3105, %3102, %3103, %3101, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.110 : Tensor = aten::relu_(%3117) # torch/nn/functional.py:1117:17
  %3119 : Tensor = prim::GetAttr[name="weight"](%3094)
  %3120 : Tensor? = prim::GetAttr[name="bias"](%3094)
  %3121 : int[] = prim::ListConstruct(%27, %27)
  %3122 : int[] = prim::ListConstruct(%27, %27)
  %3123 : int[] = prim::ListConstruct(%27, %27)
  %new_features.109 : Tensor = aten::conv2d(%result.110, %3119, %3120, %3121, %3122, %3123, %27) # torch/nn/modules/conv.py:415:15
  %3125 : float = prim::GetAttr[name="drop_rate"](%1888)
  %3126 : bool = aten::gt(%3125, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.110 : Tensor = prim::If(%3126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3128 : float = prim::GetAttr[name="drop_rate"](%1888)
      %3129 : bool = prim::GetAttr[name="training"](%1888)
      %3130 : bool = aten::lt(%3128, %16) # torch/nn/functional.py:968:7
      %3131 : bool = prim::If(%3130) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3132 : bool = aten::gt(%3128, %17) # torch/nn/functional.py:968:17
          -> (%3132)
       = prim::If(%3131) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3133 : Tensor = aten::dropout(%new_features.109, %3128, %3129) # torch/nn/functional.py:973:17
      -> (%3133)
    block1():
      -> (%new_features.109)
  %3134 : Tensor[] = aten::append(%features.4, %new_features.110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3135 : Tensor = prim::Uninitialized()
  %3136 : bool = prim::GetAttr[name="memory_efficient"](%1889)
  %3137 : bool = prim::If(%3136) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3138 : bool = prim::Uninitialized()
      %3139 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3140 : bool = aten::gt(%3139, %24)
      %3141 : bool, %3142 : bool, %3143 : int = prim::Loop(%18, %3140, %19, %3138, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3144 : int, %3145 : bool, %3146 : bool, %3147 : int):
          %tensor.56 : Tensor = aten::__getitem__(%features.4, %3147) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3149 : bool = prim::requires_grad(%tensor.56)
          %3150 : bool, %3151 : bool = prim::If(%3149) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3138)
          %3152 : int = aten::add(%3147, %27)
          %3153 : bool = aten::lt(%3152, %3139)
          %3154 : bool = aten::__and__(%3153, %3150)
          -> (%3154, %3149, %3151, %3152)
      %3155 : bool = prim::If(%3141)
        block0():
          -> (%3142)
        block1():
          -> (%19)
      -> (%3155)
    block1():
      -> (%19)
  %bottleneck_output.110 : Tensor = prim::If(%3137) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3135)
    block1():
      %concated_features.56 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3158 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv1"](%1889)
      %3159 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%1889)
      %3160 : int = aten::dim(%concated_features.56) # torch/nn/modules/batchnorm.py:276:11
      %3161 : bool = aten::ne(%3160, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3161) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3162 : bool = prim::GetAttr[name="training"](%3159)
       = prim::If(%3162) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3163 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3159)
          %3164 : Tensor = aten::add(%3163, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3159, %3164)
          -> ()
        block1():
          -> ()
      %3165 : bool = prim::GetAttr[name="training"](%3159)
      %3166 : Tensor = prim::GetAttr[name="running_mean"](%3159)
      %3167 : Tensor = prim::GetAttr[name="running_var"](%3159)
      %3168 : Tensor = prim::GetAttr[name="weight"](%3159)
      %3169 : Tensor = prim::GetAttr[name="bias"](%3159)
       = prim::If(%3165) # torch/nn/functional.py:2011:4
        block0():
          %3170 : int[] = aten::size(%concated_features.56) # torch/nn/functional.py:2012:27
          %size_prods.456 : int = aten::__getitem__(%3170, %24) # torch/nn/functional.py:1991:17
          %3172 : int = aten::len(%3170) # torch/nn/functional.py:1992:19
          %3173 : int = aten::sub(%3172, %26) # torch/nn/functional.py:1992:19
          %size_prods.457 : int = prim::Loop(%3173, %25, %size_prods.456) # torch/nn/functional.py:1992:4
            block0(%i.115 : int, %size_prods.458 : int):
              %3177 : int = aten::add(%i.115, %26) # torch/nn/functional.py:1993:27
              %3178 : int = aten::__getitem__(%3170, %3177) # torch/nn/functional.py:1993:22
              %size_prods.459 : int = aten::mul(%size_prods.458, %3178) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.459)
          %3180 : bool = aten::eq(%size_prods.457, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3180) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3181 : Tensor = aten::batch_norm(%concated_features.56, %3168, %3169, %3166, %3167, %3165, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.111 : Tensor = aten::relu_(%3181) # torch/nn/functional.py:1117:17
      %3183 : Tensor = prim::GetAttr[name="weight"](%3158)
      %3184 : Tensor? = prim::GetAttr[name="bias"](%3158)
      %3185 : int[] = prim::ListConstruct(%27, %27)
      %3186 : int[] = prim::ListConstruct(%24, %24)
      %3187 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.111 : Tensor = aten::conv2d(%result.111, %3183, %3184, %3185, %3186, %3187, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.111)
  %3189 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1889)
  %3190 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1889)
  %3191 : int = aten::dim(%bottleneck_output.110) # torch/nn/modules/batchnorm.py:276:11
  %3192 : bool = aten::ne(%3191, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3192) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3193 : bool = prim::GetAttr[name="training"](%3190)
   = prim::If(%3193) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3194 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3190)
      %3195 : Tensor = aten::add(%3194, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3190, %3195)
      -> ()
    block1():
      -> ()
  %3196 : bool = prim::GetAttr[name="training"](%3190)
  %3197 : Tensor = prim::GetAttr[name="running_mean"](%3190)
  %3198 : Tensor = prim::GetAttr[name="running_var"](%3190)
  %3199 : Tensor = prim::GetAttr[name="weight"](%3190)
  %3200 : Tensor = prim::GetAttr[name="bias"](%3190)
   = prim::If(%3196) # torch/nn/functional.py:2011:4
    block0():
      %3201 : int[] = aten::size(%bottleneck_output.110) # torch/nn/functional.py:2012:27
      %size_prods.460 : int = aten::__getitem__(%3201, %24) # torch/nn/functional.py:1991:17
      %3203 : int = aten::len(%3201) # torch/nn/functional.py:1992:19
      %3204 : int = aten::sub(%3203, %26) # torch/nn/functional.py:1992:19
      %size_prods.461 : int = prim::Loop(%3204, %25, %size_prods.460) # torch/nn/functional.py:1992:4
        block0(%i.116 : int, %size_prods.462 : int):
          %3208 : int = aten::add(%i.116, %26) # torch/nn/functional.py:1993:27
          %3209 : int = aten::__getitem__(%3201, %3208) # torch/nn/functional.py:1993:22
          %size_prods.463 : int = aten::mul(%size_prods.462, %3209) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.463)
      %3211 : bool = aten::eq(%size_prods.461, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3211) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3212 : Tensor = aten::batch_norm(%bottleneck_output.110, %3199, %3200, %3197, %3198, %3196, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.112 : Tensor = aten::relu_(%3212) # torch/nn/functional.py:1117:17
  %3214 : Tensor = prim::GetAttr[name="weight"](%3189)
  %3215 : Tensor? = prim::GetAttr[name="bias"](%3189)
  %3216 : int[] = prim::ListConstruct(%27, %27)
  %3217 : int[] = prim::ListConstruct(%27, %27)
  %3218 : int[] = prim::ListConstruct(%27, %27)
  %new_features.111 : Tensor = aten::conv2d(%result.112, %3214, %3215, %3216, %3217, %3218, %27) # torch/nn/modules/conv.py:415:15
  %3220 : float = prim::GetAttr[name="drop_rate"](%1889)
  %3221 : bool = aten::gt(%3220, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.112 : Tensor = prim::If(%3221) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3223 : float = prim::GetAttr[name="drop_rate"](%1889)
      %3224 : bool = prim::GetAttr[name="training"](%1889)
      %3225 : bool = aten::lt(%3223, %16) # torch/nn/functional.py:968:7
      %3226 : bool = prim::If(%3225) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3227 : bool = aten::gt(%3223, %17) # torch/nn/functional.py:968:17
          -> (%3227)
       = prim::If(%3226) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3228 : Tensor = aten::dropout(%new_features.111, %3223, %3224) # torch/nn/functional.py:973:17
      -> (%3228)
    block1():
      -> (%new_features.111)
  %3229 : Tensor[] = aten::append(%features.4, %new_features.112) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3230 : Tensor = prim::Uninitialized()
  %3231 : bool = prim::GetAttr[name="memory_efficient"](%1890)
  %3232 : bool = prim::If(%3231) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3233 : bool = prim::Uninitialized()
      %3234 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3235 : bool = aten::gt(%3234, %24)
      %3236 : bool, %3237 : bool, %3238 : int = prim::Loop(%18, %3235, %19, %3233, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3239 : int, %3240 : bool, %3241 : bool, %3242 : int):
          %tensor.57 : Tensor = aten::__getitem__(%features.4, %3242) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3244 : bool = prim::requires_grad(%tensor.57)
          %3245 : bool, %3246 : bool = prim::If(%3244) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3233)
          %3247 : int = aten::add(%3242, %27)
          %3248 : bool = aten::lt(%3247, %3234)
          %3249 : bool = aten::__and__(%3248, %3245)
          -> (%3249, %3244, %3246, %3247)
      %3250 : bool = prim::If(%3236)
        block0():
          -> (%3237)
        block1():
          -> (%19)
      -> (%3250)
    block1():
      -> (%19)
  %bottleneck_output.112 : Tensor = prim::If(%3232) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3230)
    block1():
      %concated_features.57 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3253 : __torch__.torch.nn.modules.conv.___torch_mangle_128.Conv2d = prim::GetAttr[name="conv1"](%1890)
      %3254 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="norm1"](%1890)
      %3255 : int = aten::dim(%concated_features.57) # torch/nn/modules/batchnorm.py:276:11
      %3256 : bool = aten::ne(%3255, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3256) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3257 : bool = prim::GetAttr[name="training"](%3254)
       = prim::If(%3257) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3258 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3254)
          %3259 : Tensor = aten::add(%3258, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3254, %3259)
          -> ()
        block1():
          -> ()
      %3260 : bool = prim::GetAttr[name="training"](%3254)
      %3261 : Tensor = prim::GetAttr[name="running_mean"](%3254)
      %3262 : Tensor = prim::GetAttr[name="running_var"](%3254)
      %3263 : Tensor = prim::GetAttr[name="weight"](%3254)
      %3264 : Tensor = prim::GetAttr[name="bias"](%3254)
       = prim::If(%3260) # torch/nn/functional.py:2011:4
        block0():
          %3265 : int[] = aten::size(%concated_features.57) # torch/nn/functional.py:2012:27
          %size_prods.464 : int = aten::__getitem__(%3265, %24) # torch/nn/functional.py:1991:17
          %3267 : int = aten::len(%3265) # torch/nn/functional.py:1992:19
          %3268 : int = aten::sub(%3267, %26) # torch/nn/functional.py:1992:19
          %size_prods.465 : int = prim::Loop(%3268, %25, %size_prods.464) # torch/nn/functional.py:1992:4
            block0(%i.117 : int, %size_prods.466 : int):
              %3272 : int = aten::add(%i.117, %26) # torch/nn/functional.py:1993:27
              %3273 : int = aten::__getitem__(%3265, %3272) # torch/nn/functional.py:1993:22
              %size_prods.467 : int = aten::mul(%size_prods.466, %3273) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.467)
          %3275 : bool = aten::eq(%size_prods.465, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3275) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3276 : Tensor = aten::batch_norm(%concated_features.57, %3263, %3264, %3261, %3262, %3260, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.113 : Tensor = aten::relu_(%3276) # torch/nn/functional.py:1117:17
      %3278 : Tensor = prim::GetAttr[name="weight"](%3253)
      %3279 : Tensor? = prim::GetAttr[name="bias"](%3253)
      %3280 : int[] = prim::ListConstruct(%27, %27)
      %3281 : int[] = prim::ListConstruct(%24, %24)
      %3282 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.113 : Tensor = aten::conv2d(%result.113, %3278, %3279, %3280, %3281, %3282, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.113)
  %3284 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1890)
  %3285 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1890)
  %3286 : int = aten::dim(%bottleneck_output.112) # torch/nn/modules/batchnorm.py:276:11
  %3287 : bool = aten::ne(%3286, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3287) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3288 : bool = prim::GetAttr[name="training"](%3285)
   = prim::If(%3288) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3289 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3285)
      %3290 : Tensor = aten::add(%3289, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3285, %3290)
      -> ()
    block1():
      -> ()
  %3291 : bool = prim::GetAttr[name="training"](%3285)
  %3292 : Tensor = prim::GetAttr[name="running_mean"](%3285)
  %3293 : Tensor = prim::GetAttr[name="running_var"](%3285)
  %3294 : Tensor = prim::GetAttr[name="weight"](%3285)
  %3295 : Tensor = prim::GetAttr[name="bias"](%3285)
   = prim::If(%3291) # torch/nn/functional.py:2011:4
    block0():
      %3296 : int[] = aten::size(%bottleneck_output.112) # torch/nn/functional.py:2012:27
      %size_prods.468 : int = aten::__getitem__(%3296, %24) # torch/nn/functional.py:1991:17
      %3298 : int = aten::len(%3296) # torch/nn/functional.py:1992:19
      %3299 : int = aten::sub(%3298, %26) # torch/nn/functional.py:1992:19
      %size_prods.469 : int = prim::Loop(%3299, %25, %size_prods.468) # torch/nn/functional.py:1992:4
        block0(%i.118 : int, %size_prods.470 : int):
          %3303 : int = aten::add(%i.118, %26) # torch/nn/functional.py:1993:27
          %3304 : int = aten::__getitem__(%3296, %3303) # torch/nn/functional.py:1993:22
          %size_prods.471 : int = aten::mul(%size_prods.470, %3304) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.471)
      %3306 : bool = aten::eq(%size_prods.469, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3306) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3307 : Tensor = aten::batch_norm(%bottleneck_output.112, %3294, %3295, %3292, %3293, %3291, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.114 : Tensor = aten::relu_(%3307) # torch/nn/functional.py:1117:17
  %3309 : Tensor = prim::GetAttr[name="weight"](%3284)
  %3310 : Tensor? = prim::GetAttr[name="bias"](%3284)
  %3311 : int[] = prim::ListConstruct(%27, %27)
  %3312 : int[] = prim::ListConstruct(%27, %27)
  %3313 : int[] = prim::ListConstruct(%27, %27)
  %new_features.113 : Tensor = aten::conv2d(%result.114, %3309, %3310, %3311, %3312, %3313, %27) # torch/nn/modules/conv.py:415:15
  %3315 : float = prim::GetAttr[name="drop_rate"](%1890)
  %3316 : bool = aten::gt(%3315, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.114 : Tensor = prim::If(%3316) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3318 : float = prim::GetAttr[name="drop_rate"](%1890)
      %3319 : bool = prim::GetAttr[name="training"](%1890)
      %3320 : bool = aten::lt(%3318, %16) # torch/nn/functional.py:968:7
      %3321 : bool = prim::If(%3320) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3322 : bool = aten::gt(%3318, %17) # torch/nn/functional.py:968:17
          -> (%3322)
       = prim::If(%3321) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3323 : Tensor = aten::dropout(%new_features.113, %3318, %3319) # torch/nn/functional.py:973:17
      -> (%3323)
    block1():
      -> (%new_features.113)
  %3324 : Tensor[] = aten::append(%features.4, %new_features.114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3325 : Tensor = prim::Uninitialized()
  %3326 : bool = prim::GetAttr[name="memory_efficient"](%1891)
  %3327 : bool = prim::If(%3326) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3328 : bool = prim::Uninitialized()
      %3329 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3330 : bool = aten::gt(%3329, %24)
      %3331 : bool, %3332 : bool, %3333 : int = prim::Loop(%18, %3330, %19, %3328, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3334 : int, %3335 : bool, %3336 : bool, %3337 : int):
          %tensor.17 : Tensor = aten::__getitem__(%features.4, %3337) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3339 : bool = prim::requires_grad(%tensor.17)
          %3340 : bool, %3341 : bool = prim::If(%3339) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3328)
          %3342 : int = aten::add(%3337, %27)
          %3343 : bool = aten::lt(%3342, %3329)
          %3344 : bool = aten::__and__(%3343, %3340)
          -> (%3344, %3339, %3341, %3342)
      %3345 : bool = prim::If(%3331)
        block0():
          -> (%3332)
        block1():
          -> (%19)
      -> (%3345)
    block1():
      -> (%19)
  %bottleneck_output.32 : Tensor = prim::If(%3327) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3325)
    block1():
      %concated_features.17 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3348 : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d = prim::GetAttr[name="conv1"](%1891)
      %3349 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_130.BatchNorm2d = prim::GetAttr[name="norm1"](%1891)
      %3350 : int = aten::dim(%concated_features.17) # torch/nn/modules/batchnorm.py:276:11
      %3351 : bool = aten::ne(%3350, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3351) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3352 : bool = prim::GetAttr[name="training"](%3349)
       = prim::If(%3352) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3353 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3349)
          %3354 : Tensor = aten::add(%3353, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3349, %3354)
          -> ()
        block1():
          -> ()
      %3355 : bool = prim::GetAttr[name="training"](%3349)
      %3356 : Tensor = prim::GetAttr[name="running_mean"](%3349)
      %3357 : Tensor = prim::GetAttr[name="running_var"](%3349)
      %3358 : Tensor = prim::GetAttr[name="weight"](%3349)
      %3359 : Tensor = prim::GetAttr[name="bias"](%3349)
       = prim::If(%3355) # torch/nn/functional.py:2011:4
        block0():
          %3360 : int[] = aten::size(%concated_features.17) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%3360, %24) # torch/nn/functional.py:1991:17
          %3362 : int = aten::len(%3360) # torch/nn/functional.py:1992:19
          %3363 : int = aten::sub(%3362, %26) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%3363, %25, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %3367 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
              %3368 : int = aten::__getitem__(%3360, %3367) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %3368) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.131)
          %3370 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3370) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3371 : Tensor = aten::batch_norm(%concated_features.17, %3358, %3359, %3356, %3357, %3355, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.33 : Tensor = aten::relu_(%3371) # torch/nn/functional.py:1117:17
      %3373 : Tensor = prim::GetAttr[name="weight"](%3348)
      %3374 : Tensor? = prim::GetAttr[name="bias"](%3348)
      %3375 : int[] = prim::ListConstruct(%27, %27)
      %3376 : int[] = prim::ListConstruct(%24, %24)
      %3377 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.33 : Tensor = aten::conv2d(%result.33, %3373, %3374, %3375, %3376, %3377, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.33)
  %3379 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1891)
  %3380 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1891)
  %3381 : int = aten::dim(%bottleneck_output.32) # torch/nn/modules/batchnorm.py:276:11
  %3382 : bool = aten::ne(%3381, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3382) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3383 : bool = prim::GetAttr[name="training"](%3380)
   = prim::If(%3383) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3384 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3380)
      %3385 : Tensor = aten::add(%3384, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3380, %3385)
      -> ()
    block1():
      -> ()
  %3386 : bool = prim::GetAttr[name="training"](%3380)
  %3387 : Tensor = prim::GetAttr[name="running_mean"](%3380)
  %3388 : Tensor = prim::GetAttr[name="running_var"](%3380)
  %3389 : Tensor = prim::GetAttr[name="weight"](%3380)
  %3390 : Tensor = prim::GetAttr[name="bias"](%3380)
   = prim::If(%3386) # torch/nn/functional.py:2011:4
    block0():
      %3391 : int[] = aten::size(%bottleneck_output.32) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%3391, %24) # torch/nn/functional.py:1991:17
      %3393 : int = aten::len(%3391) # torch/nn/functional.py:1992:19
      %3394 : int = aten::sub(%3393, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%3394, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %3398 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %3399 : int = aten::__getitem__(%3391, %3398) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %3399) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %3401 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3401) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3402 : Tensor = aten::batch_norm(%bottleneck_output.32, %3389, %3390, %3387, %3388, %3386, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.34 : Tensor = aten::relu_(%3402) # torch/nn/functional.py:1117:17
  %3404 : Tensor = prim::GetAttr[name="weight"](%3379)
  %3405 : Tensor? = prim::GetAttr[name="bias"](%3379)
  %3406 : int[] = prim::ListConstruct(%27, %27)
  %3407 : int[] = prim::ListConstruct(%27, %27)
  %3408 : int[] = prim::ListConstruct(%27, %27)
  %new_features.34 : Tensor = aten::conv2d(%result.34, %3404, %3405, %3406, %3407, %3408, %27) # torch/nn/modules/conv.py:415:15
  %3410 : float = prim::GetAttr[name="drop_rate"](%1891)
  %3411 : bool = aten::gt(%3410, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.116 : Tensor = prim::If(%3411) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3413 : float = prim::GetAttr[name="drop_rate"](%1891)
      %3414 : bool = prim::GetAttr[name="training"](%1891)
      %3415 : bool = aten::lt(%3413, %16) # torch/nn/functional.py:968:7
      %3416 : bool = prim::If(%3415) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3417 : bool = aten::gt(%3413, %17) # torch/nn/functional.py:968:17
          -> (%3417)
       = prim::If(%3416) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3418 : Tensor = aten::dropout(%new_features.34, %3413, %3414) # torch/nn/functional.py:973:17
      -> (%3418)
    block1():
      -> (%new_features.34)
  %3419 : Tensor[] = aten::append(%features.4, %new_features.116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3420 : Tensor = prim::Uninitialized()
  %3421 : bool = prim::GetAttr[name="memory_efficient"](%1892)
  %3422 : bool = prim::If(%3421) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3423 : bool = prim::Uninitialized()
      %3424 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3425 : bool = aten::gt(%3424, %24)
      %3426 : bool, %3427 : bool, %3428 : int = prim::Loop(%18, %3425, %19, %3423, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3429 : int, %3430 : bool, %3431 : bool, %3432 : int):
          %tensor.18 : Tensor = aten::__getitem__(%features.4, %3432) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3434 : bool = prim::requires_grad(%tensor.18)
          %3435 : bool, %3436 : bool = prim::If(%3434) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3423)
          %3437 : int = aten::add(%3432, %27)
          %3438 : bool = aten::lt(%3437, %3424)
          %3439 : bool = aten::__and__(%3438, %3435)
          -> (%3439, %3434, %3436, %3437)
      %3440 : bool = prim::If(%3426)
        block0():
          -> (%3427)
        block1():
          -> (%19)
      -> (%3440)
    block1():
      -> (%19)
  %bottleneck_output.34 : Tensor = prim::If(%3422) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3420)
    block1():
      %concated_features.18 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3443 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%1892)
      %3444 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%1892)
      %3445 : int = aten::dim(%concated_features.18) # torch/nn/modules/batchnorm.py:276:11
      %3446 : bool = aten::ne(%3445, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3446) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3447 : bool = prim::GetAttr[name="training"](%3444)
       = prim::If(%3447) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3448 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3444)
          %3449 : Tensor = aten::add(%3448, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3444, %3449)
          -> ()
        block1():
          -> ()
      %3450 : bool = prim::GetAttr[name="training"](%3444)
      %3451 : Tensor = prim::GetAttr[name="running_mean"](%3444)
      %3452 : Tensor = prim::GetAttr[name="running_var"](%3444)
      %3453 : Tensor = prim::GetAttr[name="weight"](%3444)
      %3454 : Tensor = prim::GetAttr[name="bias"](%3444)
       = prim::If(%3450) # torch/nn/functional.py:2011:4
        block0():
          %3455 : int[] = aten::size(%concated_features.18) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%3455, %24) # torch/nn/functional.py:1991:17
          %3457 : int = aten::len(%3455) # torch/nn/functional.py:1992:19
          %3458 : int = aten::sub(%3457, %26) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%3458, %25, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %3462 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
              %3463 : int = aten::__getitem__(%3455, %3462) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %3463) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.139)
          %3465 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3465) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3466 : Tensor = aten::batch_norm(%concated_features.18, %3453, %3454, %3451, %3452, %3450, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.35 : Tensor = aten::relu_(%3466) # torch/nn/functional.py:1117:17
      %3468 : Tensor = prim::GetAttr[name="weight"](%3443)
      %3469 : Tensor? = prim::GetAttr[name="bias"](%3443)
      %3470 : int[] = prim::ListConstruct(%27, %27)
      %3471 : int[] = prim::ListConstruct(%24, %24)
      %3472 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.35 : Tensor = aten::conv2d(%result.35, %3468, %3469, %3470, %3471, %3472, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.35)
  %3474 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1892)
  %3475 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1892)
  %3476 : int = aten::dim(%bottleneck_output.34) # torch/nn/modules/batchnorm.py:276:11
  %3477 : bool = aten::ne(%3476, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3477) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3478 : bool = prim::GetAttr[name="training"](%3475)
   = prim::If(%3478) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3479 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3475)
      %3480 : Tensor = aten::add(%3479, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3475, %3480)
      -> ()
    block1():
      -> ()
  %3481 : bool = prim::GetAttr[name="training"](%3475)
  %3482 : Tensor = prim::GetAttr[name="running_mean"](%3475)
  %3483 : Tensor = prim::GetAttr[name="running_var"](%3475)
  %3484 : Tensor = prim::GetAttr[name="weight"](%3475)
  %3485 : Tensor = prim::GetAttr[name="bias"](%3475)
   = prim::If(%3481) # torch/nn/functional.py:2011:4
    block0():
      %3486 : int[] = aten::size(%bottleneck_output.34) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%3486, %24) # torch/nn/functional.py:1991:17
      %3488 : int = aten::len(%3486) # torch/nn/functional.py:1992:19
      %3489 : int = aten::sub(%3488, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%3489, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %3493 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %3494 : int = aten::__getitem__(%3486, %3493) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %3494) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %3496 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3496) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3497 : Tensor = aten::batch_norm(%bottleneck_output.34, %3484, %3485, %3482, %3483, %3481, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.36 : Tensor = aten::relu_(%3497) # torch/nn/functional.py:1117:17
  %3499 : Tensor = prim::GetAttr[name="weight"](%3474)
  %3500 : Tensor? = prim::GetAttr[name="bias"](%3474)
  %3501 : int[] = prim::ListConstruct(%27, %27)
  %3502 : int[] = prim::ListConstruct(%27, %27)
  %3503 : int[] = prim::ListConstruct(%27, %27)
  %new_features.36 : Tensor = aten::conv2d(%result.36, %3499, %3500, %3501, %3502, %3503, %27) # torch/nn/modules/conv.py:415:15
  %3505 : float = prim::GetAttr[name="drop_rate"](%1892)
  %3506 : bool = aten::gt(%3505, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.33 : Tensor = prim::If(%3506) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3508 : float = prim::GetAttr[name="drop_rate"](%1892)
      %3509 : bool = prim::GetAttr[name="training"](%1892)
      %3510 : bool = aten::lt(%3508, %16) # torch/nn/functional.py:968:7
      %3511 : bool = prim::If(%3510) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3512 : bool = aten::gt(%3508, %17) # torch/nn/functional.py:968:17
          -> (%3512)
       = prim::If(%3511) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3513 : Tensor = aten::dropout(%new_features.36, %3508, %3509) # torch/nn/functional.py:973:17
      -> (%3513)
    block1():
      -> (%new_features.36)
  %3514 : Tensor[] = aten::append(%features.4, %new_features.33) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3515 : Tensor = prim::Uninitialized()
  %3516 : bool = prim::GetAttr[name="memory_efficient"](%1893)
  %3517 : bool = prim::If(%3516) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3518 : bool = prim::Uninitialized()
      %3519 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3520 : bool = aten::gt(%3519, %24)
      %3521 : bool, %3522 : bool, %3523 : int = prim::Loop(%18, %3520, %19, %3518, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3524 : int, %3525 : bool, %3526 : bool, %3527 : int):
          %tensor.19 : Tensor = aten::__getitem__(%features.4, %3527) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3529 : bool = prim::requires_grad(%tensor.19)
          %3530 : bool, %3531 : bool = prim::If(%3529) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3518)
          %3532 : int = aten::add(%3527, %27)
          %3533 : bool = aten::lt(%3532, %3519)
          %3534 : bool = aten::__and__(%3533, %3530)
          -> (%3534, %3529, %3531, %3532)
      %3535 : bool = prim::If(%3521)
        block0():
          -> (%3522)
        block1():
          -> (%19)
      -> (%3535)
    block1():
      -> (%19)
  %bottleneck_output.36 : Tensor = prim::If(%3517) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3515)
    block1():
      %concated_features.19 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3538 : __torch__.torch.nn.modules.conv.___torch_mangle_137.Conv2d = prim::GetAttr[name="conv1"](%1893)
      %3539 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_136.BatchNorm2d = prim::GetAttr[name="norm1"](%1893)
      %3540 : int = aten::dim(%concated_features.19) # torch/nn/modules/batchnorm.py:276:11
      %3541 : bool = aten::ne(%3540, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3541) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3542 : bool = prim::GetAttr[name="training"](%3539)
       = prim::If(%3542) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3543 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3539)
          %3544 : Tensor = aten::add(%3543, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3539, %3544)
          -> ()
        block1():
          -> ()
      %3545 : bool = prim::GetAttr[name="training"](%3539)
      %3546 : Tensor = prim::GetAttr[name="running_mean"](%3539)
      %3547 : Tensor = prim::GetAttr[name="running_var"](%3539)
      %3548 : Tensor = prim::GetAttr[name="weight"](%3539)
      %3549 : Tensor = prim::GetAttr[name="bias"](%3539)
       = prim::If(%3545) # torch/nn/functional.py:2011:4
        block0():
          %3550 : int[] = aten::size(%concated_features.19) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%3550, %24) # torch/nn/functional.py:1991:17
          %3552 : int = aten::len(%3550) # torch/nn/functional.py:1992:19
          %3553 : int = aten::sub(%3552, %26) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%3553, %25, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %3557 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
              %3558 : int = aten::__getitem__(%3550, %3557) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %3558) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.147)
          %3560 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3560) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3561 : Tensor = aten::batch_norm(%concated_features.19, %3548, %3549, %3546, %3547, %3545, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.37 : Tensor = aten::relu_(%3561) # torch/nn/functional.py:1117:17
      %3563 : Tensor = prim::GetAttr[name="weight"](%3538)
      %3564 : Tensor? = prim::GetAttr[name="bias"](%3538)
      %3565 : int[] = prim::ListConstruct(%27, %27)
      %3566 : int[] = prim::ListConstruct(%24, %24)
      %3567 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.37 : Tensor = aten::conv2d(%result.37, %3563, %3564, %3565, %3566, %3567, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.37)
  %3569 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1893)
  %3570 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1893)
  %3571 : int = aten::dim(%bottleneck_output.36) # torch/nn/modules/batchnorm.py:276:11
  %3572 : bool = aten::ne(%3571, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3572) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3573 : bool = prim::GetAttr[name="training"](%3570)
   = prim::If(%3573) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3574 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3570)
      %3575 : Tensor = aten::add(%3574, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3570, %3575)
      -> ()
    block1():
      -> ()
  %3576 : bool = prim::GetAttr[name="training"](%3570)
  %3577 : Tensor = prim::GetAttr[name="running_mean"](%3570)
  %3578 : Tensor = prim::GetAttr[name="running_var"](%3570)
  %3579 : Tensor = prim::GetAttr[name="weight"](%3570)
  %3580 : Tensor = prim::GetAttr[name="bias"](%3570)
   = prim::If(%3576) # torch/nn/functional.py:2011:4
    block0():
      %3581 : int[] = aten::size(%bottleneck_output.36) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%3581, %24) # torch/nn/functional.py:1991:17
      %3583 : int = aten::len(%3581) # torch/nn/functional.py:1992:19
      %3584 : int = aten::sub(%3583, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%3584, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %3588 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %3589 : int = aten::__getitem__(%3581, %3588) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %3589) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %3591 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3591) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3592 : Tensor = aten::batch_norm(%bottleneck_output.36, %3579, %3580, %3577, %3578, %3576, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.38 : Tensor = aten::relu_(%3592) # torch/nn/functional.py:1117:17
  %3594 : Tensor = prim::GetAttr[name="weight"](%3569)
  %3595 : Tensor? = prim::GetAttr[name="bias"](%3569)
  %3596 : int[] = prim::ListConstruct(%27, %27)
  %3597 : int[] = prim::ListConstruct(%27, %27)
  %3598 : int[] = prim::ListConstruct(%27, %27)
  %new_features.38 : Tensor = aten::conv2d(%result.38, %3594, %3595, %3596, %3597, %3598, %27) # torch/nn/modules/conv.py:415:15
  %3600 : float = prim::GetAttr[name="drop_rate"](%1893)
  %3601 : bool = aten::gt(%3600, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.35 : Tensor = prim::If(%3601) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3603 : float = prim::GetAttr[name="drop_rate"](%1893)
      %3604 : bool = prim::GetAttr[name="training"](%1893)
      %3605 : bool = aten::lt(%3603, %16) # torch/nn/functional.py:968:7
      %3606 : bool = prim::If(%3605) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3607 : bool = aten::gt(%3603, %17) # torch/nn/functional.py:968:17
          -> (%3607)
       = prim::If(%3606) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3608 : Tensor = aten::dropout(%new_features.38, %3603, %3604) # torch/nn/functional.py:973:17
      -> (%3608)
    block1():
      -> (%new_features.38)
  %3609 : Tensor[] = aten::append(%features.4, %new_features.35) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3610 : Tensor = prim::Uninitialized()
  %3611 : bool = prim::GetAttr[name="memory_efficient"](%1894)
  %3612 : bool = prim::If(%3611) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3613 : bool = prim::Uninitialized()
      %3614 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3615 : bool = aten::gt(%3614, %24)
      %3616 : bool, %3617 : bool, %3618 : int = prim::Loop(%18, %3615, %19, %3613, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3619 : int, %3620 : bool, %3621 : bool, %3622 : int):
          %tensor.20 : Tensor = aten::__getitem__(%features.4, %3622) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3624 : bool = prim::requires_grad(%tensor.20)
          %3625 : bool, %3626 : bool = prim::If(%3624) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3613)
          %3627 : int = aten::add(%3622, %27)
          %3628 : bool = aten::lt(%3627, %3614)
          %3629 : bool = aten::__and__(%3628, %3625)
          -> (%3629, %3624, %3626, %3627)
      %3630 : bool = prim::If(%3616)
        block0():
          -> (%3617)
        block1():
          -> (%19)
      -> (%3630)
    block1():
      -> (%19)
  %bottleneck_output.38 : Tensor = prim::If(%3612) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3610)
    block1():
      %concated_features.20 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3633 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv1"](%1894)
      %3634 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="norm1"](%1894)
      %3635 : int = aten::dim(%concated_features.20) # torch/nn/modules/batchnorm.py:276:11
      %3636 : bool = aten::ne(%3635, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3636) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3637 : bool = prim::GetAttr[name="training"](%3634)
       = prim::If(%3637) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3638 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3634)
          %3639 : Tensor = aten::add(%3638, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3634, %3639)
          -> ()
        block1():
          -> ()
      %3640 : bool = prim::GetAttr[name="training"](%3634)
      %3641 : Tensor = prim::GetAttr[name="running_mean"](%3634)
      %3642 : Tensor = prim::GetAttr[name="running_var"](%3634)
      %3643 : Tensor = prim::GetAttr[name="weight"](%3634)
      %3644 : Tensor = prim::GetAttr[name="bias"](%3634)
       = prim::If(%3640) # torch/nn/functional.py:2011:4
        block0():
          %3645 : int[] = aten::size(%concated_features.20) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%3645, %24) # torch/nn/functional.py:1991:17
          %3647 : int = aten::len(%3645) # torch/nn/functional.py:1992:19
          %3648 : int = aten::sub(%3647, %26) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%3648, %25, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %3652 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
              %3653 : int = aten::__getitem__(%3645, %3652) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %3653) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.155)
          %3655 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3655) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3656 : Tensor = aten::batch_norm(%concated_features.20, %3643, %3644, %3641, %3642, %3640, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.39 : Tensor = aten::relu_(%3656) # torch/nn/functional.py:1117:17
      %3658 : Tensor = prim::GetAttr[name="weight"](%3633)
      %3659 : Tensor? = prim::GetAttr[name="bias"](%3633)
      %3660 : int[] = prim::ListConstruct(%27, %27)
      %3661 : int[] = prim::ListConstruct(%24, %24)
      %3662 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.39 : Tensor = aten::conv2d(%result.39, %3658, %3659, %3660, %3661, %3662, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.39)
  %3664 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1894)
  %3665 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1894)
  %3666 : int = aten::dim(%bottleneck_output.38) # torch/nn/modules/batchnorm.py:276:11
  %3667 : bool = aten::ne(%3666, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3667) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3668 : bool = prim::GetAttr[name="training"](%3665)
   = prim::If(%3668) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3669 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3665)
      %3670 : Tensor = aten::add(%3669, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3665, %3670)
      -> ()
    block1():
      -> ()
  %3671 : bool = prim::GetAttr[name="training"](%3665)
  %3672 : Tensor = prim::GetAttr[name="running_mean"](%3665)
  %3673 : Tensor = prim::GetAttr[name="running_var"](%3665)
  %3674 : Tensor = prim::GetAttr[name="weight"](%3665)
  %3675 : Tensor = prim::GetAttr[name="bias"](%3665)
   = prim::If(%3671) # torch/nn/functional.py:2011:4
    block0():
      %3676 : int[] = aten::size(%bottleneck_output.38) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%3676, %24) # torch/nn/functional.py:1991:17
      %3678 : int = aten::len(%3676) # torch/nn/functional.py:1992:19
      %3679 : int = aten::sub(%3678, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%3679, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %3683 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %3684 : int = aten::__getitem__(%3676, %3683) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %3684) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %3686 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3686) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3687 : Tensor = aten::batch_norm(%bottleneck_output.38, %3674, %3675, %3672, %3673, %3671, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.40 : Tensor = aten::relu_(%3687) # torch/nn/functional.py:1117:17
  %3689 : Tensor = prim::GetAttr[name="weight"](%3664)
  %3690 : Tensor? = prim::GetAttr[name="bias"](%3664)
  %3691 : int[] = prim::ListConstruct(%27, %27)
  %3692 : int[] = prim::ListConstruct(%27, %27)
  %3693 : int[] = prim::ListConstruct(%27, %27)
  %new_features.40 : Tensor = aten::conv2d(%result.40, %3689, %3690, %3691, %3692, %3693, %27) # torch/nn/modules/conv.py:415:15
  %3695 : float = prim::GetAttr[name="drop_rate"](%1894)
  %3696 : bool = aten::gt(%3695, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.37 : Tensor = prim::If(%3696) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3698 : float = prim::GetAttr[name="drop_rate"](%1894)
      %3699 : bool = prim::GetAttr[name="training"](%1894)
      %3700 : bool = aten::lt(%3698, %16) # torch/nn/functional.py:968:7
      %3701 : bool = prim::If(%3700) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3702 : bool = aten::gt(%3698, %17) # torch/nn/functional.py:968:17
          -> (%3702)
       = prim::If(%3701) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3703 : Tensor = aten::dropout(%new_features.40, %3698, %3699) # torch/nn/functional.py:973:17
      -> (%3703)
    block1():
      -> (%new_features.40)
  %3704 : Tensor[] = aten::append(%features.4, %new_features.37) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3705 : Tensor = prim::Uninitialized()
  %3706 : bool = prim::GetAttr[name="memory_efficient"](%1895)
  %3707 : bool = prim::If(%3706) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3708 : bool = prim::Uninitialized()
      %3709 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3710 : bool = aten::gt(%3709, %24)
      %3711 : bool, %3712 : bool, %3713 : int = prim::Loop(%18, %3710, %19, %3708, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3714 : int, %3715 : bool, %3716 : bool, %3717 : int):
          %tensor.21 : Tensor = aten::__getitem__(%features.4, %3717) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3719 : bool = prim::requires_grad(%tensor.21)
          %3720 : bool, %3721 : bool = prim::If(%3719) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3708)
          %3722 : int = aten::add(%3717, %27)
          %3723 : bool = aten::lt(%3722, %3709)
          %3724 : bool = aten::__and__(%3723, %3720)
          -> (%3724, %3719, %3721, %3722)
      %3725 : bool = prim::If(%3711)
        block0():
          -> (%3712)
        block1():
          -> (%19)
      -> (%3725)
    block1():
      -> (%19)
  %bottleneck_output.40 : Tensor = prim::If(%3707) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3705)
    block1():
      %concated_features.21 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3728 : __torch__.torch.nn.modules.conv.___torch_mangle_143.Conv2d = prim::GetAttr[name="conv1"](%1895)
      %3729 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%1895)
      %3730 : int = aten::dim(%concated_features.21) # torch/nn/modules/batchnorm.py:276:11
      %3731 : bool = aten::ne(%3730, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3731) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3732 : bool = prim::GetAttr[name="training"](%3729)
       = prim::If(%3732) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3733 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3729)
          %3734 : Tensor = aten::add(%3733, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3729, %3734)
          -> ()
        block1():
          -> ()
      %3735 : bool = prim::GetAttr[name="training"](%3729)
      %3736 : Tensor = prim::GetAttr[name="running_mean"](%3729)
      %3737 : Tensor = prim::GetAttr[name="running_var"](%3729)
      %3738 : Tensor = prim::GetAttr[name="weight"](%3729)
      %3739 : Tensor = prim::GetAttr[name="bias"](%3729)
       = prim::If(%3735) # torch/nn/functional.py:2011:4
        block0():
          %3740 : int[] = aten::size(%concated_features.21) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%3740, %24) # torch/nn/functional.py:1991:17
          %3742 : int = aten::len(%3740) # torch/nn/functional.py:1992:19
          %3743 : int = aten::sub(%3742, %26) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%3743, %25, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %3747 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
              %3748 : int = aten::__getitem__(%3740, %3747) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %3748) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.163)
          %3750 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3750) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3751 : Tensor = aten::batch_norm(%concated_features.21, %3738, %3739, %3736, %3737, %3735, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.41 : Tensor = aten::relu_(%3751) # torch/nn/functional.py:1117:17
      %3753 : Tensor = prim::GetAttr[name="weight"](%3728)
      %3754 : Tensor? = prim::GetAttr[name="bias"](%3728)
      %3755 : int[] = prim::ListConstruct(%27, %27)
      %3756 : int[] = prim::ListConstruct(%24, %24)
      %3757 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.41 : Tensor = aten::conv2d(%result.41, %3753, %3754, %3755, %3756, %3757, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.41)
  %3759 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1895)
  %3760 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1895)
  %3761 : int = aten::dim(%bottleneck_output.40) # torch/nn/modules/batchnorm.py:276:11
  %3762 : bool = aten::ne(%3761, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3762) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3763 : bool = prim::GetAttr[name="training"](%3760)
   = prim::If(%3763) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3764 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3760)
      %3765 : Tensor = aten::add(%3764, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3760, %3765)
      -> ()
    block1():
      -> ()
  %3766 : bool = prim::GetAttr[name="training"](%3760)
  %3767 : Tensor = prim::GetAttr[name="running_mean"](%3760)
  %3768 : Tensor = prim::GetAttr[name="running_var"](%3760)
  %3769 : Tensor = prim::GetAttr[name="weight"](%3760)
  %3770 : Tensor = prim::GetAttr[name="bias"](%3760)
   = prim::If(%3766) # torch/nn/functional.py:2011:4
    block0():
      %3771 : int[] = aten::size(%bottleneck_output.40) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%3771, %24) # torch/nn/functional.py:1991:17
      %3773 : int = aten::len(%3771) # torch/nn/functional.py:1992:19
      %3774 : int = aten::sub(%3773, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%3774, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %3778 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %3779 : int = aten::__getitem__(%3771, %3778) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %3779) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %3781 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3781) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3782 : Tensor = aten::batch_norm(%bottleneck_output.40, %3769, %3770, %3767, %3768, %3766, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.42 : Tensor = aten::relu_(%3782) # torch/nn/functional.py:1117:17
  %3784 : Tensor = prim::GetAttr[name="weight"](%3759)
  %3785 : Tensor? = prim::GetAttr[name="bias"](%3759)
  %3786 : int[] = prim::ListConstruct(%27, %27)
  %3787 : int[] = prim::ListConstruct(%27, %27)
  %3788 : int[] = prim::ListConstruct(%27, %27)
  %new_features.42 : Tensor = aten::conv2d(%result.42, %3784, %3785, %3786, %3787, %3788, %27) # torch/nn/modules/conv.py:415:15
  %3790 : float = prim::GetAttr[name="drop_rate"](%1895)
  %3791 : bool = aten::gt(%3790, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.39 : Tensor = prim::If(%3791) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3793 : float = prim::GetAttr[name="drop_rate"](%1895)
      %3794 : bool = prim::GetAttr[name="training"](%1895)
      %3795 : bool = aten::lt(%3793, %16) # torch/nn/functional.py:968:7
      %3796 : bool = prim::If(%3795) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3797 : bool = aten::gt(%3793, %17) # torch/nn/functional.py:968:17
          -> (%3797)
       = prim::If(%3796) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3798 : Tensor = aten::dropout(%new_features.42, %3793, %3794) # torch/nn/functional.py:973:17
      -> (%3798)
    block1():
      -> (%new_features.42)
  %3799 : Tensor[] = aten::append(%features.4, %new_features.39) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3800 : Tensor = prim::Uninitialized()
  %3801 : bool = prim::GetAttr[name="memory_efficient"](%1896)
  %3802 : bool = prim::If(%3801) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3803 : bool = prim::Uninitialized()
      %3804 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3805 : bool = aten::gt(%3804, %24)
      %3806 : bool, %3807 : bool, %3808 : int = prim::Loop(%18, %3805, %19, %3803, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3809 : int, %3810 : bool, %3811 : bool, %3812 : int):
          %tensor.22 : Tensor = aten::__getitem__(%features.4, %3812) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3814 : bool = prim::requires_grad(%tensor.22)
          %3815 : bool, %3816 : bool = prim::If(%3814) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3803)
          %3817 : int = aten::add(%3812, %27)
          %3818 : bool = aten::lt(%3817, %3804)
          %3819 : bool = aten::__and__(%3818, %3815)
          -> (%3819, %3814, %3816, %3817)
      %3820 : bool = prim::If(%3806)
        block0():
          -> (%3807)
        block1():
          -> (%19)
      -> (%3820)
    block1():
      -> (%19)
  %bottleneck_output.42 : Tensor = prim::If(%3802) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3800)
    block1():
      %concated_features.22 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3823 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%1896)
      %3824 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%1896)
      %3825 : int = aten::dim(%concated_features.22) # torch/nn/modules/batchnorm.py:276:11
      %3826 : bool = aten::ne(%3825, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3826) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3827 : bool = prim::GetAttr[name="training"](%3824)
       = prim::If(%3827) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3828 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3824)
          %3829 : Tensor = aten::add(%3828, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3824, %3829)
          -> ()
        block1():
          -> ()
      %3830 : bool = prim::GetAttr[name="training"](%3824)
      %3831 : Tensor = prim::GetAttr[name="running_mean"](%3824)
      %3832 : Tensor = prim::GetAttr[name="running_var"](%3824)
      %3833 : Tensor = prim::GetAttr[name="weight"](%3824)
      %3834 : Tensor = prim::GetAttr[name="bias"](%3824)
       = prim::If(%3830) # torch/nn/functional.py:2011:4
        block0():
          %3835 : int[] = aten::size(%concated_features.22) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%3835, %24) # torch/nn/functional.py:1991:17
          %3837 : int = aten::len(%3835) # torch/nn/functional.py:1992:19
          %3838 : int = aten::sub(%3837, %26) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%3838, %25, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %3842 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
              %3843 : int = aten::__getitem__(%3835, %3842) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %3843) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.171)
          %3845 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3845) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3846 : Tensor = aten::batch_norm(%concated_features.22, %3833, %3834, %3831, %3832, %3830, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.43 : Tensor = aten::relu_(%3846) # torch/nn/functional.py:1117:17
      %3848 : Tensor = prim::GetAttr[name="weight"](%3823)
      %3849 : Tensor? = prim::GetAttr[name="bias"](%3823)
      %3850 : int[] = prim::ListConstruct(%27, %27)
      %3851 : int[] = prim::ListConstruct(%24, %24)
      %3852 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.43 : Tensor = aten::conv2d(%result.43, %3848, %3849, %3850, %3851, %3852, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.43)
  %3854 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1896)
  %3855 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1896)
  %3856 : int = aten::dim(%bottleneck_output.42) # torch/nn/modules/batchnorm.py:276:11
  %3857 : bool = aten::ne(%3856, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3857) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3858 : bool = prim::GetAttr[name="training"](%3855)
   = prim::If(%3858) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3859 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3855)
      %3860 : Tensor = aten::add(%3859, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3855, %3860)
      -> ()
    block1():
      -> ()
  %3861 : bool = prim::GetAttr[name="training"](%3855)
  %3862 : Tensor = prim::GetAttr[name="running_mean"](%3855)
  %3863 : Tensor = prim::GetAttr[name="running_var"](%3855)
  %3864 : Tensor = prim::GetAttr[name="weight"](%3855)
  %3865 : Tensor = prim::GetAttr[name="bias"](%3855)
   = prim::If(%3861) # torch/nn/functional.py:2011:4
    block0():
      %3866 : int[] = aten::size(%bottleneck_output.42) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%3866, %24) # torch/nn/functional.py:1991:17
      %3868 : int = aten::len(%3866) # torch/nn/functional.py:1992:19
      %3869 : int = aten::sub(%3868, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%3869, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %3873 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %3874 : int = aten::__getitem__(%3866, %3873) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %3874) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %3876 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3876) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3877 : Tensor = aten::batch_norm(%bottleneck_output.42, %3864, %3865, %3862, %3863, %3861, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.44 : Tensor = aten::relu_(%3877) # torch/nn/functional.py:1117:17
  %3879 : Tensor = prim::GetAttr[name="weight"](%3854)
  %3880 : Tensor? = prim::GetAttr[name="bias"](%3854)
  %3881 : int[] = prim::ListConstruct(%27, %27)
  %3882 : int[] = prim::ListConstruct(%27, %27)
  %3883 : int[] = prim::ListConstruct(%27, %27)
  %new_features.44 : Tensor = aten::conv2d(%result.44, %3879, %3880, %3881, %3882, %3883, %27) # torch/nn/modules/conv.py:415:15
  %3885 : float = prim::GetAttr[name="drop_rate"](%1896)
  %3886 : bool = aten::gt(%3885, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.41 : Tensor = prim::If(%3886) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3888 : float = prim::GetAttr[name="drop_rate"](%1896)
      %3889 : bool = prim::GetAttr[name="training"](%1896)
      %3890 : bool = aten::lt(%3888, %16) # torch/nn/functional.py:968:7
      %3891 : bool = prim::If(%3890) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3892 : bool = aten::gt(%3888, %17) # torch/nn/functional.py:968:17
          -> (%3892)
       = prim::If(%3891) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3893 : Tensor = aten::dropout(%new_features.44, %3888, %3889) # torch/nn/functional.py:973:17
      -> (%3893)
    block1():
      -> (%new_features.44)
  %3894 : Tensor[] = aten::append(%features.4, %new_features.41) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3895 : Tensor = prim::Uninitialized()
  %3896 : bool = prim::GetAttr[name="memory_efficient"](%1897)
  %3897 : bool = prim::If(%3896) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3898 : bool = prim::Uninitialized()
      %3899 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3900 : bool = aten::gt(%3899, %24)
      %3901 : bool, %3902 : bool, %3903 : int = prim::Loop(%18, %3900, %19, %3898, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3904 : int, %3905 : bool, %3906 : bool, %3907 : int):
          %tensor.23 : Tensor = aten::__getitem__(%features.4, %3907) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3909 : bool = prim::requires_grad(%tensor.23)
          %3910 : bool, %3911 : bool = prim::If(%3909) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3898)
          %3912 : int = aten::add(%3907, %27)
          %3913 : bool = aten::lt(%3912, %3899)
          %3914 : bool = aten::__and__(%3913, %3910)
          -> (%3914, %3909, %3911, %3912)
      %3915 : bool = prim::If(%3901)
        block0():
          -> (%3902)
        block1():
          -> (%19)
      -> (%3915)
    block1():
      -> (%19)
  %bottleneck_output.44 : Tensor = prim::If(%3897) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3895)
    block1():
      %concated_features.23 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3918 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%1897)
      %3919 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%1897)
      %3920 : int = aten::dim(%concated_features.23) # torch/nn/modules/batchnorm.py:276:11
      %3921 : bool = aten::ne(%3920, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3921) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3922 : bool = prim::GetAttr[name="training"](%3919)
       = prim::If(%3922) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3923 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3919)
          %3924 : Tensor = aten::add(%3923, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3919, %3924)
          -> ()
        block1():
          -> ()
      %3925 : bool = prim::GetAttr[name="training"](%3919)
      %3926 : Tensor = prim::GetAttr[name="running_mean"](%3919)
      %3927 : Tensor = prim::GetAttr[name="running_var"](%3919)
      %3928 : Tensor = prim::GetAttr[name="weight"](%3919)
      %3929 : Tensor = prim::GetAttr[name="bias"](%3919)
       = prim::If(%3925) # torch/nn/functional.py:2011:4
        block0():
          %3930 : int[] = aten::size(%concated_features.23) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%3930, %24) # torch/nn/functional.py:1991:17
          %3932 : int = aten::len(%3930) # torch/nn/functional.py:1992:19
          %3933 : int = aten::sub(%3932, %26) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%3933, %25, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %3937 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
              %3938 : int = aten::__getitem__(%3930, %3937) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %3938) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.179)
          %3940 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3940) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3941 : Tensor = aten::batch_norm(%concated_features.23, %3928, %3929, %3926, %3927, %3925, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.45 : Tensor = aten::relu_(%3941) # torch/nn/functional.py:1117:17
      %3943 : Tensor = prim::GetAttr[name="weight"](%3918)
      %3944 : Tensor? = prim::GetAttr[name="bias"](%3918)
      %3945 : int[] = prim::ListConstruct(%27, %27)
      %3946 : int[] = prim::ListConstruct(%24, %24)
      %3947 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.45 : Tensor = aten::conv2d(%result.45, %3943, %3944, %3945, %3946, %3947, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.45)
  %3949 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1897)
  %3950 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1897)
  %3951 : int = aten::dim(%bottleneck_output.44) # torch/nn/modules/batchnorm.py:276:11
  %3952 : bool = aten::ne(%3951, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3952) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3953 : bool = prim::GetAttr[name="training"](%3950)
   = prim::If(%3953) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3954 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3950)
      %3955 : Tensor = aten::add(%3954, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3950, %3955)
      -> ()
    block1():
      -> ()
  %3956 : bool = prim::GetAttr[name="training"](%3950)
  %3957 : Tensor = prim::GetAttr[name="running_mean"](%3950)
  %3958 : Tensor = prim::GetAttr[name="running_var"](%3950)
  %3959 : Tensor = prim::GetAttr[name="weight"](%3950)
  %3960 : Tensor = prim::GetAttr[name="bias"](%3950)
   = prim::If(%3956) # torch/nn/functional.py:2011:4
    block0():
      %3961 : int[] = aten::size(%bottleneck_output.44) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%3961, %24) # torch/nn/functional.py:1991:17
      %3963 : int = aten::len(%3961) # torch/nn/functional.py:1992:19
      %3964 : int = aten::sub(%3963, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%3964, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %3968 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %3969 : int = aten::__getitem__(%3961, %3968) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %3969) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %3971 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3971) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3972 : Tensor = aten::batch_norm(%bottleneck_output.44, %3959, %3960, %3957, %3958, %3956, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.46 : Tensor = aten::relu_(%3972) # torch/nn/functional.py:1117:17
  %3974 : Tensor = prim::GetAttr[name="weight"](%3949)
  %3975 : Tensor? = prim::GetAttr[name="bias"](%3949)
  %3976 : int[] = prim::ListConstruct(%27, %27)
  %3977 : int[] = prim::ListConstruct(%27, %27)
  %3978 : int[] = prim::ListConstruct(%27, %27)
  %new_features.46 : Tensor = aten::conv2d(%result.46, %3974, %3975, %3976, %3977, %3978, %27) # torch/nn/modules/conv.py:415:15
  %3980 : float = prim::GetAttr[name="drop_rate"](%1897)
  %3981 : bool = aten::gt(%3980, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.43 : Tensor = prim::If(%3981) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3983 : float = prim::GetAttr[name="drop_rate"](%1897)
      %3984 : bool = prim::GetAttr[name="training"](%1897)
      %3985 : bool = aten::lt(%3983, %16) # torch/nn/functional.py:968:7
      %3986 : bool = prim::If(%3985) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3987 : bool = aten::gt(%3983, %17) # torch/nn/functional.py:968:17
          -> (%3987)
       = prim::If(%3986) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3988 : Tensor = aten::dropout(%new_features.46, %3983, %3984) # torch/nn/functional.py:973:17
      -> (%3988)
    block1():
      -> (%new_features.46)
  %3989 : Tensor[] = aten::append(%features.4, %new_features.43) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3990 : Tensor = prim::Uninitialized()
  %3991 : bool = prim::GetAttr[name="memory_efficient"](%1898)
  %3992 : bool = prim::If(%3991) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3993 : bool = prim::Uninitialized()
      %3994 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3995 : bool = aten::gt(%3994, %24)
      %3996 : bool, %3997 : bool, %3998 : int = prim::Loop(%18, %3995, %19, %3993, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3999 : int, %4000 : bool, %4001 : bool, %4002 : int):
          %tensor.24 : Tensor = aten::__getitem__(%features.4, %4002) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4004 : bool = prim::requires_grad(%tensor.24)
          %4005 : bool, %4006 : bool = prim::If(%4004) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3993)
          %4007 : int = aten::add(%4002, %27)
          %4008 : bool = aten::lt(%4007, %3994)
          %4009 : bool = aten::__and__(%4008, %4005)
          -> (%4009, %4004, %4006, %4007)
      %4010 : bool = prim::If(%3996)
        block0():
          -> (%3997)
        block1():
          -> (%19)
      -> (%4010)
    block1():
      -> (%19)
  %bottleneck_output.46 : Tensor = prim::If(%3992) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3990)
    block1():
      %concated_features.24 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4013 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%1898)
      %4014 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%1898)
      %4015 : int = aten::dim(%concated_features.24) # torch/nn/modules/batchnorm.py:276:11
      %4016 : bool = aten::ne(%4015, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4016) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4017 : bool = prim::GetAttr[name="training"](%4014)
       = prim::If(%4017) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4018 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4014)
          %4019 : Tensor = aten::add(%4018, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4014, %4019)
          -> ()
        block1():
          -> ()
      %4020 : bool = prim::GetAttr[name="training"](%4014)
      %4021 : Tensor = prim::GetAttr[name="running_mean"](%4014)
      %4022 : Tensor = prim::GetAttr[name="running_var"](%4014)
      %4023 : Tensor = prim::GetAttr[name="weight"](%4014)
      %4024 : Tensor = prim::GetAttr[name="bias"](%4014)
       = prim::If(%4020) # torch/nn/functional.py:2011:4
        block0():
          %4025 : int[] = aten::size(%concated_features.24) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%4025, %24) # torch/nn/functional.py:1991:17
          %4027 : int = aten::len(%4025) # torch/nn/functional.py:1992:19
          %4028 : int = aten::sub(%4027, %26) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%4028, %25, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %4032 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
              %4033 : int = aten::__getitem__(%4025, %4032) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %4033) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.187)
          %4035 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4035) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4036 : Tensor = aten::batch_norm(%concated_features.24, %4023, %4024, %4021, %4022, %4020, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.47 : Tensor = aten::relu_(%4036) # torch/nn/functional.py:1117:17
      %4038 : Tensor = prim::GetAttr[name="weight"](%4013)
      %4039 : Tensor? = prim::GetAttr[name="bias"](%4013)
      %4040 : int[] = prim::ListConstruct(%27, %27)
      %4041 : int[] = prim::ListConstruct(%24, %24)
      %4042 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.47 : Tensor = aten::conv2d(%result.47, %4038, %4039, %4040, %4041, %4042, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.47)
  %4044 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1898)
  %4045 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1898)
  %4046 : int = aten::dim(%bottleneck_output.46) # torch/nn/modules/batchnorm.py:276:11
  %4047 : bool = aten::ne(%4046, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4047) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4048 : bool = prim::GetAttr[name="training"](%4045)
   = prim::If(%4048) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4049 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4045)
      %4050 : Tensor = aten::add(%4049, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4045, %4050)
      -> ()
    block1():
      -> ()
  %4051 : bool = prim::GetAttr[name="training"](%4045)
  %4052 : Tensor = prim::GetAttr[name="running_mean"](%4045)
  %4053 : Tensor = prim::GetAttr[name="running_var"](%4045)
  %4054 : Tensor = prim::GetAttr[name="weight"](%4045)
  %4055 : Tensor = prim::GetAttr[name="bias"](%4045)
   = prim::If(%4051) # torch/nn/functional.py:2011:4
    block0():
      %4056 : int[] = aten::size(%bottleneck_output.46) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%4056, %24) # torch/nn/functional.py:1991:17
      %4058 : int = aten::len(%4056) # torch/nn/functional.py:1992:19
      %4059 : int = aten::sub(%4058, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%4059, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %4063 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %4064 : int = aten::__getitem__(%4056, %4063) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %4064) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %4066 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4066) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4067 : Tensor = aten::batch_norm(%bottleneck_output.46, %4054, %4055, %4052, %4053, %4051, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.48 : Tensor = aten::relu_(%4067) # torch/nn/functional.py:1117:17
  %4069 : Tensor = prim::GetAttr[name="weight"](%4044)
  %4070 : Tensor? = prim::GetAttr[name="bias"](%4044)
  %4071 : int[] = prim::ListConstruct(%27, %27)
  %4072 : int[] = prim::ListConstruct(%27, %27)
  %4073 : int[] = prim::ListConstruct(%27, %27)
  %new_features.48 : Tensor = aten::conv2d(%result.48, %4069, %4070, %4071, %4072, %4073, %27) # torch/nn/modules/conv.py:415:15
  %4075 : float = prim::GetAttr[name="drop_rate"](%1898)
  %4076 : bool = aten::gt(%4075, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.45 : Tensor = prim::If(%4076) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4078 : float = prim::GetAttr[name="drop_rate"](%1898)
      %4079 : bool = prim::GetAttr[name="training"](%1898)
      %4080 : bool = aten::lt(%4078, %16) # torch/nn/functional.py:968:7
      %4081 : bool = prim::If(%4080) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4082 : bool = aten::gt(%4078, %17) # torch/nn/functional.py:968:17
          -> (%4082)
       = prim::If(%4081) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4083 : Tensor = aten::dropout(%new_features.48, %4078, %4079) # torch/nn/functional.py:973:17
      -> (%4083)
    block1():
      -> (%new_features.48)
  %4084 : Tensor[] = aten::append(%features.4, %new_features.45) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4085 : Tensor = prim::Uninitialized()
  %4086 : bool = prim::GetAttr[name="memory_efficient"](%1899)
  %4087 : bool = prim::If(%4086) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4088 : bool = prim::Uninitialized()
      %4089 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4090 : bool = aten::gt(%4089, %24)
      %4091 : bool, %4092 : bool, %4093 : int = prim::Loop(%18, %4090, %19, %4088, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4094 : int, %4095 : bool, %4096 : bool, %4097 : int):
          %tensor.58 : Tensor = aten::__getitem__(%features.4, %4097) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4099 : bool = prim::requires_grad(%tensor.58)
          %4100 : bool, %4101 : bool = prim::If(%4099) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4088)
          %4102 : int = aten::add(%4097, %27)
          %4103 : bool = aten::lt(%4102, %4089)
          %4104 : bool = aten::__and__(%4103, %4100)
          -> (%4104, %4099, %4101, %4102)
      %4105 : bool = prim::If(%4091)
        block0():
          -> (%4092)
        block1():
          -> (%19)
      -> (%4105)
    block1():
      -> (%19)
  %bottleneck_output.114 : Tensor = prim::If(%4087) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4085)
    block1():
      %concated_features.58 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4108 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%1899)
      %4109 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%1899)
      %4110 : int = aten::dim(%concated_features.58) # torch/nn/modules/batchnorm.py:276:11
      %4111 : bool = aten::ne(%4110, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4111) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4112 : bool = prim::GetAttr[name="training"](%4109)
       = prim::If(%4112) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4113 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4109)
          %4114 : Tensor = aten::add(%4113, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4109, %4114)
          -> ()
        block1():
          -> ()
      %4115 : bool = prim::GetAttr[name="training"](%4109)
      %4116 : Tensor = prim::GetAttr[name="running_mean"](%4109)
      %4117 : Tensor = prim::GetAttr[name="running_var"](%4109)
      %4118 : Tensor = prim::GetAttr[name="weight"](%4109)
      %4119 : Tensor = prim::GetAttr[name="bias"](%4109)
       = prim::If(%4115) # torch/nn/functional.py:2011:4
        block0():
          %4120 : int[] = aten::size(%concated_features.58) # torch/nn/functional.py:2012:27
          %size_prods.472 : int = aten::__getitem__(%4120, %24) # torch/nn/functional.py:1991:17
          %4122 : int = aten::len(%4120) # torch/nn/functional.py:1992:19
          %4123 : int = aten::sub(%4122, %26) # torch/nn/functional.py:1992:19
          %size_prods.473 : int = prim::Loop(%4123, %25, %size_prods.472) # torch/nn/functional.py:1992:4
            block0(%i.119 : int, %size_prods.474 : int):
              %4127 : int = aten::add(%i.119, %26) # torch/nn/functional.py:1993:27
              %4128 : int = aten::__getitem__(%4120, %4127) # torch/nn/functional.py:1993:22
              %size_prods.475 : int = aten::mul(%size_prods.474, %4128) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.475)
          %4130 : bool = aten::eq(%size_prods.473, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4130) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4131 : Tensor = aten::batch_norm(%concated_features.58, %4118, %4119, %4116, %4117, %4115, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.115 : Tensor = aten::relu_(%4131) # torch/nn/functional.py:1117:17
      %4133 : Tensor = prim::GetAttr[name="weight"](%4108)
      %4134 : Tensor? = prim::GetAttr[name="bias"](%4108)
      %4135 : int[] = prim::ListConstruct(%27, %27)
      %4136 : int[] = prim::ListConstruct(%24, %24)
      %4137 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.115 : Tensor = aten::conv2d(%result.115, %4133, %4134, %4135, %4136, %4137, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.115)
  %4139 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1899)
  %4140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1899)
  %4141 : int = aten::dim(%bottleneck_output.114) # torch/nn/modules/batchnorm.py:276:11
  %4142 : bool = aten::ne(%4141, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4142) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4143 : bool = prim::GetAttr[name="training"](%4140)
   = prim::If(%4143) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4144 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4140)
      %4145 : Tensor = aten::add(%4144, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4140, %4145)
      -> ()
    block1():
      -> ()
  %4146 : bool = prim::GetAttr[name="training"](%4140)
  %4147 : Tensor = prim::GetAttr[name="running_mean"](%4140)
  %4148 : Tensor = prim::GetAttr[name="running_var"](%4140)
  %4149 : Tensor = prim::GetAttr[name="weight"](%4140)
  %4150 : Tensor = prim::GetAttr[name="bias"](%4140)
   = prim::If(%4146) # torch/nn/functional.py:2011:4
    block0():
      %4151 : int[] = aten::size(%bottleneck_output.114) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%4151, %24) # torch/nn/functional.py:1991:17
      %4153 : int = aten::len(%4151) # torch/nn/functional.py:1992:19
      %4154 : int = aten::sub(%4153, %26) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%4154, %25, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %4158 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
          %4159 : int = aten::__getitem__(%4151, %4158) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %4159) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.351)
      %4161 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4161) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4162 : Tensor = aten::batch_norm(%bottleneck_output.114, %4149, %4150, %4147, %4148, %4146, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.116 : Tensor = aten::relu_(%4162) # torch/nn/functional.py:1117:17
  %4164 : Tensor = prim::GetAttr[name="weight"](%4139)
  %4165 : Tensor? = prim::GetAttr[name="bias"](%4139)
  %4166 : int[] = prim::ListConstruct(%27, %27)
  %4167 : int[] = prim::ListConstruct(%27, %27)
  %4168 : int[] = prim::ListConstruct(%27, %27)
  %new_features.115 : Tensor = aten::conv2d(%result.116, %4164, %4165, %4166, %4167, %4168, %27) # torch/nn/modules/conv.py:415:15
  %4170 : float = prim::GetAttr[name="drop_rate"](%1899)
  %4171 : bool = aten::gt(%4170, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.47 : Tensor = prim::If(%4171) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4173 : float = prim::GetAttr[name="drop_rate"](%1899)
      %4174 : bool = prim::GetAttr[name="training"](%1899)
      %4175 : bool = aten::lt(%4173, %16) # torch/nn/functional.py:968:7
      %4176 : bool = prim::If(%4175) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4177 : bool = aten::gt(%4173, %17) # torch/nn/functional.py:968:17
          -> (%4177)
       = prim::If(%4176) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4178 : Tensor = aten::dropout(%new_features.115, %4173, %4174) # torch/nn/functional.py:973:17
      -> (%4178)
    block1():
      -> (%new_features.115)
  %4179 : Tensor[] = aten::append(%features.4, %new_features.47) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.19 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %4181 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm"](%36)
  %4182 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv"](%36)
  %4183 : int = aten::dim(%input.19) # torch/nn/modules/batchnorm.py:276:11
  %4184 : bool = aten::ne(%4183, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4184) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4185 : bool = prim::GetAttr[name="training"](%4181)
   = prim::If(%4185) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4186 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4181)
      %4187 : Tensor = aten::add(%4186, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4181, %4187)
      -> ()
    block1():
      -> ()
  %4188 : bool = prim::GetAttr[name="training"](%4181)
  %4189 : Tensor = prim::GetAttr[name="running_mean"](%4181)
  %4190 : Tensor = prim::GetAttr[name="running_var"](%4181)
  %4191 : Tensor = prim::GetAttr[name="weight"](%4181)
  %4192 : Tensor = prim::GetAttr[name="bias"](%4181)
   = prim::If(%4188) # torch/nn/functional.py:2011:4
    block0():
      %4193 : int[] = aten::size(%input.19) # torch/nn/functional.py:2012:27
      %size_prods.476 : int = aten::__getitem__(%4193, %24) # torch/nn/functional.py:1991:17
      %4195 : int = aten::len(%4193) # torch/nn/functional.py:1992:19
      %4196 : int = aten::sub(%4195, %26) # torch/nn/functional.py:1992:19
      %size_prods.477 : int = prim::Loop(%4196, %25, %size_prods.476) # torch/nn/functional.py:1992:4
        block0(%i.120 : int, %size_prods.478 : int):
          %4200 : int = aten::add(%i.120, %26) # torch/nn/functional.py:1993:27
          %4201 : int = aten::__getitem__(%4193, %4200) # torch/nn/functional.py:1993:22
          %size_prods.479 : int = aten::mul(%size_prods.478, %4201) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.479)
      %4203 : bool = aten::eq(%size_prods.477, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4203) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.3 : Tensor = aten::batch_norm(%input.19, %4191, %4192, %4189, %4190, %4188, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.5 : Tensor = aten::relu_(%input.3) # torch/nn/functional.py:1117:17
  %4206 : Tensor = prim::GetAttr[name="weight"](%4182)
  %4207 : Tensor? = prim::GetAttr[name="bias"](%4182)
  %4208 : int[] = prim::ListConstruct(%27, %27)
  %4209 : int[] = prim::ListConstruct(%24, %24)
  %4210 : int[] = prim::ListConstruct(%27, %27)
  %input.7 : Tensor = aten::conv2d(%input.5, %4206, %4207, %4208, %4209, %4210, %27) # torch/nn/modules/conv.py:415:15
  %4212 : int[] = prim::ListConstruct(%26, %26)
  %4213 : int[] = prim::ListConstruct(%26, %26)
  %4214 : int[] = prim::ListConstruct(%24, %24)
  %input.21 : Tensor = aten::avg_pool2d(%input.7, %4212, %4213, %4214, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.1 : Tensor[] = prim::ListConstruct(%input.21)
  %4217 : __torch__.torchvision.models.densenet.___torch_mangle_111._DenseLayer = prim::GetAttr[name="denselayer1"](%37)
  %4218 : __torch__.torchvision.models.densenet.___torch_mangle_114._DenseLayer = prim::GetAttr[name="denselayer2"](%37)
  %4219 : __torch__.torchvision.models.densenet.___torch_mangle_117._DenseLayer = prim::GetAttr[name="denselayer3"](%37)
  %4220 : __torch__.torchvision.models.densenet.___torch_mangle_120._DenseLayer = prim::GetAttr[name="denselayer4"](%37)
  %4221 : __torch__.torchvision.models.densenet.___torch_mangle_123._DenseLayer = prim::GetAttr[name="denselayer5"](%37)
  %4222 : __torch__.torchvision.models.densenet.___torch_mangle_126._DenseLayer = prim::GetAttr[name="denselayer6"](%37)
  %4223 : __torch__.torchvision.models.densenet.___torch_mangle_129._DenseLayer = prim::GetAttr[name="denselayer7"](%37)
  %4224 : __torch__.torchvision.models.densenet.___torch_mangle_132._DenseLayer = prim::GetAttr[name="denselayer8"](%37)
  %4225 : __torch__.torchvision.models.densenet.___torch_mangle_135._DenseLayer = prim::GetAttr[name="denselayer9"](%37)
  %4226 : __torch__.torchvision.models.densenet.___torch_mangle_138._DenseLayer = prim::GetAttr[name="denselayer10"](%37)
  %4227 : __torch__.torchvision.models.densenet.___torch_mangle_141._DenseLayer = prim::GetAttr[name="denselayer11"](%37)
  %4228 : __torch__.torchvision.models.densenet.___torch_mangle_144._DenseLayer = prim::GetAttr[name="denselayer12"](%37)
  %4229 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer13"](%37)
  %4230 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer14"](%37)
  %4231 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer15"](%37)
  %4232 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer16"](%37)
  %4233 : Tensor = prim::Uninitialized()
  %4234 : bool = prim::GetAttr[name="memory_efficient"](%4217)
  %4235 : bool = prim::If(%4234) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4236 : bool = prim::Uninitialized()
      %4237 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4238 : bool = aten::gt(%4237, %24)
      %4239 : bool, %4240 : bool, %4241 : int = prim::Loop(%18, %4238, %19, %4236, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4242 : int, %4243 : bool, %4244 : bool, %4245 : int):
          %tensor.2 : Tensor = aten::__getitem__(%features.1, %4245) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4247 : bool = prim::requires_grad(%tensor.2)
          %4248 : bool, %4249 : bool = prim::If(%4247) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4236)
          %4250 : int = aten::add(%4245, %27)
          %4251 : bool = aten::lt(%4250, %4237)
          %4252 : bool = aten::__and__(%4251, %4248)
          -> (%4252, %4247, %4249, %4250)
      %4253 : bool = prim::If(%4239)
        block0():
          -> (%4240)
        block1():
          -> (%19)
      -> (%4253)
    block1():
      -> (%19)
  %bottleneck_output.1 : Tensor = prim::If(%4235) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4233)
    block1():
      %concated_features.2 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4256 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%4217)
      %4257 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm1"](%4217)
      %4258 : int = aten::dim(%concated_features.2) # torch/nn/modules/batchnorm.py:276:11
      %4259 : bool = aten::ne(%4258, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4259) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4260 : bool = prim::GetAttr[name="training"](%4257)
       = prim::If(%4260) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4261 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4257)
          %4262 : Tensor = aten::add(%4261, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4257, %4262)
          -> ()
        block1():
          -> ()
      %4263 : bool = prim::GetAttr[name="training"](%4257)
      %4264 : Tensor = prim::GetAttr[name="running_mean"](%4257)
      %4265 : Tensor = prim::GetAttr[name="running_var"](%4257)
      %4266 : Tensor = prim::GetAttr[name="weight"](%4257)
      %4267 : Tensor = prim::GetAttr[name="bias"](%4257)
       = prim::If(%4263) # torch/nn/functional.py:2011:4
        block0():
          %4268 : int[] = aten::size(%concated_features.2) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%4268, %24) # torch/nn/functional.py:1991:17
          %4270 : int = aten::len(%4268) # torch/nn/functional.py:1992:19
          %4271 : int = aten::sub(%4270, %26) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%4271, %25, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %4275 : int = aten::add(%i.3, %26) # torch/nn/functional.py:1993:27
              %4276 : int = aten::__getitem__(%4268, %4275) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %4276) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.11)
          %4278 : bool = aten::eq(%size_prods.9, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4278) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4279 : Tensor = aten::batch_norm(%concated_features.2, %4266, %4267, %4264, %4265, %4263, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.3 : Tensor = aten::relu_(%4279) # torch/nn/functional.py:1117:17
      %4281 : Tensor = prim::GetAttr[name="weight"](%4256)
      %4282 : Tensor? = prim::GetAttr[name="bias"](%4256)
      %4283 : int[] = prim::ListConstruct(%27, %27)
      %4284 : int[] = prim::ListConstruct(%24, %24)
      %4285 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.3 : Tensor = aten::conv2d(%result.3, %4281, %4282, %4283, %4284, %4285, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.3)
  %4287 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4217)
  %4288 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4217)
  %4289 : int = aten::dim(%bottleneck_output.1) # torch/nn/modules/batchnorm.py:276:11
  %4290 : bool = aten::ne(%4289, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4290) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4291 : bool = prim::GetAttr[name="training"](%4288)
   = prim::If(%4291) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4292 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4288)
      %4293 : Tensor = aten::add(%4292, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4288, %4293)
      -> ()
    block1():
      -> ()
  %4294 : bool = prim::GetAttr[name="training"](%4288)
  %4295 : Tensor = prim::GetAttr[name="running_mean"](%4288)
  %4296 : Tensor = prim::GetAttr[name="running_var"](%4288)
  %4297 : Tensor = prim::GetAttr[name="weight"](%4288)
  %4298 : Tensor = prim::GetAttr[name="bias"](%4288)
   = prim::If(%4294) # torch/nn/functional.py:2011:4
    block0():
      %4299 : int[] = aten::size(%bottleneck_output.1) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%4299, %24) # torch/nn/functional.py:1991:17
      %4301 : int = aten::len(%4299) # torch/nn/functional.py:1992:19
      %4302 : int = aten::sub(%4301, %26) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%4302, %25, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %4306 : int = aten::add(%i.4, %26) # torch/nn/functional.py:1993:27
          %4307 : int = aten::__getitem__(%4299, %4306) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %4307) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.15)
      %4309 : bool = aten::eq(%size_prods.13, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4309) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4310 : Tensor = aten::batch_norm(%bottleneck_output.1, %4297, %4298, %4295, %4296, %4294, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.4 : Tensor = aten::relu_(%4310) # torch/nn/functional.py:1117:17
  %4312 : Tensor = prim::GetAttr[name="weight"](%4287)
  %4313 : Tensor? = prim::GetAttr[name="bias"](%4287)
  %4314 : int[] = prim::ListConstruct(%27, %27)
  %4315 : int[] = prim::ListConstruct(%27, %27)
  %4316 : int[] = prim::ListConstruct(%27, %27)
  %new_features.4 : Tensor = aten::conv2d(%result.4, %4312, %4313, %4314, %4315, %4316, %27) # torch/nn/modules/conv.py:415:15
  %4318 : float = prim::GetAttr[name="drop_rate"](%4217)
  %4319 : bool = aten::gt(%4318, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.2 : Tensor = prim::If(%4319) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4321 : float = prim::GetAttr[name="drop_rate"](%4217)
      %4322 : bool = prim::GetAttr[name="training"](%4217)
      %4323 : bool = aten::lt(%4321, %16) # torch/nn/functional.py:968:7
      %4324 : bool = prim::If(%4323) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4325 : bool = aten::gt(%4321, %17) # torch/nn/functional.py:968:17
          -> (%4325)
       = prim::If(%4324) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4326 : Tensor = aten::dropout(%new_features.4, %4321, %4322) # torch/nn/functional.py:973:17
      -> (%4326)
    block1():
      -> (%new_features.4)
  %4327 : Tensor[] = aten::append(%features.1, %new_features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4328 : Tensor = prim::Uninitialized()
  %4329 : bool = prim::GetAttr[name="memory_efficient"](%4218)
  %4330 : bool = prim::If(%4329) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4331 : bool = prim::Uninitialized()
      %4332 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4333 : bool = aten::gt(%4332, %24)
      %4334 : bool, %4335 : bool, %4336 : int = prim::Loop(%18, %4333, %19, %4331, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4337 : int, %4338 : bool, %4339 : bool, %4340 : int):
          %tensor.3 : Tensor = aten::__getitem__(%features.1, %4340) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4342 : bool = prim::requires_grad(%tensor.3)
          %4343 : bool, %4344 : bool = prim::If(%4342) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4331)
          %4345 : int = aten::add(%4340, %27)
          %4346 : bool = aten::lt(%4345, %4332)
          %4347 : bool = aten::__and__(%4346, %4343)
          -> (%4347, %4342, %4344, %4345)
      %4348 : bool = prim::If(%4334)
        block0():
          -> (%4335)
        block1():
          -> (%19)
      -> (%4348)
    block1():
      -> (%19)
  %bottleneck_output.4 : Tensor = prim::If(%4330) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4328)
    block1():
      %concated_features.3 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4351 : __torch__.torch.nn.modules.conv.___torch_mangle_113.Conv2d = prim::GetAttr[name="conv1"](%4218)
      %4352 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_112.BatchNorm2d = prim::GetAttr[name="norm1"](%4218)
      %4353 : int = aten::dim(%concated_features.3) # torch/nn/modules/batchnorm.py:276:11
      %4354 : bool = aten::ne(%4353, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4354) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4355 : bool = prim::GetAttr[name="training"](%4352)
       = prim::If(%4355) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4356 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4352)
          %4357 : Tensor = aten::add(%4356, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4352, %4357)
          -> ()
        block1():
          -> ()
      %4358 : bool = prim::GetAttr[name="training"](%4352)
      %4359 : Tensor = prim::GetAttr[name="running_mean"](%4352)
      %4360 : Tensor = prim::GetAttr[name="running_var"](%4352)
      %4361 : Tensor = prim::GetAttr[name="weight"](%4352)
      %4362 : Tensor = prim::GetAttr[name="bias"](%4352)
       = prim::If(%4358) # torch/nn/functional.py:2011:4
        block0():
          %4363 : int[] = aten::size(%concated_features.3) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%4363, %24) # torch/nn/functional.py:1991:17
          %4365 : int = aten::len(%4363) # torch/nn/functional.py:1992:19
          %4366 : int = aten::sub(%4365, %26) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%4366, %25, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %4370 : int = aten::add(%i.5, %26) # torch/nn/functional.py:1993:27
              %4371 : int = aten::__getitem__(%4363, %4370) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %4371) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.19)
          %4373 : bool = aten::eq(%size_prods.17, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4373) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4374 : Tensor = aten::batch_norm(%concated_features.3, %4361, %4362, %4359, %4360, %4358, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.5 : Tensor = aten::relu_(%4374) # torch/nn/functional.py:1117:17
      %4376 : Tensor = prim::GetAttr[name="weight"](%4351)
      %4377 : Tensor? = prim::GetAttr[name="bias"](%4351)
      %4378 : int[] = prim::ListConstruct(%27, %27)
      %4379 : int[] = prim::ListConstruct(%24, %24)
      %4380 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.5 : Tensor = aten::conv2d(%result.5, %4376, %4377, %4378, %4379, %4380, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.5)
  %4382 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4218)
  %4383 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4218)
  %4384 : int = aten::dim(%bottleneck_output.4) # torch/nn/modules/batchnorm.py:276:11
  %4385 : bool = aten::ne(%4384, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4385) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4386 : bool = prim::GetAttr[name="training"](%4383)
   = prim::If(%4386) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4387 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4383)
      %4388 : Tensor = aten::add(%4387, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4383, %4388)
      -> ()
    block1():
      -> ()
  %4389 : bool = prim::GetAttr[name="training"](%4383)
  %4390 : Tensor = prim::GetAttr[name="running_mean"](%4383)
  %4391 : Tensor = prim::GetAttr[name="running_var"](%4383)
  %4392 : Tensor = prim::GetAttr[name="weight"](%4383)
  %4393 : Tensor = prim::GetAttr[name="bias"](%4383)
   = prim::If(%4389) # torch/nn/functional.py:2011:4
    block0():
      %4394 : int[] = aten::size(%bottleneck_output.4) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%4394, %24) # torch/nn/functional.py:1991:17
      %4396 : int = aten::len(%4394) # torch/nn/functional.py:1992:19
      %4397 : int = aten::sub(%4396, %26) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%4397, %25, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %4401 : int = aten::add(%i.6, %26) # torch/nn/functional.py:1993:27
          %4402 : int = aten::__getitem__(%4394, %4401) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %4402) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.23)
      %4404 : bool = aten::eq(%size_prods.21, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4404) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4405 : Tensor = aten::batch_norm(%bottleneck_output.4, %4392, %4393, %4390, %4391, %4389, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.6 : Tensor = aten::relu_(%4405) # torch/nn/functional.py:1117:17
  %4407 : Tensor = prim::GetAttr[name="weight"](%4382)
  %4408 : Tensor? = prim::GetAttr[name="bias"](%4382)
  %4409 : int[] = prim::ListConstruct(%27, %27)
  %4410 : int[] = prim::ListConstruct(%27, %27)
  %4411 : int[] = prim::ListConstruct(%27, %27)
  %new_features.6 : Tensor = aten::conv2d(%result.6, %4407, %4408, %4409, %4410, %4411, %27) # torch/nn/modules/conv.py:415:15
  %4413 : float = prim::GetAttr[name="drop_rate"](%4218)
  %4414 : bool = aten::gt(%4413, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.3 : Tensor = prim::If(%4414) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4416 : float = prim::GetAttr[name="drop_rate"](%4218)
      %4417 : bool = prim::GetAttr[name="training"](%4218)
      %4418 : bool = aten::lt(%4416, %16) # torch/nn/functional.py:968:7
      %4419 : bool = prim::If(%4418) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4420 : bool = aten::gt(%4416, %17) # torch/nn/functional.py:968:17
          -> (%4420)
       = prim::If(%4419) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4421 : Tensor = aten::dropout(%new_features.6, %4416, %4417) # torch/nn/functional.py:973:17
      -> (%4421)
    block1():
      -> (%new_features.6)
  %4422 : Tensor[] = aten::append(%features.1, %new_features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4423 : Tensor = prim::Uninitialized()
  %4424 : bool = prim::GetAttr[name="memory_efficient"](%4219)
  %4425 : bool = prim::If(%4424) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4426 : bool = prim::Uninitialized()
      %4427 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4428 : bool = aten::gt(%4427, %24)
      %4429 : bool, %4430 : bool, %4431 : int = prim::Loop(%18, %4428, %19, %4426, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4432 : int, %4433 : bool, %4434 : bool, %4435 : int):
          %tensor.4 : Tensor = aten::__getitem__(%features.1, %4435) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4437 : bool = prim::requires_grad(%tensor.4)
          %4438 : bool, %4439 : bool = prim::If(%4437) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4426)
          %4440 : int = aten::add(%4435, %27)
          %4441 : bool = aten::lt(%4440, %4427)
          %4442 : bool = aten::__and__(%4441, %4438)
          -> (%4442, %4437, %4439, %4440)
      %4443 : bool = prim::If(%4429)
        block0():
          -> (%4430)
        block1():
          -> (%19)
      -> (%4443)
    block1():
      -> (%19)
  %bottleneck_output.6 : Tensor = prim::If(%4425) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4423)
    block1():
      %concated_features.4 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4446 : __torch__.torch.nn.modules.conv.___torch_mangle_116.Conv2d = prim::GetAttr[name="conv1"](%4219)
      %4447 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%4219)
      %4448 : int = aten::dim(%concated_features.4) # torch/nn/modules/batchnorm.py:276:11
      %4449 : bool = aten::ne(%4448, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4449) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4450 : bool = prim::GetAttr[name="training"](%4447)
       = prim::If(%4450) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4451 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4447)
          %4452 : Tensor = aten::add(%4451, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4447, %4452)
          -> ()
        block1():
          -> ()
      %4453 : bool = prim::GetAttr[name="training"](%4447)
      %4454 : Tensor = prim::GetAttr[name="running_mean"](%4447)
      %4455 : Tensor = prim::GetAttr[name="running_var"](%4447)
      %4456 : Tensor = prim::GetAttr[name="weight"](%4447)
      %4457 : Tensor = prim::GetAttr[name="bias"](%4447)
       = prim::If(%4453) # torch/nn/functional.py:2011:4
        block0():
          %4458 : int[] = aten::size(%concated_features.4) # torch/nn/functional.py:2012:27
          %size_prods.24 : int = aten::__getitem__(%4458, %24) # torch/nn/functional.py:1991:17
          %4460 : int = aten::len(%4458) # torch/nn/functional.py:1992:19
          %4461 : int = aten::sub(%4460, %26) # torch/nn/functional.py:1992:19
          %size_prods.25 : int = prim::Loop(%4461, %25, %size_prods.24) # torch/nn/functional.py:1992:4
            block0(%i.7 : int, %size_prods.26 : int):
              %4465 : int = aten::add(%i.7, %26) # torch/nn/functional.py:1993:27
              %4466 : int = aten::__getitem__(%4458, %4465) # torch/nn/functional.py:1993:22
              %size_prods.27 : int = aten::mul(%size_prods.26, %4466) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.27)
          %4468 : bool = aten::eq(%size_prods.25, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4468) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4469 : Tensor = aten::batch_norm(%concated_features.4, %4456, %4457, %4454, %4455, %4453, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.7 : Tensor = aten::relu_(%4469) # torch/nn/functional.py:1117:17
      %4471 : Tensor = prim::GetAttr[name="weight"](%4446)
      %4472 : Tensor? = prim::GetAttr[name="bias"](%4446)
      %4473 : int[] = prim::ListConstruct(%27, %27)
      %4474 : int[] = prim::ListConstruct(%24, %24)
      %4475 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.7 : Tensor = aten::conv2d(%result.7, %4471, %4472, %4473, %4474, %4475, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.7)
  %4477 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4219)
  %4478 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4219)
  %4479 : int = aten::dim(%bottleneck_output.6) # torch/nn/modules/batchnorm.py:276:11
  %4480 : bool = aten::ne(%4479, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4480) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4481 : bool = prim::GetAttr[name="training"](%4478)
   = prim::If(%4481) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4482 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4478)
      %4483 : Tensor = aten::add(%4482, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4478, %4483)
      -> ()
    block1():
      -> ()
  %4484 : bool = prim::GetAttr[name="training"](%4478)
  %4485 : Tensor = prim::GetAttr[name="running_mean"](%4478)
  %4486 : Tensor = prim::GetAttr[name="running_var"](%4478)
  %4487 : Tensor = prim::GetAttr[name="weight"](%4478)
  %4488 : Tensor = prim::GetAttr[name="bias"](%4478)
   = prim::If(%4484) # torch/nn/functional.py:2011:4
    block0():
      %4489 : int[] = aten::size(%bottleneck_output.6) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%4489, %24) # torch/nn/functional.py:1991:17
      %4491 : int = aten::len(%4489) # torch/nn/functional.py:1992:19
      %4492 : int = aten::sub(%4491, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%4492, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %4496 : int = aten::add(%i.8, %26) # torch/nn/functional.py:1993:27
          %4497 : int = aten::__getitem__(%4489, %4496) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %4497) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %4499 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4499) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4500 : Tensor = aten::batch_norm(%bottleneck_output.6, %4487, %4488, %4485, %4486, %4484, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.8 : Tensor = aten::relu_(%4500) # torch/nn/functional.py:1117:17
  %4502 : Tensor = prim::GetAttr[name="weight"](%4477)
  %4503 : Tensor? = prim::GetAttr[name="bias"](%4477)
  %4504 : int[] = prim::ListConstruct(%27, %27)
  %4505 : int[] = prim::ListConstruct(%27, %27)
  %4506 : int[] = prim::ListConstruct(%27, %27)
  %new_features.8 : Tensor = aten::conv2d(%result.8, %4502, %4503, %4504, %4505, %4506, %27) # torch/nn/modules/conv.py:415:15
  %4508 : float = prim::GetAttr[name="drop_rate"](%4219)
  %4509 : bool = aten::gt(%4508, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.5 : Tensor = prim::If(%4509) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4511 : float = prim::GetAttr[name="drop_rate"](%4219)
      %4512 : bool = prim::GetAttr[name="training"](%4219)
      %4513 : bool = aten::lt(%4511, %16) # torch/nn/functional.py:968:7
      %4514 : bool = prim::If(%4513) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4515 : bool = aten::gt(%4511, %17) # torch/nn/functional.py:968:17
          -> (%4515)
       = prim::If(%4514) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4516 : Tensor = aten::dropout(%new_features.8, %4511, %4512) # torch/nn/functional.py:973:17
      -> (%4516)
    block1():
      -> (%new_features.8)
  %4517 : Tensor[] = aten::append(%features.1, %new_features.5) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4518 : Tensor = prim::Uninitialized()
  %4519 : bool = prim::GetAttr[name="memory_efficient"](%4220)
  %4520 : bool = prim::If(%4519) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4521 : bool = prim::Uninitialized()
      %4522 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4523 : bool = aten::gt(%4522, %24)
      %4524 : bool, %4525 : bool, %4526 : int = prim::Loop(%18, %4523, %19, %4521, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4527 : int, %4528 : bool, %4529 : bool, %4530 : int):
          %tensor.5 : Tensor = aten::__getitem__(%features.1, %4530) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4532 : bool = prim::requires_grad(%tensor.5)
          %4533 : bool, %4534 : bool = prim::If(%4532) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4521)
          %4535 : int = aten::add(%4530, %27)
          %4536 : bool = aten::lt(%4535, %4522)
          %4537 : bool = aten::__and__(%4536, %4533)
          -> (%4537, %4532, %4534, %4535)
      %4538 : bool = prim::If(%4524)
        block0():
          -> (%4525)
        block1():
          -> (%19)
      -> (%4538)
    block1():
      -> (%19)
  %bottleneck_output.8 : Tensor = prim::If(%4520) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4518)
    block1():
      %concated_features.5 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4541 : __torch__.torch.nn.modules.conv.___torch_mangle_119.Conv2d = prim::GetAttr[name="conv1"](%4220)
      %4542 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="norm1"](%4220)
      %4543 : int = aten::dim(%concated_features.5) # torch/nn/modules/batchnorm.py:276:11
      %4544 : bool = aten::ne(%4543, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4544) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4545 : bool = prim::GetAttr[name="training"](%4542)
       = prim::If(%4545) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4546 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4542)
          %4547 : Tensor = aten::add(%4546, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4542, %4547)
          -> ()
        block1():
          -> ()
      %4548 : bool = prim::GetAttr[name="training"](%4542)
      %4549 : Tensor = prim::GetAttr[name="running_mean"](%4542)
      %4550 : Tensor = prim::GetAttr[name="running_var"](%4542)
      %4551 : Tensor = prim::GetAttr[name="weight"](%4542)
      %4552 : Tensor = prim::GetAttr[name="bias"](%4542)
       = prim::If(%4548) # torch/nn/functional.py:2011:4
        block0():
          %4553 : int[] = aten::size(%concated_features.5) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%4553, %24) # torch/nn/functional.py:1991:17
          %4555 : int = aten::len(%4553) # torch/nn/functional.py:1992:19
          %4556 : int = aten::sub(%4555, %26) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%4556, %25, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %4560 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
              %4561 : int = aten::__getitem__(%4553, %4560) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %4561) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.35)
          %4563 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4563) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4564 : Tensor = aten::batch_norm(%concated_features.5, %4551, %4552, %4549, %4550, %4548, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.9 : Tensor = aten::relu_(%4564) # torch/nn/functional.py:1117:17
      %4566 : Tensor = prim::GetAttr[name="weight"](%4541)
      %4567 : Tensor? = prim::GetAttr[name="bias"](%4541)
      %4568 : int[] = prim::ListConstruct(%27, %27)
      %4569 : int[] = prim::ListConstruct(%24, %24)
      %4570 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.9 : Tensor = aten::conv2d(%result.9, %4566, %4567, %4568, %4569, %4570, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.9)
  %4572 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4220)
  %4573 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4220)
  %4574 : int = aten::dim(%bottleneck_output.8) # torch/nn/modules/batchnorm.py:276:11
  %4575 : bool = aten::ne(%4574, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4575) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4576 : bool = prim::GetAttr[name="training"](%4573)
   = prim::If(%4576) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4577 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4573)
      %4578 : Tensor = aten::add(%4577, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4573, %4578)
      -> ()
    block1():
      -> ()
  %4579 : bool = prim::GetAttr[name="training"](%4573)
  %4580 : Tensor = prim::GetAttr[name="running_mean"](%4573)
  %4581 : Tensor = prim::GetAttr[name="running_var"](%4573)
  %4582 : Tensor = prim::GetAttr[name="weight"](%4573)
  %4583 : Tensor = prim::GetAttr[name="bias"](%4573)
   = prim::If(%4579) # torch/nn/functional.py:2011:4
    block0():
      %4584 : int[] = aten::size(%bottleneck_output.8) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%4584, %24) # torch/nn/functional.py:1991:17
      %4586 : int = aten::len(%4584) # torch/nn/functional.py:1992:19
      %4587 : int = aten::sub(%4586, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%4587, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %4591 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %4592 : int = aten::__getitem__(%4584, %4591) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %4592) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %4594 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4594) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4595 : Tensor = aten::batch_norm(%bottleneck_output.8, %4582, %4583, %4580, %4581, %4579, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.10 : Tensor = aten::relu_(%4595) # torch/nn/functional.py:1117:17
  %4597 : Tensor = prim::GetAttr[name="weight"](%4572)
  %4598 : Tensor? = prim::GetAttr[name="bias"](%4572)
  %4599 : int[] = prim::ListConstruct(%27, %27)
  %4600 : int[] = prim::ListConstruct(%27, %27)
  %4601 : int[] = prim::ListConstruct(%27, %27)
  %new_features.10 : Tensor = aten::conv2d(%result.10, %4597, %4598, %4599, %4600, %4601, %27) # torch/nn/modules/conv.py:415:15
  %4603 : float = prim::GetAttr[name="drop_rate"](%4220)
  %4604 : bool = aten::gt(%4603, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.7 : Tensor = prim::If(%4604) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4606 : float = prim::GetAttr[name="drop_rate"](%4220)
      %4607 : bool = prim::GetAttr[name="training"](%4220)
      %4608 : bool = aten::lt(%4606, %16) # torch/nn/functional.py:968:7
      %4609 : bool = prim::If(%4608) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4610 : bool = aten::gt(%4606, %17) # torch/nn/functional.py:968:17
          -> (%4610)
       = prim::If(%4609) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4611 : Tensor = aten::dropout(%new_features.10, %4606, %4607) # torch/nn/functional.py:973:17
      -> (%4611)
    block1():
      -> (%new_features.10)
  %4612 : Tensor[] = aten::append(%features.1, %new_features.7) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4613 : Tensor = prim::Uninitialized()
  %4614 : bool = prim::GetAttr[name="memory_efficient"](%4221)
  %4615 : bool = prim::If(%4614) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4616 : bool = prim::Uninitialized()
      %4617 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4618 : bool = aten::gt(%4617, %24)
      %4619 : bool, %4620 : bool, %4621 : int = prim::Loop(%18, %4618, %19, %4616, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4622 : int, %4623 : bool, %4624 : bool, %4625 : int):
          %tensor.6 : Tensor = aten::__getitem__(%features.1, %4625) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4627 : bool = prim::requires_grad(%tensor.6)
          %4628 : bool, %4629 : bool = prim::If(%4627) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4616)
          %4630 : int = aten::add(%4625, %27)
          %4631 : bool = aten::lt(%4630, %4617)
          %4632 : bool = aten::__and__(%4631, %4628)
          -> (%4632, %4627, %4629, %4630)
      %4633 : bool = prim::If(%4619)
        block0():
          -> (%4620)
        block1():
          -> (%19)
      -> (%4633)
    block1():
      -> (%19)
  %bottleneck_output.10 : Tensor = prim::If(%4615) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4613)
    block1():
      %concated_features.6 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4636 : __torch__.torch.nn.modules.conv.___torch_mangle_122.Conv2d = prim::GetAttr[name="conv1"](%4221)
      %4637 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_121.BatchNorm2d = prim::GetAttr[name="norm1"](%4221)
      %4638 : int = aten::dim(%concated_features.6) # torch/nn/modules/batchnorm.py:276:11
      %4639 : bool = aten::ne(%4638, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4639) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4640 : bool = prim::GetAttr[name="training"](%4637)
       = prim::If(%4640) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4641 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4637)
          %4642 : Tensor = aten::add(%4641, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4637, %4642)
          -> ()
        block1():
          -> ()
      %4643 : bool = prim::GetAttr[name="training"](%4637)
      %4644 : Tensor = prim::GetAttr[name="running_mean"](%4637)
      %4645 : Tensor = prim::GetAttr[name="running_var"](%4637)
      %4646 : Tensor = prim::GetAttr[name="weight"](%4637)
      %4647 : Tensor = prim::GetAttr[name="bias"](%4637)
       = prim::If(%4643) # torch/nn/functional.py:2011:4
        block0():
          %4648 : int[] = aten::size(%concated_features.6) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%4648, %24) # torch/nn/functional.py:1991:17
          %4650 : int = aten::len(%4648) # torch/nn/functional.py:1992:19
          %4651 : int = aten::sub(%4650, %26) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%4651, %25, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %4655 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
              %4656 : int = aten::__getitem__(%4648, %4655) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %4656) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.43)
          %4658 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4658) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4659 : Tensor = aten::batch_norm(%concated_features.6, %4646, %4647, %4644, %4645, %4643, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.11 : Tensor = aten::relu_(%4659) # torch/nn/functional.py:1117:17
      %4661 : Tensor = prim::GetAttr[name="weight"](%4636)
      %4662 : Tensor? = prim::GetAttr[name="bias"](%4636)
      %4663 : int[] = prim::ListConstruct(%27, %27)
      %4664 : int[] = prim::ListConstruct(%24, %24)
      %4665 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.11 : Tensor = aten::conv2d(%result.11, %4661, %4662, %4663, %4664, %4665, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.11)
  %4667 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4221)
  %4668 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4221)
  %4669 : int = aten::dim(%bottleneck_output.10) # torch/nn/modules/batchnorm.py:276:11
  %4670 : bool = aten::ne(%4669, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4670) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4671 : bool = prim::GetAttr[name="training"](%4668)
   = prim::If(%4671) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4672 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4668)
      %4673 : Tensor = aten::add(%4672, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4668, %4673)
      -> ()
    block1():
      -> ()
  %4674 : bool = prim::GetAttr[name="training"](%4668)
  %4675 : Tensor = prim::GetAttr[name="running_mean"](%4668)
  %4676 : Tensor = prim::GetAttr[name="running_var"](%4668)
  %4677 : Tensor = prim::GetAttr[name="weight"](%4668)
  %4678 : Tensor = prim::GetAttr[name="bias"](%4668)
   = prim::If(%4674) # torch/nn/functional.py:2011:4
    block0():
      %4679 : int[] = aten::size(%bottleneck_output.10) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%4679, %24) # torch/nn/functional.py:1991:17
      %4681 : int = aten::len(%4679) # torch/nn/functional.py:1992:19
      %4682 : int = aten::sub(%4681, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%4682, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %4686 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %4687 : int = aten::__getitem__(%4679, %4686) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %4687) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %4689 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4689) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4690 : Tensor = aten::batch_norm(%bottleneck_output.10, %4677, %4678, %4675, %4676, %4674, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.12 : Tensor = aten::relu_(%4690) # torch/nn/functional.py:1117:17
  %4692 : Tensor = prim::GetAttr[name="weight"](%4667)
  %4693 : Tensor? = prim::GetAttr[name="bias"](%4667)
  %4694 : int[] = prim::ListConstruct(%27, %27)
  %4695 : int[] = prim::ListConstruct(%27, %27)
  %4696 : int[] = prim::ListConstruct(%27, %27)
  %new_features.12 : Tensor = aten::conv2d(%result.12, %4692, %4693, %4694, %4695, %4696, %27) # torch/nn/modules/conv.py:415:15
  %4698 : float = prim::GetAttr[name="drop_rate"](%4221)
  %4699 : bool = aten::gt(%4698, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.9 : Tensor = prim::If(%4699) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4701 : float = prim::GetAttr[name="drop_rate"](%4221)
      %4702 : bool = prim::GetAttr[name="training"](%4221)
      %4703 : bool = aten::lt(%4701, %16) # torch/nn/functional.py:968:7
      %4704 : bool = prim::If(%4703) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4705 : bool = aten::gt(%4701, %17) # torch/nn/functional.py:968:17
          -> (%4705)
       = prim::If(%4704) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4706 : Tensor = aten::dropout(%new_features.12, %4701, %4702) # torch/nn/functional.py:973:17
      -> (%4706)
    block1():
      -> (%new_features.12)
  %4707 : Tensor[] = aten::append(%features.1, %new_features.9) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4708 : Tensor = prim::Uninitialized()
  %4709 : bool = prim::GetAttr[name="memory_efficient"](%4222)
  %4710 : bool = prim::If(%4709) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4711 : bool = prim::Uninitialized()
      %4712 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4713 : bool = aten::gt(%4712, %24)
      %4714 : bool, %4715 : bool, %4716 : int = prim::Loop(%18, %4713, %19, %4711, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4717 : int, %4718 : bool, %4719 : bool, %4720 : int):
          %tensor.7 : Tensor = aten::__getitem__(%features.1, %4720) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4722 : bool = prim::requires_grad(%tensor.7)
          %4723 : bool, %4724 : bool = prim::If(%4722) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4711)
          %4725 : int = aten::add(%4720, %27)
          %4726 : bool = aten::lt(%4725, %4712)
          %4727 : bool = aten::__and__(%4726, %4723)
          -> (%4727, %4722, %4724, %4725)
      %4728 : bool = prim::If(%4714)
        block0():
          -> (%4715)
        block1():
          -> (%19)
      -> (%4728)
    block1():
      -> (%19)
  %bottleneck_output.12 : Tensor = prim::If(%4710) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4708)
    block1():
      %concated_features.7 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4731 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv1"](%4222)
      %4732 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%4222)
      %4733 : int = aten::dim(%concated_features.7) # torch/nn/modules/batchnorm.py:276:11
      %4734 : bool = aten::ne(%4733, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4734) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4735 : bool = prim::GetAttr[name="training"](%4732)
       = prim::If(%4735) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4736 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4732)
          %4737 : Tensor = aten::add(%4736, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4732, %4737)
          -> ()
        block1():
          -> ()
      %4738 : bool = prim::GetAttr[name="training"](%4732)
      %4739 : Tensor = prim::GetAttr[name="running_mean"](%4732)
      %4740 : Tensor = prim::GetAttr[name="running_var"](%4732)
      %4741 : Tensor = prim::GetAttr[name="weight"](%4732)
      %4742 : Tensor = prim::GetAttr[name="bias"](%4732)
       = prim::If(%4738) # torch/nn/functional.py:2011:4
        block0():
          %4743 : int[] = aten::size(%concated_features.7) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%4743, %24) # torch/nn/functional.py:1991:17
          %4745 : int = aten::len(%4743) # torch/nn/functional.py:1992:19
          %4746 : int = aten::sub(%4745, %26) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%4746, %25, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %4750 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
              %4751 : int = aten::__getitem__(%4743, %4750) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %4751) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.51)
          %4753 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4753) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4754 : Tensor = aten::batch_norm(%concated_features.7, %4741, %4742, %4739, %4740, %4738, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.13 : Tensor = aten::relu_(%4754) # torch/nn/functional.py:1117:17
      %4756 : Tensor = prim::GetAttr[name="weight"](%4731)
      %4757 : Tensor? = prim::GetAttr[name="bias"](%4731)
      %4758 : int[] = prim::ListConstruct(%27, %27)
      %4759 : int[] = prim::ListConstruct(%24, %24)
      %4760 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.13 : Tensor = aten::conv2d(%result.13, %4756, %4757, %4758, %4759, %4760, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.13)
  %4762 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4222)
  %4763 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4222)
  %4764 : int = aten::dim(%bottleneck_output.12) # torch/nn/modules/batchnorm.py:276:11
  %4765 : bool = aten::ne(%4764, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4765) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4766 : bool = prim::GetAttr[name="training"](%4763)
   = prim::If(%4766) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4767 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4763)
      %4768 : Tensor = aten::add(%4767, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4763, %4768)
      -> ()
    block1():
      -> ()
  %4769 : bool = prim::GetAttr[name="training"](%4763)
  %4770 : Tensor = prim::GetAttr[name="running_mean"](%4763)
  %4771 : Tensor = prim::GetAttr[name="running_var"](%4763)
  %4772 : Tensor = prim::GetAttr[name="weight"](%4763)
  %4773 : Tensor = prim::GetAttr[name="bias"](%4763)
   = prim::If(%4769) # torch/nn/functional.py:2011:4
    block0():
      %4774 : int[] = aten::size(%bottleneck_output.12) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%4774, %24) # torch/nn/functional.py:1991:17
      %4776 : int = aten::len(%4774) # torch/nn/functional.py:1992:19
      %4777 : int = aten::sub(%4776, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%4777, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %4781 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %4782 : int = aten::__getitem__(%4774, %4781) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %4782) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %4784 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4784) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4785 : Tensor = aten::batch_norm(%bottleneck_output.12, %4772, %4773, %4770, %4771, %4769, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.14 : Tensor = aten::relu_(%4785) # torch/nn/functional.py:1117:17
  %4787 : Tensor = prim::GetAttr[name="weight"](%4762)
  %4788 : Tensor? = prim::GetAttr[name="bias"](%4762)
  %4789 : int[] = prim::ListConstruct(%27, %27)
  %4790 : int[] = prim::ListConstruct(%27, %27)
  %4791 : int[] = prim::ListConstruct(%27, %27)
  %new_features.14 : Tensor = aten::conv2d(%result.14, %4787, %4788, %4789, %4790, %4791, %27) # torch/nn/modules/conv.py:415:15
  %4793 : float = prim::GetAttr[name="drop_rate"](%4222)
  %4794 : bool = aten::gt(%4793, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.11 : Tensor = prim::If(%4794) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4796 : float = prim::GetAttr[name="drop_rate"](%4222)
      %4797 : bool = prim::GetAttr[name="training"](%4222)
      %4798 : bool = aten::lt(%4796, %16) # torch/nn/functional.py:968:7
      %4799 : bool = prim::If(%4798) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4800 : bool = aten::gt(%4796, %17) # torch/nn/functional.py:968:17
          -> (%4800)
       = prim::If(%4799) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4801 : Tensor = aten::dropout(%new_features.14, %4796, %4797) # torch/nn/functional.py:973:17
      -> (%4801)
    block1():
      -> (%new_features.14)
  %4802 : Tensor[] = aten::append(%features.1, %new_features.11) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4803 : Tensor = prim::Uninitialized()
  %4804 : bool = prim::GetAttr[name="memory_efficient"](%4223)
  %4805 : bool = prim::If(%4804) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4806 : bool = prim::Uninitialized()
      %4807 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4808 : bool = aten::gt(%4807, %24)
      %4809 : bool, %4810 : bool, %4811 : int = prim::Loop(%18, %4808, %19, %4806, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4812 : int, %4813 : bool, %4814 : bool, %4815 : int):
          %tensor.8 : Tensor = aten::__getitem__(%features.1, %4815) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4817 : bool = prim::requires_grad(%tensor.8)
          %4818 : bool, %4819 : bool = prim::If(%4817) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4806)
          %4820 : int = aten::add(%4815, %27)
          %4821 : bool = aten::lt(%4820, %4807)
          %4822 : bool = aten::__and__(%4821, %4818)
          -> (%4822, %4817, %4819, %4820)
      %4823 : bool = prim::If(%4809)
        block0():
          -> (%4810)
        block1():
          -> (%19)
      -> (%4823)
    block1():
      -> (%19)
  %bottleneck_output.14 : Tensor = prim::If(%4805) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4803)
    block1():
      %concated_features.8 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4826 : __torch__.torch.nn.modules.conv.___torch_mangle_128.Conv2d = prim::GetAttr[name="conv1"](%4223)
      %4827 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="norm1"](%4223)
      %4828 : int = aten::dim(%concated_features.8) # torch/nn/modules/batchnorm.py:276:11
      %4829 : bool = aten::ne(%4828, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4829) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4830 : bool = prim::GetAttr[name="training"](%4827)
       = prim::If(%4830) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4831 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4827)
          %4832 : Tensor = aten::add(%4831, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4827, %4832)
          -> ()
        block1():
          -> ()
      %4833 : bool = prim::GetAttr[name="training"](%4827)
      %4834 : Tensor = prim::GetAttr[name="running_mean"](%4827)
      %4835 : Tensor = prim::GetAttr[name="running_var"](%4827)
      %4836 : Tensor = prim::GetAttr[name="weight"](%4827)
      %4837 : Tensor = prim::GetAttr[name="bias"](%4827)
       = prim::If(%4833) # torch/nn/functional.py:2011:4
        block0():
          %4838 : int[] = aten::size(%concated_features.8) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%4838, %24) # torch/nn/functional.py:1991:17
          %4840 : int = aten::len(%4838) # torch/nn/functional.py:1992:19
          %4841 : int = aten::sub(%4840, %26) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%4841, %25, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %4845 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
              %4846 : int = aten::__getitem__(%4838, %4845) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %4846) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.59)
          %4848 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4848) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4849 : Tensor = aten::batch_norm(%concated_features.8, %4836, %4837, %4834, %4835, %4833, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.15 : Tensor = aten::relu_(%4849) # torch/nn/functional.py:1117:17
      %4851 : Tensor = prim::GetAttr[name="weight"](%4826)
      %4852 : Tensor? = prim::GetAttr[name="bias"](%4826)
      %4853 : int[] = prim::ListConstruct(%27, %27)
      %4854 : int[] = prim::ListConstruct(%24, %24)
      %4855 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.15 : Tensor = aten::conv2d(%result.15, %4851, %4852, %4853, %4854, %4855, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.15)
  %4857 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4223)
  %4858 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4223)
  %4859 : int = aten::dim(%bottleneck_output.14) # torch/nn/modules/batchnorm.py:276:11
  %4860 : bool = aten::ne(%4859, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4860) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4861 : bool = prim::GetAttr[name="training"](%4858)
   = prim::If(%4861) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4862 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4858)
      %4863 : Tensor = aten::add(%4862, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4858, %4863)
      -> ()
    block1():
      -> ()
  %4864 : bool = prim::GetAttr[name="training"](%4858)
  %4865 : Tensor = prim::GetAttr[name="running_mean"](%4858)
  %4866 : Tensor = prim::GetAttr[name="running_var"](%4858)
  %4867 : Tensor = prim::GetAttr[name="weight"](%4858)
  %4868 : Tensor = prim::GetAttr[name="bias"](%4858)
   = prim::If(%4864) # torch/nn/functional.py:2011:4
    block0():
      %4869 : int[] = aten::size(%bottleneck_output.14) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%4869, %24) # torch/nn/functional.py:1991:17
      %4871 : int = aten::len(%4869) # torch/nn/functional.py:1992:19
      %4872 : int = aten::sub(%4871, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%4872, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %4876 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %4877 : int = aten::__getitem__(%4869, %4876) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %4877) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %4879 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4879) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4880 : Tensor = aten::batch_norm(%bottleneck_output.14, %4867, %4868, %4865, %4866, %4864, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.16 : Tensor = aten::relu_(%4880) # torch/nn/functional.py:1117:17
  %4882 : Tensor = prim::GetAttr[name="weight"](%4857)
  %4883 : Tensor? = prim::GetAttr[name="bias"](%4857)
  %4884 : int[] = prim::ListConstruct(%27, %27)
  %4885 : int[] = prim::ListConstruct(%27, %27)
  %4886 : int[] = prim::ListConstruct(%27, %27)
  %new_features.16 : Tensor = aten::conv2d(%result.16, %4882, %4883, %4884, %4885, %4886, %27) # torch/nn/modules/conv.py:415:15
  %4888 : float = prim::GetAttr[name="drop_rate"](%4223)
  %4889 : bool = aten::gt(%4888, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.13 : Tensor = prim::If(%4889) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4891 : float = prim::GetAttr[name="drop_rate"](%4223)
      %4892 : bool = prim::GetAttr[name="training"](%4223)
      %4893 : bool = aten::lt(%4891, %16) # torch/nn/functional.py:968:7
      %4894 : bool = prim::If(%4893) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4895 : bool = aten::gt(%4891, %17) # torch/nn/functional.py:968:17
          -> (%4895)
       = prim::If(%4894) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4896 : Tensor = aten::dropout(%new_features.16, %4891, %4892) # torch/nn/functional.py:973:17
      -> (%4896)
    block1():
      -> (%new_features.16)
  %4897 : Tensor[] = aten::append(%features.1, %new_features.13) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4898 : Tensor = prim::Uninitialized()
  %4899 : bool = prim::GetAttr[name="memory_efficient"](%4224)
  %4900 : bool = prim::If(%4899) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4901 : bool = prim::Uninitialized()
      %4902 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4903 : bool = aten::gt(%4902, %24)
      %4904 : bool, %4905 : bool, %4906 : int = prim::Loop(%18, %4903, %19, %4901, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4907 : int, %4908 : bool, %4909 : bool, %4910 : int):
          %tensor.9 : Tensor = aten::__getitem__(%features.1, %4910) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4912 : bool = prim::requires_grad(%tensor.9)
          %4913 : bool, %4914 : bool = prim::If(%4912) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4901)
          %4915 : int = aten::add(%4910, %27)
          %4916 : bool = aten::lt(%4915, %4902)
          %4917 : bool = aten::__and__(%4916, %4913)
          -> (%4917, %4912, %4914, %4915)
      %4918 : bool = prim::If(%4904)
        block0():
          -> (%4905)
        block1():
          -> (%19)
      -> (%4918)
    block1():
      -> (%19)
  %bottleneck_output.16 : Tensor = prim::If(%4900) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4898)
    block1():
      %concated_features.9 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4921 : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d = prim::GetAttr[name="conv1"](%4224)
      %4922 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_130.BatchNorm2d = prim::GetAttr[name="norm1"](%4224)
      %4923 : int = aten::dim(%concated_features.9) # torch/nn/modules/batchnorm.py:276:11
      %4924 : bool = aten::ne(%4923, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4924) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4925 : bool = prim::GetAttr[name="training"](%4922)
       = prim::If(%4925) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4926 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4922)
          %4927 : Tensor = aten::add(%4926, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4922, %4927)
          -> ()
        block1():
          -> ()
      %4928 : bool = prim::GetAttr[name="training"](%4922)
      %4929 : Tensor = prim::GetAttr[name="running_mean"](%4922)
      %4930 : Tensor = prim::GetAttr[name="running_var"](%4922)
      %4931 : Tensor = prim::GetAttr[name="weight"](%4922)
      %4932 : Tensor = prim::GetAttr[name="bias"](%4922)
       = prim::If(%4928) # torch/nn/functional.py:2011:4
        block0():
          %4933 : int[] = aten::size(%concated_features.9) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%4933, %24) # torch/nn/functional.py:1991:17
          %4935 : int = aten::len(%4933) # torch/nn/functional.py:1992:19
          %4936 : int = aten::sub(%4935, %26) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%4936, %25, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %4940 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
              %4941 : int = aten::__getitem__(%4933, %4940) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %4941) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.67)
          %4943 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4943) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4944 : Tensor = aten::batch_norm(%concated_features.9, %4931, %4932, %4929, %4930, %4928, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.17 : Tensor = aten::relu_(%4944) # torch/nn/functional.py:1117:17
      %4946 : Tensor = prim::GetAttr[name="weight"](%4921)
      %4947 : Tensor? = prim::GetAttr[name="bias"](%4921)
      %4948 : int[] = prim::ListConstruct(%27, %27)
      %4949 : int[] = prim::ListConstruct(%24, %24)
      %4950 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.17 : Tensor = aten::conv2d(%result.17, %4946, %4947, %4948, %4949, %4950, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.17)
  %4952 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4224)
  %4953 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4224)
  %4954 : int = aten::dim(%bottleneck_output.16) # torch/nn/modules/batchnorm.py:276:11
  %4955 : bool = aten::ne(%4954, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4955) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4956 : bool = prim::GetAttr[name="training"](%4953)
   = prim::If(%4956) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4957 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4953)
      %4958 : Tensor = aten::add(%4957, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4953, %4958)
      -> ()
    block1():
      -> ()
  %4959 : bool = prim::GetAttr[name="training"](%4953)
  %4960 : Tensor = prim::GetAttr[name="running_mean"](%4953)
  %4961 : Tensor = prim::GetAttr[name="running_var"](%4953)
  %4962 : Tensor = prim::GetAttr[name="weight"](%4953)
  %4963 : Tensor = prim::GetAttr[name="bias"](%4953)
   = prim::If(%4959) # torch/nn/functional.py:2011:4
    block0():
      %4964 : int[] = aten::size(%bottleneck_output.16) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%4964, %24) # torch/nn/functional.py:1991:17
      %4966 : int = aten::len(%4964) # torch/nn/functional.py:1992:19
      %4967 : int = aten::sub(%4966, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%4967, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %4971 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %4972 : int = aten::__getitem__(%4964, %4971) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %4972) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %4974 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4974) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4975 : Tensor = aten::batch_norm(%bottleneck_output.16, %4962, %4963, %4960, %4961, %4959, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.18 : Tensor = aten::relu_(%4975) # torch/nn/functional.py:1117:17
  %4977 : Tensor = prim::GetAttr[name="weight"](%4952)
  %4978 : Tensor? = prim::GetAttr[name="bias"](%4952)
  %4979 : int[] = prim::ListConstruct(%27, %27)
  %4980 : int[] = prim::ListConstruct(%27, %27)
  %4981 : int[] = prim::ListConstruct(%27, %27)
  %new_features.18 : Tensor = aten::conv2d(%result.18, %4977, %4978, %4979, %4980, %4981, %27) # torch/nn/modules/conv.py:415:15
  %4983 : float = prim::GetAttr[name="drop_rate"](%4224)
  %4984 : bool = aten::gt(%4983, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.15 : Tensor = prim::If(%4984) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4986 : float = prim::GetAttr[name="drop_rate"](%4224)
      %4987 : bool = prim::GetAttr[name="training"](%4224)
      %4988 : bool = aten::lt(%4986, %16) # torch/nn/functional.py:968:7
      %4989 : bool = prim::If(%4988) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4990 : bool = aten::gt(%4986, %17) # torch/nn/functional.py:968:17
          -> (%4990)
       = prim::If(%4989) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4991 : Tensor = aten::dropout(%new_features.18, %4986, %4987) # torch/nn/functional.py:973:17
      -> (%4991)
    block1():
      -> (%new_features.18)
  %4992 : Tensor[] = aten::append(%features.1, %new_features.15) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4993 : Tensor = prim::Uninitialized()
  %4994 : bool = prim::GetAttr[name="memory_efficient"](%4225)
  %4995 : bool = prim::If(%4994) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4996 : bool = prim::Uninitialized()
      %4997 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4998 : bool = aten::gt(%4997, %24)
      %4999 : bool, %5000 : bool, %5001 : int = prim::Loop(%18, %4998, %19, %4996, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5002 : int, %5003 : bool, %5004 : bool, %5005 : int):
          %tensor.10 : Tensor = aten::__getitem__(%features.1, %5005) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5007 : bool = prim::requires_grad(%tensor.10)
          %5008 : bool, %5009 : bool = prim::If(%5007) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4996)
          %5010 : int = aten::add(%5005, %27)
          %5011 : bool = aten::lt(%5010, %4997)
          %5012 : bool = aten::__and__(%5011, %5008)
          -> (%5012, %5007, %5009, %5010)
      %5013 : bool = prim::If(%4999)
        block0():
          -> (%5000)
        block1():
          -> (%19)
      -> (%5013)
    block1():
      -> (%19)
  %bottleneck_output.18 : Tensor = prim::If(%4995) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4993)
    block1():
      %concated_features.10 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5016 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%4225)
      %5017 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%4225)
      %5018 : int = aten::dim(%concated_features.10) # torch/nn/modules/batchnorm.py:276:11
      %5019 : bool = aten::ne(%5018, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5019) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5020 : bool = prim::GetAttr[name="training"](%5017)
       = prim::If(%5020) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5021 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5017)
          %5022 : Tensor = aten::add(%5021, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5017, %5022)
          -> ()
        block1():
          -> ()
      %5023 : bool = prim::GetAttr[name="training"](%5017)
      %5024 : Tensor = prim::GetAttr[name="running_mean"](%5017)
      %5025 : Tensor = prim::GetAttr[name="running_var"](%5017)
      %5026 : Tensor = prim::GetAttr[name="weight"](%5017)
      %5027 : Tensor = prim::GetAttr[name="bias"](%5017)
       = prim::If(%5023) # torch/nn/functional.py:2011:4
        block0():
          %5028 : int[] = aten::size(%concated_features.10) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%5028, %24) # torch/nn/functional.py:1991:17
          %5030 : int = aten::len(%5028) # torch/nn/functional.py:1992:19
          %5031 : int = aten::sub(%5030, %26) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%5031, %25, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %5035 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
              %5036 : int = aten::__getitem__(%5028, %5035) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %5036) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.75)
          %5038 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5038) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5039 : Tensor = aten::batch_norm(%concated_features.10, %5026, %5027, %5024, %5025, %5023, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.19 : Tensor = aten::relu_(%5039) # torch/nn/functional.py:1117:17
      %5041 : Tensor = prim::GetAttr[name="weight"](%5016)
      %5042 : Tensor? = prim::GetAttr[name="bias"](%5016)
      %5043 : int[] = prim::ListConstruct(%27, %27)
      %5044 : int[] = prim::ListConstruct(%24, %24)
      %5045 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.19 : Tensor = aten::conv2d(%result.19, %5041, %5042, %5043, %5044, %5045, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.19)
  %5047 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4225)
  %5048 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4225)
  %5049 : int = aten::dim(%bottleneck_output.18) # torch/nn/modules/batchnorm.py:276:11
  %5050 : bool = aten::ne(%5049, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5050) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5051 : bool = prim::GetAttr[name="training"](%5048)
   = prim::If(%5051) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5052 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5048)
      %5053 : Tensor = aten::add(%5052, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5048, %5053)
      -> ()
    block1():
      -> ()
  %5054 : bool = prim::GetAttr[name="training"](%5048)
  %5055 : Tensor = prim::GetAttr[name="running_mean"](%5048)
  %5056 : Tensor = prim::GetAttr[name="running_var"](%5048)
  %5057 : Tensor = prim::GetAttr[name="weight"](%5048)
  %5058 : Tensor = prim::GetAttr[name="bias"](%5048)
   = prim::If(%5054) # torch/nn/functional.py:2011:4
    block0():
      %5059 : int[] = aten::size(%bottleneck_output.18) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%5059, %24) # torch/nn/functional.py:1991:17
      %5061 : int = aten::len(%5059) # torch/nn/functional.py:1992:19
      %5062 : int = aten::sub(%5061, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%5062, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %5066 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %5067 : int = aten::__getitem__(%5059, %5066) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %5067) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %5069 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5069) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5070 : Tensor = aten::batch_norm(%bottleneck_output.18, %5057, %5058, %5055, %5056, %5054, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.20 : Tensor = aten::relu_(%5070) # torch/nn/functional.py:1117:17
  %5072 : Tensor = prim::GetAttr[name="weight"](%5047)
  %5073 : Tensor? = prim::GetAttr[name="bias"](%5047)
  %5074 : int[] = prim::ListConstruct(%27, %27)
  %5075 : int[] = prim::ListConstruct(%27, %27)
  %5076 : int[] = prim::ListConstruct(%27, %27)
  %new_features.20 : Tensor = aten::conv2d(%result.20, %5072, %5073, %5074, %5075, %5076, %27) # torch/nn/modules/conv.py:415:15
  %5078 : float = prim::GetAttr[name="drop_rate"](%4225)
  %5079 : bool = aten::gt(%5078, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.17 : Tensor = prim::If(%5079) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5081 : float = prim::GetAttr[name="drop_rate"](%4225)
      %5082 : bool = prim::GetAttr[name="training"](%4225)
      %5083 : bool = aten::lt(%5081, %16) # torch/nn/functional.py:968:7
      %5084 : bool = prim::If(%5083) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5085 : bool = aten::gt(%5081, %17) # torch/nn/functional.py:968:17
          -> (%5085)
       = prim::If(%5084) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5086 : Tensor = aten::dropout(%new_features.20, %5081, %5082) # torch/nn/functional.py:973:17
      -> (%5086)
    block1():
      -> (%new_features.20)
  %5087 : Tensor[] = aten::append(%features.1, %new_features.17) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5088 : Tensor = prim::Uninitialized()
  %5089 : bool = prim::GetAttr[name="memory_efficient"](%4226)
  %5090 : bool = prim::If(%5089) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5091 : bool = prim::Uninitialized()
      %5092 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5093 : bool = aten::gt(%5092, %24)
      %5094 : bool, %5095 : bool, %5096 : int = prim::Loop(%18, %5093, %19, %5091, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5097 : int, %5098 : bool, %5099 : bool, %5100 : int):
          %tensor.11 : Tensor = aten::__getitem__(%features.1, %5100) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5102 : bool = prim::requires_grad(%tensor.11)
          %5103 : bool, %5104 : bool = prim::If(%5102) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5091)
          %5105 : int = aten::add(%5100, %27)
          %5106 : bool = aten::lt(%5105, %5092)
          %5107 : bool = aten::__and__(%5106, %5103)
          -> (%5107, %5102, %5104, %5105)
      %5108 : bool = prim::If(%5094)
        block0():
          -> (%5095)
        block1():
          -> (%19)
      -> (%5108)
    block1():
      -> (%19)
  %bottleneck_output.20 : Tensor = prim::If(%5090) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5088)
    block1():
      %concated_features.11 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5111 : __torch__.torch.nn.modules.conv.___torch_mangle_137.Conv2d = prim::GetAttr[name="conv1"](%4226)
      %5112 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_136.BatchNorm2d = prim::GetAttr[name="norm1"](%4226)
      %5113 : int = aten::dim(%concated_features.11) # torch/nn/modules/batchnorm.py:276:11
      %5114 : bool = aten::ne(%5113, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5114) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5115 : bool = prim::GetAttr[name="training"](%5112)
       = prim::If(%5115) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5116 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5112)
          %5117 : Tensor = aten::add(%5116, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5112, %5117)
          -> ()
        block1():
          -> ()
      %5118 : bool = prim::GetAttr[name="training"](%5112)
      %5119 : Tensor = prim::GetAttr[name="running_mean"](%5112)
      %5120 : Tensor = prim::GetAttr[name="running_var"](%5112)
      %5121 : Tensor = prim::GetAttr[name="weight"](%5112)
      %5122 : Tensor = prim::GetAttr[name="bias"](%5112)
       = prim::If(%5118) # torch/nn/functional.py:2011:4
        block0():
          %5123 : int[] = aten::size(%concated_features.11) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%5123, %24) # torch/nn/functional.py:1991:17
          %5125 : int = aten::len(%5123) # torch/nn/functional.py:1992:19
          %5126 : int = aten::sub(%5125, %26) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%5126, %25, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %5130 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
              %5131 : int = aten::__getitem__(%5123, %5130) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %5131) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.83)
          %5133 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5133) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5134 : Tensor = aten::batch_norm(%concated_features.11, %5121, %5122, %5119, %5120, %5118, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.21 : Tensor = aten::relu_(%5134) # torch/nn/functional.py:1117:17
      %5136 : Tensor = prim::GetAttr[name="weight"](%5111)
      %5137 : Tensor? = prim::GetAttr[name="bias"](%5111)
      %5138 : int[] = prim::ListConstruct(%27, %27)
      %5139 : int[] = prim::ListConstruct(%24, %24)
      %5140 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.21 : Tensor = aten::conv2d(%result.21, %5136, %5137, %5138, %5139, %5140, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.21)
  %5142 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4226)
  %5143 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4226)
  %5144 : int = aten::dim(%bottleneck_output.20) # torch/nn/modules/batchnorm.py:276:11
  %5145 : bool = aten::ne(%5144, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5145) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5146 : bool = prim::GetAttr[name="training"](%5143)
   = prim::If(%5146) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5147 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5143)
      %5148 : Tensor = aten::add(%5147, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5143, %5148)
      -> ()
    block1():
      -> ()
  %5149 : bool = prim::GetAttr[name="training"](%5143)
  %5150 : Tensor = prim::GetAttr[name="running_mean"](%5143)
  %5151 : Tensor = prim::GetAttr[name="running_var"](%5143)
  %5152 : Tensor = prim::GetAttr[name="weight"](%5143)
  %5153 : Tensor = prim::GetAttr[name="bias"](%5143)
   = prim::If(%5149) # torch/nn/functional.py:2011:4
    block0():
      %5154 : int[] = aten::size(%bottleneck_output.20) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%5154, %24) # torch/nn/functional.py:1991:17
      %5156 : int = aten::len(%5154) # torch/nn/functional.py:1992:19
      %5157 : int = aten::sub(%5156, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%5157, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %5161 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %5162 : int = aten::__getitem__(%5154, %5161) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %5162) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %5164 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5164) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5165 : Tensor = aten::batch_norm(%bottleneck_output.20, %5152, %5153, %5150, %5151, %5149, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.22 : Tensor = aten::relu_(%5165) # torch/nn/functional.py:1117:17
  %5167 : Tensor = prim::GetAttr[name="weight"](%5142)
  %5168 : Tensor? = prim::GetAttr[name="bias"](%5142)
  %5169 : int[] = prim::ListConstruct(%27, %27)
  %5170 : int[] = prim::ListConstruct(%27, %27)
  %5171 : int[] = prim::ListConstruct(%27, %27)
  %new_features.22 : Tensor = aten::conv2d(%result.22, %5167, %5168, %5169, %5170, %5171, %27) # torch/nn/modules/conv.py:415:15
  %5173 : float = prim::GetAttr[name="drop_rate"](%4226)
  %5174 : bool = aten::gt(%5173, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.19 : Tensor = prim::If(%5174) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5176 : float = prim::GetAttr[name="drop_rate"](%4226)
      %5177 : bool = prim::GetAttr[name="training"](%4226)
      %5178 : bool = aten::lt(%5176, %16) # torch/nn/functional.py:968:7
      %5179 : bool = prim::If(%5178) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5180 : bool = aten::gt(%5176, %17) # torch/nn/functional.py:968:17
          -> (%5180)
       = prim::If(%5179) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5181 : Tensor = aten::dropout(%new_features.22, %5176, %5177) # torch/nn/functional.py:973:17
      -> (%5181)
    block1():
      -> (%new_features.22)
  %5182 : Tensor[] = aten::append(%features.1, %new_features.19) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5183 : Tensor = prim::Uninitialized()
  %5184 : bool = prim::GetAttr[name="memory_efficient"](%4227)
  %5185 : bool = prim::If(%5184) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5186 : bool = prim::Uninitialized()
      %5187 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5188 : bool = aten::gt(%5187, %24)
      %5189 : bool, %5190 : bool, %5191 : int = prim::Loop(%18, %5188, %19, %5186, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5192 : int, %5193 : bool, %5194 : bool, %5195 : int):
          %tensor.12 : Tensor = aten::__getitem__(%features.1, %5195) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5197 : bool = prim::requires_grad(%tensor.12)
          %5198 : bool, %5199 : bool = prim::If(%5197) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5186)
          %5200 : int = aten::add(%5195, %27)
          %5201 : bool = aten::lt(%5200, %5187)
          %5202 : bool = aten::__and__(%5201, %5198)
          -> (%5202, %5197, %5199, %5200)
      %5203 : bool = prim::If(%5189)
        block0():
          -> (%5190)
        block1():
          -> (%19)
      -> (%5203)
    block1():
      -> (%19)
  %bottleneck_output.22 : Tensor = prim::If(%5185) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5183)
    block1():
      %concated_features.12 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5206 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv1"](%4227)
      %5207 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="norm1"](%4227)
      %5208 : int = aten::dim(%concated_features.12) # torch/nn/modules/batchnorm.py:276:11
      %5209 : bool = aten::ne(%5208, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5209) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5210 : bool = prim::GetAttr[name="training"](%5207)
       = prim::If(%5210) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5211 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5207)
          %5212 : Tensor = aten::add(%5211, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5207, %5212)
          -> ()
        block1():
          -> ()
      %5213 : bool = prim::GetAttr[name="training"](%5207)
      %5214 : Tensor = prim::GetAttr[name="running_mean"](%5207)
      %5215 : Tensor = prim::GetAttr[name="running_var"](%5207)
      %5216 : Tensor = prim::GetAttr[name="weight"](%5207)
      %5217 : Tensor = prim::GetAttr[name="bias"](%5207)
       = prim::If(%5213) # torch/nn/functional.py:2011:4
        block0():
          %5218 : int[] = aten::size(%concated_features.12) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%5218, %24) # torch/nn/functional.py:1991:17
          %5220 : int = aten::len(%5218) # torch/nn/functional.py:1992:19
          %5221 : int = aten::sub(%5220, %26) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%5221, %25, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %5225 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
              %5226 : int = aten::__getitem__(%5218, %5225) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %5226) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.91)
          %5228 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5228) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5229 : Tensor = aten::batch_norm(%concated_features.12, %5216, %5217, %5214, %5215, %5213, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.23 : Tensor = aten::relu_(%5229) # torch/nn/functional.py:1117:17
      %5231 : Tensor = prim::GetAttr[name="weight"](%5206)
      %5232 : Tensor? = prim::GetAttr[name="bias"](%5206)
      %5233 : int[] = prim::ListConstruct(%27, %27)
      %5234 : int[] = prim::ListConstruct(%24, %24)
      %5235 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.23 : Tensor = aten::conv2d(%result.23, %5231, %5232, %5233, %5234, %5235, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.23)
  %5237 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4227)
  %5238 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4227)
  %5239 : int = aten::dim(%bottleneck_output.22) # torch/nn/modules/batchnorm.py:276:11
  %5240 : bool = aten::ne(%5239, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5240) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5241 : bool = prim::GetAttr[name="training"](%5238)
   = prim::If(%5241) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5242 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5238)
      %5243 : Tensor = aten::add(%5242, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5238, %5243)
      -> ()
    block1():
      -> ()
  %5244 : bool = prim::GetAttr[name="training"](%5238)
  %5245 : Tensor = prim::GetAttr[name="running_mean"](%5238)
  %5246 : Tensor = prim::GetAttr[name="running_var"](%5238)
  %5247 : Tensor = prim::GetAttr[name="weight"](%5238)
  %5248 : Tensor = prim::GetAttr[name="bias"](%5238)
   = prim::If(%5244) # torch/nn/functional.py:2011:4
    block0():
      %5249 : int[] = aten::size(%bottleneck_output.22) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%5249, %24) # torch/nn/functional.py:1991:17
      %5251 : int = aten::len(%5249) # torch/nn/functional.py:1992:19
      %5252 : int = aten::sub(%5251, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%5252, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %5256 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %5257 : int = aten::__getitem__(%5249, %5256) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %5257) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %5259 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5259) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5260 : Tensor = aten::batch_norm(%bottleneck_output.22, %5247, %5248, %5245, %5246, %5244, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.24 : Tensor = aten::relu_(%5260) # torch/nn/functional.py:1117:17
  %5262 : Tensor = prim::GetAttr[name="weight"](%5237)
  %5263 : Tensor? = prim::GetAttr[name="bias"](%5237)
  %5264 : int[] = prim::ListConstruct(%27, %27)
  %5265 : int[] = prim::ListConstruct(%27, %27)
  %5266 : int[] = prim::ListConstruct(%27, %27)
  %new_features.24 : Tensor = aten::conv2d(%result.24, %5262, %5263, %5264, %5265, %5266, %27) # torch/nn/modules/conv.py:415:15
  %5268 : float = prim::GetAttr[name="drop_rate"](%4227)
  %5269 : bool = aten::gt(%5268, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.21 : Tensor = prim::If(%5269) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5271 : float = prim::GetAttr[name="drop_rate"](%4227)
      %5272 : bool = prim::GetAttr[name="training"](%4227)
      %5273 : bool = aten::lt(%5271, %16) # torch/nn/functional.py:968:7
      %5274 : bool = prim::If(%5273) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5275 : bool = aten::gt(%5271, %17) # torch/nn/functional.py:968:17
          -> (%5275)
       = prim::If(%5274) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5276 : Tensor = aten::dropout(%new_features.24, %5271, %5272) # torch/nn/functional.py:973:17
      -> (%5276)
    block1():
      -> (%new_features.24)
  %5277 : Tensor[] = aten::append(%features.1, %new_features.21) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5278 : Tensor = prim::Uninitialized()
  %5279 : bool = prim::GetAttr[name="memory_efficient"](%4228)
  %5280 : bool = prim::If(%5279) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5281 : bool = prim::Uninitialized()
      %5282 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5283 : bool = aten::gt(%5282, %24)
      %5284 : bool, %5285 : bool, %5286 : int = prim::Loop(%18, %5283, %19, %5281, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5287 : int, %5288 : bool, %5289 : bool, %5290 : int):
          %tensor.13 : Tensor = aten::__getitem__(%features.1, %5290) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5292 : bool = prim::requires_grad(%tensor.13)
          %5293 : bool, %5294 : bool = prim::If(%5292) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5281)
          %5295 : int = aten::add(%5290, %27)
          %5296 : bool = aten::lt(%5295, %5282)
          %5297 : bool = aten::__and__(%5296, %5293)
          -> (%5297, %5292, %5294, %5295)
      %5298 : bool = prim::If(%5284)
        block0():
          -> (%5285)
        block1():
          -> (%19)
      -> (%5298)
    block1():
      -> (%19)
  %bottleneck_output.24 : Tensor = prim::If(%5280) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5278)
    block1():
      %concated_features.13 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5301 : __torch__.torch.nn.modules.conv.___torch_mangle_143.Conv2d = prim::GetAttr[name="conv1"](%4228)
      %5302 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%4228)
      %5303 : int = aten::dim(%concated_features.13) # torch/nn/modules/batchnorm.py:276:11
      %5304 : bool = aten::ne(%5303, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5304) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5305 : bool = prim::GetAttr[name="training"](%5302)
       = prim::If(%5305) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5306 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5302)
          %5307 : Tensor = aten::add(%5306, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5302, %5307)
          -> ()
        block1():
          -> ()
      %5308 : bool = prim::GetAttr[name="training"](%5302)
      %5309 : Tensor = prim::GetAttr[name="running_mean"](%5302)
      %5310 : Tensor = prim::GetAttr[name="running_var"](%5302)
      %5311 : Tensor = prim::GetAttr[name="weight"](%5302)
      %5312 : Tensor = prim::GetAttr[name="bias"](%5302)
       = prim::If(%5308) # torch/nn/functional.py:2011:4
        block0():
          %5313 : int[] = aten::size(%concated_features.13) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%5313, %24) # torch/nn/functional.py:1991:17
          %5315 : int = aten::len(%5313) # torch/nn/functional.py:1992:19
          %5316 : int = aten::sub(%5315, %26) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%5316, %25, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %5320 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
              %5321 : int = aten::__getitem__(%5313, %5320) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %5321) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.99)
          %5323 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5323) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5324 : Tensor = aten::batch_norm(%concated_features.13, %5311, %5312, %5309, %5310, %5308, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.25 : Tensor = aten::relu_(%5324) # torch/nn/functional.py:1117:17
      %5326 : Tensor = prim::GetAttr[name="weight"](%5301)
      %5327 : Tensor? = prim::GetAttr[name="bias"](%5301)
      %5328 : int[] = prim::ListConstruct(%27, %27)
      %5329 : int[] = prim::ListConstruct(%24, %24)
      %5330 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.25 : Tensor = aten::conv2d(%result.25, %5326, %5327, %5328, %5329, %5330, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.25)
  %5332 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4228)
  %5333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4228)
  %5334 : int = aten::dim(%bottleneck_output.24) # torch/nn/modules/batchnorm.py:276:11
  %5335 : bool = aten::ne(%5334, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5335) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5336 : bool = prim::GetAttr[name="training"](%5333)
   = prim::If(%5336) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5337 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5333)
      %5338 : Tensor = aten::add(%5337, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5333, %5338)
      -> ()
    block1():
      -> ()
  %5339 : bool = prim::GetAttr[name="training"](%5333)
  %5340 : Tensor = prim::GetAttr[name="running_mean"](%5333)
  %5341 : Tensor = prim::GetAttr[name="running_var"](%5333)
  %5342 : Tensor = prim::GetAttr[name="weight"](%5333)
  %5343 : Tensor = prim::GetAttr[name="bias"](%5333)
   = prim::If(%5339) # torch/nn/functional.py:2011:4
    block0():
      %5344 : int[] = aten::size(%bottleneck_output.24) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%5344, %24) # torch/nn/functional.py:1991:17
      %5346 : int = aten::len(%5344) # torch/nn/functional.py:1992:19
      %5347 : int = aten::sub(%5346, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%5347, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %5351 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %5352 : int = aten::__getitem__(%5344, %5351) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %5352) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %5354 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5354) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5355 : Tensor = aten::batch_norm(%bottleneck_output.24, %5342, %5343, %5340, %5341, %5339, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.26 : Tensor = aten::relu_(%5355) # torch/nn/functional.py:1117:17
  %5357 : Tensor = prim::GetAttr[name="weight"](%5332)
  %5358 : Tensor? = prim::GetAttr[name="bias"](%5332)
  %5359 : int[] = prim::ListConstruct(%27, %27)
  %5360 : int[] = prim::ListConstruct(%27, %27)
  %5361 : int[] = prim::ListConstruct(%27, %27)
  %new_features.26 : Tensor = aten::conv2d(%result.26, %5357, %5358, %5359, %5360, %5361, %27) # torch/nn/modules/conv.py:415:15
  %5363 : float = prim::GetAttr[name="drop_rate"](%4228)
  %5364 : bool = aten::gt(%5363, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.23 : Tensor = prim::If(%5364) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5366 : float = prim::GetAttr[name="drop_rate"](%4228)
      %5367 : bool = prim::GetAttr[name="training"](%4228)
      %5368 : bool = aten::lt(%5366, %16) # torch/nn/functional.py:968:7
      %5369 : bool = prim::If(%5368) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5370 : bool = aten::gt(%5366, %17) # torch/nn/functional.py:968:17
          -> (%5370)
       = prim::If(%5369) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5371 : Tensor = aten::dropout(%new_features.26, %5366, %5367) # torch/nn/functional.py:973:17
      -> (%5371)
    block1():
      -> (%new_features.26)
  %5372 : Tensor[] = aten::append(%features.1, %new_features.23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5373 : Tensor = prim::Uninitialized()
  %5374 : bool = prim::GetAttr[name="memory_efficient"](%4229)
  %5375 : bool = prim::If(%5374) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5376 : bool = prim::Uninitialized()
      %5377 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5378 : bool = aten::gt(%5377, %24)
      %5379 : bool, %5380 : bool, %5381 : int = prim::Loop(%18, %5378, %19, %5376, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5382 : int, %5383 : bool, %5384 : bool, %5385 : int):
          %tensor.14 : Tensor = aten::__getitem__(%features.1, %5385) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5387 : bool = prim::requires_grad(%tensor.14)
          %5388 : bool, %5389 : bool = prim::If(%5387) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5376)
          %5390 : int = aten::add(%5385, %27)
          %5391 : bool = aten::lt(%5390, %5377)
          %5392 : bool = aten::__and__(%5391, %5388)
          -> (%5392, %5387, %5389, %5390)
      %5393 : bool = prim::If(%5379)
        block0():
          -> (%5380)
        block1():
          -> (%19)
      -> (%5393)
    block1():
      -> (%19)
  %bottleneck_output.26 : Tensor = prim::If(%5375) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5373)
    block1():
      %concated_features.14 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5396 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%4229)
      %5397 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%4229)
      %5398 : int = aten::dim(%concated_features.14) # torch/nn/modules/batchnorm.py:276:11
      %5399 : bool = aten::ne(%5398, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5399) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5400 : bool = prim::GetAttr[name="training"](%5397)
       = prim::If(%5400) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5401 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5397)
          %5402 : Tensor = aten::add(%5401, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5397, %5402)
          -> ()
        block1():
          -> ()
      %5403 : bool = prim::GetAttr[name="training"](%5397)
      %5404 : Tensor = prim::GetAttr[name="running_mean"](%5397)
      %5405 : Tensor = prim::GetAttr[name="running_var"](%5397)
      %5406 : Tensor = prim::GetAttr[name="weight"](%5397)
      %5407 : Tensor = prim::GetAttr[name="bias"](%5397)
       = prim::If(%5403) # torch/nn/functional.py:2011:4
        block0():
          %5408 : int[] = aten::size(%concated_features.14) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%5408, %24) # torch/nn/functional.py:1991:17
          %5410 : int = aten::len(%5408) # torch/nn/functional.py:1992:19
          %5411 : int = aten::sub(%5410, %26) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%5411, %25, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %5415 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
              %5416 : int = aten::__getitem__(%5408, %5415) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %5416) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.107)
          %5418 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5418) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5419 : Tensor = aten::batch_norm(%concated_features.14, %5406, %5407, %5404, %5405, %5403, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.27 : Tensor = aten::relu_(%5419) # torch/nn/functional.py:1117:17
      %5421 : Tensor = prim::GetAttr[name="weight"](%5396)
      %5422 : Tensor? = prim::GetAttr[name="bias"](%5396)
      %5423 : int[] = prim::ListConstruct(%27, %27)
      %5424 : int[] = prim::ListConstruct(%24, %24)
      %5425 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.27 : Tensor = aten::conv2d(%result.27, %5421, %5422, %5423, %5424, %5425, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.27)
  %5427 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4229)
  %5428 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4229)
  %5429 : int = aten::dim(%bottleneck_output.26) # torch/nn/modules/batchnorm.py:276:11
  %5430 : bool = aten::ne(%5429, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5430) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5431 : bool = prim::GetAttr[name="training"](%5428)
   = prim::If(%5431) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5432 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5428)
      %5433 : Tensor = aten::add(%5432, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5428, %5433)
      -> ()
    block1():
      -> ()
  %5434 : bool = prim::GetAttr[name="training"](%5428)
  %5435 : Tensor = prim::GetAttr[name="running_mean"](%5428)
  %5436 : Tensor = prim::GetAttr[name="running_var"](%5428)
  %5437 : Tensor = prim::GetAttr[name="weight"](%5428)
  %5438 : Tensor = prim::GetAttr[name="bias"](%5428)
   = prim::If(%5434) # torch/nn/functional.py:2011:4
    block0():
      %5439 : int[] = aten::size(%bottleneck_output.26) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%5439, %24) # torch/nn/functional.py:1991:17
      %5441 : int = aten::len(%5439) # torch/nn/functional.py:1992:19
      %5442 : int = aten::sub(%5441, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%5442, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %5446 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %5447 : int = aten::__getitem__(%5439, %5446) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %5447) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %5449 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5449) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5450 : Tensor = aten::batch_norm(%bottleneck_output.26, %5437, %5438, %5435, %5436, %5434, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.28 : Tensor = aten::relu_(%5450) # torch/nn/functional.py:1117:17
  %5452 : Tensor = prim::GetAttr[name="weight"](%5427)
  %5453 : Tensor? = prim::GetAttr[name="bias"](%5427)
  %5454 : int[] = prim::ListConstruct(%27, %27)
  %5455 : int[] = prim::ListConstruct(%27, %27)
  %5456 : int[] = prim::ListConstruct(%27, %27)
  %new_features.28 : Tensor = aten::conv2d(%result.28, %5452, %5453, %5454, %5455, %5456, %27) # torch/nn/modules/conv.py:415:15
  %5458 : float = prim::GetAttr[name="drop_rate"](%4229)
  %5459 : bool = aten::gt(%5458, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.25 : Tensor = prim::If(%5459) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5461 : float = prim::GetAttr[name="drop_rate"](%4229)
      %5462 : bool = prim::GetAttr[name="training"](%4229)
      %5463 : bool = aten::lt(%5461, %16) # torch/nn/functional.py:968:7
      %5464 : bool = prim::If(%5463) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5465 : bool = aten::gt(%5461, %17) # torch/nn/functional.py:968:17
          -> (%5465)
       = prim::If(%5464) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5466 : Tensor = aten::dropout(%new_features.28, %5461, %5462) # torch/nn/functional.py:973:17
      -> (%5466)
    block1():
      -> (%new_features.28)
  %5467 : Tensor[] = aten::append(%features.1, %new_features.25) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5468 : Tensor = prim::Uninitialized()
  %5469 : bool = prim::GetAttr[name="memory_efficient"](%4230)
  %5470 : bool = prim::If(%5469) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5471 : bool = prim::Uninitialized()
      %5472 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5473 : bool = aten::gt(%5472, %24)
      %5474 : bool, %5475 : bool, %5476 : int = prim::Loop(%18, %5473, %19, %5471, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5477 : int, %5478 : bool, %5479 : bool, %5480 : int):
          %tensor.15 : Tensor = aten::__getitem__(%features.1, %5480) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5482 : bool = prim::requires_grad(%tensor.15)
          %5483 : bool, %5484 : bool = prim::If(%5482) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5471)
          %5485 : int = aten::add(%5480, %27)
          %5486 : bool = aten::lt(%5485, %5472)
          %5487 : bool = aten::__and__(%5486, %5483)
          -> (%5487, %5482, %5484, %5485)
      %5488 : bool = prim::If(%5474)
        block0():
          -> (%5475)
        block1():
          -> (%19)
      -> (%5488)
    block1():
      -> (%19)
  %bottleneck_output.28 : Tensor = prim::If(%5470) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5468)
    block1():
      %concated_features.15 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5491 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%4230)
      %5492 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%4230)
      %5493 : int = aten::dim(%concated_features.15) # torch/nn/modules/batchnorm.py:276:11
      %5494 : bool = aten::ne(%5493, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5494) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5495 : bool = prim::GetAttr[name="training"](%5492)
       = prim::If(%5495) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5496 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5492)
          %5497 : Tensor = aten::add(%5496, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5492, %5497)
          -> ()
        block1():
          -> ()
      %5498 : bool = prim::GetAttr[name="training"](%5492)
      %5499 : Tensor = prim::GetAttr[name="running_mean"](%5492)
      %5500 : Tensor = prim::GetAttr[name="running_var"](%5492)
      %5501 : Tensor = prim::GetAttr[name="weight"](%5492)
      %5502 : Tensor = prim::GetAttr[name="bias"](%5492)
       = prim::If(%5498) # torch/nn/functional.py:2011:4
        block0():
          %5503 : int[] = aten::size(%concated_features.15) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%5503, %24) # torch/nn/functional.py:1991:17
          %5505 : int = aten::len(%5503) # torch/nn/functional.py:1992:19
          %5506 : int = aten::sub(%5505, %26) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%5506, %25, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %5510 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
              %5511 : int = aten::__getitem__(%5503, %5510) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %5511) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.115)
          %5513 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5513) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5514 : Tensor = aten::batch_norm(%concated_features.15, %5501, %5502, %5499, %5500, %5498, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.29 : Tensor = aten::relu_(%5514) # torch/nn/functional.py:1117:17
      %5516 : Tensor = prim::GetAttr[name="weight"](%5491)
      %5517 : Tensor? = prim::GetAttr[name="bias"](%5491)
      %5518 : int[] = prim::ListConstruct(%27, %27)
      %5519 : int[] = prim::ListConstruct(%24, %24)
      %5520 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.29 : Tensor = aten::conv2d(%result.29, %5516, %5517, %5518, %5519, %5520, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.29)
  %5522 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4230)
  %5523 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4230)
  %5524 : int = aten::dim(%bottleneck_output.28) # torch/nn/modules/batchnorm.py:276:11
  %5525 : bool = aten::ne(%5524, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5525) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5526 : bool = prim::GetAttr[name="training"](%5523)
   = prim::If(%5526) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5527 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5523)
      %5528 : Tensor = aten::add(%5527, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5523, %5528)
      -> ()
    block1():
      -> ()
  %5529 : bool = prim::GetAttr[name="training"](%5523)
  %5530 : Tensor = prim::GetAttr[name="running_mean"](%5523)
  %5531 : Tensor = prim::GetAttr[name="running_var"](%5523)
  %5532 : Tensor = prim::GetAttr[name="weight"](%5523)
  %5533 : Tensor = prim::GetAttr[name="bias"](%5523)
   = prim::If(%5529) # torch/nn/functional.py:2011:4
    block0():
      %5534 : int[] = aten::size(%bottleneck_output.28) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%5534, %24) # torch/nn/functional.py:1991:17
      %5536 : int = aten::len(%5534) # torch/nn/functional.py:1992:19
      %5537 : int = aten::sub(%5536, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%5537, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %5541 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %5542 : int = aten::__getitem__(%5534, %5541) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %5542) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %5544 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5544) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5545 : Tensor = aten::batch_norm(%bottleneck_output.28, %5532, %5533, %5530, %5531, %5529, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.30 : Tensor = aten::relu_(%5545) # torch/nn/functional.py:1117:17
  %5547 : Tensor = prim::GetAttr[name="weight"](%5522)
  %5548 : Tensor? = prim::GetAttr[name="bias"](%5522)
  %5549 : int[] = prim::ListConstruct(%27, %27)
  %5550 : int[] = prim::ListConstruct(%27, %27)
  %5551 : int[] = prim::ListConstruct(%27, %27)
  %new_features.30 : Tensor = aten::conv2d(%result.30, %5547, %5548, %5549, %5550, %5551, %27) # torch/nn/modules/conv.py:415:15
  %5553 : float = prim::GetAttr[name="drop_rate"](%4230)
  %5554 : bool = aten::gt(%5553, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.27 : Tensor = prim::If(%5554) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5556 : float = prim::GetAttr[name="drop_rate"](%4230)
      %5557 : bool = prim::GetAttr[name="training"](%4230)
      %5558 : bool = aten::lt(%5556, %16) # torch/nn/functional.py:968:7
      %5559 : bool = prim::If(%5558) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5560 : bool = aten::gt(%5556, %17) # torch/nn/functional.py:968:17
          -> (%5560)
       = prim::If(%5559) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5561 : Tensor = aten::dropout(%new_features.30, %5556, %5557) # torch/nn/functional.py:973:17
      -> (%5561)
    block1():
      -> (%new_features.30)
  %5562 : Tensor[] = aten::append(%features.1, %new_features.27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5563 : Tensor = prim::Uninitialized()
  %5564 : bool = prim::GetAttr[name="memory_efficient"](%4231)
  %5565 : bool = prim::If(%5564) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5566 : bool = prim::Uninitialized()
      %5567 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5568 : bool = aten::gt(%5567, %24)
      %5569 : bool, %5570 : bool, %5571 : int = prim::Loop(%18, %5568, %19, %5566, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5572 : int, %5573 : bool, %5574 : bool, %5575 : int):
          %tensor.16 : Tensor = aten::__getitem__(%features.1, %5575) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5577 : bool = prim::requires_grad(%tensor.16)
          %5578 : bool, %5579 : bool = prim::If(%5577) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5566)
          %5580 : int = aten::add(%5575, %27)
          %5581 : bool = aten::lt(%5580, %5567)
          %5582 : bool = aten::__and__(%5581, %5578)
          -> (%5582, %5577, %5579, %5580)
      %5583 : bool = prim::If(%5569)
        block0():
          -> (%5570)
        block1():
          -> (%19)
      -> (%5583)
    block1():
      -> (%19)
  %bottleneck_output.30 : Tensor = prim::If(%5565) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5563)
    block1():
      %concated_features.16 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5586 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%4231)
      %5587 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%4231)
      %5588 : int = aten::dim(%concated_features.16) # torch/nn/modules/batchnorm.py:276:11
      %5589 : bool = aten::ne(%5588, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5589) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5590 : bool = prim::GetAttr[name="training"](%5587)
       = prim::If(%5590) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5591 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5587)
          %5592 : Tensor = aten::add(%5591, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5587, %5592)
          -> ()
        block1():
          -> ()
      %5593 : bool = prim::GetAttr[name="training"](%5587)
      %5594 : Tensor = prim::GetAttr[name="running_mean"](%5587)
      %5595 : Tensor = prim::GetAttr[name="running_var"](%5587)
      %5596 : Tensor = prim::GetAttr[name="weight"](%5587)
      %5597 : Tensor = prim::GetAttr[name="bias"](%5587)
       = prim::If(%5593) # torch/nn/functional.py:2011:4
        block0():
          %5598 : int[] = aten::size(%concated_features.16) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%5598, %24) # torch/nn/functional.py:1991:17
          %5600 : int = aten::len(%5598) # torch/nn/functional.py:1992:19
          %5601 : int = aten::sub(%5600, %26) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%5601, %25, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %5605 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
              %5606 : int = aten::__getitem__(%5598, %5605) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %5606) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.123)
          %5608 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5608) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5609 : Tensor = aten::batch_norm(%concated_features.16, %5596, %5597, %5594, %5595, %5593, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.31 : Tensor = aten::relu_(%5609) # torch/nn/functional.py:1117:17
      %5611 : Tensor = prim::GetAttr[name="weight"](%5586)
      %5612 : Tensor? = prim::GetAttr[name="bias"](%5586)
      %5613 : int[] = prim::ListConstruct(%27, %27)
      %5614 : int[] = prim::ListConstruct(%24, %24)
      %5615 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.31 : Tensor = aten::conv2d(%result.31, %5611, %5612, %5613, %5614, %5615, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.31)
  %5617 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4231)
  %5618 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4231)
  %5619 : int = aten::dim(%bottleneck_output.30) # torch/nn/modules/batchnorm.py:276:11
  %5620 : bool = aten::ne(%5619, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5620) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5621 : bool = prim::GetAttr[name="training"](%5618)
   = prim::If(%5621) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5622 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5618)
      %5623 : Tensor = aten::add(%5622, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5618, %5623)
      -> ()
    block1():
      -> ()
  %5624 : bool = prim::GetAttr[name="training"](%5618)
  %5625 : Tensor = prim::GetAttr[name="running_mean"](%5618)
  %5626 : Tensor = prim::GetAttr[name="running_var"](%5618)
  %5627 : Tensor = prim::GetAttr[name="weight"](%5618)
  %5628 : Tensor = prim::GetAttr[name="bias"](%5618)
   = prim::If(%5624) # torch/nn/functional.py:2011:4
    block0():
      %5629 : int[] = aten::size(%bottleneck_output.30) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%5629, %24) # torch/nn/functional.py:1991:17
      %5631 : int = aten::len(%5629) # torch/nn/functional.py:1992:19
      %5632 : int = aten::sub(%5631, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%5632, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %5636 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %5637 : int = aten::__getitem__(%5629, %5636) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %5637) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %5639 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5639) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5640 : Tensor = aten::batch_norm(%bottleneck_output.30, %5627, %5628, %5625, %5626, %5624, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.32 : Tensor = aten::relu_(%5640) # torch/nn/functional.py:1117:17
  %5642 : Tensor = prim::GetAttr[name="weight"](%5617)
  %5643 : Tensor? = prim::GetAttr[name="bias"](%5617)
  %5644 : int[] = prim::ListConstruct(%27, %27)
  %5645 : int[] = prim::ListConstruct(%27, %27)
  %5646 : int[] = prim::ListConstruct(%27, %27)
  %new_features.32 : Tensor = aten::conv2d(%result.32, %5642, %5643, %5644, %5645, %5646, %27) # torch/nn/modules/conv.py:415:15
  %5648 : float = prim::GetAttr[name="drop_rate"](%4231)
  %5649 : bool = aten::gt(%5648, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.29 : Tensor = prim::If(%5649) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5651 : float = prim::GetAttr[name="drop_rate"](%4231)
      %5652 : bool = prim::GetAttr[name="training"](%4231)
      %5653 : bool = aten::lt(%5651, %16) # torch/nn/functional.py:968:7
      %5654 : bool = prim::If(%5653) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5655 : bool = aten::gt(%5651, %17) # torch/nn/functional.py:968:17
          -> (%5655)
       = prim::If(%5654) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5656 : Tensor = aten::dropout(%new_features.32, %5651, %5652) # torch/nn/functional.py:973:17
      -> (%5656)
    block1():
      -> (%new_features.32)
  %5657 : Tensor[] = aten::append(%features.1, %new_features.29) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5658 : Tensor = prim::Uninitialized()
  %5659 : bool = prim::GetAttr[name="memory_efficient"](%4232)
  %5660 : bool = prim::If(%5659) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5661 : bool = prim::Uninitialized()
      %5662 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5663 : bool = aten::gt(%5662, %24)
      %5664 : bool, %5665 : bool, %5666 : int = prim::Loop(%18, %5663, %19, %5661, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5667 : int, %5668 : bool, %5669 : bool, %5670 : int):
          %tensor.1 : Tensor = aten::__getitem__(%features.1, %5670) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5672 : bool = prim::requires_grad(%tensor.1)
          %5673 : bool, %5674 : bool = prim::If(%5672) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5661)
          %5675 : int = aten::add(%5670, %27)
          %5676 : bool = aten::lt(%5675, %5662)
          %5677 : bool = aten::__and__(%5676, %5673)
          -> (%5677, %5672, %5674, %5675)
      %5678 : bool = prim::If(%5664)
        block0():
          -> (%5665)
        block1():
          -> (%19)
      -> (%5678)
    block1():
      -> (%19)
  %bottleneck_output : Tensor = prim::If(%5660) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5658)
    block1():
      %concated_features.1 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5681 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%4232)
      %5682 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%4232)
      %5683 : int = aten::dim(%concated_features.1) # torch/nn/modules/batchnorm.py:276:11
      %5684 : bool = aten::ne(%5683, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5684) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5685 : bool = prim::GetAttr[name="training"](%5682)
       = prim::If(%5685) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5686 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5682)
          %5687 : Tensor = aten::add(%5686, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5682, %5687)
          -> ()
        block1():
          -> ()
      %5688 : bool = prim::GetAttr[name="training"](%5682)
      %5689 : Tensor = prim::GetAttr[name="running_mean"](%5682)
      %5690 : Tensor = prim::GetAttr[name="running_var"](%5682)
      %5691 : Tensor = prim::GetAttr[name="weight"](%5682)
      %5692 : Tensor = prim::GetAttr[name="bias"](%5682)
       = prim::If(%5688) # torch/nn/functional.py:2011:4
        block0():
          %5693 : int[] = aten::size(%concated_features.1) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%5693, %24) # torch/nn/functional.py:1991:17
          %5695 : int = aten::len(%5693) # torch/nn/functional.py:1992:19
          %5696 : int = aten::sub(%5695, %26) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%5696, %25, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %5700 : int = aten::add(%i.2, %26) # torch/nn/functional.py:1993:27
              %5701 : int = aten::__getitem__(%5693, %5700) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %5701) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.5)
          %5703 : bool = aten::eq(%size_prods.4, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5703) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5704 : Tensor = aten::batch_norm(%concated_features.1, %5691, %5692, %5689, %5690, %5688, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.118 : Tensor = aten::relu_(%5704) # torch/nn/functional.py:1117:17
      %5706 : Tensor = prim::GetAttr[name="weight"](%5681)
      %5707 : Tensor? = prim::GetAttr[name="bias"](%5681)
      %5708 : int[] = prim::ListConstruct(%27, %27)
      %5709 : int[] = prim::ListConstruct(%24, %24)
      %5710 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.2 : Tensor = aten::conv2d(%result.118, %5706, %5707, %5708, %5709, %5710, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.2)
  %5712 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4232)
  %5713 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4232)
  %5714 : int = aten::dim(%bottleneck_output) # torch/nn/modules/batchnorm.py:276:11
  %5715 : bool = aten::ne(%5714, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5715) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5716 : bool = prim::GetAttr[name="training"](%5713)
   = prim::If(%5716) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5717 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5713)
      %5718 : Tensor = aten::add(%5717, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5713, %5718)
      -> ()
    block1():
      -> ()
  %5719 : bool = prim::GetAttr[name="training"](%5713)
  %5720 : Tensor = prim::GetAttr[name="running_mean"](%5713)
  %5721 : Tensor = prim::GetAttr[name="running_var"](%5713)
  %5722 : Tensor = prim::GetAttr[name="weight"](%5713)
  %5723 : Tensor = prim::GetAttr[name="bias"](%5713)
   = prim::If(%5719) # torch/nn/functional.py:2011:4
    block0():
      %5724 : int[] = aten::size(%bottleneck_output) # torch/nn/functional.py:2012:27
      %size_prods.480 : int = aten::__getitem__(%5724, %24) # torch/nn/functional.py:1991:17
      %5726 : int = aten::len(%5724) # torch/nn/functional.py:1992:19
      %5727 : int = aten::sub(%5726, %26) # torch/nn/functional.py:1992:19
      %size_prods.481 : int = prim::Loop(%5727, %25, %size_prods.480) # torch/nn/functional.py:1992:4
        block0(%i.121 : int, %size_prods.482 : int):
          %5731 : int = aten::add(%i.121, %26) # torch/nn/functional.py:1993:27
          %5732 : int = aten::__getitem__(%5724, %5731) # torch/nn/functional.py:1993:22
          %size_prods.483 : int = aten::mul(%size_prods.482, %5732) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.483)
      %5734 : bool = aten::eq(%size_prods.481, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5734) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5735 : Tensor = aten::batch_norm(%bottleneck_output, %5722, %5723, %5720, %5721, %5719, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.117 : Tensor = aten::relu_(%5735) # torch/nn/functional.py:1117:17
  %5737 : Tensor = prim::GetAttr[name="weight"](%5712)
  %5738 : Tensor? = prim::GetAttr[name="bias"](%5712)
  %5739 : int[] = prim::ListConstruct(%27, %27)
  %5740 : int[] = prim::ListConstruct(%27, %27)
  %5741 : int[] = prim::ListConstruct(%27, %27)
  %new_features.1 : Tensor = aten::conv2d(%result.117, %5737, %5738, %5739, %5740, %5741, %27) # torch/nn/modules/conv.py:415:15
  %5743 : float = prim::GetAttr[name="drop_rate"](%4232)
  %5744 : bool = aten::gt(%5743, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.31 : Tensor = prim::If(%5744) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5746 : float = prim::GetAttr[name="drop_rate"](%4232)
      %5747 : bool = prim::GetAttr[name="training"](%4232)
      %5748 : bool = aten::lt(%5746, %16) # torch/nn/functional.py:968:7
      %5749 : bool = prim::If(%5748) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5750 : bool = aten::gt(%5746, %17) # torch/nn/functional.py:968:17
          -> (%5750)
       = prim::If(%5749) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5751 : Tensor = aten::dropout(%new_features.1, %5746, %5747) # torch/nn/functional.py:973:17
      -> (%5751)
    block1():
      -> (%new_features.1)
  %5752 : Tensor[] = aten::append(%features.1, %new_features.31) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %5754 : int = aten::dim(%input.23) # torch/nn/modules/batchnorm.py:276:11
  %5755 : bool = aten::ne(%5754, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5755) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5756 : bool = prim::GetAttr[name="training"](%38)
   = prim::If(%5756) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5757 : Tensor = prim::GetAttr[name="num_batches_tracked"](%38)
      %5758 : Tensor = aten::add(%5757, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%38, %5758)
      -> ()
    block1():
      -> ()
  %5759 : bool = prim::GetAttr[name="training"](%38)
  %5760 : Tensor = prim::GetAttr[name="running_mean"](%38)
  %5761 : Tensor = prim::GetAttr[name="running_var"](%38)
  %5762 : Tensor = prim::GetAttr[name="weight"](%38)
  %5763 : Tensor = prim::GetAttr[name="bias"](%38)
   = prim::If(%5759) # torch/nn/functional.py:2011:4
    block0():
      %5764 : int[] = aten::size(%input.23) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%5764, %24) # torch/nn/functional.py:1991:17
      %5766 : int = aten::len(%5764) # torch/nn/functional.py:1992:19
      %5767 : int = aten::sub(%5766, %26) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%5767, %25, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %5771 : int = aten::add(%i.1, %26) # torch/nn/functional.py:1993:27
          %5772 : int = aten::__getitem__(%5764, %5771) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %5772) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.3)
      %5774 : bool = aten::eq(%size_prods, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5774) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %features.5 : Tensor = aten::batch_norm(%input.23, %5762, %5763, %5760, %5761, %5759, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %out.1 : Tensor = prim::If(%5) # torch/nn/functional.py:1116:4
    block0():
      %result.1 : Tensor = aten::relu_(%features.5) # torch/nn/functional.py:1117:17
      -> (%result.1)
    block1():
      %result.2 : Tensor = aten::relu(%features.5) # torch/nn/functional.py:1119:17
      -> (%result.2)
  %10 : int[] = prim::ListConstruct(%6, %6)
  %5779 : str = prim::Constant[value="Exception"]() # <string>:5:2
  %5780 : int[] = aten::size(%out.1) # torch/nn/functional.py:925:51
  %5781 : int = aten::len(%5780) # <string>:5:9
  %5782 : int = aten::len(%10) # <string>:5:25
  %5783 : bool = aten::gt(%5781, %5782) # <string>:5:9
   = prim::If(%5783) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%5779) # <string>:5:2
      -> ()
  %out.3 : Tensor = aten::adaptive_avg_pool2d(%out.1, %10) # torch/nn/functional.py:926:11
  %out.5 : Tensor = aten::flatten(%out.3, %6, %2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:195:14
  %13 : __torch__.torch.nn.modules.linear.___torch_mangle_161.Linear = prim::GetAttr[name="classifier"](%self)
  %5785 : int = prim::Constant[value=1]()
  %5786 : int = prim::Constant[value=2]() # torch/nn/functional.py:1672:22
  %5787 : Tensor = prim::GetAttr[name="weight"](%13)
  %5788 : Tensor = prim::GetAttr[name="bias"](%13)
  %5789 : int = aten::dim(%out.5) # torch/nn/functional.py:1672:7
  %5790 : bool = aten::eq(%5789, %5786) # torch/nn/functional.py:1672:7
  %out.7 : Tensor = prim::If(%5790) # torch/nn/functional.py:1672:4
    block0():
      %5792 : Tensor = aten::t(%5787) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%5788, %out.5, %5792, %5785, %5785) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %5794 : Tensor = aten::t(%5787) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%out.5, %5794) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %5788, %5785) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%out.7)
