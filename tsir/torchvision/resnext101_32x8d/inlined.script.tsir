graph(%self : __torch__.torchvision.models.resnet.___torch_mangle_1004.ResNet,
      %x.1 : Tensor):
  %3 : int = prim::Constant[value=32]() # torch/nn/modules/conv.py:414:53
  %4 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %5 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.1 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %7 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %8 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %9 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %10 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %11 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %12 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %13 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:29
  %14 : int = prim::Constant[value=-1]()
  %15 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv1"](%self)
  %16 : Tensor = prim::GetAttr[name="weight"](%15)
  %17 : Tensor? = prim::GetAttr[name="bias"](%15)
  %18 : int[] = prim::ListConstruct(%11, %11)
  %19 : int[] = prim::ListConstruct(%12, %12)
  %20 : int[] = prim::ListConstruct(%13, %13)
  %x.3 : Tensor = aten::conv2d(%x.1, %16, %17, %18, %19, %20, %13) # torch/nn/modules/conv.py:415:15
  %22 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%self)
  %23 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %24 : bool = aten::ne(%23, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%24) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %25 : bool = prim::GetAttr[name="training"](%22)
   = prim::If(%25) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %26 : Tensor = prim::GetAttr[name="num_batches_tracked"](%22)
      %27 : Tensor = aten::add(%26, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%22, %27)
      -> ()
    block1():
      -> ()
  %28 : bool = prim::GetAttr[name="training"](%22)
  %29 : Tensor = prim::GetAttr[name="running_mean"](%22)
  %30 : Tensor = prim::GetAttr[name="running_var"](%22)
  %31 : Tensor = prim::GetAttr[name="weight"](%22)
  %32 : Tensor = prim::GetAttr[name="bias"](%22)
   = prim::If(%28) # torch/nn/functional.py:2011:4
    block0():
      %33 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%33, %9) # torch/nn/functional.py:1991:17
      %35 : int = aten::len(%33) # torch/nn/functional.py:1992:19
      %36 : int = aten::sub(%35, %11) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%36, %10, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %40 : int = aten::add(%i.82, %11) # torch/nn/functional.py:1993:27
          %41 : int = aten::__getitem__(%33, %40) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %41) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.327)
      %43 : bool = aten::eq(%size_prods.325, %13) # torch/nn/functional.py:1994:7
       = prim::If(%43) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.5 : Tensor = aten::batch_norm(%x.3, %31, %32, %29, %30, %28, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %x.7 : Tensor = aten::relu_(%x.5) # torch/nn/functional.py:1117:17
  %46 : int[] = prim::ListConstruct(%12, %12)
  %47 : int[] = prim::ListConstruct(%11, %11)
  %48 : int[] = prim::ListConstruct(%13, %13)
  %49 : int[] = prim::ListConstruct(%13, %13)
  %x.9 : Tensor = aten::max_pool2d(%x.7, %46, %47, %48, %49, %4) # torch/nn/functional.py:575:11
  %51 : __torch__.torch.nn.modules.container.___torch_mangle_984.Sequential = prim::GetAttr[name="layer1"](%self)
  %52 : __torch__.torchvision.models.resnet.___torch_mangle_982.Bottleneck = prim::GetAttr[name="0"](%51)
  %53 : __torch__.torchvision.models.resnet.___torch_mangle_983.Bottleneck = prim::GetAttr[name="1"](%51)
  %54 : __torch__.torchvision.models.resnet.___torch_mangle_983.Bottleneck = prim::GetAttr[name="2"](%51)
  %55 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv1"](%52)
  %56 : Tensor = prim::GetAttr[name="weight"](%55)
  %57 : Tensor? = prim::GetAttr[name="bias"](%55)
  %58 : int[] = prim::ListConstruct(%13, %13)
  %59 : int[] = prim::ListConstruct(%9, %9)
  %60 : int[] = prim::ListConstruct(%13, %13)
  %out.19 : Tensor = aten::conv2d(%x.9, %56, %57, %58, %59, %60, %13) # torch/nn/modules/conv.py:415:15
  %62 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%52)
  %63 : int = aten::dim(%out.19) # torch/nn/modules/batchnorm.py:276:11
  %64 : bool = aten::ne(%63, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%64) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %65 : bool = prim::GetAttr[name="training"](%62)
   = prim::If(%65) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %66 : Tensor = prim::GetAttr[name="num_batches_tracked"](%62)
      %67 : Tensor = aten::add(%66, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%62, %67)
      -> ()
    block1():
      -> ()
  %68 : bool = prim::GetAttr[name="training"](%62)
  %69 : Tensor = prim::GetAttr[name="running_mean"](%62)
  %70 : Tensor = prim::GetAttr[name="running_var"](%62)
  %71 : Tensor = prim::GetAttr[name="weight"](%62)
  %72 : Tensor = prim::GetAttr[name="bias"](%62)
   = prim::If(%68) # torch/nn/functional.py:2011:4
    block0():
      %73 : int[] = aten::size(%out.19) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%73, %9) # torch/nn/functional.py:1991:17
      %75 : int = aten::len(%73) # torch/nn/functional.py:1992:19
      %76 : int = aten::sub(%75, %11) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%76, %10, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %80 : int = aten::add(%i.83, %11) # torch/nn/functional.py:1993:27
          %81 : int = aten::__getitem__(%73, %80) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %81) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.331)
      %83 : bool = aten::eq(%size_prods.329, %13) # torch/nn/functional.py:1994:7
       = prim::If(%83) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.209 : Tensor = aten::batch_norm(%out.19, %71, %72, %69, %70, %68, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.210 : Tensor = aten::relu_(%out.209) # torch/nn/functional.py:1117:17
  %86 : __torch__.torch.nn.modules.conv.___torch_mangle_980.Conv2d = prim::GetAttr[name="conv2"](%52)
  %87 : Tensor = prim::GetAttr[name="weight"](%86)
  %88 : Tensor? = prim::GetAttr[name="bias"](%86)
  %89 : int[] = prim::ListConstruct(%13, %13)
  %90 : int[] = prim::ListConstruct(%13, %13)
  %91 : int[] = prim::ListConstruct(%13, %13)
  %out.211 : Tensor = aten::conv2d(%out.210, %87, %88, %89, %90, %91, %3) # torch/nn/modules/conv.py:415:15
  %93 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%52)
  %94 : int = aten::dim(%out.211) # torch/nn/modules/batchnorm.py:276:11
  %95 : bool = aten::ne(%94, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%95) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %96 : bool = prim::GetAttr[name="training"](%93)
   = prim::If(%96) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %97 : Tensor = prim::GetAttr[name="num_batches_tracked"](%93)
      %98 : Tensor = aten::add(%97, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%93, %98)
      -> ()
    block1():
      -> ()
  %99 : bool = prim::GetAttr[name="training"](%93)
  %100 : Tensor = prim::GetAttr[name="running_mean"](%93)
  %101 : Tensor = prim::GetAttr[name="running_var"](%93)
  %102 : Tensor = prim::GetAttr[name="weight"](%93)
  %103 : Tensor = prim::GetAttr[name="bias"](%93)
   = prim::If(%99) # torch/nn/functional.py:2011:4
    block0():
      %104 : int[] = aten::size(%out.211) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%104, %9) # torch/nn/functional.py:1991:17
      %106 : int = aten::len(%104) # torch/nn/functional.py:1992:19
      %107 : int = aten::sub(%106, %11) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%107, %10, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %111 : int = aten::add(%i.84, %11) # torch/nn/functional.py:1993:27
          %112 : int = aten::__getitem__(%104, %111) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %112) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.335)
      %114 : bool = aten::eq(%size_prods.333, %13) # torch/nn/functional.py:1994:7
       = prim::If(%114) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.212 : Tensor = aten::batch_norm(%out.211, %102, %103, %100, %101, %99, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.213 : Tensor = aten::relu_(%out.212) # torch/nn/functional.py:1117:17
  %117 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv3"](%52)
  %118 : Tensor = prim::GetAttr[name="weight"](%117)
  %119 : Tensor? = prim::GetAttr[name="bias"](%117)
  %120 : int[] = prim::ListConstruct(%13, %13)
  %121 : int[] = prim::ListConstruct(%9, %9)
  %122 : int[] = prim::ListConstruct(%13, %13)
  %out.214 : Tensor = aten::conv2d(%out.213, %118, %119, %120, %121, %122, %13) # torch/nn/modules/conv.py:415:15
  %124 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%52)
  %125 : int = aten::dim(%out.214) # torch/nn/modules/batchnorm.py:276:11
  %126 : bool = aten::ne(%125, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%126) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %127 : bool = prim::GetAttr[name="training"](%124)
   = prim::If(%127) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %128 : Tensor = prim::GetAttr[name="num_batches_tracked"](%124)
      %129 : Tensor = aten::add(%128, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%124, %129)
      -> ()
    block1():
      -> ()
  %130 : bool = prim::GetAttr[name="training"](%124)
  %131 : Tensor = prim::GetAttr[name="running_mean"](%124)
  %132 : Tensor = prim::GetAttr[name="running_var"](%124)
  %133 : Tensor = prim::GetAttr[name="weight"](%124)
  %134 : Tensor = prim::GetAttr[name="bias"](%124)
   = prim::If(%130) # torch/nn/functional.py:2011:4
    block0():
      %135 : int[] = aten::size(%out.214) # torch/nn/functional.py:2012:27
      %size_prods.304 : int = aten::__getitem__(%135, %9) # torch/nn/functional.py:1991:17
      %137 : int = aten::len(%135) # torch/nn/functional.py:1992:19
      %138 : int = aten::sub(%137, %11) # torch/nn/functional.py:1992:19
      %size_prods.305 : int = prim::Loop(%138, %10, %size_prods.304) # torch/nn/functional.py:1992:4
        block0(%i.77 : int, %size_prods.306 : int):
          %142 : int = aten::add(%i.77, %11) # torch/nn/functional.py:1993:27
          %143 : int = aten::__getitem__(%135, %142) # torch/nn/functional.py:1993:22
          %size_prods.307 : int = aten::mul(%size_prods.306, %143) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.307)
      %145 : bool = aten::eq(%size_prods.305, %13) # torch/nn/functional.py:1994:7
       = prim::If(%145) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.215 : Tensor = aten::batch_norm(%out.214, %133, %134, %131, %132, %130, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %147 : __torch__.torch.nn.modules.container.___torch_mangle_13.Sequential = prim::GetAttr[name="downsample"](%52)
  %148 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="0"](%147)
  %149 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%147)
  %150 : Tensor = prim::GetAttr[name="weight"](%148)
  %151 : Tensor? = prim::GetAttr[name="bias"](%148)
  %152 : int[] = prim::ListConstruct(%13, %13)
  %153 : int[] = prim::ListConstruct(%9, %9)
  %154 : int[] = prim::ListConstruct(%13, %13)
  %input.6 : Tensor = aten::conv2d(%x.9, %150, %151, %152, %153, %154, %13) # torch/nn/modules/conv.py:415:15
  %156 : int = aten::dim(%input.6) # torch/nn/modules/batchnorm.py:276:11
  %157 : bool = aten::ne(%156, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%157) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %158 : bool = prim::GetAttr[name="training"](%149)
   = prim::If(%158) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %159 : Tensor = prim::GetAttr[name="num_batches_tracked"](%149)
      %160 : Tensor = aten::add(%159, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%149, %160)
      -> ()
    block1():
      -> ()
  %161 : bool = prim::GetAttr[name="training"](%149)
  %162 : Tensor = prim::GetAttr[name="running_mean"](%149)
  %163 : Tensor = prim::GetAttr[name="running_var"](%149)
  %164 : Tensor = prim::GetAttr[name="weight"](%149)
  %165 : Tensor = prim::GetAttr[name="bias"](%149)
   = prim::If(%161) # torch/nn/functional.py:2011:4
    block0():
      %166 : int[] = aten::size(%input.6) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%166, %9) # torch/nn/functional.py:1991:17
      %168 : int = aten::len(%166) # torch/nn/functional.py:1992:19
      %169 : int = aten::sub(%168, %11) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%169, %10, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %173 : int = aten::add(%i.78, %11) # torch/nn/functional.py:1993:27
          %174 : int = aten::__getitem__(%166, %173) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %174) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.311)
      %176 : bool = aten::eq(%size_prods.309, %13) # torch/nn/functional.py:1994:7
       = prim::If(%176) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.2 : Tensor = aten::batch_norm(%input.6, %164, %165, %162, %163, %161, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.216 : Tensor = aten::add_(%out.215, %identity.2, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.10 : Tensor = aten::relu_(%out.216) # torch/nn/functional.py:1117:17
  %180 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv1"](%53)
  %181 : Tensor = prim::GetAttr[name="weight"](%180)
  %182 : Tensor? = prim::GetAttr[name="bias"](%180)
  %183 : int[] = prim::ListConstruct(%13, %13)
  %184 : int[] = prim::ListConstruct(%9, %9)
  %185 : int[] = prim::ListConstruct(%13, %13)
  %out.226 : Tensor = aten::conv2d(%input.10, %181, %182, %183, %184, %185, %13) # torch/nn/modules/conv.py:415:15
  %187 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%53)
  %188 : int = aten::dim(%out.226) # torch/nn/modules/batchnorm.py:276:11
  %189 : bool = aten::ne(%188, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%189) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %190 : bool = prim::GetAttr[name="training"](%187)
   = prim::If(%190) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %191 : Tensor = prim::GetAttr[name="num_batches_tracked"](%187)
      %192 : Tensor = aten::add(%191, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%187, %192)
      -> ()
    block1():
      -> ()
  %193 : bool = prim::GetAttr[name="training"](%187)
  %194 : Tensor = prim::GetAttr[name="running_mean"](%187)
  %195 : Tensor = prim::GetAttr[name="running_var"](%187)
  %196 : Tensor = prim::GetAttr[name="weight"](%187)
  %197 : Tensor = prim::GetAttr[name="bias"](%187)
   = prim::If(%193) # torch/nn/functional.py:2011:4
    block0():
      %198 : int[] = aten::size(%out.226) # torch/nn/functional.py:2012:27
      %size_prods.312 : int = aten::__getitem__(%198, %9) # torch/nn/functional.py:1991:17
      %200 : int = aten::len(%198) # torch/nn/functional.py:1992:19
      %201 : int = aten::sub(%200, %11) # torch/nn/functional.py:1992:19
      %size_prods.313 : int = prim::Loop(%201, %10, %size_prods.312) # torch/nn/functional.py:1992:4
        block0(%i.79 : int, %size_prods.314 : int):
          %205 : int = aten::add(%i.79, %11) # torch/nn/functional.py:1993:27
          %206 : int = aten::__getitem__(%198, %205) # torch/nn/functional.py:1993:22
          %size_prods.315 : int = aten::mul(%size_prods.314, %206) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.315)
      %208 : bool = aten::eq(%size_prods.313, %13) # torch/nn/functional.py:1994:7
       = prim::If(%208) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.254 : Tensor = aten::batch_norm(%out.226, %196, %197, %194, %195, %193, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.255 : Tensor = aten::relu_(%out.254) # torch/nn/functional.py:1117:17
  %211 : __torch__.torch.nn.modules.conv.___torch_mangle_980.Conv2d = prim::GetAttr[name="conv2"](%53)
  %212 : Tensor = prim::GetAttr[name="weight"](%211)
  %213 : Tensor? = prim::GetAttr[name="bias"](%211)
  %214 : int[] = prim::ListConstruct(%13, %13)
  %215 : int[] = prim::ListConstruct(%13, %13)
  %216 : int[] = prim::ListConstruct(%13, %13)
  %out.256 : Tensor = aten::conv2d(%out.255, %212, %213, %214, %215, %216, %3) # torch/nn/modules/conv.py:415:15
  %218 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%53)
  %219 : int = aten::dim(%out.256) # torch/nn/modules/batchnorm.py:276:11
  %220 : bool = aten::ne(%219, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%220) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %221 : bool = prim::GetAttr[name="training"](%218)
   = prim::If(%221) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %222 : Tensor = prim::GetAttr[name="num_batches_tracked"](%218)
      %223 : Tensor = aten::add(%222, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%218, %223)
      -> ()
    block1():
      -> ()
  %224 : bool = prim::GetAttr[name="training"](%218)
  %225 : Tensor = prim::GetAttr[name="running_mean"](%218)
  %226 : Tensor = prim::GetAttr[name="running_var"](%218)
  %227 : Tensor = prim::GetAttr[name="weight"](%218)
  %228 : Tensor = prim::GetAttr[name="bias"](%218)
   = prim::If(%224) # torch/nn/functional.py:2011:4
    block0():
      %229 : int[] = aten::size(%out.256) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%229, %9) # torch/nn/functional.py:1991:17
      %231 : int = aten::len(%229) # torch/nn/functional.py:1992:19
      %232 : int = aten::sub(%231, %11) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%232, %10, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %236 : int = aten::add(%i.80, %11) # torch/nn/functional.py:1993:27
          %237 : int = aten::__getitem__(%229, %236) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %237) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.319)
      %239 : bool = aten::eq(%size_prods.317, %13) # torch/nn/functional.py:1994:7
       = prim::If(%239) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.257 : Tensor = aten::batch_norm(%out.256, %227, %228, %225, %226, %224, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.258 : Tensor = aten::relu_(%out.257) # torch/nn/functional.py:1117:17
  %242 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv3"](%53)
  %243 : Tensor = prim::GetAttr[name="weight"](%242)
  %244 : Tensor? = prim::GetAttr[name="bias"](%242)
  %245 : int[] = prim::ListConstruct(%13, %13)
  %246 : int[] = prim::ListConstruct(%9, %9)
  %247 : int[] = prim::ListConstruct(%13, %13)
  %out.259 : Tensor = aten::conv2d(%out.258, %243, %244, %245, %246, %247, %13) # torch/nn/modules/conv.py:415:15
  %249 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%53)
  %250 : int = aten::dim(%out.259) # torch/nn/modules/batchnorm.py:276:11
  %251 : bool = aten::ne(%250, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%251) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %252 : bool = prim::GetAttr[name="training"](%249)
   = prim::If(%252) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %253 : Tensor = prim::GetAttr[name="num_batches_tracked"](%249)
      %254 : Tensor = aten::add(%253, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%249, %254)
      -> ()
    block1():
      -> ()
  %255 : bool = prim::GetAttr[name="training"](%249)
  %256 : Tensor = prim::GetAttr[name="running_mean"](%249)
  %257 : Tensor = prim::GetAttr[name="running_var"](%249)
  %258 : Tensor = prim::GetAttr[name="weight"](%249)
  %259 : Tensor = prim::GetAttr[name="bias"](%249)
   = prim::If(%255) # torch/nn/functional.py:2011:4
    block0():
      %260 : int[] = aten::size(%out.259) # torch/nn/functional.py:2012:27
      %size_prods.320 : int = aten::__getitem__(%260, %9) # torch/nn/functional.py:1991:17
      %262 : int = aten::len(%260) # torch/nn/functional.py:1992:19
      %263 : int = aten::sub(%262, %11) # torch/nn/functional.py:1992:19
      %size_prods.321 : int = prim::Loop(%263, %10, %size_prods.320) # torch/nn/functional.py:1992:4
        block0(%i.81 : int, %size_prods.322 : int):
          %267 : int = aten::add(%i.81, %11) # torch/nn/functional.py:1993:27
          %268 : int = aten::__getitem__(%260, %267) # torch/nn/functional.py:1993:22
          %size_prods.323 : int = aten::mul(%size_prods.322, %268) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.323)
      %270 : bool = aten::eq(%size_prods.321, %13) # torch/nn/functional.py:1994:7
       = prim::If(%270) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.260 : Tensor = aten::batch_norm(%out.259, %258, %259, %256, %257, %255, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.261 : Tensor = aten::add_(%out.260, %input.10, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.8 : Tensor = aten::relu_(%out.261) # torch/nn/functional.py:1117:17
  %274 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv1"](%54)
  %275 : Tensor = prim::GetAttr[name="weight"](%274)
  %276 : Tensor? = prim::GetAttr[name="bias"](%274)
  %277 : int[] = prim::ListConstruct(%13, %13)
  %278 : int[] = prim::ListConstruct(%9, %9)
  %279 : int[] = prim::ListConstruct(%13, %13)
  %out.235 : Tensor = aten::conv2d(%input.8, %275, %276, %277, %278, %279, %13) # torch/nn/modules/conv.py:415:15
  %281 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%54)
  %282 : int = aten::dim(%out.235) # torch/nn/modules/batchnorm.py:276:11
  %283 : bool = aten::ne(%282, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%283) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %284 : bool = prim::GetAttr[name="training"](%281)
   = prim::If(%284) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %285 : Tensor = prim::GetAttr[name="num_batches_tracked"](%281)
      %286 : Tensor = aten::add(%285, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%281, %286)
      -> ()
    block1():
      -> ()
  %287 : bool = prim::GetAttr[name="training"](%281)
  %288 : Tensor = prim::GetAttr[name="running_mean"](%281)
  %289 : Tensor = prim::GetAttr[name="running_var"](%281)
  %290 : Tensor = prim::GetAttr[name="weight"](%281)
  %291 : Tensor = prim::GetAttr[name="bias"](%281)
   = prim::If(%287) # torch/nn/functional.py:2011:4
    block0():
      %292 : int[] = aten::size(%out.235) # torch/nn/functional.py:2012:27
      %size_prods.336 : int = aten::__getitem__(%292, %9) # torch/nn/functional.py:1991:17
      %294 : int = aten::len(%292) # torch/nn/functional.py:1992:19
      %295 : int = aten::sub(%294, %11) # torch/nn/functional.py:1992:19
      %size_prods.337 : int = prim::Loop(%295, %10, %size_prods.336) # torch/nn/functional.py:1992:4
        block0(%i.85 : int, %size_prods.338 : int):
          %299 : int = aten::add(%i.85, %11) # torch/nn/functional.py:1993:27
          %300 : int = aten::__getitem__(%292, %299) # torch/nn/functional.py:1993:22
          %size_prods.339 : int = aten::mul(%size_prods.338, %300) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.339)
      %302 : bool = aten::eq(%size_prods.337, %13) # torch/nn/functional.py:1994:7
       = prim::If(%302) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.218 : Tensor = aten::batch_norm(%out.235, %290, %291, %288, %289, %287, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.219 : Tensor = aten::relu_(%out.218) # torch/nn/functional.py:1117:17
  %305 : __torch__.torch.nn.modules.conv.___torch_mangle_980.Conv2d = prim::GetAttr[name="conv2"](%54)
  %306 : Tensor = prim::GetAttr[name="weight"](%305)
  %307 : Tensor? = prim::GetAttr[name="bias"](%305)
  %308 : int[] = prim::ListConstruct(%13, %13)
  %309 : int[] = prim::ListConstruct(%13, %13)
  %310 : int[] = prim::ListConstruct(%13, %13)
  %out.220 : Tensor = aten::conv2d(%out.219, %306, %307, %308, %309, %310, %3) # torch/nn/modules/conv.py:415:15
  %312 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%54)
  %313 : int = aten::dim(%out.220) # torch/nn/modules/batchnorm.py:276:11
  %314 : bool = aten::ne(%313, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%314) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %315 : bool = prim::GetAttr[name="training"](%312)
   = prim::If(%315) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %316 : Tensor = prim::GetAttr[name="num_batches_tracked"](%312)
      %317 : Tensor = aten::add(%316, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%312, %317)
      -> ()
    block1():
      -> ()
  %318 : bool = prim::GetAttr[name="training"](%312)
  %319 : Tensor = prim::GetAttr[name="running_mean"](%312)
  %320 : Tensor = prim::GetAttr[name="running_var"](%312)
  %321 : Tensor = prim::GetAttr[name="weight"](%312)
  %322 : Tensor = prim::GetAttr[name="bias"](%312)
   = prim::If(%318) # torch/nn/functional.py:2011:4
    block0():
      %323 : int[] = aten::size(%out.220) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%323, %9) # torch/nn/functional.py:1991:17
      %325 : int = aten::len(%323) # torch/nn/functional.py:1992:19
      %326 : int = aten::sub(%325, %11) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%326, %10, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %330 : int = aten::add(%i.86, %11) # torch/nn/functional.py:1993:27
          %331 : int = aten::__getitem__(%323, %330) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %331) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.343)
      %333 : bool = aten::eq(%size_prods.341, %13) # torch/nn/functional.py:1994:7
       = prim::If(%333) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.221 : Tensor = aten::batch_norm(%out.220, %321, %322, %319, %320, %318, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.222 : Tensor = aten::relu_(%out.221) # torch/nn/functional.py:1117:17
  %336 : __torch__.torch.nn.modules.conv.___torch_mangle_981.Conv2d = prim::GetAttr[name="conv3"](%54)
  %337 : Tensor = prim::GetAttr[name="weight"](%336)
  %338 : Tensor? = prim::GetAttr[name="bias"](%336)
  %339 : int[] = prim::ListConstruct(%13, %13)
  %340 : int[] = prim::ListConstruct(%9, %9)
  %341 : int[] = prim::ListConstruct(%13, %13)
  %out.223 : Tensor = aten::conv2d(%out.222, %337, %338, %339, %340, %341, %13) # torch/nn/modules/conv.py:415:15
  %343 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%54)
  %344 : int = aten::dim(%out.223) # torch/nn/modules/batchnorm.py:276:11
  %345 : bool = aten::ne(%344, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%345) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %346 : bool = prim::GetAttr[name="training"](%343)
   = prim::If(%346) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%343)
      %348 : Tensor = aten::add(%347, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%343, %348)
      -> ()
    block1():
      -> ()
  %349 : bool = prim::GetAttr[name="training"](%343)
  %350 : Tensor = prim::GetAttr[name="running_mean"](%343)
  %351 : Tensor = prim::GetAttr[name="running_var"](%343)
  %352 : Tensor = prim::GetAttr[name="weight"](%343)
  %353 : Tensor = prim::GetAttr[name="bias"](%343)
   = prim::If(%349) # torch/nn/functional.py:2011:4
    block0():
      %354 : int[] = aten::size(%out.223) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%354, %9) # torch/nn/functional.py:1991:17
      %356 : int = aten::len(%354) # torch/nn/functional.py:1992:19
      %357 : int = aten::sub(%356, %11) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%357, %10, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %361 : int = aten::add(%i.87, %11) # torch/nn/functional.py:1993:27
          %362 : int = aten::__getitem__(%354, %361) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %362) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.347)
      %364 : bool = aten::eq(%size_prods.345, %13) # torch/nn/functional.py:1994:7
       = prim::If(%364) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.224 : Tensor = aten::batch_norm(%out.223, %352, %353, %350, %351, %349, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.225 : Tensor = aten::add_(%out.224, %input.8, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.11 : Tensor = aten::relu_(%out.225) # torch/nn/functional.py:1117:17
  %368 : __torch__.torch.nn.modules.container.___torch_mangle_991.Sequential = prim::GetAttr[name="layer2"](%self)
  %369 : __torch__.torchvision.models.resnet.___torch_mangle_988.Bottleneck = prim::GetAttr[name="0"](%368)
  %370 : __torch__.torchvision.models.resnet.___torch_mangle_990.Bottleneck = prim::GetAttr[name="1"](%368)
  %371 : __torch__.torchvision.models.resnet.___torch_mangle_990.Bottleneck = prim::GetAttr[name="2"](%368)
  %372 : __torch__.torchvision.models.resnet.___torch_mangle_990.Bottleneck = prim::GetAttr[name="3"](%368)
  %373 : __torch__.torch.nn.modules.conv.___torch_mangle_985.Conv2d = prim::GetAttr[name="conv1"](%369)
  %374 : Tensor = prim::GetAttr[name="weight"](%373)
  %375 : Tensor? = prim::GetAttr[name="bias"](%373)
  %376 : int[] = prim::ListConstruct(%13, %13)
  %377 : int[] = prim::ListConstruct(%9, %9)
  %378 : int[] = prim::ListConstruct(%13, %13)
  %out.244 : Tensor = aten::conv2d(%x.11, %374, %375, %376, %377, %378, %13) # torch/nn/modules/conv.py:415:15
  %380 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%369)
  %381 : int = aten::dim(%out.244) # torch/nn/modules/batchnorm.py:276:11
  %382 : bool = aten::ne(%381, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%382) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %383 : bool = prim::GetAttr[name="training"](%380)
   = prim::If(%383) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %384 : Tensor = prim::GetAttr[name="num_batches_tracked"](%380)
      %385 : Tensor = aten::add(%384, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%380, %385)
      -> ()
    block1():
      -> ()
  %386 : bool = prim::GetAttr[name="training"](%380)
  %387 : Tensor = prim::GetAttr[name="running_mean"](%380)
  %388 : Tensor = prim::GetAttr[name="running_var"](%380)
  %389 : Tensor = prim::GetAttr[name="weight"](%380)
  %390 : Tensor = prim::GetAttr[name="bias"](%380)
   = prim::If(%386) # torch/nn/functional.py:2011:4
    block0():
      %391 : int[] = aten::size(%out.244) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%391, %9) # torch/nn/functional.py:1991:17
      %393 : int = aten::len(%391) # torch/nn/functional.py:1992:19
      %394 : int = aten::sub(%393, %11) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%394, %10, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %398 : int = aten::add(%i.88, %11) # torch/nn/functional.py:1993:27
          %399 : int = aten::__getitem__(%391, %398) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %399) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.351)
      %401 : bool = aten::eq(%size_prods.349, %13) # torch/nn/functional.py:1994:7
       = prim::If(%401) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.227 : Tensor = aten::batch_norm(%out.244, %389, %390, %387, %388, %386, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.228 : Tensor = aten::relu_(%out.227) # torch/nn/functional.py:1117:17
  %404 : __torch__.torch.nn.modules.conv.___torch_mangle_986.Conv2d = prim::GetAttr[name="conv2"](%369)
  %405 : Tensor = prim::GetAttr[name="weight"](%404)
  %406 : Tensor? = prim::GetAttr[name="bias"](%404)
  %407 : int[] = prim::ListConstruct(%11, %11)
  %408 : int[] = prim::ListConstruct(%13, %13)
  %409 : int[] = prim::ListConstruct(%13, %13)
  %out.229 : Tensor = aten::conv2d(%out.228, %405, %406, %407, %408, %409, %3) # torch/nn/modules/conv.py:415:15
  %411 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%369)
  %412 : int = aten::dim(%out.229) # torch/nn/modules/batchnorm.py:276:11
  %413 : bool = aten::ne(%412, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%413) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %414 : bool = prim::GetAttr[name="training"](%411)
   = prim::If(%414) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %415 : Tensor = prim::GetAttr[name="num_batches_tracked"](%411)
      %416 : Tensor = aten::add(%415, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%411, %416)
      -> ()
    block1():
      -> ()
  %417 : bool = prim::GetAttr[name="training"](%411)
  %418 : Tensor = prim::GetAttr[name="running_mean"](%411)
  %419 : Tensor = prim::GetAttr[name="running_var"](%411)
  %420 : Tensor = prim::GetAttr[name="weight"](%411)
  %421 : Tensor = prim::GetAttr[name="bias"](%411)
   = prim::If(%417) # torch/nn/functional.py:2011:4
    block0():
      %422 : int[] = aten::size(%out.229) # torch/nn/functional.py:2012:27
      %size_prods.352 : int = aten::__getitem__(%422, %9) # torch/nn/functional.py:1991:17
      %424 : int = aten::len(%422) # torch/nn/functional.py:1992:19
      %425 : int = aten::sub(%424, %11) # torch/nn/functional.py:1992:19
      %size_prods.353 : int = prim::Loop(%425, %10, %size_prods.352) # torch/nn/functional.py:1992:4
        block0(%i.89 : int, %size_prods.354 : int):
          %429 : int = aten::add(%i.89, %11) # torch/nn/functional.py:1993:27
          %430 : int = aten::__getitem__(%422, %429) # torch/nn/functional.py:1993:22
          %size_prods.355 : int = aten::mul(%size_prods.354, %430) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.355)
      %432 : bool = aten::eq(%size_prods.353, %13) # torch/nn/functional.py:1994:7
       = prim::If(%432) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.230 : Tensor = aten::batch_norm(%out.229, %420, %421, %418, %419, %417, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.231 : Tensor = aten::relu_(%out.230) # torch/nn/functional.py:1117:17
  %435 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv3"](%369)
  %436 : Tensor = prim::GetAttr[name="weight"](%435)
  %437 : Tensor? = prim::GetAttr[name="bias"](%435)
  %438 : int[] = prim::ListConstruct(%13, %13)
  %439 : int[] = prim::ListConstruct(%9, %9)
  %440 : int[] = prim::ListConstruct(%13, %13)
  %out.232 : Tensor = aten::conv2d(%out.231, %436, %437, %438, %439, %440, %13) # torch/nn/modules/conv.py:415:15
  %442 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%369)
  %443 : int = aten::dim(%out.232) # torch/nn/modules/batchnorm.py:276:11
  %444 : bool = aten::ne(%443, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%444) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %445 : bool = prim::GetAttr[name="training"](%442)
   = prim::If(%445) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %446 : Tensor = prim::GetAttr[name="num_batches_tracked"](%442)
      %447 : Tensor = aten::add(%446, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%442, %447)
      -> ()
    block1():
      -> ()
  %448 : bool = prim::GetAttr[name="training"](%442)
  %449 : Tensor = prim::GetAttr[name="running_mean"](%442)
  %450 : Tensor = prim::GetAttr[name="running_var"](%442)
  %451 : Tensor = prim::GetAttr[name="weight"](%442)
  %452 : Tensor = prim::GetAttr[name="bias"](%442)
   = prim::If(%448) # torch/nn/functional.py:2011:4
    block0():
      %453 : int[] = aten::size(%out.232) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%453, %9) # torch/nn/functional.py:1991:17
      %455 : int = aten::len(%453) # torch/nn/functional.py:1992:19
      %456 : int = aten::sub(%455, %11) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%456, %10, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %460 : int = aten::add(%i.90, %11) # torch/nn/functional.py:1993:27
          %461 : int = aten::__getitem__(%453, %460) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %461) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.359)
      %463 : bool = aten::eq(%size_prods.357, %13) # torch/nn/functional.py:1994:7
       = prim::If(%463) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.233 : Tensor = aten::batch_norm(%out.232, %451, %452, %449, %450, %448, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %465 : __torch__.torch.nn.modules.container.___torch_mangle_23.Sequential = prim::GetAttr[name="downsample"](%369)
  %466 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="0"](%465)
  %467 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="1"](%465)
  %468 : Tensor = prim::GetAttr[name="weight"](%466)
  %469 : Tensor? = prim::GetAttr[name="bias"](%466)
  %470 : int[] = prim::ListConstruct(%11, %11)
  %471 : int[] = prim::ListConstruct(%9, %9)
  %472 : int[] = prim::ListConstruct(%13, %13)
  %input.14 : Tensor = aten::conv2d(%x.11, %468, %469, %470, %471, %472, %13) # torch/nn/modules/conv.py:415:15
  %474 : int = aten::dim(%input.14) # torch/nn/modules/batchnorm.py:276:11
  %475 : bool = aten::ne(%474, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%475) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %476 : bool = prim::GetAttr[name="training"](%467)
   = prim::If(%476) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %477 : Tensor = prim::GetAttr[name="num_batches_tracked"](%467)
      %478 : Tensor = aten::add(%477, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%467, %478)
      -> ()
    block1():
      -> ()
  %479 : bool = prim::GetAttr[name="training"](%467)
  %480 : Tensor = prim::GetAttr[name="running_mean"](%467)
  %481 : Tensor = prim::GetAttr[name="running_var"](%467)
  %482 : Tensor = prim::GetAttr[name="weight"](%467)
  %483 : Tensor = prim::GetAttr[name="bias"](%467)
   = prim::If(%479) # torch/nn/functional.py:2011:4
    block0():
      %484 : int[] = aten::size(%input.14) # torch/nn/functional.py:2012:27
      %size_prods.360 : int = aten::__getitem__(%484, %9) # torch/nn/functional.py:1991:17
      %486 : int = aten::len(%484) # torch/nn/functional.py:1992:19
      %487 : int = aten::sub(%486, %11) # torch/nn/functional.py:1992:19
      %size_prods.361 : int = prim::Loop(%487, %10, %size_prods.360) # torch/nn/functional.py:1992:4
        block0(%i.91 : int, %size_prods.362 : int):
          %491 : int = aten::add(%i.91, %11) # torch/nn/functional.py:1993:27
          %492 : int = aten::__getitem__(%484, %491) # torch/nn/functional.py:1993:22
          %size_prods.363 : int = aten::mul(%size_prods.362, %492) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.363)
      %494 : bool = aten::eq(%size_prods.361, %13) # torch/nn/functional.py:1994:7
       = prim::If(%494) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.3 : Tensor = aten::batch_norm(%input.14, %482, %483, %480, %481, %479, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.234 : Tensor = aten::add_(%out.233, %identity.3, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.18 : Tensor = aten::relu_(%out.234) # torch/nn/functional.py:1117:17
  %498 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv1"](%370)
  %499 : Tensor = prim::GetAttr[name="weight"](%498)
  %500 : Tensor? = prim::GetAttr[name="bias"](%498)
  %501 : int[] = prim::ListConstruct(%13, %13)
  %502 : int[] = prim::ListConstruct(%9, %9)
  %503 : int[] = prim::ListConstruct(%13, %13)
  %out.253 : Tensor = aten::conv2d(%input.18, %499, %500, %501, %502, %503, %13) # torch/nn/modules/conv.py:415:15
  %505 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%370)
  %506 : int = aten::dim(%out.253) # torch/nn/modules/batchnorm.py:276:11
  %507 : bool = aten::ne(%506, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%507) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %508 : bool = prim::GetAttr[name="training"](%505)
   = prim::If(%508) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %509 : Tensor = prim::GetAttr[name="num_batches_tracked"](%505)
      %510 : Tensor = aten::add(%509, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%505, %510)
      -> ()
    block1():
      -> ()
  %511 : bool = prim::GetAttr[name="training"](%505)
  %512 : Tensor = prim::GetAttr[name="running_mean"](%505)
  %513 : Tensor = prim::GetAttr[name="running_var"](%505)
  %514 : Tensor = prim::GetAttr[name="weight"](%505)
  %515 : Tensor = prim::GetAttr[name="bias"](%505)
   = prim::If(%511) # torch/nn/functional.py:2011:4
    block0():
      %516 : int[] = aten::size(%out.253) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%516, %9) # torch/nn/functional.py:1991:17
      %518 : int = aten::len(%516) # torch/nn/functional.py:1992:19
      %519 : int = aten::sub(%518, %11) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%519, %10, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %523 : int = aten::add(%i.71, %11) # torch/nn/functional.py:1993:27
          %524 : int = aten::__getitem__(%516, %523) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %524) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.283)
      %526 : bool = aten::eq(%size_prods.281, %13) # torch/nn/functional.py:1994:7
       = prim::If(%526) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.236 : Tensor = aten::batch_norm(%out.253, %514, %515, %512, %513, %511, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.237 : Tensor = aten::relu_(%out.236) # torch/nn/functional.py:1117:17
  %529 : __torch__.torch.nn.modules.conv.___torch_mangle_989.Conv2d = prim::GetAttr[name="conv2"](%370)
  %530 : Tensor = prim::GetAttr[name="weight"](%529)
  %531 : Tensor? = prim::GetAttr[name="bias"](%529)
  %532 : int[] = prim::ListConstruct(%13, %13)
  %533 : int[] = prim::ListConstruct(%13, %13)
  %534 : int[] = prim::ListConstruct(%13, %13)
  %out.238 : Tensor = aten::conv2d(%out.237, %530, %531, %532, %533, %534, %3) # torch/nn/modules/conv.py:415:15
  %536 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%370)
  %537 : int = aten::dim(%out.238) # torch/nn/modules/batchnorm.py:276:11
  %538 : bool = aten::ne(%537, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%538) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %539 : bool = prim::GetAttr[name="training"](%536)
   = prim::If(%539) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %540 : Tensor = prim::GetAttr[name="num_batches_tracked"](%536)
      %541 : Tensor = aten::add(%540, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%536, %541)
      -> ()
    block1():
      -> ()
  %542 : bool = prim::GetAttr[name="training"](%536)
  %543 : Tensor = prim::GetAttr[name="running_mean"](%536)
  %544 : Tensor = prim::GetAttr[name="running_var"](%536)
  %545 : Tensor = prim::GetAttr[name="weight"](%536)
  %546 : Tensor = prim::GetAttr[name="bias"](%536)
   = prim::If(%542) # torch/nn/functional.py:2011:4
    block0():
      %547 : int[] = aten::size(%out.238) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%547, %9) # torch/nn/functional.py:1991:17
      %549 : int = aten::len(%547) # torch/nn/functional.py:1992:19
      %550 : int = aten::sub(%549, %11) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%550, %10, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %554 : int = aten::add(%i.72, %11) # torch/nn/functional.py:1993:27
          %555 : int = aten::__getitem__(%547, %554) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %555) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.287)
      %557 : bool = aten::eq(%size_prods.285, %13) # torch/nn/functional.py:1994:7
       = prim::If(%557) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.239 : Tensor = aten::batch_norm(%out.238, %545, %546, %543, %544, %542, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.240 : Tensor = aten::relu_(%out.239) # torch/nn/functional.py:1117:17
  %560 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv3"](%370)
  %561 : Tensor = prim::GetAttr[name="weight"](%560)
  %562 : Tensor? = prim::GetAttr[name="bias"](%560)
  %563 : int[] = prim::ListConstruct(%13, %13)
  %564 : int[] = prim::ListConstruct(%9, %9)
  %565 : int[] = prim::ListConstruct(%13, %13)
  %out.241 : Tensor = aten::conv2d(%out.240, %561, %562, %563, %564, %565, %13) # torch/nn/modules/conv.py:415:15
  %567 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%370)
  %568 : int = aten::dim(%out.241) # torch/nn/modules/batchnorm.py:276:11
  %569 : bool = aten::ne(%568, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%569) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %570 : bool = prim::GetAttr[name="training"](%567)
   = prim::If(%570) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %571 : Tensor = prim::GetAttr[name="num_batches_tracked"](%567)
      %572 : Tensor = aten::add(%571, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%567, %572)
      -> ()
    block1():
      -> ()
  %573 : bool = prim::GetAttr[name="training"](%567)
  %574 : Tensor = prim::GetAttr[name="running_mean"](%567)
  %575 : Tensor = prim::GetAttr[name="running_var"](%567)
  %576 : Tensor = prim::GetAttr[name="weight"](%567)
  %577 : Tensor = prim::GetAttr[name="bias"](%567)
   = prim::If(%573) # torch/nn/functional.py:2011:4
    block0():
      %578 : int[] = aten::size(%out.241) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%578, %9) # torch/nn/functional.py:1991:17
      %580 : int = aten::len(%578) # torch/nn/functional.py:1992:19
      %581 : int = aten::sub(%580, %11) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%581, %10, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %585 : int = aten::add(%i.73, %11) # torch/nn/functional.py:1993:27
          %586 : int = aten::__getitem__(%578, %585) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %586) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.291)
      %588 : bool = aten::eq(%size_prods.289, %13) # torch/nn/functional.py:1994:7
       = prim::If(%588) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.242 : Tensor = aten::batch_norm(%out.241, %576, %577, %574, %575, %573, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.243 : Tensor = aten::add_(%out.242, %input.18, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.12 : Tensor = aten::relu_(%out.243) # torch/nn/functional.py:1117:17
  %592 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv1"](%371)
  %593 : Tensor = prim::GetAttr[name="weight"](%592)
  %594 : Tensor? = prim::GetAttr[name="bias"](%592)
  %595 : int[] = prim::ListConstruct(%13, %13)
  %596 : int[] = prim::ListConstruct(%9, %9)
  %597 : int[] = prim::ListConstruct(%13, %13)
  %out.217 : Tensor = aten::conv2d(%input.12, %593, %594, %595, %596, %597, %13) # torch/nn/modules/conv.py:415:15
  %599 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%371)
  %600 : int = aten::dim(%out.217) # torch/nn/modules/batchnorm.py:276:11
  %601 : bool = aten::ne(%600, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%601) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %602 : bool = prim::GetAttr[name="training"](%599)
   = prim::If(%602) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %603 : Tensor = prim::GetAttr[name="num_batches_tracked"](%599)
      %604 : Tensor = aten::add(%603, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%599, %604)
      -> ()
    block1():
      -> ()
  %605 : bool = prim::GetAttr[name="training"](%599)
  %606 : Tensor = prim::GetAttr[name="running_mean"](%599)
  %607 : Tensor = prim::GetAttr[name="running_var"](%599)
  %608 : Tensor = prim::GetAttr[name="weight"](%599)
  %609 : Tensor = prim::GetAttr[name="bias"](%599)
   = prim::If(%605) # torch/nn/functional.py:2011:4
    block0():
      %610 : int[] = aten::size(%out.217) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%610, %9) # torch/nn/functional.py:1991:17
      %612 : int = aten::len(%610) # torch/nn/functional.py:1992:19
      %613 : int = aten::sub(%612, %11) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%613, %10, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %617 : int = aten::add(%i.74, %11) # torch/nn/functional.py:1993:27
          %618 : int = aten::__getitem__(%610, %617) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %618) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.295)
      %620 : bool = aten::eq(%size_prods.293, %13) # torch/nn/functional.py:1994:7
       = prim::If(%620) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.245 : Tensor = aten::batch_norm(%out.217, %608, %609, %606, %607, %605, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.246 : Tensor = aten::relu_(%out.245) # torch/nn/functional.py:1117:17
  %623 : __torch__.torch.nn.modules.conv.___torch_mangle_989.Conv2d = prim::GetAttr[name="conv2"](%371)
  %624 : Tensor = prim::GetAttr[name="weight"](%623)
  %625 : Tensor? = prim::GetAttr[name="bias"](%623)
  %626 : int[] = prim::ListConstruct(%13, %13)
  %627 : int[] = prim::ListConstruct(%13, %13)
  %628 : int[] = prim::ListConstruct(%13, %13)
  %out.247 : Tensor = aten::conv2d(%out.246, %624, %625, %626, %627, %628, %3) # torch/nn/modules/conv.py:415:15
  %630 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%371)
  %631 : int = aten::dim(%out.247) # torch/nn/modules/batchnorm.py:276:11
  %632 : bool = aten::ne(%631, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%632) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %633 : bool = prim::GetAttr[name="training"](%630)
   = prim::If(%633) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %634 : Tensor = prim::GetAttr[name="num_batches_tracked"](%630)
      %635 : Tensor = aten::add(%634, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%630, %635)
      -> ()
    block1():
      -> ()
  %636 : bool = prim::GetAttr[name="training"](%630)
  %637 : Tensor = prim::GetAttr[name="running_mean"](%630)
  %638 : Tensor = prim::GetAttr[name="running_var"](%630)
  %639 : Tensor = prim::GetAttr[name="weight"](%630)
  %640 : Tensor = prim::GetAttr[name="bias"](%630)
   = prim::If(%636) # torch/nn/functional.py:2011:4
    block0():
      %641 : int[] = aten::size(%out.247) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%641, %9) # torch/nn/functional.py:1991:17
      %643 : int = aten::len(%641) # torch/nn/functional.py:1992:19
      %644 : int = aten::sub(%643, %11) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%644, %10, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %648 : int = aten::add(%i.75, %11) # torch/nn/functional.py:1993:27
          %649 : int = aten::__getitem__(%641, %648) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %649) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.299)
      %651 : bool = aten::eq(%size_prods.297, %13) # torch/nn/functional.py:1994:7
       = prim::If(%651) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.248 : Tensor = aten::batch_norm(%out.247, %639, %640, %637, %638, %636, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.249 : Tensor = aten::relu_(%out.248) # torch/nn/functional.py:1117:17
  %654 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv3"](%371)
  %655 : Tensor = prim::GetAttr[name="weight"](%654)
  %656 : Tensor? = prim::GetAttr[name="bias"](%654)
  %657 : int[] = prim::ListConstruct(%13, %13)
  %658 : int[] = prim::ListConstruct(%9, %9)
  %659 : int[] = prim::ListConstruct(%13, %13)
  %out.250 : Tensor = aten::conv2d(%out.249, %655, %656, %657, %658, %659, %13) # torch/nn/modules/conv.py:415:15
  %661 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%371)
  %662 : int = aten::dim(%out.250) # torch/nn/modules/batchnorm.py:276:11
  %663 : bool = aten::ne(%662, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%663) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %664 : bool = prim::GetAttr[name="training"](%661)
   = prim::If(%664) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %665 : Tensor = prim::GetAttr[name="num_batches_tracked"](%661)
      %666 : Tensor = aten::add(%665, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%661, %666)
      -> ()
    block1():
      -> ()
  %667 : bool = prim::GetAttr[name="training"](%661)
  %668 : Tensor = prim::GetAttr[name="running_mean"](%661)
  %669 : Tensor = prim::GetAttr[name="running_var"](%661)
  %670 : Tensor = prim::GetAttr[name="weight"](%661)
  %671 : Tensor = prim::GetAttr[name="bias"](%661)
   = prim::If(%667) # torch/nn/functional.py:2011:4
    block0():
      %672 : int[] = aten::size(%out.250) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%672, %9) # torch/nn/functional.py:1991:17
      %674 : int = aten::len(%672) # torch/nn/functional.py:1992:19
      %675 : int = aten::sub(%674, %11) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%675, %10, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %679 : int = aten::add(%i.76, %11) # torch/nn/functional.py:1993:27
          %680 : int = aten::__getitem__(%672, %679) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %680) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.303)
      %682 : bool = aten::eq(%size_prods.301, %13) # torch/nn/functional.py:1994:7
       = prim::If(%682) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.251 : Tensor = aten::batch_norm(%out.250, %670, %671, %668, %669, %667, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.252 : Tensor = aten::add_(%out.251, %input.12, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.16 : Tensor = aten::relu_(%out.252) # torch/nn/functional.py:1117:17
  %686 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv1"](%372)
  %687 : Tensor = prim::GetAttr[name="weight"](%686)
  %688 : Tensor? = prim::GetAttr[name="bias"](%686)
  %689 : int[] = prim::ListConstruct(%13, %13)
  %690 : int[] = prim::ListConstruct(%9, %9)
  %691 : int[] = prim::ListConstruct(%13, %13)
  %out.262 : Tensor = aten::conv2d(%input.16, %687, %688, %689, %690, %691, %13) # torch/nn/modules/conv.py:415:15
  %693 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%372)
  %694 : int = aten::dim(%out.262) # torch/nn/modules/batchnorm.py:276:11
  %695 : bool = aten::ne(%694, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%695) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %696 : bool = prim::GetAttr[name="training"](%693)
   = prim::If(%696) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %697 : Tensor = prim::GetAttr[name="num_batches_tracked"](%693)
      %698 : Tensor = aten::add(%697, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%693, %698)
      -> ()
    block1():
      -> ()
  %699 : bool = prim::GetAttr[name="training"](%693)
  %700 : Tensor = prim::GetAttr[name="running_mean"](%693)
  %701 : Tensor = prim::GetAttr[name="running_var"](%693)
  %702 : Tensor = prim::GetAttr[name="weight"](%693)
  %703 : Tensor = prim::GetAttr[name="bias"](%693)
   = prim::If(%699) # torch/nn/functional.py:2011:4
    block0():
      %704 : int[] = aten::size(%out.262) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%704, %9) # torch/nn/functional.py:1991:17
      %706 : int = aten::len(%704) # torch/nn/functional.py:1992:19
      %707 : int = aten::sub(%706, %11) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%707, %10, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %711 : int = aten::add(%i.92, %11) # torch/nn/functional.py:1993:27
          %712 : int = aten::__getitem__(%704, %711) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %712) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.367)
      %714 : bool = aten::eq(%size_prods.365, %13) # torch/nn/functional.py:1994:7
       = prim::If(%714) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.263 : Tensor = aten::batch_norm(%out.262, %702, %703, %700, %701, %699, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.264 : Tensor = aten::relu_(%out.263) # torch/nn/functional.py:1117:17
  %717 : __torch__.torch.nn.modules.conv.___torch_mangle_989.Conv2d = prim::GetAttr[name="conv2"](%372)
  %718 : Tensor = prim::GetAttr[name="weight"](%717)
  %719 : Tensor? = prim::GetAttr[name="bias"](%717)
  %720 : int[] = prim::ListConstruct(%13, %13)
  %721 : int[] = prim::ListConstruct(%13, %13)
  %722 : int[] = prim::ListConstruct(%13, %13)
  %out.265 : Tensor = aten::conv2d(%out.264, %718, %719, %720, %721, %722, %3) # torch/nn/modules/conv.py:415:15
  %724 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%372)
  %725 : int = aten::dim(%out.265) # torch/nn/modules/batchnorm.py:276:11
  %726 : bool = aten::ne(%725, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%726) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %727 : bool = prim::GetAttr[name="training"](%724)
   = prim::If(%727) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %728 : Tensor = prim::GetAttr[name="num_batches_tracked"](%724)
      %729 : Tensor = aten::add(%728, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%724, %729)
      -> ()
    block1():
      -> ()
  %730 : bool = prim::GetAttr[name="training"](%724)
  %731 : Tensor = prim::GetAttr[name="running_mean"](%724)
  %732 : Tensor = prim::GetAttr[name="running_var"](%724)
  %733 : Tensor = prim::GetAttr[name="weight"](%724)
  %734 : Tensor = prim::GetAttr[name="bias"](%724)
   = prim::If(%730) # torch/nn/functional.py:2011:4
    block0():
      %735 : int[] = aten::size(%out.265) # torch/nn/functional.py:2012:27
      %size_prods.368 : int = aten::__getitem__(%735, %9) # torch/nn/functional.py:1991:17
      %737 : int = aten::len(%735) # torch/nn/functional.py:1992:19
      %738 : int = aten::sub(%737, %11) # torch/nn/functional.py:1992:19
      %size_prods.369 : int = prim::Loop(%738, %10, %size_prods.368) # torch/nn/functional.py:1992:4
        block0(%i.93 : int, %size_prods.370 : int):
          %742 : int = aten::add(%i.93, %11) # torch/nn/functional.py:1993:27
          %743 : int = aten::__getitem__(%735, %742) # torch/nn/functional.py:1993:22
          %size_prods.371 : int = aten::mul(%size_prods.370, %743) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.371)
      %745 : bool = aten::eq(%size_prods.369, %13) # torch/nn/functional.py:1994:7
       = prim::If(%745) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.266 : Tensor = aten::batch_norm(%out.265, %733, %734, %731, %732, %730, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.267 : Tensor = aten::relu_(%out.266) # torch/nn/functional.py:1117:17
  %748 : __torch__.torch.nn.modules.conv.___torch_mangle_987.Conv2d = prim::GetAttr[name="conv3"](%372)
  %749 : Tensor = prim::GetAttr[name="weight"](%748)
  %750 : Tensor? = prim::GetAttr[name="bias"](%748)
  %751 : int[] = prim::ListConstruct(%13, %13)
  %752 : int[] = prim::ListConstruct(%9, %9)
  %753 : int[] = prim::ListConstruct(%13, %13)
  %out.268 : Tensor = aten::conv2d(%out.267, %749, %750, %751, %752, %753, %13) # torch/nn/modules/conv.py:415:15
  %755 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%372)
  %756 : int = aten::dim(%out.268) # torch/nn/modules/batchnorm.py:276:11
  %757 : bool = aten::ne(%756, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%757) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %758 : bool = prim::GetAttr[name="training"](%755)
   = prim::If(%758) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %759 : Tensor = prim::GetAttr[name="num_batches_tracked"](%755)
      %760 : Tensor = aten::add(%759, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%755, %760)
      -> ()
    block1():
      -> ()
  %761 : bool = prim::GetAttr[name="training"](%755)
  %762 : Tensor = prim::GetAttr[name="running_mean"](%755)
  %763 : Tensor = prim::GetAttr[name="running_var"](%755)
  %764 : Tensor = prim::GetAttr[name="weight"](%755)
  %765 : Tensor = prim::GetAttr[name="bias"](%755)
   = prim::If(%761) # torch/nn/functional.py:2011:4
    block0():
      %766 : int[] = aten::size(%out.268) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%766, %9) # torch/nn/functional.py:1991:17
      %768 : int = aten::len(%766) # torch/nn/functional.py:1992:19
      %769 : int = aten::sub(%768, %11) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%769, %10, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %773 : int = aten::add(%i.94, %11) # torch/nn/functional.py:1993:27
          %774 : int = aten::__getitem__(%766, %773) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %774) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.375)
      %776 : bool = aten::eq(%size_prods.373, %13) # torch/nn/functional.py:1994:7
       = prim::If(%776) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.269 : Tensor = aten::batch_norm(%out.268, %764, %765, %762, %763, %761, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.270 : Tensor = aten::add_(%out.269, %input.16, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.13 : Tensor = aten::relu_(%out.270) # torch/nn/functional.py:1117:17
  %780 : __torch__.torch.nn.modules.container.___torch_mangle_997.Sequential = prim::GetAttr[name="layer3"](%self)
  %781 : __torch__.torchvision.models.resnet.___torch_mangle_994.Bottleneck = prim::GetAttr[name="0"](%780)
  %782 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="1"](%780)
  %783 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="2"](%780)
  %784 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="3"](%780)
  %785 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="4"](%780)
  %786 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="5"](%780)
  %787 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="6"](%780)
  %788 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="7"](%780)
  %789 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="8"](%780)
  %790 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="9"](%780)
  %791 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="10"](%780)
  %792 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="11"](%780)
  %793 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="12"](%780)
  %794 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="13"](%780)
  %795 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="14"](%780)
  %796 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="15"](%780)
  %797 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="16"](%780)
  %798 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="17"](%780)
  %799 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="18"](%780)
  %800 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="19"](%780)
  %801 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="20"](%780)
  %802 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="21"](%780)
  %803 : __torch__.torchvision.models.resnet.___torch_mangle_996.Bottleneck = prim::GetAttr[name="22"](%780)
  %804 : __torch__.torch.nn.modules.conv.___torch_mangle_32.Conv2d = prim::GetAttr[name="conv1"](%781)
  %805 : Tensor = prim::GetAttr[name="weight"](%804)
  %806 : Tensor? = prim::GetAttr[name="bias"](%804)
  %807 : int[] = prim::ListConstruct(%13, %13)
  %808 : int[] = prim::ListConstruct(%9, %9)
  %809 : int[] = prim::ListConstruct(%13, %13)
  %out.271 : Tensor = aten::conv2d(%x.13, %805, %806, %807, %808, %809, %13) # torch/nn/modules/conv.py:415:15
  %811 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%781)
  %812 : int = aten::dim(%out.271) # torch/nn/modules/batchnorm.py:276:11
  %813 : bool = aten::ne(%812, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%813) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %814 : bool = prim::GetAttr[name="training"](%811)
   = prim::If(%814) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %815 : Tensor = prim::GetAttr[name="num_batches_tracked"](%811)
      %816 : Tensor = aten::add(%815, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%811, %816)
      -> ()
    block1():
      -> ()
  %817 : bool = prim::GetAttr[name="training"](%811)
  %818 : Tensor = prim::GetAttr[name="running_mean"](%811)
  %819 : Tensor = prim::GetAttr[name="running_var"](%811)
  %820 : Tensor = prim::GetAttr[name="weight"](%811)
  %821 : Tensor = prim::GetAttr[name="bias"](%811)
   = prim::If(%817) # torch/nn/functional.py:2011:4
    block0():
      %822 : int[] = aten::size(%out.271) # torch/nn/functional.py:2012:27
      %size_prods.376 : int = aten::__getitem__(%822, %9) # torch/nn/functional.py:1991:17
      %824 : int = aten::len(%822) # torch/nn/functional.py:1992:19
      %825 : int = aten::sub(%824, %11) # torch/nn/functional.py:1992:19
      %size_prods.377 : int = prim::Loop(%825, %10, %size_prods.376) # torch/nn/functional.py:1992:4
        block0(%i.95 : int, %size_prods.378 : int):
          %829 : int = aten::add(%i.95, %11) # torch/nn/functional.py:1993:27
          %830 : int = aten::__getitem__(%822, %829) # torch/nn/functional.py:1993:22
          %size_prods.379 : int = aten::mul(%size_prods.378, %830) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.379)
      %832 : bool = aten::eq(%size_prods.377, %13) # torch/nn/functional.py:1994:7
       = prim::If(%832) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.272 : Tensor = aten::batch_norm(%out.271, %820, %821, %818, %819, %817, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.273 : Tensor = aten::relu_(%out.272) # torch/nn/functional.py:1117:17
  %835 : __torch__.torch.nn.modules.conv.___torch_mangle_992.Conv2d = prim::GetAttr[name="conv2"](%781)
  %836 : Tensor = prim::GetAttr[name="weight"](%835)
  %837 : Tensor? = prim::GetAttr[name="bias"](%835)
  %838 : int[] = prim::ListConstruct(%11, %11)
  %839 : int[] = prim::ListConstruct(%13, %13)
  %840 : int[] = prim::ListConstruct(%13, %13)
  %out.274 : Tensor = aten::conv2d(%out.273, %836, %837, %838, %839, %840, %3) # torch/nn/modules/conv.py:415:15
  %842 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%781)
  %843 : int = aten::dim(%out.274) # torch/nn/modules/batchnorm.py:276:11
  %844 : bool = aten::ne(%843, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%844) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %845 : bool = prim::GetAttr[name="training"](%842)
   = prim::If(%845) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %846 : Tensor = prim::GetAttr[name="num_batches_tracked"](%842)
      %847 : Tensor = aten::add(%846, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%842, %847)
      -> ()
    block1():
      -> ()
  %848 : bool = prim::GetAttr[name="training"](%842)
  %849 : Tensor = prim::GetAttr[name="running_mean"](%842)
  %850 : Tensor = prim::GetAttr[name="running_var"](%842)
  %851 : Tensor = prim::GetAttr[name="weight"](%842)
  %852 : Tensor = prim::GetAttr[name="bias"](%842)
   = prim::If(%848) # torch/nn/functional.py:2011:4
    block0():
      %853 : int[] = aten::size(%out.274) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%853, %9) # torch/nn/functional.py:1991:17
      %855 : int = aten::len(%853) # torch/nn/functional.py:1992:19
      %856 : int = aten::sub(%855, %11) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%856, %10, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %860 : int = aten::add(%i.96, %11) # torch/nn/functional.py:1993:27
          %861 : int = aten::__getitem__(%853, %860) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %861) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.383)
      %863 : bool = aten::eq(%size_prods.381, %13) # torch/nn/functional.py:1994:7
       = prim::If(%863) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.275 : Tensor = aten::batch_norm(%out.274, %851, %852, %849, %850, %848, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.276 : Tensor = aten::relu_(%out.275) # torch/nn/functional.py:1117:17
  %866 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%781)
  %867 : Tensor = prim::GetAttr[name="weight"](%866)
  %868 : Tensor? = prim::GetAttr[name="bias"](%866)
  %869 : int[] = prim::ListConstruct(%13, %13)
  %870 : int[] = prim::ListConstruct(%9, %9)
  %871 : int[] = prim::ListConstruct(%13, %13)
  %out.277 : Tensor = aten::conv2d(%out.276, %867, %868, %869, %870, %871, %13) # torch/nn/modules/conv.py:415:15
  %873 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%781)
  %874 : int = aten::dim(%out.277) # torch/nn/modules/batchnorm.py:276:11
  %875 : bool = aten::ne(%874, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%875) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %876 : bool = prim::GetAttr[name="training"](%873)
   = prim::If(%876) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %877 : Tensor = prim::GetAttr[name="num_batches_tracked"](%873)
      %878 : Tensor = aten::add(%877, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%873, %878)
      -> ()
    block1():
      -> ()
  %879 : bool = prim::GetAttr[name="training"](%873)
  %880 : Tensor = prim::GetAttr[name="running_mean"](%873)
  %881 : Tensor = prim::GetAttr[name="running_var"](%873)
  %882 : Tensor = prim::GetAttr[name="weight"](%873)
  %883 : Tensor = prim::GetAttr[name="bias"](%873)
   = prim::If(%879) # torch/nn/functional.py:2011:4
    block0():
      %884 : int[] = aten::size(%out.277) # torch/nn/functional.py:2012:27
      %size_prods.384 : int = aten::__getitem__(%884, %9) # torch/nn/functional.py:1991:17
      %886 : int = aten::len(%884) # torch/nn/functional.py:1992:19
      %887 : int = aten::sub(%886, %11) # torch/nn/functional.py:1992:19
      %size_prods.385 : int = prim::Loop(%887, %10, %size_prods.384) # torch/nn/functional.py:1992:4
        block0(%i.97 : int, %size_prods.386 : int):
          %891 : int = aten::add(%i.97, %11) # torch/nn/functional.py:1993:27
          %892 : int = aten::__getitem__(%884, %891) # torch/nn/functional.py:1993:22
          %size_prods.387 : int = aten::mul(%size_prods.386, %892) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.387)
      %894 : bool = aten::eq(%size_prods.385, %13) # torch/nn/functional.py:1994:7
       = prim::If(%894) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.278 : Tensor = aten::batch_norm(%out.277, %882, %883, %880, %881, %879, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %896 : __torch__.torch.nn.modules.container.___torch_mangle_940.Sequential = prim::GetAttr[name="downsample"](%781)
  %897 : __torch__.torch.nn.modules.conv.___torch_mangle_939.Conv2d = prim::GetAttr[name="0"](%896)
  %898 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="1"](%896)
  %899 : Tensor = prim::GetAttr[name="weight"](%897)
  %900 : Tensor? = prim::GetAttr[name="bias"](%897)
  %901 : int[] = prim::ListConstruct(%11, %11)
  %902 : int[] = prim::ListConstruct(%9, %9)
  %903 : int[] = prim::ListConstruct(%13, %13)
  %input.20 : Tensor = aten::conv2d(%x.13, %899, %900, %901, %902, %903, %13) # torch/nn/modules/conv.py:415:15
  %905 : int = aten::dim(%input.20) # torch/nn/modules/batchnorm.py:276:11
  %906 : bool = aten::ne(%905, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%906) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %907 : bool = prim::GetAttr[name="training"](%898)
   = prim::If(%907) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %908 : Tensor = prim::GetAttr[name="num_batches_tracked"](%898)
      %909 : Tensor = aten::add(%908, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%898, %909)
      -> ()
    block1():
      -> ()
  %910 : bool = prim::GetAttr[name="training"](%898)
  %911 : Tensor = prim::GetAttr[name="running_mean"](%898)
  %912 : Tensor = prim::GetAttr[name="running_var"](%898)
  %913 : Tensor = prim::GetAttr[name="weight"](%898)
  %914 : Tensor = prim::GetAttr[name="bias"](%898)
   = prim::If(%910) # torch/nn/functional.py:2011:4
    block0():
      %915 : int[] = aten::size(%input.20) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%915, %9) # torch/nn/functional.py:1991:17
      %917 : int = aten::len(%915) # torch/nn/functional.py:1992:19
      %918 : int = aten::sub(%917, %11) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%918, %10, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %922 : int = aten::add(%i.98, %11) # torch/nn/functional.py:1993:27
          %923 : int = aten::__getitem__(%915, %922) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %923) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.391)
      %925 : bool = aten::eq(%size_prods.389, %13) # torch/nn/functional.py:1994:7
       = prim::If(%925) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.4 : Tensor = aten::batch_norm(%input.20, %913, %914, %911, %912, %910, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.279 : Tensor = aten::add_(%out.278, %identity.4, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.22 : Tensor = aten::relu_(%out.279) # torch/nn/functional.py:1117:17
  %929 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%782)
  %930 : Tensor = prim::GetAttr[name="weight"](%929)
  %931 : Tensor? = prim::GetAttr[name="bias"](%929)
  %932 : int[] = prim::ListConstruct(%13, %13)
  %933 : int[] = prim::ListConstruct(%9, %9)
  %934 : int[] = prim::ListConstruct(%13, %13)
  %out.280 : Tensor = aten::conv2d(%input.22, %930, %931, %932, %933, %934, %13) # torch/nn/modules/conv.py:415:15
  %936 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%782)
  %937 : int = aten::dim(%out.280) # torch/nn/modules/batchnorm.py:276:11
  %938 : bool = aten::ne(%937, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%938) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %939 : bool = prim::GetAttr[name="training"](%936)
   = prim::If(%939) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %940 : Tensor = prim::GetAttr[name="num_batches_tracked"](%936)
      %941 : Tensor = aten::add(%940, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%936, %941)
      -> ()
    block1():
      -> ()
  %942 : bool = prim::GetAttr[name="training"](%936)
  %943 : Tensor = prim::GetAttr[name="running_mean"](%936)
  %944 : Tensor = prim::GetAttr[name="running_var"](%936)
  %945 : Tensor = prim::GetAttr[name="weight"](%936)
  %946 : Tensor = prim::GetAttr[name="bias"](%936)
   = prim::If(%942) # torch/nn/functional.py:2011:4
    block0():
      %947 : int[] = aten::size(%out.280) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%947, %9) # torch/nn/functional.py:1991:17
      %949 : int = aten::len(%947) # torch/nn/functional.py:1992:19
      %950 : int = aten::sub(%949, %11) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%950, %10, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %954 : int = aten::add(%i.99, %11) # torch/nn/functional.py:1993:27
          %955 : int = aten::__getitem__(%947, %954) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %955) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.395)
      %957 : bool = aten::eq(%size_prods.393, %13) # torch/nn/functional.py:1994:7
       = prim::If(%957) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.281 : Tensor = aten::batch_norm(%out.280, %945, %946, %943, %944, %942, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.282 : Tensor = aten::relu_(%out.281) # torch/nn/functional.py:1117:17
  %960 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%782)
  %961 : Tensor = prim::GetAttr[name="weight"](%960)
  %962 : Tensor? = prim::GetAttr[name="bias"](%960)
  %963 : int[] = prim::ListConstruct(%13, %13)
  %964 : int[] = prim::ListConstruct(%13, %13)
  %965 : int[] = prim::ListConstruct(%13, %13)
  %out.283 : Tensor = aten::conv2d(%out.282, %961, %962, %963, %964, %965, %3) # torch/nn/modules/conv.py:415:15
  %967 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%782)
  %968 : int = aten::dim(%out.283) # torch/nn/modules/batchnorm.py:276:11
  %969 : bool = aten::ne(%968, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%969) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %970 : bool = prim::GetAttr[name="training"](%967)
   = prim::If(%970) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %971 : Tensor = prim::GetAttr[name="num_batches_tracked"](%967)
      %972 : Tensor = aten::add(%971, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%967, %972)
      -> ()
    block1():
      -> ()
  %973 : bool = prim::GetAttr[name="training"](%967)
  %974 : Tensor = prim::GetAttr[name="running_mean"](%967)
  %975 : Tensor = prim::GetAttr[name="running_var"](%967)
  %976 : Tensor = prim::GetAttr[name="weight"](%967)
  %977 : Tensor = prim::GetAttr[name="bias"](%967)
   = prim::If(%973) # torch/nn/functional.py:2011:4
    block0():
      %978 : int[] = aten::size(%out.283) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%978, %9) # torch/nn/functional.py:1991:17
      %980 : int = aten::len(%978) # torch/nn/functional.py:1992:19
      %981 : int = aten::sub(%980, %11) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%981, %10, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %985 : int = aten::add(%i.100, %11) # torch/nn/functional.py:1993:27
          %986 : int = aten::__getitem__(%978, %985) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %986) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.399)
      %988 : bool = aten::eq(%size_prods.397, %13) # torch/nn/functional.py:1994:7
       = prim::If(%988) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.284 : Tensor = aten::batch_norm(%out.283, %976, %977, %974, %975, %973, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.285 : Tensor = aten::relu_(%out.284) # torch/nn/functional.py:1117:17
  %991 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%782)
  %992 : Tensor = prim::GetAttr[name="weight"](%991)
  %993 : Tensor? = prim::GetAttr[name="bias"](%991)
  %994 : int[] = prim::ListConstruct(%13, %13)
  %995 : int[] = prim::ListConstruct(%9, %9)
  %996 : int[] = prim::ListConstruct(%13, %13)
  %out.286 : Tensor = aten::conv2d(%out.285, %992, %993, %994, %995, %996, %13) # torch/nn/modules/conv.py:415:15
  %998 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%782)
  %999 : int = aten::dim(%out.286) # torch/nn/modules/batchnorm.py:276:11
  %1000 : bool = aten::ne(%999, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1000) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1001 : bool = prim::GetAttr[name="training"](%998)
   = prim::If(%1001) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1002 : Tensor = prim::GetAttr[name="num_batches_tracked"](%998)
      %1003 : Tensor = aten::add(%1002, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%998, %1003)
      -> ()
    block1():
      -> ()
  %1004 : bool = prim::GetAttr[name="training"](%998)
  %1005 : Tensor = prim::GetAttr[name="running_mean"](%998)
  %1006 : Tensor = prim::GetAttr[name="running_var"](%998)
  %1007 : Tensor = prim::GetAttr[name="weight"](%998)
  %1008 : Tensor = prim::GetAttr[name="bias"](%998)
   = prim::If(%1004) # torch/nn/functional.py:2011:4
    block0():
      %1009 : int[] = aten::size(%out.286) # torch/nn/functional.py:2012:27
      %size_prods.400 : int = aten::__getitem__(%1009, %9) # torch/nn/functional.py:1991:17
      %1011 : int = aten::len(%1009) # torch/nn/functional.py:1992:19
      %1012 : int = aten::sub(%1011, %11) # torch/nn/functional.py:1992:19
      %size_prods.401 : int = prim::Loop(%1012, %10, %size_prods.400) # torch/nn/functional.py:1992:4
        block0(%i.101 : int, %size_prods.402 : int):
          %1016 : int = aten::add(%i.101, %11) # torch/nn/functional.py:1993:27
          %1017 : int = aten::__getitem__(%1009, %1016) # torch/nn/functional.py:1993:22
          %size_prods.403 : int = aten::mul(%size_prods.402, %1017) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.403)
      %1019 : bool = aten::eq(%size_prods.401, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1019) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.287 : Tensor = aten::batch_norm(%out.286, %1007, %1008, %1005, %1006, %1004, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.288 : Tensor = aten::add_(%out.287, %input.22, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.24 : Tensor = aten::relu_(%out.288) # torch/nn/functional.py:1117:17
  %1023 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%783)
  %1024 : Tensor = prim::GetAttr[name="weight"](%1023)
  %1025 : Tensor? = prim::GetAttr[name="bias"](%1023)
  %1026 : int[] = prim::ListConstruct(%13, %13)
  %1027 : int[] = prim::ListConstruct(%9, %9)
  %1028 : int[] = prim::ListConstruct(%13, %13)
  %out.37 : Tensor = aten::conv2d(%input.24, %1024, %1025, %1026, %1027, %1028, %13) # torch/nn/modules/conv.py:415:15
  %1030 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%783)
  %1031 : int = aten::dim(%out.37) # torch/nn/modules/batchnorm.py:276:11
  %1032 : bool = aten::ne(%1031, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1032) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1033 : bool = prim::GetAttr[name="training"](%1030)
   = prim::If(%1033) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1034 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1030)
      %1035 : Tensor = aten::add(%1034, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1030, %1035)
      -> ()
    block1():
      -> ()
  %1036 : bool = prim::GetAttr[name="training"](%1030)
  %1037 : Tensor = prim::GetAttr[name="running_mean"](%1030)
  %1038 : Tensor = prim::GetAttr[name="running_var"](%1030)
  %1039 : Tensor = prim::GetAttr[name="weight"](%1030)
  %1040 : Tensor = prim::GetAttr[name="bias"](%1030)
   = prim::If(%1036) # torch/nn/functional.py:2011:4
    block0():
      %1041 : int[] = aten::size(%out.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%1041, %9) # torch/nn/functional.py:1991:17
      %1043 : int = aten::len(%1041) # torch/nn/functional.py:1992:19
      %1044 : int = aten::sub(%1043, %11) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%1044, %10, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %1048 : int = aten::add(%i.11, %11) # torch/nn/functional.py:1993:27
          %1049 : int = aten::__getitem__(%1041, %1048) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %1049) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.43)
      %1051 : bool = aten::eq(%size_prods.41, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1051) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.29 : Tensor = aten::batch_norm(%out.37, %1039, %1040, %1037, %1038, %1036, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.30 : Tensor = aten::relu_(%out.29) # torch/nn/functional.py:1117:17
  %1054 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%783)
  %1055 : Tensor = prim::GetAttr[name="weight"](%1054)
  %1056 : Tensor? = prim::GetAttr[name="bias"](%1054)
  %1057 : int[] = prim::ListConstruct(%13, %13)
  %1058 : int[] = prim::ListConstruct(%13, %13)
  %1059 : int[] = prim::ListConstruct(%13, %13)
  %out.31 : Tensor = aten::conv2d(%out.30, %1055, %1056, %1057, %1058, %1059, %3) # torch/nn/modules/conv.py:415:15
  %1061 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%783)
  %1062 : int = aten::dim(%out.31) # torch/nn/modules/batchnorm.py:276:11
  %1063 : bool = aten::ne(%1062, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1063) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1064 : bool = prim::GetAttr[name="training"](%1061)
   = prim::If(%1064) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1065 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1061)
      %1066 : Tensor = aten::add(%1065, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1061, %1066)
      -> ()
    block1():
      -> ()
  %1067 : bool = prim::GetAttr[name="training"](%1061)
  %1068 : Tensor = prim::GetAttr[name="running_mean"](%1061)
  %1069 : Tensor = prim::GetAttr[name="running_var"](%1061)
  %1070 : Tensor = prim::GetAttr[name="weight"](%1061)
  %1071 : Tensor = prim::GetAttr[name="bias"](%1061)
   = prim::If(%1067) # torch/nn/functional.py:2011:4
    block0():
      %1072 : int[] = aten::size(%out.31) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%1072, %9) # torch/nn/functional.py:1991:17
      %1074 : int = aten::len(%1072) # torch/nn/functional.py:1992:19
      %1075 : int = aten::sub(%1074, %11) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%1075, %10, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %1079 : int = aten::add(%i.12, %11) # torch/nn/functional.py:1993:27
          %1080 : int = aten::__getitem__(%1072, %1079) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %1080) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.47)
      %1082 : bool = aten::eq(%size_prods.45, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1082) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.32 : Tensor = aten::batch_norm(%out.31, %1070, %1071, %1068, %1069, %1067, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.33 : Tensor = aten::relu_(%out.32) # torch/nn/functional.py:1117:17
  %1085 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%783)
  %1086 : Tensor = prim::GetAttr[name="weight"](%1085)
  %1087 : Tensor? = prim::GetAttr[name="bias"](%1085)
  %1088 : int[] = prim::ListConstruct(%13, %13)
  %1089 : int[] = prim::ListConstruct(%9, %9)
  %1090 : int[] = prim::ListConstruct(%13, %13)
  %out.34 : Tensor = aten::conv2d(%out.33, %1086, %1087, %1088, %1089, %1090, %13) # torch/nn/modules/conv.py:415:15
  %1092 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%783)
  %1093 : int = aten::dim(%out.34) # torch/nn/modules/batchnorm.py:276:11
  %1094 : bool = aten::ne(%1093, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1094) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1095 : bool = prim::GetAttr[name="training"](%1092)
   = prim::If(%1095) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1096 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1092)
      %1097 : Tensor = aten::add(%1096, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1092, %1097)
      -> ()
    block1():
      -> ()
  %1098 : bool = prim::GetAttr[name="training"](%1092)
  %1099 : Tensor = prim::GetAttr[name="running_mean"](%1092)
  %1100 : Tensor = prim::GetAttr[name="running_var"](%1092)
  %1101 : Tensor = prim::GetAttr[name="weight"](%1092)
  %1102 : Tensor = prim::GetAttr[name="bias"](%1092)
   = prim::If(%1098) # torch/nn/functional.py:2011:4
    block0():
      %1103 : int[] = aten::size(%out.34) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%1103, %9) # torch/nn/functional.py:1991:17
      %1105 : int = aten::len(%1103) # torch/nn/functional.py:1992:19
      %1106 : int = aten::sub(%1105, %11) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%1106, %10, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %1110 : int = aten::add(%i.13, %11) # torch/nn/functional.py:1993:27
          %1111 : int = aten::__getitem__(%1103, %1110) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %1111) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.51)
      %1113 : bool = aten::eq(%size_prods.49, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1113) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.35 : Tensor = aten::batch_norm(%out.34, %1101, %1102, %1099, %1100, %1098, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.36 : Tensor = aten::add_(%out.35, %input.24, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.26 : Tensor = aten::relu_(%out.36) # torch/nn/functional.py:1117:17
  %1117 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%784)
  %1118 : Tensor = prim::GetAttr[name="weight"](%1117)
  %1119 : Tensor? = prim::GetAttr[name="bias"](%1117)
  %1120 : int[] = prim::ListConstruct(%13, %13)
  %1121 : int[] = prim::ListConstruct(%9, %9)
  %1122 : int[] = prim::ListConstruct(%13, %13)
  %out.46 : Tensor = aten::conv2d(%input.26, %1118, %1119, %1120, %1121, %1122, %13) # torch/nn/modules/conv.py:415:15
  %1124 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%784)
  %1125 : int = aten::dim(%out.46) # torch/nn/modules/batchnorm.py:276:11
  %1126 : bool = aten::ne(%1125, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1126) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1127 : bool = prim::GetAttr[name="training"](%1124)
   = prim::If(%1127) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1128 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1124)
      %1129 : Tensor = aten::add(%1128, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1124, %1129)
      -> ()
    block1():
      -> ()
  %1130 : bool = prim::GetAttr[name="training"](%1124)
  %1131 : Tensor = prim::GetAttr[name="running_mean"](%1124)
  %1132 : Tensor = prim::GetAttr[name="running_var"](%1124)
  %1133 : Tensor = prim::GetAttr[name="weight"](%1124)
  %1134 : Tensor = prim::GetAttr[name="bias"](%1124)
   = prim::If(%1130) # torch/nn/functional.py:2011:4
    block0():
      %1135 : int[] = aten::size(%out.46) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%1135, %9) # torch/nn/functional.py:1991:17
      %1137 : int = aten::len(%1135) # torch/nn/functional.py:1992:19
      %1138 : int = aten::sub(%1137, %11) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%1138, %10, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %1142 : int = aten::add(%i.14, %11) # torch/nn/functional.py:1993:27
          %1143 : int = aten::__getitem__(%1135, %1142) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %1143) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.55)
      %1145 : bool = aten::eq(%size_prods.53, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1145) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.38 : Tensor = aten::batch_norm(%out.46, %1133, %1134, %1131, %1132, %1130, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.39 : Tensor = aten::relu_(%out.38) # torch/nn/functional.py:1117:17
  %1148 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%784)
  %1149 : Tensor = prim::GetAttr[name="weight"](%1148)
  %1150 : Tensor? = prim::GetAttr[name="bias"](%1148)
  %1151 : int[] = prim::ListConstruct(%13, %13)
  %1152 : int[] = prim::ListConstruct(%13, %13)
  %1153 : int[] = prim::ListConstruct(%13, %13)
  %out.40 : Tensor = aten::conv2d(%out.39, %1149, %1150, %1151, %1152, %1153, %3) # torch/nn/modules/conv.py:415:15
  %1155 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%784)
  %1156 : int = aten::dim(%out.40) # torch/nn/modules/batchnorm.py:276:11
  %1157 : bool = aten::ne(%1156, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1157) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1158 : bool = prim::GetAttr[name="training"](%1155)
   = prim::If(%1158) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1159 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1155)
      %1160 : Tensor = aten::add(%1159, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1155, %1160)
      -> ()
    block1():
      -> ()
  %1161 : bool = prim::GetAttr[name="training"](%1155)
  %1162 : Tensor = prim::GetAttr[name="running_mean"](%1155)
  %1163 : Tensor = prim::GetAttr[name="running_var"](%1155)
  %1164 : Tensor = prim::GetAttr[name="weight"](%1155)
  %1165 : Tensor = prim::GetAttr[name="bias"](%1155)
   = prim::If(%1161) # torch/nn/functional.py:2011:4
    block0():
      %1166 : int[] = aten::size(%out.40) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%1166, %9) # torch/nn/functional.py:1991:17
      %1168 : int = aten::len(%1166) # torch/nn/functional.py:1992:19
      %1169 : int = aten::sub(%1168, %11) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%1169, %10, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %1173 : int = aten::add(%i.15, %11) # torch/nn/functional.py:1993:27
          %1174 : int = aten::__getitem__(%1166, %1173) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %1174) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.59)
      %1176 : bool = aten::eq(%size_prods.57, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1176) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.41 : Tensor = aten::batch_norm(%out.40, %1164, %1165, %1162, %1163, %1161, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.42 : Tensor = aten::relu_(%out.41) # torch/nn/functional.py:1117:17
  %1179 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%784)
  %1180 : Tensor = prim::GetAttr[name="weight"](%1179)
  %1181 : Tensor? = prim::GetAttr[name="bias"](%1179)
  %1182 : int[] = prim::ListConstruct(%13, %13)
  %1183 : int[] = prim::ListConstruct(%9, %9)
  %1184 : int[] = prim::ListConstruct(%13, %13)
  %out.43 : Tensor = aten::conv2d(%out.42, %1180, %1181, %1182, %1183, %1184, %13) # torch/nn/modules/conv.py:415:15
  %1186 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%784)
  %1187 : int = aten::dim(%out.43) # torch/nn/modules/batchnorm.py:276:11
  %1188 : bool = aten::ne(%1187, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1188) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1189 : bool = prim::GetAttr[name="training"](%1186)
   = prim::If(%1189) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1190 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1186)
      %1191 : Tensor = aten::add(%1190, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1186, %1191)
      -> ()
    block1():
      -> ()
  %1192 : bool = prim::GetAttr[name="training"](%1186)
  %1193 : Tensor = prim::GetAttr[name="running_mean"](%1186)
  %1194 : Tensor = prim::GetAttr[name="running_var"](%1186)
  %1195 : Tensor = prim::GetAttr[name="weight"](%1186)
  %1196 : Tensor = prim::GetAttr[name="bias"](%1186)
   = prim::If(%1192) # torch/nn/functional.py:2011:4
    block0():
      %1197 : int[] = aten::size(%out.43) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%1197, %9) # torch/nn/functional.py:1991:17
      %1199 : int = aten::len(%1197) # torch/nn/functional.py:1992:19
      %1200 : int = aten::sub(%1199, %11) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%1200, %10, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %1204 : int = aten::add(%i.16, %11) # torch/nn/functional.py:1993:27
          %1205 : int = aten::__getitem__(%1197, %1204) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %1205) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.63)
      %1207 : bool = aten::eq(%size_prods.61, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1207) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.44 : Tensor = aten::batch_norm(%out.43, %1195, %1196, %1193, %1194, %1192, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.45 : Tensor = aten::add_(%out.44, %input.26, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.9 : Tensor = aten::relu_(%out.45) # torch/nn/functional.py:1117:17
  %1211 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%785)
  %1212 : Tensor = prim::GetAttr[name="weight"](%1211)
  %1213 : Tensor? = prim::GetAttr[name="bias"](%1211)
  %1214 : int[] = prim::ListConstruct(%13, %13)
  %1215 : int[] = prim::ListConstruct(%9, %9)
  %1216 : int[] = prim::ListConstruct(%13, %13)
  %out.55 : Tensor = aten::conv2d(%input.9, %1212, %1213, %1214, %1215, %1216, %13) # torch/nn/modules/conv.py:415:15
  %1218 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%785)
  %1219 : int = aten::dim(%out.55) # torch/nn/modules/batchnorm.py:276:11
  %1220 : bool = aten::ne(%1219, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1220) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1221 : bool = prim::GetAttr[name="training"](%1218)
   = prim::If(%1221) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1222 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1218)
      %1223 : Tensor = aten::add(%1222, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1218, %1223)
      -> ()
    block1():
      -> ()
  %1224 : bool = prim::GetAttr[name="training"](%1218)
  %1225 : Tensor = prim::GetAttr[name="running_mean"](%1218)
  %1226 : Tensor = prim::GetAttr[name="running_var"](%1218)
  %1227 : Tensor = prim::GetAttr[name="weight"](%1218)
  %1228 : Tensor = prim::GetAttr[name="bias"](%1218)
   = prim::If(%1224) # torch/nn/functional.py:2011:4
    block0():
      %1229 : int[] = aten::size(%out.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%1229, %9) # torch/nn/functional.py:1991:17
      %1231 : int = aten::len(%1229) # torch/nn/functional.py:1992:19
      %1232 : int = aten::sub(%1231, %11) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%1232, %10, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %1236 : int = aten::add(%i.17, %11) # torch/nn/functional.py:1993:27
          %1237 : int = aten::__getitem__(%1229, %1236) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %1237) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.67)
      %1239 : bool = aten::eq(%size_prods.65, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1239) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.47 : Tensor = aten::batch_norm(%out.55, %1227, %1228, %1225, %1226, %1224, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.48 : Tensor = aten::relu_(%out.47) # torch/nn/functional.py:1117:17
  %1242 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%785)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1242)
  %1244 : Tensor? = prim::GetAttr[name="bias"](%1242)
  %1245 : int[] = prim::ListConstruct(%13, %13)
  %1246 : int[] = prim::ListConstruct(%13, %13)
  %1247 : int[] = prim::ListConstruct(%13, %13)
  %out.49 : Tensor = aten::conv2d(%out.48, %1243, %1244, %1245, %1246, %1247, %3) # torch/nn/modules/conv.py:415:15
  %1249 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%785)
  %1250 : int = aten::dim(%out.49) # torch/nn/modules/batchnorm.py:276:11
  %1251 : bool = aten::ne(%1250, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1251) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1252 : bool = prim::GetAttr[name="training"](%1249)
   = prim::If(%1252) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1253 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1249)
      %1254 : Tensor = aten::add(%1253, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1249, %1254)
      -> ()
    block1():
      -> ()
  %1255 : bool = prim::GetAttr[name="training"](%1249)
  %1256 : Tensor = prim::GetAttr[name="running_mean"](%1249)
  %1257 : Tensor = prim::GetAttr[name="running_var"](%1249)
  %1258 : Tensor = prim::GetAttr[name="weight"](%1249)
  %1259 : Tensor = prim::GetAttr[name="bias"](%1249)
   = prim::If(%1255) # torch/nn/functional.py:2011:4
    block0():
      %1260 : int[] = aten::size(%out.49) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%1260, %9) # torch/nn/functional.py:1991:17
      %1262 : int = aten::len(%1260) # torch/nn/functional.py:1992:19
      %1263 : int = aten::sub(%1262, %11) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%1263, %10, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %1267 : int = aten::add(%i.18, %11) # torch/nn/functional.py:1993:27
          %1268 : int = aten::__getitem__(%1260, %1267) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %1268) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.71)
      %1270 : bool = aten::eq(%size_prods.69, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1270) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.50 : Tensor = aten::batch_norm(%out.49, %1258, %1259, %1256, %1257, %1255, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.51 : Tensor = aten::relu_(%out.50) # torch/nn/functional.py:1117:17
  %1273 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%785)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1273)
  %1275 : Tensor? = prim::GetAttr[name="bias"](%1273)
  %1276 : int[] = prim::ListConstruct(%13, %13)
  %1277 : int[] = prim::ListConstruct(%9, %9)
  %1278 : int[] = prim::ListConstruct(%13, %13)
  %out.52 : Tensor = aten::conv2d(%out.51, %1274, %1275, %1276, %1277, %1278, %13) # torch/nn/modules/conv.py:415:15
  %1280 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%785)
  %1281 : int = aten::dim(%out.52) # torch/nn/modules/batchnorm.py:276:11
  %1282 : bool = aten::ne(%1281, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1282) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1283 : bool = prim::GetAttr[name="training"](%1280)
   = prim::If(%1283) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1284 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1280)
      %1285 : Tensor = aten::add(%1284, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1280, %1285)
      -> ()
    block1():
      -> ()
  %1286 : bool = prim::GetAttr[name="training"](%1280)
  %1287 : Tensor = prim::GetAttr[name="running_mean"](%1280)
  %1288 : Tensor = prim::GetAttr[name="running_var"](%1280)
  %1289 : Tensor = prim::GetAttr[name="weight"](%1280)
  %1290 : Tensor = prim::GetAttr[name="bias"](%1280)
   = prim::If(%1286) # torch/nn/functional.py:2011:4
    block0():
      %1291 : int[] = aten::size(%out.52) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%1291, %9) # torch/nn/functional.py:1991:17
      %1293 : int = aten::len(%1291) # torch/nn/functional.py:1992:19
      %1294 : int = aten::sub(%1293, %11) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%1294, %10, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %1298 : int = aten::add(%i.19, %11) # torch/nn/functional.py:1993:27
          %1299 : int = aten::__getitem__(%1291, %1298) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %1299) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.75)
      %1301 : bool = aten::eq(%size_prods.73, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1301) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.53 : Tensor = aten::batch_norm(%out.52, %1289, %1290, %1287, %1288, %1286, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.54 : Tensor = aten::add_(%out.53, %input.9, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.11 : Tensor = aten::relu_(%out.54) # torch/nn/functional.py:1117:17
  %1305 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%786)
  %1306 : Tensor = prim::GetAttr[name="weight"](%1305)
  %1307 : Tensor? = prim::GetAttr[name="bias"](%1305)
  %1308 : int[] = prim::ListConstruct(%13, %13)
  %1309 : int[] = prim::ListConstruct(%9, %9)
  %1310 : int[] = prim::ListConstruct(%13, %13)
  %out.64 : Tensor = aten::conv2d(%input.11, %1306, %1307, %1308, %1309, %1310, %13) # torch/nn/modules/conv.py:415:15
  %1312 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%786)
  %1313 : int = aten::dim(%out.64) # torch/nn/modules/batchnorm.py:276:11
  %1314 : bool = aten::ne(%1313, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1314) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1315 : bool = prim::GetAttr[name="training"](%1312)
   = prim::If(%1315) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1316 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1312)
      %1317 : Tensor = aten::add(%1316, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1312, %1317)
      -> ()
    block1():
      -> ()
  %1318 : bool = prim::GetAttr[name="training"](%1312)
  %1319 : Tensor = prim::GetAttr[name="running_mean"](%1312)
  %1320 : Tensor = prim::GetAttr[name="running_var"](%1312)
  %1321 : Tensor = prim::GetAttr[name="weight"](%1312)
  %1322 : Tensor = prim::GetAttr[name="bias"](%1312)
   = prim::If(%1318) # torch/nn/functional.py:2011:4
    block0():
      %1323 : int[] = aten::size(%out.64) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%1323, %9) # torch/nn/functional.py:1991:17
      %1325 : int = aten::len(%1323) # torch/nn/functional.py:1992:19
      %1326 : int = aten::sub(%1325, %11) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%1326, %10, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %1330 : int = aten::add(%i.20, %11) # torch/nn/functional.py:1993:27
          %1331 : int = aten::__getitem__(%1323, %1330) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %1331) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.79)
      %1333 : bool = aten::eq(%size_prods.77, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1333) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.56 : Tensor = aten::batch_norm(%out.64, %1321, %1322, %1319, %1320, %1318, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.57 : Tensor = aten::relu_(%out.56) # torch/nn/functional.py:1117:17
  %1336 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%786)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1336)
  %1338 : Tensor? = prim::GetAttr[name="bias"](%1336)
  %1339 : int[] = prim::ListConstruct(%13, %13)
  %1340 : int[] = prim::ListConstruct(%13, %13)
  %1341 : int[] = prim::ListConstruct(%13, %13)
  %out.58 : Tensor = aten::conv2d(%out.57, %1337, %1338, %1339, %1340, %1341, %3) # torch/nn/modules/conv.py:415:15
  %1343 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%786)
  %1344 : int = aten::dim(%out.58) # torch/nn/modules/batchnorm.py:276:11
  %1345 : bool = aten::ne(%1344, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1345) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1346 : bool = prim::GetAttr[name="training"](%1343)
   = prim::If(%1346) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1343)
      %1348 : Tensor = aten::add(%1347, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1343, %1348)
      -> ()
    block1():
      -> ()
  %1349 : bool = prim::GetAttr[name="training"](%1343)
  %1350 : Tensor = prim::GetAttr[name="running_mean"](%1343)
  %1351 : Tensor = prim::GetAttr[name="running_var"](%1343)
  %1352 : Tensor = prim::GetAttr[name="weight"](%1343)
  %1353 : Tensor = prim::GetAttr[name="bias"](%1343)
   = prim::If(%1349) # torch/nn/functional.py:2011:4
    block0():
      %1354 : int[] = aten::size(%out.58) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%1354, %9) # torch/nn/functional.py:1991:17
      %1356 : int = aten::len(%1354) # torch/nn/functional.py:1992:19
      %1357 : int = aten::sub(%1356, %11) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%1357, %10, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %1361 : int = aten::add(%i.21, %11) # torch/nn/functional.py:1993:27
          %1362 : int = aten::__getitem__(%1354, %1361) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %1362) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.83)
      %1364 : bool = aten::eq(%size_prods.81, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1364) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.59 : Tensor = aten::batch_norm(%out.58, %1352, %1353, %1350, %1351, %1349, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.60 : Tensor = aten::relu_(%out.59) # torch/nn/functional.py:1117:17
  %1367 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%786)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1367)
  %1369 : Tensor? = prim::GetAttr[name="bias"](%1367)
  %1370 : int[] = prim::ListConstruct(%13, %13)
  %1371 : int[] = prim::ListConstruct(%9, %9)
  %1372 : int[] = prim::ListConstruct(%13, %13)
  %out.61 : Tensor = aten::conv2d(%out.60, %1368, %1369, %1370, %1371, %1372, %13) # torch/nn/modules/conv.py:415:15
  %1374 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%786)
  %1375 : int = aten::dim(%out.61) # torch/nn/modules/batchnorm.py:276:11
  %1376 : bool = aten::ne(%1375, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1376) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1377 : bool = prim::GetAttr[name="training"](%1374)
   = prim::If(%1377) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1378 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1374)
      %1379 : Tensor = aten::add(%1378, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1374, %1379)
      -> ()
    block1():
      -> ()
  %1380 : bool = prim::GetAttr[name="training"](%1374)
  %1381 : Tensor = prim::GetAttr[name="running_mean"](%1374)
  %1382 : Tensor = prim::GetAttr[name="running_var"](%1374)
  %1383 : Tensor = prim::GetAttr[name="weight"](%1374)
  %1384 : Tensor = prim::GetAttr[name="bias"](%1374)
   = prim::If(%1380) # torch/nn/functional.py:2011:4
    block0():
      %1385 : int[] = aten::size(%out.61) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%1385, %9) # torch/nn/functional.py:1991:17
      %1387 : int = aten::len(%1385) # torch/nn/functional.py:1992:19
      %1388 : int = aten::sub(%1387, %11) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%1388, %10, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %1392 : int = aten::add(%i.22, %11) # torch/nn/functional.py:1993:27
          %1393 : int = aten::__getitem__(%1385, %1392) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %1393) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.87)
      %1395 : bool = aten::eq(%size_prods.85, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1395) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.62 : Tensor = aten::batch_norm(%out.61, %1383, %1384, %1381, %1382, %1380, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.63 : Tensor = aten::add_(%out.62, %input.11, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.13 : Tensor = aten::relu_(%out.63) # torch/nn/functional.py:1117:17
  %1399 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%787)
  %1400 : Tensor = prim::GetAttr[name="weight"](%1399)
  %1401 : Tensor? = prim::GetAttr[name="bias"](%1399)
  %1402 : int[] = prim::ListConstruct(%13, %13)
  %1403 : int[] = prim::ListConstruct(%9, %9)
  %1404 : int[] = prim::ListConstruct(%13, %13)
  %out.73 : Tensor = aten::conv2d(%input.13, %1400, %1401, %1402, %1403, %1404, %13) # torch/nn/modules/conv.py:415:15
  %1406 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%787)
  %1407 : int = aten::dim(%out.73) # torch/nn/modules/batchnorm.py:276:11
  %1408 : bool = aten::ne(%1407, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1408) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1409 : bool = prim::GetAttr[name="training"](%1406)
   = prim::If(%1409) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1410 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1406)
      %1411 : Tensor = aten::add(%1410, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1406, %1411)
      -> ()
    block1():
      -> ()
  %1412 : bool = prim::GetAttr[name="training"](%1406)
  %1413 : Tensor = prim::GetAttr[name="running_mean"](%1406)
  %1414 : Tensor = prim::GetAttr[name="running_var"](%1406)
  %1415 : Tensor = prim::GetAttr[name="weight"](%1406)
  %1416 : Tensor = prim::GetAttr[name="bias"](%1406)
   = prim::If(%1412) # torch/nn/functional.py:2011:4
    block0():
      %1417 : int[] = aten::size(%out.73) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%1417, %9) # torch/nn/functional.py:1991:17
      %1419 : int = aten::len(%1417) # torch/nn/functional.py:1992:19
      %1420 : int = aten::sub(%1419, %11) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%1420, %10, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %1424 : int = aten::add(%i.23, %11) # torch/nn/functional.py:1993:27
          %1425 : int = aten::__getitem__(%1417, %1424) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %1425) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.91)
      %1427 : bool = aten::eq(%size_prods.89, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1427) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.65 : Tensor = aten::batch_norm(%out.73, %1415, %1416, %1413, %1414, %1412, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.66 : Tensor = aten::relu_(%out.65) # torch/nn/functional.py:1117:17
  %1430 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%787)
  %1431 : Tensor = prim::GetAttr[name="weight"](%1430)
  %1432 : Tensor? = prim::GetAttr[name="bias"](%1430)
  %1433 : int[] = prim::ListConstruct(%13, %13)
  %1434 : int[] = prim::ListConstruct(%13, %13)
  %1435 : int[] = prim::ListConstruct(%13, %13)
  %out.67 : Tensor = aten::conv2d(%out.66, %1431, %1432, %1433, %1434, %1435, %3) # torch/nn/modules/conv.py:415:15
  %1437 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%787)
  %1438 : int = aten::dim(%out.67) # torch/nn/modules/batchnorm.py:276:11
  %1439 : bool = aten::ne(%1438, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1439) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1440 : bool = prim::GetAttr[name="training"](%1437)
   = prim::If(%1440) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1441 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1437)
      %1442 : Tensor = aten::add(%1441, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1437, %1442)
      -> ()
    block1():
      -> ()
  %1443 : bool = prim::GetAttr[name="training"](%1437)
  %1444 : Tensor = prim::GetAttr[name="running_mean"](%1437)
  %1445 : Tensor = prim::GetAttr[name="running_var"](%1437)
  %1446 : Tensor = prim::GetAttr[name="weight"](%1437)
  %1447 : Tensor = prim::GetAttr[name="bias"](%1437)
   = prim::If(%1443) # torch/nn/functional.py:2011:4
    block0():
      %1448 : int[] = aten::size(%out.67) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%1448, %9) # torch/nn/functional.py:1991:17
      %1450 : int = aten::len(%1448) # torch/nn/functional.py:1992:19
      %1451 : int = aten::sub(%1450, %11) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%1451, %10, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %1455 : int = aten::add(%i.24, %11) # torch/nn/functional.py:1993:27
          %1456 : int = aten::__getitem__(%1448, %1455) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %1456) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.95)
      %1458 : bool = aten::eq(%size_prods.93, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1458) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.68 : Tensor = aten::batch_norm(%out.67, %1446, %1447, %1444, %1445, %1443, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.69 : Tensor = aten::relu_(%out.68) # torch/nn/functional.py:1117:17
  %1461 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%787)
  %1462 : Tensor = prim::GetAttr[name="weight"](%1461)
  %1463 : Tensor? = prim::GetAttr[name="bias"](%1461)
  %1464 : int[] = prim::ListConstruct(%13, %13)
  %1465 : int[] = prim::ListConstruct(%9, %9)
  %1466 : int[] = prim::ListConstruct(%13, %13)
  %out.70 : Tensor = aten::conv2d(%out.69, %1462, %1463, %1464, %1465, %1466, %13) # torch/nn/modules/conv.py:415:15
  %1468 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%787)
  %1469 : int = aten::dim(%out.70) # torch/nn/modules/batchnorm.py:276:11
  %1470 : bool = aten::ne(%1469, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1470) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1471 : bool = prim::GetAttr[name="training"](%1468)
   = prim::If(%1471) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1472 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1468)
      %1473 : Tensor = aten::add(%1472, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1468, %1473)
      -> ()
    block1():
      -> ()
  %1474 : bool = prim::GetAttr[name="training"](%1468)
  %1475 : Tensor = prim::GetAttr[name="running_mean"](%1468)
  %1476 : Tensor = prim::GetAttr[name="running_var"](%1468)
  %1477 : Tensor = prim::GetAttr[name="weight"](%1468)
  %1478 : Tensor = prim::GetAttr[name="bias"](%1468)
   = prim::If(%1474) # torch/nn/functional.py:2011:4
    block0():
      %1479 : int[] = aten::size(%out.70) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%1479, %9) # torch/nn/functional.py:1991:17
      %1481 : int = aten::len(%1479) # torch/nn/functional.py:1992:19
      %1482 : int = aten::sub(%1481, %11) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%1482, %10, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %1486 : int = aten::add(%i.25, %11) # torch/nn/functional.py:1993:27
          %1487 : int = aten::__getitem__(%1479, %1486) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %1487) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.99)
      %1489 : bool = aten::eq(%size_prods.97, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1489) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.71 : Tensor = aten::batch_norm(%out.70, %1477, %1478, %1475, %1476, %1474, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.72 : Tensor = aten::add_(%out.71, %input.13, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.15 : Tensor = aten::relu_(%out.72) # torch/nn/functional.py:1117:17
  %1493 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%788)
  %1494 : Tensor = prim::GetAttr[name="weight"](%1493)
  %1495 : Tensor? = prim::GetAttr[name="bias"](%1493)
  %1496 : int[] = prim::ListConstruct(%13, %13)
  %1497 : int[] = prim::ListConstruct(%9, %9)
  %1498 : int[] = prim::ListConstruct(%13, %13)
  %out.82 : Tensor = aten::conv2d(%input.15, %1494, %1495, %1496, %1497, %1498, %13) # torch/nn/modules/conv.py:415:15
  %1500 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%788)
  %1501 : int = aten::dim(%out.82) # torch/nn/modules/batchnorm.py:276:11
  %1502 : bool = aten::ne(%1501, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1502) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1503 : bool = prim::GetAttr[name="training"](%1500)
   = prim::If(%1503) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1504 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1500)
      %1505 : Tensor = aten::add(%1504, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1500, %1505)
      -> ()
    block1():
      -> ()
  %1506 : bool = prim::GetAttr[name="training"](%1500)
  %1507 : Tensor = prim::GetAttr[name="running_mean"](%1500)
  %1508 : Tensor = prim::GetAttr[name="running_var"](%1500)
  %1509 : Tensor = prim::GetAttr[name="weight"](%1500)
  %1510 : Tensor = prim::GetAttr[name="bias"](%1500)
   = prim::If(%1506) # torch/nn/functional.py:2011:4
    block0():
      %1511 : int[] = aten::size(%out.82) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%1511, %9) # torch/nn/functional.py:1991:17
      %1513 : int = aten::len(%1511) # torch/nn/functional.py:1992:19
      %1514 : int = aten::sub(%1513, %11) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%1514, %10, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %1518 : int = aten::add(%i.26, %11) # torch/nn/functional.py:1993:27
          %1519 : int = aten::__getitem__(%1511, %1518) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %1519) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.103)
      %1521 : bool = aten::eq(%size_prods.101, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1521) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.74 : Tensor = aten::batch_norm(%out.82, %1509, %1510, %1507, %1508, %1506, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.75 : Tensor = aten::relu_(%out.74) # torch/nn/functional.py:1117:17
  %1524 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%788)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1524)
  %1526 : Tensor? = prim::GetAttr[name="bias"](%1524)
  %1527 : int[] = prim::ListConstruct(%13, %13)
  %1528 : int[] = prim::ListConstruct(%13, %13)
  %1529 : int[] = prim::ListConstruct(%13, %13)
  %out.76 : Tensor = aten::conv2d(%out.75, %1525, %1526, %1527, %1528, %1529, %3) # torch/nn/modules/conv.py:415:15
  %1531 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%788)
  %1532 : int = aten::dim(%out.76) # torch/nn/modules/batchnorm.py:276:11
  %1533 : bool = aten::ne(%1532, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1533) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1534 : bool = prim::GetAttr[name="training"](%1531)
   = prim::If(%1534) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1535 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1531)
      %1536 : Tensor = aten::add(%1535, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1531, %1536)
      -> ()
    block1():
      -> ()
  %1537 : bool = prim::GetAttr[name="training"](%1531)
  %1538 : Tensor = prim::GetAttr[name="running_mean"](%1531)
  %1539 : Tensor = prim::GetAttr[name="running_var"](%1531)
  %1540 : Tensor = prim::GetAttr[name="weight"](%1531)
  %1541 : Tensor = prim::GetAttr[name="bias"](%1531)
   = prim::If(%1537) # torch/nn/functional.py:2011:4
    block0():
      %1542 : int[] = aten::size(%out.76) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%1542, %9) # torch/nn/functional.py:1991:17
      %1544 : int = aten::len(%1542) # torch/nn/functional.py:1992:19
      %1545 : int = aten::sub(%1544, %11) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%1545, %10, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %1549 : int = aten::add(%i.27, %11) # torch/nn/functional.py:1993:27
          %1550 : int = aten::__getitem__(%1542, %1549) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %1550) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.107)
      %1552 : bool = aten::eq(%size_prods.105, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1552) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.77 : Tensor = aten::batch_norm(%out.76, %1540, %1541, %1538, %1539, %1537, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.78 : Tensor = aten::relu_(%out.77) # torch/nn/functional.py:1117:17
  %1555 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%788)
  %1556 : Tensor = prim::GetAttr[name="weight"](%1555)
  %1557 : Tensor? = prim::GetAttr[name="bias"](%1555)
  %1558 : int[] = prim::ListConstruct(%13, %13)
  %1559 : int[] = prim::ListConstruct(%9, %9)
  %1560 : int[] = prim::ListConstruct(%13, %13)
  %out.79 : Tensor = aten::conv2d(%out.78, %1556, %1557, %1558, %1559, %1560, %13) # torch/nn/modules/conv.py:415:15
  %1562 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%788)
  %1563 : int = aten::dim(%out.79) # torch/nn/modules/batchnorm.py:276:11
  %1564 : bool = aten::ne(%1563, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1564) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1565 : bool = prim::GetAttr[name="training"](%1562)
   = prim::If(%1565) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1566 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1562)
      %1567 : Tensor = aten::add(%1566, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1562, %1567)
      -> ()
    block1():
      -> ()
  %1568 : bool = prim::GetAttr[name="training"](%1562)
  %1569 : Tensor = prim::GetAttr[name="running_mean"](%1562)
  %1570 : Tensor = prim::GetAttr[name="running_var"](%1562)
  %1571 : Tensor = prim::GetAttr[name="weight"](%1562)
  %1572 : Tensor = prim::GetAttr[name="bias"](%1562)
   = prim::If(%1568) # torch/nn/functional.py:2011:4
    block0():
      %1573 : int[] = aten::size(%out.79) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%1573, %9) # torch/nn/functional.py:1991:17
      %1575 : int = aten::len(%1573) # torch/nn/functional.py:1992:19
      %1576 : int = aten::sub(%1575, %11) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%1576, %10, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %1580 : int = aten::add(%i.28, %11) # torch/nn/functional.py:1993:27
          %1581 : int = aten::__getitem__(%1573, %1580) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %1581) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.111)
      %1583 : bool = aten::eq(%size_prods.109, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1583) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.80 : Tensor = aten::batch_norm(%out.79, %1571, %1572, %1569, %1570, %1568, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.81 : Tensor = aten::add_(%out.80, %input.15, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.17 : Tensor = aten::relu_(%out.81) # torch/nn/functional.py:1117:17
  %1587 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%789)
  %1588 : Tensor = prim::GetAttr[name="weight"](%1587)
  %1589 : Tensor? = prim::GetAttr[name="bias"](%1587)
  %1590 : int[] = prim::ListConstruct(%13, %13)
  %1591 : int[] = prim::ListConstruct(%9, %9)
  %1592 : int[] = prim::ListConstruct(%13, %13)
  %out.91 : Tensor = aten::conv2d(%input.17, %1588, %1589, %1590, %1591, %1592, %13) # torch/nn/modules/conv.py:415:15
  %1594 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%789)
  %1595 : int = aten::dim(%out.91) # torch/nn/modules/batchnorm.py:276:11
  %1596 : bool = aten::ne(%1595, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1596) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1597 : bool = prim::GetAttr[name="training"](%1594)
   = prim::If(%1597) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1598 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1594)
      %1599 : Tensor = aten::add(%1598, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1594, %1599)
      -> ()
    block1():
      -> ()
  %1600 : bool = prim::GetAttr[name="training"](%1594)
  %1601 : Tensor = prim::GetAttr[name="running_mean"](%1594)
  %1602 : Tensor = prim::GetAttr[name="running_var"](%1594)
  %1603 : Tensor = prim::GetAttr[name="weight"](%1594)
  %1604 : Tensor = prim::GetAttr[name="bias"](%1594)
   = prim::If(%1600) # torch/nn/functional.py:2011:4
    block0():
      %1605 : int[] = aten::size(%out.91) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%1605, %9) # torch/nn/functional.py:1991:17
      %1607 : int = aten::len(%1605) # torch/nn/functional.py:1992:19
      %1608 : int = aten::sub(%1607, %11) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%1608, %10, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %1612 : int = aten::add(%i.29, %11) # torch/nn/functional.py:1993:27
          %1613 : int = aten::__getitem__(%1605, %1612) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %1613) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.115)
      %1615 : bool = aten::eq(%size_prods.113, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1615) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.83 : Tensor = aten::batch_norm(%out.91, %1603, %1604, %1601, %1602, %1600, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.84 : Tensor = aten::relu_(%out.83) # torch/nn/functional.py:1117:17
  %1618 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%789)
  %1619 : Tensor = prim::GetAttr[name="weight"](%1618)
  %1620 : Tensor? = prim::GetAttr[name="bias"](%1618)
  %1621 : int[] = prim::ListConstruct(%13, %13)
  %1622 : int[] = prim::ListConstruct(%13, %13)
  %1623 : int[] = prim::ListConstruct(%13, %13)
  %out.85 : Tensor = aten::conv2d(%out.84, %1619, %1620, %1621, %1622, %1623, %3) # torch/nn/modules/conv.py:415:15
  %1625 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%789)
  %1626 : int = aten::dim(%out.85) # torch/nn/modules/batchnorm.py:276:11
  %1627 : bool = aten::ne(%1626, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1627) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1628 : bool = prim::GetAttr[name="training"](%1625)
   = prim::If(%1628) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1629 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1625)
      %1630 : Tensor = aten::add(%1629, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1625, %1630)
      -> ()
    block1():
      -> ()
  %1631 : bool = prim::GetAttr[name="training"](%1625)
  %1632 : Tensor = prim::GetAttr[name="running_mean"](%1625)
  %1633 : Tensor = prim::GetAttr[name="running_var"](%1625)
  %1634 : Tensor = prim::GetAttr[name="weight"](%1625)
  %1635 : Tensor = prim::GetAttr[name="bias"](%1625)
   = prim::If(%1631) # torch/nn/functional.py:2011:4
    block0():
      %1636 : int[] = aten::size(%out.85) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%1636, %9) # torch/nn/functional.py:1991:17
      %1638 : int = aten::len(%1636) # torch/nn/functional.py:1992:19
      %1639 : int = aten::sub(%1638, %11) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%1639, %10, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %1643 : int = aten::add(%i.30, %11) # torch/nn/functional.py:1993:27
          %1644 : int = aten::__getitem__(%1636, %1643) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %1644) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.119)
      %1646 : bool = aten::eq(%size_prods.117, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1646) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.86 : Tensor = aten::batch_norm(%out.85, %1634, %1635, %1632, %1633, %1631, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.87 : Tensor = aten::relu_(%out.86) # torch/nn/functional.py:1117:17
  %1649 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%789)
  %1650 : Tensor = prim::GetAttr[name="weight"](%1649)
  %1651 : Tensor? = prim::GetAttr[name="bias"](%1649)
  %1652 : int[] = prim::ListConstruct(%13, %13)
  %1653 : int[] = prim::ListConstruct(%9, %9)
  %1654 : int[] = prim::ListConstruct(%13, %13)
  %out.88 : Tensor = aten::conv2d(%out.87, %1650, %1651, %1652, %1653, %1654, %13) # torch/nn/modules/conv.py:415:15
  %1656 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%789)
  %1657 : int = aten::dim(%out.88) # torch/nn/modules/batchnorm.py:276:11
  %1658 : bool = aten::ne(%1657, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1658) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1659 : bool = prim::GetAttr[name="training"](%1656)
   = prim::If(%1659) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1660 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1656)
      %1661 : Tensor = aten::add(%1660, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1656, %1661)
      -> ()
    block1():
      -> ()
  %1662 : bool = prim::GetAttr[name="training"](%1656)
  %1663 : Tensor = prim::GetAttr[name="running_mean"](%1656)
  %1664 : Tensor = prim::GetAttr[name="running_var"](%1656)
  %1665 : Tensor = prim::GetAttr[name="weight"](%1656)
  %1666 : Tensor = prim::GetAttr[name="bias"](%1656)
   = prim::If(%1662) # torch/nn/functional.py:2011:4
    block0():
      %1667 : int[] = aten::size(%out.88) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%1667, %9) # torch/nn/functional.py:1991:17
      %1669 : int = aten::len(%1667) # torch/nn/functional.py:1992:19
      %1670 : int = aten::sub(%1669, %11) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%1670, %10, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %1674 : int = aten::add(%i.31, %11) # torch/nn/functional.py:1993:27
          %1675 : int = aten::__getitem__(%1667, %1674) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %1675) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.123)
      %1677 : bool = aten::eq(%size_prods.121, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1677) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.89 : Tensor = aten::batch_norm(%out.88, %1665, %1666, %1663, %1664, %1662, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.90 : Tensor = aten::add_(%out.89, %input.17, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.19 : Tensor = aten::relu_(%out.90) # torch/nn/functional.py:1117:17
  %1681 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%790)
  %1682 : Tensor = prim::GetAttr[name="weight"](%1681)
  %1683 : Tensor? = prim::GetAttr[name="bias"](%1681)
  %1684 : int[] = prim::ListConstruct(%13, %13)
  %1685 : int[] = prim::ListConstruct(%9, %9)
  %1686 : int[] = prim::ListConstruct(%13, %13)
  %out.100 : Tensor = aten::conv2d(%input.19, %1682, %1683, %1684, %1685, %1686, %13) # torch/nn/modules/conv.py:415:15
  %1688 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%790)
  %1689 : int = aten::dim(%out.100) # torch/nn/modules/batchnorm.py:276:11
  %1690 : bool = aten::ne(%1689, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1690) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1691 : bool = prim::GetAttr[name="training"](%1688)
   = prim::If(%1691) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1692 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1688)
      %1693 : Tensor = aten::add(%1692, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1688, %1693)
      -> ()
    block1():
      -> ()
  %1694 : bool = prim::GetAttr[name="training"](%1688)
  %1695 : Tensor = prim::GetAttr[name="running_mean"](%1688)
  %1696 : Tensor = prim::GetAttr[name="running_var"](%1688)
  %1697 : Tensor = prim::GetAttr[name="weight"](%1688)
  %1698 : Tensor = prim::GetAttr[name="bias"](%1688)
   = prim::If(%1694) # torch/nn/functional.py:2011:4
    block0():
      %1699 : int[] = aten::size(%out.100) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%1699, %9) # torch/nn/functional.py:1991:17
      %1701 : int = aten::len(%1699) # torch/nn/functional.py:1992:19
      %1702 : int = aten::sub(%1701, %11) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%1702, %10, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %1706 : int = aten::add(%i.32, %11) # torch/nn/functional.py:1993:27
          %1707 : int = aten::__getitem__(%1699, %1706) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %1707) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.127)
      %1709 : bool = aten::eq(%size_prods.125, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1709) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.92 : Tensor = aten::batch_norm(%out.100, %1697, %1698, %1695, %1696, %1694, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.93 : Tensor = aten::relu_(%out.92) # torch/nn/functional.py:1117:17
  %1712 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%790)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1712)
  %1714 : Tensor? = prim::GetAttr[name="bias"](%1712)
  %1715 : int[] = prim::ListConstruct(%13, %13)
  %1716 : int[] = prim::ListConstruct(%13, %13)
  %1717 : int[] = prim::ListConstruct(%13, %13)
  %out.94 : Tensor = aten::conv2d(%out.93, %1713, %1714, %1715, %1716, %1717, %3) # torch/nn/modules/conv.py:415:15
  %1719 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%790)
  %1720 : int = aten::dim(%out.94) # torch/nn/modules/batchnorm.py:276:11
  %1721 : bool = aten::ne(%1720, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1721) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1722 : bool = prim::GetAttr[name="training"](%1719)
   = prim::If(%1722) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1723 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1719)
      %1724 : Tensor = aten::add(%1723, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1719, %1724)
      -> ()
    block1():
      -> ()
  %1725 : bool = prim::GetAttr[name="training"](%1719)
  %1726 : Tensor = prim::GetAttr[name="running_mean"](%1719)
  %1727 : Tensor = prim::GetAttr[name="running_var"](%1719)
  %1728 : Tensor = prim::GetAttr[name="weight"](%1719)
  %1729 : Tensor = prim::GetAttr[name="bias"](%1719)
   = prim::If(%1725) # torch/nn/functional.py:2011:4
    block0():
      %1730 : int[] = aten::size(%out.94) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%1730, %9) # torch/nn/functional.py:1991:17
      %1732 : int = aten::len(%1730) # torch/nn/functional.py:1992:19
      %1733 : int = aten::sub(%1732, %11) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%1733, %10, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %1737 : int = aten::add(%i.33, %11) # torch/nn/functional.py:1993:27
          %1738 : int = aten::__getitem__(%1730, %1737) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %1738) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.131)
      %1740 : bool = aten::eq(%size_prods.129, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1740) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.95 : Tensor = aten::batch_norm(%out.94, %1728, %1729, %1726, %1727, %1725, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.96 : Tensor = aten::relu_(%out.95) # torch/nn/functional.py:1117:17
  %1743 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%790)
  %1744 : Tensor = prim::GetAttr[name="weight"](%1743)
  %1745 : Tensor? = prim::GetAttr[name="bias"](%1743)
  %1746 : int[] = prim::ListConstruct(%13, %13)
  %1747 : int[] = prim::ListConstruct(%9, %9)
  %1748 : int[] = prim::ListConstruct(%13, %13)
  %out.97 : Tensor = aten::conv2d(%out.96, %1744, %1745, %1746, %1747, %1748, %13) # torch/nn/modules/conv.py:415:15
  %1750 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%790)
  %1751 : int = aten::dim(%out.97) # torch/nn/modules/batchnorm.py:276:11
  %1752 : bool = aten::ne(%1751, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1752) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1753 : bool = prim::GetAttr[name="training"](%1750)
   = prim::If(%1753) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1754 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1750)
      %1755 : Tensor = aten::add(%1754, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1750, %1755)
      -> ()
    block1():
      -> ()
  %1756 : bool = prim::GetAttr[name="training"](%1750)
  %1757 : Tensor = prim::GetAttr[name="running_mean"](%1750)
  %1758 : Tensor = prim::GetAttr[name="running_var"](%1750)
  %1759 : Tensor = prim::GetAttr[name="weight"](%1750)
  %1760 : Tensor = prim::GetAttr[name="bias"](%1750)
   = prim::If(%1756) # torch/nn/functional.py:2011:4
    block0():
      %1761 : int[] = aten::size(%out.97) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%1761, %9) # torch/nn/functional.py:1991:17
      %1763 : int = aten::len(%1761) # torch/nn/functional.py:1992:19
      %1764 : int = aten::sub(%1763, %11) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%1764, %10, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %1768 : int = aten::add(%i.34, %11) # torch/nn/functional.py:1993:27
          %1769 : int = aten::__getitem__(%1761, %1768) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %1769) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.135)
      %1771 : bool = aten::eq(%size_prods.133, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1771) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.98 : Tensor = aten::batch_norm(%out.97, %1759, %1760, %1757, %1758, %1756, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.99 : Tensor = aten::add_(%out.98, %input.19, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.21 : Tensor = aten::relu_(%out.99) # torch/nn/functional.py:1117:17
  %1775 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%791)
  %1776 : Tensor = prim::GetAttr[name="weight"](%1775)
  %1777 : Tensor? = prim::GetAttr[name="bias"](%1775)
  %1778 : int[] = prim::ListConstruct(%13, %13)
  %1779 : int[] = prim::ListConstruct(%9, %9)
  %1780 : int[] = prim::ListConstruct(%13, %13)
  %out.109 : Tensor = aten::conv2d(%input.21, %1776, %1777, %1778, %1779, %1780, %13) # torch/nn/modules/conv.py:415:15
  %1782 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%791)
  %1783 : int = aten::dim(%out.109) # torch/nn/modules/batchnorm.py:276:11
  %1784 : bool = aten::ne(%1783, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1784) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1785 : bool = prim::GetAttr[name="training"](%1782)
   = prim::If(%1785) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1786 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1782)
      %1787 : Tensor = aten::add(%1786, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1782, %1787)
      -> ()
    block1():
      -> ()
  %1788 : bool = prim::GetAttr[name="training"](%1782)
  %1789 : Tensor = prim::GetAttr[name="running_mean"](%1782)
  %1790 : Tensor = prim::GetAttr[name="running_var"](%1782)
  %1791 : Tensor = prim::GetAttr[name="weight"](%1782)
  %1792 : Tensor = prim::GetAttr[name="bias"](%1782)
   = prim::If(%1788) # torch/nn/functional.py:2011:4
    block0():
      %1793 : int[] = aten::size(%out.109) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%1793, %9) # torch/nn/functional.py:1991:17
      %1795 : int = aten::len(%1793) # torch/nn/functional.py:1992:19
      %1796 : int = aten::sub(%1795, %11) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%1796, %10, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %1800 : int = aten::add(%i.35, %11) # torch/nn/functional.py:1993:27
          %1801 : int = aten::__getitem__(%1793, %1800) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %1801) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.139)
      %1803 : bool = aten::eq(%size_prods.137, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1803) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.101 : Tensor = aten::batch_norm(%out.109, %1791, %1792, %1789, %1790, %1788, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.102 : Tensor = aten::relu_(%out.101) # torch/nn/functional.py:1117:17
  %1806 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%791)
  %1807 : Tensor = prim::GetAttr[name="weight"](%1806)
  %1808 : Tensor? = prim::GetAttr[name="bias"](%1806)
  %1809 : int[] = prim::ListConstruct(%13, %13)
  %1810 : int[] = prim::ListConstruct(%13, %13)
  %1811 : int[] = prim::ListConstruct(%13, %13)
  %out.103 : Tensor = aten::conv2d(%out.102, %1807, %1808, %1809, %1810, %1811, %3) # torch/nn/modules/conv.py:415:15
  %1813 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%791)
  %1814 : int = aten::dim(%out.103) # torch/nn/modules/batchnorm.py:276:11
  %1815 : bool = aten::ne(%1814, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1815) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1816 : bool = prim::GetAttr[name="training"](%1813)
   = prim::If(%1816) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1817 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1813)
      %1818 : Tensor = aten::add(%1817, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1813, %1818)
      -> ()
    block1():
      -> ()
  %1819 : bool = prim::GetAttr[name="training"](%1813)
  %1820 : Tensor = prim::GetAttr[name="running_mean"](%1813)
  %1821 : Tensor = prim::GetAttr[name="running_var"](%1813)
  %1822 : Tensor = prim::GetAttr[name="weight"](%1813)
  %1823 : Tensor = prim::GetAttr[name="bias"](%1813)
   = prim::If(%1819) # torch/nn/functional.py:2011:4
    block0():
      %1824 : int[] = aten::size(%out.103) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%1824, %9) # torch/nn/functional.py:1991:17
      %1826 : int = aten::len(%1824) # torch/nn/functional.py:1992:19
      %1827 : int = aten::sub(%1826, %11) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%1827, %10, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %1831 : int = aten::add(%i.36, %11) # torch/nn/functional.py:1993:27
          %1832 : int = aten::__getitem__(%1824, %1831) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %1832) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.143)
      %1834 : bool = aten::eq(%size_prods.141, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1834) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.104 : Tensor = aten::batch_norm(%out.103, %1822, %1823, %1820, %1821, %1819, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.105 : Tensor = aten::relu_(%out.104) # torch/nn/functional.py:1117:17
  %1837 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%791)
  %1838 : Tensor = prim::GetAttr[name="weight"](%1837)
  %1839 : Tensor? = prim::GetAttr[name="bias"](%1837)
  %1840 : int[] = prim::ListConstruct(%13, %13)
  %1841 : int[] = prim::ListConstruct(%9, %9)
  %1842 : int[] = prim::ListConstruct(%13, %13)
  %out.106 : Tensor = aten::conv2d(%out.105, %1838, %1839, %1840, %1841, %1842, %13) # torch/nn/modules/conv.py:415:15
  %1844 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%791)
  %1845 : int = aten::dim(%out.106) # torch/nn/modules/batchnorm.py:276:11
  %1846 : bool = aten::ne(%1845, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1846) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1847 : bool = prim::GetAttr[name="training"](%1844)
   = prim::If(%1847) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1848 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1844)
      %1849 : Tensor = aten::add(%1848, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1844, %1849)
      -> ()
    block1():
      -> ()
  %1850 : bool = prim::GetAttr[name="training"](%1844)
  %1851 : Tensor = prim::GetAttr[name="running_mean"](%1844)
  %1852 : Tensor = prim::GetAttr[name="running_var"](%1844)
  %1853 : Tensor = prim::GetAttr[name="weight"](%1844)
  %1854 : Tensor = prim::GetAttr[name="bias"](%1844)
   = prim::If(%1850) # torch/nn/functional.py:2011:4
    block0():
      %1855 : int[] = aten::size(%out.106) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%1855, %9) # torch/nn/functional.py:1991:17
      %1857 : int = aten::len(%1855) # torch/nn/functional.py:1992:19
      %1858 : int = aten::sub(%1857, %11) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%1858, %10, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %1862 : int = aten::add(%i.37, %11) # torch/nn/functional.py:1993:27
          %1863 : int = aten::__getitem__(%1855, %1862) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %1863) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.147)
      %1865 : bool = aten::eq(%size_prods.145, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1865) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.107 : Tensor = aten::batch_norm(%out.106, %1853, %1854, %1851, %1852, %1850, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.108 : Tensor = aten::add_(%out.107, %input.21, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.23 : Tensor = aten::relu_(%out.108) # torch/nn/functional.py:1117:17
  %1869 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%792)
  %1870 : Tensor = prim::GetAttr[name="weight"](%1869)
  %1871 : Tensor? = prim::GetAttr[name="bias"](%1869)
  %1872 : int[] = prim::ListConstruct(%13, %13)
  %1873 : int[] = prim::ListConstruct(%9, %9)
  %1874 : int[] = prim::ListConstruct(%13, %13)
  %out.118 : Tensor = aten::conv2d(%input.23, %1870, %1871, %1872, %1873, %1874, %13) # torch/nn/modules/conv.py:415:15
  %1876 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%792)
  %1877 : int = aten::dim(%out.118) # torch/nn/modules/batchnorm.py:276:11
  %1878 : bool = aten::ne(%1877, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1878) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1879 : bool = prim::GetAttr[name="training"](%1876)
   = prim::If(%1879) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1880 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1876)
      %1881 : Tensor = aten::add(%1880, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1876, %1881)
      -> ()
    block1():
      -> ()
  %1882 : bool = prim::GetAttr[name="training"](%1876)
  %1883 : Tensor = prim::GetAttr[name="running_mean"](%1876)
  %1884 : Tensor = prim::GetAttr[name="running_var"](%1876)
  %1885 : Tensor = prim::GetAttr[name="weight"](%1876)
  %1886 : Tensor = prim::GetAttr[name="bias"](%1876)
   = prim::If(%1882) # torch/nn/functional.py:2011:4
    block0():
      %1887 : int[] = aten::size(%out.118) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%1887, %9) # torch/nn/functional.py:1991:17
      %1889 : int = aten::len(%1887) # torch/nn/functional.py:1992:19
      %1890 : int = aten::sub(%1889, %11) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%1890, %10, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %1894 : int = aten::add(%i.38, %11) # torch/nn/functional.py:1993:27
          %1895 : int = aten::__getitem__(%1887, %1894) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %1895) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.151)
      %1897 : bool = aten::eq(%size_prods.149, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1897) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.110 : Tensor = aten::batch_norm(%out.118, %1885, %1886, %1883, %1884, %1882, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.111 : Tensor = aten::relu_(%out.110) # torch/nn/functional.py:1117:17
  %1900 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%792)
  %1901 : Tensor = prim::GetAttr[name="weight"](%1900)
  %1902 : Tensor? = prim::GetAttr[name="bias"](%1900)
  %1903 : int[] = prim::ListConstruct(%13, %13)
  %1904 : int[] = prim::ListConstruct(%13, %13)
  %1905 : int[] = prim::ListConstruct(%13, %13)
  %out.112 : Tensor = aten::conv2d(%out.111, %1901, %1902, %1903, %1904, %1905, %3) # torch/nn/modules/conv.py:415:15
  %1907 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%792)
  %1908 : int = aten::dim(%out.112) # torch/nn/modules/batchnorm.py:276:11
  %1909 : bool = aten::ne(%1908, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1909) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1910 : bool = prim::GetAttr[name="training"](%1907)
   = prim::If(%1910) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1911 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1907)
      %1912 : Tensor = aten::add(%1911, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1907, %1912)
      -> ()
    block1():
      -> ()
  %1913 : bool = prim::GetAttr[name="training"](%1907)
  %1914 : Tensor = prim::GetAttr[name="running_mean"](%1907)
  %1915 : Tensor = prim::GetAttr[name="running_var"](%1907)
  %1916 : Tensor = prim::GetAttr[name="weight"](%1907)
  %1917 : Tensor = prim::GetAttr[name="bias"](%1907)
   = prim::If(%1913) # torch/nn/functional.py:2011:4
    block0():
      %1918 : int[] = aten::size(%out.112) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%1918, %9) # torch/nn/functional.py:1991:17
      %1920 : int = aten::len(%1918) # torch/nn/functional.py:1992:19
      %1921 : int = aten::sub(%1920, %11) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%1921, %10, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %1925 : int = aten::add(%i.39, %11) # torch/nn/functional.py:1993:27
          %1926 : int = aten::__getitem__(%1918, %1925) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %1926) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.155)
      %1928 : bool = aten::eq(%size_prods.153, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1928) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.113 : Tensor = aten::batch_norm(%out.112, %1916, %1917, %1914, %1915, %1913, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.114 : Tensor = aten::relu_(%out.113) # torch/nn/functional.py:1117:17
  %1931 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%792)
  %1932 : Tensor = prim::GetAttr[name="weight"](%1931)
  %1933 : Tensor? = prim::GetAttr[name="bias"](%1931)
  %1934 : int[] = prim::ListConstruct(%13, %13)
  %1935 : int[] = prim::ListConstruct(%9, %9)
  %1936 : int[] = prim::ListConstruct(%13, %13)
  %out.115 : Tensor = aten::conv2d(%out.114, %1932, %1933, %1934, %1935, %1936, %13) # torch/nn/modules/conv.py:415:15
  %1938 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%792)
  %1939 : int = aten::dim(%out.115) # torch/nn/modules/batchnorm.py:276:11
  %1940 : bool = aten::ne(%1939, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1940) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1941 : bool = prim::GetAttr[name="training"](%1938)
   = prim::If(%1941) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1942 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1938)
      %1943 : Tensor = aten::add(%1942, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1938, %1943)
      -> ()
    block1():
      -> ()
  %1944 : bool = prim::GetAttr[name="training"](%1938)
  %1945 : Tensor = prim::GetAttr[name="running_mean"](%1938)
  %1946 : Tensor = prim::GetAttr[name="running_var"](%1938)
  %1947 : Tensor = prim::GetAttr[name="weight"](%1938)
  %1948 : Tensor = prim::GetAttr[name="bias"](%1938)
   = prim::If(%1944) # torch/nn/functional.py:2011:4
    block0():
      %1949 : int[] = aten::size(%out.115) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%1949, %9) # torch/nn/functional.py:1991:17
      %1951 : int = aten::len(%1949) # torch/nn/functional.py:1992:19
      %1952 : int = aten::sub(%1951, %11) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%1952, %10, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %1956 : int = aten::add(%i.40, %11) # torch/nn/functional.py:1993:27
          %1957 : int = aten::__getitem__(%1949, %1956) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %1957) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.159)
      %1959 : bool = aten::eq(%size_prods.157, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1959) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.116 : Tensor = aten::batch_norm(%out.115, %1947, %1948, %1945, %1946, %1944, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.117 : Tensor = aten::add_(%out.116, %input.23, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.25 : Tensor = aten::relu_(%out.117) # torch/nn/functional.py:1117:17
  %1963 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%793)
  %1964 : Tensor = prim::GetAttr[name="weight"](%1963)
  %1965 : Tensor? = prim::GetAttr[name="bias"](%1963)
  %1966 : int[] = prim::ListConstruct(%13, %13)
  %1967 : int[] = prim::ListConstruct(%9, %9)
  %1968 : int[] = prim::ListConstruct(%13, %13)
  %out.127 : Tensor = aten::conv2d(%input.25, %1964, %1965, %1966, %1967, %1968, %13) # torch/nn/modules/conv.py:415:15
  %1970 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%793)
  %1971 : int = aten::dim(%out.127) # torch/nn/modules/batchnorm.py:276:11
  %1972 : bool = aten::ne(%1971, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1972) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1973 : bool = prim::GetAttr[name="training"](%1970)
   = prim::If(%1973) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1974 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1970)
      %1975 : Tensor = aten::add(%1974, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1970, %1975)
      -> ()
    block1():
      -> ()
  %1976 : bool = prim::GetAttr[name="training"](%1970)
  %1977 : Tensor = prim::GetAttr[name="running_mean"](%1970)
  %1978 : Tensor = prim::GetAttr[name="running_var"](%1970)
  %1979 : Tensor = prim::GetAttr[name="weight"](%1970)
  %1980 : Tensor = prim::GetAttr[name="bias"](%1970)
   = prim::If(%1976) # torch/nn/functional.py:2011:4
    block0():
      %1981 : int[] = aten::size(%out.127) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%1981, %9) # torch/nn/functional.py:1991:17
      %1983 : int = aten::len(%1981) # torch/nn/functional.py:1992:19
      %1984 : int = aten::sub(%1983, %11) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%1984, %10, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %1988 : int = aten::add(%i.41, %11) # torch/nn/functional.py:1993:27
          %1989 : int = aten::__getitem__(%1981, %1988) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %1989) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.163)
      %1991 : bool = aten::eq(%size_prods.161, %13) # torch/nn/functional.py:1994:7
       = prim::If(%1991) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.119 : Tensor = aten::batch_norm(%out.127, %1979, %1980, %1977, %1978, %1976, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.120 : Tensor = aten::relu_(%out.119) # torch/nn/functional.py:1117:17
  %1994 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%793)
  %1995 : Tensor = prim::GetAttr[name="weight"](%1994)
  %1996 : Tensor? = prim::GetAttr[name="bias"](%1994)
  %1997 : int[] = prim::ListConstruct(%13, %13)
  %1998 : int[] = prim::ListConstruct(%13, %13)
  %1999 : int[] = prim::ListConstruct(%13, %13)
  %out.121 : Tensor = aten::conv2d(%out.120, %1995, %1996, %1997, %1998, %1999, %3) # torch/nn/modules/conv.py:415:15
  %2001 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%793)
  %2002 : int = aten::dim(%out.121) # torch/nn/modules/batchnorm.py:276:11
  %2003 : bool = aten::ne(%2002, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2003) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2004 : bool = prim::GetAttr[name="training"](%2001)
   = prim::If(%2004) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2005 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2001)
      %2006 : Tensor = aten::add(%2005, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2001, %2006)
      -> ()
    block1():
      -> ()
  %2007 : bool = prim::GetAttr[name="training"](%2001)
  %2008 : Tensor = prim::GetAttr[name="running_mean"](%2001)
  %2009 : Tensor = prim::GetAttr[name="running_var"](%2001)
  %2010 : Tensor = prim::GetAttr[name="weight"](%2001)
  %2011 : Tensor = prim::GetAttr[name="bias"](%2001)
   = prim::If(%2007) # torch/nn/functional.py:2011:4
    block0():
      %2012 : int[] = aten::size(%out.121) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%2012, %9) # torch/nn/functional.py:1991:17
      %2014 : int = aten::len(%2012) # torch/nn/functional.py:1992:19
      %2015 : int = aten::sub(%2014, %11) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%2015, %10, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %2019 : int = aten::add(%i.42, %11) # torch/nn/functional.py:1993:27
          %2020 : int = aten::__getitem__(%2012, %2019) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %2020) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.167)
      %2022 : bool = aten::eq(%size_prods.165, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2022) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.122 : Tensor = aten::batch_norm(%out.121, %2010, %2011, %2008, %2009, %2007, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.123 : Tensor = aten::relu_(%out.122) # torch/nn/functional.py:1117:17
  %2025 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%793)
  %2026 : Tensor = prim::GetAttr[name="weight"](%2025)
  %2027 : Tensor? = prim::GetAttr[name="bias"](%2025)
  %2028 : int[] = prim::ListConstruct(%13, %13)
  %2029 : int[] = prim::ListConstruct(%9, %9)
  %2030 : int[] = prim::ListConstruct(%13, %13)
  %out.124 : Tensor = aten::conv2d(%out.123, %2026, %2027, %2028, %2029, %2030, %13) # torch/nn/modules/conv.py:415:15
  %2032 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%793)
  %2033 : int = aten::dim(%out.124) # torch/nn/modules/batchnorm.py:276:11
  %2034 : bool = aten::ne(%2033, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2034) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2035 : bool = prim::GetAttr[name="training"](%2032)
   = prim::If(%2035) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2036 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2032)
      %2037 : Tensor = aten::add(%2036, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2032, %2037)
      -> ()
    block1():
      -> ()
  %2038 : bool = prim::GetAttr[name="training"](%2032)
  %2039 : Tensor = prim::GetAttr[name="running_mean"](%2032)
  %2040 : Tensor = prim::GetAttr[name="running_var"](%2032)
  %2041 : Tensor = prim::GetAttr[name="weight"](%2032)
  %2042 : Tensor = prim::GetAttr[name="bias"](%2032)
   = prim::If(%2038) # torch/nn/functional.py:2011:4
    block0():
      %2043 : int[] = aten::size(%out.124) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%2043, %9) # torch/nn/functional.py:1991:17
      %2045 : int = aten::len(%2043) # torch/nn/functional.py:1992:19
      %2046 : int = aten::sub(%2045, %11) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%2046, %10, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %2050 : int = aten::add(%i.43, %11) # torch/nn/functional.py:1993:27
          %2051 : int = aten::__getitem__(%2043, %2050) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %2051) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.171)
      %2053 : bool = aten::eq(%size_prods.169, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2053) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.125 : Tensor = aten::batch_norm(%out.124, %2041, %2042, %2039, %2040, %2038, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.126 : Tensor = aten::add_(%out.125, %input.25, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.27 : Tensor = aten::relu_(%out.126) # torch/nn/functional.py:1117:17
  %2057 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%794)
  %2058 : Tensor = prim::GetAttr[name="weight"](%2057)
  %2059 : Tensor? = prim::GetAttr[name="bias"](%2057)
  %2060 : int[] = prim::ListConstruct(%13, %13)
  %2061 : int[] = prim::ListConstruct(%9, %9)
  %2062 : int[] = prim::ListConstruct(%13, %13)
  %out.136 : Tensor = aten::conv2d(%input.27, %2058, %2059, %2060, %2061, %2062, %13) # torch/nn/modules/conv.py:415:15
  %2064 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%794)
  %2065 : int = aten::dim(%out.136) # torch/nn/modules/batchnorm.py:276:11
  %2066 : bool = aten::ne(%2065, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2066) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2067 : bool = prim::GetAttr[name="training"](%2064)
   = prim::If(%2067) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2068 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2064)
      %2069 : Tensor = aten::add(%2068, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2064, %2069)
      -> ()
    block1():
      -> ()
  %2070 : bool = prim::GetAttr[name="training"](%2064)
  %2071 : Tensor = prim::GetAttr[name="running_mean"](%2064)
  %2072 : Tensor = prim::GetAttr[name="running_var"](%2064)
  %2073 : Tensor = prim::GetAttr[name="weight"](%2064)
  %2074 : Tensor = prim::GetAttr[name="bias"](%2064)
   = prim::If(%2070) # torch/nn/functional.py:2011:4
    block0():
      %2075 : int[] = aten::size(%out.136) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%2075, %9) # torch/nn/functional.py:1991:17
      %2077 : int = aten::len(%2075) # torch/nn/functional.py:1992:19
      %2078 : int = aten::sub(%2077, %11) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%2078, %10, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %2082 : int = aten::add(%i.44, %11) # torch/nn/functional.py:1993:27
          %2083 : int = aten::__getitem__(%2075, %2082) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %2083) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.175)
      %2085 : bool = aten::eq(%size_prods.173, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2085) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.128 : Tensor = aten::batch_norm(%out.136, %2073, %2074, %2071, %2072, %2070, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.129 : Tensor = aten::relu_(%out.128) # torch/nn/functional.py:1117:17
  %2088 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%794)
  %2089 : Tensor = prim::GetAttr[name="weight"](%2088)
  %2090 : Tensor? = prim::GetAttr[name="bias"](%2088)
  %2091 : int[] = prim::ListConstruct(%13, %13)
  %2092 : int[] = prim::ListConstruct(%13, %13)
  %2093 : int[] = prim::ListConstruct(%13, %13)
  %out.130 : Tensor = aten::conv2d(%out.129, %2089, %2090, %2091, %2092, %2093, %3) # torch/nn/modules/conv.py:415:15
  %2095 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%794)
  %2096 : int = aten::dim(%out.130) # torch/nn/modules/batchnorm.py:276:11
  %2097 : bool = aten::ne(%2096, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2097) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2098 : bool = prim::GetAttr[name="training"](%2095)
   = prim::If(%2098) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2099 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2095)
      %2100 : Tensor = aten::add(%2099, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2095, %2100)
      -> ()
    block1():
      -> ()
  %2101 : bool = prim::GetAttr[name="training"](%2095)
  %2102 : Tensor = prim::GetAttr[name="running_mean"](%2095)
  %2103 : Tensor = prim::GetAttr[name="running_var"](%2095)
  %2104 : Tensor = prim::GetAttr[name="weight"](%2095)
  %2105 : Tensor = prim::GetAttr[name="bias"](%2095)
   = prim::If(%2101) # torch/nn/functional.py:2011:4
    block0():
      %2106 : int[] = aten::size(%out.130) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%2106, %9) # torch/nn/functional.py:1991:17
      %2108 : int = aten::len(%2106) # torch/nn/functional.py:1992:19
      %2109 : int = aten::sub(%2108, %11) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%2109, %10, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %2113 : int = aten::add(%i.45, %11) # torch/nn/functional.py:1993:27
          %2114 : int = aten::__getitem__(%2106, %2113) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %2114) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.179)
      %2116 : bool = aten::eq(%size_prods.177, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2116) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.131 : Tensor = aten::batch_norm(%out.130, %2104, %2105, %2102, %2103, %2101, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.132 : Tensor = aten::relu_(%out.131) # torch/nn/functional.py:1117:17
  %2119 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%794)
  %2120 : Tensor = prim::GetAttr[name="weight"](%2119)
  %2121 : Tensor? = prim::GetAttr[name="bias"](%2119)
  %2122 : int[] = prim::ListConstruct(%13, %13)
  %2123 : int[] = prim::ListConstruct(%9, %9)
  %2124 : int[] = prim::ListConstruct(%13, %13)
  %out.133 : Tensor = aten::conv2d(%out.132, %2120, %2121, %2122, %2123, %2124, %13) # torch/nn/modules/conv.py:415:15
  %2126 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%794)
  %2127 : int = aten::dim(%out.133) # torch/nn/modules/batchnorm.py:276:11
  %2128 : bool = aten::ne(%2127, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2128) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2129 : bool = prim::GetAttr[name="training"](%2126)
   = prim::If(%2129) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2130 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2126)
      %2131 : Tensor = aten::add(%2130, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2126, %2131)
      -> ()
    block1():
      -> ()
  %2132 : bool = prim::GetAttr[name="training"](%2126)
  %2133 : Tensor = prim::GetAttr[name="running_mean"](%2126)
  %2134 : Tensor = prim::GetAttr[name="running_var"](%2126)
  %2135 : Tensor = prim::GetAttr[name="weight"](%2126)
  %2136 : Tensor = prim::GetAttr[name="bias"](%2126)
   = prim::If(%2132) # torch/nn/functional.py:2011:4
    block0():
      %2137 : int[] = aten::size(%out.133) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%2137, %9) # torch/nn/functional.py:1991:17
      %2139 : int = aten::len(%2137) # torch/nn/functional.py:1992:19
      %2140 : int = aten::sub(%2139, %11) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%2140, %10, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %2144 : int = aten::add(%i.46, %11) # torch/nn/functional.py:1993:27
          %2145 : int = aten::__getitem__(%2137, %2144) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %2145) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.183)
      %2147 : bool = aten::eq(%size_prods.181, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2147) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.134 : Tensor = aten::batch_norm(%out.133, %2135, %2136, %2133, %2134, %2132, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.135 : Tensor = aten::add_(%out.134, %input.27, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.29 : Tensor = aten::relu_(%out.135) # torch/nn/functional.py:1117:17
  %2151 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%795)
  %2152 : Tensor = prim::GetAttr[name="weight"](%2151)
  %2153 : Tensor? = prim::GetAttr[name="bias"](%2151)
  %2154 : int[] = prim::ListConstruct(%13, %13)
  %2155 : int[] = prim::ListConstruct(%9, %9)
  %2156 : int[] = prim::ListConstruct(%13, %13)
  %out.145 : Tensor = aten::conv2d(%input.29, %2152, %2153, %2154, %2155, %2156, %13) # torch/nn/modules/conv.py:415:15
  %2158 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%795)
  %2159 : int = aten::dim(%out.145) # torch/nn/modules/batchnorm.py:276:11
  %2160 : bool = aten::ne(%2159, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2160) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2161 : bool = prim::GetAttr[name="training"](%2158)
   = prim::If(%2161) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2162 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2158)
      %2163 : Tensor = aten::add(%2162, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2158, %2163)
      -> ()
    block1():
      -> ()
  %2164 : bool = prim::GetAttr[name="training"](%2158)
  %2165 : Tensor = prim::GetAttr[name="running_mean"](%2158)
  %2166 : Tensor = prim::GetAttr[name="running_var"](%2158)
  %2167 : Tensor = prim::GetAttr[name="weight"](%2158)
  %2168 : Tensor = prim::GetAttr[name="bias"](%2158)
   = prim::If(%2164) # torch/nn/functional.py:2011:4
    block0():
      %2169 : int[] = aten::size(%out.145) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%2169, %9) # torch/nn/functional.py:1991:17
      %2171 : int = aten::len(%2169) # torch/nn/functional.py:1992:19
      %2172 : int = aten::sub(%2171, %11) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%2172, %10, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %2176 : int = aten::add(%i.47, %11) # torch/nn/functional.py:1993:27
          %2177 : int = aten::__getitem__(%2169, %2176) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %2177) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.187)
      %2179 : bool = aten::eq(%size_prods.185, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2179) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.137 : Tensor = aten::batch_norm(%out.145, %2167, %2168, %2165, %2166, %2164, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.138 : Tensor = aten::relu_(%out.137) # torch/nn/functional.py:1117:17
  %2182 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%795)
  %2183 : Tensor = prim::GetAttr[name="weight"](%2182)
  %2184 : Tensor? = prim::GetAttr[name="bias"](%2182)
  %2185 : int[] = prim::ListConstruct(%13, %13)
  %2186 : int[] = prim::ListConstruct(%13, %13)
  %2187 : int[] = prim::ListConstruct(%13, %13)
  %out.139 : Tensor = aten::conv2d(%out.138, %2183, %2184, %2185, %2186, %2187, %3) # torch/nn/modules/conv.py:415:15
  %2189 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%795)
  %2190 : int = aten::dim(%out.139) # torch/nn/modules/batchnorm.py:276:11
  %2191 : bool = aten::ne(%2190, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2191) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2192 : bool = prim::GetAttr[name="training"](%2189)
   = prim::If(%2192) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2193 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2189)
      %2194 : Tensor = aten::add(%2193, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2189, %2194)
      -> ()
    block1():
      -> ()
  %2195 : bool = prim::GetAttr[name="training"](%2189)
  %2196 : Tensor = prim::GetAttr[name="running_mean"](%2189)
  %2197 : Tensor = prim::GetAttr[name="running_var"](%2189)
  %2198 : Tensor = prim::GetAttr[name="weight"](%2189)
  %2199 : Tensor = prim::GetAttr[name="bias"](%2189)
   = prim::If(%2195) # torch/nn/functional.py:2011:4
    block0():
      %2200 : int[] = aten::size(%out.139) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%2200, %9) # torch/nn/functional.py:1991:17
      %2202 : int = aten::len(%2200) # torch/nn/functional.py:1992:19
      %2203 : int = aten::sub(%2202, %11) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%2203, %10, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %2207 : int = aten::add(%i.48, %11) # torch/nn/functional.py:1993:27
          %2208 : int = aten::__getitem__(%2200, %2207) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %2208) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.191)
      %2210 : bool = aten::eq(%size_prods.189, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2210) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.140 : Tensor = aten::batch_norm(%out.139, %2198, %2199, %2196, %2197, %2195, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.141 : Tensor = aten::relu_(%out.140) # torch/nn/functional.py:1117:17
  %2213 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%795)
  %2214 : Tensor = prim::GetAttr[name="weight"](%2213)
  %2215 : Tensor? = prim::GetAttr[name="bias"](%2213)
  %2216 : int[] = prim::ListConstruct(%13, %13)
  %2217 : int[] = prim::ListConstruct(%9, %9)
  %2218 : int[] = prim::ListConstruct(%13, %13)
  %out.142 : Tensor = aten::conv2d(%out.141, %2214, %2215, %2216, %2217, %2218, %13) # torch/nn/modules/conv.py:415:15
  %2220 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%795)
  %2221 : int = aten::dim(%out.142) # torch/nn/modules/batchnorm.py:276:11
  %2222 : bool = aten::ne(%2221, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2222) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2223 : bool = prim::GetAttr[name="training"](%2220)
   = prim::If(%2223) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2224 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2220)
      %2225 : Tensor = aten::add(%2224, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2220, %2225)
      -> ()
    block1():
      -> ()
  %2226 : bool = prim::GetAttr[name="training"](%2220)
  %2227 : Tensor = prim::GetAttr[name="running_mean"](%2220)
  %2228 : Tensor = prim::GetAttr[name="running_var"](%2220)
  %2229 : Tensor = prim::GetAttr[name="weight"](%2220)
  %2230 : Tensor = prim::GetAttr[name="bias"](%2220)
   = prim::If(%2226) # torch/nn/functional.py:2011:4
    block0():
      %2231 : int[] = aten::size(%out.142) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%2231, %9) # torch/nn/functional.py:1991:17
      %2233 : int = aten::len(%2231) # torch/nn/functional.py:1992:19
      %2234 : int = aten::sub(%2233, %11) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%2234, %10, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %2238 : int = aten::add(%i.49, %11) # torch/nn/functional.py:1993:27
          %2239 : int = aten::__getitem__(%2231, %2238) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %2239) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.195)
      %2241 : bool = aten::eq(%size_prods.193, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2241) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.143 : Tensor = aten::batch_norm(%out.142, %2229, %2230, %2227, %2228, %2226, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.144 : Tensor = aten::add_(%out.143, %input.29, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.31 : Tensor = aten::relu_(%out.144) # torch/nn/functional.py:1117:17
  %2245 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%796)
  %2246 : Tensor = prim::GetAttr[name="weight"](%2245)
  %2247 : Tensor? = prim::GetAttr[name="bias"](%2245)
  %2248 : int[] = prim::ListConstruct(%13, %13)
  %2249 : int[] = prim::ListConstruct(%9, %9)
  %2250 : int[] = prim::ListConstruct(%13, %13)
  %out.154 : Tensor = aten::conv2d(%input.31, %2246, %2247, %2248, %2249, %2250, %13) # torch/nn/modules/conv.py:415:15
  %2252 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%796)
  %2253 : int = aten::dim(%out.154) # torch/nn/modules/batchnorm.py:276:11
  %2254 : bool = aten::ne(%2253, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2254) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2255 : bool = prim::GetAttr[name="training"](%2252)
   = prim::If(%2255) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2256 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2252)
      %2257 : Tensor = aten::add(%2256, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2252, %2257)
      -> ()
    block1():
      -> ()
  %2258 : bool = prim::GetAttr[name="training"](%2252)
  %2259 : Tensor = prim::GetAttr[name="running_mean"](%2252)
  %2260 : Tensor = prim::GetAttr[name="running_var"](%2252)
  %2261 : Tensor = prim::GetAttr[name="weight"](%2252)
  %2262 : Tensor = prim::GetAttr[name="bias"](%2252)
   = prim::If(%2258) # torch/nn/functional.py:2011:4
    block0():
      %2263 : int[] = aten::size(%out.154) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%2263, %9) # torch/nn/functional.py:1991:17
      %2265 : int = aten::len(%2263) # torch/nn/functional.py:1992:19
      %2266 : int = aten::sub(%2265, %11) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%2266, %10, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %2270 : int = aten::add(%i.50, %11) # torch/nn/functional.py:1993:27
          %2271 : int = aten::__getitem__(%2263, %2270) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %2271) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.199)
      %2273 : bool = aten::eq(%size_prods.197, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2273) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.146 : Tensor = aten::batch_norm(%out.154, %2261, %2262, %2259, %2260, %2258, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.147 : Tensor = aten::relu_(%out.146) # torch/nn/functional.py:1117:17
  %2276 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%796)
  %2277 : Tensor = prim::GetAttr[name="weight"](%2276)
  %2278 : Tensor? = prim::GetAttr[name="bias"](%2276)
  %2279 : int[] = prim::ListConstruct(%13, %13)
  %2280 : int[] = prim::ListConstruct(%13, %13)
  %2281 : int[] = prim::ListConstruct(%13, %13)
  %out.148 : Tensor = aten::conv2d(%out.147, %2277, %2278, %2279, %2280, %2281, %3) # torch/nn/modules/conv.py:415:15
  %2283 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%796)
  %2284 : int = aten::dim(%out.148) # torch/nn/modules/batchnorm.py:276:11
  %2285 : bool = aten::ne(%2284, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2285) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2286 : bool = prim::GetAttr[name="training"](%2283)
   = prim::If(%2286) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2287 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2283)
      %2288 : Tensor = aten::add(%2287, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2283, %2288)
      -> ()
    block1():
      -> ()
  %2289 : bool = prim::GetAttr[name="training"](%2283)
  %2290 : Tensor = prim::GetAttr[name="running_mean"](%2283)
  %2291 : Tensor = prim::GetAttr[name="running_var"](%2283)
  %2292 : Tensor = prim::GetAttr[name="weight"](%2283)
  %2293 : Tensor = prim::GetAttr[name="bias"](%2283)
   = prim::If(%2289) # torch/nn/functional.py:2011:4
    block0():
      %2294 : int[] = aten::size(%out.148) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%2294, %9) # torch/nn/functional.py:1991:17
      %2296 : int = aten::len(%2294) # torch/nn/functional.py:1992:19
      %2297 : int = aten::sub(%2296, %11) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%2297, %10, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %2301 : int = aten::add(%i.51, %11) # torch/nn/functional.py:1993:27
          %2302 : int = aten::__getitem__(%2294, %2301) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %2302) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.203)
      %2304 : bool = aten::eq(%size_prods.201, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2304) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.149 : Tensor = aten::batch_norm(%out.148, %2292, %2293, %2290, %2291, %2289, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.150 : Tensor = aten::relu_(%out.149) # torch/nn/functional.py:1117:17
  %2307 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%796)
  %2308 : Tensor = prim::GetAttr[name="weight"](%2307)
  %2309 : Tensor? = prim::GetAttr[name="bias"](%2307)
  %2310 : int[] = prim::ListConstruct(%13, %13)
  %2311 : int[] = prim::ListConstruct(%9, %9)
  %2312 : int[] = prim::ListConstruct(%13, %13)
  %out.151 : Tensor = aten::conv2d(%out.150, %2308, %2309, %2310, %2311, %2312, %13) # torch/nn/modules/conv.py:415:15
  %2314 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%796)
  %2315 : int = aten::dim(%out.151) # torch/nn/modules/batchnorm.py:276:11
  %2316 : bool = aten::ne(%2315, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2316) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2317 : bool = prim::GetAttr[name="training"](%2314)
   = prim::If(%2317) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2318 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2314)
      %2319 : Tensor = aten::add(%2318, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2314, %2319)
      -> ()
    block1():
      -> ()
  %2320 : bool = prim::GetAttr[name="training"](%2314)
  %2321 : Tensor = prim::GetAttr[name="running_mean"](%2314)
  %2322 : Tensor = prim::GetAttr[name="running_var"](%2314)
  %2323 : Tensor = prim::GetAttr[name="weight"](%2314)
  %2324 : Tensor = prim::GetAttr[name="bias"](%2314)
   = prim::If(%2320) # torch/nn/functional.py:2011:4
    block0():
      %2325 : int[] = aten::size(%out.151) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%2325, %9) # torch/nn/functional.py:1991:17
      %2327 : int = aten::len(%2325) # torch/nn/functional.py:1992:19
      %2328 : int = aten::sub(%2327, %11) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%2328, %10, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %2332 : int = aten::add(%i.52, %11) # torch/nn/functional.py:1993:27
          %2333 : int = aten::__getitem__(%2325, %2332) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %2333) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.207)
      %2335 : bool = aten::eq(%size_prods.205, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2335) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.152 : Tensor = aten::batch_norm(%out.151, %2323, %2324, %2321, %2322, %2320, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.153 : Tensor = aten::add_(%out.152, %input.31, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.33 : Tensor = aten::relu_(%out.153) # torch/nn/functional.py:1117:17
  %2339 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%797)
  %2340 : Tensor = prim::GetAttr[name="weight"](%2339)
  %2341 : Tensor? = prim::GetAttr[name="bias"](%2339)
  %2342 : int[] = prim::ListConstruct(%13, %13)
  %2343 : int[] = prim::ListConstruct(%9, %9)
  %2344 : int[] = prim::ListConstruct(%13, %13)
  %out.163 : Tensor = aten::conv2d(%input.33, %2340, %2341, %2342, %2343, %2344, %13) # torch/nn/modules/conv.py:415:15
  %2346 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%797)
  %2347 : int = aten::dim(%out.163) # torch/nn/modules/batchnorm.py:276:11
  %2348 : bool = aten::ne(%2347, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2348) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2349 : bool = prim::GetAttr[name="training"](%2346)
   = prim::If(%2349) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2350 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2346)
      %2351 : Tensor = aten::add(%2350, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2346, %2351)
      -> ()
    block1():
      -> ()
  %2352 : bool = prim::GetAttr[name="training"](%2346)
  %2353 : Tensor = prim::GetAttr[name="running_mean"](%2346)
  %2354 : Tensor = prim::GetAttr[name="running_var"](%2346)
  %2355 : Tensor = prim::GetAttr[name="weight"](%2346)
  %2356 : Tensor = prim::GetAttr[name="bias"](%2346)
   = prim::If(%2352) # torch/nn/functional.py:2011:4
    block0():
      %2357 : int[] = aten::size(%out.163) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%2357, %9) # torch/nn/functional.py:1991:17
      %2359 : int = aten::len(%2357) # torch/nn/functional.py:1992:19
      %2360 : int = aten::sub(%2359, %11) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%2360, %10, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %2364 : int = aten::add(%i.53, %11) # torch/nn/functional.py:1993:27
          %2365 : int = aten::__getitem__(%2357, %2364) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %2365) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.211)
      %2367 : bool = aten::eq(%size_prods.209, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2367) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.155 : Tensor = aten::batch_norm(%out.163, %2355, %2356, %2353, %2354, %2352, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.156 : Tensor = aten::relu_(%out.155) # torch/nn/functional.py:1117:17
  %2370 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%797)
  %2371 : Tensor = prim::GetAttr[name="weight"](%2370)
  %2372 : Tensor? = prim::GetAttr[name="bias"](%2370)
  %2373 : int[] = prim::ListConstruct(%13, %13)
  %2374 : int[] = prim::ListConstruct(%13, %13)
  %2375 : int[] = prim::ListConstruct(%13, %13)
  %out.157 : Tensor = aten::conv2d(%out.156, %2371, %2372, %2373, %2374, %2375, %3) # torch/nn/modules/conv.py:415:15
  %2377 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%797)
  %2378 : int = aten::dim(%out.157) # torch/nn/modules/batchnorm.py:276:11
  %2379 : bool = aten::ne(%2378, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2379) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2380 : bool = prim::GetAttr[name="training"](%2377)
   = prim::If(%2380) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2381 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2377)
      %2382 : Tensor = aten::add(%2381, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2377, %2382)
      -> ()
    block1():
      -> ()
  %2383 : bool = prim::GetAttr[name="training"](%2377)
  %2384 : Tensor = prim::GetAttr[name="running_mean"](%2377)
  %2385 : Tensor = prim::GetAttr[name="running_var"](%2377)
  %2386 : Tensor = prim::GetAttr[name="weight"](%2377)
  %2387 : Tensor = prim::GetAttr[name="bias"](%2377)
   = prim::If(%2383) # torch/nn/functional.py:2011:4
    block0():
      %2388 : int[] = aten::size(%out.157) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%2388, %9) # torch/nn/functional.py:1991:17
      %2390 : int = aten::len(%2388) # torch/nn/functional.py:1992:19
      %2391 : int = aten::sub(%2390, %11) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%2391, %10, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %2395 : int = aten::add(%i.54, %11) # torch/nn/functional.py:1993:27
          %2396 : int = aten::__getitem__(%2388, %2395) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %2396) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.215)
      %2398 : bool = aten::eq(%size_prods.213, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2398) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.158 : Tensor = aten::batch_norm(%out.157, %2386, %2387, %2384, %2385, %2383, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.159 : Tensor = aten::relu_(%out.158) # torch/nn/functional.py:1117:17
  %2401 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%797)
  %2402 : Tensor = prim::GetAttr[name="weight"](%2401)
  %2403 : Tensor? = prim::GetAttr[name="bias"](%2401)
  %2404 : int[] = prim::ListConstruct(%13, %13)
  %2405 : int[] = prim::ListConstruct(%9, %9)
  %2406 : int[] = prim::ListConstruct(%13, %13)
  %out.160 : Tensor = aten::conv2d(%out.159, %2402, %2403, %2404, %2405, %2406, %13) # torch/nn/modules/conv.py:415:15
  %2408 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%797)
  %2409 : int = aten::dim(%out.160) # torch/nn/modules/batchnorm.py:276:11
  %2410 : bool = aten::ne(%2409, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2410) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2411 : bool = prim::GetAttr[name="training"](%2408)
   = prim::If(%2411) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2412 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2408)
      %2413 : Tensor = aten::add(%2412, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2408, %2413)
      -> ()
    block1():
      -> ()
  %2414 : bool = prim::GetAttr[name="training"](%2408)
  %2415 : Tensor = prim::GetAttr[name="running_mean"](%2408)
  %2416 : Tensor = prim::GetAttr[name="running_var"](%2408)
  %2417 : Tensor = prim::GetAttr[name="weight"](%2408)
  %2418 : Tensor = prim::GetAttr[name="bias"](%2408)
   = prim::If(%2414) # torch/nn/functional.py:2011:4
    block0():
      %2419 : int[] = aten::size(%out.160) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%2419, %9) # torch/nn/functional.py:1991:17
      %2421 : int = aten::len(%2419) # torch/nn/functional.py:1992:19
      %2422 : int = aten::sub(%2421, %11) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%2422, %10, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %2426 : int = aten::add(%i.55, %11) # torch/nn/functional.py:1993:27
          %2427 : int = aten::__getitem__(%2419, %2426) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %2427) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.219)
      %2429 : bool = aten::eq(%size_prods.217, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2429) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.161 : Tensor = aten::batch_norm(%out.160, %2417, %2418, %2415, %2416, %2414, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.162 : Tensor = aten::add_(%out.161, %input.33, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.35 : Tensor = aten::relu_(%out.162) # torch/nn/functional.py:1117:17
  %2433 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%798)
  %2434 : Tensor = prim::GetAttr[name="weight"](%2433)
  %2435 : Tensor? = prim::GetAttr[name="bias"](%2433)
  %2436 : int[] = prim::ListConstruct(%13, %13)
  %2437 : int[] = prim::ListConstruct(%9, %9)
  %2438 : int[] = prim::ListConstruct(%13, %13)
  %out.172 : Tensor = aten::conv2d(%input.35, %2434, %2435, %2436, %2437, %2438, %13) # torch/nn/modules/conv.py:415:15
  %2440 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%798)
  %2441 : int = aten::dim(%out.172) # torch/nn/modules/batchnorm.py:276:11
  %2442 : bool = aten::ne(%2441, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2442) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2443 : bool = prim::GetAttr[name="training"](%2440)
   = prim::If(%2443) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2444 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2440)
      %2445 : Tensor = aten::add(%2444, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2440, %2445)
      -> ()
    block1():
      -> ()
  %2446 : bool = prim::GetAttr[name="training"](%2440)
  %2447 : Tensor = prim::GetAttr[name="running_mean"](%2440)
  %2448 : Tensor = prim::GetAttr[name="running_var"](%2440)
  %2449 : Tensor = prim::GetAttr[name="weight"](%2440)
  %2450 : Tensor = prim::GetAttr[name="bias"](%2440)
   = prim::If(%2446) # torch/nn/functional.py:2011:4
    block0():
      %2451 : int[] = aten::size(%out.172) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%2451, %9) # torch/nn/functional.py:1991:17
      %2453 : int = aten::len(%2451) # torch/nn/functional.py:1992:19
      %2454 : int = aten::sub(%2453, %11) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%2454, %10, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %2458 : int = aten::add(%i.56, %11) # torch/nn/functional.py:1993:27
          %2459 : int = aten::__getitem__(%2451, %2458) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %2459) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.223)
      %2461 : bool = aten::eq(%size_prods.221, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2461) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.164 : Tensor = aten::batch_norm(%out.172, %2449, %2450, %2447, %2448, %2446, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.165 : Tensor = aten::relu_(%out.164) # torch/nn/functional.py:1117:17
  %2464 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%798)
  %2465 : Tensor = prim::GetAttr[name="weight"](%2464)
  %2466 : Tensor? = prim::GetAttr[name="bias"](%2464)
  %2467 : int[] = prim::ListConstruct(%13, %13)
  %2468 : int[] = prim::ListConstruct(%13, %13)
  %2469 : int[] = prim::ListConstruct(%13, %13)
  %out.166 : Tensor = aten::conv2d(%out.165, %2465, %2466, %2467, %2468, %2469, %3) # torch/nn/modules/conv.py:415:15
  %2471 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%798)
  %2472 : int = aten::dim(%out.166) # torch/nn/modules/batchnorm.py:276:11
  %2473 : bool = aten::ne(%2472, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2473) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2474 : bool = prim::GetAttr[name="training"](%2471)
   = prim::If(%2474) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2475 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2471)
      %2476 : Tensor = aten::add(%2475, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2471, %2476)
      -> ()
    block1():
      -> ()
  %2477 : bool = prim::GetAttr[name="training"](%2471)
  %2478 : Tensor = prim::GetAttr[name="running_mean"](%2471)
  %2479 : Tensor = prim::GetAttr[name="running_var"](%2471)
  %2480 : Tensor = prim::GetAttr[name="weight"](%2471)
  %2481 : Tensor = prim::GetAttr[name="bias"](%2471)
   = prim::If(%2477) # torch/nn/functional.py:2011:4
    block0():
      %2482 : int[] = aten::size(%out.166) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%2482, %9) # torch/nn/functional.py:1991:17
      %2484 : int = aten::len(%2482) # torch/nn/functional.py:1992:19
      %2485 : int = aten::sub(%2484, %11) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%2485, %10, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %2489 : int = aten::add(%i.57, %11) # torch/nn/functional.py:1993:27
          %2490 : int = aten::__getitem__(%2482, %2489) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %2490) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.227)
      %2492 : bool = aten::eq(%size_prods.225, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2492) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.167 : Tensor = aten::batch_norm(%out.166, %2480, %2481, %2478, %2479, %2477, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.168 : Tensor = aten::relu_(%out.167) # torch/nn/functional.py:1117:17
  %2495 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%798)
  %2496 : Tensor = prim::GetAttr[name="weight"](%2495)
  %2497 : Tensor? = prim::GetAttr[name="bias"](%2495)
  %2498 : int[] = prim::ListConstruct(%13, %13)
  %2499 : int[] = prim::ListConstruct(%9, %9)
  %2500 : int[] = prim::ListConstruct(%13, %13)
  %out.169 : Tensor = aten::conv2d(%out.168, %2496, %2497, %2498, %2499, %2500, %13) # torch/nn/modules/conv.py:415:15
  %2502 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%798)
  %2503 : int = aten::dim(%out.169) # torch/nn/modules/batchnorm.py:276:11
  %2504 : bool = aten::ne(%2503, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2504) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2505 : bool = prim::GetAttr[name="training"](%2502)
   = prim::If(%2505) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2506 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2502)
      %2507 : Tensor = aten::add(%2506, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2502, %2507)
      -> ()
    block1():
      -> ()
  %2508 : bool = prim::GetAttr[name="training"](%2502)
  %2509 : Tensor = prim::GetAttr[name="running_mean"](%2502)
  %2510 : Tensor = prim::GetAttr[name="running_var"](%2502)
  %2511 : Tensor = prim::GetAttr[name="weight"](%2502)
  %2512 : Tensor = prim::GetAttr[name="bias"](%2502)
   = prim::If(%2508) # torch/nn/functional.py:2011:4
    block0():
      %2513 : int[] = aten::size(%out.169) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%2513, %9) # torch/nn/functional.py:1991:17
      %2515 : int = aten::len(%2513) # torch/nn/functional.py:1992:19
      %2516 : int = aten::sub(%2515, %11) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%2516, %10, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %2520 : int = aten::add(%i.58, %11) # torch/nn/functional.py:1993:27
          %2521 : int = aten::__getitem__(%2513, %2520) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %2521) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.231)
      %2523 : bool = aten::eq(%size_prods.229, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2523) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.170 : Tensor = aten::batch_norm(%out.169, %2511, %2512, %2509, %2510, %2508, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.171 : Tensor = aten::add_(%out.170, %input.35, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.37 : Tensor = aten::relu_(%out.171) # torch/nn/functional.py:1117:17
  %2527 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%799)
  %2528 : Tensor = prim::GetAttr[name="weight"](%2527)
  %2529 : Tensor? = prim::GetAttr[name="bias"](%2527)
  %2530 : int[] = prim::ListConstruct(%13, %13)
  %2531 : int[] = prim::ListConstruct(%9, %9)
  %2532 : int[] = prim::ListConstruct(%13, %13)
  %out.181 : Tensor = aten::conv2d(%input.37, %2528, %2529, %2530, %2531, %2532, %13) # torch/nn/modules/conv.py:415:15
  %2534 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%799)
  %2535 : int = aten::dim(%out.181) # torch/nn/modules/batchnorm.py:276:11
  %2536 : bool = aten::ne(%2535, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2536) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2537 : bool = prim::GetAttr[name="training"](%2534)
   = prim::If(%2537) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2538 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2534)
      %2539 : Tensor = aten::add(%2538, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2534, %2539)
      -> ()
    block1():
      -> ()
  %2540 : bool = prim::GetAttr[name="training"](%2534)
  %2541 : Tensor = prim::GetAttr[name="running_mean"](%2534)
  %2542 : Tensor = prim::GetAttr[name="running_var"](%2534)
  %2543 : Tensor = prim::GetAttr[name="weight"](%2534)
  %2544 : Tensor = prim::GetAttr[name="bias"](%2534)
   = prim::If(%2540) # torch/nn/functional.py:2011:4
    block0():
      %2545 : int[] = aten::size(%out.181) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%2545, %9) # torch/nn/functional.py:1991:17
      %2547 : int = aten::len(%2545) # torch/nn/functional.py:1992:19
      %2548 : int = aten::sub(%2547, %11) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%2548, %10, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %2552 : int = aten::add(%i.59, %11) # torch/nn/functional.py:1993:27
          %2553 : int = aten::__getitem__(%2545, %2552) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %2553) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.235)
      %2555 : bool = aten::eq(%size_prods.233, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2555) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.173 : Tensor = aten::batch_norm(%out.181, %2543, %2544, %2541, %2542, %2540, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.174 : Tensor = aten::relu_(%out.173) # torch/nn/functional.py:1117:17
  %2558 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%799)
  %2559 : Tensor = prim::GetAttr[name="weight"](%2558)
  %2560 : Tensor? = prim::GetAttr[name="bias"](%2558)
  %2561 : int[] = prim::ListConstruct(%13, %13)
  %2562 : int[] = prim::ListConstruct(%13, %13)
  %2563 : int[] = prim::ListConstruct(%13, %13)
  %out.175 : Tensor = aten::conv2d(%out.174, %2559, %2560, %2561, %2562, %2563, %3) # torch/nn/modules/conv.py:415:15
  %2565 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%799)
  %2566 : int = aten::dim(%out.175) # torch/nn/modules/batchnorm.py:276:11
  %2567 : bool = aten::ne(%2566, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2567) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2568 : bool = prim::GetAttr[name="training"](%2565)
   = prim::If(%2568) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2569 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2565)
      %2570 : Tensor = aten::add(%2569, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2565, %2570)
      -> ()
    block1():
      -> ()
  %2571 : bool = prim::GetAttr[name="training"](%2565)
  %2572 : Tensor = prim::GetAttr[name="running_mean"](%2565)
  %2573 : Tensor = prim::GetAttr[name="running_var"](%2565)
  %2574 : Tensor = prim::GetAttr[name="weight"](%2565)
  %2575 : Tensor = prim::GetAttr[name="bias"](%2565)
   = prim::If(%2571) # torch/nn/functional.py:2011:4
    block0():
      %2576 : int[] = aten::size(%out.175) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%2576, %9) # torch/nn/functional.py:1991:17
      %2578 : int = aten::len(%2576) # torch/nn/functional.py:1992:19
      %2579 : int = aten::sub(%2578, %11) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%2579, %10, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %2583 : int = aten::add(%i.60, %11) # torch/nn/functional.py:1993:27
          %2584 : int = aten::__getitem__(%2576, %2583) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %2584) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.239)
      %2586 : bool = aten::eq(%size_prods.237, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2586) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.176 : Tensor = aten::batch_norm(%out.175, %2574, %2575, %2572, %2573, %2571, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.177 : Tensor = aten::relu_(%out.176) # torch/nn/functional.py:1117:17
  %2589 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%799)
  %2590 : Tensor = prim::GetAttr[name="weight"](%2589)
  %2591 : Tensor? = prim::GetAttr[name="bias"](%2589)
  %2592 : int[] = prim::ListConstruct(%13, %13)
  %2593 : int[] = prim::ListConstruct(%9, %9)
  %2594 : int[] = prim::ListConstruct(%13, %13)
  %out.178 : Tensor = aten::conv2d(%out.177, %2590, %2591, %2592, %2593, %2594, %13) # torch/nn/modules/conv.py:415:15
  %2596 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%799)
  %2597 : int = aten::dim(%out.178) # torch/nn/modules/batchnorm.py:276:11
  %2598 : bool = aten::ne(%2597, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2598) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2599 : bool = prim::GetAttr[name="training"](%2596)
   = prim::If(%2599) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2600 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2596)
      %2601 : Tensor = aten::add(%2600, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2596, %2601)
      -> ()
    block1():
      -> ()
  %2602 : bool = prim::GetAttr[name="training"](%2596)
  %2603 : Tensor = prim::GetAttr[name="running_mean"](%2596)
  %2604 : Tensor = prim::GetAttr[name="running_var"](%2596)
  %2605 : Tensor = prim::GetAttr[name="weight"](%2596)
  %2606 : Tensor = prim::GetAttr[name="bias"](%2596)
   = prim::If(%2602) # torch/nn/functional.py:2011:4
    block0():
      %2607 : int[] = aten::size(%out.178) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%2607, %9) # torch/nn/functional.py:1991:17
      %2609 : int = aten::len(%2607) # torch/nn/functional.py:1992:19
      %2610 : int = aten::sub(%2609, %11) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%2610, %10, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %2614 : int = aten::add(%i.61, %11) # torch/nn/functional.py:1993:27
          %2615 : int = aten::__getitem__(%2607, %2614) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %2615) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.243)
      %2617 : bool = aten::eq(%size_prods.241, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2617) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.179 : Tensor = aten::batch_norm(%out.178, %2605, %2606, %2603, %2604, %2602, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.180 : Tensor = aten::add_(%out.179, %input.37, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.39 : Tensor = aten::relu_(%out.180) # torch/nn/functional.py:1117:17
  %2621 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%800)
  %2622 : Tensor = prim::GetAttr[name="weight"](%2621)
  %2623 : Tensor? = prim::GetAttr[name="bias"](%2621)
  %2624 : int[] = prim::ListConstruct(%13, %13)
  %2625 : int[] = prim::ListConstruct(%9, %9)
  %2626 : int[] = prim::ListConstruct(%13, %13)
  %out.190 : Tensor = aten::conv2d(%input.39, %2622, %2623, %2624, %2625, %2626, %13) # torch/nn/modules/conv.py:415:15
  %2628 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%800)
  %2629 : int = aten::dim(%out.190) # torch/nn/modules/batchnorm.py:276:11
  %2630 : bool = aten::ne(%2629, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2630) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2631 : bool = prim::GetAttr[name="training"](%2628)
   = prim::If(%2631) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2632 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2628)
      %2633 : Tensor = aten::add(%2632, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2628, %2633)
      -> ()
    block1():
      -> ()
  %2634 : bool = prim::GetAttr[name="training"](%2628)
  %2635 : Tensor = prim::GetAttr[name="running_mean"](%2628)
  %2636 : Tensor = prim::GetAttr[name="running_var"](%2628)
  %2637 : Tensor = prim::GetAttr[name="weight"](%2628)
  %2638 : Tensor = prim::GetAttr[name="bias"](%2628)
   = prim::If(%2634) # torch/nn/functional.py:2011:4
    block0():
      %2639 : int[] = aten::size(%out.190) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%2639, %9) # torch/nn/functional.py:1991:17
      %2641 : int = aten::len(%2639) # torch/nn/functional.py:1992:19
      %2642 : int = aten::sub(%2641, %11) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%2642, %10, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %2646 : int = aten::add(%i.62, %11) # torch/nn/functional.py:1993:27
          %2647 : int = aten::__getitem__(%2639, %2646) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %2647) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.247)
      %2649 : bool = aten::eq(%size_prods.245, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2649) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.182 : Tensor = aten::batch_norm(%out.190, %2637, %2638, %2635, %2636, %2634, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.183 : Tensor = aten::relu_(%out.182) # torch/nn/functional.py:1117:17
  %2652 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%800)
  %2653 : Tensor = prim::GetAttr[name="weight"](%2652)
  %2654 : Tensor? = prim::GetAttr[name="bias"](%2652)
  %2655 : int[] = prim::ListConstruct(%13, %13)
  %2656 : int[] = prim::ListConstruct(%13, %13)
  %2657 : int[] = prim::ListConstruct(%13, %13)
  %out.184 : Tensor = aten::conv2d(%out.183, %2653, %2654, %2655, %2656, %2657, %3) # torch/nn/modules/conv.py:415:15
  %2659 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%800)
  %2660 : int = aten::dim(%out.184) # torch/nn/modules/batchnorm.py:276:11
  %2661 : bool = aten::ne(%2660, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2661) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2662 : bool = prim::GetAttr[name="training"](%2659)
   = prim::If(%2662) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2663 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2659)
      %2664 : Tensor = aten::add(%2663, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2659, %2664)
      -> ()
    block1():
      -> ()
  %2665 : bool = prim::GetAttr[name="training"](%2659)
  %2666 : Tensor = prim::GetAttr[name="running_mean"](%2659)
  %2667 : Tensor = prim::GetAttr[name="running_var"](%2659)
  %2668 : Tensor = prim::GetAttr[name="weight"](%2659)
  %2669 : Tensor = prim::GetAttr[name="bias"](%2659)
   = prim::If(%2665) # torch/nn/functional.py:2011:4
    block0():
      %2670 : int[] = aten::size(%out.184) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%2670, %9) # torch/nn/functional.py:1991:17
      %2672 : int = aten::len(%2670) # torch/nn/functional.py:1992:19
      %2673 : int = aten::sub(%2672, %11) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%2673, %10, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %2677 : int = aten::add(%i.63, %11) # torch/nn/functional.py:1993:27
          %2678 : int = aten::__getitem__(%2670, %2677) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %2678) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.251)
      %2680 : bool = aten::eq(%size_prods.249, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2680) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.185 : Tensor = aten::batch_norm(%out.184, %2668, %2669, %2666, %2667, %2665, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.186 : Tensor = aten::relu_(%out.185) # torch/nn/functional.py:1117:17
  %2683 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%800)
  %2684 : Tensor = prim::GetAttr[name="weight"](%2683)
  %2685 : Tensor? = prim::GetAttr[name="bias"](%2683)
  %2686 : int[] = prim::ListConstruct(%13, %13)
  %2687 : int[] = prim::ListConstruct(%9, %9)
  %2688 : int[] = prim::ListConstruct(%13, %13)
  %out.187 : Tensor = aten::conv2d(%out.186, %2684, %2685, %2686, %2687, %2688, %13) # torch/nn/modules/conv.py:415:15
  %2690 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%800)
  %2691 : int = aten::dim(%out.187) # torch/nn/modules/batchnorm.py:276:11
  %2692 : bool = aten::ne(%2691, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2692) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2693 : bool = prim::GetAttr[name="training"](%2690)
   = prim::If(%2693) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2694 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2690)
      %2695 : Tensor = aten::add(%2694, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2690, %2695)
      -> ()
    block1():
      -> ()
  %2696 : bool = prim::GetAttr[name="training"](%2690)
  %2697 : Tensor = prim::GetAttr[name="running_mean"](%2690)
  %2698 : Tensor = prim::GetAttr[name="running_var"](%2690)
  %2699 : Tensor = prim::GetAttr[name="weight"](%2690)
  %2700 : Tensor = prim::GetAttr[name="bias"](%2690)
   = prim::If(%2696) # torch/nn/functional.py:2011:4
    block0():
      %2701 : int[] = aten::size(%out.187) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%2701, %9) # torch/nn/functional.py:1991:17
      %2703 : int = aten::len(%2701) # torch/nn/functional.py:1992:19
      %2704 : int = aten::sub(%2703, %11) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%2704, %10, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %2708 : int = aten::add(%i.64, %11) # torch/nn/functional.py:1993:27
          %2709 : int = aten::__getitem__(%2701, %2708) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %2709) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.255)
      %2711 : bool = aten::eq(%size_prods.253, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2711) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.188 : Tensor = aten::batch_norm(%out.187, %2699, %2700, %2697, %2698, %2696, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.189 : Tensor = aten::add_(%out.188, %input.39, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.41 : Tensor = aten::relu_(%out.189) # torch/nn/functional.py:1117:17
  %2715 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%801)
  %2716 : Tensor = prim::GetAttr[name="weight"](%2715)
  %2717 : Tensor? = prim::GetAttr[name="bias"](%2715)
  %2718 : int[] = prim::ListConstruct(%13, %13)
  %2719 : int[] = prim::ListConstruct(%9, %9)
  %2720 : int[] = prim::ListConstruct(%13, %13)
  %out.199 : Tensor = aten::conv2d(%input.41, %2716, %2717, %2718, %2719, %2720, %13) # torch/nn/modules/conv.py:415:15
  %2722 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%801)
  %2723 : int = aten::dim(%out.199) # torch/nn/modules/batchnorm.py:276:11
  %2724 : bool = aten::ne(%2723, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2724) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2725 : bool = prim::GetAttr[name="training"](%2722)
   = prim::If(%2725) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2726 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2722)
      %2727 : Tensor = aten::add(%2726, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2722, %2727)
      -> ()
    block1():
      -> ()
  %2728 : bool = prim::GetAttr[name="training"](%2722)
  %2729 : Tensor = prim::GetAttr[name="running_mean"](%2722)
  %2730 : Tensor = prim::GetAttr[name="running_var"](%2722)
  %2731 : Tensor = prim::GetAttr[name="weight"](%2722)
  %2732 : Tensor = prim::GetAttr[name="bias"](%2722)
   = prim::If(%2728) # torch/nn/functional.py:2011:4
    block0():
      %2733 : int[] = aten::size(%out.199) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%2733, %9) # torch/nn/functional.py:1991:17
      %2735 : int = aten::len(%2733) # torch/nn/functional.py:1992:19
      %2736 : int = aten::sub(%2735, %11) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%2736, %10, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %2740 : int = aten::add(%i.65, %11) # torch/nn/functional.py:1993:27
          %2741 : int = aten::__getitem__(%2733, %2740) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %2741) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.259)
      %2743 : bool = aten::eq(%size_prods.257, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2743) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.191 : Tensor = aten::batch_norm(%out.199, %2731, %2732, %2729, %2730, %2728, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.192 : Tensor = aten::relu_(%out.191) # torch/nn/functional.py:1117:17
  %2746 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%801)
  %2747 : Tensor = prim::GetAttr[name="weight"](%2746)
  %2748 : Tensor? = prim::GetAttr[name="bias"](%2746)
  %2749 : int[] = prim::ListConstruct(%13, %13)
  %2750 : int[] = prim::ListConstruct(%13, %13)
  %2751 : int[] = prim::ListConstruct(%13, %13)
  %out.193 : Tensor = aten::conv2d(%out.192, %2747, %2748, %2749, %2750, %2751, %3) # torch/nn/modules/conv.py:415:15
  %2753 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%801)
  %2754 : int = aten::dim(%out.193) # torch/nn/modules/batchnorm.py:276:11
  %2755 : bool = aten::ne(%2754, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2755) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2756 : bool = prim::GetAttr[name="training"](%2753)
   = prim::If(%2756) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2757 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2753)
      %2758 : Tensor = aten::add(%2757, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2753, %2758)
      -> ()
    block1():
      -> ()
  %2759 : bool = prim::GetAttr[name="training"](%2753)
  %2760 : Tensor = prim::GetAttr[name="running_mean"](%2753)
  %2761 : Tensor = prim::GetAttr[name="running_var"](%2753)
  %2762 : Tensor = prim::GetAttr[name="weight"](%2753)
  %2763 : Tensor = prim::GetAttr[name="bias"](%2753)
   = prim::If(%2759) # torch/nn/functional.py:2011:4
    block0():
      %2764 : int[] = aten::size(%out.193) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%2764, %9) # torch/nn/functional.py:1991:17
      %2766 : int = aten::len(%2764) # torch/nn/functional.py:1992:19
      %2767 : int = aten::sub(%2766, %11) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%2767, %10, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %2771 : int = aten::add(%i.66, %11) # torch/nn/functional.py:1993:27
          %2772 : int = aten::__getitem__(%2764, %2771) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %2772) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.263)
      %2774 : bool = aten::eq(%size_prods.261, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2774) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.194 : Tensor = aten::batch_norm(%out.193, %2762, %2763, %2760, %2761, %2759, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.195 : Tensor = aten::relu_(%out.194) # torch/nn/functional.py:1117:17
  %2777 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%801)
  %2778 : Tensor = prim::GetAttr[name="weight"](%2777)
  %2779 : Tensor? = prim::GetAttr[name="bias"](%2777)
  %2780 : int[] = prim::ListConstruct(%13, %13)
  %2781 : int[] = prim::ListConstruct(%9, %9)
  %2782 : int[] = prim::ListConstruct(%13, %13)
  %out.196 : Tensor = aten::conv2d(%out.195, %2778, %2779, %2780, %2781, %2782, %13) # torch/nn/modules/conv.py:415:15
  %2784 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%801)
  %2785 : int = aten::dim(%out.196) # torch/nn/modules/batchnorm.py:276:11
  %2786 : bool = aten::ne(%2785, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2786) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2787 : bool = prim::GetAttr[name="training"](%2784)
   = prim::If(%2787) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2788 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2784)
      %2789 : Tensor = aten::add(%2788, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2784, %2789)
      -> ()
    block1():
      -> ()
  %2790 : bool = prim::GetAttr[name="training"](%2784)
  %2791 : Tensor = prim::GetAttr[name="running_mean"](%2784)
  %2792 : Tensor = prim::GetAttr[name="running_var"](%2784)
  %2793 : Tensor = prim::GetAttr[name="weight"](%2784)
  %2794 : Tensor = prim::GetAttr[name="bias"](%2784)
   = prim::If(%2790) # torch/nn/functional.py:2011:4
    block0():
      %2795 : int[] = aten::size(%out.196) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%2795, %9) # torch/nn/functional.py:1991:17
      %2797 : int = aten::len(%2795) # torch/nn/functional.py:1992:19
      %2798 : int = aten::sub(%2797, %11) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%2798, %10, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %2802 : int = aten::add(%i.67, %11) # torch/nn/functional.py:1993:27
          %2803 : int = aten::__getitem__(%2795, %2802) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %2803) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.267)
      %2805 : bool = aten::eq(%size_prods.265, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2805) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.197 : Tensor = aten::batch_norm(%out.196, %2793, %2794, %2791, %2792, %2790, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.198 : Tensor = aten::add_(%out.197, %input.41, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.43 : Tensor = aten::relu_(%out.198) # torch/nn/functional.py:1117:17
  %2809 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%802)
  %2810 : Tensor = prim::GetAttr[name="weight"](%2809)
  %2811 : Tensor? = prim::GetAttr[name="bias"](%2809)
  %2812 : int[] = prim::ListConstruct(%13, %13)
  %2813 : int[] = prim::ListConstruct(%9, %9)
  %2814 : int[] = prim::ListConstruct(%13, %13)
  %out.208 : Tensor = aten::conv2d(%input.43, %2810, %2811, %2812, %2813, %2814, %13) # torch/nn/modules/conv.py:415:15
  %2816 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%802)
  %2817 : int = aten::dim(%out.208) # torch/nn/modules/batchnorm.py:276:11
  %2818 : bool = aten::ne(%2817, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2818) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2819 : bool = prim::GetAttr[name="training"](%2816)
   = prim::If(%2819) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2820 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2816)
      %2821 : Tensor = aten::add(%2820, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2816, %2821)
      -> ()
    block1():
      -> ()
  %2822 : bool = prim::GetAttr[name="training"](%2816)
  %2823 : Tensor = prim::GetAttr[name="running_mean"](%2816)
  %2824 : Tensor = prim::GetAttr[name="running_var"](%2816)
  %2825 : Tensor = prim::GetAttr[name="weight"](%2816)
  %2826 : Tensor = prim::GetAttr[name="bias"](%2816)
   = prim::If(%2822) # torch/nn/functional.py:2011:4
    block0():
      %2827 : int[] = aten::size(%out.208) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%2827, %9) # torch/nn/functional.py:1991:17
      %2829 : int = aten::len(%2827) # torch/nn/functional.py:1992:19
      %2830 : int = aten::sub(%2829, %11) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%2830, %10, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %2834 : int = aten::add(%i.68, %11) # torch/nn/functional.py:1993:27
          %2835 : int = aten::__getitem__(%2827, %2834) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %2835) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.271)
      %2837 : bool = aten::eq(%size_prods.269, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2837) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.200 : Tensor = aten::batch_norm(%out.208, %2825, %2826, %2823, %2824, %2822, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.201 : Tensor = aten::relu_(%out.200) # torch/nn/functional.py:1117:17
  %2840 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%802)
  %2841 : Tensor = prim::GetAttr[name="weight"](%2840)
  %2842 : Tensor? = prim::GetAttr[name="bias"](%2840)
  %2843 : int[] = prim::ListConstruct(%13, %13)
  %2844 : int[] = prim::ListConstruct(%13, %13)
  %2845 : int[] = prim::ListConstruct(%13, %13)
  %out.202 : Tensor = aten::conv2d(%out.201, %2841, %2842, %2843, %2844, %2845, %3) # torch/nn/modules/conv.py:415:15
  %2847 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%802)
  %2848 : int = aten::dim(%out.202) # torch/nn/modules/batchnorm.py:276:11
  %2849 : bool = aten::ne(%2848, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2849) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2850 : bool = prim::GetAttr[name="training"](%2847)
   = prim::If(%2850) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2851 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2847)
      %2852 : Tensor = aten::add(%2851, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2847, %2852)
      -> ()
    block1():
      -> ()
  %2853 : bool = prim::GetAttr[name="training"](%2847)
  %2854 : Tensor = prim::GetAttr[name="running_mean"](%2847)
  %2855 : Tensor = prim::GetAttr[name="running_var"](%2847)
  %2856 : Tensor = prim::GetAttr[name="weight"](%2847)
  %2857 : Tensor = prim::GetAttr[name="bias"](%2847)
   = prim::If(%2853) # torch/nn/functional.py:2011:4
    block0():
      %2858 : int[] = aten::size(%out.202) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%2858, %9) # torch/nn/functional.py:1991:17
      %2860 : int = aten::len(%2858) # torch/nn/functional.py:1992:19
      %2861 : int = aten::sub(%2860, %11) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%2861, %10, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %2865 : int = aten::add(%i.69, %11) # torch/nn/functional.py:1993:27
          %2866 : int = aten::__getitem__(%2858, %2865) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %2866) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.275)
      %2868 : bool = aten::eq(%size_prods.273, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2868) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.203 : Tensor = aten::batch_norm(%out.202, %2856, %2857, %2854, %2855, %2853, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.204 : Tensor = aten::relu_(%out.203) # torch/nn/functional.py:1117:17
  %2871 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%802)
  %2872 : Tensor = prim::GetAttr[name="weight"](%2871)
  %2873 : Tensor? = prim::GetAttr[name="bias"](%2871)
  %2874 : int[] = prim::ListConstruct(%13, %13)
  %2875 : int[] = prim::ListConstruct(%9, %9)
  %2876 : int[] = prim::ListConstruct(%13, %13)
  %out.205 : Tensor = aten::conv2d(%out.204, %2872, %2873, %2874, %2875, %2876, %13) # torch/nn/modules/conv.py:415:15
  %2878 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%802)
  %2879 : int = aten::dim(%out.205) # torch/nn/modules/batchnorm.py:276:11
  %2880 : bool = aten::ne(%2879, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2880) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2881 : bool = prim::GetAttr[name="training"](%2878)
   = prim::If(%2881) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2882 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2878)
      %2883 : Tensor = aten::add(%2882, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2878, %2883)
      -> ()
    block1():
      -> ()
  %2884 : bool = prim::GetAttr[name="training"](%2878)
  %2885 : Tensor = prim::GetAttr[name="running_mean"](%2878)
  %2886 : Tensor = prim::GetAttr[name="running_var"](%2878)
  %2887 : Tensor = prim::GetAttr[name="weight"](%2878)
  %2888 : Tensor = prim::GetAttr[name="bias"](%2878)
   = prim::If(%2884) # torch/nn/functional.py:2011:4
    block0():
      %2889 : int[] = aten::size(%out.205) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%2889, %9) # torch/nn/functional.py:1991:17
      %2891 : int = aten::len(%2889) # torch/nn/functional.py:1992:19
      %2892 : int = aten::sub(%2891, %11) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%2892, %10, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %2896 : int = aten::add(%i.70, %11) # torch/nn/functional.py:1993:27
          %2897 : int = aten::__getitem__(%2889, %2896) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %2897) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.279)
      %2899 : bool = aten::eq(%size_prods.277, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2899) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.206 : Tensor = aten::batch_norm(%out.205, %2887, %2888, %2885, %2886, %2884, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.207 : Tensor = aten::add_(%out.206, %input.43, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.45 : Tensor = aten::relu_(%out.207) # torch/nn/functional.py:1117:17
  %2903 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv1"](%803)
  %2904 : Tensor = prim::GetAttr[name="weight"](%2903)
  %2905 : Tensor? = prim::GetAttr[name="bias"](%2903)
  %2906 : int[] = prim::ListConstruct(%13, %13)
  %2907 : int[] = prim::ListConstruct(%9, %9)
  %2908 : int[] = prim::ListConstruct(%13, %13)
  %out.289 : Tensor = aten::conv2d(%input.45, %2904, %2905, %2906, %2907, %2908, %13) # torch/nn/modules/conv.py:415:15
  %2910 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn1"](%803)
  %2911 : int = aten::dim(%out.289) # torch/nn/modules/batchnorm.py:276:11
  %2912 : bool = aten::ne(%2911, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2912) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2913 : bool = prim::GetAttr[name="training"](%2910)
   = prim::If(%2913) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2914 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2910)
      %2915 : Tensor = aten::add(%2914, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2910, %2915)
      -> ()
    block1():
      -> ()
  %2916 : bool = prim::GetAttr[name="training"](%2910)
  %2917 : Tensor = prim::GetAttr[name="running_mean"](%2910)
  %2918 : Tensor = prim::GetAttr[name="running_var"](%2910)
  %2919 : Tensor = prim::GetAttr[name="weight"](%2910)
  %2920 : Tensor = prim::GetAttr[name="bias"](%2910)
   = prim::If(%2916) # torch/nn/functional.py:2011:4
    block0():
      %2921 : int[] = aten::size(%out.289) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%2921, %9) # torch/nn/functional.py:1991:17
      %2923 : int = aten::len(%2921) # torch/nn/functional.py:1992:19
      %2924 : int = aten::sub(%2923, %11) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%2924, %10, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %2928 : int = aten::add(%i.102, %11) # torch/nn/functional.py:1993:27
          %2929 : int = aten::__getitem__(%2921, %2928) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %2929) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.407)
      %2931 : bool = aten::eq(%size_prods.405, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2931) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.290 : Tensor = aten::batch_norm(%out.289, %2919, %2920, %2917, %2918, %2916, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.291 : Tensor = aten::relu_(%out.290) # torch/nn/functional.py:1117:17
  %2934 : __torch__.torch.nn.modules.conv.___torch_mangle_995.Conv2d = prim::GetAttr[name="conv2"](%803)
  %2935 : Tensor = prim::GetAttr[name="weight"](%2934)
  %2936 : Tensor? = prim::GetAttr[name="bias"](%2934)
  %2937 : int[] = prim::ListConstruct(%13, %13)
  %2938 : int[] = prim::ListConstruct(%13, %13)
  %2939 : int[] = prim::ListConstruct(%13, %13)
  %out.292 : Tensor = aten::conv2d(%out.291, %2935, %2936, %2937, %2938, %2939, %3) # torch/nn/modules/conv.py:415:15
  %2941 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn2"](%803)
  %2942 : int = aten::dim(%out.292) # torch/nn/modules/batchnorm.py:276:11
  %2943 : bool = aten::ne(%2942, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2943) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2944 : bool = prim::GetAttr[name="training"](%2941)
   = prim::If(%2944) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2945 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2941)
      %2946 : Tensor = aten::add(%2945, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2941, %2946)
      -> ()
    block1():
      -> ()
  %2947 : bool = prim::GetAttr[name="training"](%2941)
  %2948 : Tensor = prim::GetAttr[name="running_mean"](%2941)
  %2949 : Tensor = prim::GetAttr[name="running_var"](%2941)
  %2950 : Tensor = prim::GetAttr[name="weight"](%2941)
  %2951 : Tensor = prim::GetAttr[name="bias"](%2941)
   = prim::If(%2947) # torch/nn/functional.py:2011:4
    block0():
      %2952 : int[] = aten::size(%out.292) # torch/nn/functional.py:2012:27
      %size_prods.408 : int = aten::__getitem__(%2952, %9) # torch/nn/functional.py:1991:17
      %2954 : int = aten::len(%2952) # torch/nn/functional.py:1992:19
      %2955 : int = aten::sub(%2954, %11) # torch/nn/functional.py:1992:19
      %size_prods.409 : int = prim::Loop(%2955, %10, %size_prods.408) # torch/nn/functional.py:1992:4
        block0(%i.103 : int, %size_prods.410 : int):
          %2959 : int = aten::add(%i.103, %11) # torch/nn/functional.py:1993:27
          %2960 : int = aten::__getitem__(%2952, %2959) # torch/nn/functional.py:1993:22
          %size_prods.411 : int = aten::mul(%size_prods.410, %2960) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.411)
      %2962 : bool = aten::eq(%size_prods.409, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2962) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.293 : Tensor = aten::batch_norm(%out.292, %2950, %2951, %2948, %2949, %2947, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.294 : Tensor = aten::relu_(%out.293) # torch/nn/functional.py:1117:17
  %2965 : __torch__.torch.nn.modules.conv.___torch_mangle_993.Conv2d = prim::GetAttr[name="conv3"](%803)
  %2966 : Tensor = prim::GetAttr[name="weight"](%2965)
  %2967 : Tensor? = prim::GetAttr[name="bias"](%2965)
  %2968 : int[] = prim::ListConstruct(%13, %13)
  %2969 : int[] = prim::ListConstruct(%9, %9)
  %2970 : int[] = prim::ListConstruct(%13, %13)
  %out.295 : Tensor = aten::conv2d(%out.294, %2966, %2967, %2968, %2969, %2970, %13) # torch/nn/modules/conv.py:415:15
  %2972 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%803)
  %2973 : int = aten::dim(%out.295) # torch/nn/modules/batchnorm.py:276:11
  %2974 : bool = aten::ne(%2973, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2974) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2975 : bool = prim::GetAttr[name="training"](%2972)
   = prim::If(%2975) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2976 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2972)
      %2977 : Tensor = aten::add(%2976, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2972, %2977)
      -> ()
    block1():
      -> ()
  %2978 : bool = prim::GetAttr[name="training"](%2972)
  %2979 : Tensor = prim::GetAttr[name="running_mean"](%2972)
  %2980 : Tensor = prim::GetAttr[name="running_var"](%2972)
  %2981 : Tensor = prim::GetAttr[name="weight"](%2972)
  %2982 : Tensor = prim::GetAttr[name="bias"](%2972)
   = prim::If(%2978) # torch/nn/functional.py:2011:4
    block0():
      %2983 : int[] = aten::size(%out.295) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%2983, %9) # torch/nn/functional.py:1991:17
      %2985 : int = aten::len(%2983) # torch/nn/functional.py:1992:19
      %2986 : int = aten::sub(%2985, %11) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%2986, %10, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %2990 : int = aten::add(%i.104, %11) # torch/nn/functional.py:1993:27
          %2991 : int = aten::__getitem__(%2983, %2990) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %2991) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.415)
      %2993 : bool = aten::eq(%size_prods.413, %13) # torch/nn/functional.py:1994:7
       = prim::If(%2993) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.296 : Tensor = aten::batch_norm(%out.295, %2981, %2982, %2979, %2980, %2978, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.297 : Tensor = aten::add_(%out.296, %input.45, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.15 : Tensor = aten::relu_(%out.297) # torch/nn/functional.py:1117:17
  %2997 : __torch__.torch.nn.modules.container.___torch_mangle_1003.Sequential = prim::GetAttr[name="layer4"](%self)
  %2998 : __torch__.torchvision.models.resnet.___torch_mangle_1000.Bottleneck = prim::GetAttr[name="0"](%2997)
  %2999 : __torch__.torchvision.models.resnet.___torch_mangle_1002.Bottleneck = prim::GetAttr[name="1"](%2997)
  %3000 : __torch__.torchvision.models.resnet.___torch_mangle_1002.Bottleneck = prim::GetAttr[name="2"](%2997)
  %3001 : __torch__.torch.nn.modules.conv.___torch_mangle_43.Conv2d = prim::GetAttr[name="conv1"](%2998)
  %3002 : Tensor = prim::GetAttr[name="weight"](%3001)
  %3003 : Tensor? = prim::GetAttr[name="bias"](%3001)
  %3004 : int[] = prim::ListConstruct(%13, %13)
  %3005 : int[] = prim::ListConstruct(%9, %9)
  %3006 : int[] = prim::ListConstruct(%13, %13)
  %out.2 : Tensor = aten::conv2d(%x.15, %3002, %3003, %3004, %3005, %3006, %13) # torch/nn/modules/conv.py:415:15
  %3008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn1"](%2998)
  %3009 : int = aten::dim(%out.2) # torch/nn/modules/batchnorm.py:276:11
  %3010 : bool = aten::ne(%3009, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3010) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3011 : bool = prim::GetAttr[name="training"](%3008)
   = prim::If(%3011) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3008)
      %3013 : Tensor = aten::add(%3012, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3008, %3013)
      -> ()
    block1():
      -> ()
  %3014 : bool = prim::GetAttr[name="training"](%3008)
  %3015 : Tensor = prim::GetAttr[name="running_mean"](%3008)
  %3016 : Tensor = prim::GetAttr[name="running_var"](%3008)
  %3017 : Tensor = prim::GetAttr[name="weight"](%3008)
  %3018 : Tensor = prim::GetAttr[name="bias"](%3008)
   = prim::If(%3014) # torch/nn/functional.py:2011:4
    block0():
      %3019 : int[] = aten::size(%out.2) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%3019, %9) # torch/nn/functional.py:1991:17
      %3021 : int = aten::len(%3019) # torch/nn/functional.py:1992:19
      %3022 : int = aten::sub(%3021, %11) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%3022, %10, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.18 : int):
          %3026 : int = aten::add(%i.5, %11) # torch/nn/functional.py:1993:27
          %3027 : int = aten::__getitem__(%3019, %3026) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %3027) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.19)
      %3029 : bool = aten::eq(%size_prods.17, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3029) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.4 : Tensor = aten::batch_norm(%out.2, %3017, %3018, %3015, %3016, %3014, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.6 : Tensor = aten::relu_(%out.4) # torch/nn/functional.py:1117:17
  %3032 : __torch__.torch.nn.modules.conv.___torch_mangle_998.Conv2d = prim::GetAttr[name="conv2"](%2998)
  %3033 : Tensor = prim::GetAttr[name="weight"](%3032)
  %3034 : Tensor? = prim::GetAttr[name="bias"](%3032)
  %3035 : int[] = prim::ListConstruct(%11, %11)
  %3036 : int[] = prim::ListConstruct(%13, %13)
  %3037 : int[] = prim::ListConstruct(%13, %13)
  %out.8 : Tensor = aten::conv2d(%out.6, %3033, %3034, %3035, %3036, %3037, %3) # torch/nn/modules/conv.py:415:15
  %3039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn2"](%2998)
  %3040 : int = aten::dim(%out.8) # torch/nn/modules/batchnorm.py:276:11
  %3041 : bool = aten::ne(%3040, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3042 : bool = prim::GetAttr[name="training"](%3039)
   = prim::If(%3042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3039)
      %3044 : Tensor = aten::add(%3043, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3039, %3044)
      -> ()
    block1():
      -> ()
  %3045 : bool = prim::GetAttr[name="training"](%3039)
  %3046 : Tensor = prim::GetAttr[name="running_mean"](%3039)
  %3047 : Tensor = prim::GetAttr[name="running_var"](%3039)
  %3048 : Tensor = prim::GetAttr[name="weight"](%3039)
  %3049 : Tensor = prim::GetAttr[name="bias"](%3039)
   = prim::If(%3045) # torch/nn/functional.py:2011:4
    block0():
      %3050 : int[] = aten::size(%out.8) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%3050, %9) # torch/nn/functional.py:1991:17
      %3052 : int = aten::len(%3050) # torch/nn/functional.py:1992:19
      %3053 : int = aten::sub(%3052, %11) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%3053, %10, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %3057 : int = aten::add(%i.6, %11) # torch/nn/functional.py:1993:27
          %3058 : int = aten::__getitem__(%3050, %3057) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %3058) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.23)
      %3060 : bool = aten::eq(%size_prods.21, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.10 : Tensor = aten::batch_norm(%out.8, %3048, %3049, %3046, %3047, %3045, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.12 : Tensor = aten::relu_(%out.10) # torch/nn/functional.py:1117:17
  %3063 : __torch__.torch.nn.modules.conv.___torch_mangle_999.Conv2d = prim::GetAttr[name="conv3"](%2998)
  %3064 : Tensor = prim::GetAttr[name="weight"](%3063)
  %3065 : Tensor? = prim::GetAttr[name="bias"](%3063)
  %3066 : int[] = prim::ListConstruct(%13, %13)
  %3067 : int[] = prim::ListConstruct(%9, %9)
  %3068 : int[] = prim::ListConstruct(%13, %13)
  %out.14 : Tensor = aten::conv2d(%out.12, %3064, %3065, %3066, %3067, %3068, %13) # torch/nn/modules/conv.py:415:15
  %3070 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%2998)
  %3071 : int = aten::dim(%out.14) # torch/nn/modules/batchnorm.py:276:11
  %3072 : bool = aten::ne(%3071, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3072) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3073 : bool = prim::GetAttr[name="training"](%3070)
   = prim::If(%3073) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3074 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3070)
      %3075 : Tensor = aten::add(%3074, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3070, %3075)
      -> ()
    block1():
      -> ()
  %3076 : bool = prim::GetAttr[name="training"](%3070)
  %3077 : Tensor = prim::GetAttr[name="running_mean"](%3070)
  %3078 : Tensor = prim::GetAttr[name="running_var"](%3070)
  %3079 : Tensor = prim::GetAttr[name="weight"](%3070)
  %3080 : Tensor = prim::GetAttr[name="bias"](%3070)
   = prim::If(%3076) # torch/nn/functional.py:2011:4
    block0():
      %3081 : int[] = aten::size(%out.14) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%3081, %9) # torch/nn/functional.py:1991:17
      %3083 : int = aten::len(%3081) # torch/nn/functional.py:1992:19
      %3084 : int = aten::sub(%3083, %11) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%3084, %10, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %3088 : int = aten::add(%i.4, %11) # torch/nn/functional.py:1993:27
          %3089 : int = aten::__getitem__(%3081, %3088) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %3089) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.15)
      %3091 : bool = aten::eq(%size_prods.13, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3091) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.16 : Tensor = aten::batch_norm(%out.14, %3079, %3080, %3077, %3078, %3076, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %3093 : __torch__.torch.nn.modules.container.___torch_mangle_946.Sequential = prim::GetAttr[name="downsample"](%2998)
  %3094 : __torch__.torch.nn.modules.conv.___torch_mangle_945.Conv2d = prim::GetAttr[name="0"](%3093)
  %3095 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="1"](%3093)
  %3096 : Tensor = prim::GetAttr[name="weight"](%3094)
  %3097 : Tensor? = prim::GetAttr[name="bias"](%3094)
  %3098 : int[] = prim::ListConstruct(%11, %11)
  %3099 : int[] = prim::ListConstruct(%9, %9)
  %3100 : int[] = prim::ListConstruct(%13, %13)
  %input.3 : Tensor = aten::conv2d(%x.15, %3096, %3097, %3098, %3099, %3100, %13) # torch/nn/modules/conv.py:415:15
  %3102 : int = aten::dim(%input.3) # torch/nn/modules/batchnorm.py:276:11
  %3103 : bool = aten::ne(%3102, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3103) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3104 : bool = prim::GetAttr[name="training"](%3095)
   = prim::If(%3104) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3105 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3095)
      %3106 : Tensor = aten::add(%3105, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3095, %3106)
      -> ()
    block1():
      -> ()
  %3107 : bool = prim::GetAttr[name="training"](%3095)
  %3108 : Tensor = prim::GetAttr[name="running_mean"](%3095)
  %3109 : Tensor = prim::GetAttr[name="running_var"](%3095)
  %3110 : Tensor = prim::GetAttr[name="weight"](%3095)
  %3111 : Tensor = prim::GetAttr[name="bias"](%3095)
   = prim::If(%3107) # torch/nn/functional.py:2011:4
    block0():
      %3112 : int[] = aten::size(%input.3) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%3112, %9) # torch/nn/functional.py:1991:17
      %3114 : int = aten::len(%3112) # torch/nn/functional.py:1992:19
      %3115 : int = aten::sub(%3114, %11) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%3115, %10, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %3119 : int = aten::add(%i.7, %11) # torch/nn/functional.py:1993:27
          %3120 : int = aten::__getitem__(%3112, %3119) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %3120) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.27)
      %3122 : bool = aten::eq(%size_prods.25, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3122) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.1 : Tensor = aten::batch_norm(%input.3, %3110, %3111, %3108, %3109, %3107, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.18 : Tensor = aten::add_(%out.16, %identity.1, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.4 : Tensor = aten::relu_(%out.18) # torch/nn/functional.py:1117:17
  %3126 : __torch__.torch.nn.modules.conv.___torch_mangle_999.Conv2d = prim::GetAttr[name="conv1"](%2999)
  %3127 : Tensor = prim::GetAttr[name="weight"](%3126)
  %3128 : Tensor? = prim::GetAttr[name="bias"](%3126)
  %3129 : int[] = prim::ListConstruct(%13, %13)
  %3130 : int[] = prim::ListConstruct(%9, %9)
  %3131 : int[] = prim::ListConstruct(%13, %13)
  %out.28 : Tensor = aten::conv2d(%input.4, %3127, %3128, %3129, %3130, %3131, %13) # torch/nn/modules/conv.py:415:15
  %3133 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn1"](%2999)
  %3134 : int = aten::dim(%out.28) # torch/nn/modules/batchnorm.py:276:11
  %3135 : bool = aten::ne(%3134, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3135) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3136 : bool = prim::GetAttr[name="training"](%3133)
   = prim::If(%3136) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3137 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3133)
      %3138 : Tensor = aten::add(%3137, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3133, %3138)
      -> ()
    block1():
      -> ()
  %3139 : bool = prim::GetAttr[name="training"](%3133)
  %3140 : Tensor = prim::GetAttr[name="running_mean"](%3133)
  %3141 : Tensor = prim::GetAttr[name="running_var"](%3133)
  %3142 : Tensor = prim::GetAttr[name="weight"](%3133)
  %3143 : Tensor = prim::GetAttr[name="bias"](%3133)
   = prim::If(%3139) # torch/nn/functional.py:2011:4
    block0():
      %3144 : int[] = aten::size(%out.28) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%3144, %9) # torch/nn/functional.py:1991:17
      %3146 : int = aten::len(%3144) # torch/nn/functional.py:1992:19
      %3147 : int = aten::sub(%3146, %11) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%3147, %10, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %3151 : int = aten::add(%i.8, %11) # torch/nn/functional.py:1993:27
          %3152 : int = aten::__getitem__(%3144, %3151) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %3152) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.31)
      %3154 : bool = aten::eq(%size_prods.29, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3154) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.20 : Tensor = aten::batch_norm(%out.28, %3142, %3143, %3140, %3141, %3139, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.21 : Tensor = aten::relu_(%out.20) # torch/nn/functional.py:1117:17
  %3157 : __torch__.torch.nn.modules.conv.___torch_mangle_1001.Conv2d = prim::GetAttr[name="conv2"](%2999)
  %3158 : Tensor = prim::GetAttr[name="weight"](%3157)
  %3159 : Tensor? = prim::GetAttr[name="bias"](%3157)
  %3160 : int[] = prim::ListConstruct(%13, %13)
  %3161 : int[] = prim::ListConstruct(%13, %13)
  %3162 : int[] = prim::ListConstruct(%13, %13)
  %out.22 : Tensor = aten::conv2d(%out.21, %3158, %3159, %3160, %3161, %3162, %3) # torch/nn/modules/conv.py:415:15
  %3164 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn2"](%2999)
  %3165 : int = aten::dim(%out.22) # torch/nn/modules/batchnorm.py:276:11
  %3166 : bool = aten::ne(%3165, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3166) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3167 : bool = prim::GetAttr[name="training"](%3164)
   = prim::If(%3167) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3168 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3164)
      %3169 : Tensor = aten::add(%3168, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3164, %3169)
      -> ()
    block1():
      -> ()
  %3170 : bool = prim::GetAttr[name="training"](%3164)
  %3171 : Tensor = prim::GetAttr[name="running_mean"](%3164)
  %3172 : Tensor = prim::GetAttr[name="running_var"](%3164)
  %3173 : Tensor = prim::GetAttr[name="weight"](%3164)
  %3174 : Tensor = prim::GetAttr[name="bias"](%3164)
   = prim::If(%3170) # torch/nn/functional.py:2011:4
    block0():
      %3175 : int[] = aten::size(%out.22) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%3175, %9) # torch/nn/functional.py:1991:17
      %3177 : int = aten::len(%3175) # torch/nn/functional.py:1992:19
      %3178 : int = aten::sub(%3177, %11) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%3178, %10, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %3182 : int = aten::add(%i.9, %11) # torch/nn/functional.py:1993:27
          %3183 : int = aten::__getitem__(%3175, %3182) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %3183) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.35)
      %3185 : bool = aten::eq(%size_prods.33, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3185) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.23 : Tensor = aten::batch_norm(%out.22, %3173, %3174, %3171, %3172, %3170, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.24 : Tensor = aten::relu_(%out.23) # torch/nn/functional.py:1117:17
  %3188 : __torch__.torch.nn.modules.conv.___torch_mangle_999.Conv2d = prim::GetAttr[name="conv3"](%2999)
  %3189 : Tensor = prim::GetAttr[name="weight"](%3188)
  %3190 : Tensor? = prim::GetAttr[name="bias"](%3188)
  %3191 : int[] = prim::ListConstruct(%13, %13)
  %3192 : int[] = prim::ListConstruct(%9, %9)
  %3193 : int[] = prim::ListConstruct(%13, %13)
  %out.25 : Tensor = aten::conv2d(%out.24, %3189, %3190, %3191, %3192, %3193, %13) # torch/nn/modules/conv.py:415:15
  %3195 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%2999)
  %3196 : int = aten::dim(%out.25) # torch/nn/modules/batchnorm.py:276:11
  %3197 : bool = aten::ne(%3196, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3197) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3198 : bool = prim::GetAttr[name="training"](%3195)
   = prim::If(%3198) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3199 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3195)
      %3200 : Tensor = aten::add(%3199, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3195, %3200)
      -> ()
    block1():
      -> ()
  %3201 : bool = prim::GetAttr[name="training"](%3195)
  %3202 : Tensor = prim::GetAttr[name="running_mean"](%3195)
  %3203 : Tensor = prim::GetAttr[name="running_var"](%3195)
  %3204 : Tensor = prim::GetAttr[name="weight"](%3195)
  %3205 : Tensor = prim::GetAttr[name="bias"](%3195)
   = prim::If(%3201) # torch/nn/functional.py:2011:4
    block0():
      %3206 : int[] = aten::size(%out.25) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%3206, %9) # torch/nn/functional.py:1991:17
      %3208 : int = aten::len(%3206) # torch/nn/functional.py:1992:19
      %3209 : int = aten::sub(%3208, %11) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%3209, %10, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %3213 : int = aten::add(%i.10, %11) # torch/nn/functional.py:1993:27
          %3214 : int = aten::__getitem__(%3206, %3213) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %3214) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.39)
      %3216 : bool = aten::eq(%size_prods.37, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3216) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.26 : Tensor = aten::batch_norm(%out.25, %3204, %3205, %3202, %3203, %3201, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.27 : Tensor = aten::add_(%out.26, %input.4, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.5 : Tensor = aten::relu_(%out.27) # torch/nn/functional.py:1117:17
  %3220 : __torch__.torch.nn.modules.conv.___torch_mangle_999.Conv2d = prim::GetAttr[name="conv1"](%3000)
  %3221 : Tensor = prim::GetAttr[name="weight"](%3220)
  %3222 : Tensor? = prim::GetAttr[name="bias"](%3220)
  %3223 : int[] = prim::ListConstruct(%13, %13)
  %3224 : int[] = prim::ListConstruct(%9, %9)
  %3225 : int[] = prim::ListConstruct(%13, %13)
  %out.1 : Tensor = aten::conv2d(%input.5, %3221, %3222, %3223, %3224, %3225, %13) # torch/nn/modules/conv.py:415:15
  %3227 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn1"](%3000)
  %3228 : int = aten::dim(%out.1) # torch/nn/modules/batchnorm.py:276:11
  %3229 : bool = aten::ne(%3228, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3229) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3230 : bool = prim::GetAttr[name="training"](%3227)
   = prim::If(%3230) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3231 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3227)
      %3232 : Tensor = aten::add(%3231, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3227, %3232)
      -> ()
    block1():
      -> ()
  %3233 : bool = prim::GetAttr[name="training"](%3227)
  %3234 : Tensor = prim::GetAttr[name="running_mean"](%3227)
  %3235 : Tensor = prim::GetAttr[name="running_var"](%3227)
  %3236 : Tensor = prim::GetAttr[name="weight"](%3227)
  %3237 : Tensor = prim::GetAttr[name="bias"](%3227)
   = prim::If(%3233) # torch/nn/functional.py:2011:4
    block0():
      %3238 : int[] = aten::size(%out.1) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%3238, %9) # torch/nn/functional.py:1991:17
      %3240 : int = aten::len(%3238) # torch/nn/functional.py:1992:19
      %3241 : int = aten::sub(%3240, %11) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%3241, %10, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.7 : int):
          %3245 : int = aten::add(%i.2, %11) # torch/nn/functional.py:1993:27
          %3246 : int = aten::__getitem__(%3238, %3245) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %3246) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.5)
      %3248 : bool = aten::eq(%size_prods.4, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3248) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.3 : Tensor = aten::batch_norm(%out.1, %3236, %3237, %3234, %3235, %3233, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.5 : Tensor = aten::relu_(%out.3) # torch/nn/functional.py:1117:17
  %3251 : __torch__.torch.nn.modules.conv.___torch_mangle_1001.Conv2d = prim::GetAttr[name="conv2"](%3000)
  %3252 : Tensor = prim::GetAttr[name="weight"](%3251)
  %3253 : Tensor? = prim::GetAttr[name="bias"](%3251)
  %3254 : int[] = prim::ListConstruct(%13, %13)
  %3255 : int[] = prim::ListConstruct(%13, %13)
  %3256 : int[] = prim::ListConstruct(%13, %13)
  %out.7 : Tensor = aten::conv2d(%out.5, %3252, %3253, %3254, %3255, %3256, %3) # torch/nn/modules/conv.py:415:15
  %3258 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn2"](%3000)
  %3259 : int = aten::dim(%out.7) # torch/nn/modules/batchnorm.py:276:11
  %3260 : bool = aten::ne(%3259, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3260) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3261 : bool = prim::GetAttr[name="training"](%3258)
   = prim::If(%3261) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3262 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3258)
      %3263 : Tensor = aten::add(%3262, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3258, %3263)
      -> ()
    block1():
      -> ()
  %3264 : bool = prim::GetAttr[name="training"](%3258)
  %3265 : Tensor = prim::GetAttr[name="running_mean"](%3258)
  %3266 : Tensor = prim::GetAttr[name="running_var"](%3258)
  %3267 : Tensor = prim::GetAttr[name="weight"](%3258)
  %3268 : Tensor = prim::GetAttr[name="bias"](%3258)
   = prim::If(%3264) # torch/nn/functional.py:2011:4
    block0():
      %3269 : int[] = aten::size(%out.7) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%3269, %9) # torch/nn/functional.py:1991:17
      %3271 : int = aten::len(%3269) # torch/nn/functional.py:1992:19
      %3272 : int = aten::sub(%3271, %11) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%3272, %10, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %3276 : int = aten::add(%i.3, %11) # torch/nn/functional.py:1993:27
          %3277 : int = aten::__getitem__(%3269, %3276) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %3277) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.11)
      %3279 : bool = aten::eq(%size_prods.9, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3279) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.9 : Tensor = aten::batch_norm(%out.7, %3267, %3268, %3265, %3266, %3264, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.11 : Tensor = aten::relu_(%out.9) # torch/nn/functional.py:1117:17
  %3282 : __torch__.torch.nn.modules.conv.___torch_mangle_999.Conv2d = prim::GetAttr[name="conv3"](%3000)
  %3283 : Tensor = prim::GetAttr[name="weight"](%3282)
  %3284 : Tensor? = prim::GetAttr[name="bias"](%3282)
  %3285 : int[] = prim::ListConstruct(%13, %13)
  %3286 : int[] = prim::ListConstruct(%9, %9)
  %3287 : int[] = prim::ListConstruct(%13, %13)
  %out.13 : Tensor = aten::conv2d(%out.11, %3283, %3284, %3285, %3286, %3287, %13) # torch/nn/modules/conv.py:415:15
  %3289 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%3000)
  %3290 : int = aten::dim(%out.13) # torch/nn/modules/batchnorm.py:276:11
  %3291 : bool = aten::ne(%3290, %7) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3291) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%8) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3292 : bool = prim::GetAttr[name="training"](%3289)
   = prim::If(%3292) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3293 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3289)
      %3294 : Tensor = aten::add(%3293, %13, %13) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3289, %3294)
      -> ()
    block1():
      -> ()
  %3295 : bool = prim::GetAttr[name="training"](%3289)
  %3296 : Tensor = prim::GetAttr[name="running_mean"](%3289)
  %3297 : Tensor = prim::GetAttr[name="running_var"](%3289)
  %3298 : Tensor = prim::GetAttr[name="weight"](%3289)
  %3299 : Tensor = prim::GetAttr[name="bias"](%3289)
   = prim::If(%3295) # torch/nn/functional.py:2011:4
    block0():
      %3300 : int[] = aten::size(%out.13) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3300, %9) # torch/nn/functional.py:1991:17
      %3302 : int = aten::len(%3300) # torch/nn/functional.py:1992:19
      %3303 : int = aten::sub(%3302, %11) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3303, %10, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %3307 : int = aten::add(%i.1, %11) # torch/nn/functional.py:1993:27
          %3308 : int = aten::__getitem__(%3300, %3307) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3308) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.3)
      %3310 : bool = aten::eq(%size_prods, %13) # torch/nn/functional.py:1994:7
       = prim::If(%3310) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%8) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.15 : Tensor = aten::batch_norm(%out.13, %3298, %3299, %3296, %3297, %3295, %exponential_average_factor.1, %5, %10) # torch/nn/functional.py:2014:11
  %out.17 : Tensor = aten::add_(%out.15, %input.5, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.17 : Tensor = aten::relu_(%out.17) # torch/nn/functional.py:1117:17
  %3314 : int[] = prim::ListConstruct(%13, %13)
  %3315 : int[] = aten::size(%x.17) # torch/nn/functional.py:925:51
  %3316 : int = aten::len(%3315) # <string>:5:9
  %3317 : bool = aten::gt(%3316, %11) # <string>:5:9
   = prim::If(%3317) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%8) # <string>:5:2
      -> ()
  %x.19 : Tensor = aten::adaptive_avg_pool2d(%x.17, %3314) # torch/nn/functional.py:926:11
  %x.21 : Tensor = aten::flatten(%x.19, %13, %14) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:12
  %3320 : __torch__.torch.nn.modules.linear.___torch_mangle_621.Linear = prim::GetAttr[name="fc"](%self)
  %3321 : Tensor = prim::GetAttr[name="weight"](%3320)
  %3322 : Tensor = prim::GetAttr[name="bias"](%3320)
  %3323 : int = aten::dim(%x.21) # torch/nn/functional.py:1672:7
  %3324 : bool = aten::eq(%3323, %11) # torch/nn/functional.py:1672:7
  %x.23 : Tensor = prim::If(%3324) # torch/nn/functional.py:1672:4
    block0():
      %3326 : Tensor = aten::t(%3321) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%3322, %x.21, %3326, %13, %13) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %3328 : Tensor = aten::t(%3321) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%x.21, %3328) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %3322, %13) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%x.23)
