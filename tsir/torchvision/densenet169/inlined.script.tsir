graph(%self : __torch__.torchvision.models.densenet.___torch_mangle_358.DenseNet,
      %x.1 : Tensor):
  %2 : int = prim::Constant[value=-1]()
  %3 : Function = prim::Constant[name="adaptive_avg_pool2d"]()
  %4 : Function = prim::Constant[name="relu"]()
  %5 : bool = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:193:39
  %6 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:194:42
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_356.Sequential = prim::GetAttr[name="features"](%self)
  %15 : None = prim::Constant() # torch/nn/modules/pooling.py:599:82
  %16 : float = prim::Constant[value=0.]() # torch/nn/functional.py:968:11
  %17 : float = prim::Constant[value=1.]() # torch/nn/functional.py:968:21
  %18 : int = prim::Constant[value=9223372036854775807]()
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %29 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv0"](%7)
  %30 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm0"](%7)
  %31 : __torch__.torchvision.models.densenet._DenseBlock = prim::GetAttr[name="denseblock1"](%7)
  %32 : __torch__.torchvision.models.densenet._Transition = prim::GetAttr[name="transition1"](%7)
  %33 : __torch__.torchvision.models.densenet.___torch_mangle_109._DenseBlock = prim::GetAttr[name="denseblock2"](%7)
  %34 : __torch__.torchvision.models.densenet.___torch_mangle_110._Transition = prim::GetAttr[name="transition2"](%7)
  %35 : __torch__.torchvision.models.densenet.___torch_mangle_319._DenseBlock = prim::GetAttr[name="denseblock3"](%7)
  %36 : __torch__.torchvision.models.densenet.___torch_mangle_322._Transition = prim::GetAttr[name="transition3"](%7)
  %37 : __torch__.torchvision.models.densenet.___torch_mangle_354._DenseBlock = prim::GetAttr[name="denseblock4"](%7)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_355.BatchNorm2d = prim::GetAttr[name="norm5"](%7)
  %39 : Tensor = prim::GetAttr[name="weight"](%29)
  %40 : Tensor? = prim::GetAttr[name="bias"](%29)
  %41 : int[] = prim::ListConstruct(%26, %26)
  %42 : int[] = prim::ListConstruct(%28, %28)
  %43 : int[] = prim::ListConstruct(%27, %27)
  %input.4 : Tensor = aten::conv2d(%x.1, %39, %40, %41, %42, %43, %27) # torch/nn/modules/conv.py:415:15
  %45 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %46 : bool = aten::ne(%45, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%46) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %47 : bool = prim::GetAttr[name="training"](%30)
   = prim::If(%47) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %48 : Tensor = prim::GetAttr[name="num_batches_tracked"](%30)
      %49 : Tensor = aten::add(%48, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%30, %49)
      -> ()
    block1():
      -> ()
  %50 : bool = prim::GetAttr[name="training"](%30)
  %51 : Tensor = prim::GetAttr[name="running_mean"](%30)
  %52 : Tensor = prim::GetAttr[name="running_var"](%30)
  %53 : Tensor = prim::GetAttr[name="weight"](%30)
  %54 : Tensor = prim::GetAttr[name="bias"](%30)
   = prim::If(%50) # torch/nn/functional.py:2011:4
    block0():
      %55 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%55, %24) # torch/nn/functional.py:1991:17
      %57 : int = aten::len(%55) # torch/nn/functional.py:1992:19
      %58 : int = aten::sub(%57, %26) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%58, %25, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %62 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
          %63 : int = aten::__getitem__(%55, %62) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %63) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.395)
      %65 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
       = prim::If(%65) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %53, %54, %51, %52, %50, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu_(%input.6) # torch/nn/functional.py:1117:17
  %68 : int[] = prim::ListConstruct(%28, %28)
  %69 : int[] = prim::ListConstruct(%26, %26)
  %70 : int[] = prim::ListConstruct(%27, %27)
  %71 : int[] = prim::ListConstruct(%27, %27)
  %input.10 : Tensor = aten::max_pool2d(%input.8, %68, %69, %70, %71, %19) # torch/nn/functional.py:575:11
  %features.2 : Tensor[] = prim::ListConstruct(%input.10)
  %74 : __torch__.torchvision.models.densenet._DenseLayer = prim::GetAttr[name="denselayer1"](%31)
  %75 : __torch__.torchvision.models.densenet.___torch_mangle_75._DenseLayer = prim::GetAttr[name="denselayer2"](%31)
  %76 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer3"](%31)
  %77 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer4"](%31)
  %78 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer5"](%31)
  %79 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer6"](%31)
  %80 : Tensor = prim::Uninitialized()
  %81 : bool = prim::GetAttr[name="memory_efficient"](%74)
  %82 : bool = prim::If(%81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %83 : bool = prim::Uninitialized()
      %84 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %85 : bool = aten::gt(%84, %24)
      %86 : bool, %87 : bool, %88 : int = prim::Loop(%18, %85, %19, %83, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%89 : int, %90 : bool, %91 : bool, %92 : int):
          %tensor.34 : Tensor = aten::__getitem__(%features.2, %92) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %94 : bool = prim::requires_grad(%tensor.34)
          %95 : bool, %96 : bool = prim::If(%94) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %83)
          %97 : int = aten::add(%92, %27)
          %98 : bool = aten::lt(%97, %84)
          %99 : bool = aten::__and__(%98, %95)
          -> (%99, %94, %96, %97)
      %100 : bool = prim::If(%86)
        block0():
          -> (%87)
        block1():
          -> (%19)
      -> (%100)
    block1():
      -> (%19)
  %bottleneck_output.66 : Tensor = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%80)
    block1():
      %concated_features.34 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %103 : __torch__.torch.nn.modules.conv.___torch_mangle_71.Conv2d = prim::GetAttr[name="conv1"](%74)
      %104 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="norm1"](%74)
      %105 : int = aten::dim(%concated_features.34) # torch/nn/modules/batchnorm.py:276:11
      %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %107 : bool = prim::GetAttr[name="training"](%104)
       = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
          %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%104, %109)
          -> ()
        block1():
          -> ()
      %110 : bool = prim::GetAttr[name="training"](%104)
      %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
      %112 : Tensor = prim::GetAttr[name="running_var"](%104)
      %113 : Tensor = prim::GetAttr[name="weight"](%104)
      %114 : Tensor = prim::GetAttr[name="bias"](%104)
       = prim::If(%110) # torch/nn/functional.py:2011:4
        block0():
          %115 : int[] = aten::size(%concated_features.34) # torch/nn/functional.py:2012:27
          %size_prods.400 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
          %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
          %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
          %size_prods.401 : int = prim::Loop(%118, %25, %size_prods.400) # torch/nn/functional.py:1992:4
            block0(%i.101 : int, %size_prods.402 : int):
              %122 : int = aten::add(%i.101, %26) # torch/nn/functional.py:1993:27
              %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
              %size_prods.403 : int = aten::mul(%size_prods.402, %123) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.403)
          %125 : bool = aten::eq(%size_prods.401, %27) # torch/nn/functional.py:1994:7
           = prim::If(%125) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %126 : Tensor = aten::batch_norm(%concated_features.34, %113, %114, %111, %112, %110, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.67 : Tensor = aten::relu_(%126) # torch/nn/functional.py:1117:17
      %128 : Tensor = prim::GetAttr[name="weight"](%103)
      %129 : Tensor? = prim::GetAttr[name="bias"](%103)
      %130 : int[] = prim::ListConstruct(%27, %27)
      %131 : int[] = prim::ListConstruct(%24, %24)
      %132 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.67 : Tensor = aten::conv2d(%result.67, %128, %129, %130, %131, %132, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.67)
  %134 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%74)
  %135 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%74)
  %136 : int = aten::dim(%bottleneck_output.66) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%bottleneck_output.66) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%149, %25, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %153 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.259)
      %156 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %157 : Tensor = aten::batch_norm(%bottleneck_output.66, %144, %145, %142, %143, %141, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.68 : Tensor = aten::relu_(%157) # torch/nn/functional.py:1117:17
  %159 : Tensor = prim::GetAttr[name="weight"](%134)
  %160 : Tensor? = prim::GetAttr[name="bias"](%134)
  %161 : int[] = prim::ListConstruct(%27, %27)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%27, %27)
  %new_features.96 : Tensor = aten::conv2d(%result.68, %159, %160, %161, %162, %163, %27) # torch/nn/modules/conv.py:415:15
  %165 : float = prim::GetAttr[name="drop_rate"](%74)
  %166 : bool = aten::gt(%165, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.69 : Tensor = prim::If(%166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %168 : float = prim::GetAttr[name="drop_rate"](%74)
      %169 : bool = prim::GetAttr[name="training"](%74)
      %170 : bool = aten::lt(%168, %16) # torch/nn/functional.py:968:7
      %171 : bool = prim::If(%170) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %172 : bool = aten::gt(%168, %17) # torch/nn/functional.py:968:17
          -> (%172)
       = prim::If(%171) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %173 : Tensor = aten::dropout(%new_features.96, %168, %169) # torch/nn/functional.py:973:17
      -> (%173)
    block1():
      -> (%new_features.96)
  %174 : Tensor[] = aten::append(%features.2, %new_features.69) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %175 : Tensor = prim::Uninitialized()
  %176 : bool = prim::GetAttr[name="memory_efficient"](%75)
  %177 : bool = prim::If(%176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %178 : bool = prim::Uninitialized()
      %179 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %180 : bool = aten::gt(%179, %24)
      %181 : bool, %182 : bool, %183 : int = prim::Loop(%18, %180, %19, %178, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%184 : int, %185 : bool, %186 : bool, %187 : int):
          %tensor.47 : Tensor = aten::__getitem__(%features.2, %187) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %189 : bool = prim::requires_grad(%tensor.47)
          %190 : bool, %191 : bool = prim::If(%189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %178)
          %192 : int = aten::add(%187, %27)
          %193 : bool = aten::lt(%192, %179)
          %194 : bool = aten::__and__(%193, %190)
          -> (%194, %189, %191, %192)
      %195 : bool = prim::If(%181)
        block0():
          -> (%182)
        block1():
          -> (%19)
      -> (%195)
    block1():
      -> (%19)
  %bottleneck_output.92 : Tensor = prim::If(%177) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%175)
    block1():
      %concated_features.47 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %198 : __torch__.torch.nn.modules.conv.___torch_mangle_74.Conv2d = prim::GetAttr[name="conv1"](%75)
      %199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="norm1"](%75)
      %200 : int = aten::dim(%concated_features.47) # torch/nn/modules/batchnorm.py:276:11
      %201 : bool = aten::ne(%200, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%201) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %202 : bool = prim::GetAttr[name="training"](%199)
       = prim::If(%202) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %203 : Tensor = prim::GetAttr[name="num_batches_tracked"](%199)
          %204 : Tensor = aten::add(%203, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%199, %204)
          -> ()
        block1():
          -> ()
      %205 : bool = prim::GetAttr[name="training"](%199)
      %206 : Tensor = prim::GetAttr[name="running_mean"](%199)
      %207 : Tensor = prim::GetAttr[name="running_var"](%199)
      %208 : Tensor = prim::GetAttr[name="weight"](%199)
      %209 : Tensor = prim::GetAttr[name="bias"](%199)
       = prim::If(%205) # torch/nn/functional.py:2011:4
        block0():
          %210 : int[] = aten::size(%concated_features.47) # torch/nn/functional.py:2012:27
          %size_prods.260 : int = aten::__getitem__(%210, %24) # torch/nn/functional.py:1991:17
          %212 : int = aten::len(%210) # torch/nn/functional.py:1992:19
          %213 : int = aten::sub(%212, %26) # torch/nn/functional.py:1992:19
          %size_prods.261 : int = prim::Loop(%213, %25, %size_prods.260) # torch/nn/functional.py:1992:4
            block0(%i.66 : int, %size_prods.262 : int):
              %217 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
              %218 : int = aten::__getitem__(%210, %217) # torch/nn/functional.py:1993:22
              %size_prods.263 : int = aten::mul(%size_prods.262, %218) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.263)
          %220 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
           = prim::If(%220) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %221 : Tensor = aten::batch_norm(%concated_features.47, %208, %209, %206, %207, %205, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.93 : Tensor = aten::relu_(%221) # torch/nn/functional.py:1117:17
      %223 : Tensor = prim::GetAttr[name="weight"](%198)
      %224 : Tensor? = prim::GetAttr[name="bias"](%198)
      %225 : int[] = prim::ListConstruct(%27, %27)
      %226 : int[] = prim::ListConstruct(%24, %24)
      %227 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.93 : Tensor = aten::conv2d(%result.93, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.93)
  %229 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%75)
  %230 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%75)
  %231 : int = aten::dim(%bottleneck_output.92) # torch/nn/modules/batchnorm.py:276:11
  %232 : bool = aten::ne(%231, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%232) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %233 : bool = prim::GetAttr[name="training"](%230)
   = prim::If(%233) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %234 : Tensor = prim::GetAttr[name="num_batches_tracked"](%230)
      %235 : Tensor = aten::add(%234, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%230, %235)
      -> ()
    block1():
      -> ()
  %236 : bool = prim::GetAttr[name="training"](%230)
  %237 : Tensor = prim::GetAttr[name="running_mean"](%230)
  %238 : Tensor = prim::GetAttr[name="running_var"](%230)
  %239 : Tensor = prim::GetAttr[name="weight"](%230)
  %240 : Tensor = prim::GetAttr[name="bias"](%230)
   = prim::If(%236) # torch/nn/functional.py:2011:4
    block0():
      %241 : int[] = aten::size(%bottleneck_output.92) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%241, %24) # torch/nn/functional.py:1991:17
      %243 : int = aten::len(%241) # torch/nn/functional.py:1992:19
      %244 : int = aten::sub(%243, %26) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%244, %25, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %248 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
          %249 : int = aten::__getitem__(%241, %248) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %249) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.267)
      %251 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
       = prim::If(%251) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %252 : Tensor = aten::batch_norm(%bottleneck_output.92, %239, %240, %237, %238, %236, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.94 : Tensor = aten::relu_(%252) # torch/nn/functional.py:1117:17
  %254 : Tensor = prim::GetAttr[name="weight"](%229)
  %255 : Tensor? = prim::GetAttr[name="bias"](%229)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %new_features.98 : Tensor = aten::conv2d(%result.94, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : float = prim::GetAttr[name="drop_rate"](%75)
  %261 : bool = aten::gt(%260, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.95 : Tensor = prim::If(%261) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %263 : float = prim::GetAttr[name="drop_rate"](%75)
      %264 : bool = prim::GetAttr[name="training"](%75)
      %265 : bool = aten::lt(%263, %16) # torch/nn/functional.py:968:7
      %266 : bool = prim::If(%265) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %267 : bool = aten::gt(%263, %17) # torch/nn/functional.py:968:17
          -> (%267)
       = prim::If(%266) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %268 : Tensor = aten::dropout(%new_features.98, %263, %264) # torch/nn/functional.py:973:17
      -> (%268)
    block1():
      -> (%new_features.98)
  %269 : Tensor[] = aten::append(%features.2, %new_features.95) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %270 : Tensor = prim::Uninitialized()
  %271 : bool = prim::GetAttr[name="memory_efficient"](%76)
  %272 : bool = prim::If(%271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %273 : bool = prim::Uninitialized()
      %274 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %275 : bool = aten::gt(%274, %24)
      %276 : bool, %277 : bool, %278 : int = prim::Loop(%18, %275, %19, %273, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%279 : int, %280 : bool, %281 : bool, %282 : int):
          %tensor.48 : Tensor = aten::__getitem__(%features.2, %282) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %284 : bool = prim::requires_grad(%tensor.48)
          %285 : bool, %286 : bool = prim::If(%284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %273)
          %287 : int = aten::add(%282, %27)
          %288 : bool = aten::lt(%287, %274)
          %289 : bool = aten::__and__(%288, %285)
          -> (%289, %284, %286, %287)
      %290 : bool = prim::If(%276)
        block0():
          -> (%277)
        block1():
          -> (%19)
      -> (%290)
    block1():
      -> (%19)
  %bottleneck_output.94 : Tensor = prim::If(%272) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%270)
    block1():
      %concated_features.48 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %293 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%76)
      %294 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%76)
      %295 : int = aten::dim(%concated_features.48) # torch/nn/modules/batchnorm.py:276:11
      %296 : bool = aten::ne(%295, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%296) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %297 : bool = prim::GetAttr[name="training"](%294)
       = prim::If(%297) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%294)
          %299 : Tensor = aten::add(%298, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%294, %299)
          -> ()
        block1():
          -> ()
      %300 : bool = prim::GetAttr[name="training"](%294)
      %301 : Tensor = prim::GetAttr[name="running_mean"](%294)
      %302 : Tensor = prim::GetAttr[name="running_var"](%294)
      %303 : Tensor = prim::GetAttr[name="weight"](%294)
      %304 : Tensor = prim::GetAttr[name="bias"](%294)
       = prim::If(%300) # torch/nn/functional.py:2011:4
        block0():
          %305 : int[] = aten::size(%concated_features.48) # torch/nn/functional.py:2012:27
          %size_prods.268 : int = aten::__getitem__(%305, %24) # torch/nn/functional.py:1991:17
          %307 : int = aten::len(%305) # torch/nn/functional.py:1992:19
          %308 : int = aten::sub(%307, %26) # torch/nn/functional.py:1992:19
          %size_prods.269 : int = prim::Loop(%308, %25, %size_prods.268) # torch/nn/functional.py:1992:4
            block0(%i.68 : int, %size_prods.270 : int):
              %312 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
              %313 : int = aten::__getitem__(%305, %312) # torch/nn/functional.py:1993:22
              %size_prods.271 : int = aten::mul(%size_prods.270, %313) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.271)
          %315 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
           = prim::If(%315) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %316 : Tensor = aten::batch_norm(%concated_features.48, %303, %304, %301, %302, %300, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.95 : Tensor = aten::relu_(%316) # torch/nn/functional.py:1117:17
      %318 : Tensor = prim::GetAttr[name="weight"](%293)
      %319 : Tensor? = prim::GetAttr[name="bias"](%293)
      %320 : int[] = prim::ListConstruct(%27, %27)
      %321 : int[] = prim::ListConstruct(%24, %24)
      %322 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.95 : Tensor = aten::conv2d(%result.95, %318, %319, %320, %321, %322, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.95)
  %324 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%76)
  %325 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%76)
  %326 : int = aten::dim(%bottleneck_output.94) # torch/nn/modules/batchnorm.py:276:11
  %327 : bool = aten::ne(%326, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%327) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %328 : bool = prim::GetAttr[name="training"](%325)
   = prim::If(%328) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %329 : Tensor = prim::GetAttr[name="num_batches_tracked"](%325)
      %330 : Tensor = aten::add(%329, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%325, %330)
      -> ()
    block1():
      -> ()
  %331 : bool = prim::GetAttr[name="training"](%325)
  %332 : Tensor = prim::GetAttr[name="running_mean"](%325)
  %333 : Tensor = prim::GetAttr[name="running_var"](%325)
  %334 : Tensor = prim::GetAttr[name="weight"](%325)
  %335 : Tensor = prim::GetAttr[name="bias"](%325)
   = prim::If(%331) # torch/nn/functional.py:2011:4
    block0():
      %336 : int[] = aten::size(%bottleneck_output.94) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%336, %24) # torch/nn/functional.py:1991:17
      %338 : int = aten::len(%336) # torch/nn/functional.py:1992:19
      %339 : int = aten::sub(%338, %26) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%339, %25, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %343 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
          %344 : int = aten::__getitem__(%336, %343) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %344) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.275)
      %346 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
       = prim::If(%346) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %347 : Tensor = aten::batch_norm(%bottleneck_output.94, %334, %335, %332, %333, %331, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.96 : Tensor = aten::relu_(%347) # torch/nn/functional.py:1117:17
  %349 : Tensor = prim::GetAttr[name="weight"](%324)
  %350 : Tensor? = prim::GetAttr[name="bias"](%324)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %353 : int[] = prim::ListConstruct(%27, %27)
  %new_features.100 : Tensor = aten::conv2d(%result.96, %349, %350, %351, %352, %353, %27) # torch/nn/modules/conv.py:415:15
  %355 : float = prim::GetAttr[name="drop_rate"](%76)
  %356 : bool = aten::gt(%355, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.97 : Tensor = prim::If(%356) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %358 : float = prim::GetAttr[name="drop_rate"](%76)
      %359 : bool = prim::GetAttr[name="training"](%76)
      %360 : bool = aten::lt(%358, %16) # torch/nn/functional.py:968:7
      %361 : bool = prim::If(%360) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %362 : bool = aten::gt(%358, %17) # torch/nn/functional.py:968:17
          -> (%362)
       = prim::If(%361) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %363 : Tensor = aten::dropout(%new_features.100, %358, %359) # torch/nn/functional.py:973:17
      -> (%363)
    block1():
      -> (%new_features.100)
  %364 : Tensor[] = aten::append(%features.2, %new_features.97) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %365 : Tensor = prim::Uninitialized()
  %366 : bool = prim::GetAttr[name="memory_efficient"](%77)
  %367 : bool = prim::If(%366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %368 : bool = prim::Uninitialized()
      %369 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %370 : bool = aten::gt(%369, %24)
      %371 : bool, %372 : bool, %373 : int = prim::Loop(%18, %370, %19, %368, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%374 : int, %375 : bool, %376 : bool, %377 : int):
          %tensor.49 : Tensor = aten::__getitem__(%features.2, %377) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %379 : bool = prim::requires_grad(%tensor.49)
          %380 : bool, %381 : bool = prim::If(%379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %368)
          %382 : int = aten::add(%377, %27)
          %383 : bool = aten::lt(%382, %369)
          %384 : bool = aten::__and__(%383, %380)
          -> (%384, %379, %381, %382)
      %385 : bool = prim::If(%371)
        block0():
          -> (%372)
        block1():
          -> (%19)
      -> (%385)
    block1():
      -> (%19)
  %bottleneck_output.96 : Tensor = prim::If(%367) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%365)
    block1():
      %concated_features.49 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %388 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%77)
      %389 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%77)
      %390 : int = aten::dim(%concated_features.49) # torch/nn/modules/batchnorm.py:276:11
      %391 : bool = aten::ne(%390, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%391) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %392 : bool = prim::GetAttr[name="training"](%389)
       = prim::If(%392) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %393 : Tensor = prim::GetAttr[name="num_batches_tracked"](%389)
          %394 : Tensor = aten::add(%393, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%389, %394)
          -> ()
        block1():
          -> ()
      %395 : bool = prim::GetAttr[name="training"](%389)
      %396 : Tensor = prim::GetAttr[name="running_mean"](%389)
      %397 : Tensor = prim::GetAttr[name="running_var"](%389)
      %398 : Tensor = prim::GetAttr[name="weight"](%389)
      %399 : Tensor = prim::GetAttr[name="bias"](%389)
       = prim::If(%395) # torch/nn/functional.py:2011:4
        block0():
          %400 : int[] = aten::size(%concated_features.49) # torch/nn/functional.py:2012:27
          %size_prods.276 : int = aten::__getitem__(%400, %24) # torch/nn/functional.py:1991:17
          %402 : int = aten::len(%400) # torch/nn/functional.py:1992:19
          %403 : int = aten::sub(%402, %26) # torch/nn/functional.py:1992:19
          %size_prods.277 : int = prim::Loop(%403, %25, %size_prods.276) # torch/nn/functional.py:1992:4
            block0(%i.70 : int, %size_prods.278 : int):
              %407 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
              %408 : int = aten::__getitem__(%400, %407) # torch/nn/functional.py:1993:22
              %size_prods.279 : int = aten::mul(%size_prods.278, %408) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.279)
          %410 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
           = prim::If(%410) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %411 : Tensor = aten::batch_norm(%concated_features.49, %398, %399, %396, %397, %395, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.97 : Tensor = aten::relu_(%411) # torch/nn/functional.py:1117:17
      %413 : Tensor = prim::GetAttr[name="weight"](%388)
      %414 : Tensor? = prim::GetAttr[name="bias"](%388)
      %415 : int[] = prim::ListConstruct(%27, %27)
      %416 : int[] = prim::ListConstruct(%24, %24)
      %417 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.97 : Tensor = aten::conv2d(%result.97, %413, %414, %415, %416, %417, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.97)
  %419 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%77)
  %420 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%77)
  %421 : int = aten::dim(%bottleneck_output.96) # torch/nn/modules/batchnorm.py:276:11
  %422 : bool = aten::ne(%421, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%422) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %423 : bool = prim::GetAttr[name="training"](%420)
   = prim::If(%423) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %424 : Tensor = prim::GetAttr[name="num_batches_tracked"](%420)
      %425 : Tensor = aten::add(%424, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%420, %425)
      -> ()
    block1():
      -> ()
  %426 : bool = prim::GetAttr[name="training"](%420)
  %427 : Tensor = prim::GetAttr[name="running_mean"](%420)
  %428 : Tensor = prim::GetAttr[name="running_var"](%420)
  %429 : Tensor = prim::GetAttr[name="weight"](%420)
  %430 : Tensor = prim::GetAttr[name="bias"](%420)
   = prim::If(%426) # torch/nn/functional.py:2011:4
    block0():
      %431 : int[] = aten::size(%bottleneck_output.96) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%431, %24) # torch/nn/functional.py:1991:17
      %433 : int = aten::len(%431) # torch/nn/functional.py:1992:19
      %434 : int = aten::sub(%433, %26) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%434, %25, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %438 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
          %439 : int = aten::__getitem__(%431, %438) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %439) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.283)
      %441 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
       = prim::If(%441) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %442 : Tensor = aten::batch_norm(%bottleneck_output.96, %429, %430, %427, %428, %426, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.98 : Tensor = aten::relu_(%442) # torch/nn/functional.py:1117:17
  %444 : Tensor = prim::GetAttr[name="weight"](%419)
  %445 : Tensor? = prim::GetAttr[name="bias"](%419)
  %446 : int[] = prim::ListConstruct(%27, %27)
  %447 : int[] = prim::ListConstruct(%27, %27)
  %448 : int[] = prim::ListConstruct(%27, %27)
  %new_features.65 : Tensor = aten::conv2d(%result.98, %444, %445, %446, %447, %448, %27) # torch/nn/modules/conv.py:415:15
  %450 : float = prim::GetAttr[name="drop_rate"](%77)
  %451 : bool = aten::gt(%450, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.66 : Tensor = prim::If(%451) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %453 : float = prim::GetAttr[name="drop_rate"](%77)
      %454 : bool = prim::GetAttr[name="training"](%77)
      %455 : bool = aten::lt(%453, %16) # torch/nn/functional.py:968:7
      %456 : bool = prim::If(%455) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %457 : bool = aten::gt(%453, %17) # torch/nn/functional.py:968:17
          -> (%457)
       = prim::If(%456) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %458 : Tensor = aten::dropout(%new_features.65, %453, %454) # torch/nn/functional.py:973:17
      -> (%458)
    block1():
      -> (%new_features.65)
  %459 : Tensor[] = aten::append(%features.2, %new_features.66) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %460 : Tensor = prim::Uninitialized()
  %461 : bool = prim::GetAttr[name="memory_efficient"](%78)
  %462 : bool = prim::If(%461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %463 : bool = prim::Uninitialized()
      %464 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %465 : bool = aten::gt(%464, %24)
      %466 : bool, %467 : bool, %468 : int = prim::Loop(%18, %465, %19, %463, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%469 : int, %470 : bool, %471 : bool, %472 : int):
          %tensor.33 : Tensor = aten::__getitem__(%features.2, %472) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %474 : bool = prim::requires_grad(%tensor.33)
          %475 : bool, %476 : bool = prim::If(%474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %463)
          %477 : int = aten::add(%472, %27)
          %478 : bool = aten::lt(%477, %464)
          %479 : bool = aten::__and__(%478, %475)
          -> (%479, %474, %476, %477)
      %480 : bool = prim::If(%466)
        block0():
          -> (%467)
        block1():
          -> (%19)
      -> (%480)
    block1():
      -> (%19)
  %bottleneck_output.64 : Tensor = prim::If(%462) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%460)
    block1():
      %concated_features.33 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %483 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%78)
      %484 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%78)
      %485 : int = aten::dim(%concated_features.33) # torch/nn/modules/batchnorm.py:276:11
      %486 : bool = aten::ne(%485, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%486) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %487 : bool = prim::GetAttr[name="training"](%484)
       = prim::If(%487) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %488 : Tensor = prim::GetAttr[name="num_batches_tracked"](%484)
          %489 : Tensor = aten::add(%488, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%484, %489)
          -> ()
        block1():
          -> ()
      %490 : bool = prim::GetAttr[name="training"](%484)
      %491 : Tensor = prim::GetAttr[name="running_mean"](%484)
      %492 : Tensor = prim::GetAttr[name="running_var"](%484)
      %493 : Tensor = prim::GetAttr[name="weight"](%484)
      %494 : Tensor = prim::GetAttr[name="bias"](%484)
       = prim::If(%490) # torch/nn/functional.py:2011:4
        block0():
          %495 : int[] = aten::size(%concated_features.33) # torch/nn/functional.py:2012:27
          %size_prods.284 : int = aten::__getitem__(%495, %24) # torch/nn/functional.py:1991:17
          %497 : int = aten::len(%495) # torch/nn/functional.py:1992:19
          %498 : int = aten::sub(%497, %26) # torch/nn/functional.py:1992:19
          %size_prods.285 : int = prim::Loop(%498, %25, %size_prods.284) # torch/nn/functional.py:1992:4
            block0(%i.72 : int, %size_prods.286 : int):
              %502 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
              %503 : int = aten::__getitem__(%495, %502) # torch/nn/functional.py:1993:22
              %size_prods.287 : int = aten::mul(%size_prods.286, %503) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.287)
          %505 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
           = prim::If(%505) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %506 : Tensor = aten::batch_norm(%concated_features.33, %493, %494, %491, %492, %490, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.65 : Tensor = aten::relu_(%506) # torch/nn/functional.py:1117:17
      %508 : Tensor = prim::GetAttr[name="weight"](%483)
      %509 : Tensor? = prim::GetAttr[name="bias"](%483)
      %510 : int[] = prim::ListConstruct(%27, %27)
      %511 : int[] = prim::ListConstruct(%24, %24)
      %512 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.65 : Tensor = aten::conv2d(%result.65, %508, %509, %510, %511, %512, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.65)
  %514 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%78)
  %515 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%78)
  %516 : int = aten::dim(%bottleneck_output.64) # torch/nn/modules/batchnorm.py:276:11
  %517 : bool = aten::ne(%516, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%517) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %518 : bool = prim::GetAttr[name="training"](%515)
   = prim::If(%518) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %519 : Tensor = prim::GetAttr[name="num_batches_tracked"](%515)
      %520 : Tensor = aten::add(%519, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%515, %520)
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%515)
  %522 : Tensor = prim::GetAttr[name="running_mean"](%515)
  %523 : Tensor = prim::GetAttr[name="running_var"](%515)
  %524 : Tensor = prim::GetAttr[name="weight"](%515)
  %525 : Tensor = prim::GetAttr[name="bias"](%515)
   = prim::If(%521) # torch/nn/functional.py:2011:4
    block0():
      %526 : int[] = aten::size(%bottleneck_output.64) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%526, %24) # torch/nn/functional.py:1991:17
      %528 : int = aten::len(%526) # torch/nn/functional.py:1992:19
      %529 : int = aten::sub(%528, %26) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%529, %25, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %533 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
          %534 : int = aten::__getitem__(%526, %533) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %534) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.291)
      %536 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
       = prim::If(%536) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %537 : Tensor = aten::batch_norm(%bottleneck_output.64, %524, %525, %522, %523, %521, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.66 : Tensor = aten::relu_(%537) # torch/nn/functional.py:1117:17
  %539 : Tensor = prim::GetAttr[name="weight"](%514)
  %540 : Tensor? = prim::GetAttr[name="bias"](%514)
  %541 : int[] = prim::ListConstruct(%27, %27)
  %542 : int[] = prim::ListConstruct(%27, %27)
  %543 : int[] = prim::ListConstruct(%27, %27)
  %new_features.67 : Tensor = aten::conv2d(%result.66, %539, %540, %541, %542, %543, %27) # torch/nn/modules/conv.py:415:15
  %545 : float = prim::GetAttr[name="drop_rate"](%78)
  %546 : bool = aten::gt(%545, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.68 : Tensor = prim::If(%546) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %548 : float = prim::GetAttr[name="drop_rate"](%78)
      %549 : bool = prim::GetAttr[name="training"](%78)
      %550 : bool = aten::lt(%548, %16) # torch/nn/functional.py:968:7
      %551 : bool = prim::If(%550) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %552 : bool = aten::gt(%548, %17) # torch/nn/functional.py:968:17
          -> (%552)
       = prim::If(%551) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %553 : Tensor = aten::dropout(%new_features.67, %548, %549) # torch/nn/functional.py:973:17
      -> (%553)
    block1():
      -> (%new_features.67)
  %554 : Tensor[] = aten::append(%features.2, %new_features.68) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %555 : Tensor = prim::Uninitialized()
  %556 : bool = prim::GetAttr[name="memory_efficient"](%79)
  %557 : bool = prim::If(%556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %558 : bool = prim::Uninitialized()
      %559 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %560 : bool = aten::gt(%559, %24)
      %561 : bool, %562 : bool, %563 : int = prim::Loop(%18, %560, %19, %558, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%564 : int, %565 : bool, %566 : bool, %567 : int):
          %tensor.35 : Tensor = aten::__getitem__(%features.2, %567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %569 : bool = prim::requires_grad(%tensor.35)
          %570 : bool, %571 : bool = prim::If(%569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %558)
          %572 : int = aten::add(%567, %27)
          %573 : bool = aten::lt(%572, %559)
          %574 : bool = aten::__and__(%573, %570)
          -> (%574, %569, %571, %572)
      %575 : bool = prim::If(%561)
        block0():
          -> (%562)
        block1():
          -> (%19)
      -> (%575)
    block1():
      -> (%19)
  %bottleneck_output.68 : Tensor = prim::If(%557) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%555)
    block1():
      %concated_features.35 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %578 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%79)
      %579 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%79)
      %580 : int = aten::dim(%concated_features.35) # torch/nn/modules/batchnorm.py:276:11
      %581 : bool = aten::ne(%580, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%581) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %582 : bool = prim::GetAttr[name="training"](%579)
       = prim::If(%582) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %583 : Tensor = prim::GetAttr[name="num_batches_tracked"](%579)
          %584 : Tensor = aten::add(%583, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%579, %584)
          -> ()
        block1():
          -> ()
      %585 : bool = prim::GetAttr[name="training"](%579)
      %586 : Tensor = prim::GetAttr[name="running_mean"](%579)
      %587 : Tensor = prim::GetAttr[name="running_var"](%579)
      %588 : Tensor = prim::GetAttr[name="weight"](%579)
      %589 : Tensor = prim::GetAttr[name="bias"](%579)
       = prim::If(%585) # torch/nn/functional.py:2011:4
        block0():
          %590 : int[] = aten::size(%concated_features.35) # torch/nn/functional.py:2012:27
          %size_prods.292 : int = aten::__getitem__(%590, %24) # torch/nn/functional.py:1991:17
          %592 : int = aten::len(%590) # torch/nn/functional.py:1992:19
          %593 : int = aten::sub(%592, %26) # torch/nn/functional.py:1992:19
          %size_prods.293 : int = prim::Loop(%593, %25, %size_prods.292) # torch/nn/functional.py:1992:4
            block0(%i.74 : int, %size_prods.294 : int):
              %597 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
              %598 : int = aten::__getitem__(%590, %597) # torch/nn/functional.py:1993:22
              %size_prods.295 : int = aten::mul(%size_prods.294, %598) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.295)
          %600 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
           = prim::If(%600) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %601 : Tensor = aten::batch_norm(%concated_features.35, %588, %589, %586, %587, %585, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.69 : Tensor = aten::relu_(%601) # torch/nn/functional.py:1117:17
      %603 : Tensor = prim::GetAttr[name="weight"](%578)
      %604 : Tensor? = prim::GetAttr[name="bias"](%578)
      %605 : int[] = prim::ListConstruct(%27, %27)
      %606 : int[] = prim::ListConstruct(%24, %24)
      %607 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.69 : Tensor = aten::conv2d(%result.69, %603, %604, %605, %606, %607, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.69)
  %609 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%79)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%79)
  %611 : int = aten::dim(%bottleneck_output.68) # torch/nn/modules/batchnorm.py:276:11
  %612 : bool = aten::ne(%611, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %613 : bool = prim::GetAttr[name="training"](%610)
   = prim::If(%613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%610)
      %615 : Tensor = aten::add(%614, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%610, %615)
      -> ()
    block1():
      -> ()
  %616 : bool = prim::GetAttr[name="training"](%610)
  %617 : Tensor = prim::GetAttr[name="running_mean"](%610)
  %618 : Tensor = prim::GetAttr[name="running_var"](%610)
  %619 : Tensor = prim::GetAttr[name="weight"](%610)
  %620 : Tensor = prim::GetAttr[name="bias"](%610)
   = prim::If(%616) # torch/nn/functional.py:2011:4
    block0():
      %621 : int[] = aten::size(%bottleneck_output.68) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%621, %24) # torch/nn/functional.py:1991:17
      %623 : int = aten::len(%621) # torch/nn/functional.py:1992:19
      %624 : int = aten::sub(%623, %26) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%624, %25, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %628 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
          %629 : int = aten::__getitem__(%621, %628) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %629) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.399)
      %631 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
       = prim::If(%631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %632 : Tensor = aten::batch_norm(%bottleneck_output.68, %619, %620, %617, %618, %616, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.70 : Tensor = aten::relu_(%632) # torch/nn/functional.py:1117:17
  %634 : Tensor = prim::GetAttr[name="weight"](%609)
  %635 : Tensor? = prim::GetAttr[name="bias"](%609)
  %636 : int[] = prim::ListConstruct(%27, %27)
  %637 : int[] = prim::ListConstruct(%27, %27)
  %638 : int[] = prim::ListConstruct(%27, %27)
  %new_features.72 : Tensor = aten::conv2d(%result.70, %634, %635, %636, %637, %638, %27) # torch/nn/modules/conv.py:415:15
  %640 : float = prim::GetAttr[name="drop_rate"](%79)
  %641 : bool = aten::gt(%640, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.70 : Tensor = prim::If(%641) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %643 : float = prim::GetAttr[name="drop_rate"](%79)
      %644 : bool = prim::GetAttr[name="training"](%79)
      %645 : bool = aten::lt(%643, %16) # torch/nn/functional.py:968:7
      %646 : bool = prim::If(%645) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %647 : bool = aten::gt(%643, %17) # torch/nn/functional.py:968:17
          -> (%647)
       = prim::If(%646) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %648 : Tensor = aten::dropout(%new_features.72, %643, %644) # torch/nn/functional.py:973:17
      -> (%648)
    block1():
      -> (%new_features.72)
  %649 : Tensor[] = aten::append(%features.2, %new_features.70) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.11 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %651 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm"](%32)
  %652 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv"](%32)
  %653 : int = aten::dim(%input.11) # torch/nn/modules/batchnorm.py:276:11
  %654 : bool = aten::ne(%653, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%654) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %655 : bool = prim::GetAttr[name="training"](%651)
   = prim::If(%655) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %656 : Tensor = prim::GetAttr[name="num_batches_tracked"](%651)
      %657 : Tensor = aten::add(%656, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%651, %657)
      -> ()
    block1():
      -> ()
  %658 : bool = prim::GetAttr[name="training"](%651)
  %659 : Tensor = prim::GetAttr[name="running_mean"](%651)
  %660 : Tensor = prim::GetAttr[name="running_var"](%651)
  %661 : Tensor = prim::GetAttr[name="weight"](%651)
  %662 : Tensor = prim::GetAttr[name="bias"](%651)
   = prim::If(%658) # torch/nn/functional.py:2011:4
    block0():
      %663 : int[] = aten::size(%input.11) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%663, %24) # torch/nn/functional.py:1991:17
      %665 : int = aten::len(%663) # torch/nn/functional.py:1992:19
      %666 : int = aten::sub(%665, %26) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%666, %25, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %670 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
          %671 : int = aten::__getitem__(%663, %670) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %671) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.299)
      %673 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
       = prim::If(%673) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.11, %661, %662, %659, %660, %658, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.12 : Tensor = aten::relu_(%input.16) # torch/nn/functional.py:1117:17
  %676 : Tensor = prim::GetAttr[name="weight"](%652)
  %677 : Tensor? = prim::GetAttr[name="bias"](%652)
  %678 : int[] = prim::ListConstruct(%27, %27)
  %679 : int[] = prim::ListConstruct(%24, %24)
  %680 : int[] = prim::ListConstruct(%27, %27)
  %input.14 : Tensor = aten::conv2d(%input.12, %676, %677, %678, %679, %680, %27) # torch/nn/modules/conv.py:415:15
  %682 : int[] = prim::ListConstruct(%26, %26)
  %683 : int[] = prim::ListConstruct(%26, %26)
  %684 : int[] = prim::ListConstruct(%24, %24)
  %input.13 : Tensor = aten::avg_pool2d(%input.14, %682, %683, %684, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.3 : Tensor[] = prim::ListConstruct(%input.13)
  %687 : __torch__.torchvision.models.densenet.___torch_mangle_77._DenseLayer = prim::GetAttr[name="denselayer1"](%33)
  %688 : __torch__.torchvision.models.densenet.___torch_mangle_80._DenseLayer = prim::GetAttr[name="denselayer2"](%33)
  %689 : __torch__.torchvision.models.densenet.___torch_mangle_83._DenseLayer = prim::GetAttr[name="denselayer3"](%33)
  %690 : __torch__.torchvision.models.densenet.___torch_mangle_86._DenseLayer = prim::GetAttr[name="denselayer4"](%33)
  %691 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer5"](%33)
  %692 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer6"](%33)
  %693 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer7"](%33)
  %694 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer8"](%33)
  %695 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer9"](%33)
  %696 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer10"](%33)
  %697 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer11"](%33)
  %698 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer12"](%33)
  %699 : Tensor = prim::Uninitialized()
  %700 : bool = prim::GetAttr[name="memory_efficient"](%687)
  %701 : bool = prim::If(%700) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %702 : bool = prim::Uninitialized()
      %703 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %704 : bool = aten::gt(%703, %24)
      %705 : bool, %706 : bool, %707 : int = prim::Loop(%18, %704, %19, %702, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%708 : int, %709 : bool, %710 : bool, %711 : int):
          %tensor.36 : Tensor = aten::__getitem__(%features.3, %711) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %713 : bool = prim::requires_grad(%tensor.36)
          %714 : bool, %715 : bool = prim::If(%713) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %702)
          %716 : int = aten::add(%711, %27)
          %717 : bool = aten::lt(%716, %703)
          %718 : bool = aten::__and__(%717, %714)
          -> (%718, %713, %715, %716)
      %719 : bool = prim::If(%705)
        block0():
          -> (%706)
        block1():
          -> (%19)
      -> (%719)
    block1():
      -> (%19)
  %bottleneck_output.70 : Tensor = prim::If(%701) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%699)
    block1():
      %concated_features.36 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %722 : __torch__.torch.nn.modules.conv.___torch_mangle_76.Conv2d = prim::GetAttr[name="conv1"](%687)
      %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm1"](%687)
      %724 : int = aten::dim(%concated_features.36) # torch/nn/modules/batchnorm.py:276:11
      %725 : bool = aten::ne(%724, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%725) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %726 : bool = prim::GetAttr[name="training"](%723)
       = prim::If(%726) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
          %728 : Tensor = aten::add(%727, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%723, %728)
          -> ()
        block1():
          -> ()
      %729 : bool = prim::GetAttr[name="training"](%723)
      %730 : Tensor = prim::GetAttr[name="running_mean"](%723)
      %731 : Tensor = prim::GetAttr[name="running_var"](%723)
      %732 : Tensor = prim::GetAttr[name="weight"](%723)
      %733 : Tensor = prim::GetAttr[name="bias"](%723)
       = prim::If(%729) # torch/nn/functional.py:2011:4
        block0():
          %734 : int[] = aten::size(%concated_features.36) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%734, %24) # torch/nn/functional.py:1991:17
          %736 : int = aten::len(%734) # torch/nn/functional.py:1992:19
          %737 : int = aten::sub(%736, %26) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%737, %25, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %741 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
              %742 : int = aten::__getitem__(%734, %741) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %742) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.307)
          %744 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
           = prim::If(%744) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %745 : Tensor = aten::batch_norm(%concated_features.36, %732, %733, %730, %731, %729, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.71 : Tensor = aten::relu_(%745) # torch/nn/functional.py:1117:17
      %747 : Tensor = prim::GetAttr[name="weight"](%722)
      %748 : Tensor? = prim::GetAttr[name="bias"](%722)
      %749 : int[] = prim::ListConstruct(%27, %27)
      %750 : int[] = prim::ListConstruct(%24, %24)
      %751 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.71 : Tensor = aten::conv2d(%result.71, %747, %748, %749, %750, %751, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.71)
  %753 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%687)
  %754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%687)
  %755 : int = aten::dim(%bottleneck_output.70) # torch/nn/modules/batchnorm.py:276:11
  %756 : bool = aten::ne(%755, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%756) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %757 : bool = prim::GetAttr[name="training"](%754)
   = prim::If(%757) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %758 : Tensor = prim::GetAttr[name="num_batches_tracked"](%754)
      %759 : Tensor = aten::add(%758, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%754, %759)
      -> ()
    block1():
      -> ()
  %760 : bool = prim::GetAttr[name="training"](%754)
  %761 : Tensor = prim::GetAttr[name="running_mean"](%754)
  %762 : Tensor = prim::GetAttr[name="running_var"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Tensor = prim::GetAttr[name="bias"](%754)
   = prim::If(%760) # torch/nn/functional.py:2011:4
    block0():
      %765 : int[] = aten::size(%bottleneck_output.70) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%765, %24) # torch/nn/functional.py:1991:17
      %767 : int = aten::len(%765) # torch/nn/functional.py:1992:19
      %768 : int = aten::sub(%767, %26) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%768, %25, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %772 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
          %773 : int = aten::__getitem__(%765, %772) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %773) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.311)
      %775 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
       = prim::If(%775) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %776 : Tensor = aten::batch_norm(%bottleneck_output.70, %763, %764, %761, %762, %760, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.72 : Tensor = aten::relu_(%776) # torch/nn/functional.py:1117:17
  %778 : Tensor = prim::GetAttr[name="weight"](%753)
  %779 : Tensor? = prim::GetAttr[name="bias"](%753)
  %780 : int[] = prim::ListConstruct(%27, %27)
  %781 : int[] = prim::ListConstruct(%27, %27)
  %782 : int[] = prim::ListConstruct(%27, %27)
  %new_features.74 : Tensor = aten::conv2d(%result.72, %778, %779, %780, %781, %782, %27) # torch/nn/modules/conv.py:415:15
  %784 : float = prim::GetAttr[name="drop_rate"](%687)
  %785 : bool = aten::gt(%784, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.71 : Tensor = prim::If(%785) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %787 : float = prim::GetAttr[name="drop_rate"](%687)
      %788 : bool = prim::GetAttr[name="training"](%687)
      %789 : bool = aten::lt(%787, %16) # torch/nn/functional.py:968:7
      %790 : bool = prim::If(%789) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %791 : bool = aten::gt(%787, %17) # torch/nn/functional.py:968:17
          -> (%791)
       = prim::If(%790) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %792 : Tensor = aten::dropout(%new_features.74, %787, %788) # torch/nn/functional.py:973:17
      -> (%792)
    block1():
      -> (%new_features.74)
  %793 : Tensor[] = aten::append(%features.3, %new_features.71) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %794 : Tensor = prim::Uninitialized()
  %795 : bool = prim::GetAttr[name="memory_efficient"](%688)
  %796 : bool = prim::If(%795) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %797 : bool = prim::Uninitialized()
      %798 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %799 : bool = aten::gt(%798, %24)
      %800 : bool, %801 : bool, %802 : int = prim::Loop(%18, %799, %19, %797, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%803 : int, %804 : bool, %805 : bool, %806 : int):
          %tensor.37 : Tensor = aten::__getitem__(%features.3, %806) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %808 : bool = prim::requires_grad(%tensor.37)
          %809 : bool, %810 : bool = prim::If(%808) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %797)
          %811 : int = aten::add(%806, %27)
          %812 : bool = aten::lt(%811, %798)
          %813 : bool = aten::__and__(%812, %809)
          -> (%813, %808, %810, %811)
      %814 : bool = prim::If(%800)
        block0():
          -> (%801)
        block1():
          -> (%19)
      -> (%814)
    block1():
      -> (%19)
  %bottleneck_output.72 : Tensor = prim::If(%796) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%794)
    block1():
      %concated_features.37 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %817 : __torch__.torch.nn.modules.conv.___torch_mangle_79.Conv2d = prim::GetAttr[name="conv1"](%688)
      %818 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="norm1"](%688)
      %819 : int = aten::dim(%concated_features.37) # torch/nn/modules/batchnorm.py:276:11
      %820 : bool = aten::ne(%819, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%820) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %821 : bool = prim::GetAttr[name="training"](%818)
       = prim::If(%821) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %822 : Tensor = prim::GetAttr[name="num_batches_tracked"](%818)
          %823 : Tensor = aten::add(%822, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%818, %823)
          -> ()
        block1():
          -> ()
      %824 : bool = prim::GetAttr[name="training"](%818)
      %825 : Tensor = prim::GetAttr[name="running_mean"](%818)
      %826 : Tensor = prim::GetAttr[name="running_var"](%818)
      %827 : Tensor = prim::GetAttr[name="weight"](%818)
      %828 : Tensor = prim::GetAttr[name="bias"](%818)
       = prim::If(%824) # torch/nn/functional.py:2011:4
        block0():
          %829 : int[] = aten::size(%concated_features.37) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%829, %24) # torch/nn/functional.py:1991:17
          %831 : int = aten::len(%829) # torch/nn/functional.py:1992:19
          %832 : int = aten::sub(%831, %26) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%832, %25, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %836 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
              %837 : int = aten::__getitem__(%829, %836) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %837) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.315)
          %839 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
           = prim::If(%839) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %840 : Tensor = aten::batch_norm(%concated_features.37, %827, %828, %825, %826, %824, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.73 : Tensor = aten::relu_(%840) # torch/nn/functional.py:1117:17
      %842 : Tensor = prim::GetAttr[name="weight"](%817)
      %843 : Tensor? = prim::GetAttr[name="bias"](%817)
      %844 : int[] = prim::ListConstruct(%27, %27)
      %845 : int[] = prim::ListConstruct(%24, %24)
      %846 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.73 : Tensor = aten::conv2d(%result.73, %842, %843, %844, %845, %846, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.73)
  %848 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%688)
  %849 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%688)
  %850 : int = aten::dim(%bottleneck_output.72) # torch/nn/modules/batchnorm.py:276:11
  %851 : bool = aten::ne(%850, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%851) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %852 : bool = prim::GetAttr[name="training"](%849)
   = prim::If(%852) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %853 : Tensor = prim::GetAttr[name="num_batches_tracked"](%849)
      %854 : Tensor = aten::add(%853, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%849, %854)
      -> ()
    block1():
      -> ()
  %855 : bool = prim::GetAttr[name="training"](%849)
  %856 : Tensor = prim::GetAttr[name="running_mean"](%849)
  %857 : Tensor = prim::GetAttr[name="running_var"](%849)
  %858 : Tensor = prim::GetAttr[name="weight"](%849)
  %859 : Tensor = prim::GetAttr[name="bias"](%849)
   = prim::If(%855) # torch/nn/functional.py:2011:4
    block0():
      %860 : int[] = aten::size(%bottleneck_output.72) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%860, %24) # torch/nn/functional.py:1991:17
      %862 : int = aten::len(%860) # torch/nn/functional.py:1992:19
      %863 : int = aten::sub(%862, %26) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%863, %25, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %867 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
          %868 : int = aten::__getitem__(%860, %867) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %868) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.319)
      %870 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
       = prim::If(%870) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %871 : Tensor = aten::batch_norm(%bottleneck_output.72, %858, %859, %856, %857, %855, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.74 : Tensor = aten::relu_(%871) # torch/nn/functional.py:1117:17
  %873 : Tensor = prim::GetAttr[name="weight"](%848)
  %874 : Tensor? = prim::GetAttr[name="bias"](%848)
  %875 : int[] = prim::ListConstruct(%27, %27)
  %876 : int[] = prim::ListConstruct(%27, %27)
  %877 : int[] = prim::ListConstruct(%27, %27)
  %new_features.76 : Tensor = aten::conv2d(%result.74, %873, %874, %875, %876, %877, %27) # torch/nn/modules/conv.py:415:15
  %879 : float = prim::GetAttr[name="drop_rate"](%688)
  %880 : bool = aten::gt(%879, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.73 : Tensor = prim::If(%880) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %882 : float = prim::GetAttr[name="drop_rate"](%688)
      %883 : bool = prim::GetAttr[name="training"](%688)
      %884 : bool = aten::lt(%882, %16) # torch/nn/functional.py:968:7
      %885 : bool = prim::If(%884) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %886 : bool = aten::gt(%882, %17) # torch/nn/functional.py:968:17
          -> (%886)
       = prim::If(%885) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %887 : Tensor = aten::dropout(%new_features.76, %882, %883) # torch/nn/functional.py:973:17
      -> (%887)
    block1():
      -> (%new_features.76)
  %888 : Tensor[] = aten::append(%features.3, %new_features.73) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %889 : Tensor = prim::Uninitialized()
  %890 : bool = prim::GetAttr[name="memory_efficient"](%689)
  %891 : bool = prim::If(%890) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %892 : bool = prim::Uninitialized()
      %893 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %894 : bool = aten::gt(%893, %24)
      %895 : bool, %896 : bool, %897 : int = prim::Loop(%18, %894, %19, %892, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%898 : int, %899 : bool, %900 : bool, %901 : int):
          %tensor.38 : Tensor = aten::__getitem__(%features.3, %901) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %903 : bool = prim::requires_grad(%tensor.38)
          %904 : bool, %905 : bool = prim::If(%903) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %892)
          %906 : int = aten::add(%901, %27)
          %907 : bool = aten::lt(%906, %893)
          %908 : bool = aten::__and__(%907, %904)
          -> (%908, %903, %905, %906)
      %909 : bool = prim::If(%895)
        block0():
          -> (%896)
        block1():
          -> (%19)
      -> (%909)
    block1():
      -> (%19)
  %bottleneck_output.74 : Tensor = prim::If(%891) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%889)
    block1():
      %concated_features.38 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %912 : __torch__.torch.nn.modules.conv.___torch_mangle_82.Conv2d = prim::GetAttr[name="conv1"](%689)
      %913 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%689)
      %914 : int = aten::dim(%concated_features.38) # torch/nn/modules/batchnorm.py:276:11
      %915 : bool = aten::ne(%914, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%915) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %916 : bool = prim::GetAttr[name="training"](%913)
       = prim::If(%916) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %917 : Tensor = prim::GetAttr[name="num_batches_tracked"](%913)
          %918 : Tensor = aten::add(%917, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%913, %918)
          -> ()
        block1():
          -> ()
      %919 : bool = prim::GetAttr[name="training"](%913)
      %920 : Tensor = prim::GetAttr[name="running_mean"](%913)
      %921 : Tensor = prim::GetAttr[name="running_var"](%913)
      %922 : Tensor = prim::GetAttr[name="weight"](%913)
      %923 : Tensor = prim::GetAttr[name="bias"](%913)
       = prim::If(%919) # torch/nn/functional.py:2011:4
        block0():
          %924 : int[] = aten::size(%concated_features.38) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%924, %24) # torch/nn/functional.py:1991:17
          %926 : int = aten::len(%924) # torch/nn/functional.py:1992:19
          %927 : int = aten::sub(%926, %26) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%927, %25, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %931 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
              %932 : int = aten::__getitem__(%924, %931) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %932) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.323)
          %934 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
           = prim::If(%934) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %935 : Tensor = aten::batch_norm(%concated_features.38, %922, %923, %920, %921, %919, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.75 : Tensor = aten::relu_(%935) # torch/nn/functional.py:1117:17
      %937 : Tensor = prim::GetAttr[name="weight"](%912)
      %938 : Tensor? = prim::GetAttr[name="bias"](%912)
      %939 : int[] = prim::ListConstruct(%27, %27)
      %940 : int[] = prim::ListConstruct(%24, %24)
      %941 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.75 : Tensor = aten::conv2d(%result.75, %937, %938, %939, %940, %941, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.75)
  %943 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%689)
  %944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%689)
  %945 : int = aten::dim(%bottleneck_output.74) # torch/nn/modules/batchnorm.py:276:11
  %946 : bool = aten::ne(%945, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%946) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %947 : bool = prim::GetAttr[name="training"](%944)
   = prim::If(%947) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %948 : Tensor = prim::GetAttr[name="num_batches_tracked"](%944)
      %949 : Tensor = aten::add(%948, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%944, %949)
      -> ()
    block1():
      -> ()
  %950 : bool = prim::GetAttr[name="training"](%944)
  %951 : Tensor = prim::GetAttr[name="running_mean"](%944)
  %952 : Tensor = prim::GetAttr[name="running_var"](%944)
  %953 : Tensor = prim::GetAttr[name="weight"](%944)
  %954 : Tensor = prim::GetAttr[name="bias"](%944)
   = prim::If(%950) # torch/nn/functional.py:2011:4
    block0():
      %955 : int[] = aten::size(%bottleneck_output.74) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%955, %24) # torch/nn/functional.py:1991:17
      %957 : int = aten::len(%955) # torch/nn/functional.py:1992:19
      %958 : int = aten::sub(%957, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%958, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %962 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %963 : int = aten::__getitem__(%955, %962) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %963) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %965 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%965) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %966 : Tensor = aten::batch_norm(%bottleneck_output.74, %953, %954, %951, %952, %950, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.76 : Tensor = aten::relu_(%966) # torch/nn/functional.py:1117:17
  %968 : Tensor = prim::GetAttr[name="weight"](%943)
  %969 : Tensor? = prim::GetAttr[name="bias"](%943)
  %970 : int[] = prim::ListConstruct(%27, %27)
  %971 : int[] = prim::ListConstruct(%27, %27)
  %972 : int[] = prim::ListConstruct(%27, %27)
  %new_features.78 : Tensor = aten::conv2d(%result.76, %968, %969, %970, %971, %972, %27) # torch/nn/modules/conv.py:415:15
  %974 : float = prim::GetAttr[name="drop_rate"](%689)
  %975 : bool = aten::gt(%974, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.75 : Tensor = prim::If(%975) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %977 : float = prim::GetAttr[name="drop_rate"](%689)
      %978 : bool = prim::GetAttr[name="training"](%689)
      %979 : bool = aten::lt(%977, %16) # torch/nn/functional.py:968:7
      %980 : bool = prim::If(%979) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %981 : bool = aten::gt(%977, %17) # torch/nn/functional.py:968:17
          -> (%981)
       = prim::If(%980) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %982 : Tensor = aten::dropout(%new_features.78, %977, %978) # torch/nn/functional.py:973:17
      -> (%982)
    block1():
      -> (%new_features.78)
  %983 : Tensor[] = aten::append(%features.3, %new_features.75) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %984 : Tensor = prim::Uninitialized()
  %985 : bool = prim::GetAttr[name="memory_efficient"](%690)
  %986 : bool = prim::If(%985) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %987 : bool = prim::Uninitialized()
      %988 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %989 : bool = aten::gt(%988, %24)
      %990 : bool, %991 : bool, %992 : int = prim::Loop(%18, %989, %19, %987, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%993 : int, %994 : bool, %995 : bool, %996 : int):
          %tensor.39 : Tensor = aten::__getitem__(%features.3, %996) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %998 : bool = prim::requires_grad(%tensor.39)
          %999 : bool, %1000 : bool = prim::If(%998) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %987)
          %1001 : int = aten::add(%996, %27)
          %1002 : bool = aten::lt(%1001, %988)
          %1003 : bool = aten::__and__(%1002, %999)
          -> (%1003, %998, %1000, %1001)
      %1004 : bool = prim::If(%990)
        block0():
          -> (%991)
        block1():
          -> (%19)
      -> (%1004)
    block1():
      -> (%19)
  %bottleneck_output.76 : Tensor = prim::If(%986) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%984)
    block1():
      %concated_features.39 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1007 : __torch__.torch.nn.modules.conv.___torch_mangle_85.Conv2d = prim::GetAttr[name="conv1"](%690)
      %1008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_84.BatchNorm2d = prim::GetAttr[name="norm1"](%690)
      %1009 : int = aten::dim(%concated_features.39) # torch/nn/modules/batchnorm.py:276:11
      %1010 : bool = aten::ne(%1009, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1010) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1011 : bool = prim::GetAttr[name="training"](%1008)
       = prim::If(%1011) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1008)
          %1013 : Tensor = aten::add(%1012, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1008, %1013)
          -> ()
        block1():
          -> ()
      %1014 : bool = prim::GetAttr[name="training"](%1008)
      %1015 : Tensor = prim::GetAttr[name="running_mean"](%1008)
      %1016 : Tensor = prim::GetAttr[name="running_var"](%1008)
      %1017 : Tensor = prim::GetAttr[name="weight"](%1008)
      %1018 : Tensor = prim::GetAttr[name="bias"](%1008)
       = prim::If(%1014) # torch/nn/functional.py:2011:4
        block0():
          %1019 : int[] = aten::size(%concated_features.39) # torch/nn/functional.py:2012:27
          %size_prods.328 : int = aten::__getitem__(%1019, %24) # torch/nn/functional.py:1991:17
          %1021 : int = aten::len(%1019) # torch/nn/functional.py:1992:19
          %1022 : int = aten::sub(%1021, %26) # torch/nn/functional.py:1992:19
          %size_prods.329 : int = prim::Loop(%1022, %25, %size_prods.328) # torch/nn/functional.py:1992:4
            block0(%i.83 : int, %size_prods.330 : int):
              %1026 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
              %1027 : int = aten::__getitem__(%1019, %1026) # torch/nn/functional.py:1993:22
              %size_prods.331 : int = aten::mul(%size_prods.330, %1027) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.331)
          %1029 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1029) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1030 : Tensor = aten::batch_norm(%concated_features.39, %1017, %1018, %1015, %1016, %1014, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.77 : Tensor = aten::relu_(%1030) # torch/nn/functional.py:1117:17
      %1032 : Tensor = prim::GetAttr[name="weight"](%1007)
      %1033 : Tensor? = prim::GetAttr[name="bias"](%1007)
      %1034 : int[] = prim::ListConstruct(%27, %27)
      %1035 : int[] = prim::ListConstruct(%24, %24)
      %1036 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.77 : Tensor = aten::conv2d(%result.77, %1032, %1033, %1034, %1035, %1036, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.77)
  %1038 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%690)
  %1039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%690)
  %1040 : int = aten::dim(%bottleneck_output.76) # torch/nn/modules/batchnorm.py:276:11
  %1041 : bool = aten::ne(%1040, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1042 : bool = prim::GetAttr[name="training"](%1039)
   = prim::If(%1042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1039)
      %1044 : Tensor = aten::add(%1043, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1039, %1044)
      -> ()
    block1():
      -> ()
  %1045 : bool = prim::GetAttr[name="training"](%1039)
  %1046 : Tensor = prim::GetAttr[name="running_mean"](%1039)
  %1047 : Tensor = prim::GetAttr[name="running_var"](%1039)
  %1048 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1039)
   = prim::If(%1045) # torch/nn/functional.py:2011:4
    block0():
      %1050 : int[] = aten::size(%bottleneck_output.76) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%1050, %24) # torch/nn/functional.py:1991:17
      %1052 : int = aten::len(%1050) # torch/nn/functional.py:1992:19
      %1053 : int = aten::sub(%1052, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%1053, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %1057 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %1058 : int = aten::__getitem__(%1050, %1057) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %1058) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %1060 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1061 : Tensor = aten::batch_norm(%bottleneck_output.76, %1048, %1049, %1046, %1047, %1045, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.78 : Tensor = aten::relu_(%1061) # torch/nn/functional.py:1117:17
  %1063 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1064 : Tensor? = prim::GetAttr[name="bias"](%1038)
  %1065 : int[] = prim::ListConstruct(%27, %27)
  %1066 : int[] = prim::ListConstruct(%27, %27)
  %1067 : int[] = prim::ListConstruct(%27, %27)
  %new_features.80 : Tensor = aten::conv2d(%result.78, %1063, %1064, %1065, %1066, %1067, %27) # torch/nn/modules/conv.py:415:15
  %1069 : float = prim::GetAttr[name="drop_rate"](%690)
  %1070 : bool = aten::gt(%1069, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.77 : Tensor = prim::If(%1070) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1072 : float = prim::GetAttr[name="drop_rate"](%690)
      %1073 : bool = prim::GetAttr[name="training"](%690)
      %1074 : bool = aten::lt(%1072, %16) # torch/nn/functional.py:968:7
      %1075 : bool = prim::If(%1074) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1076 : bool = aten::gt(%1072, %17) # torch/nn/functional.py:968:17
          -> (%1076)
       = prim::If(%1075) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1077 : Tensor = aten::dropout(%new_features.80, %1072, %1073) # torch/nn/functional.py:973:17
      -> (%1077)
    block1():
      -> (%new_features.80)
  %1078 : Tensor[] = aten::append(%features.3, %new_features.77) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1079 : Tensor = prim::Uninitialized()
  %1080 : bool = prim::GetAttr[name="memory_efficient"](%691)
  %1081 : bool = prim::If(%1080) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1082 : bool = prim::Uninitialized()
      %1083 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1084 : bool = aten::gt(%1083, %24)
      %1085 : bool, %1086 : bool, %1087 : int = prim::Loop(%18, %1084, %19, %1082, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1088 : int, %1089 : bool, %1090 : bool, %1091 : int):
          %tensor.40 : Tensor = aten::__getitem__(%features.3, %1091) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1093 : bool = prim::requires_grad(%tensor.40)
          %1094 : bool, %1095 : bool = prim::If(%1093) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1082)
          %1096 : int = aten::add(%1091, %27)
          %1097 : bool = aten::lt(%1096, %1083)
          %1098 : bool = aten::__and__(%1097, %1094)
          -> (%1098, %1093, %1095, %1096)
      %1099 : bool = prim::If(%1085)
        block0():
          -> (%1086)
        block1():
          -> (%19)
      -> (%1099)
    block1():
      -> (%19)
  %bottleneck_output.78 : Tensor = prim::If(%1081) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1079)
    block1():
      %concated_features.40 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1102 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%691)
      %1103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%691)
      %1104 : int = aten::dim(%concated_features.40) # torch/nn/modules/batchnorm.py:276:11
      %1105 : bool = aten::ne(%1104, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1105) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1106 : bool = prim::GetAttr[name="training"](%1103)
       = prim::If(%1106) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1103)
          %1108 : Tensor = aten::add(%1107, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1103, %1108)
          -> ()
        block1():
          -> ()
      %1109 : bool = prim::GetAttr[name="training"](%1103)
      %1110 : Tensor = prim::GetAttr[name="running_mean"](%1103)
      %1111 : Tensor = prim::GetAttr[name="running_var"](%1103)
      %1112 : Tensor = prim::GetAttr[name="weight"](%1103)
      %1113 : Tensor = prim::GetAttr[name="bias"](%1103)
       = prim::If(%1109) # torch/nn/functional.py:2011:4
        block0():
          %1114 : int[] = aten::size(%concated_features.40) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%1114, %24) # torch/nn/functional.py:1991:17
          %1116 : int = aten::len(%1114) # torch/nn/functional.py:1992:19
          %1117 : int = aten::sub(%1116, %26) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%1117, %25, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %1121 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
              %1122 : int = aten::__getitem__(%1114, %1121) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %1122) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.339)
          %1124 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1124) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1125 : Tensor = aten::batch_norm(%concated_features.40, %1112, %1113, %1110, %1111, %1109, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.79 : Tensor = aten::relu_(%1125) # torch/nn/functional.py:1117:17
      %1127 : Tensor = prim::GetAttr[name="weight"](%1102)
      %1128 : Tensor? = prim::GetAttr[name="bias"](%1102)
      %1129 : int[] = prim::ListConstruct(%27, %27)
      %1130 : int[] = prim::ListConstruct(%24, %24)
      %1131 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.79 : Tensor = aten::conv2d(%result.79, %1127, %1128, %1129, %1130, %1131, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.79)
  %1133 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%691)
  %1134 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%691)
  %1135 : int = aten::dim(%bottleneck_output.78) # torch/nn/modules/batchnorm.py:276:11
  %1136 : bool = aten::ne(%1135, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1136) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1137 : bool = prim::GetAttr[name="training"](%1134)
   = prim::If(%1137) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1138 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1134)
      %1139 : Tensor = aten::add(%1138, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1134, %1139)
      -> ()
    block1():
      -> ()
  %1140 : bool = prim::GetAttr[name="training"](%1134)
  %1141 : Tensor = prim::GetAttr[name="running_mean"](%1134)
  %1142 : Tensor = prim::GetAttr[name="running_var"](%1134)
  %1143 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
   = prim::If(%1140) # torch/nn/functional.py:2011:4
    block0():
      %1145 : int[] = aten::size(%bottleneck_output.78) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%1145, %24) # torch/nn/functional.py:1991:17
      %1147 : int = aten::len(%1145) # torch/nn/functional.py:1992:19
      %1148 : int = aten::sub(%1147, %26) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%1148, %25, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %1152 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
          %1153 : int = aten::__getitem__(%1145, %1152) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %1153) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.343)
      %1155 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1155) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1156 : Tensor = aten::batch_norm(%bottleneck_output.78, %1143, %1144, %1141, %1142, %1140, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.80 : Tensor = aten::relu_(%1156) # torch/nn/functional.py:1117:17
  %1158 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1159 : Tensor? = prim::GetAttr[name="bias"](%1133)
  %1160 : int[] = prim::ListConstruct(%27, %27)
  %1161 : int[] = prim::ListConstruct(%27, %27)
  %1162 : int[] = prim::ListConstruct(%27, %27)
  %new_features.82 : Tensor = aten::conv2d(%result.80, %1158, %1159, %1160, %1161, %1162, %27) # torch/nn/modules/conv.py:415:15
  %1164 : float = prim::GetAttr[name="drop_rate"](%691)
  %1165 : bool = aten::gt(%1164, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.79 : Tensor = prim::If(%1165) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1167 : float = prim::GetAttr[name="drop_rate"](%691)
      %1168 : bool = prim::GetAttr[name="training"](%691)
      %1169 : bool = aten::lt(%1167, %16) # torch/nn/functional.py:968:7
      %1170 : bool = prim::If(%1169) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1171 : bool = aten::gt(%1167, %17) # torch/nn/functional.py:968:17
          -> (%1171)
       = prim::If(%1170) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1172 : Tensor = aten::dropout(%new_features.82, %1167, %1168) # torch/nn/functional.py:973:17
      -> (%1172)
    block1():
      -> (%new_features.82)
  %1173 : Tensor[] = aten::append(%features.3, %new_features.79) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1174 : Tensor = prim::Uninitialized()
  %1175 : bool = prim::GetAttr[name="memory_efficient"](%692)
  %1176 : bool = prim::If(%1175) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1177 : bool = prim::Uninitialized()
      %1178 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1179 : bool = aten::gt(%1178, %24)
      %1180 : bool, %1181 : bool, %1182 : int = prim::Loop(%18, %1179, %19, %1177, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1183 : int, %1184 : bool, %1185 : bool, %1186 : int):
          %tensor.41 : Tensor = aten::__getitem__(%features.3, %1186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1188 : bool = prim::requires_grad(%tensor.41)
          %1189 : bool, %1190 : bool = prim::If(%1188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1177)
          %1191 : int = aten::add(%1186, %27)
          %1192 : bool = aten::lt(%1191, %1178)
          %1193 : bool = aten::__and__(%1192, %1189)
          -> (%1193, %1188, %1190, %1191)
      %1194 : bool = prim::If(%1180)
        block0():
          -> (%1181)
        block1():
          -> (%19)
      -> (%1194)
    block1():
      -> (%19)
  %bottleneck_output.80 : Tensor = prim::If(%1176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1174)
    block1():
      %concated_features.41 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1197 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%692)
      %1198 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%692)
      %1199 : int = aten::dim(%concated_features.41) # torch/nn/modules/batchnorm.py:276:11
      %1200 : bool = aten::ne(%1199, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1200) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1201 : bool = prim::GetAttr[name="training"](%1198)
       = prim::If(%1201) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1202 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1198)
          %1203 : Tensor = aten::add(%1202, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1198, %1203)
          -> ()
        block1():
          -> ()
      %1204 : bool = prim::GetAttr[name="training"](%1198)
      %1205 : Tensor = prim::GetAttr[name="running_mean"](%1198)
      %1206 : Tensor = prim::GetAttr[name="running_var"](%1198)
      %1207 : Tensor = prim::GetAttr[name="weight"](%1198)
      %1208 : Tensor = prim::GetAttr[name="bias"](%1198)
       = prim::If(%1204) # torch/nn/functional.py:2011:4
        block0():
          %1209 : int[] = aten::size(%concated_features.41) # torch/nn/functional.py:2012:27
          %size_prods.344 : int = aten::__getitem__(%1209, %24) # torch/nn/functional.py:1991:17
          %1211 : int = aten::len(%1209) # torch/nn/functional.py:1992:19
          %1212 : int = aten::sub(%1211, %26) # torch/nn/functional.py:1992:19
          %size_prods.345 : int = prim::Loop(%1212, %25, %size_prods.344) # torch/nn/functional.py:1992:4
            block0(%i.87 : int, %size_prods.346 : int):
              %1216 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
              %1217 : int = aten::__getitem__(%1209, %1216) # torch/nn/functional.py:1993:22
              %size_prods.347 : int = aten::mul(%size_prods.346, %1217) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.347)
          %1219 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1219) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1220 : Tensor = aten::batch_norm(%concated_features.41, %1207, %1208, %1205, %1206, %1204, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.81 : Tensor = aten::relu_(%1220) # torch/nn/functional.py:1117:17
      %1222 : Tensor = prim::GetAttr[name="weight"](%1197)
      %1223 : Tensor? = prim::GetAttr[name="bias"](%1197)
      %1224 : int[] = prim::ListConstruct(%27, %27)
      %1225 : int[] = prim::ListConstruct(%24, %24)
      %1226 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.81 : Tensor = aten::conv2d(%result.81, %1222, %1223, %1224, %1225, %1226, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.81)
  %1228 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%692)
  %1229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%692)
  %1230 : int = aten::dim(%bottleneck_output.80) # torch/nn/modules/batchnorm.py:276:11
  %1231 : bool = aten::ne(%1230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1229)
   = prim::If(%1232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1229)
      %1234 : Tensor = aten::add(%1233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1229, %1234)
      -> ()
    block1():
      -> ()
  %1235 : bool = prim::GetAttr[name="training"](%1229)
  %1236 : Tensor = prim::GetAttr[name="running_mean"](%1229)
  %1237 : Tensor = prim::GetAttr[name="running_var"](%1229)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1239 : Tensor = prim::GetAttr[name="bias"](%1229)
   = prim::If(%1235) # torch/nn/functional.py:2011:4
    block0():
      %1240 : int[] = aten::size(%bottleneck_output.80) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%1240, %24) # torch/nn/functional.py:1991:17
      %1242 : int = aten::len(%1240) # torch/nn/functional.py:1992:19
      %1243 : int = aten::sub(%1242, %26) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%1243, %25, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %1247 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
          %1248 : int = aten::__getitem__(%1240, %1247) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %1248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.351)
      %1250 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1251 : Tensor = aten::batch_norm(%bottleneck_output.80, %1238, %1239, %1236, %1237, %1235, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.82 : Tensor = aten::relu_(%1251) # torch/nn/functional.py:1117:17
  %1253 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1254 : Tensor? = prim::GetAttr[name="bias"](%1228)
  %1255 : int[] = prim::ListConstruct(%27, %27)
  %1256 : int[] = prim::ListConstruct(%27, %27)
  %1257 : int[] = prim::ListConstruct(%27, %27)
  %new_features.84 : Tensor = aten::conv2d(%result.82, %1253, %1254, %1255, %1256, %1257, %27) # torch/nn/modules/conv.py:415:15
  %1259 : float = prim::GetAttr[name="drop_rate"](%692)
  %1260 : bool = aten::gt(%1259, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.81 : Tensor = prim::If(%1260) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1262 : float = prim::GetAttr[name="drop_rate"](%692)
      %1263 : bool = prim::GetAttr[name="training"](%692)
      %1264 : bool = aten::lt(%1262, %16) # torch/nn/functional.py:968:7
      %1265 : bool = prim::If(%1264) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1266 : bool = aten::gt(%1262, %17) # torch/nn/functional.py:968:17
          -> (%1266)
       = prim::If(%1265) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1267 : Tensor = aten::dropout(%new_features.84, %1262, %1263) # torch/nn/functional.py:973:17
      -> (%1267)
    block1():
      -> (%new_features.84)
  %1268 : Tensor[] = aten::append(%features.3, %new_features.81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1269 : Tensor = prim::Uninitialized()
  %1270 : bool = prim::GetAttr[name="memory_efficient"](%693)
  %1271 : bool = prim::If(%1270) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1272 : bool = prim::Uninitialized()
      %1273 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1274 : bool = aten::gt(%1273, %24)
      %1275 : bool, %1276 : bool, %1277 : int = prim::Loop(%18, %1274, %19, %1272, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1278 : int, %1279 : bool, %1280 : bool, %1281 : int):
          %tensor.42 : Tensor = aten::__getitem__(%features.3, %1281) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1283 : bool = prim::requires_grad(%tensor.42)
          %1284 : bool, %1285 : bool = prim::If(%1283) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1272)
          %1286 : int = aten::add(%1281, %27)
          %1287 : bool = aten::lt(%1286, %1273)
          %1288 : bool = aten::__and__(%1287, %1284)
          -> (%1288, %1283, %1285, %1286)
      %1289 : bool = prim::If(%1275)
        block0():
          -> (%1276)
        block1():
          -> (%19)
      -> (%1289)
    block1():
      -> (%19)
  %bottleneck_output.82 : Tensor = prim::If(%1271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1269)
    block1():
      %concated_features.42 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1292 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%693)
      %1293 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%693)
      %1294 : int = aten::dim(%concated_features.42) # torch/nn/modules/batchnorm.py:276:11
      %1295 : bool = aten::ne(%1294, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1295) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1296 : bool = prim::GetAttr[name="training"](%1293)
       = prim::If(%1296) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1297 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1293)
          %1298 : Tensor = aten::add(%1297, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1293, %1298)
          -> ()
        block1():
          -> ()
      %1299 : bool = prim::GetAttr[name="training"](%1293)
      %1300 : Tensor = prim::GetAttr[name="running_mean"](%1293)
      %1301 : Tensor = prim::GetAttr[name="running_var"](%1293)
      %1302 : Tensor = prim::GetAttr[name="weight"](%1293)
      %1303 : Tensor = prim::GetAttr[name="bias"](%1293)
       = prim::If(%1299) # torch/nn/functional.py:2011:4
        block0():
          %1304 : int[] = aten::size(%concated_features.42) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%1304, %24) # torch/nn/functional.py:1991:17
          %1306 : int = aten::len(%1304) # torch/nn/functional.py:1992:19
          %1307 : int = aten::sub(%1306, %26) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%1307, %25, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %1311 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
              %1312 : int = aten::__getitem__(%1304, %1311) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %1312) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.355)
          %1314 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1314) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1315 : Tensor = aten::batch_norm(%concated_features.42, %1302, %1303, %1300, %1301, %1299, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.83 : Tensor = aten::relu_(%1315) # torch/nn/functional.py:1117:17
      %1317 : Tensor = prim::GetAttr[name="weight"](%1292)
      %1318 : Tensor? = prim::GetAttr[name="bias"](%1292)
      %1319 : int[] = prim::ListConstruct(%27, %27)
      %1320 : int[] = prim::ListConstruct(%24, %24)
      %1321 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.83 : Tensor = aten::conv2d(%result.83, %1317, %1318, %1319, %1320, %1321, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.83)
  %1323 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%693)
  %1324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%693)
  %1325 : int = aten::dim(%bottleneck_output.82) # torch/nn/modules/batchnorm.py:276:11
  %1326 : bool = aten::ne(%1325, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1326) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1327 : bool = prim::GetAttr[name="training"](%1324)
   = prim::If(%1327) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1328 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1324)
      %1329 : Tensor = aten::add(%1328, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1324, %1329)
      -> ()
    block1():
      -> ()
  %1330 : bool = prim::GetAttr[name="training"](%1324)
  %1331 : Tensor = prim::GetAttr[name="running_mean"](%1324)
  %1332 : Tensor = prim::GetAttr[name="running_var"](%1324)
  %1333 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1334 : Tensor = prim::GetAttr[name="bias"](%1324)
   = prim::If(%1330) # torch/nn/functional.py:2011:4
    block0():
      %1335 : int[] = aten::size(%bottleneck_output.82) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%1335, %24) # torch/nn/functional.py:1991:17
      %1337 : int = aten::len(%1335) # torch/nn/functional.py:1992:19
      %1338 : int = aten::sub(%1337, %26) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%1338, %25, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %1342 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
          %1343 : int = aten::__getitem__(%1335, %1342) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %1343) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.359)
      %1345 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1345) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1346 : Tensor = aten::batch_norm(%bottleneck_output.82, %1333, %1334, %1331, %1332, %1330, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.84 : Tensor = aten::relu_(%1346) # torch/nn/functional.py:1117:17
  %1348 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1349 : Tensor? = prim::GetAttr[name="bias"](%1323)
  %1350 : int[] = prim::ListConstruct(%27, %27)
  %1351 : int[] = prim::ListConstruct(%27, %27)
  %1352 : int[] = prim::ListConstruct(%27, %27)
  %new_features.86 : Tensor = aten::conv2d(%result.84, %1348, %1349, %1350, %1351, %1352, %27) # torch/nn/modules/conv.py:415:15
  %1354 : float = prim::GetAttr[name="drop_rate"](%693)
  %1355 : bool = aten::gt(%1354, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.83 : Tensor = prim::If(%1355) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1357 : float = prim::GetAttr[name="drop_rate"](%693)
      %1358 : bool = prim::GetAttr[name="training"](%693)
      %1359 : bool = aten::lt(%1357, %16) # torch/nn/functional.py:968:7
      %1360 : bool = prim::If(%1359) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1361 : bool = aten::gt(%1357, %17) # torch/nn/functional.py:968:17
          -> (%1361)
       = prim::If(%1360) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1362 : Tensor = aten::dropout(%new_features.86, %1357, %1358) # torch/nn/functional.py:973:17
      -> (%1362)
    block1():
      -> (%new_features.86)
  %1363 : Tensor[] = aten::append(%features.3, %new_features.83) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1364 : Tensor = prim::Uninitialized()
  %1365 : bool = prim::GetAttr[name="memory_efficient"](%694)
  %1366 : bool = prim::If(%1365) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1367 : bool = prim::Uninitialized()
      %1368 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1369 : bool = aten::gt(%1368, %24)
      %1370 : bool, %1371 : bool, %1372 : int = prim::Loop(%18, %1369, %19, %1367, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1373 : int, %1374 : bool, %1375 : bool, %1376 : int):
          %tensor.43 : Tensor = aten::__getitem__(%features.3, %1376) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1378 : bool = prim::requires_grad(%tensor.43)
          %1379 : bool, %1380 : bool = prim::If(%1378) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1367)
          %1381 : int = aten::add(%1376, %27)
          %1382 : bool = aten::lt(%1381, %1368)
          %1383 : bool = aten::__and__(%1382, %1379)
          -> (%1383, %1378, %1380, %1381)
      %1384 : bool = prim::If(%1370)
        block0():
          -> (%1371)
        block1():
          -> (%19)
      -> (%1384)
    block1():
      -> (%19)
  %bottleneck_output.84 : Tensor = prim::If(%1366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1364)
    block1():
      %concated_features.43 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1387 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%694)
      %1388 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%694)
      %1389 : int = aten::dim(%concated_features.43) # torch/nn/modules/batchnorm.py:276:11
      %1390 : bool = aten::ne(%1389, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1390) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1391 : bool = prim::GetAttr[name="training"](%1388)
       = prim::If(%1391) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1388)
          %1393 : Tensor = aten::add(%1392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1388, %1393)
          -> ()
        block1():
          -> ()
      %1394 : bool = prim::GetAttr[name="training"](%1388)
      %1395 : Tensor = prim::GetAttr[name="running_mean"](%1388)
      %1396 : Tensor = prim::GetAttr[name="running_var"](%1388)
      %1397 : Tensor = prim::GetAttr[name="weight"](%1388)
      %1398 : Tensor = prim::GetAttr[name="bias"](%1388)
       = prim::If(%1394) # torch/nn/functional.py:2011:4
        block0():
          %1399 : int[] = aten::size(%concated_features.43) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%1399, %24) # torch/nn/functional.py:1991:17
          %1401 : int = aten::len(%1399) # torch/nn/functional.py:1992:19
          %1402 : int = aten::sub(%1401, %26) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%1402, %25, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %1406 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
              %1407 : int = aten::__getitem__(%1399, %1406) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %1407) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.363)
          %1409 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1409) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1410 : Tensor = aten::batch_norm(%concated_features.43, %1397, %1398, %1395, %1396, %1394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.85 : Tensor = aten::relu_(%1410) # torch/nn/functional.py:1117:17
      %1412 : Tensor = prim::GetAttr[name="weight"](%1387)
      %1413 : Tensor? = prim::GetAttr[name="bias"](%1387)
      %1414 : int[] = prim::ListConstruct(%27, %27)
      %1415 : int[] = prim::ListConstruct(%24, %24)
      %1416 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.85 : Tensor = aten::conv2d(%result.85, %1412, %1413, %1414, %1415, %1416, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.85)
  %1418 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%694)
  %1419 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%694)
  %1420 : int = aten::dim(%bottleneck_output.84) # torch/nn/modules/batchnorm.py:276:11
  %1421 : bool = aten::ne(%1420, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1421) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1422 : bool = prim::GetAttr[name="training"](%1419)
   = prim::If(%1422) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1423 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1419)
      %1424 : Tensor = aten::add(%1423, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1419, %1424)
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1419)
  %1426 : Tensor = prim::GetAttr[name="running_mean"](%1419)
  %1427 : Tensor = prim::GetAttr[name="running_var"](%1419)
  %1428 : Tensor = prim::GetAttr[name="weight"](%1419)
  %1429 : Tensor = prim::GetAttr[name="bias"](%1419)
   = prim::If(%1425) # torch/nn/functional.py:2011:4
    block0():
      %1430 : int[] = aten::size(%bottleneck_output.84) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%1430, %24) # torch/nn/functional.py:1991:17
      %1432 : int = aten::len(%1430) # torch/nn/functional.py:1992:19
      %1433 : int = aten::sub(%1432, %26) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%1433, %25, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %1437 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
          %1438 : int = aten::__getitem__(%1430, %1437) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %1438) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.367)
      %1440 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1440) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1441 : Tensor = aten::batch_norm(%bottleneck_output.84, %1428, %1429, %1426, %1427, %1425, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.86 : Tensor = aten::relu_(%1441) # torch/nn/functional.py:1117:17
  %1443 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1444 : Tensor? = prim::GetAttr[name="bias"](%1418)
  %1445 : int[] = prim::ListConstruct(%27, %27)
  %1446 : int[] = prim::ListConstruct(%27, %27)
  %1447 : int[] = prim::ListConstruct(%27, %27)
  %new_features.88 : Tensor = aten::conv2d(%result.86, %1443, %1444, %1445, %1446, %1447, %27) # torch/nn/modules/conv.py:415:15
  %1449 : float = prim::GetAttr[name="drop_rate"](%694)
  %1450 : bool = aten::gt(%1449, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.85 : Tensor = prim::If(%1450) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1452 : float = prim::GetAttr[name="drop_rate"](%694)
      %1453 : bool = prim::GetAttr[name="training"](%694)
      %1454 : bool = aten::lt(%1452, %16) # torch/nn/functional.py:968:7
      %1455 : bool = prim::If(%1454) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1456 : bool = aten::gt(%1452, %17) # torch/nn/functional.py:968:17
          -> (%1456)
       = prim::If(%1455) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1457 : Tensor = aten::dropout(%new_features.88, %1452, %1453) # torch/nn/functional.py:973:17
      -> (%1457)
    block1():
      -> (%new_features.88)
  %1458 : Tensor[] = aten::append(%features.3, %new_features.85) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1459 : Tensor = prim::Uninitialized()
  %1460 : bool = prim::GetAttr[name="memory_efficient"](%695)
  %1461 : bool = prim::If(%1460) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1462 : bool = prim::Uninitialized()
      %1463 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1464 : bool = aten::gt(%1463, %24)
      %1465 : bool, %1466 : bool, %1467 : int = prim::Loop(%18, %1464, %19, %1462, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1468 : int, %1469 : bool, %1470 : bool, %1471 : int):
          %tensor.44 : Tensor = aten::__getitem__(%features.3, %1471) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1473 : bool = prim::requires_grad(%tensor.44)
          %1474 : bool, %1475 : bool = prim::If(%1473) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1462)
          %1476 : int = aten::add(%1471, %27)
          %1477 : bool = aten::lt(%1476, %1463)
          %1478 : bool = aten::__and__(%1477, %1474)
          -> (%1478, %1473, %1475, %1476)
      %1479 : bool = prim::If(%1465)
        block0():
          -> (%1466)
        block1():
          -> (%19)
      -> (%1479)
    block1():
      -> (%19)
  %bottleneck_output.86 : Tensor = prim::If(%1461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1459)
    block1():
      %concated_features.44 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1482 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%695)
      %1483 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%695)
      %1484 : int = aten::dim(%concated_features.44) # torch/nn/modules/batchnorm.py:276:11
      %1485 : bool = aten::ne(%1484, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1485) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1486 : bool = prim::GetAttr[name="training"](%1483)
       = prim::If(%1486) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1483)
          %1488 : Tensor = aten::add(%1487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1483, %1488)
          -> ()
        block1():
          -> ()
      %1489 : bool = prim::GetAttr[name="training"](%1483)
      %1490 : Tensor = prim::GetAttr[name="running_mean"](%1483)
      %1491 : Tensor = prim::GetAttr[name="running_var"](%1483)
      %1492 : Tensor = prim::GetAttr[name="weight"](%1483)
      %1493 : Tensor = prim::GetAttr[name="bias"](%1483)
       = prim::If(%1489) # torch/nn/functional.py:2011:4
        block0():
          %1494 : int[] = aten::size(%concated_features.44) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%1494, %24) # torch/nn/functional.py:1991:17
          %1496 : int = aten::len(%1494) # torch/nn/functional.py:1992:19
          %1497 : int = aten::sub(%1496, %26) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%1497, %25, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %1501 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
              %1502 : int = aten::__getitem__(%1494, %1501) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %1502) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.371)
          %1504 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1504) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1505 : Tensor = aten::batch_norm(%concated_features.44, %1492, %1493, %1490, %1491, %1489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.87 : Tensor = aten::relu_(%1505) # torch/nn/functional.py:1117:17
      %1507 : Tensor = prim::GetAttr[name="weight"](%1482)
      %1508 : Tensor? = prim::GetAttr[name="bias"](%1482)
      %1509 : int[] = prim::ListConstruct(%27, %27)
      %1510 : int[] = prim::ListConstruct(%24, %24)
      %1511 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.87 : Tensor = aten::conv2d(%result.87, %1507, %1508, %1509, %1510, %1511, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.87)
  %1513 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%695)
  %1514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%695)
  %1515 : int = aten::dim(%bottleneck_output.86) # torch/nn/modules/batchnorm.py:276:11
  %1516 : bool = aten::ne(%1515, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1516) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1517 : bool = prim::GetAttr[name="training"](%1514)
   = prim::If(%1517) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1518 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1514)
      %1519 : Tensor = aten::add(%1518, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1514, %1519)
      -> ()
    block1():
      -> ()
  %1520 : bool = prim::GetAttr[name="training"](%1514)
  %1521 : Tensor = prim::GetAttr[name="running_mean"](%1514)
  %1522 : Tensor = prim::GetAttr[name="running_var"](%1514)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1524 : Tensor = prim::GetAttr[name="bias"](%1514)
   = prim::If(%1520) # torch/nn/functional.py:2011:4
    block0():
      %1525 : int[] = aten::size(%bottleneck_output.86) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%1525, %24) # torch/nn/functional.py:1991:17
      %1527 : int = aten::len(%1525) # torch/nn/functional.py:1992:19
      %1528 : int = aten::sub(%1527, %26) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%1528, %25, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %1532 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
          %1533 : int = aten::__getitem__(%1525, %1532) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %1533) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.375)
      %1535 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1535) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1536 : Tensor = aten::batch_norm(%bottleneck_output.86, %1523, %1524, %1521, %1522, %1520, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.88 : Tensor = aten::relu_(%1536) # torch/nn/functional.py:1117:17
  %1538 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1539 : Tensor? = prim::GetAttr[name="bias"](%1513)
  %1540 : int[] = prim::ListConstruct(%27, %27)
  %1541 : int[] = prim::ListConstruct(%27, %27)
  %1542 : int[] = prim::ListConstruct(%27, %27)
  %new_features.90 : Tensor = aten::conv2d(%result.88, %1538, %1539, %1540, %1541, %1542, %27) # torch/nn/modules/conv.py:415:15
  %1544 : float = prim::GetAttr[name="drop_rate"](%695)
  %1545 : bool = aten::gt(%1544, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.87 : Tensor = prim::If(%1545) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1547 : float = prim::GetAttr[name="drop_rate"](%695)
      %1548 : bool = prim::GetAttr[name="training"](%695)
      %1549 : bool = aten::lt(%1547, %16) # torch/nn/functional.py:968:7
      %1550 : bool = prim::If(%1549) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1551 : bool = aten::gt(%1547, %17) # torch/nn/functional.py:968:17
          -> (%1551)
       = prim::If(%1550) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1552 : Tensor = aten::dropout(%new_features.90, %1547, %1548) # torch/nn/functional.py:973:17
      -> (%1552)
    block1():
      -> (%new_features.90)
  %1553 : Tensor[] = aten::append(%features.3, %new_features.87) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1554 : Tensor = prim::Uninitialized()
  %1555 : bool = prim::GetAttr[name="memory_efficient"](%696)
  %1556 : bool = prim::If(%1555) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1557 : bool = prim::Uninitialized()
      %1558 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1559 : bool = aten::gt(%1558, %24)
      %1560 : bool, %1561 : bool, %1562 : int = prim::Loop(%18, %1559, %19, %1557, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1563 : int, %1564 : bool, %1565 : bool, %1566 : int):
          %tensor.45 : Tensor = aten::__getitem__(%features.3, %1566) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1568 : bool = prim::requires_grad(%tensor.45)
          %1569 : bool, %1570 : bool = prim::If(%1568) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1557)
          %1571 : int = aten::add(%1566, %27)
          %1572 : bool = aten::lt(%1571, %1558)
          %1573 : bool = aten::__and__(%1572, %1569)
          -> (%1573, %1568, %1570, %1571)
      %1574 : bool = prim::If(%1560)
        block0():
          -> (%1561)
        block1():
          -> (%19)
      -> (%1574)
    block1():
      -> (%19)
  %bottleneck_output.88 : Tensor = prim::If(%1556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1554)
    block1():
      %concated_features.45 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1577 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%696)
      %1578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%696)
      %1579 : int = aten::dim(%concated_features.45) # torch/nn/modules/batchnorm.py:276:11
      %1580 : bool = aten::ne(%1579, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1580) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1581 : bool = prim::GetAttr[name="training"](%1578)
       = prim::If(%1581) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1578)
          %1583 : Tensor = aten::add(%1582, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1578, %1583)
          -> ()
        block1():
          -> ()
      %1584 : bool = prim::GetAttr[name="training"](%1578)
      %1585 : Tensor = prim::GetAttr[name="running_mean"](%1578)
      %1586 : Tensor = prim::GetAttr[name="running_var"](%1578)
      %1587 : Tensor = prim::GetAttr[name="weight"](%1578)
      %1588 : Tensor = prim::GetAttr[name="bias"](%1578)
       = prim::If(%1584) # torch/nn/functional.py:2011:4
        block0():
          %1589 : int[] = aten::size(%concated_features.45) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%1589, %24) # torch/nn/functional.py:1991:17
          %1591 : int = aten::len(%1589) # torch/nn/functional.py:1992:19
          %1592 : int = aten::sub(%1591, %26) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%1592, %25, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %1596 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
              %1597 : int = aten::__getitem__(%1589, %1596) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %1597) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.379)
          %1599 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1599) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1600 : Tensor = aten::batch_norm(%concated_features.45, %1587, %1588, %1585, %1586, %1584, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.89 : Tensor = aten::relu_(%1600) # torch/nn/functional.py:1117:17
      %1602 : Tensor = prim::GetAttr[name="weight"](%1577)
      %1603 : Tensor? = prim::GetAttr[name="bias"](%1577)
      %1604 : int[] = prim::ListConstruct(%27, %27)
      %1605 : int[] = prim::ListConstruct(%24, %24)
      %1606 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.89 : Tensor = aten::conv2d(%result.89, %1602, %1603, %1604, %1605, %1606, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.89)
  %1608 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%696)
  %1609 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%696)
  %1610 : int = aten::dim(%bottleneck_output.88) # torch/nn/modules/batchnorm.py:276:11
  %1611 : bool = aten::ne(%1610, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1611) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1612 : bool = prim::GetAttr[name="training"](%1609)
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1613 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1609)
      %1614 : Tensor = aten::add(%1613, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1609, %1614)
      -> ()
    block1():
      -> ()
  %1615 : bool = prim::GetAttr[name="training"](%1609)
  %1616 : Tensor = prim::GetAttr[name="running_mean"](%1609)
  %1617 : Tensor = prim::GetAttr[name="running_var"](%1609)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1609)
  %1619 : Tensor = prim::GetAttr[name="bias"](%1609)
   = prim::If(%1615) # torch/nn/functional.py:2011:4
    block0():
      %1620 : int[] = aten::size(%bottleneck_output.88) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%1620, %24) # torch/nn/functional.py:1991:17
      %1622 : int = aten::len(%1620) # torch/nn/functional.py:1992:19
      %1623 : int = aten::sub(%1622, %26) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%1623, %25, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %1627 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
          %1628 : int = aten::__getitem__(%1620, %1627) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %1628) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.383)
      %1630 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1630) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1631 : Tensor = aten::batch_norm(%bottleneck_output.88, %1618, %1619, %1616, %1617, %1615, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.90 : Tensor = aten::relu_(%1631) # torch/nn/functional.py:1117:17
  %1633 : Tensor = prim::GetAttr[name="weight"](%1608)
  %1634 : Tensor? = prim::GetAttr[name="bias"](%1608)
  %1635 : int[] = prim::ListConstruct(%27, %27)
  %1636 : int[] = prim::ListConstruct(%27, %27)
  %1637 : int[] = prim::ListConstruct(%27, %27)
  %new_features.92 : Tensor = aten::conv2d(%result.90, %1633, %1634, %1635, %1636, %1637, %27) # torch/nn/modules/conv.py:415:15
  %1639 : float = prim::GetAttr[name="drop_rate"](%696)
  %1640 : bool = aten::gt(%1639, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.89 : Tensor = prim::If(%1640) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1642 : float = prim::GetAttr[name="drop_rate"](%696)
      %1643 : bool = prim::GetAttr[name="training"](%696)
      %1644 : bool = aten::lt(%1642, %16) # torch/nn/functional.py:968:7
      %1645 : bool = prim::If(%1644) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1646 : bool = aten::gt(%1642, %17) # torch/nn/functional.py:968:17
          -> (%1646)
       = prim::If(%1645) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1647 : Tensor = aten::dropout(%new_features.92, %1642, %1643) # torch/nn/functional.py:973:17
      -> (%1647)
    block1():
      -> (%new_features.92)
  %1648 : Tensor[] = aten::append(%features.3, %new_features.89) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1649 : Tensor = prim::Uninitialized()
  %1650 : bool = prim::GetAttr[name="memory_efficient"](%697)
  %1651 : bool = prim::If(%1650) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1652 : bool = prim::Uninitialized()
      %1653 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1654 : bool = aten::gt(%1653, %24)
      %1655 : bool, %1656 : bool, %1657 : int = prim::Loop(%18, %1654, %19, %1652, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1658 : int, %1659 : bool, %1660 : bool, %1661 : int):
          %tensor.46 : Tensor = aten::__getitem__(%features.3, %1661) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1663 : bool = prim::requires_grad(%tensor.46)
          %1664 : bool, %1665 : bool = prim::If(%1663) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1652)
          %1666 : int = aten::add(%1661, %27)
          %1667 : bool = aten::lt(%1666, %1653)
          %1668 : bool = aten::__and__(%1667, %1664)
          -> (%1668, %1663, %1665, %1666)
      %1669 : bool = prim::If(%1655)
        block0():
          -> (%1656)
        block1():
          -> (%19)
      -> (%1669)
    block1():
      -> (%19)
  %bottleneck_output.90 : Tensor = prim::If(%1651) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1649)
    block1():
      %concated_features.46 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1672 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%697)
      %1673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%697)
      %1674 : int = aten::dim(%concated_features.46) # torch/nn/modules/batchnorm.py:276:11
      %1675 : bool = aten::ne(%1674, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1675) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1676 : bool = prim::GetAttr[name="training"](%1673)
       = prim::If(%1676) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1673)
          %1678 : Tensor = aten::add(%1677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1673, %1678)
          -> ()
        block1():
          -> ()
      %1679 : bool = prim::GetAttr[name="training"](%1673)
      %1680 : Tensor = prim::GetAttr[name="running_mean"](%1673)
      %1681 : Tensor = prim::GetAttr[name="running_var"](%1673)
      %1682 : Tensor = prim::GetAttr[name="weight"](%1673)
      %1683 : Tensor = prim::GetAttr[name="bias"](%1673)
       = prim::If(%1679) # torch/nn/functional.py:2011:4
        block0():
          %1684 : int[] = aten::size(%concated_features.46) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%1684, %24) # torch/nn/functional.py:1991:17
          %1686 : int = aten::len(%1684) # torch/nn/functional.py:1992:19
          %1687 : int = aten::sub(%1686, %26) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%1687, %25, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %1691 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
              %1692 : int = aten::__getitem__(%1684, %1691) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %1692) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.387)
          %1694 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1694) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1695 : Tensor = aten::batch_norm(%concated_features.46, %1682, %1683, %1680, %1681, %1679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.91 : Tensor = aten::relu_(%1695) # torch/nn/functional.py:1117:17
      %1697 : Tensor = prim::GetAttr[name="weight"](%1672)
      %1698 : Tensor? = prim::GetAttr[name="bias"](%1672)
      %1699 : int[] = prim::ListConstruct(%27, %27)
      %1700 : int[] = prim::ListConstruct(%24, %24)
      %1701 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.91 : Tensor = aten::conv2d(%result.91, %1697, %1698, %1699, %1700, %1701, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.91)
  %1703 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%697)
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%697)
  %1705 : int = aten::dim(%bottleneck_output.90) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%bottleneck_output.90) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%1715, %24) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %26) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%1718, %25, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %1722 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %1723) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.391)
      %1725 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1726 : Tensor = aten::batch_norm(%bottleneck_output.90, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.92 : Tensor = aten::relu_(%1726) # torch/nn/functional.py:1117:17
  %1728 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1729 : Tensor? = prim::GetAttr[name="bias"](%1703)
  %1730 : int[] = prim::ListConstruct(%27, %27)
  %1731 : int[] = prim::ListConstruct(%27, %27)
  %1732 : int[] = prim::ListConstruct(%27, %27)
  %new_features.94 : Tensor = aten::conv2d(%result.92, %1728, %1729, %1730, %1731, %1732, %27) # torch/nn/modules/conv.py:415:15
  %1734 : float = prim::GetAttr[name="drop_rate"](%697)
  %1735 : bool = aten::gt(%1734, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.91 : Tensor = prim::If(%1735) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1737 : float = prim::GetAttr[name="drop_rate"](%697)
      %1738 : bool = prim::GetAttr[name="training"](%697)
      %1739 : bool = aten::lt(%1737, %16) # torch/nn/functional.py:968:7
      %1740 : bool = prim::If(%1739) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1741 : bool = aten::gt(%1737, %17) # torch/nn/functional.py:968:17
          -> (%1741)
       = prim::If(%1740) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1742 : Tensor = aten::dropout(%new_features.94, %1737, %1738) # torch/nn/functional.py:973:17
      -> (%1742)
    block1():
      -> (%new_features.94)
  %1743 : Tensor[] = aten::append(%features.3, %new_features.91) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1744 : Tensor = prim::Uninitialized()
  %1745 : bool = prim::GetAttr[name="memory_efficient"](%698)
  %1746 : bool = prim::If(%1745) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1747 : bool = prim::Uninitialized()
      %1748 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1749 : bool = aten::gt(%1748, %24)
      %1750 : bool, %1751 : bool, %1752 : int = prim::Loop(%18, %1749, %19, %1747, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1753 : int, %1754 : bool, %1755 : bool, %1756 : int):
          %tensor.50 : Tensor = aten::__getitem__(%features.3, %1756) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1758 : bool = prim::requires_grad(%tensor.50)
          %1759 : bool, %1760 : bool = prim::If(%1758) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1747)
          %1761 : int = aten::add(%1756, %27)
          %1762 : bool = aten::lt(%1761, %1748)
          %1763 : bool = aten::__and__(%1762, %1759)
          -> (%1763, %1758, %1760, %1761)
      %1764 : bool = prim::If(%1750)
        block0():
          -> (%1751)
        block1():
          -> (%19)
      -> (%1764)
    block1():
      -> (%19)
  %bottleneck_output.98 : Tensor = prim::If(%1746) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1744)
    block1():
      %concated_features.50 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1767 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%698)
      %1768 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%698)
      %1769 : int = aten::dim(%concated_features.50) # torch/nn/modules/batchnorm.py:276:11
      %1770 : bool = aten::ne(%1769, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1770) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1771 : bool = prim::GetAttr[name="training"](%1768)
       = prim::If(%1771) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1772 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1768)
          %1773 : Tensor = aten::add(%1772, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1768, %1773)
          -> ()
        block1():
          -> ()
      %1774 : bool = prim::GetAttr[name="training"](%1768)
      %1775 : Tensor = prim::GetAttr[name="running_mean"](%1768)
      %1776 : Tensor = prim::GetAttr[name="running_var"](%1768)
      %1777 : Tensor = prim::GetAttr[name="weight"](%1768)
      %1778 : Tensor = prim::GetAttr[name="bias"](%1768)
       = prim::If(%1774) # torch/nn/functional.py:2011:4
        block0():
          %1779 : int[] = aten::size(%concated_features.50) # torch/nn/functional.py:2012:27
          %size_prods.404 : int = aten::__getitem__(%1779, %24) # torch/nn/functional.py:1991:17
          %1781 : int = aten::len(%1779) # torch/nn/functional.py:1992:19
          %1782 : int = aten::sub(%1781, %26) # torch/nn/functional.py:1992:19
          %size_prods.405 : int = prim::Loop(%1782, %25, %size_prods.404) # torch/nn/functional.py:1992:4
            block0(%i.102 : int, %size_prods.406 : int):
              %1786 : int = aten::add(%i.102, %26) # torch/nn/functional.py:1993:27
              %1787 : int = aten::__getitem__(%1779, %1786) # torch/nn/functional.py:1993:22
              %size_prods.407 : int = aten::mul(%size_prods.406, %1787) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.407)
          %1789 : bool = aten::eq(%size_prods.405, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1789) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1790 : Tensor = aten::batch_norm(%concated_features.50, %1777, %1778, %1775, %1776, %1774, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.99 : Tensor = aten::relu_(%1790) # torch/nn/functional.py:1117:17
      %1792 : Tensor = prim::GetAttr[name="weight"](%1767)
      %1793 : Tensor? = prim::GetAttr[name="bias"](%1767)
      %1794 : int[] = prim::ListConstruct(%27, %27)
      %1795 : int[] = prim::ListConstruct(%24, %24)
      %1796 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.99 : Tensor = aten::conv2d(%result.99, %1792, %1793, %1794, %1795, %1796, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.99)
  %1798 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%698)
  %1799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%698)
  %1800 : int = aten::dim(%bottleneck_output.98) # torch/nn/modules/batchnorm.py:276:11
  %1801 : bool = aten::ne(%1800, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1802 : bool = prim::GetAttr[name="training"](%1799)
   = prim::If(%1802) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1803 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1799)
      %1804 : Tensor = aten::add(%1803, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1799, %1804)
      -> ()
    block1():
      -> ()
  %1805 : bool = prim::GetAttr[name="training"](%1799)
  %1806 : Tensor = prim::GetAttr[name="running_mean"](%1799)
  %1807 : Tensor = prim::GetAttr[name="running_var"](%1799)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1799)
  %1809 : Tensor = prim::GetAttr[name="bias"](%1799)
   = prim::If(%1805) # torch/nn/functional.py:2011:4
    block0():
      %1810 : int[] = aten::size(%bottleneck_output.98) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%1810, %24) # torch/nn/functional.py:1991:17
      %1812 : int = aten::len(%1810) # torch/nn/functional.py:1992:19
      %1813 : int = aten::sub(%1812, %26) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%1813, %25, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %1817 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
          %1818 : int = aten::__getitem__(%1810, %1817) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %1818) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.303)
      %1820 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1820) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1821 : Tensor = aten::batch_norm(%bottleneck_output.98, %1808, %1809, %1806, %1807, %1805, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.100 : Tensor = aten::relu_(%1821) # torch/nn/functional.py:1117:17
  %1823 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1824 : Tensor? = prim::GetAttr[name="bias"](%1798)
  %1825 : int[] = prim::ListConstruct(%27, %27)
  %1826 : int[] = prim::ListConstruct(%27, %27)
  %1827 : int[] = prim::ListConstruct(%27, %27)
  %new_features.99 : Tensor = aten::conv2d(%result.100, %1823, %1824, %1825, %1826, %1827, %27) # torch/nn/modules/conv.py:415:15
  %1829 : float = prim::GetAttr[name="drop_rate"](%698)
  %1830 : bool = aten::gt(%1829, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.93 : Tensor = prim::If(%1830) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1832 : float = prim::GetAttr[name="drop_rate"](%698)
      %1833 : bool = prim::GetAttr[name="training"](%698)
      %1834 : bool = aten::lt(%1832, %16) # torch/nn/functional.py:968:7
      %1835 : bool = prim::If(%1834) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1836 : bool = aten::gt(%1832, %17) # torch/nn/functional.py:968:17
          -> (%1836)
       = prim::If(%1835) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1837 : Tensor = aten::dropout(%new_features.99, %1832, %1833) # torch/nn/functional.py:973:17
      -> (%1837)
    block1():
      -> (%new_features.99)
  %1838 : Tensor[] = aten::append(%features.3, %new_features.93) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.15 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %1840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm"](%34)
  %1841 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv"](%34)
  %1842 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %1843 : bool = aten::ne(%1842, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1844 : bool = prim::GetAttr[name="training"](%1840)
   = prim::If(%1844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1840)
      %1846 : Tensor = aten::add(%1845, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1840, %1846)
      -> ()
    block1():
      -> ()
  %1847 : bool = prim::GetAttr[name="training"](%1840)
  %1848 : Tensor = prim::GetAttr[name="running_mean"](%1840)
  %1849 : Tensor = prim::GetAttr[name="running_var"](%1840)
  %1850 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1851 : Tensor = prim::GetAttr[name="bias"](%1840)
   = prim::If(%1847) # torch/nn/functional.py:2011:4
    block0():
      %1852 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.408 : int = aten::__getitem__(%1852, %24) # torch/nn/functional.py:1991:17
      %1854 : int = aten::len(%1852) # torch/nn/functional.py:1992:19
      %1855 : int = aten::sub(%1854, %26) # torch/nn/functional.py:1992:19
      %size_prods.409 : int = prim::Loop(%1855, %25, %size_prods.408) # torch/nn/functional.py:1992:4
        block0(%i.103 : int, %size_prods.410 : int):
          %1859 : int = aten::add(%i.103, %26) # torch/nn/functional.py:1993:27
          %1860 : int = aten::__getitem__(%1852, %1859) # torch/nn/functional.py:1993:22
          %size_prods.411 : int = aten::mul(%size_prods.410, %1860) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.411)
      %1862 : bool = aten::eq(%size_prods.409, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.15, %1850, %1851, %1848, %1849, %1847, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.18 : Tensor = aten::relu_(%input.22) # torch/nn/functional.py:1117:17
  %1865 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1866 : Tensor? = prim::GetAttr[name="bias"](%1841)
  %1867 : int[] = prim::ListConstruct(%27, %27)
  %1868 : int[] = prim::ListConstruct(%24, %24)
  %1869 : int[] = prim::ListConstruct(%27, %27)
  %input.20 : Tensor = aten::conv2d(%input.18, %1865, %1866, %1867, %1868, %1869, %27) # torch/nn/modules/conv.py:415:15
  %1871 : int[] = prim::ListConstruct(%26, %26)
  %1872 : int[] = prim::ListConstruct(%26, %26)
  %1873 : int[] = prim::ListConstruct(%24, %24)
  %input.17 : Tensor = aten::avg_pool2d(%input.20, %1871, %1872, %1873, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.4 : Tensor[] = prim::ListConstruct(%input.17)
  %1876 : __torch__.torchvision.models.densenet.___torch_mangle_87._DenseLayer = prim::GetAttr[name="denselayer1"](%35)
  %1877 : __torch__.torchvision.models.densenet.___torch_mangle_90._DenseLayer = prim::GetAttr[name="denselayer2"](%35)
  %1878 : __torch__.torchvision.models.densenet.___torch_mangle_93._DenseLayer = prim::GetAttr[name="denselayer3"](%35)
  %1879 : __torch__.torchvision.models.densenet.___torch_mangle_96._DenseLayer = prim::GetAttr[name="denselayer4"](%35)
  %1880 : __torch__.torchvision.models.densenet.___torch_mangle_99._DenseLayer = prim::GetAttr[name="denselayer5"](%35)
  %1881 : __torch__.torchvision.models.densenet.___torch_mangle_102._DenseLayer = prim::GetAttr[name="denselayer6"](%35)
  %1882 : __torch__.torchvision.models.densenet.___torch_mangle_105._DenseLayer = prim::GetAttr[name="denselayer7"](%35)
  %1883 : __torch__.torchvision.models.densenet.___torch_mangle_108._DenseLayer = prim::GetAttr[name="denselayer8"](%35)
  %1884 : __torch__.torchvision.models.densenet.___torch_mangle_111._DenseLayer = prim::GetAttr[name="denselayer9"](%35)
  %1885 : __torch__.torchvision.models.densenet.___torch_mangle_114._DenseLayer = prim::GetAttr[name="denselayer10"](%35)
  %1886 : __torch__.torchvision.models.densenet.___torch_mangle_117._DenseLayer = prim::GetAttr[name="denselayer11"](%35)
  %1887 : __torch__.torchvision.models.densenet.___torch_mangle_120._DenseLayer = prim::GetAttr[name="denselayer12"](%35)
  %1888 : __torch__.torchvision.models.densenet.___torch_mangle_123._DenseLayer = prim::GetAttr[name="denselayer13"](%35)
  %1889 : __torch__.torchvision.models.densenet.___torch_mangle_126._DenseLayer = prim::GetAttr[name="denselayer14"](%35)
  %1890 : __torch__.torchvision.models.densenet.___torch_mangle_129._DenseLayer = prim::GetAttr[name="denselayer15"](%35)
  %1891 : __torch__.torchvision.models.densenet.___torch_mangle_132._DenseLayer = prim::GetAttr[name="denselayer16"](%35)
  %1892 : __torch__.torchvision.models.densenet.___torch_mangle_135._DenseLayer = prim::GetAttr[name="denselayer17"](%35)
  %1893 : __torch__.torchvision.models.densenet.___torch_mangle_138._DenseLayer = prim::GetAttr[name="denselayer18"](%35)
  %1894 : __torch__.torchvision.models.densenet.___torch_mangle_141._DenseLayer = prim::GetAttr[name="denselayer19"](%35)
  %1895 : __torch__.torchvision.models.densenet.___torch_mangle_144._DenseLayer = prim::GetAttr[name="denselayer20"](%35)
  %1896 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer21"](%35)
  %1897 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer22"](%35)
  %1898 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer23"](%35)
  %1899 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer24"](%35)
  %1900 : __torch__.torchvision.models.densenet.___torch_mangle_300._DenseLayer = prim::GetAttr[name="denselayer25"](%35)
  %1901 : __torch__.torchvision.models.densenet.___torch_mangle_302._DenseLayer = prim::GetAttr[name="denselayer26"](%35)
  %1902 : __torch__.torchvision.models.densenet.___torch_mangle_305._DenseLayer = prim::GetAttr[name="denselayer27"](%35)
  %1903 : __torch__.torchvision.models.densenet.___torch_mangle_308._DenseLayer = prim::GetAttr[name="denselayer28"](%35)
  %1904 : __torch__.torchvision.models.densenet.___torch_mangle_310._DenseLayer = prim::GetAttr[name="denselayer29"](%35)
  %1905 : __torch__.torchvision.models.densenet.___torch_mangle_313._DenseLayer = prim::GetAttr[name="denselayer30"](%35)
  %1906 : __torch__.torchvision.models.densenet.___torch_mangle_316._DenseLayer = prim::GetAttr[name="denselayer31"](%35)
  %1907 : __torch__.torchvision.models.densenet.___torch_mangle_318._DenseLayer = prim::GetAttr[name="denselayer32"](%35)
  %1908 : Tensor = prim::Uninitialized()
  %1909 : bool = prim::GetAttr[name="memory_efficient"](%1876)
  %1910 : bool = prim::If(%1909) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1911 : bool = prim::Uninitialized()
      %1912 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1913 : bool = aten::gt(%1912, %24)
      %1914 : bool, %1915 : bool, %1916 : int = prim::Loop(%18, %1913, %19, %1911, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1917 : int, %1918 : bool, %1919 : bool, %1920 : int):
          %tensor.51 : Tensor = aten::__getitem__(%features.4, %1920) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1922 : bool = prim::requires_grad(%tensor.51)
          %1923 : bool, %1924 : bool = prim::If(%1922) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1911)
          %1925 : int = aten::add(%1920, %27)
          %1926 : bool = aten::lt(%1925, %1912)
          %1927 : bool = aten::__and__(%1926, %1923)
          -> (%1927, %1922, %1924, %1925)
      %1928 : bool = prim::If(%1914)
        block0():
          -> (%1915)
        block1():
          -> (%19)
      -> (%1928)
    block1():
      -> (%19)
  %bottleneck_output.100 : Tensor = prim::If(%1910) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1908)
    block1():
      %concated_features.51 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1931 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%1876)
      %1932 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="norm1"](%1876)
      %1933 : int = aten::dim(%concated_features.51) # torch/nn/modules/batchnorm.py:276:11
      %1934 : bool = aten::ne(%1933, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1934) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1935 : bool = prim::GetAttr[name="training"](%1932)
       = prim::If(%1935) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1936 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1932)
          %1937 : Tensor = aten::add(%1936, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1932, %1937)
          -> ()
        block1():
          -> ()
      %1938 : bool = prim::GetAttr[name="training"](%1932)
      %1939 : Tensor = prim::GetAttr[name="running_mean"](%1932)
      %1940 : Tensor = prim::GetAttr[name="running_var"](%1932)
      %1941 : Tensor = prim::GetAttr[name="weight"](%1932)
      %1942 : Tensor = prim::GetAttr[name="bias"](%1932)
       = prim::If(%1938) # torch/nn/functional.py:2011:4
        block0():
          %1943 : int[] = aten::size(%concated_features.51) # torch/nn/functional.py:2012:27
          %size_prods.416 : int = aten::__getitem__(%1943, %24) # torch/nn/functional.py:1991:17
          %1945 : int = aten::len(%1943) # torch/nn/functional.py:1992:19
          %1946 : int = aten::sub(%1945, %26) # torch/nn/functional.py:1992:19
          %size_prods.417 : int = prim::Loop(%1946, %25, %size_prods.416) # torch/nn/functional.py:1992:4
            block0(%i.105 : int, %size_prods.418 : int):
              %1950 : int = aten::add(%i.105, %26) # torch/nn/functional.py:1993:27
              %1951 : int = aten::__getitem__(%1943, %1950) # torch/nn/functional.py:1993:22
              %size_prods.419 : int = aten::mul(%size_prods.418, %1951) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.419)
          %1953 : bool = aten::eq(%size_prods.417, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1953) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1954 : Tensor = aten::batch_norm(%concated_features.51, %1941, %1942, %1939, %1940, %1938, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.101 : Tensor = aten::relu_(%1954) # torch/nn/functional.py:1117:17
      %1956 : Tensor = prim::GetAttr[name="weight"](%1931)
      %1957 : Tensor? = prim::GetAttr[name="bias"](%1931)
      %1958 : int[] = prim::ListConstruct(%27, %27)
      %1959 : int[] = prim::ListConstruct(%24, %24)
      %1960 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.101 : Tensor = aten::conv2d(%result.101, %1956, %1957, %1958, %1959, %1960, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.101)
  %1962 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1876)
  %1963 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1876)
  %1964 : int = aten::dim(%bottleneck_output.100) # torch/nn/modules/batchnorm.py:276:11
  %1965 : bool = aten::ne(%1964, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1965) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1966 : bool = prim::GetAttr[name="training"](%1963)
   = prim::If(%1966) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1967 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1963)
      %1968 : Tensor = aten::add(%1967, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1963, %1968)
      -> ()
    block1():
      -> ()
  %1969 : bool = prim::GetAttr[name="training"](%1963)
  %1970 : Tensor = prim::GetAttr[name="running_mean"](%1963)
  %1971 : Tensor = prim::GetAttr[name="running_var"](%1963)
  %1972 : Tensor = prim::GetAttr[name="weight"](%1963)
  %1973 : Tensor = prim::GetAttr[name="bias"](%1963)
   = prim::If(%1969) # torch/nn/functional.py:2011:4
    block0():
      %1974 : int[] = aten::size(%bottleneck_output.100) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%1974, %24) # torch/nn/functional.py:1991:17
      %1976 : int = aten::len(%1974) # torch/nn/functional.py:1992:19
      %1977 : int = aten::sub(%1976, %26) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%1977, %25, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %1981 : int = aten::add(%i.106, %26) # torch/nn/functional.py:1993:27
          %1982 : int = aten::__getitem__(%1974, %1981) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %1982) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.423)
      %1984 : bool = aten::eq(%size_prods.421, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1984) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1985 : Tensor = aten::batch_norm(%bottleneck_output.100, %1972, %1973, %1970, %1971, %1969, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.102 : Tensor = aten::relu_(%1985) # torch/nn/functional.py:1117:17
  %1987 : Tensor = prim::GetAttr[name="weight"](%1962)
  %1988 : Tensor? = prim::GetAttr[name="bias"](%1962)
  %1989 : int[] = prim::ListConstruct(%27, %27)
  %1990 : int[] = prim::ListConstruct(%27, %27)
  %1991 : int[] = prim::ListConstruct(%27, %27)
  %new_features.101 : Tensor = aten::conv2d(%result.102, %1987, %1988, %1989, %1990, %1991, %27) # torch/nn/modules/conv.py:415:15
  %1993 : float = prim::GetAttr[name="drop_rate"](%1876)
  %1994 : bool = aten::gt(%1993, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.102 : Tensor = prim::If(%1994) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1996 : float = prim::GetAttr[name="drop_rate"](%1876)
      %1997 : bool = prim::GetAttr[name="training"](%1876)
      %1998 : bool = aten::lt(%1996, %16) # torch/nn/functional.py:968:7
      %1999 : bool = prim::If(%1998) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2000 : bool = aten::gt(%1996, %17) # torch/nn/functional.py:968:17
          -> (%2000)
       = prim::If(%1999) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2001 : Tensor = aten::dropout(%new_features.101, %1996, %1997) # torch/nn/functional.py:973:17
      -> (%2001)
    block1():
      -> (%new_features.101)
  %2002 : Tensor[] = aten::append(%features.4, %new_features.102) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2003 : Tensor = prim::Uninitialized()
  %2004 : bool = prim::GetAttr[name="memory_efficient"](%1877)
  %2005 : bool = prim::If(%2004) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2006 : bool = prim::Uninitialized()
      %2007 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2008 : bool = aten::gt(%2007, %24)
      %2009 : bool, %2010 : bool, %2011 : int = prim::Loop(%18, %2008, %19, %2006, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2012 : int, %2013 : bool, %2014 : bool, %2015 : int):
          %tensor.52 : Tensor = aten::__getitem__(%features.4, %2015) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2017 : bool = prim::requires_grad(%tensor.52)
          %2018 : bool, %2019 : bool = prim::If(%2017) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2006)
          %2020 : int = aten::add(%2015, %27)
          %2021 : bool = aten::lt(%2020, %2007)
          %2022 : bool = aten::__and__(%2021, %2018)
          -> (%2022, %2017, %2019, %2020)
      %2023 : bool = prim::If(%2009)
        block0():
          -> (%2010)
        block1():
          -> (%19)
      -> (%2023)
    block1():
      -> (%19)
  %bottleneck_output.102 : Tensor = prim::If(%2005) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2003)
    block1():
      %concated_features.52 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2026 : __torch__.torch.nn.modules.conv.___torch_mangle_89.Conv2d = prim::GetAttr[name="conv1"](%1877)
      %2027 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%1877)
      %2028 : int = aten::dim(%concated_features.52) # torch/nn/modules/batchnorm.py:276:11
      %2029 : bool = aten::ne(%2028, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2029) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2030 : bool = prim::GetAttr[name="training"](%2027)
       = prim::If(%2030) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2031 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2027)
          %2032 : Tensor = aten::add(%2031, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2027, %2032)
          -> ()
        block1():
          -> ()
      %2033 : bool = prim::GetAttr[name="training"](%2027)
      %2034 : Tensor = prim::GetAttr[name="running_mean"](%2027)
      %2035 : Tensor = prim::GetAttr[name="running_var"](%2027)
      %2036 : Tensor = prim::GetAttr[name="weight"](%2027)
      %2037 : Tensor = prim::GetAttr[name="bias"](%2027)
       = prim::If(%2033) # torch/nn/functional.py:2011:4
        block0():
          %2038 : int[] = aten::size(%concated_features.52) # torch/nn/functional.py:2012:27
          %size_prods.424 : int = aten::__getitem__(%2038, %24) # torch/nn/functional.py:1991:17
          %2040 : int = aten::len(%2038) # torch/nn/functional.py:1992:19
          %2041 : int = aten::sub(%2040, %26) # torch/nn/functional.py:1992:19
          %size_prods.425 : int = prim::Loop(%2041, %25, %size_prods.424) # torch/nn/functional.py:1992:4
            block0(%i.107 : int, %size_prods.426 : int):
              %2045 : int = aten::add(%i.107, %26) # torch/nn/functional.py:1993:27
              %2046 : int = aten::__getitem__(%2038, %2045) # torch/nn/functional.py:1993:22
              %size_prods.427 : int = aten::mul(%size_prods.426, %2046) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.427)
          %2048 : bool = aten::eq(%size_prods.425, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2048) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2049 : Tensor = aten::batch_norm(%concated_features.52, %2036, %2037, %2034, %2035, %2033, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.103 : Tensor = aten::relu_(%2049) # torch/nn/functional.py:1117:17
      %2051 : Tensor = prim::GetAttr[name="weight"](%2026)
      %2052 : Tensor? = prim::GetAttr[name="bias"](%2026)
      %2053 : int[] = prim::ListConstruct(%27, %27)
      %2054 : int[] = prim::ListConstruct(%24, %24)
      %2055 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.103 : Tensor = aten::conv2d(%result.103, %2051, %2052, %2053, %2054, %2055, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.103)
  %2057 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1877)
  %2058 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1877)
  %2059 : int = aten::dim(%bottleneck_output.102) # torch/nn/modules/batchnorm.py:276:11
  %2060 : bool = aten::ne(%2059, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2060) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2061 : bool = prim::GetAttr[name="training"](%2058)
   = prim::If(%2061) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2062 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2058)
      %2063 : Tensor = aten::add(%2062, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2058, %2063)
      -> ()
    block1():
      -> ()
  %2064 : bool = prim::GetAttr[name="training"](%2058)
  %2065 : Tensor = prim::GetAttr[name="running_mean"](%2058)
  %2066 : Tensor = prim::GetAttr[name="running_var"](%2058)
  %2067 : Tensor = prim::GetAttr[name="weight"](%2058)
  %2068 : Tensor = prim::GetAttr[name="bias"](%2058)
   = prim::If(%2064) # torch/nn/functional.py:2011:4
    block0():
      %2069 : int[] = aten::size(%bottleneck_output.102) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%2069, %24) # torch/nn/functional.py:1991:17
      %2071 : int = aten::len(%2069) # torch/nn/functional.py:1992:19
      %2072 : int = aten::sub(%2071, %26) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%2072, %25, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.430 : int):
          %2076 : int = aten::add(%i.108, %26) # torch/nn/functional.py:1993:27
          %2077 : int = aten::__getitem__(%2069, %2076) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %2077) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.431)
      %2079 : bool = aten::eq(%size_prods.429, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2079) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2080 : Tensor = aten::batch_norm(%bottleneck_output.102, %2067, %2068, %2065, %2066, %2064, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.104 : Tensor = aten::relu_(%2080) # torch/nn/functional.py:1117:17
  %2082 : Tensor = prim::GetAttr[name="weight"](%2057)
  %2083 : Tensor? = prim::GetAttr[name="bias"](%2057)
  %2084 : int[] = prim::ListConstruct(%27, %27)
  %2085 : int[] = prim::ListConstruct(%27, %27)
  %2086 : int[] = prim::ListConstruct(%27, %27)
  %new_features.103 : Tensor = aten::conv2d(%result.104, %2082, %2083, %2084, %2085, %2086, %27) # torch/nn/modules/conv.py:415:15
  %2088 : float = prim::GetAttr[name="drop_rate"](%1877)
  %2089 : bool = aten::gt(%2088, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.104 : Tensor = prim::If(%2089) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2091 : float = prim::GetAttr[name="drop_rate"](%1877)
      %2092 : bool = prim::GetAttr[name="training"](%1877)
      %2093 : bool = aten::lt(%2091, %16) # torch/nn/functional.py:968:7
      %2094 : bool = prim::If(%2093) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2095 : bool = aten::gt(%2091, %17) # torch/nn/functional.py:968:17
          -> (%2095)
       = prim::If(%2094) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2096 : Tensor = aten::dropout(%new_features.103, %2091, %2092) # torch/nn/functional.py:973:17
      -> (%2096)
    block1():
      -> (%new_features.103)
  %2097 : Tensor[] = aten::append(%features.4, %new_features.104) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2098 : Tensor = prim::Uninitialized()
  %2099 : bool = prim::GetAttr[name="memory_efficient"](%1878)
  %2100 : bool = prim::If(%2099) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2101 : bool = prim::Uninitialized()
      %2102 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2103 : bool = aten::gt(%2102, %24)
      %2104 : bool, %2105 : bool, %2106 : int = prim::Loop(%18, %2103, %19, %2101, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2107 : int, %2108 : bool, %2109 : bool, %2110 : int):
          %tensor.53 : Tensor = aten::__getitem__(%features.4, %2110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2112 : bool = prim::requires_grad(%tensor.53)
          %2113 : bool, %2114 : bool = prim::If(%2112) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2101)
          %2115 : int = aten::add(%2110, %27)
          %2116 : bool = aten::lt(%2115, %2102)
          %2117 : bool = aten::__and__(%2116, %2113)
          -> (%2117, %2112, %2114, %2115)
      %2118 : bool = prim::If(%2104)
        block0():
          -> (%2105)
        block1():
          -> (%19)
      -> (%2118)
    block1():
      -> (%19)
  %bottleneck_output.104 : Tensor = prim::If(%2100) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2098)
    block1():
      %concated_features.53 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2121 : __torch__.torch.nn.modules.conv.___torch_mangle_92.Conv2d = prim::GetAttr[name="conv1"](%1878)
      %2122 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="norm1"](%1878)
      %2123 : int = aten::dim(%concated_features.53) # torch/nn/modules/batchnorm.py:276:11
      %2124 : bool = aten::ne(%2123, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2124) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2125 : bool = prim::GetAttr[name="training"](%2122)
       = prim::If(%2125) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2126 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2122)
          %2127 : Tensor = aten::add(%2126, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2122, %2127)
          -> ()
        block1():
          -> ()
      %2128 : bool = prim::GetAttr[name="training"](%2122)
      %2129 : Tensor = prim::GetAttr[name="running_mean"](%2122)
      %2130 : Tensor = prim::GetAttr[name="running_var"](%2122)
      %2131 : Tensor = prim::GetAttr[name="weight"](%2122)
      %2132 : Tensor = prim::GetAttr[name="bias"](%2122)
       = prim::If(%2128) # torch/nn/functional.py:2011:4
        block0():
          %2133 : int[] = aten::size(%concated_features.53) # torch/nn/functional.py:2012:27
          %size_prods.432 : int = aten::__getitem__(%2133, %24) # torch/nn/functional.py:1991:17
          %2135 : int = aten::len(%2133) # torch/nn/functional.py:1992:19
          %2136 : int = aten::sub(%2135, %26) # torch/nn/functional.py:1992:19
          %size_prods.433 : int = prim::Loop(%2136, %25, %size_prods.432) # torch/nn/functional.py:1992:4
            block0(%i.109 : int, %size_prods.434 : int):
              %2140 : int = aten::add(%i.109, %26) # torch/nn/functional.py:1993:27
              %2141 : int = aten::__getitem__(%2133, %2140) # torch/nn/functional.py:1993:22
              %size_prods.435 : int = aten::mul(%size_prods.434, %2141) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.435)
          %2143 : bool = aten::eq(%size_prods.433, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2143) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2144 : Tensor = aten::batch_norm(%concated_features.53, %2131, %2132, %2129, %2130, %2128, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.105 : Tensor = aten::relu_(%2144) # torch/nn/functional.py:1117:17
      %2146 : Tensor = prim::GetAttr[name="weight"](%2121)
      %2147 : Tensor? = prim::GetAttr[name="bias"](%2121)
      %2148 : int[] = prim::ListConstruct(%27, %27)
      %2149 : int[] = prim::ListConstruct(%24, %24)
      %2150 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.105 : Tensor = aten::conv2d(%result.105, %2146, %2147, %2148, %2149, %2150, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.105)
  %2152 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1878)
  %2153 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1878)
  %2154 : int = aten::dim(%bottleneck_output.104) # torch/nn/modules/batchnorm.py:276:11
  %2155 : bool = aten::ne(%2154, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2155) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2156 : bool = prim::GetAttr[name="training"](%2153)
   = prim::If(%2156) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2157 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2153)
      %2158 : Tensor = aten::add(%2157, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2153, %2158)
      -> ()
    block1():
      -> ()
  %2159 : bool = prim::GetAttr[name="training"](%2153)
  %2160 : Tensor = prim::GetAttr[name="running_mean"](%2153)
  %2161 : Tensor = prim::GetAttr[name="running_var"](%2153)
  %2162 : Tensor = prim::GetAttr[name="weight"](%2153)
  %2163 : Tensor = prim::GetAttr[name="bias"](%2153)
   = prim::If(%2159) # torch/nn/functional.py:2011:4
    block0():
      %2164 : int[] = aten::size(%bottleneck_output.104) # torch/nn/functional.py:2012:27
      %size_prods.436 : int = aten::__getitem__(%2164, %24) # torch/nn/functional.py:1991:17
      %2166 : int = aten::len(%2164) # torch/nn/functional.py:1992:19
      %2167 : int = aten::sub(%2166, %26) # torch/nn/functional.py:1992:19
      %size_prods.437 : int = prim::Loop(%2167, %25, %size_prods.436) # torch/nn/functional.py:1992:4
        block0(%i.110 : int, %size_prods.438 : int):
          %2171 : int = aten::add(%i.110, %26) # torch/nn/functional.py:1993:27
          %2172 : int = aten::__getitem__(%2164, %2171) # torch/nn/functional.py:1993:22
          %size_prods.439 : int = aten::mul(%size_prods.438, %2172) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.439)
      %2174 : bool = aten::eq(%size_prods.437, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2174) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2175 : Tensor = aten::batch_norm(%bottleneck_output.104, %2162, %2163, %2160, %2161, %2159, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.106 : Tensor = aten::relu_(%2175) # torch/nn/functional.py:1117:17
  %2177 : Tensor = prim::GetAttr[name="weight"](%2152)
  %2178 : Tensor? = prim::GetAttr[name="bias"](%2152)
  %2179 : int[] = prim::ListConstruct(%27, %27)
  %2180 : int[] = prim::ListConstruct(%27, %27)
  %2181 : int[] = prim::ListConstruct(%27, %27)
  %new_features.105 : Tensor = aten::conv2d(%result.106, %2177, %2178, %2179, %2180, %2181, %27) # torch/nn/modules/conv.py:415:15
  %2183 : float = prim::GetAttr[name="drop_rate"](%1878)
  %2184 : bool = aten::gt(%2183, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.106 : Tensor = prim::If(%2184) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2186 : float = prim::GetAttr[name="drop_rate"](%1878)
      %2187 : bool = prim::GetAttr[name="training"](%1878)
      %2188 : bool = aten::lt(%2186, %16) # torch/nn/functional.py:968:7
      %2189 : bool = prim::If(%2188) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2190 : bool = aten::gt(%2186, %17) # torch/nn/functional.py:968:17
          -> (%2190)
       = prim::If(%2189) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2191 : Tensor = aten::dropout(%new_features.105, %2186, %2187) # torch/nn/functional.py:973:17
      -> (%2191)
    block1():
      -> (%new_features.105)
  %2192 : Tensor[] = aten::append(%features.4, %new_features.106) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2193 : Tensor = prim::Uninitialized()
  %2194 : bool = prim::GetAttr[name="memory_efficient"](%1879)
  %2195 : bool = prim::If(%2194) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2196 : bool = prim::Uninitialized()
      %2197 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2198 : bool = aten::gt(%2197, %24)
      %2199 : bool, %2200 : bool, %2201 : int = prim::Loop(%18, %2198, %19, %2196, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2202 : int, %2203 : bool, %2204 : bool, %2205 : int):
          %tensor.54 : Tensor = aten::__getitem__(%features.4, %2205) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2207 : bool = prim::requires_grad(%tensor.54)
          %2208 : bool, %2209 : bool = prim::If(%2207) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2196)
          %2210 : int = aten::add(%2205, %27)
          %2211 : bool = aten::lt(%2210, %2197)
          %2212 : bool = aten::__and__(%2211, %2208)
          -> (%2212, %2207, %2209, %2210)
      %2213 : bool = prim::If(%2199)
        block0():
          -> (%2200)
        block1():
          -> (%19)
      -> (%2213)
    block1():
      -> (%19)
  %bottleneck_output.106 : Tensor = prim::If(%2195) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2193)
    block1():
      %concated_features.54 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2216 : __torch__.torch.nn.modules.conv.___torch_mangle_95.Conv2d = prim::GetAttr[name="conv1"](%1879)
      %2217 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_94.BatchNorm2d = prim::GetAttr[name="norm1"](%1879)
      %2218 : int = aten::dim(%concated_features.54) # torch/nn/modules/batchnorm.py:276:11
      %2219 : bool = aten::ne(%2218, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2219) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2220 : bool = prim::GetAttr[name="training"](%2217)
       = prim::If(%2220) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2221 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2217)
          %2222 : Tensor = aten::add(%2221, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2217, %2222)
          -> ()
        block1():
          -> ()
      %2223 : bool = prim::GetAttr[name="training"](%2217)
      %2224 : Tensor = prim::GetAttr[name="running_mean"](%2217)
      %2225 : Tensor = prim::GetAttr[name="running_var"](%2217)
      %2226 : Tensor = prim::GetAttr[name="weight"](%2217)
      %2227 : Tensor = prim::GetAttr[name="bias"](%2217)
       = prim::If(%2223) # torch/nn/functional.py:2011:4
        block0():
          %2228 : int[] = aten::size(%concated_features.54) # torch/nn/functional.py:2012:27
          %size_prods.440 : int = aten::__getitem__(%2228, %24) # torch/nn/functional.py:1991:17
          %2230 : int = aten::len(%2228) # torch/nn/functional.py:1992:19
          %2231 : int = aten::sub(%2230, %26) # torch/nn/functional.py:1992:19
          %size_prods.441 : int = prim::Loop(%2231, %25, %size_prods.440) # torch/nn/functional.py:1992:4
            block0(%i.111 : int, %size_prods.442 : int):
              %2235 : int = aten::add(%i.111, %26) # torch/nn/functional.py:1993:27
              %2236 : int = aten::__getitem__(%2228, %2235) # torch/nn/functional.py:1993:22
              %size_prods.443 : int = aten::mul(%size_prods.442, %2236) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.443)
          %2238 : bool = aten::eq(%size_prods.441, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2238) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2239 : Tensor = aten::batch_norm(%concated_features.54, %2226, %2227, %2224, %2225, %2223, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.107 : Tensor = aten::relu_(%2239) # torch/nn/functional.py:1117:17
      %2241 : Tensor = prim::GetAttr[name="weight"](%2216)
      %2242 : Tensor? = prim::GetAttr[name="bias"](%2216)
      %2243 : int[] = prim::ListConstruct(%27, %27)
      %2244 : int[] = prim::ListConstruct(%24, %24)
      %2245 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.107 : Tensor = aten::conv2d(%result.107, %2241, %2242, %2243, %2244, %2245, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.107)
  %2247 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1879)
  %2248 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1879)
  %2249 : int = aten::dim(%bottleneck_output.106) # torch/nn/modules/batchnorm.py:276:11
  %2250 : bool = aten::ne(%2249, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2250) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2251 : bool = prim::GetAttr[name="training"](%2248)
   = prim::If(%2251) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2252 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2248)
      %2253 : Tensor = aten::add(%2252, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2248, %2253)
      -> ()
    block1():
      -> ()
  %2254 : bool = prim::GetAttr[name="training"](%2248)
  %2255 : Tensor = prim::GetAttr[name="running_mean"](%2248)
  %2256 : Tensor = prim::GetAttr[name="running_var"](%2248)
  %2257 : Tensor = prim::GetAttr[name="weight"](%2248)
  %2258 : Tensor = prim::GetAttr[name="bias"](%2248)
   = prim::If(%2254) # torch/nn/functional.py:2011:4
    block0():
      %2259 : int[] = aten::size(%bottleneck_output.106) # torch/nn/functional.py:2012:27
      %size_prods.444 : int = aten::__getitem__(%2259, %24) # torch/nn/functional.py:1991:17
      %2261 : int = aten::len(%2259) # torch/nn/functional.py:1992:19
      %2262 : int = aten::sub(%2261, %26) # torch/nn/functional.py:1992:19
      %size_prods.445 : int = prim::Loop(%2262, %25, %size_prods.444) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.446 : int):
          %2266 : int = aten::add(%i.112, %26) # torch/nn/functional.py:1993:27
          %2267 : int = aten::__getitem__(%2259, %2266) # torch/nn/functional.py:1993:22
          %size_prods.447 : int = aten::mul(%size_prods.446, %2267) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.447)
      %2269 : bool = aten::eq(%size_prods.445, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2269) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2270 : Tensor = aten::batch_norm(%bottleneck_output.106, %2257, %2258, %2255, %2256, %2254, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.108 : Tensor = aten::relu_(%2270) # torch/nn/functional.py:1117:17
  %2272 : Tensor = prim::GetAttr[name="weight"](%2247)
  %2273 : Tensor? = prim::GetAttr[name="bias"](%2247)
  %2274 : int[] = prim::ListConstruct(%27, %27)
  %2275 : int[] = prim::ListConstruct(%27, %27)
  %2276 : int[] = prim::ListConstruct(%27, %27)
  %new_features.107 : Tensor = aten::conv2d(%result.108, %2272, %2273, %2274, %2275, %2276, %27) # torch/nn/modules/conv.py:415:15
  %2278 : float = prim::GetAttr[name="drop_rate"](%1879)
  %2279 : bool = aten::gt(%2278, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.108 : Tensor = prim::If(%2279) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2281 : float = prim::GetAttr[name="drop_rate"](%1879)
      %2282 : bool = prim::GetAttr[name="training"](%1879)
      %2283 : bool = aten::lt(%2281, %16) # torch/nn/functional.py:968:7
      %2284 : bool = prim::If(%2283) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2285 : bool = aten::gt(%2281, %17) # torch/nn/functional.py:968:17
          -> (%2285)
       = prim::If(%2284) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2286 : Tensor = aten::dropout(%new_features.107, %2281, %2282) # torch/nn/functional.py:973:17
      -> (%2286)
    block1():
      -> (%new_features.107)
  %2287 : Tensor[] = aten::append(%features.4, %new_features.108) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2288 : Tensor = prim::Uninitialized()
  %2289 : bool = prim::GetAttr[name="memory_efficient"](%1880)
  %2290 : bool = prim::If(%2289) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2291 : bool = prim::Uninitialized()
      %2292 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2293 : bool = aten::gt(%2292, %24)
      %2294 : bool, %2295 : bool, %2296 : int = prim::Loop(%18, %2293, %19, %2291, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2297 : int, %2298 : bool, %2299 : bool, %2300 : int):
          %tensor.55 : Tensor = aten::__getitem__(%features.4, %2300) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2302 : bool = prim::requires_grad(%tensor.55)
          %2303 : bool, %2304 : bool = prim::If(%2302) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2291)
          %2305 : int = aten::add(%2300, %27)
          %2306 : bool = aten::lt(%2305, %2292)
          %2307 : bool = aten::__and__(%2306, %2303)
          -> (%2307, %2302, %2304, %2305)
      %2308 : bool = prim::If(%2294)
        block0():
          -> (%2295)
        block1():
          -> (%19)
      -> (%2308)
    block1():
      -> (%19)
  %bottleneck_output.108 : Tensor = prim::If(%2290) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2288)
    block1():
      %concated_features.55 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2311 : __torch__.torch.nn.modules.conv.___torch_mangle_98.Conv2d = prim::GetAttr[name="conv1"](%1880)
      %2312 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%1880)
      %2313 : int = aten::dim(%concated_features.55) # torch/nn/modules/batchnorm.py:276:11
      %2314 : bool = aten::ne(%2313, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2314) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2315 : bool = prim::GetAttr[name="training"](%2312)
       = prim::If(%2315) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2316 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2312)
          %2317 : Tensor = aten::add(%2316, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2312, %2317)
          -> ()
        block1():
          -> ()
      %2318 : bool = prim::GetAttr[name="training"](%2312)
      %2319 : Tensor = prim::GetAttr[name="running_mean"](%2312)
      %2320 : Tensor = prim::GetAttr[name="running_var"](%2312)
      %2321 : Tensor = prim::GetAttr[name="weight"](%2312)
      %2322 : Tensor = prim::GetAttr[name="bias"](%2312)
       = prim::If(%2318) # torch/nn/functional.py:2011:4
        block0():
          %2323 : int[] = aten::size(%concated_features.55) # torch/nn/functional.py:2012:27
          %size_prods.448 : int = aten::__getitem__(%2323, %24) # torch/nn/functional.py:1991:17
          %2325 : int = aten::len(%2323) # torch/nn/functional.py:1992:19
          %2326 : int = aten::sub(%2325, %26) # torch/nn/functional.py:1992:19
          %size_prods.449 : int = prim::Loop(%2326, %25, %size_prods.448) # torch/nn/functional.py:1992:4
            block0(%i.113 : int, %size_prods.450 : int):
              %2330 : int = aten::add(%i.113, %26) # torch/nn/functional.py:1993:27
              %2331 : int = aten::__getitem__(%2323, %2330) # torch/nn/functional.py:1993:22
              %size_prods.451 : int = aten::mul(%size_prods.450, %2331) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.451)
          %2333 : bool = aten::eq(%size_prods.449, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2333) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2334 : Tensor = aten::batch_norm(%concated_features.55, %2321, %2322, %2319, %2320, %2318, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.109 : Tensor = aten::relu_(%2334) # torch/nn/functional.py:1117:17
      %2336 : Tensor = prim::GetAttr[name="weight"](%2311)
      %2337 : Tensor? = prim::GetAttr[name="bias"](%2311)
      %2338 : int[] = prim::ListConstruct(%27, %27)
      %2339 : int[] = prim::ListConstruct(%24, %24)
      %2340 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.109 : Tensor = aten::conv2d(%result.109, %2336, %2337, %2338, %2339, %2340, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.109)
  %2342 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1880)
  %2343 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1880)
  %2344 : int = aten::dim(%bottleneck_output.108) # torch/nn/modules/batchnorm.py:276:11
  %2345 : bool = aten::ne(%2344, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2345) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2346 : bool = prim::GetAttr[name="training"](%2343)
   = prim::If(%2346) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2343)
      %2348 : Tensor = aten::add(%2347, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2343, %2348)
      -> ()
    block1():
      -> ()
  %2349 : bool = prim::GetAttr[name="training"](%2343)
  %2350 : Tensor = prim::GetAttr[name="running_mean"](%2343)
  %2351 : Tensor = prim::GetAttr[name="running_var"](%2343)
  %2352 : Tensor = prim::GetAttr[name="weight"](%2343)
  %2353 : Tensor = prim::GetAttr[name="bias"](%2343)
   = prim::If(%2349) # torch/nn/functional.py:2011:4
    block0():
      %2354 : int[] = aten::size(%bottleneck_output.108) # torch/nn/functional.py:2012:27
      %size_prods.452 : int = aten::__getitem__(%2354, %24) # torch/nn/functional.py:1991:17
      %2356 : int = aten::len(%2354) # torch/nn/functional.py:1992:19
      %2357 : int = aten::sub(%2356, %26) # torch/nn/functional.py:1992:19
      %size_prods.453 : int = prim::Loop(%2357, %25, %size_prods.452) # torch/nn/functional.py:1992:4
        block0(%i.114 : int, %size_prods.454 : int):
          %2361 : int = aten::add(%i.114, %26) # torch/nn/functional.py:1993:27
          %2362 : int = aten::__getitem__(%2354, %2361) # torch/nn/functional.py:1993:22
          %size_prods.455 : int = aten::mul(%size_prods.454, %2362) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.455)
      %2364 : bool = aten::eq(%size_prods.453, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2364) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2365 : Tensor = aten::batch_norm(%bottleneck_output.108, %2352, %2353, %2350, %2351, %2349, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.110 : Tensor = aten::relu_(%2365) # torch/nn/functional.py:1117:17
  %2367 : Tensor = prim::GetAttr[name="weight"](%2342)
  %2368 : Tensor? = prim::GetAttr[name="bias"](%2342)
  %2369 : int[] = prim::ListConstruct(%27, %27)
  %2370 : int[] = prim::ListConstruct(%27, %27)
  %2371 : int[] = prim::ListConstruct(%27, %27)
  %new_features.109 : Tensor = aten::conv2d(%result.110, %2367, %2368, %2369, %2370, %2371, %27) # torch/nn/modules/conv.py:415:15
  %2373 : float = prim::GetAttr[name="drop_rate"](%1880)
  %2374 : bool = aten::gt(%2373, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.110 : Tensor = prim::If(%2374) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2376 : float = prim::GetAttr[name="drop_rate"](%1880)
      %2377 : bool = prim::GetAttr[name="training"](%1880)
      %2378 : bool = aten::lt(%2376, %16) # torch/nn/functional.py:968:7
      %2379 : bool = prim::If(%2378) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2380 : bool = aten::gt(%2376, %17) # torch/nn/functional.py:968:17
          -> (%2380)
       = prim::If(%2379) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2381 : Tensor = aten::dropout(%new_features.109, %2376, %2377) # torch/nn/functional.py:973:17
      -> (%2381)
    block1():
      -> (%new_features.109)
  %2382 : Tensor[] = aten::append(%features.4, %new_features.110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2383 : Tensor = prim::Uninitialized()
  %2384 : bool = prim::GetAttr[name="memory_efficient"](%1881)
  %2385 : bool = prim::If(%2384) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2386 : bool = prim::Uninitialized()
      %2387 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2388 : bool = aten::gt(%2387, %24)
      %2389 : bool, %2390 : bool, %2391 : int = prim::Loop(%18, %2388, %19, %2386, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2392 : int, %2393 : bool, %2394 : bool, %2395 : int):
          %tensor.56 : Tensor = aten::__getitem__(%features.4, %2395) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2397 : bool = prim::requires_grad(%tensor.56)
          %2398 : bool, %2399 : bool = prim::If(%2397) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2386)
          %2400 : int = aten::add(%2395, %27)
          %2401 : bool = aten::lt(%2400, %2387)
          %2402 : bool = aten::__and__(%2401, %2398)
          -> (%2402, %2397, %2399, %2400)
      %2403 : bool = prim::If(%2389)
        block0():
          -> (%2390)
        block1():
          -> (%19)
      -> (%2403)
    block1():
      -> (%19)
  %bottleneck_output.110 : Tensor = prim::If(%2385) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2383)
    block1():
      %concated_features.56 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2406 : __torch__.torch.nn.modules.conv.___torch_mangle_101.Conv2d = prim::GetAttr[name="conv1"](%1881)
      %2407 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_100.BatchNorm2d = prim::GetAttr[name="norm1"](%1881)
      %2408 : int = aten::dim(%concated_features.56) # torch/nn/modules/batchnorm.py:276:11
      %2409 : bool = aten::ne(%2408, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2409) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2410 : bool = prim::GetAttr[name="training"](%2407)
       = prim::If(%2410) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2411 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2407)
          %2412 : Tensor = aten::add(%2411, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2407, %2412)
          -> ()
        block1():
          -> ()
      %2413 : bool = prim::GetAttr[name="training"](%2407)
      %2414 : Tensor = prim::GetAttr[name="running_mean"](%2407)
      %2415 : Tensor = prim::GetAttr[name="running_var"](%2407)
      %2416 : Tensor = prim::GetAttr[name="weight"](%2407)
      %2417 : Tensor = prim::GetAttr[name="bias"](%2407)
       = prim::If(%2413) # torch/nn/functional.py:2011:4
        block0():
          %2418 : int[] = aten::size(%concated_features.56) # torch/nn/functional.py:2012:27
          %size_prods.456 : int = aten::__getitem__(%2418, %24) # torch/nn/functional.py:1991:17
          %2420 : int = aten::len(%2418) # torch/nn/functional.py:1992:19
          %2421 : int = aten::sub(%2420, %26) # torch/nn/functional.py:1992:19
          %size_prods.457 : int = prim::Loop(%2421, %25, %size_prods.456) # torch/nn/functional.py:1992:4
            block0(%i.115 : int, %size_prods.458 : int):
              %2425 : int = aten::add(%i.115, %26) # torch/nn/functional.py:1993:27
              %2426 : int = aten::__getitem__(%2418, %2425) # torch/nn/functional.py:1993:22
              %size_prods.459 : int = aten::mul(%size_prods.458, %2426) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.459)
          %2428 : bool = aten::eq(%size_prods.457, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2428) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2429 : Tensor = aten::batch_norm(%concated_features.56, %2416, %2417, %2414, %2415, %2413, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.111 : Tensor = aten::relu_(%2429) # torch/nn/functional.py:1117:17
      %2431 : Tensor = prim::GetAttr[name="weight"](%2406)
      %2432 : Tensor? = prim::GetAttr[name="bias"](%2406)
      %2433 : int[] = prim::ListConstruct(%27, %27)
      %2434 : int[] = prim::ListConstruct(%24, %24)
      %2435 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.111 : Tensor = aten::conv2d(%result.111, %2431, %2432, %2433, %2434, %2435, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.111)
  %2437 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1881)
  %2438 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1881)
  %2439 : int = aten::dim(%bottleneck_output.110) # torch/nn/modules/batchnorm.py:276:11
  %2440 : bool = aten::ne(%2439, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2440) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2441 : bool = prim::GetAttr[name="training"](%2438)
   = prim::If(%2441) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2442 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2438)
      %2443 : Tensor = aten::add(%2442, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2438, %2443)
      -> ()
    block1():
      -> ()
  %2444 : bool = prim::GetAttr[name="training"](%2438)
  %2445 : Tensor = prim::GetAttr[name="running_mean"](%2438)
  %2446 : Tensor = prim::GetAttr[name="running_var"](%2438)
  %2447 : Tensor = prim::GetAttr[name="weight"](%2438)
  %2448 : Tensor = prim::GetAttr[name="bias"](%2438)
   = prim::If(%2444) # torch/nn/functional.py:2011:4
    block0():
      %2449 : int[] = aten::size(%bottleneck_output.110) # torch/nn/functional.py:2012:27
      %size_prods.460 : int = aten::__getitem__(%2449, %24) # torch/nn/functional.py:1991:17
      %2451 : int = aten::len(%2449) # torch/nn/functional.py:1992:19
      %2452 : int = aten::sub(%2451, %26) # torch/nn/functional.py:1992:19
      %size_prods.461 : int = prim::Loop(%2452, %25, %size_prods.460) # torch/nn/functional.py:1992:4
        block0(%i.116 : int, %size_prods.462 : int):
          %2456 : int = aten::add(%i.116, %26) # torch/nn/functional.py:1993:27
          %2457 : int = aten::__getitem__(%2449, %2456) # torch/nn/functional.py:1993:22
          %size_prods.463 : int = aten::mul(%size_prods.462, %2457) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.463)
      %2459 : bool = aten::eq(%size_prods.461, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2459) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2460 : Tensor = aten::batch_norm(%bottleneck_output.110, %2447, %2448, %2445, %2446, %2444, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.112 : Tensor = aten::relu_(%2460) # torch/nn/functional.py:1117:17
  %2462 : Tensor = prim::GetAttr[name="weight"](%2437)
  %2463 : Tensor? = prim::GetAttr[name="bias"](%2437)
  %2464 : int[] = prim::ListConstruct(%27, %27)
  %2465 : int[] = prim::ListConstruct(%27, %27)
  %2466 : int[] = prim::ListConstruct(%27, %27)
  %new_features.111 : Tensor = aten::conv2d(%result.112, %2462, %2463, %2464, %2465, %2466, %27) # torch/nn/modules/conv.py:415:15
  %2468 : float = prim::GetAttr[name="drop_rate"](%1881)
  %2469 : bool = aten::gt(%2468, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.112 : Tensor = prim::If(%2469) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2471 : float = prim::GetAttr[name="drop_rate"](%1881)
      %2472 : bool = prim::GetAttr[name="training"](%1881)
      %2473 : bool = aten::lt(%2471, %16) # torch/nn/functional.py:968:7
      %2474 : bool = prim::If(%2473) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2475 : bool = aten::gt(%2471, %17) # torch/nn/functional.py:968:17
          -> (%2475)
       = prim::If(%2474) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2476 : Tensor = aten::dropout(%new_features.111, %2471, %2472) # torch/nn/functional.py:973:17
      -> (%2476)
    block1():
      -> (%new_features.111)
  %2477 : Tensor[] = aten::append(%features.4, %new_features.112) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2478 : Tensor = prim::Uninitialized()
  %2479 : bool = prim::GetAttr[name="memory_efficient"](%1882)
  %2480 : bool = prim::If(%2479) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2481 : bool = prim::Uninitialized()
      %2482 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2483 : bool = aten::gt(%2482, %24)
      %2484 : bool, %2485 : bool, %2486 : int = prim::Loop(%18, %2483, %19, %2481, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2487 : int, %2488 : bool, %2489 : bool, %2490 : int):
          %tensor.57 : Tensor = aten::__getitem__(%features.4, %2490) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2492 : bool = prim::requires_grad(%tensor.57)
          %2493 : bool, %2494 : bool = prim::If(%2492) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2481)
          %2495 : int = aten::add(%2490, %27)
          %2496 : bool = aten::lt(%2495, %2482)
          %2497 : bool = aten::__and__(%2496, %2493)
          -> (%2497, %2492, %2494, %2495)
      %2498 : bool = prim::If(%2484)
        block0():
          -> (%2485)
        block1():
          -> (%19)
      -> (%2498)
    block1():
      -> (%19)
  %bottleneck_output.112 : Tensor = prim::If(%2480) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2478)
    block1():
      %concated_features.57 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2501 : __torch__.torch.nn.modules.conv.___torch_mangle_104.Conv2d = prim::GetAttr[name="conv1"](%1882)
      %2502 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_103.BatchNorm2d = prim::GetAttr[name="norm1"](%1882)
      %2503 : int = aten::dim(%concated_features.57) # torch/nn/modules/batchnorm.py:276:11
      %2504 : bool = aten::ne(%2503, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2504) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2505 : bool = prim::GetAttr[name="training"](%2502)
       = prim::If(%2505) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2506 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2502)
          %2507 : Tensor = aten::add(%2506, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2502, %2507)
          -> ()
        block1():
          -> ()
      %2508 : bool = prim::GetAttr[name="training"](%2502)
      %2509 : Tensor = prim::GetAttr[name="running_mean"](%2502)
      %2510 : Tensor = prim::GetAttr[name="running_var"](%2502)
      %2511 : Tensor = prim::GetAttr[name="weight"](%2502)
      %2512 : Tensor = prim::GetAttr[name="bias"](%2502)
       = prim::If(%2508) # torch/nn/functional.py:2011:4
        block0():
          %2513 : int[] = aten::size(%concated_features.57) # torch/nn/functional.py:2012:27
          %size_prods.464 : int = aten::__getitem__(%2513, %24) # torch/nn/functional.py:1991:17
          %2515 : int = aten::len(%2513) # torch/nn/functional.py:1992:19
          %2516 : int = aten::sub(%2515, %26) # torch/nn/functional.py:1992:19
          %size_prods.465 : int = prim::Loop(%2516, %25, %size_prods.464) # torch/nn/functional.py:1992:4
            block0(%i.117 : int, %size_prods.466 : int):
              %2520 : int = aten::add(%i.117, %26) # torch/nn/functional.py:1993:27
              %2521 : int = aten::__getitem__(%2513, %2520) # torch/nn/functional.py:1993:22
              %size_prods.467 : int = aten::mul(%size_prods.466, %2521) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.467)
          %2523 : bool = aten::eq(%size_prods.465, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2523) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2524 : Tensor = aten::batch_norm(%concated_features.57, %2511, %2512, %2509, %2510, %2508, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.113 : Tensor = aten::relu_(%2524) # torch/nn/functional.py:1117:17
      %2526 : Tensor = prim::GetAttr[name="weight"](%2501)
      %2527 : Tensor? = prim::GetAttr[name="bias"](%2501)
      %2528 : int[] = prim::ListConstruct(%27, %27)
      %2529 : int[] = prim::ListConstruct(%24, %24)
      %2530 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.113 : Tensor = aten::conv2d(%result.113, %2526, %2527, %2528, %2529, %2530, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.113)
  %2532 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1882)
  %2533 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1882)
  %2534 : int = aten::dim(%bottleneck_output.112) # torch/nn/modules/batchnorm.py:276:11
  %2535 : bool = aten::ne(%2534, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2535) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2536 : bool = prim::GetAttr[name="training"](%2533)
   = prim::If(%2536) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2537 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2533)
      %2538 : Tensor = aten::add(%2537, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2533, %2538)
      -> ()
    block1():
      -> ()
  %2539 : bool = prim::GetAttr[name="training"](%2533)
  %2540 : Tensor = prim::GetAttr[name="running_mean"](%2533)
  %2541 : Tensor = prim::GetAttr[name="running_var"](%2533)
  %2542 : Tensor = prim::GetAttr[name="weight"](%2533)
  %2543 : Tensor = prim::GetAttr[name="bias"](%2533)
   = prim::If(%2539) # torch/nn/functional.py:2011:4
    block0():
      %2544 : int[] = aten::size(%bottleneck_output.112) # torch/nn/functional.py:2012:27
      %size_prods.468 : int = aten::__getitem__(%2544, %24) # torch/nn/functional.py:1991:17
      %2546 : int = aten::len(%2544) # torch/nn/functional.py:1992:19
      %2547 : int = aten::sub(%2546, %26) # torch/nn/functional.py:1992:19
      %size_prods.469 : int = prim::Loop(%2547, %25, %size_prods.468) # torch/nn/functional.py:1992:4
        block0(%i.118 : int, %size_prods.470 : int):
          %2551 : int = aten::add(%i.118, %26) # torch/nn/functional.py:1993:27
          %2552 : int = aten::__getitem__(%2544, %2551) # torch/nn/functional.py:1993:22
          %size_prods.471 : int = aten::mul(%size_prods.470, %2552) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.471)
      %2554 : bool = aten::eq(%size_prods.469, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2554) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2555 : Tensor = aten::batch_norm(%bottleneck_output.112, %2542, %2543, %2540, %2541, %2539, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.114 : Tensor = aten::relu_(%2555) # torch/nn/functional.py:1117:17
  %2557 : Tensor = prim::GetAttr[name="weight"](%2532)
  %2558 : Tensor? = prim::GetAttr[name="bias"](%2532)
  %2559 : int[] = prim::ListConstruct(%27, %27)
  %2560 : int[] = prim::ListConstruct(%27, %27)
  %2561 : int[] = prim::ListConstruct(%27, %27)
  %new_features.113 : Tensor = aten::conv2d(%result.114, %2557, %2558, %2559, %2560, %2561, %27) # torch/nn/modules/conv.py:415:15
  %2563 : float = prim::GetAttr[name="drop_rate"](%1882)
  %2564 : bool = aten::gt(%2563, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.114 : Tensor = prim::If(%2564) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2566 : float = prim::GetAttr[name="drop_rate"](%1882)
      %2567 : bool = prim::GetAttr[name="training"](%1882)
      %2568 : bool = aten::lt(%2566, %16) # torch/nn/functional.py:968:7
      %2569 : bool = prim::If(%2568) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2570 : bool = aten::gt(%2566, %17) # torch/nn/functional.py:968:17
          -> (%2570)
       = prim::If(%2569) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2571 : Tensor = aten::dropout(%new_features.113, %2566, %2567) # torch/nn/functional.py:973:17
      -> (%2571)
    block1():
      -> (%new_features.113)
  %2572 : Tensor[] = aten::append(%features.4, %new_features.114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2573 : Tensor = prim::Uninitialized()
  %2574 : bool = prim::GetAttr[name="memory_efficient"](%1883)
  %2575 : bool = prim::If(%2574) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2576 : bool = prim::Uninitialized()
      %2577 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2578 : bool = aten::gt(%2577, %24)
      %2579 : bool, %2580 : bool, %2581 : int = prim::Loop(%18, %2578, %19, %2576, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2582 : int, %2583 : bool, %2584 : bool, %2585 : int):
          %tensor.58 : Tensor = aten::__getitem__(%features.4, %2585) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2587 : bool = prim::requires_grad(%tensor.58)
          %2588 : bool, %2589 : bool = prim::If(%2587) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2576)
          %2590 : int = aten::add(%2585, %27)
          %2591 : bool = aten::lt(%2590, %2577)
          %2592 : bool = aten::__and__(%2591, %2588)
          -> (%2592, %2587, %2589, %2590)
      %2593 : bool = prim::If(%2579)
        block0():
          -> (%2580)
        block1():
          -> (%19)
      -> (%2593)
    block1():
      -> (%19)
  %bottleneck_output.114 : Tensor = prim::If(%2575) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2573)
    block1():
      %concated_features.58 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2596 : __torch__.torch.nn.modules.conv.___torch_mangle_107.Conv2d = prim::GetAttr[name="conv1"](%1883)
      %2597 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%1883)
      %2598 : int = aten::dim(%concated_features.58) # torch/nn/modules/batchnorm.py:276:11
      %2599 : bool = aten::ne(%2598, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2599) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2600 : bool = prim::GetAttr[name="training"](%2597)
       = prim::If(%2600) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2601 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2597)
          %2602 : Tensor = aten::add(%2601, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2597, %2602)
          -> ()
        block1():
          -> ()
      %2603 : bool = prim::GetAttr[name="training"](%2597)
      %2604 : Tensor = prim::GetAttr[name="running_mean"](%2597)
      %2605 : Tensor = prim::GetAttr[name="running_var"](%2597)
      %2606 : Tensor = prim::GetAttr[name="weight"](%2597)
      %2607 : Tensor = prim::GetAttr[name="bias"](%2597)
       = prim::If(%2603) # torch/nn/functional.py:2011:4
        block0():
          %2608 : int[] = aten::size(%concated_features.58) # torch/nn/functional.py:2012:27
          %size_prods.472 : int = aten::__getitem__(%2608, %24) # torch/nn/functional.py:1991:17
          %2610 : int = aten::len(%2608) # torch/nn/functional.py:1992:19
          %2611 : int = aten::sub(%2610, %26) # torch/nn/functional.py:1992:19
          %size_prods.473 : int = prim::Loop(%2611, %25, %size_prods.472) # torch/nn/functional.py:1992:4
            block0(%i.119 : int, %size_prods.474 : int):
              %2615 : int = aten::add(%i.119, %26) # torch/nn/functional.py:1993:27
              %2616 : int = aten::__getitem__(%2608, %2615) # torch/nn/functional.py:1993:22
              %size_prods.475 : int = aten::mul(%size_prods.474, %2616) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.475)
          %2618 : bool = aten::eq(%size_prods.473, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2618) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2619 : Tensor = aten::batch_norm(%concated_features.58, %2606, %2607, %2604, %2605, %2603, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.115 : Tensor = aten::relu_(%2619) # torch/nn/functional.py:1117:17
      %2621 : Tensor = prim::GetAttr[name="weight"](%2596)
      %2622 : Tensor? = prim::GetAttr[name="bias"](%2596)
      %2623 : int[] = prim::ListConstruct(%27, %27)
      %2624 : int[] = prim::ListConstruct(%24, %24)
      %2625 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.115 : Tensor = aten::conv2d(%result.115, %2621, %2622, %2623, %2624, %2625, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.115)
  %2627 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1883)
  %2628 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1883)
  %2629 : int = aten::dim(%bottleneck_output.114) # torch/nn/modules/batchnorm.py:276:11
  %2630 : bool = aten::ne(%2629, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2630) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2631 : bool = prim::GetAttr[name="training"](%2628)
   = prim::If(%2631) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2632 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2628)
      %2633 : Tensor = aten::add(%2632, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2628, %2633)
      -> ()
    block1():
      -> ()
  %2634 : bool = prim::GetAttr[name="training"](%2628)
  %2635 : Tensor = prim::GetAttr[name="running_mean"](%2628)
  %2636 : Tensor = prim::GetAttr[name="running_var"](%2628)
  %2637 : Tensor = prim::GetAttr[name="weight"](%2628)
  %2638 : Tensor = prim::GetAttr[name="bias"](%2628)
   = prim::If(%2634) # torch/nn/functional.py:2011:4
    block0():
      %2639 : int[] = aten::size(%bottleneck_output.114) # torch/nn/functional.py:2012:27
      %size_prods.476 : int = aten::__getitem__(%2639, %24) # torch/nn/functional.py:1991:17
      %2641 : int = aten::len(%2639) # torch/nn/functional.py:1992:19
      %2642 : int = aten::sub(%2641, %26) # torch/nn/functional.py:1992:19
      %size_prods.477 : int = prim::Loop(%2642, %25, %size_prods.476) # torch/nn/functional.py:1992:4
        block0(%i.120 : int, %size_prods.478 : int):
          %2646 : int = aten::add(%i.120, %26) # torch/nn/functional.py:1993:27
          %2647 : int = aten::__getitem__(%2639, %2646) # torch/nn/functional.py:1993:22
          %size_prods.479 : int = aten::mul(%size_prods.478, %2647) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.479)
      %2649 : bool = aten::eq(%size_prods.477, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2649) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2650 : Tensor = aten::batch_norm(%bottleneck_output.114, %2637, %2638, %2635, %2636, %2634, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.116 : Tensor = aten::relu_(%2650) # torch/nn/functional.py:1117:17
  %2652 : Tensor = prim::GetAttr[name="weight"](%2627)
  %2653 : Tensor? = prim::GetAttr[name="bias"](%2627)
  %2654 : int[] = prim::ListConstruct(%27, %27)
  %2655 : int[] = prim::ListConstruct(%27, %27)
  %2656 : int[] = prim::ListConstruct(%27, %27)
  %new_features.115 : Tensor = aten::conv2d(%result.116, %2652, %2653, %2654, %2655, %2656, %27) # torch/nn/modules/conv.py:415:15
  %2658 : float = prim::GetAttr[name="drop_rate"](%1883)
  %2659 : bool = aten::gt(%2658, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.116 : Tensor = prim::If(%2659) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2661 : float = prim::GetAttr[name="drop_rate"](%1883)
      %2662 : bool = prim::GetAttr[name="training"](%1883)
      %2663 : bool = aten::lt(%2661, %16) # torch/nn/functional.py:968:7
      %2664 : bool = prim::If(%2663) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2665 : bool = aten::gt(%2661, %17) # torch/nn/functional.py:968:17
          -> (%2665)
       = prim::If(%2664) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2666 : Tensor = aten::dropout(%new_features.115, %2661, %2662) # torch/nn/functional.py:973:17
      -> (%2666)
    block1():
      -> (%new_features.115)
  %2667 : Tensor[] = aten::append(%features.4, %new_features.116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2668 : Tensor = prim::Uninitialized()
  %2669 : bool = prim::GetAttr[name="memory_efficient"](%1884)
  %2670 : bool = prim::If(%2669) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2671 : bool = prim::Uninitialized()
      %2672 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2673 : bool = aten::gt(%2672, %24)
      %2674 : bool, %2675 : bool, %2676 : int = prim::Loop(%18, %2673, %19, %2671, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2677 : int, %2678 : bool, %2679 : bool, %2680 : int):
          %tensor.59 : Tensor = aten::__getitem__(%features.4, %2680) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2682 : bool = prim::requires_grad(%tensor.59)
          %2683 : bool, %2684 : bool = prim::If(%2682) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2671)
          %2685 : int = aten::add(%2680, %27)
          %2686 : bool = aten::lt(%2685, %2672)
          %2687 : bool = aten::__and__(%2686, %2683)
          -> (%2687, %2682, %2684, %2685)
      %2688 : bool = prim::If(%2674)
        block0():
          -> (%2675)
        block1():
          -> (%19)
      -> (%2688)
    block1():
      -> (%19)
  %bottleneck_output.116 : Tensor = prim::If(%2670) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2668)
    block1():
      %concated_features.59 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2691 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%1884)
      %2692 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="norm1"](%1884)
      %2693 : int = aten::dim(%concated_features.59) # torch/nn/modules/batchnorm.py:276:11
      %2694 : bool = aten::ne(%2693, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2694) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2695 : bool = prim::GetAttr[name="training"](%2692)
       = prim::If(%2695) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2696 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2692)
          %2697 : Tensor = aten::add(%2696, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2692, %2697)
          -> ()
        block1():
          -> ()
      %2698 : bool = prim::GetAttr[name="training"](%2692)
      %2699 : Tensor = prim::GetAttr[name="running_mean"](%2692)
      %2700 : Tensor = prim::GetAttr[name="running_var"](%2692)
      %2701 : Tensor = prim::GetAttr[name="weight"](%2692)
      %2702 : Tensor = prim::GetAttr[name="bias"](%2692)
       = prim::If(%2698) # torch/nn/functional.py:2011:4
        block0():
          %2703 : int[] = aten::size(%concated_features.59) # torch/nn/functional.py:2012:27
          %size_prods.480 : int = aten::__getitem__(%2703, %24) # torch/nn/functional.py:1991:17
          %2705 : int = aten::len(%2703) # torch/nn/functional.py:1992:19
          %2706 : int = aten::sub(%2705, %26) # torch/nn/functional.py:1992:19
          %size_prods.481 : int = prim::Loop(%2706, %25, %size_prods.480) # torch/nn/functional.py:1992:4
            block0(%i.121 : int, %size_prods.482 : int):
              %2710 : int = aten::add(%i.121, %26) # torch/nn/functional.py:1993:27
              %2711 : int = aten::__getitem__(%2703, %2710) # torch/nn/functional.py:1993:22
              %size_prods.483 : int = aten::mul(%size_prods.482, %2711) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.483)
          %2713 : bool = aten::eq(%size_prods.481, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2713) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2714 : Tensor = aten::batch_norm(%concated_features.59, %2701, %2702, %2699, %2700, %2698, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.117 : Tensor = aten::relu_(%2714) # torch/nn/functional.py:1117:17
      %2716 : Tensor = prim::GetAttr[name="weight"](%2691)
      %2717 : Tensor? = prim::GetAttr[name="bias"](%2691)
      %2718 : int[] = prim::ListConstruct(%27, %27)
      %2719 : int[] = prim::ListConstruct(%24, %24)
      %2720 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.117 : Tensor = aten::conv2d(%result.117, %2716, %2717, %2718, %2719, %2720, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.117)
  %2722 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1884)
  %2723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1884)
  %2724 : int = aten::dim(%bottleneck_output.116) # torch/nn/modules/batchnorm.py:276:11
  %2725 : bool = aten::ne(%2724, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2725) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2726 : bool = prim::GetAttr[name="training"](%2723)
   = prim::If(%2726) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2723)
      %2728 : Tensor = aten::add(%2727, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2723, %2728)
      -> ()
    block1():
      -> ()
  %2729 : bool = prim::GetAttr[name="training"](%2723)
  %2730 : Tensor = prim::GetAttr[name="running_mean"](%2723)
  %2731 : Tensor = prim::GetAttr[name="running_var"](%2723)
  %2732 : Tensor = prim::GetAttr[name="weight"](%2723)
  %2733 : Tensor = prim::GetAttr[name="bias"](%2723)
   = prim::If(%2729) # torch/nn/functional.py:2011:4
    block0():
      %2734 : int[] = aten::size(%bottleneck_output.116) # torch/nn/functional.py:2012:27
      %size_prods.484 : int = aten::__getitem__(%2734, %24) # torch/nn/functional.py:1991:17
      %2736 : int = aten::len(%2734) # torch/nn/functional.py:1992:19
      %2737 : int = aten::sub(%2736, %26) # torch/nn/functional.py:1992:19
      %size_prods.485 : int = prim::Loop(%2737, %25, %size_prods.484) # torch/nn/functional.py:1992:4
        block0(%i.122 : int, %size_prods.486 : int):
          %2741 : int = aten::add(%i.122, %26) # torch/nn/functional.py:1993:27
          %2742 : int = aten::__getitem__(%2734, %2741) # torch/nn/functional.py:1993:22
          %size_prods.487 : int = aten::mul(%size_prods.486, %2742) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.487)
      %2744 : bool = aten::eq(%size_prods.485, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2744) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2745 : Tensor = aten::batch_norm(%bottleneck_output.116, %2732, %2733, %2730, %2731, %2729, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.118 : Tensor = aten::relu_(%2745) # torch/nn/functional.py:1117:17
  %2747 : Tensor = prim::GetAttr[name="weight"](%2722)
  %2748 : Tensor? = prim::GetAttr[name="bias"](%2722)
  %2749 : int[] = prim::ListConstruct(%27, %27)
  %2750 : int[] = prim::ListConstruct(%27, %27)
  %2751 : int[] = prim::ListConstruct(%27, %27)
  %new_features.117 : Tensor = aten::conv2d(%result.118, %2747, %2748, %2749, %2750, %2751, %27) # torch/nn/modules/conv.py:415:15
  %2753 : float = prim::GetAttr[name="drop_rate"](%1884)
  %2754 : bool = aten::gt(%2753, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.118 : Tensor = prim::If(%2754) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2756 : float = prim::GetAttr[name="drop_rate"](%1884)
      %2757 : bool = prim::GetAttr[name="training"](%1884)
      %2758 : bool = aten::lt(%2756, %16) # torch/nn/functional.py:968:7
      %2759 : bool = prim::If(%2758) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2760 : bool = aten::gt(%2756, %17) # torch/nn/functional.py:968:17
          -> (%2760)
       = prim::If(%2759) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2761 : Tensor = aten::dropout(%new_features.117, %2756, %2757) # torch/nn/functional.py:973:17
      -> (%2761)
    block1():
      -> (%new_features.117)
  %2762 : Tensor[] = aten::append(%features.4, %new_features.118) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2763 : Tensor = prim::Uninitialized()
  %2764 : bool = prim::GetAttr[name="memory_efficient"](%1885)
  %2765 : bool = prim::If(%2764) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2766 : bool = prim::Uninitialized()
      %2767 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2768 : bool = aten::gt(%2767, %24)
      %2769 : bool, %2770 : bool, %2771 : int = prim::Loop(%18, %2768, %19, %2766, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2772 : int, %2773 : bool, %2774 : bool, %2775 : int):
          %tensor.60 : Tensor = aten::__getitem__(%features.4, %2775) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2777 : bool = prim::requires_grad(%tensor.60)
          %2778 : bool, %2779 : bool = prim::If(%2777) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2766)
          %2780 : int = aten::add(%2775, %27)
          %2781 : bool = aten::lt(%2780, %2767)
          %2782 : bool = aten::__and__(%2781, %2778)
          -> (%2782, %2777, %2779, %2780)
      %2783 : bool = prim::If(%2769)
        block0():
          -> (%2770)
        block1():
          -> (%19)
      -> (%2783)
    block1():
      -> (%19)
  %bottleneck_output.118 : Tensor = prim::If(%2765) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2763)
    block1():
      %concated_features.60 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2786 : __torch__.torch.nn.modules.conv.___torch_mangle_113.Conv2d = prim::GetAttr[name="conv1"](%1885)
      %2787 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_112.BatchNorm2d = prim::GetAttr[name="norm1"](%1885)
      %2788 : int = aten::dim(%concated_features.60) # torch/nn/modules/batchnorm.py:276:11
      %2789 : bool = aten::ne(%2788, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2789) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2790 : bool = prim::GetAttr[name="training"](%2787)
       = prim::If(%2790) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2791 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2787)
          %2792 : Tensor = aten::add(%2791, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2787, %2792)
          -> ()
        block1():
          -> ()
      %2793 : bool = prim::GetAttr[name="training"](%2787)
      %2794 : Tensor = prim::GetAttr[name="running_mean"](%2787)
      %2795 : Tensor = prim::GetAttr[name="running_var"](%2787)
      %2796 : Tensor = prim::GetAttr[name="weight"](%2787)
      %2797 : Tensor = prim::GetAttr[name="bias"](%2787)
       = prim::If(%2793) # torch/nn/functional.py:2011:4
        block0():
          %2798 : int[] = aten::size(%concated_features.60) # torch/nn/functional.py:2012:27
          %size_prods.488 : int = aten::__getitem__(%2798, %24) # torch/nn/functional.py:1991:17
          %2800 : int = aten::len(%2798) # torch/nn/functional.py:1992:19
          %2801 : int = aten::sub(%2800, %26) # torch/nn/functional.py:1992:19
          %size_prods.489 : int = prim::Loop(%2801, %25, %size_prods.488) # torch/nn/functional.py:1992:4
            block0(%i.123 : int, %size_prods.490 : int):
              %2805 : int = aten::add(%i.123, %26) # torch/nn/functional.py:1993:27
              %2806 : int = aten::__getitem__(%2798, %2805) # torch/nn/functional.py:1993:22
              %size_prods.491 : int = aten::mul(%size_prods.490, %2806) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.491)
          %2808 : bool = aten::eq(%size_prods.489, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2808) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2809 : Tensor = aten::batch_norm(%concated_features.60, %2796, %2797, %2794, %2795, %2793, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.119 : Tensor = aten::relu_(%2809) # torch/nn/functional.py:1117:17
      %2811 : Tensor = prim::GetAttr[name="weight"](%2786)
      %2812 : Tensor? = prim::GetAttr[name="bias"](%2786)
      %2813 : int[] = prim::ListConstruct(%27, %27)
      %2814 : int[] = prim::ListConstruct(%24, %24)
      %2815 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.119 : Tensor = aten::conv2d(%result.119, %2811, %2812, %2813, %2814, %2815, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.119)
  %2817 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1885)
  %2818 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1885)
  %2819 : int = aten::dim(%bottleneck_output.118) # torch/nn/modules/batchnorm.py:276:11
  %2820 : bool = aten::ne(%2819, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2820) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2821 : bool = prim::GetAttr[name="training"](%2818)
   = prim::If(%2821) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2822 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2818)
      %2823 : Tensor = aten::add(%2822, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2818, %2823)
      -> ()
    block1():
      -> ()
  %2824 : bool = prim::GetAttr[name="training"](%2818)
  %2825 : Tensor = prim::GetAttr[name="running_mean"](%2818)
  %2826 : Tensor = prim::GetAttr[name="running_var"](%2818)
  %2827 : Tensor = prim::GetAttr[name="weight"](%2818)
  %2828 : Tensor = prim::GetAttr[name="bias"](%2818)
   = prim::If(%2824) # torch/nn/functional.py:2011:4
    block0():
      %2829 : int[] = aten::size(%bottleneck_output.118) # torch/nn/functional.py:2012:27
      %size_prods.492 : int = aten::__getitem__(%2829, %24) # torch/nn/functional.py:1991:17
      %2831 : int = aten::len(%2829) # torch/nn/functional.py:1992:19
      %2832 : int = aten::sub(%2831, %26) # torch/nn/functional.py:1992:19
      %size_prods.493 : int = prim::Loop(%2832, %25, %size_prods.492) # torch/nn/functional.py:1992:4
        block0(%i.124 : int, %size_prods.494 : int):
          %2836 : int = aten::add(%i.124, %26) # torch/nn/functional.py:1993:27
          %2837 : int = aten::__getitem__(%2829, %2836) # torch/nn/functional.py:1993:22
          %size_prods.495 : int = aten::mul(%size_prods.494, %2837) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.495)
      %2839 : bool = aten::eq(%size_prods.493, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2839) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2840 : Tensor = aten::batch_norm(%bottleneck_output.118, %2827, %2828, %2825, %2826, %2824, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.120 : Tensor = aten::relu_(%2840) # torch/nn/functional.py:1117:17
  %2842 : Tensor = prim::GetAttr[name="weight"](%2817)
  %2843 : Tensor? = prim::GetAttr[name="bias"](%2817)
  %2844 : int[] = prim::ListConstruct(%27, %27)
  %2845 : int[] = prim::ListConstruct(%27, %27)
  %2846 : int[] = prim::ListConstruct(%27, %27)
  %new_features.119 : Tensor = aten::conv2d(%result.120, %2842, %2843, %2844, %2845, %2846, %27) # torch/nn/modules/conv.py:415:15
  %2848 : float = prim::GetAttr[name="drop_rate"](%1885)
  %2849 : bool = aten::gt(%2848, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.120 : Tensor = prim::If(%2849) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2851 : float = prim::GetAttr[name="drop_rate"](%1885)
      %2852 : bool = prim::GetAttr[name="training"](%1885)
      %2853 : bool = aten::lt(%2851, %16) # torch/nn/functional.py:968:7
      %2854 : bool = prim::If(%2853) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2855 : bool = aten::gt(%2851, %17) # torch/nn/functional.py:968:17
          -> (%2855)
       = prim::If(%2854) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2856 : Tensor = aten::dropout(%new_features.119, %2851, %2852) # torch/nn/functional.py:973:17
      -> (%2856)
    block1():
      -> (%new_features.119)
  %2857 : Tensor[] = aten::append(%features.4, %new_features.120) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2858 : Tensor = prim::Uninitialized()
  %2859 : bool = prim::GetAttr[name="memory_efficient"](%1886)
  %2860 : bool = prim::If(%2859) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2861 : bool = prim::Uninitialized()
      %2862 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2863 : bool = aten::gt(%2862, %24)
      %2864 : bool, %2865 : bool, %2866 : int = prim::Loop(%18, %2863, %19, %2861, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2867 : int, %2868 : bool, %2869 : bool, %2870 : int):
          %tensor.61 : Tensor = aten::__getitem__(%features.4, %2870) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2872 : bool = prim::requires_grad(%tensor.61)
          %2873 : bool, %2874 : bool = prim::If(%2872) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2861)
          %2875 : int = aten::add(%2870, %27)
          %2876 : bool = aten::lt(%2875, %2862)
          %2877 : bool = aten::__and__(%2876, %2873)
          -> (%2877, %2872, %2874, %2875)
      %2878 : bool = prim::If(%2864)
        block0():
          -> (%2865)
        block1():
          -> (%19)
      -> (%2878)
    block1():
      -> (%19)
  %bottleneck_output.120 : Tensor = prim::If(%2860) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2858)
    block1():
      %concated_features.61 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2881 : __torch__.torch.nn.modules.conv.___torch_mangle_116.Conv2d = prim::GetAttr[name="conv1"](%1886)
      %2882 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%1886)
      %2883 : int = aten::dim(%concated_features.61) # torch/nn/modules/batchnorm.py:276:11
      %2884 : bool = aten::ne(%2883, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2884) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2885 : bool = prim::GetAttr[name="training"](%2882)
       = prim::If(%2885) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2886 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2882)
          %2887 : Tensor = aten::add(%2886, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2882, %2887)
          -> ()
        block1():
          -> ()
      %2888 : bool = prim::GetAttr[name="training"](%2882)
      %2889 : Tensor = prim::GetAttr[name="running_mean"](%2882)
      %2890 : Tensor = prim::GetAttr[name="running_var"](%2882)
      %2891 : Tensor = prim::GetAttr[name="weight"](%2882)
      %2892 : Tensor = prim::GetAttr[name="bias"](%2882)
       = prim::If(%2888) # torch/nn/functional.py:2011:4
        block0():
          %2893 : int[] = aten::size(%concated_features.61) # torch/nn/functional.py:2012:27
          %size_prods.496 : int = aten::__getitem__(%2893, %24) # torch/nn/functional.py:1991:17
          %2895 : int = aten::len(%2893) # torch/nn/functional.py:1992:19
          %2896 : int = aten::sub(%2895, %26) # torch/nn/functional.py:1992:19
          %size_prods.497 : int = prim::Loop(%2896, %25, %size_prods.496) # torch/nn/functional.py:1992:4
            block0(%i.125 : int, %size_prods.498 : int):
              %2900 : int = aten::add(%i.125, %26) # torch/nn/functional.py:1993:27
              %2901 : int = aten::__getitem__(%2893, %2900) # torch/nn/functional.py:1993:22
              %size_prods.499 : int = aten::mul(%size_prods.498, %2901) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.499)
          %2903 : bool = aten::eq(%size_prods.497, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2903) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2904 : Tensor = aten::batch_norm(%concated_features.61, %2891, %2892, %2889, %2890, %2888, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.121 : Tensor = aten::relu_(%2904) # torch/nn/functional.py:1117:17
      %2906 : Tensor = prim::GetAttr[name="weight"](%2881)
      %2907 : Tensor? = prim::GetAttr[name="bias"](%2881)
      %2908 : int[] = prim::ListConstruct(%27, %27)
      %2909 : int[] = prim::ListConstruct(%24, %24)
      %2910 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.121 : Tensor = aten::conv2d(%result.121, %2906, %2907, %2908, %2909, %2910, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.121)
  %2912 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1886)
  %2913 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1886)
  %2914 : int = aten::dim(%bottleneck_output.120) # torch/nn/modules/batchnorm.py:276:11
  %2915 : bool = aten::ne(%2914, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2915) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2916 : bool = prim::GetAttr[name="training"](%2913)
   = prim::If(%2916) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2917 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2913)
      %2918 : Tensor = aten::add(%2917, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2913, %2918)
      -> ()
    block1():
      -> ()
  %2919 : bool = prim::GetAttr[name="training"](%2913)
  %2920 : Tensor = prim::GetAttr[name="running_mean"](%2913)
  %2921 : Tensor = prim::GetAttr[name="running_var"](%2913)
  %2922 : Tensor = prim::GetAttr[name="weight"](%2913)
  %2923 : Tensor = prim::GetAttr[name="bias"](%2913)
   = prim::If(%2919) # torch/nn/functional.py:2011:4
    block0():
      %2924 : int[] = aten::size(%bottleneck_output.120) # torch/nn/functional.py:2012:27
      %size_prods.500 : int = aten::__getitem__(%2924, %24) # torch/nn/functional.py:1991:17
      %2926 : int = aten::len(%2924) # torch/nn/functional.py:1992:19
      %2927 : int = aten::sub(%2926, %26) # torch/nn/functional.py:1992:19
      %size_prods.501 : int = prim::Loop(%2927, %25, %size_prods.500) # torch/nn/functional.py:1992:4
        block0(%i.126 : int, %size_prods.502 : int):
          %2931 : int = aten::add(%i.126, %26) # torch/nn/functional.py:1993:27
          %2932 : int = aten::__getitem__(%2924, %2931) # torch/nn/functional.py:1993:22
          %size_prods.503 : int = aten::mul(%size_prods.502, %2932) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.503)
      %2934 : bool = aten::eq(%size_prods.501, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2934) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2935 : Tensor = aten::batch_norm(%bottleneck_output.120, %2922, %2923, %2920, %2921, %2919, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.122 : Tensor = aten::relu_(%2935) # torch/nn/functional.py:1117:17
  %2937 : Tensor = prim::GetAttr[name="weight"](%2912)
  %2938 : Tensor? = prim::GetAttr[name="bias"](%2912)
  %2939 : int[] = prim::ListConstruct(%27, %27)
  %2940 : int[] = prim::ListConstruct(%27, %27)
  %2941 : int[] = prim::ListConstruct(%27, %27)
  %new_features.121 : Tensor = aten::conv2d(%result.122, %2937, %2938, %2939, %2940, %2941, %27) # torch/nn/modules/conv.py:415:15
  %2943 : float = prim::GetAttr[name="drop_rate"](%1886)
  %2944 : bool = aten::gt(%2943, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.122 : Tensor = prim::If(%2944) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2946 : float = prim::GetAttr[name="drop_rate"](%1886)
      %2947 : bool = prim::GetAttr[name="training"](%1886)
      %2948 : bool = aten::lt(%2946, %16) # torch/nn/functional.py:968:7
      %2949 : bool = prim::If(%2948) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2950 : bool = aten::gt(%2946, %17) # torch/nn/functional.py:968:17
          -> (%2950)
       = prim::If(%2949) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2951 : Tensor = aten::dropout(%new_features.121, %2946, %2947) # torch/nn/functional.py:973:17
      -> (%2951)
    block1():
      -> (%new_features.121)
  %2952 : Tensor[] = aten::append(%features.4, %new_features.122) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2953 : Tensor = prim::Uninitialized()
  %2954 : bool = prim::GetAttr[name="memory_efficient"](%1887)
  %2955 : bool = prim::If(%2954) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2956 : bool = prim::Uninitialized()
      %2957 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2958 : bool = aten::gt(%2957, %24)
      %2959 : bool, %2960 : bool, %2961 : int = prim::Loop(%18, %2958, %19, %2956, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2962 : int, %2963 : bool, %2964 : bool, %2965 : int):
          %tensor.62 : Tensor = aten::__getitem__(%features.4, %2965) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2967 : bool = prim::requires_grad(%tensor.62)
          %2968 : bool, %2969 : bool = prim::If(%2967) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2956)
          %2970 : int = aten::add(%2965, %27)
          %2971 : bool = aten::lt(%2970, %2957)
          %2972 : bool = aten::__and__(%2971, %2968)
          -> (%2972, %2967, %2969, %2970)
      %2973 : bool = prim::If(%2959)
        block0():
          -> (%2960)
        block1():
          -> (%19)
      -> (%2973)
    block1():
      -> (%19)
  %bottleneck_output.122 : Tensor = prim::If(%2955) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2953)
    block1():
      %concated_features.62 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2976 : __torch__.torch.nn.modules.conv.___torch_mangle_119.Conv2d = prim::GetAttr[name="conv1"](%1887)
      %2977 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_118.BatchNorm2d = prim::GetAttr[name="norm1"](%1887)
      %2978 : int = aten::dim(%concated_features.62) # torch/nn/modules/batchnorm.py:276:11
      %2979 : bool = aten::ne(%2978, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2979) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2980 : bool = prim::GetAttr[name="training"](%2977)
       = prim::If(%2980) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2981 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2977)
          %2982 : Tensor = aten::add(%2981, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2977, %2982)
          -> ()
        block1():
          -> ()
      %2983 : bool = prim::GetAttr[name="training"](%2977)
      %2984 : Tensor = prim::GetAttr[name="running_mean"](%2977)
      %2985 : Tensor = prim::GetAttr[name="running_var"](%2977)
      %2986 : Tensor = prim::GetAttr[name="weight"](%2977)
      %2987 : Tensor = prim::GetAttr[name="bias"](%2977)
       = prim::If(%2983) # torch/nn/functional.py:2011:4
        block0():
          %2988 : int[] = aten::size(%concated_features.62) # torch/nn/functional.py:2012:27
          %size_prods.504 : int = aten::__getitem__(%2988, %24) # torch/nn/functional.py:1991:17
          %2990 : int = aten::len(%2988) # torch/nn/functional.py:1992:19
          %2991 : int = aten::sub(%2990, %26) # torch/nn/functional.py:1992:19
          %size_prods.505 : int = prim::Loop(%2991, %25, %size_prods.504) # torch/nn/functional.py:1992:4
            block0(%i.127 : int, %size_prods.506 : int):
              %2995 : int = aten::add(%i.127, %26) # torch/nn/functional.py:1993:27
              %2996 : int = aten::__getitem__(%2988, %2995) # torch/nn/functional.py:1993:22
              %size_prods.507 : int = aten::mul(%size_prods.506, %2996) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.507)
          %2998 : bool = aten::eq(%size_prods.505, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2998) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2999 : Tensor = aten::batch_norm(%concated_features.62, %2986, %2987, %2984, %2985, %2983, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.123 : Tensor = aten::relu_(%2999) # torch/nn/functional.py:1117:17
      %3001 : Tensor = prim::GetAttr[name="weight"](%2976)
      %3002 : Tensor? = prim::GetAttr[name="bias"](%2976)
      %3003 : int[] = prim::ListConstruct(%27, %27)
      %3004 : int[] = prim::ListConstruct(%24, %24)
      %3005 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.123 : Tensor = aten::conv2d(%result.123, %3001, %3002, %3003, %3004, %3005, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.123)
  %3007 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1887)
  %3008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1887)
  %3009 : int = aten::dim(%bottleneck_output.122) # torch/nn/modules/batchnorm.py:276:11
  %3010 : bool = aten::ne(%3009, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3010) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3011 : bool = prim::GetAttr[name="training"](%3008)
   = prim::If(%3011) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3008)
      %3013 : Tensor = aten::add(%3012, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3008, %3013)
      -> ()
    block1():
      -> ()
  %3014 : bool = prim::GetAttr[name="training"](%3008)
  %3015 : Tensor = prim::GetAttr[name="running_mean"](%3008)
  %3016 : Tensor = prim::GetAttr[name="running_var"](%3008)
  %3017 : Tensor = prim::GetAttr[name="weight"](%3008)
  %3018 : Tensor = prim::GetAttr[name="bias"](%3008)
   = prim::If(%3014) # torch/nn/functional.py:2011:4
    block0():
      %3019 : int[] = aten::size(%bottleneck_output.122) # torch/nn/functional.py:2012:27
      %size_prods.508 : int = aten::__getitem__(%3019, %24) # torch/nn/functional.py:1991:17
      %3021 : int = aten::len(%3019) # torch/nn/functional.py:1992:19
      %3022 : int = aten::sub(%3021, %26) # torch/nn/functional.py:1992:19
      %size_prods.509 : int = prim::Loop(%3022, %25, %size_prods.508) # torch/nn/functional.py:1992:4
        block0(%i.128 : int, %size_prods.510 : int):
          %3026 : int = aten::add(%i.128, %26) # torch/nn/functional.py:1993:27
          %3027 : int = aten::__getitem__(%3019, %3026) # torch/nn/functional.py:1993:22
          %size_prods.511 : int = aten::mul(%size_prods.510, %3027) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.511)
      %3029 : bool = aten::eq(%size_prods.509, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3029) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3030 : Tensor = aten::batch_norm(%bottleneck_output.122, %3017, %3018, %3015, %3016, %3014, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.124 : Tensor = aten::relu_(%3030) # torch/nn/functional.py:1117:17
  %3032 : Tensor = prim::GetAttr[name="weight"](%3007)
  %3033 : Tensor? = prim::GetAttr[name="bias"](%3007)
  %3034 : int[] = prim::ListConstruct(%27, %27)
  %3035 : int[] = prim::ListConstruct(%27, %27)
  %3036 : int[] = prim::ListConstruct(%27, %27)
  %new_features.123 : Tensor = aten::conv2d(%result.124, %3032, %3033, %3034, %3035, %3036, %27) # torch/nn/modules/conv.py:415:15
  %3038 : float = prim::GetAttr[name="drop_rate"](%1887)
  %3039 : bool = aten::gt(%3038, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.124 : Tensor = prim::If(%3039) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3041 : float = prim::GetAttr[name="drop_rate"](%1887)
      %3042 : bool = prim::GetAttr[name="training"](%1887)
      %3043 : bool = aten::lt(%3041, %16) # torch/nn/functional.py:968:7
      %3044 : bool = prim::If(%3043) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3045 : bool = aten::gt(%3041, %17) # torch/nn/functional.py:968:17
          -> (%3045)
       = prim::If(%3044) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3046 : Tensor = aten::dropout(%new_features.123, %3041, %3042) # torch/nn/functional.py:973:17
      -> (%3046)
    block1():
      -> (%new_features.123)
  %3047 : Tensor[] = aten::append(%features.4, %new_features.124) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3048 : Tensor = prim::Uninitialized()
  %3049 : bool = prim::GetAttr[name="memory_efficient"](%1888)
  %3050 : bool = prim::If(%3049) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3051 : bool = prim::Uninitialized()
      %3052 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3053 : bool = aten::gt(%3052, %24)
      %3054 : bool, %3055 : bool, %3056 : int = prim::Loop(%18, %3053, %19, %3051, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3057 : int, %3058 : bool, %3059 : bool, %3060 : int):
          %tensor.63 : Tensor = aten::__getitem__(%features.4, %3060) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3062 : bool = prim::requires_grad(%tensor.63)
          %3063 : bool, %3064 : bool = prim::If(%3062) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3051)
          %3065 : int = aten::add(%3060, %27)
          %3066 : bool = aten::lt(%3065, %3052)
          %3067 : bool = aten::__and__(%3066, %3063)
          -> (%3067, %3062, %3064, %3065)
      %3068 : bool = prim::If(%3054)
        block0():
          -> (%3055)
        block1():
          -> (%19)
      -> (%3068)
    block1():
      -> (%19)
  %bottleneck_output.124 : Tensor = prim::If(%3050) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3048)
    block1():
      %concated_features.63 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3071 : __torch__.torch.nn.modules.conv.___torch_mangle_122.Conv2d = prim::GetAttr[name="conv1"](%1888)
      %3072 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_121.BatchNorm2d = prim::GetAttr[name="norm1"](%1888)
      %3073 : int = aten::dim(%concated_features.63) # torch/nn/modules/batchnorm.py:276:11
      %3074 : bool = aten::ne(%3073, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3074) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3075 : bool = prim::GetAttr[name="training"](%3072)
       = prim::If(%3075) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3076 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3072)
          %3077 : Tensor = aten::add(%3076, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3072, %3077)
          -> ()
        block1():
          -> ()
      %3078 : bool = prim::GetAttr[name="training"](%3072)
      %3079 : Tensor = prim::GetAttr[name="running_mean"](%3072)
      %3080 : Tensor = prim::GetAttr[name="running_var"](%3072)
      %3081 : Tensor = prim::GetAttr[name="weight"](%3072)
      %3082 : Tensor = prim::GetAttr[name="bias"](%3072)
       = prim::If(%3078) # torch/nn/functional.py:2011:4
        block0():
          %3083 : int[] = aten::size(%concated_features.63) # torch/nn/functional.py:2012:27
          %size_prods.512 : int = aten::__getitem__(%3083, %24) # torch/nn/functional.py:1991:17
          %3085 : int = aten::len(%3083) # torch/nn/functional.py:1992:19
          %3086 : int = aten::sub(%3085, %26) # torch/nn/functional.py:1992:19
          %size_prods.513 : int = prim::Loop(%3086, %25, %size_prods.512) # torch/nn/functional.py:1992:4
            block0(%i.129 : int, %size_prods.514 : int):
              %3090 : int = aten::add(%i.129, %26) # torch/nn/functional.py:1993:27
              %3091 : int = aten::__getitem__(%3083, %3090) # torch/nn/functional.py:1993:22
              %size_prods.515 : int = aten::mul(%size_prods.514, %3091) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.515)
          %3093 : bool = aten::eq(%size_prods.513, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3093) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3094 : Tensor = aten::batch_norm(%concated_features.63, %3081, %3082, %3079, %3080, %3078, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.125 : Tensor = aten::relu_(%3094) # torch/nn/functional.py:1117:17
      %3096 : Tensor = prim::GetAttr[name="weight"](%3071)
      %3097 : Tensor? = prim::GetAttr[name="bias"](%3071)
      %3098 : int[] = prim::ListConstruct(%27, %27)
      %3099 : int[] = prim::ListConstruct(%24, %24)
      %3100 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.125 : Tensor = aten::conv2d(%result.125, %3096, %3097, %3098, %3099, %3100, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.125)
  %3102 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1888)
  %3103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1888)
  %3104 : int = aten::dim(%bottleneck_output.124) # torch/nn/modules/batchnorm.py:276:11
  %3105 : bool = aten::ne(%3104, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3105) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3106 : bool = prim::GetAttr[name="training"](%3103)
   = prim::If(%3106) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3103)
      %3108 : Tensor = aten::add(%3107, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3103, %3108)
      -> ()
    block1():
      -> ()
  %3109 : bool = prim::GetAttr[name="training"](%3103)
  %3110 : Tensor = prim::GetAttr[name="running_mean"](%3103)
  %3111 : Tensor = prim::GetAttr[name="running_var"](%3103)
  %3112 : Tensor = prim::GetAttr[name="weight"](%3103)
  %3113 : Tensor = prim::GetAttr[name="bias"](%3103)
   = prim::If(%3109) # torch/nn/functional.py:2011:4
    block0():
      %3114 : int[] = aten::size(%bottleneck_output.124) # torch/nn/functional.py:2012:27
      %size_prods.516 : int = aten::__getitem__(%3114, %24) # torch/nn/functional.py:1991:17
      %3116 : int = aten::len(%3114) # torch/nn/functional.py:1992:19
      %3117 : int = aten::sub(%3116, %26) # torch/nn/functional.py:1992:19
      %size_prods.517 : int = prim::Loop(%3117, %25, %size_prods.516) # torch/nn/functional.py:1992:4
        block0(%i.130 : int, %size_prods.518 : int):
          %3121 : int = aten::add(%i.130, %26) # torch/nn/functional.py:1993:27
          %3122 : int = aten::__getitem__(%3114, %3121) # torch/nn/functional.py:1993:22
          %size_prods.519 : int = aten::mul(%size_prods.518, %3122) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.519)
      %3124 : bool = aten::eq(%size_prods.517, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3124) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3125 : Tensor = aten::batch_norm(%bottleneck_output.124, %3112, %3113, %3110, %3111, %3109, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.126 : Tensor = aten::relu_(%3125) # torch/nn/functional.py:1117:17
  %3127 : Tensor = prim::GetAttr[name="weight"](%3102)
  %3128 : Tensor? = prim::GetAttr[name="bias"](%3102)
  %3129 : int[] = prim::ListConstruct(%27, %27)
  %3130 : int[] = prim::ListConstruct(%27, %27)
  %3131 : int[] = prim::ListConstruct(%27, %27)
  %new_features.125 : Tensor = aten::conv2d(%result.126, %3127, %3128, %3129, %3130, %3131, %27) # torch/nn/modules/conv.py:415:15
  %3133 : float = prim::GetAttr[name="drop_rate"](%1888)
  %3134 : bool = aten::gt(%3133, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.126 : Tensor = prim::If(%3134) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3136 : float = prim::GetAttr[name="drop_rate"](%1888)
      %3137 : bool = prim::GetAttr[name="training"](%1888)
      %3138 : bool = aten::lt(%3136, %16) # torch/nn/functional.py:968:7
      %3139 : bool = prim::If(%3138) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3140 : bool = aten::gt(%3136, %17) # torch/nn/functional.py:968:17
          -> (%3140)
       = prim::If(%3139) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3141 : Tensor = aten::dropout(%new_features.125, %3136, %3137) # torch/nn/functional.py:973:17
      -> (%3141)
    block1():
      -> (%new_features.125)
  %3142 : Tensor[] = aten::append(%features.4, %new_features.126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3143 : Tensor = prim::Uninitialized()
  %3144 : bool = prim::GetAttr[name="memory_efficient"](%1889)
  %3145 : bool = prim::If(%3144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3146 : bool = prim::Uninitialized()
      %3147 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3148 : bool = aten::gt(%3147, %24)
      %3149 : bool, %3150 : bool, %3151 : int = prim::Loop(%18, %3148, %19, %3146, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3152 : int, %3153 : bool, %3154 : bool, %3155 : int):
          %tensor.64 : Tensor = aten::__getitem__(%features.4, %3155) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3157 : bool = prim::requires_grad(%tensor.64)
          %3158 : bool, %3159 : bool = prim::If(%3157) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3146)
          %3160 : int = aten::add(%3155, %27)
          %3161 : bool = aten::lt(%3160, %3147)
          %3162 : bool = aten::__and__(%3161, %3158)
          -> (%3162, %3157, %3159, %3160)
      %3163 : bool = prim::If(%3149)
        block0():
          -> (%3150)
        block1():
          -> (%19)
      -> (%3163)
    block1():
      -> (%19)
  %bottleneck_output.126 : Tensor = prim::If(%3145) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3143)
    block1():
      %concated_features.64 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3166 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv1"](%1889)
      %3167 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%1889)
      %3168 : int = aten::dim(%concated_features.64) # torch/nn/modules/batchnorm.py:276:11
      %3169 : bool = aten::ne(%3168, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3169) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3170 : bool = prim::GetAttr[name="training"](%3167)
       = prim::If(%3170) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3171 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3167)
          %3172 : Tensor = aten::add(%3171, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3167, %3172)
          -> ()
        block1():
          -> ()
      %3173 : bool = prim::GetAttr[name="training"](%3167)
      %3174 : Tensor = prim::GetAttr[name="running_mean"](%3167)
      %3175 : Tensor = prim::GetAttr[name="running_var"](%3167)
      %3176 : Tensor = prim::GetAttr[name="weight"](%3167)
      %3177 : Tensor = prim::GetAttr[name="bias"](%3167)
       = prim::If(%3173) # torch/nn/functional.py:2011:4
        block0():
          %3178 : int[] = aten::size(%concated_features.64) # torch/nn/functional.py:2012:27
          %size_prods.520 : int = aten::__getitem__(%3178, %24) # torch/nn/functional.py:1991:17
          %3180 : int = aten::len(%3178) # torch/nn/functional.py:1992:19
          %3181 : int = aten::sub(%3180, %26) # torch/nn/functional.py:1992:19
          %size_prods.521 : int = prim::Loop(%3181, %25, %size_prods.520) # torch/nn/functional.py:1992:4
            block0(%i.131 : int, %size_prods.522 : int):
              %3185 : int = aten::add(%i.131, %26) # torch/nn/functional.py:1993:27
              %3186 : int = aten::__getitem__(%3178, %3185) # torch/nn/functional.py:1993:22
              %size_prods.523 : int = aten::mul(%size_prods.522, %3186) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.523)
          %3188 : bool = aten::eq(%size_prods.521, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3188) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3189 : Tensor = aten::batch_norm(%concated_features.64, %3176, %3177, %3174, %3175, %3173, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.127 : Tensor = aten::relu_(%3189) # torch/nn/functional.py:1117:17
      %3191 : Tensor = prim::GetAttr[name="weight"](%3166)
      %3192 : Tensor? = prim::GetAttr[name="bias"](%3166)
      %3193 : int[] = prim::ListConstruct(%27, %27)
      %3194 : int[] = prim::ListConstruct(%24, %24)
      %3195 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.127 : Tensor = aten::conv2d(%result.127, %3191, %3192, %3193, %3194, %3195, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.127)
  %3197 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1889)
  %3198 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1889)
  %3199 : int = aten::dim(%bottleneck_output.126) # torch/nn/modules/batchnorm.py:276:11
  %3200 : bool = aten::ne(%3199, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3200) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3201 : bool = prim::GetAttr[name="training"](%3198)
   = prim::If(%3201) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3202 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3198)
      %3203 : Tensor = aten::add(%3202, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3198, %3203)
      -> ()
    block1():
      -> ()
  %3204 : bool = prim::GetAttr[name="training"](%3198)
  %3205 : Tensor = prim::GetAttr[name="running_mean"](%3198)
  %3206 : Tensor = prim::GetAttr[name="running_var"](%3198)
  %3207 : Tensor = prim::GetAttr[name="weight"](%3198)
  %3208 : Tensor = prim::GetAttr[name="bias"](%3198)
   = prim::If(%3204) # torch/nn/functional.py:2011:4
    block0():
      %3209 : int[] = aten::size(%bottleneck_output.126) # torch/nn/functional.py:2012:27
      %size_prods.524 : int = aten::__getitem__(%3209, %24) # torch/nn/functional.py:1991:17
      %3211 : int = aten::len(%3209) # torch/nn/functional.py:1992:19
      %3212 : int = aten::sub(%3211, %26) # torch/nn/functional.py:1992:19
      %size_prods.525 : int = prim::Loop(%3212, %25, %size_prods.524) # torch/nn/functional.py:1992:4
        block0(%i.132 : int, %size_prods.526 : int):
          %3216 : int = aten::add(%i.132, %26) # torch/nn/functional.py:1993:27
          %3217 : int = aten::__getitem__(%3209, %3216) # torch/nn/functional.py:1993:22
          %size_prods.527 : int = aten::mul(%size_prods.526, %3217) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.527)
      %3219 : bool = aten::eq(%size_prods.525, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3219) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3220 : Tensor = aten::batch_norm(%bottleneck_output.126, %3207, %3208, %3205, %3206, %3204, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.128 : Tensor = aten::relu_(%3220) # torch/nn/functional.py:1117:17
  %3222 : Tensor = prim::GetAttr[name="weight"](%3197)
  %3223 : Tensor? = prim::GetAttr[name="bias"](%3197)
  %3224 : int[] = prim::ListConstruct(%27, %27)
  %3225 : int[] = prim::ListConstruct(%27, %27)
  %3226 : int[] = prim::ListConstruct(%27, %27)
  %new_features.127 : Tensor = aten::conv2d(%result.128, %3222, %3223, %3224, %3225, %3226, %27) # torch/nn/modules/conv.py:415:15
  %3228 : float = prim::GetAttr[name="drop_rate"](%1889)
  %3229 : bool = aten::gt(%3228, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.128 : Tensor = prim::If(%3229) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3231 : float = prim::GetAttr[name="drop_rate"](%1889)
      %3232 : bool = prim::GetAttr[name="training"](%1889)
      %3233 : bool = aten::lt(%3231, %16) # torch/nn/functional.py:968:7
      %3234 : bool = prim::If(%3233) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3235 : bool = aten::gt(%3231, %17) # torch/nn/functional.py:968:17
          -> (%3235)
       = prim::If(%3234) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3236 : Tensor = aten::dropout(%new_features.127, %3231, %3232) # torch/nn/functional.py:973:17
      -> (%3236)
    block1():
      -> (%new_features.127)
  %3237 : Tensor[] = aten::append(%features.4, %new_features.128) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3238 : Tensor = prim::Uninitialized()
  %3239 : bool = prim::GetAttr[name="memory_efficient"](%1890)
  %3240 : bool = prim::If(%3239) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3241 : bool = prim::Uninitialized()
      %3242 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3243 : bool = aten::gt(%3242, %24)
      %3244 : bool, %3245 : bool, %3246 : int = prim::Loop(%18, %3243, %19, %3241, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3247 : int, %3248 : bool, %3249 : bool, %3250 : int):
          %tensor.65 : Tensor = aten::__getitem__(%features.4, %3250) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3252 : bool = prim::requires_grad(%tensor.65)
          %3253 : bool, %3254 : bool = prim::If(%3252) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3241)
          %3255 : int = aten::add(%3250, %27)
          %3256 : bool = aten::lt(%3255, %3242)
          %3257 : bool = aten::__and__(%3256, %3253)
          -> (%3257, %3252, %3254, %3255)
      %3258 : bool = prim::If(%3244)
        block0():
          -> (%3245)
        block1():
          -> (%19)
      -> (%3258)
    block1():
      -> (%19)
  %bottleneck_output.128 : Tensor = prim::If(%3240) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3238)
    block1():
      %concated_features.65 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3261 : __torch__.torch.nn.modules.conv.___torch_mangle_128.Conv2d = prim::GetAttr[name="conv1"](%1890)
      %3262 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="norm1"](%1890)
      %3263 : int = aten::dim(%concated_features.65) # torch/nn/modules/batchnorm.py:276:11
      %3264 : bool = aten::ne(%3263, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3264) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3265 : bool = prim::GetAttr[name="training"](%3262)
       = prim::If(%3265) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3266 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3262)
          %3267 : Tensor = aten::add(%3266, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3262, %3267)
          -> ()
        block1():
          -> ()
      %3268 : bool = prim::GetAttr[name="training"](%3262)
      %3269 : Tensor = prim::GetAttr[name="running_mean"](%3262)
      %3270 : Tensor = prim::GetAttr[name="running_var"](%3262)
      %3271 : Tensor = prim::GetAttr[name="weight"](%3262)
      %3272 : Tensor = prim::GetAttr[name="bias"](%3262)
       = prim::If(%3268) # torch/nn/functional.py:2011:4
        block0():
          %3273 : int[] = aten::size(%concated_features.65) # torch/nn/functional.py:2012:27
          %size_prods.528 : int = aten::__getitem__(%3273, %24) # torch/nn/functional.py:1991:17
          %3275 : int = aten::len(%3273) # torch/nn/functional.py:1992:19
          %3276 : int = aten::sub(%3275, %26) # torch/nn/functional.py:1992:19
          %size_prods.529 : int = prim::Loop(%3276, %25, %size_prods.528) # torch/nn/functional.py:1992:4
            block0(%i.133 : int, %size_prods.530 : int):
              %3280 : int = aten::add(%i.133, %26) # torch/nn/functional.py:1993:27
              %3281 : int = aten::__getitem__(%3273, %3280) # torch/nn/functional.py:1993:22
              %size_prods.531 : int = aten::mul(%size_prods.530, %3281) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.531)
          %3283 : bool = aten::eq(%size_prods.529, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3283) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3284 : Tensor = aten::batch_norm(%concated_features.65, %3271, %3272, %3269, %3270, %3268, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.129 : Tensor = aten::relu_(%3284) # torch/nn/functional.py:1117:17
      %3286 : Tensor = prim::GetAttr[name="weight"](%3261)
      %3287 : Tensor? = prim::GetAttr[name="bias"](%3261)
      %3288 : int[] = prim::ListConstruct(%27, %27)
      %3289 : int[] = prim::ListConstruct(%24, %24)
      %3290 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.129 : Tensor = aten::conv2d(%result.129, %3286, %3287, %3288, %3289, %3290, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.129)
  %3292 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1890)
  %3293 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1890)
  %3294 : int = aten::dim(%bottleneck_output.128) # torch/nn/modules/batchnorm.py:276:11
  %3295 : bool = aten::ne(%3294, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3295) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3296 : bool = prim::GetAttr[name="training"](%3293)
   = prim::If(%3296) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3297 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3293)
      %3298 : Tensor = aten::add(%3297, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3293, %3298)
      -> ()
    block1():
      -> ()
  %3299 : bool = prim::GetAttr[name="training"](%3293)
  %3300 : Tensor = prim::GetAttr[name="running_mean"](%3293)
  %3301 : Tensor = prim::GetAttr[name="running_var"](%3293)
  %3302 : Tensor = prim::GetAttr[name="weight"](%3293)
  %3303 : Tensor = prim::GetAttr[name="bias"](%3293)
   = prim::If(%3299) # torch/nn/functional.py:2011:4
    block0():
      %3304 : int[] = aten::size(%bottleneck_output.128) # torch/nn/functional.py:2012:27
      %size_prods.532 : int = aten::__getitem__(%3304, %24) # torch/nn/functional.py:1991:17
      %3306 : int = aten::len(%3304) # torch/nn/functional.py:1992:19
      %3307 : int = aten::sub(%3306, %26) # torch/nn/functional.py:1992:19
      %size_prods.533 : int = prim::Loop(%3307, %25, %size_prods.532) # torch/nn/functional.py:1992:4
        block0(%i.134 : int, %size_prods.534 : int):
          %3311 : int = aten::add(%i.134, %26) # torch/nn/functional.py:1993:27
          %3312 : int = aten::__getitem__(%3304, %3311) # torch/nn/functional.py:1993:22
          %size_prods.535 : int = aten::mul(%size_prods.534, %3312) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.535)
      %3314 : bool = aten::eq(%size_prods.533, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3314) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3315 : Tensor = aten::batch_norm(%bottleneck_output.128, %3302, %3303, %3300, %3301, %3299, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.130 : Tensor = aten::relu_(%3315) # torch/nn/functional.py:1117:17
  %3317 : Tensor = prim::GetAttr[name="weight"](%3292)
  %3318 : Tensor? = prim::GetAttr[name="bias"](%3292)
  %3319 : int[] = prim::ListConstruct(%27, %27)
  %3320 : int[] = prim::ListConstruct(%27, %27)
  %3321 : int[] = prim::ListConstruct(%27, %27)
  %new_features.129 : Tensor = aten::conv2d(%result.130, %3317, %3318, %3319, %3320, %3321, %27) # torch/nn/modules/conv.py:415:15
  %3323 : float = prim::GetAttr[name="drop_rate"](%1890)
  %3324 : bool = aten::gt(%3323, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.130 : Tensor = prim::If(%3324) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3326 : float = prim::GetAttr[name="drop_rate"](%1890)
      %3327 : bool = prim::GetAttr[name="training"](%1890)
      %3328 : bool = aten::lt(%3326, %16) # torch/nn/functional.py:968:7
      %3329 : bool = prim::If(%3328) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3330 : bool = aten::gt(%3326, %17) # torch/nn/functional.py:968:17
          -> (%3330)
       = prim::If(%3329) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3331 : Tensor = aten::dropout(%new_features.129, %3326, %3327) # torch/nn/functional.py:973:17
      -> (%3331)
    block1():
      -> (%new_features.129)
  %3332 : Tensor[] = aten::append(%features.4, %new_features.130) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3333 : Tensor = prim::Uninitialized()
  %3334 : bool = prim::GetAttr[name="memory_efficient"](%1891)
  %3335 : bool = prim::If(%3334) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3336 : bool = prim::Uninitialized()
      %3337 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3338 : bool = aten::gt(%3337, %24)
      %3339 : bool, %3340 : bool, %3341 : int = prim::Loop(%18, %3338, %19, %3336, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3342 : int, %3343 : bool, %3344 : bool, %3345 : int):
          %tensor.66 : Tensor = aten::__getitem__(%features.4, %3345) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3347 : bool = prim::requires_grad(%tensor.66)
          %3348 : bool, %3349 : bool = prim::If(%3347) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3336)
          %3350 : int = aten::add(%3345, %27)
          %3351 : bool = aten::lt(%3350, %3337)
          %3352 : bool = aten::__and__(%3351, %3348)
          -> (%3352, %3347, %3349, %3350)
      %3353 : bool = prim::If(%3339)
        block0():
          -> (%3340)
        block1():
          -> (%19)
      -> (%3353)
    block1():
      -> (%19)
  %bottleneck_output.130 : Tensor = prim::If(%3335) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3333)
    block1():
      %concated_features.66 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3356 : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d = prim::GetAttr[name="conv1"](%1891)
      %3357 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_130.BatchNorm2d = prim::GetAttr[name="norm1"](%1891)
      %3358 : int = aten::dim(%concated_features.66) # torch/nn/modules/batchnorm.py:276:11
      %3359 : bool = aten::ne(%3358, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3359) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3360 : bool = prim::GetAttr[name="training"](%3357)
       = prim::If(%3360) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3361 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3357)
          %3362 : Tensor = aten::add(%3361, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3357, %3362)
          -> ()
        block1():
          -> ()
      %3363 : bool = prim::GetAttr[name="training"](%3357)
      %3364 : Tensor = prim::GetAttr[name="running_mean"](%3357)
      %3365 : Tensor = prim::GetAttr[name="running_var"](%3357)
      %3366 : Tensor = prim::GetAttr[name="weight"](%3357)
      %3367 : Tensor = prim::GetAttr[name="bias"](%3357)
       = prim::If(%3363) # torch/nn/functional.py:2011:4
        block0():
          %3368 : int[] = aten::size(%concated_features.66) # torch/nn/functional.py:2012:27
          %size_prods.536 : int = aten::__getitem__(%3368, %24) # torch/nn/functional.py:1991:17
          %3370 : int = aten::len(%3368) # torch/nn/functional.py:1992:19
          %3371 : int = aten::sub(%3370, %26) # torch/nn/functional.py:1992:19
          %size_prods.537 : int = prim::Loop(%3371, %25, %size_prods.536) # torch/nn/functional.py:1992:4
            block0(%i.135 : int, %size_prods.538 : int):
              %3375 : int = aten::add(%i.135, %26) # torch/nn/functional.py:1993:27
              %3376 : int = aten::__getitem__(%3368, %3375) # torch/nn/functional.py:1993:22
              %size_prods.539 : int = aten::mul(%size_prods.538, %3376) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.539)
          %3378 : bool = aten::eq(%size_prods.537, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3378) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3379 : Tensor = aten::batch_norm(%concated_features.66, %3366, %3367, %3364, %3365, %3363, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.131 : Tensor = aten::relu_(%3379) # torch/nn/functional.py:1117:17
      %3381 : Tensor = prim::GetAttr[name="weight"](%3356)
      %3382 : Tensor? = prim::GetAttr[name="bias"](%3356)
      %3383 : int[] = prim::ListConstruct(%27, %27)
      %3384 : int[] = prim::ListConstruct(%24, %24)
      %3385 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.131 : Tensor = aten::conv2d(%result.131, %3381, %3382, %3383, %3384, %3385, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.131)
  %3387 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1891)
  %3388 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1891)
  %3389 : int = aten::dim(%bottleneck_output.130) # torch/nn/modules/batchnorm.py:276:11
  %3390 : bool = aten::ne(%3389, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3390) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3391 : bool = prim::GetAttr[name="training"](%3388)
   = prim::If(%3391) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3388)
      %3393 : Tensor = aten::add(%3392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3388, %3393)
      -> ()
    block1():
      -> ()
  %3394 : bool = prim::GetAttr[name="training"](%3388)
  %3395 : Tensor = prim::GetAttr[name="running_mean"](%3388)
  %3396 : Tensor = prim::GetAttr[name="running_var"](%3388)
  %3397 : Tensor = prim::GetAttr[name="weight"](%3388)
  %3398 : Tensor = prim::GetAttr[name="bias"](%3388)
   = prim::If(%3394) # torch/nn/functional.py:2011:4
    block0():
      %3399 : int[] = aten::size(%bottleneck_output.130) # torch/nn/functional.py:2012:27
      %size_prods.540 : int = aten::__getitem__(%3399, %24) # torch/nn/functional.py:1991:17
      %3401 : int = aten::len(%3399) # torch/nn/functional.py:1992:19
      %3402 : int = aten::sub(%3401, %26) # torch/nn/functional.py:1992:19
      %size_prods.541 : int = prim::Loop(%3402, %25, %size_prods.540) # torch/nn/functional.py:1992:4
        block0(%i.136 : int, %size_prods.542 : int):
          %3406 : int = aten::add(%i.136, %26) # torch/nn/functional.py:1993:27
          %3407 : int = aten::__getitem__(%3399, %3406) # torch/nn/functional.py:1993:22
          %size_prods.543 : int = aten::mul(%size_prods.542, %3407) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.543)
      %3409 : bool = aten::eq(%size_prods.541, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3409) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3410 : Tensor = aten::batch_norm(%bottleneck_output.130, %3397, %3398, %3395, %3396, %3394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.132 : Tensor = aten::relu_(%3410) # torch/nn/functional.py:1117:17
  %3412 : Tensor = prim::GetAttr[name="weight"](%3387)
  %3413 : Tensor? = prim::GetAttr[name="bias"](%3387)
  %3414 : int[] = prim::ListConstruct(%27, %27)
  %3415 : int[] = prim::ListConstruct(%27, %27)
  %3416 : int[] = prim::ListConstruct(%27, %27)
  %new_features.131 : Tensor = aten::conv2d(%result.132, %3412, %3413, %3414, %3415, %3416, %27) # torch/nn/modules/conv.py:415:15
  %3418 : float = prim::GetAttr[name="drop_rate"](%1891)
  %3419 : bool = aten::gt(%3418, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.132 : Tensor = prim::If(%3419) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3421 : float = prim::GetAttr[name="drop_rate"](%1891)
      %3422 : bool = prim::GetAttr[name="training"](%1891)
      %3423 : bool = aten::lt(%3421, %16) # torch/nn/functional.py:968:7
      %3424 : bool = prim::If(%3423) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3425 : bool = aten::gt(%3421, %17) # torch/nn/functional.py:968:17
          -> (%3425)
       = prim::If(%3424) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3426 : Tensor = aten::dropout(%new_features.131, %3421, %3422) # torch/nn/functional.py:973:17
      -> (%3426)
    block1():
      -> (%new_features.131)
  %3427 : Tensor[] = aten::append(%features.4, %new_features.132) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3428 : Tensor = prim::Uninitialized()
  %3429 : bool = prim::GetAttr[name="memory_efficient"](%1892)
  %3430 : bool = prim::If(%3429) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3431 : bool = prim::Uninitialized()
      %3432 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3433 : bool = aten::gt(%3432, %24)
      %3434 : bool, %3435 : bool, %3436 : int = prim::Loop(%18, %3433, %19, %3431, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3437 : int, %3438 : bool, %3439 : bool, %3440 : int):
          %tensor.67 : Tensor = aten::__getitem__(%features.4, %3440) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3442 : bool = prim::requires_grad(%tensor.67)
          %3443 : bool, %3444 : bool = prim::If(%3442) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3431)
          %3445 : int = aten::add(%3440, %27)
          %3446 : bool = aten::lt(%3445, %3432)
          %3447 : bool = aten::__and__(%3446, %3443)
          -> (%3447, %3442, %3444, %3445)
      %3448 : bool = prim::If(%3434)
        block0():
          -> (%3435)
        block1():
          -> (%19)
      -> (%3448)
    block1():
      -> (%19)
  %bottleneck_output.132 : Tensor = prim::If(%3430) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3428)
    block1():
      %concated_features.67 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3451 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%1892)
      %3452 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%1892)
      %3453 : int = aten::dim(%concated_features.67) # torch/nn/modules/batchnorm.py:276:11
      %3454 : bool = aten::ne(%3453, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3454) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3455 : bool = prim::GetAttr[name="training"](%3452)
       = prim::If(%3455) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3456 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3452)
          %3457 : Tensor = aten::add(%3456, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3452, %3457)
          -> ()
        block1():
          -> ()
      %3458 : bool = prim::GetAttr[name="training"](%3452)
      %3459 : Tensor = prim::GetAttr[name="running_mean"](%3452)
      %3460 : Tensor = prim::GetAttr[name="running_var"](%3452)
      %3461 : Tensor = prim::GetAttr[name="weight"](%3452)
      %3462 : Tensor = prim::GetAttr[name="bias"](%3452)
       = prim::If(%3458) # torch/nn/functional.py:2011:4
        block0():
          %3463 : int[] = aten::size(%concated_features.67) # torch/nn/functional.py:2012:27
          %size_prods.544 : int = aten::__getitem__(%3463, %24) # torch/nn/functional.py:1991:17
          %3465 : int = aten::len(%3463) # torch/nn/functional.py:1992:19
          %3466 : int = aten::sub(%3465, %26) # torch/nn/functional.py:1992:19
          %size_prods.545 : int = prim::Loop(%3466, %25, %size_prods.544) # torch/nn/functional.py:1992:4
            block0(%i.137 : int, %size_prods.546 : int):
              %3470 : int = aten::add(%i.137, %26) # torch/nn/functional.py:1993:27
              %3471 : int = aten::__getitem__(%3463, %3470) # torch/nn/functional.py:1993:22
              %size_prods.547 : int = aten::mul(%size_prods.546, %3471) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.547)
          %3473 : bool = aten::eq(%size_prods.545, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3473) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3474 : Tensor = aten::batch_norm(%concated_features.67, %3461, %3462, %3459, %3460, %3458, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.133 : Tensor = aten::relu_(%3474) # torch/nn/functional.py:1117:17
      %3476 : Tensor = prim::GetAttr[name="weight"](%3451)
      %3477 : Tensor? = prim::GetAttr[name="bias"](%3451)
      %3478 : int[] = prim::ListConstruct(%27, %27)
      %3479 : int[] = prim::ListConstruct(%24, %24)
      %3480 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.133 : Tensor = aten::conv2d(%result.133, %3476, %3477, %3478, %3479, %3480, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.133)
  %3482 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1892)
  %3483 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1892)
  %3484 : int = aten::dim(%bottleneck_output.132) # torch/nn/modules/batchnorm.py:276:11
  %3485 : bool = aten::ne(%3484, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3485) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3486 : bool = prim::GetAttr[name="training"](%3483)
   = prim::If(%3486) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3483)
      %3488 : Tensor = aten::add(%3487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3483, %3488)
      -> ()
    block1():
      -> ()
  %3489 : bool = prim::GetAttr[name="training"](%3483)
  %3490 : Tensor = prim::GetAttr[name="running_mean"](%3483)
  %3491 : Tensor = prim::GetAttr[name="running_var"](%3483)
  %3492 : Tensor = prim::GetAttr[name="weight"](%3483)
  %3493 : Tensor = prim::GetAttr[name="bias"](%3483)
   = prim::If(%3489) # torch/nn/functional.py:2011:4
    block0():
      %3494 : int[] = aten::size(%bottleneck_output.132) # torch/nn/functional.py:2012:27
      %size_prods.548 : int = aten::__getitem__(%3494, %24) # torch/nn/functional.py:1991:17
      %3496 : int = aten::len(%3494) # torch/nn/functional.py:1992:19
      %3497 : int = aten::sub(%3496, %26) # torch/nn/functional.py:1992:19
      %size_prods.549 : int = prim::Loop(%3497, %25, %size_prods.548) # torch/nn/functional.py:1992:4
        block0(%i.138 : int, %size_prods.550 : int):
          %3501 : int = aten::add(%i.138, %26) # torch/nn/functional.py:1993:27
          %3502 : int = aten::__getitem__(%3494, %3501) # torch/nn/functional.py:1993:22
          %size_prods.551 : int = aten::mul(%size_prods.550, %3502) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.551)
      %3504 : bool = aten::eq(%size_prods.549, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3504) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3505 : Tensor = aten::batch_norm(%bottleneck_output.132, %3492, %3493, %3490, %3491, %3489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.134 : Tensor = aten::relu_(%3505) # torch/nn/functional.py:1117:17
  %3507 : Tensor = prim::GetAttr[name="weight"](%3482)
  %3508 : Tensor? = prim::GetAttr[name="bias"](%3482)
  %3509 : int[] = prim::ListConstruct(%27, %27)
  %3510 : int[] = prim::ListConstruct(%27, %27)
  %3511 : int[] = prim::ListConstruct(%27, %27)
  %new_features.133 : Tensor = aten::conv2d(%result.134, %3507, %3508, %3509, %3510, %3511, %27) # torch/nn/modules/conv.py:415:15
  %3513 : float = prim::GetAttr[name="drop_rate"](%1892)
  %3514 : bool = aten::gt(%3513, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.134 : Tensor = prim::If(%3514) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3516 : float = prim::GetAttr[name="drop_rate"](%1892)
      %3517 : bool = prim::GetAttr[name="training"](%1892)
      %3518 : bool = aten::lt(%3516, %16) # torch/nn/functional.py:968:7
      %3519 : bool = prim::If(%3518) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3520 : bool = aten::gt(%3516, %17) # torch/nn/functional.py:968:17
          -> (%3520)
       = prim::If(%3519) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3521 : Tensor = aten::dropout(%new_features.133, %3516, %3517) # torch/nn/functional.py:973:17
      -> (%3521)
    block1():
      -> (%new_features.133)
  %3522 : Tensor[] = aten::append(%features.4, %new_features.134) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3523 : Tensor = prim::Uninitialized()
  %3524 : bool = prim::GetAttr[name="memory_efficient"](%1893)
  %3525 : bool = prim::If(%3524) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3526 : bool = prim::Uninitialized()
      %3527 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3528 : bool = aten::gt(%3527, %24)
      %3529 : bool, %3530 : bool, %3531 : int = prim::Loop(%18, %3528, %19, %3526, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3532 : int, %3533 : bool, %3534 : bool, %3535 : int):
          %tensor.68 : Tensor = aten::__getitem__(%features.4, %3535) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3537 : bool = prim::requires_grad(%tensor.68)
          %3538 : bool, %3539 : bool = prim::If(%3537) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3526)
          %3540 : int = aten::add(%3535, %27)
          %3541 : bool = aten::lt(%3540, %3527)
          %3542 : bool = aten::__and__(%3541, %3538)
          -> (%3542, %3537, %3539, %3540)
      %3543 : bool = prim::If(%3529)
        block0():
          -> (%3530)
        block1():
          -> (%19)
      -> (%3543)
    block1():
      -> (%19)
  %bottleneck_output.134 : Tensor = prim::If(%3525) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3523)
    block1():
      %concated_features.68 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3546 : __torch__.torch.nn.modules.conv.___torch_mangle_137.Conv2d = prim::GetAttr[name="conv1"](%1893)
      %3547 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_136.BatchNorm2d = prim::GetAttr[name="norm1"](%1893)
      %3548 : int = aten::dim(%concated_features.68) # torch/nn/modules/batchnorm.py:276:11
      %3549 : bool = aten::ne(%3548, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3549) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3550 : bool = prim::GetAttr[name="training"](%3547)
       = prim::If(%3550) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3551 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3547)
          %3552 : Tensor = aten::add(%3551, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3547, %3552)
          -> ()
        block1():
          -> ()
      %3553 : bool = prim::GetAttr[name="training"](%3547)
      %3554 : Tensor = prim::GetAttr[name="running_mean"](%3547)
      %3555 : Tensor = prim::GetAttr[name="running_var"](%3547)
      %3556 : Tensor = prim::GetAttr[name="weight"](%3547)
      %3557 : Tensor = prim::GetAttr[name="bias"](%3547)
       = prim::If(%3553) # torch/nn/functional.py:2011:4
        block0():
          %3558 : int[] = aten::size(%concated_features.68) # torch/nn/functional.py:2012:27
          %size_prods.552 : int = aten::__getitem__(%3558, %24) # torch/nn/functional.py:1991:17
          %3560 : int = aten::len(%3558) # torch/nn/functional.py:1992:19
          %3561 : int = aten::sub(%3560, %26) # torch/nn/functional.py:1992:19
          %size_prods.553 : int = prim::Loop(%3561, %25, %size_prods.552) # torch/nn/functional.py:1992:4
            block0(%i.139 : int, %size_prods.554 : int):
              %3565 : int = aten::add(%i.139, %26) # torch/nn/functional.py:1993:27
              %3566 : int = aten::__getitem__(%3558, %3565) # torch/nn/functional.py:1993:22
              %size_prods.555 : int = aten::mul(%size_prods.554, %3566) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.555)
          %3568 : bool = aten::eq(%size_prods.553, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3568) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3569 : Tensor = aten::batch_norm(%concated_features.68, %3556, %3557, %3554, %3555, %3553, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.135 : Tensor = aten::relu_(%3569) # torch/nn/functional.py:1117:17
      %3571 : Tensor = prim::GetAttr[name="weight"](%3546)
      %3572 : Tensor? = prim::GetAttr[name="bias"](%3546)
      %3573 : int[] = prim::ListConstruct(%27, %27)
      %3574 : int[] = prim::ListConstruct(%24, %24)
      %3575 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.135 : Tensor = aten::conv2d(%result.135, %3571, %3572, %3573, %3574, %3575, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.135)
  %3577 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1893)
  %3578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1893)
  %3579 : int = aten::dim(%bottleneck_output.134) # torch/nn/modules/batchnorm.py:276:11
  %3580 : bool = aten::ne(%3579, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3580) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3581 : bool = prim::GetAttr[name="training"](%3578)
   = prim::If(%3581) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3578)
      %3583 : Tensor = aten::add(%3582, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3578, %3583)
      -> ()
    block1():
      -> ()
  %3584 : bool = prim::GetAttr[name="training"](%3578)
  %3585 : Tensor = prim::GetAttr[name="running_mean"](%3578)
  %3586 : Tensor = prim::GetAttr[name="running_var"](%3578)
  %3587 : Tensor = prim::GetAttr[name="weight"](%3578)
  %3588 : Tensor = prim::GetAttr[name="bias"](%3578)
   = prim::If(%3584) # torch/nn/functional.py:2011:4
    block0():
      %3589 : int[] = aten::size(%bottleneck_output.134) # torch/nn/functional.py:2012:27
      %size_prods.556 : int = aten::__getitem__(%3589, %24) # torch/nn/functional.py:1991:17
      %3591 : int = aten::len(%3589) # torch/nn/functional.py:1992:19
      %3592 : int = aten::sub(%3591, %26) # torch/nn/functional.py:1992:19
      %size_prods.557 : int = prim::Loop(%3592, %25, %size_prods.556) # torch/nn/functional.py:1992:4
        block0(%i.140 : int, %size_prods.558 : int):
          %3596 : int = aten::add(%i.140, %26) # torch/nn/functional.py:1993:27
          %3597 : int = aten::__getitem__(%3589, %3596) # torch/nn/functional.py:1993:22
          %size_prods.559 : int = aten::mul(%size_prods.558, %3597) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.559)
      %3599 : bool = aten::eq(%size_prods.557, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3599) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3600 : Tensor = aten::batch_norm(%bottleneck_output.134, %3587, %3588, %3585, %3586, %3584, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.136 : Tensor = aten::relu_(%3600) # torch/nn/functional.py:1117:17
  %3602 : Tensor = prim::GetAttr[name="weight"](%3577)
  %3603 : Tensor? = prim::GetAttr[name="bias"](%3577)
  %3604 : int[] = prim::ListConstruct(%27, %27)
  %3605 : int[] = prim::ListConstruct(%27, %27)
  %3606 : int[] = prim::ListConstruct(%27, %27)
  %new_features.135 : Tensor = aten::conv2d(%result.136, %3602, %3603, %3604, %3605, %3606, %27) # torch/nn/modules/conv.py:415:15
  %3608 : float = prim::GetAttr[name="drop_rate"](%1893)
  %3609 : bool = aten::gt(%3608, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.136 : Tensor = prim::If(%3609) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3611 : float = prim::GetAttr[name="drop_rate"](%1893)
      %3612 : bool = prim::GetAttr[name="training"](%1893)
      %3613 : bool = aten::lt(%3611, %16) # torch/nn/functional.py:968:7
      %3614 : bool = prim::If(%3613) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3615 : bool = aten::gt(%3611, %17) # torch/nn/functional.py:968:17
          -> (%3615)
       = prim::If(%3614) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3616 : Tensor = aten::dropout(%new_features.135, %3611, %3612) # torch/nn/functional.py:973:17
      -> (%3616)
    block1():
      -> (%new_features.135)
  %3617 : Tensor[] = aten::append(%features.4, %new_features.136) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3618 : Tensor = prim::Uninitialized()
  %3619 : bool = prim::GetAttr[name="memory_efficient"](%1894)
  %3620 : bool = prim::If(%3619) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3621 : bool = prim::Uninitialized()
      %3622 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3623 : bool = aten::gt(%3622, %24)
      %3624 : bool, %3625 : bool, %3626 : int = prim::Loop(%18, %3623, %19, %3621, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3627 : int, %3628 : bool, %3629 : bool, %3630 : int):
          %tensor.69 : Tensor = aten::__getitem__(%features.4, %3630) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3632 : bool = prim::requires_grad(%tensor.69)
          %3633 : bool, %3634 : bool = prim::If(%3632) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3621)
          %3635 : int = aten::add(%3630, %27)
          %3636 : bool = aten::lt(%3635, %3622)
          %3637 : bool = aten::__and__(%3636, %3633)
          -> (%3637, %3632, %3634, %3635)
      %3638 : bool = prim::If(%3624)
        block0():
          -> (%3625)
        block1():
          -> (%19)
      -> (%3638)
    block1():
      -> (%19)
  %bottleneck_output.136 : Tensor = prim::If(%3620) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3618)
    block1():
      %concated_features.69 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3641 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv1"](%1894)
      %3642 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="norm1"](%1894)
      %3643 : int = aten::dim(%concated_features.69) # torch/nn/modules/batchnorm.py:276:11
      %3644 : bool = aten::ne(%3643, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3644) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3645 : bool = prim::GetAttr[name="training"](%3642)
       = prim::If(%3645) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3646 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3642)
          %3647 : Tensor = aten::add(%3646, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3642, %3647)
          -> ()
        block1():
          -> ()
      %3648 : bool = prim::GetAttr[name="training"](%3642)
      %3649 : Tensor = prim::GetAttr[name="running_mean"](%3642)
      %3650 : Tensor = prim::GetAttr[name="running_var"](%3642)
      %3651 : Tensor = prim::GetAttr[name="weight"](%3642)
      %3652 : Tensor = prim::GetAttr[name="bias"](%3642)
       = prim::If(%3648) # torch/nn/functional.py:2011:4
        block0():
          %3653 : int[] = aten::size(%concated_features.69) # torch/nn/functional.py:2012:27
          %size_prods.560 : int = aten::__getitem__(%3653, %24) # torch/nn/functional.py:1991:17
          %3655 : int = aten::len(%3653) # torch/nn/functional.py:1992:19
          %3656 : int = aten::sub(%3655, %26) # torch/nn/functional.py:1992:19
          %size_prods.561 : int = prim::Loop(%3656, %25, %size_prods.560) # torch/nn/functional.py:1992:4
            block0(%i.141 : int, %size_prods.562 : int):
              %3660 : int = aten::add(%i.141, %26) # torch/nn/functional.py:1993:27
              %3661 : int = aten::__getitem__(%3653, %3660) # torch/nn/functional.py:1993:22
              %size_prods.563 : int = aten::mul(%size_prods.562, %3661) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.563)
          %3663 : bool = aten::eq(%size_prods.561, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3663) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3664 : Tensor = aten::batch_norm(%concated_features.69, %3651, %3652, %3649, %3650, %3648, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.137 : Tensor = aten::relu_(%3664) # torch/nn/functional.py:1117:17
      %3666 : Tensor = prim::GetAttr[name="weight"](%3641)
      %3667 : Tensor? = prim::GetAttr[name="bias"](%3641)
      %3668 : int[] = prim::ListConstruct(%27, %27)
      %3669 : int[] = prim::ListConstruct(%24, %24)
      %3670 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.137 : Tensor = aten::conv2d(%result.137, %3666, %3667, %3668, %3669, %3670, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.137)
  %3672 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1894)
  %3673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1894)
  %3674 : int = aten::dim(%bottleneck_output.136) # torch/nn/modules/batchnorm.py:276:11
  %3675 : bool = aten::ne(%3674, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3675) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3676 : bool = prim::GetAttr[name="training"](%3673)
   = prim::If(%3676) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3673)
      %3678 : Tensor = aten::add(%3677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3673, %3678)
      -> ()
    block1():
      -> ()
  %3679 : bool = prim::GetAttr[name="training"](%3673)
  %3680 : Tensor = prim::GetAttr[name="running_mean"](%3673)
  %3681 : Tensor = prim::GetAttr[name="running_var"](%3673)
  %3682 : Tensor = prim::GetAttr[name="weight"](%3673)
  %3683 : Tensor = prim::GetAttr[name="bias"](%3673)
   = prim::If(%3679) # torch/nn/functional.py:2011:4
    block0():
      %3684 : int[] = aten::size(%bottleneck_output.136) # torch/nn/functional.py:2012:27
      %size_prods.564 : int = aten::__getitem__(%3684, %24) # torch/nn/functional.py:1991:17
      %3686 : int = aten::len(%3684) # torch/nn/functional.py:1992:19
      %3687 : int = aten::sub(%3686, %26) # torch/nn/functional.py:1992:19
      %size_prods.565 : int = prim::Loop(%3687, %25, %size_prods.564) # torch/nn/functional.py:1992:4
        block0(%i.142 : int, %size_prods.566 : int):
          %3691 : int = aten::add(%i.142, %26) # torch/nn/functional.py:1993:27
          %3692 : int = aten::__getitem__(%3684, %3691) # torch/nn/functional.py:1993:22
          %size_prods.567 : int = aten::mul(%size_prods.566, %3692) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.567)
      %3694 : bool = aten::eq(%size_prods.565, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3694) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3695 : Tensor = aten::batch_norm(%bottleneck_output.136, %3682, %3683, %3680, %3681, %3679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.138 : Tensor = aten::relu_(%3695) # torch/nn/functional.py:1117:17
  %3697 : Tensor = prim::GetAttr[name="weight"](%3672)
  %3698 : Tensor? = prim::GetAttr[name="bias"](%3672)
  %3699 : int[] = prim::ListConstruct(%27, %27)
  %3700 : int[] = prim::ListConstruct(%27, %27)
  %3701 : int[] = prim::ListConstruct(%27, %27)
  %new_features.137 : Tensor = aten::conv2d(%result.138, %3697, %3698, %3699, %3700, %3701, %27) # torch/nn/modules/conv.py:415:15
  %3703 : float = prim::GetAttr[name="drop_rate"](%1894)
  %3704 : bool = aten::gt(%3703, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.138 : Tensor = prim::If(%3704) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3706 : float = prim::GetAttr[name="drop_rate"](%1894)
      %3707 : bool = prim::GetAttr[name="training"](%1894)
      %3708 : bool = aten::lt(%3706, %16) # torch/nn/functional.py:968:7
      %3709 : bool = prim::If(%3708) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3710 : bool = aten::gt(%3706, %17) # torch/nn/functional.py:968:17
          -> (%3710)
       = prim::If(%3709) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3711 : Tensor = aten::dropout(%new_features.137, %3706, %3707) # torch/nn/functional.py:973:17
      -> (%3711)
    block1():
      -> (%new_features.137)
  %3712 : Tensor[] = aten::append(%features.4, %new_features.138) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3713 : Tensor = prim::Uninitialized()
  %3714 : bool = prim::GetAttr[name="memory_efficient"](%1895)
  %3715 : bool = prim::If(%3714) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3716 : bool = prim::Uninitialized()
      %3717 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3718 : bool = aten::gt(%3717, %24)
      %3719 : bool, %3720 : bool, %3721 : int = prim::Loop(%18, %3718, %19, %3716, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3722 : int, %3723 : bool, %3724 : bool, %3725 : int):
          %tensor.70 : Tensor = aten::__getitem__(%features.4, %3725) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3727 : bool = prim::requires_grad(%tensor.70)
          %3728 : bool, %3729 : bool = prim::If(%3727) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3716)
          %3730 : int = aten::add(%3725, %27)
          %3731 : bool = aten::lt(%3730, %3717)
          %3732 : bool = aten::__and__(%3731, %3728)
          -> (%3732, %3727, %3729, %3730)
      %3733 : bool = prim::If(%3719)
        block0():
          -> (%3720)
        block1():
          -> (%19)
      -> (%3733)
    block1():
      -> (%19)
  %bottleneck_output.138 : Tensor = prim::If(%3715) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3713)
    block1():
      %concated_features.70 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3736 : __torch__.torch.nn.modules.conv.___torch_mangle_143.Conv2d = prim::GetAttr[name="conv1"](%1895)
      %3737 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%1895)
      %3738 : int = aten::dim(%concated_features.70) # torch/nn/modules/batchnorm.py:276:11
      %3739 : bool = aten::ne(%3738, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3739) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3740 : bool = prim::GetAttr[name="training"](%3737)
       = prim::If(%3740) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3741 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3737)
          %3742 : Tensor = aten::add(%3741, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3737, %3742)
          -> ()
        block1():
          -> ()
      %3743 : bool = prim::GetAttr[name="training"](%3737)
      %3744 : Tensor = prim::GetAttr[name="running_mean"](%3737)
      %3745 : Tensor = prim::GetAttr[name="running_var"](%3737)
      %3746 : Tensor = prim::GetAttr[name="weight"](%3737)
      %3747 : Tensor = prim::GetAttr[name="bias"](%3737)
       = prim::If(%3743) # torch/nn/functional.py:2011:4
        block0():
          %3748 : int[] = aten::size(%concated_features.70) # torch/nn/functional.py:2012:27
          %size_prods.568 : int = aten::__getitem__(%3748, %24) # torch/nn/functional.py:1991:17
          %3750 : int = aten::len(%3748) # torch/nn/functional.py:1992:19
          %3751 : int = aten::sub(%3750, %26) # torch/nn/functional.py:1992:19
          %size_prods.569 : int = prim::Loop(%3751, %25, %size_prods.568) # torch/nn/functional.py:1992:4
            block0(%i.143 : int, %size_prods.570 : int):
              %3755 : int = aten::add(%i.143, %26) # torch/nn/functional.py:1993:27
              %3756 : int = aten::__getitem__(%3748, %3755) # torch/nn/functional.py:1993:22
              %size_prods.571 : int = aten::mul(%size_prods.570, %3756) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.571)
          %3758 : bool = aten::eq(%size_prods.569, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3758) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3759 : Tensor = aten::batch_norm(%concated_features.70, %3746, %3747, %3744, %3745, %3743, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.139 : Tensor = aten::relu_(%3759) # torch/nn/functional.py:1117:17
      %3761 : Tensor = prim::GetAttr[name="weight"](%3736)
      %3762 : Tensor? = prim::GetAttr[name="bias"](%3736)
      %3763 : int[] = prim::ListConstruct(%27, %27)
      %3764 : int[] = prim::ListConstruct(%24, %24)
      %3765 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.139 : Tensor = aten::conv2d(%result.139, %3761, %3762, %3763, %3764, %3765, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.139)
  %3767 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1895)
  %3768 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1895)
  %3769 : int = aten::dim(%bottleneck_output.138) # torch/nn/modules/batchnorm.py:276:11
  %3770 : bool = aten::ne(%3769, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3770) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3771 : bool = prim::GetAttr[name="training"](%3768)
   = prim::If(%3771) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3772 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3768)
      %3773 : Tensor = aten::add(%3772, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3768, %3773)
      -> ()
    block1():
      -> ()
  %3774 : bool = prim::GetAttr[name="training"](%3768)
  %3775 : Tensor = prim::GetAttr[name="running_mean"](%3768)
  %3776 : Tensor = prim::GetAttr[name="running_var"](%3768)
  %3777 : Tensor = prim::GetAttr[name="weight"](%3768)
  %3778 : Tensor = prim::GetAttr[name="bias"](%3768)
   = prim::If(%3774) # torch/nn/functional.py:2011:4
    block0():
      %3779 : int[] = aten::size(%bottleneck_output.138) # torch/nn/functional.py:2012:27
      %size_prods.572 : int = aten::__getitem__(%3779, %24) # torch/nn/functional.py:1991:17
      %3781 : int = aten::len(%3779) # torch/nn/functional.py:1992:19
      %3782 : int = aten::sub(%3781, %26) # torch/nn/functional.py:1992:19
      %size_prods.573 : int = prim::Loop(%3782, %25, %size_prods.572) # torch/nn/functional.py:1992:4
        block0(%i.144 : int, %size_prods.574 : int):
          %3786 : int = aten::add(%i.144, %26) # torch/nn/functional.py:1993:27
          %3787 : int = aten::__getitem__(%3779, %3786) # torch/nn/functional.py:1993:22
          %size_prods.575 : int = aten::mul(%size_prods.574, %3787) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.575)
      %3789 : bool = aten::eq(%size_prods.573, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3789) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3790 : Tensor = aten::batch_norm(%bottleneck_output.138, %3777, %3778, %3775, %3776, %3774, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.140 : Tensor = aten::relu_(%3790) # torch/nn/functional.py:1117:17
  %3792 : Tensor = prim::GetAttr[name="weight"](%3767)
  %3793 : Tensor? = prim::GetAttr[name="bias"](%3767)
  %3794 : int[] = prim::ListConstruct(%27, %27)
  %3795 : int[] = prim::ListConstruct(%27, %27)
  %3796 : int[] = prim::ListConstruct(%27, %27)
  %new_features.139 : Tensor = aten::conv2d(%result.140, %3792, %3793, %3794, %3795, %3796, %27) # torch/nn/modules/conv.py:415:15
  %3798 : float = prim::GetAttr[name="drop_rate"](%1895)
  %3799 : bool = aten::gt(%3798, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.140 : Tensor = prim::If(%3799) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3801 : float = prim::GetAttr[name="drop_rate"](%1895)
      %3802 : bool = prim::GetAttr[name="training"](%1895)
      %3803 : bool = aten::lt(%3801, %16) # torch/nn/functional.py:968:7
      %3804 : bool = prim::If(%3803) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3805 : bool = aten::gt(%3801, %17) # torch/nn/functional.py:968:17
          -> (%3805)
       = prim::If(%3804) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3806 : Tensor = aten::dropout(%new_features.139, %3801, %3802) # torch/nn/functional.py:973:17
      -> (%3806)
    block1():
      -> (%new_features.139)
  %3807 : Tensor[] = aten::append(%features.4, %new_features.140) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3808 : Tensor = prim::Uninitialized()
  %3809 : bool = prim::GetAttr[name="memory_efficient"](%1896)
  %3810 : bool = prim::If(%3809) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3811 : bool = prim::Uninitialized()
      %3812 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3813 : bool = aten::gt(%3812, %24)
      %3814 : bool, %3815 : bool, %3816 : int = prim::Loop(%18, %3813, %19, %3811, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3817 : int, %3818 : bool, %3819 : bool, %3820 : int):
          %tensor.71 : Tensor = aten::__getitem__(%features.4, %3820) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3822 : bool = prim::requires_grad(%tensor.71)
          %3823 : bool, %3824 : bool = prim::If(%3822) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3811)
          %3825 : int = aten::add(%3820, %27)
          %3826 : bool = aten::lt(%3825, %3812)
          %3827 : bool = aten::__and__(%3826, %3823)
          -> (%3827, %3822, %3824, %3825)
      %3828 : bool = prim::If(%3814)
        block0():
          -> (%3815)
        block1():
          -> (%19)
      -> (%3828)
    block1():
      -> (%19)
  %bottleneck_output.140 : Tensor = prim::If(%3810) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3808)
    block1():
      %concated_features.71 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3831 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%1896)
      %3832 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%1896)
      %3833 : int = aten::dim(%concated_features.71) # torch/nn/modules/batchnorm.py:276:11
      %3834 : bool = aten::ne(%3833, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3834) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3835 : bool = prim::GetAttr[name="training"](%3832)
       = prim::If(%3835) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3836 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3832)
          %3837 : Tensor = aten::add(%3836, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3832, %3837)
          -> ()
        block1():
          -> ()
      %3838 : bool = prim::GetAttr[name="training"](%3832)
      %3839 : Tensor = prim::GetAttr[name="running_mean"](%3832)
      %3840 : Tensor = prim::GetAttr[name="running_var"](%3832)
      %3841 : Tensor = prim::GetAttr[name="weight"](%3832)
      %3842 : Tensor = prim::GetAttr[name="bias"](%3832)
       = prim::If(%3838) # torch/nn/functional.py:2011:4
        block0():
          %3843 : int[] = aten::size(%concated_features.71) # torch/nn/functional.py:2012:27
          %size_prods.576 : int = aten::__getitem__(%3843, %24) # torch/nn/functional.py:1991:17
          %3845 : int = aten::len(%3843) # torch/nn/functional.py:1992:19
          %3846 : int = aten::sub(%3845, %26) # torch/nn/functional.py:1992:19
          %size_prods.577 : int = prim::Loop(%3846, %25, %size_prods.576) # torch/nn/functional.py:1992:4
            block0(%i.145 : int, %size_prods.578 : int):
              %3850 : int = aten::add(%i.145, %26) # torch/nn/functional.py:1993:27
              %3851 : int = aten::__getitem__(%3843, %3850) # torch/nn/functional.py:1993:22
              %size_prods.579 : int = aten::mul(%size_prods.578, %3851) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.579)
          %3853 : bool = aten::eq(%size_prods.577, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3853) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3854 : Tensor = aten::batch_norm(%concated_features.71, %3841, %3842, %3839, %3840, %3838, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.141 : Tensor = aten::relu_(%3854) # torch/nn/functional.py:1117:17
      %3856 : Tensor = prim::GetAttr[name="weight"](%3831)
      %3857 : Tensor? = prim::GetAttr[name="bias"](%3831)
      %3858 : int[] = prim::ListConstruct(%27, %27)
      %3859 : int[] = prim::ListConstruct(%24, %24)
      %3860 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.141 : Tensor = aten::conv2d(%result.141, %3856, %3857, %3858, %3859, %3860, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.141)
  %3862 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1896)
  %3863 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1896)
  %3864 : int = aten::dim(%bottleneck_output.140) # torch/nn/modules/batchnorm.py:276:11
  %3865 : bool = aten::ne(%3864, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3865) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3866 : bool = prim::GetAttr[name="training"](%3863)
   = prim::If(%3866) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3867 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3863)
      %3868 : Tensor = aten::add(%3867, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3863, %3868)
      -> ()
    block1():
      -> ()
  %3869 : bool = prim::GetAttr[name="training"](%3863)
  %3870 : Tensor = prim::GetAttr[name="running_mean"](%3863)
  %3871 : Tensor = prim::GetAttr[name="running_var"](%3863)
  %3872 : Tensor = prim::GetAttr[name="weight"](%3863)
  %3873 : Tensor = prim::GetAttr[name="bias"](%3863)
   = prim::If(%3869) # torch/nn/functional.py:2011:4
    block0():
      %3874 : int[] = aten::size(%bottleneck_output.140) # torch/nn/functional.py:2012:27
      %size_prods.580 : int = aten::__getitem__(%3874, %24) # torch/nn/functional.py:1991:17
      %3876 : int = aten::len(%3874) # torch/nn/functional.py:1992:19
      %3877 : int = aten::sub(%3876, %26) # torch/nn/functional.py:1992:19
      %size_prods.581 : int = prim::Loop(%3877, %25, %size_prods.580) # torch/nn/functional.py:1992:4
        block0(%i.146 : int, %size_prods.582 : int):
          %3881 : int = aten::add(%i.146, %26) # torch/nn/functional.py:1993:27
          %3882 : int = aten::__getitem__(%3874, %3881) # torch/nn/functional.py:1993:22
          %size_prods.583 : int = aten::mul(%size_prods.582, %3882) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.583)
      %3884 : bool = aten::eq(%size_prods.581, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3884) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3885 : Tensor = aten::batch_norm(%bottleneck_output.140, %3872, %3873, %3870, %3871, %3869, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.142 : Tensor = aten::relu_(%3885) # torch/nn/functional.py:1117:17
  %3887 : Tensor = prim::GetAttr[name="weight"](%3862)
  %3888 : Tensor? = prim::GetAttr[name="bias"](%3862)
  %3889 : int[] = prim::ListConstruct(%27, %27)
  %3890 : int[] = prim::ListConstruct(%27, %27)
  %3891 : int[] = prim::ListConstruct(%27, %27)
  %new_features.141 : Tensor = aten::conv2d(%result.142, %3887, %3888, %3889, %3890, %3891, %27) # torch/nn/modules/conv.py:415:15
  %3893 : float = prim::GetAttr[name="drop_rate"](%1896)
  %3894 : bool = aten::gt(%3893, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.142 : Tensor = prim::If(%3894) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3896 : float = prim::GetAttr[name="drop_rate"](%1896)
      %3897 : bool = prim::GetAttr[name="training"](%1896)
      %3898 : bool = aten::lt(%3896, %16) # torch/nn/functional.py:968:7
      %3899 : bool = prim::If(%3898) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3900 : bool = aten::gt(%3896, %17) # torch/nn/functional.py:968:17
          -> (%3900)
       = prim::If(%3899) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3901 : Tensor = aten::dropout(%new_features.141, %3896, %3897) # torch/nn/functional.py:973:17
      -> (%3901)
    block1():
      -> (%new_features.141)
  %3902 : Tensor[] = aten::append(%features.4, %new_features.142) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3903 : Tensor = prim::Uninitialized()
  %3904 : bool = prim::GetAttr[name="memory_efficient"](%1897)
  %3905 : bool = prim::If(%3904) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3906 : bool = prim::Uninitialized()
      %3907 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3908 : bool = aten::gt(%3907, %24)
      %3909 : bool, %3910 : bool, %3911 : int = prim::Loop(%18, %3908, %19, %3906, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3912 : int, %3913 : bool, %3914 : bool, %3915 : int):
          %tensor.72 : Tensor = aten::__getitem__(%features.4, %3915) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3917 : bool = prim::requires_grad(%tensor.72)
          %3918 : bool, %3919 : bool = prim::If(%3917) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3906)
          %3920 : int = aten::add(%3915, %27)
          %3921 : bool = aten::lt(%3920, %3907)
          %3922 : bool = aten::__and__(%3921, %3918)
          -> (%3922, %3917, %3919, %3920)
      %3923 : bool = prim::If(%3909)
        block0():
          -> (%3910)
        block1():
          -> (%19)
      -> (%3923)
    block1():
      -> (%19)
  %bottleneck_output.142 : Tensor = prim::If(%3905) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3903)
    block1():
      %concated_features.72 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3926 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%1897)
      %3927 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%1897)
      %3928 : int = aten::dim(%concated_features.72) # torch/nn/modules/batchnorm.py:276:11
      %3929 : bool = aten::ne(%3928, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3929) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3930 : bool = prim::GetAttr[name="training"](%3927)
       = prim::If(%3930) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3931 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3927)
          %3932 : Tensor = aten::add(%3931, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3927, %3932)
          -> ()
        block1():
          -> ()
      %3933 : bool = prim::GetAttr[name="training"](%3927)
      %3934 : Tensor = prim::GetAttr[name="running_mean"](%3927)
      %3935 : Tensor = prim::GetAttr[name="running_var"](%3927)
      %3936 : Tensor = prim::GetAttr[name="weight"](%3927)
      %3937 : Tensor = prim::GetAttr[name="bias"](%3927)
       = prim::If(%3933) # torch/nn/functional.py:2011:4
        block0():
          %3938 : int[] = aten::size(%concated_features.72) # torch/nn/functional.py:2012:27
          %size_prods.584 : int = aten::__getitem__(%3938, %24) # torch/nn/functional.py:1991:17
          %3940 : int = aten::len(%3938) # torch/nn/functional.py:1992:19
          %3941 : int = aten::sub(%3940, %26) # torch/nn/functional.py:1992:19
          %size_prods.585 : int = prim::Loop(%3941, %25, %size_prods.584) # torch/nn/functional.py:1992:4
            block0(%i.147 : int, %size_prods.586 : int):
              %3945 : int = aten::add(%i.147, %26) # torch/nn/functional.py:1993:27
              %3946 : int = aten::__getitem__(%3938, %3945) # torch/nn/functional.py:1993:22
              %size_prods.587 : int = aten::mul(%size_prods.586, %3946) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.587)
          %3948 : bool = aten::eq(%size_prods.585, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3948) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3949 : Tensor = aten::batch_norm(%concated_features.72, %3936, %3937, %3934, %3935, %3933, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.143 : Tensor = aten::relu_(%3949) # torch/nn/functional.py:1117:17
      %3951 : Tensor = prim::GetAttr[name="weight"](%3926)
      %3952 : Tensor? = prim::GetAttr[name="bias"](%3926)
      %3953 : int[] = prim::ListConstruct(%27, %27)
      %3954 : int[] = prim::ListConstruct(%24, %24)
      %3955 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.143 : Tensor = aten::conv2d(%result.143, %3951, %3952, %3953, %3954, %3955, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.143)
  %3957 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1897)
  %3958 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1897)
  %3959 : int = aten::dim(%bottleneck_output.142) # torch/nn/modules/batchnorm.py:276:11
  %3960 : bool = aten::ne(%3959, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3960) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3961 : bool = prim::GetAttr[name="training"](%3958)
   = prim::If(%3961) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3962 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3958)
      %3963 : Tensor = aten::add(%3962, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3958, %3963)
      -> ()
    block1():
      -> ()
  %3964 : bool = prim::GetAttr[name="training"](%3958)
  %3965 : Tensor = prim::GetAttr[name="running_mean"](%3958)
  %3966 : Tensor = prim::GetAttr[name="running_var"](%3958)
  %3967 : Tensor = prim::GetAttr[name="weight"](%3958)
  %3968 : Tensor = prim::GetAttr[name="bias"](%3958)
   = prim::If(%3964) # torch/nn/functional.py:2011:4
    block0():
      %3969 : int[] = aten::size(%bottleneck_output.142) # torch/nn/functional.py:2012:27
      %size_prods.588 : int = aten::__getitem__(%3969, %24) # torch/nn/functional.py:1991:17
      %3971 : int = aten::len(%3969) # torch/nn/functional.py:1992:19
      %3972 : int = aten::sub(%3971, %26) # torch/nn/functional.py:1992:19
      %size_prods.589 : int = prim::Loop(%3972, %25, %size_prods.588) # torch/nn/functional.py:1992:4
        block0(%i.148 : int, %size_prods.590 : int):
          %3976 : int = aten::add(%i.148, %26) # torch/nn/functional.py:1993:27
          %3977 : int = aten::__getitem__(%3969, %3976) # torch/nn/functional.py:1993:22
          %size_prods.591 : int = aten::mul(%size_prods.590, %3977) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.591)
      %3979 : bool = aten::eq(%size_prods.589, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3979) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3980 : Tensor = aten::batch_norm(%bottleneck_output.142, %3967, %3968, %3965, %3966, %3964, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.144 : Tensor = aten::relu_(%3980) # torch/nn/functional.py:1117:17
  %3982 : Tensor = prim::GetAttr[name="weight"](%3957)
  %3983 : Tensor? = prim::GetAttr[name="bias"](%3957)
  %3984 : int[] = prim::ListConstruct(%27, %27)
  %3985 : int[] = prim::ListConstruct(%27, %27)
  %3986 : int[] = prim::ListConstruct(%27, %27)
  %new_features.143 : Tensor = aten::conv2d(%result.144, %3982, %3983, %3984, %3985, %3986, %27) # torch/nn/modules/conv.py:415:15
  %3988 : float = prim::GetAttr[name="drop_rate"](%1897)
  %3989 : bool = aten::gt(%3988, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.144 : Tensor = prim::If(%3989) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3991 : float = prim::GetAttr[name="drop_rate"](%1897)
      %3992 : bool = prim::GetAttr[name="training"](%1897)
      %3993 : bool = aten::lt(%3991, %16) # torch/nn/functional.py:968:7
      %3994 : bool = prim::If(%3993) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3995 : bool = aten::gt(%3991, %17) # torch/nn/functional.py:968:17
          -> (%3995)
       = prim::If(%3994) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3996 : Tensor = aten::dropout(%new_features.143, %3991, %3992) # torch/nn/functional.py:973:17
      -> (%3996)
    block1():
      -> (%new_features.143)
  %3997 : Tensor[] = aten::append(%features.4, %new_features.144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3998 : Tensor = prim::Uninitialized()
  %3999 : bool = prim::GetAttr[name="memory_efficient"](%1898)
  %4000 : bool = prim::If(%3999) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4001 : bool = prim::Uninitialized()
      %4002 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4003 : bool = aten::gt(%4002, %24)
      %4004 : bool, %4005 : bool, %4006 : int = prim::Loop(%18, %4003, %19, %4001, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4007 : int, %4008 : bool, %4009 : bool, %4010 : int):
          %tensor.73 : Tensor = aten::__getitem__(%features.4, %4010) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4012 : bool = prim::requires_grad(%tensor.73)
          %4013 : bool, %4014 : bool = prim::If(%4012) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4001)
          %4015 : int = aten::add(%4010, %27)
          %4016 : bool = aten::lt(%4015, %4002)
          %4017 : bool = aten::__and__(%4016, %4013)
          -> (%4017, %4012, %4014, %4015)
      %4018 : bool = prim::If(%4004)
        block0():
          -> (%4005)
        block1():
          -> (%19)
      -> (%4018)
    block1():
      -> (%19)
  %bottleneck_output.144 : Tensor = prim::If(%4000) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3998)
    block1():
      %concated_features.73 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4021 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%1898)
      %4022 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%1898)
      %4023 : int = aten::dim(%concated_features.73) # torch/nn/modules/batchnorm.py:276:11
      %4024 : bool = aten::ne(%4023, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4024) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4025 : bool = prim::GetAttr[name="training"](%4022)
       = prim::If(%4025) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4026 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4022)
          %4027 : Tensor = aten::add(%4026, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4022, %4027)
          -> ()
        block1():
          -> ()
      %4028 : bool = prim::GetAttr[name="training"](%4022)
      %4029 : Tensor = prim::GetAttr[name="running_mean"](%4022)
      %4030 : Tensor = prim::GetAttr[name="running_var"](%4022)
      %4031 : Tensor = prim::GetAttr[name="weight"](%4022)
      %4032 : Tensor = prim::GetAttr[name="bias"](%4022)
       = prim::If(%4028) # torch/nn/functional.py:2011:4
        block0():
          %4033 : int[] = aten::size(%concated_features.73) # torch/nn/functional.py:2012:27
          %size_prods.592 : int = aten::__getitem__(%4033, %24) # torch/nn/functional.py:1991:17
          %4035 : int = aten::len(%4033) # torch/nn/functional.py:1992:19
          %4036 : int = aten::sub(%4035, %26) # torch/nn/functional.py:1992:19
          %size_prods.593 : int = prim::Loop(%4036, %25, %size_prods.592) # torch/nn/functional.py:1992:4
            block0(%i.149 : int, %size_prods.594 : int):
              %4040 : int = aten::add(%i.149, %26) # torch/nn/functional.py:1993:27
              %4041 : int = aten::__getitem__(%4033, %4040) # torch/nn/functional.py:1993:22
              %size_prods.595 : int = aten::mul(%size_prods.594, %4041) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.595)
          %4043 : bool = aten::eq(%size_prods.593, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4043) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4044 : Tensor = aten::batch_norm(%concated_features.73, %4031, %4032, %4029, %4030, %4028, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.145 : Tensor = aten::relu_(%4044) # torch/nn/functional.py:1117:17
      %4046 : Tensor = prim::GetAttr[name="weight"](%4021)
      %4047 : Tensor? = prim::GetAttr[name="bias"](%4021)
      %4048 : int[] = prim::ListConstruct(%27, %27)
      %4049 : int[] = prim::ListConstruct(%24, %24)
      %4050 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.145 : Tensor = aten::conv2d(%result.145, %4046, %4047, %4048, %4049, %4050, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.145)
  %4052 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1898)
  %4053 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1898)
  %4054 : int = aten::dim(%bottleneck_output.144) # torch/nn/modules/batchnorm.py:276:11
  %4055 : bool = aten::ne(%4054, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4055) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4056 : bool = prim::GetAttr[name="training"](%4053)
   = prim::If(%4056) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4057 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4053)
      %4058 : Tensor = aten::add(%4057, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4053, %4058)
      -> ()
    block1():
      -> ()
  %4059 : bool = prim::GetAttr[name="training"](%4053)
  %4060 : Tensor = prim::GetAttr[name="running_mean"](%4053)
  %4061 : Tensor = prim::GetAttr[name="running_var"](%4053)
  %4062 : Tensor = prim::GetAttr[name="weight"](%4053)
  %4063 : Tensor = prim::GetAttr[name="bias"](%4053)
   = prim::If(%4059) # torch/nn/functional.py:2011:4
    block0():
      %4064 : int[] = aten::size(%bottleneck_output.144) # torch/nn/functional.py:2012:27
      %size_prods.596 : int = aten::__getitem__(%4064, %24) # torch/nn/functional.py:1991:17
      %4066 : int = aten::len(%4064) # torch/nn/functional.py:1992:19
      %4067 : int = aten::sub(%4066, %26) # torch/nn/functional.py:1992:19
      %size_prods.597 : int = prim::Loop(%4067, %25, %size_prods.596) # torch/nn/functional.py:1992:4
        block0(%i.150 : int, %size_prods.598 : int):
          %4071 : int = aten::add(%i.150, %26) # torch/nn/functional.py:1993:27
          %4072 : int = aten::__getitem__(%4064, %4071) # torch/nn/functional.py:1993:22
          %size_prods.599 : int = aten::mul(%size_prods.598, %4072) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.599)
      %4074 : bool = aten::eq(%size_prods.597, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4074) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4075 : Tensor = aten::batch_norm(%bottleneck_output.144, %4062, %4063, %4060, %4061, %4059, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.146 : Tensor = aten::relu_(%4075) # torch/nn/functional.py:1117:17
  %4077 : Tensor = prim::GetAttr[name="weight"](%4052)
  %4078 : Tensor? = prim::GetAttr[name="bias"](%4052)
  %4079 : int[] = prim::ListConstruct(%27, %27)
  %4080 : int[] = prim::ListConstruct(%27, %27)
  %4081 : int[] = prim::ListConstruct(%27, %27)
  %new_features.145 : Tensor = aten::conv2d(%result.146, %4077, %4078, %4079, %4080, %4081, %27) # torch/nn/modules/conv.py:415:15
  %4083 : float = prim::GetAttr[name="drop_rate"](%1898)
  %4084 : bool = aten::gt(%4083, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.146 : Tensor = prim::If(%4084) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4086 : float = prim::GetAttr[name="drop_rate"](%1898)
      %4087 : bool = prim::GetAttr[name="training"](%1898)
      %4088 : bool = aten::lt(%4086, %16) # torch/nn/functional.py:968:7
      %4089 : bool = prim::If(%4088) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4090 : bool = aten::gt(%4086, %17) # torch/nn/functional.py:968:17
          -> (%4090)
       = prim::If(%4089) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4091 : Tensor = aten::dropout(%new_features.145, %4086, %4087) # torch/nn/functional.py:973:17
      -> (%4091)
    block1():
      -> (%new_features.145)
  %4092 : Tensor[] = aten::append(%features.4, %new_features.146) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4093 : Tensor = prim::Uninitialized()
  %4094 : bool = prim::GetAttr[name="memory_efficient"](%1899)
  %4095 : bool = prim::If(%4094) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4096 : bool = prim::Uninitialized()
      %4097 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4098 : bool = aten::gt(%4097, %24)
      %4099 : bool, %4100 : bool, %4101 : int = prim::Loop(%18, %4098, %19, %4096, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4102 : int, %4103 : bool, %4104 : bool, %4105 : int):
          %tensor.74 : Tensor = aten::__getitem__(%features.4, %4105) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4107 : bool = prim::requires_grad(%tensor.74)
          %4108 : bool, %4109 : bool = prim::If(%4107) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4096)
          %4110 : int = aten::add(%4105, %27)
          %4111 : bool = aten::lt(%4110, %4097)
          %4112 : bool = aten::__and__(%4111, %4108)
          -> (%4112, %4107, %4109, %4110)
      %4113 : bool = prim::If(%4099)
        block0():
          -> (%4100)
        block1():
          -> (%19)
      -> (%4113)
    block1():
      -> (%19)
  %bottleneck_output.146 : Tensor = prim::If(%4095) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4093)
    block1():
      %concated_features.74 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4116 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%1899)
      %4117 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%1899)
      %4118 : int = aten::dim(%concated_features.74) # torch/nn/modules/batchnorm.py:276:11
      %4119 : bool = aten::ne(%4118, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4119) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4120 : bool = prim::GetAttr[name="training"](%4117)
       = prim::If(%4120) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4121 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4117)
          %4122 : Tensor = aten::add(%4121, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4117, %4122)
          -> ()
        block1():
          -> ()
      %4123 : bool = prim::GetAttr[name="training"](%4117)
      %4124 : Tensor = prim::GetAttr[name="running_mean"](%4117)
      %4125 : Tensor = prim::GetAttr[name="running_var"](%4117)
      %4126 : Tensor = prim::GetAttr[name="weight"](%4117)
      %4127 : Tensor = prim::GetAttr[name="bias"](%4117)
       = prim::If(%4123) # torch/nn/functional.py:2011:4
        block0():
          %4128 : int[] = aten::size(%concated_features.74) # torch/nn/functional.py:2012:27
          %size_prods.600 : int = aten::__getitem__(%4128, %24) # torch/nn/functional.py:1991:17
          %4130 : int = aten::len(%4128) # torch/nn/functional.py:1992:19
          %4131 : int = aten::sub(%4130, %26) # torch/nn/functional.py:1992:19
          %size_prods.601 : int = prim::Loop(%4131, %25, %size_prods.600) # torch/nn/functional.py:1992:4
            block0(%i.151 : int, %size_prods.602 : int):
              %4135 : int = aten::add(%i.151, %26) # torch/nn/functional.py:1993:27
              %4136 : int = aten::__getitem__(%4128, %4135) # torch/nn/functional.py:1993:22
              %size_prods.603 : int = aten::mul(%size_prods.602, %4136) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.603)
          %4138 : bool = aten::eq(%size_prods.601, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4138) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4139 : Tensor = aten::batch_norm(%concated_features.74, %4126, %4127, %4124, %4125, %4123, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.147 : Tensor = aten::relu_(%4139) # torch/nn/functional.py:1117:17
      %4141 : Tensor = prim::GetAttr[name="weight"](%4116)
      %4142 : Tensor? = prim::GetAttr[name="bias"](%4116)
      %4143 : int[] = prim::ListConstruct(%27, %27)
      %4144 : int[] = prim::ListConstruct(%24, %24)
      %4145 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.147 : Tensor = aten::conv2d(%result.147, %4141, %4142, %4143, %4144, %4145, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.147)
  %4147 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1899)
  %4148 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1899)
  %4149 : int = aten::dim(%bottleneck_output.146) # torch/nn/modules/batchnorm.py:276:11
  %4150 : bool = aten::ne(%4149, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4150) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4151 : bool = prim::GetAttr[name="training"](%4148)
   = prim::If(%4151) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4152 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4148)
      %4153 : Tensor = aten::add(%4152, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4148, %4153)
      -> ()
    block1():
      -> ()
  %4154 : bool = prim::GetAttr[name="training"](%4148)
  %4155 : Tensor = prim::GetAttr[name="running_mean"](%4148)
  %4156 : Tensor = prim::GetAttr[name="running_var"](%4148)
  %4157 : Tensor = prim::GetAttr[name="weight"](%4148)
  %4158 : Tensor = prim::GetAttr[name="bias"](%4148)
   = prim::If(%4154) # torch/nn/functional.py:2011:4
    block0():
      %4159 : int[] = aten::size(%bottleneck_output.146) # torch/nn/functional.py:2012:27
      %size_prods.604 : int = aten::__getitem__(%4159, %24) # torch/nn/functional.py:1991:17
      %4161 : int = aten::len(%4159) # torch/nn/functional.py:1992:19
      %4162 : int = aten::sub(%4161, %26) # torch/nn/functional.py:1992:19
      %size_prods.605 : int = prim::Loop(%4162, %25, %size_prods.604) # torch/nn/functional.py:1992:4
        block0(%i.152 : int, %size_prods.606 : int):
          %4166 : int = aten::add(%i.152, %26) # torch/nn/functional.py:1993:27
          %4167 : int = aten::__getitem__(%4159, %4166) # torch/nn/functional.py:1993:22
          %size_prods.607 : int = aten::mul(%size_prods.606, %4167) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.607)
      %4169 : bool = aten::eq(%size_prods.605, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4169) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4170 : Tensor = aten::batch_norm(%bottleneck_output.146, %4157, %4158, %4155, %4156, %4154, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.148 : Tensor = aten::relu_(%4170) # torch/nn/functional.py:1117:17
  %4172 : Tensor = prim::GetAttr[name="weight"](%4147)
  %4173 : Tensor? = prim::GetAttr[name="bias"](%4147)
  %4174 : int[] = prim::ListConstruct(%27, %27)
  %4175 : int[] = prim::ListConstruct(%27, %27)
  %4176 : int[] = prim::ListConstruct(%27, %27)
  %new_features.147 : Tensor = aten::conv2d(%result.148, %4172, %4173, %4174, %4175, %4176, %27) # torch/nn/modules/conv.py:415:15
  %4178 : float = prim::GetAttr[name="drop_rate"](%1899)
  %4179 : bool = aten::gt(%4178, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.148 : Tensor = prim::If(%4179) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4181 : float = prim::GetAttr[name="drop_rate"](%1899)
      %4182 : bool = prim::GetAttr[name="training"](%1899)
      %4183 : bool = aten::lt(%4181, %16) # torch/nn/functional.py:968:7
      %4184 : bool = prim::If(%4183) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4185 : bool = aten::gt(%4181, %17) # torch/nn/functional.py:968:17
          -> (%4185)
       = prim::If(%4184) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4186 : Tensor = aten::dropout(%new_features.147, %4181, %4182) # torch/nn/functional.py:973:17
      -> (%4186)
    block1():
      -> (%new_features.147)
  %4187 : Tensor[] = aten::append(%features.4, %new_features.148) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4188 : Tensor = prim::Uninitialized()
  %4189 : bool = prim::GetAttr[name="memory_efficient"](%1900)
  %4190 : bool = prim::If(%4189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4191 : bool = prim::Uninitialized()
      %4192 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4193 : bool = aten::gt(%4192, %24)
      %4194 : bool, %4195 : bool, %4196 : int = prim::Loop(%18, %4193, %19, %4191, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4197 : int, %4198 : bool, %4199 : bool, %4200 : int):
          %tensor.75 : Tensor = aten::__getitem__(%features.4, %4200) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4202 : bool = prim::requires_grad(%tensor.75)
          %4203 : bool, %4204 : bool = prim::If(%4202) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4191)
          %4205 : int = aten::add(%4200, %27)
          %4206 : bool = aten::lt(%4205, %4192)
          %4207 : bool = aten::__and__(%4206, %4203)
          -> (%4207, %4202, %4204, %4205)
      %4208 : bool = prim::If(%4194)
        block0():
          -> (%4195)
        block1():
          -> (%19)
      -> (%4208)
    block1():
      -> (%19)
  %bottleneck_output.148 : Tensor = prim::If(%4190) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4188)
    block1():
      %concated_features.75 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4211 : __torch__.torch.nn.modules.conv.___torch_mangle_299.Conv2d = prim::GetAttr[name="conv1"](%1900)
      %4212 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm1"](%1900)
      %4213 : int = aten::dim(%concated_features.75) # torch/nn/modules/batchnorm.py:276:11
      %4214 : bool = aten::ne(%4213, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4214) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4215 : bool = prim::GetAttr[name="training"](%4212)
       = prim::If(%4215) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4216 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4212)
          %4217 : Tensor = aten::add(%4216, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4212, %4217)
          -> ()
        block1():
          -> ()
      %4218 : bool = prim::GetAttr[name="training"](%4212)
      %4219 : Tensor = prim::GetAttr[name="running_mean"](%4212)
      %4220 : Tensor = prim::GetAttr[name="running_var"](%4212)
      %4221 : Tensor = prim::GetAttr[name="weight"](%4212)
      %4222 : Tensor = prim::GetAttr[name="bias"](%4212)
       = prim::If(%4218) # torch/nn/functional.py:2011:4
        block0():
          %4223 : int[] = aten::size(%concated_features.75) # torch/nn/functional.py:2012:27
          %size_prods.608 : int = aten::__getitem__(%4223, %24) # torch/nn/functional.py:1991:17
          %4225 : int = aten::len(%4223) # torch/nn/functional.py:1992:19
          %4226 : int = aten::sub(%4225, %26) # torch/nn/functional.py:1992:19
          %size_prods.609 : int = prim::Loop(%4226, %25, %size_prods.608) # torch/nn/functional.py:1992:4
            block0(%i.153 : int, %size_prods.610 : int):
              %4230 : int = aten::add(%i.153, %26) # torch/nn/functional.py:1993:27
              %4231 : int = aten::__getitem__(%4223, %4230) # torch/nn/functional.py:1993:22
              %size_prods.611 : int = aten::mul(%size_prods.610, %4231) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.611)
          %4233 : bool = aten::eq(%size_prods.609, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4233) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4234 : Tensor = aten::batch_norm(%concated_features.75, %4221, %4222, %4219, %4220, %4218, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.149 : Tensor = aten::relu_(%4234) # torch/nn/functional.py:1117:17
      %4236 : Tensor = prim::GetAttr[name="weight"](%4211)
      %4237 : Tensor? = prim::GetAttr[name="bias"](%4211)
      %4238 : int[] = prim::ListConstruct(%27, %27)
      %4239 : int[] = prim::ListConstruct(%24, %24)
      %4240 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.149 : Tensor = aten::conv2d(%result.149, %4236, %4237, %4238, %4239, %4240, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.149)
  %4242 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1900)
  %4243 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1900)
  %4244 : int = aten::dim(%bottleneck_output.148) # torch/nn/modules/batchnorm.py:276:11
  %4245 : bool = aten::ne(%4244, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4245) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4246 : bool = prim::GetAttr[name="training"](%4243)
   = prim::If(%4246) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4247 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4243)
      %4248 : Tensor = aten::add(%4247, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4243, %4248)
      -> ()
    block1():
      -> ()
  %4249 : bool = prim::GetAttr[name="training"](%4243)
  %4250 : Tensor = prim::GetAttr[name="running_mean"](%4243)
  %4251 : Tensor = prim::GetAttr[name="running_var"](%4243)
  %4252 : Tensor = prim::GetAttr[name="weight"](%4243)
  %4253 : Tensor = prim::GetAttr[name="bias"](%4243)
   = prim::If(%4249) # torch/nn/functional.py:2011:4
    block0():
      %4254 : int[] = aten::size(%bottleneck_output.148) # torch/nn/functional.py:2012:27
      %size_prods.612 : int = aten::__getitem__(%4254, %24) # torch/nn/functional.py:1991:17
      %4256 : int = aten::len(%4254) # torch/nn/functional.py:1992:19
      %4257 : int = aten::sub(%4256, %26) # torch/nn/functional.py:1992:19
      %size_prods.613 : int = prim::Loop(%4257, %25, %size_prods.612) # torch/nn/functional.py:1992:4
        block0(%i.154 : int, %size_prods.614 : int):
          %4261 : int = aten::add(%i.154, %26) # torch/nn/functional.py:1993:27
          %4262 : int = aten::__getitem__(%4254, %4261) # torch/nn/functional.py:1993:22
          %size_prods.615 : int = aten::mul(%size_prods.614, %4262) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.615)
      %4264 : bool = aten::eq(%size_prods.613, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4264) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4265 : Tensor = aten::batch_norm(%bottleneck_output.148, %4252, %4253, %4250, %4251, %4249, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.150 : Tensor = aten::relu_(%4265) # torch/nn/functional.py:1117:17
  %4267 : Tensor = prim::GetAttr[name="weight"](%4242)
  %4268 : Tensor? = prim::GetAttr[name="bias"](%4242)
  %4269 : int[] = prim::ListConstruct(%27, %27)
  %4270 : int[] = prim::ListConstruct(%27, %27)
  %4271 : int[] = prim::ListConstruct(%27, %27)
  %new_features.149 : Tensor = aten::conv2d(%result.150, %4267, %4268, %4269, %4270, %4271, %27) # torch/nn/modules/conv.py:415:15
  %4273 : float = prim::GetAttr[name="drop_rate"](%1900)
  %4274 : bool = aten::gt(%4273, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.150 : Tensor = prim::If(%4274) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4276 : float = prim::GetAttr[name="drop_rate"](%1900)
      %4277 : bool = prim::GetAttr[name="training"](%1900)
      %4278 : bool = aten::lt(%4276, %16) # torch/nn/functional.py:968:7
      %4279 : bool = prim::If(%4278) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4280 : bool = aten::gt(%4276, %17) # torch/nn/functional.py:968:17
          -> (%4280)
       = prim::If(%4279) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4281 : Tensor = aten::dropout(%new_features.149, %4276, %4277) # torch/nn/functional.py:973:17
      -> (%4281)
    block1():
      -> (%new_features.149)
  %4282 : Tensor[] = aten::append(%features.4, %new_features.150) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4283 : Tensor = prim::Uninitialized()
  %4284 : bool = prim::GetAttr[name="memory_efficient"](%1901)
  %4285 : bool = prim::If(%4284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4286 : bool = prim::Uninitialized()
      %4287 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4288 : bool = aten::gt(%4287, %24)
      %4289 : bool, %4290 : bool, %4291 : int = prim::Loop(%18, %4288, %19, %4286, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4292 : int, %4293 : bool, %4294 : bool, %4295 : int):
          %tensor.76 : Tensor = aten::__getitem__(%features.4, %4295) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4297 : bool = prim::requires_grad(%tensor.76)
          %4298 : bool, %4299 : bool = prim::If(%4297) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4286)
          %4300 : int = aten::add(%4295, %27)
          %4301 : bool = aten::lt(%4300, %4287)
          %4302 : bool = aten::__and__(%4301, %4298)
          -> (%4302, %4297, %4299, %4300)
      %4303 : bool = prim::If(%4289)
        block0():
          -> (%4290)
        block1():
          -> (%19)
      -> (%4303)
    block1():
      -> (%19)
  %bottleneck_output.150 : Tensor = prim::If(%4285) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4283)
    block1():
      %concated_features.76 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4306 : __torch__.torch.nn.modules.conv.___torch_mangle_301.Conv2d = prim::GetAttr[name="conv1"](%1901)
      %4307 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%1901)
      %4308 : int = aten::dim(%concated_features.76) # torch/nn/modules/batchnorm.py:276:11
      %4309 : bool = aten::ne(%4308, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4309) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4310 : bool = prim::GetAttr[name="training"](%4307)
       = prim::If(%4310) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4311 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4307)
          %4312 : Tensor = aten::add(%4311, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4307, %4312)
          -> ()
        block1():
          -> ()
      %4313 : bool = prim::GetAttr[name="training"](%4307)
      %4314 : Tensor = prim::GetAttr[name="running_mean"](%4307)
      %4315 : Tensor = prim::GetAttr[name="running_var"](%4307)
      %4316 : Tensor = prim::GetAttr[name="weight"](%4307)
      %4317 : Tensor = prim::GetAttr[name="bias"](%4307)
       = prim::If(%4313) # torch/nn/functional.py:2011:4
        block0():
          %4318 : int[] = aten::size(%concated_features.76) # torch/nn/functional.py:2012:27
          %size_prods.616 : int = aten::__getitem__(%4318, %24) # torch/nn/functional.py:1991:17
          %4320 : int = aten::len(%4318) # torch/nn/functional.py:1992:19
          %4321 : int = aten::sub(%4320, %26) # torch/nn/functional.py:1992:19
          %size_prods.617 : int = prim::Loop(%4321, %25, %size_prods.616) # torch/nn/functional.py:1992:4
            block0(%i.155 : int, %size_prods.618 : int):
              %4325 : int = aten::add(%i.155, %26) # torch/nn/functional.py:1993:27
              %4326 : int = aten::__getitem__(%4318, %4325) # torch/nn/functional.py:1993:22
              %size_prods.619 : int = aten::mul(%size_prods.618, %4326) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.619)
          %4328 : bool = aten::eq(%size_prods.617, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4328) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4329 : Tensor = aten::batch_norm(%concated_features.76, %4316, %4317, %4314, %4315, %4313, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.151 : Tensor = aten::relu_(%4329) # torch/nn/functional.py:1117:17
      %4331 : Tensor = prim::GetAttr[name="weight"](%4306)
      %4332 : Tensor? = prim::GetAttr[name="bias"](%4306)
      %4333 : int[] = prim::ListConstruct(%27, %27)
      %4334 : int[] = prim::ListConstruct(%24, %24)
      %4335 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.151 : Tensor = aten::conv2d(%result.151, %4331, %4332, %4333, %4334, %4335, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.151)
  %4337 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1901)
  %4338 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1901)
  %4339 : int = aten::dim(%bottleneck_output.150) # torch/nn/modules/batchnorm.py:276:11
  %4340 : bool = aten::ne(%4339, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4340) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4341 : bool = prim::GetAttr[name="training"](%4338)
   = prim::If(%4341) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4342 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4338)
      %4343 : Tensor = aten::add(%4342, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4338, %4343)
      -> ()
    block1():
      -> ()
  %4344 : bool = prim::GetAttr[name="training"](%4338)
  %4345 : Tensor = prim::GetAttr[name="running_mean"](%4338)
  %4346 : Tensor = prim::GetAttr[name="running_var"](%4338)
  %4347 : Tensor = prim::GetAttr[name="weight"](%4338)
  %4348 : Tensor = prim::GetAttr[name="bias"](%4338)
   = prim::If(%4344) # torch/nn/functional.py:2011:4
    block0():
      %4349 : int[] = aten::size(%bottleneck_output.150) # torch/nn/functional.py:2012:27
      %size_prods.620 : int = aten::__getitem__(%4349, %24) # torch/nn/functional.py:1991:17
      %4351 : int = aten::len(%4349) # torch/nn/functional.py:1992:19
      %4352 : int = aten::sub(%4351, %26) # torch/nn/functional.py:1992:19
      %size_prods.621 : int = prim::Loop(%4352, %25, %size_prods.620) # torch/nn/functional.py:1992:4
        block0(%i.156 : int, %size_prods.622 : int):
          %4356 : int = aten::add(%i.156, %26) # torch/nn/functional.py:1993:27
          %4357 : int = aten::__getitem__(%4349, %4356) # torch/nn/functional.py:1993:22
          %size_prods.623 : int = aten::mul(%size_prods.622, %4357) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.623)
      %4359 : bool = aten::eq(%size_prods.621, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4359) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4360 : Tensor = aten::batch_norm(%bottleneck_output.150, %4347, %4348, %4345, %4346, %4344, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.152 : Tensor = aten::relu_(%4360) # torch/nn/functional.py:1117:17
  %4362 : Tensor = prim::GetAttr[name="weight"](%4337)
  %4363 : Tensor? = prim::GetAttr[name="bias"](%4337)
  %4364 : int[] = prim::ListConstruct(%27, %27)
  %4365 : int[] = prim::ListConstruct(%27, %27)
  %4366 : int[] = prim::ListConstruct(%27, %27)
  %new_features.151 : Tensor = aten::conv2d(%result.152, %4362, %4363, %4364, %4365, %4366, %27) # torch/nn/modules/conv.py:415:15
  %4368 : float = prim::GetAttr[name="drop_rate"](%1901)
  %4369 : bool = aten::gt(%4368, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.152 : Tensor = prim::If(%4369) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4371 : float = prim::GetAttr[name="drop_rate"](%1901)
      %4372 : bool = prim::GetAttr[name="training"](%1901)
      %4373 : bool = aten::lt(%4371, %16) # torch/nn/functional.py:968:7
      %4374 : bool = prim::If(%4373) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4375 : bool = aten::gt(%4371, %17) # torch/nn/functional.py:968:17
          -> (%4375)
       = prim::If(%4374) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4376 : Tensor = aten::dropout(%new_features.151, %4371, %4372) # torch/nn/functional.py:973:17
      -> (%4376)
    block1():
      -> (%new_features.151)
  %4377 : Tensor[] = aten::append(%features.4, %new_features.152) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4378 : Tensor = prim::Uninitialized()
  %4379 : bool = prim::GetAttr[name="memory_efficient"](%1902)
  %4380 : bool = prim::If(%4379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4381 : bool = prim::Uninitialized()
      %4382 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4383 : bool = aten::gt(%4382, %24)
      %4384 : bool, %4385 : bool, %4386 : int = prim::Loop(%18, %4383, %19, %4381, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4387 : int, %4388 : bool, %4389 : bool, %4390 : int):
          %tensor.77 : Tensor = aten::__getitem__(%features.4, %4390) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4392 : bool = prim::requires_grad(%tensor.77)
          %4393 : bool, %4394 : bool = prim::If(%4392) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4381)
          %4395 : int = aten::add(%4390, %27)
          %4396 : bool = aten::lt(%4395, %4382)
          %4397 : bool = aten::__and__(%4396, %4393)
          -> (%4397, %4392, %4394, %4395)
      %4398 : bool = prim::If(%4384)
        block0():
          -> (%4385)
        block1():
          -> (%19)
      -> (%4398)
    block1():
      -> (%19)
  %bottleneck_output.152 : Tensor = prim::If(%4380) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4378)
    block1():
      %concated_features.77 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4401 : __torch__.torch.nn.modules.conv.___torch_mangle_304.Conv2d = prim::GetAttr[name="conv1"](%1902)
      %4402 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_303.BatchNorm2d = prim::GetAttr[name="norm1"](%1902)
      %4403 : int = aten::dim(%concated_features.77) # torch/nn/modules/batchnorm.py:276:11
      %4404 : bool = aten::ne(%4403, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4404) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4405 : bool = prim::GetAttr[name="training"](%4402)
       = prim::If(%4405) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4406 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4402)
          %4407 : Tensor = aten::add(%4406, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4402, %4407)
          -> ()
        block1():
          -> ()
      %4408 : bool = prim::GetAttr[name="training"](%4402)
      %4409 : Tensor = prim::GetAttr[name="running_mean"](%4402)
      %4410 : Tensor = prim::GetAttr[name="running_var"](%4402)
      %4411 : Tensor = prim::GetAttr[name="weight"](%4402)
      %4412 : Tensor = prim::GetAttr[name="bias"](%4402)
       = prim::If(%4408) # torch/nn/functional.py:2011:4
        block0():
          %4413 : int[] = aten::size(%concated_features.77) # torch/nn/functional.py:2012:27
          %size_prods.624 : int = aten::__getitem__(%4413, %24) # torch/nn/functional.py:1991:17
          %4415 : int = aten::len(%4413) # torch/nn/functional.py:1992:19
          %4416 : int = aten::sub(%4415, %26) # torch/nn/functional.py:1992:19
          %size_prods.625 : int = prim::Loop(%4416, %25, %size_prods.624) # torch/nn/functional.py:1992:4
            block0(%i.157 : int, %size_prods.626 : int):
              %4420 : int = aten::add(%i.157, %26) # torch/nn/functional.py:1993:27
              %4421 : int = aten::__getitem__(%4413, %4420) # torch/nn/functional.py:1993:22
              %size_prods.627 : int = aten::mul(%size_prods.626, %4421) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.627)
          %4423 : bool = aten::eq(%size_prods.625, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4423) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4424 : Tensor = aten::batch_norm(%concated_features.77, %4411, %4412, %4409, %4410, %4408, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.153 : Tensor = aten::relu_(%4424) # torch/nn/functional.py:1117:17
      %4426 : Tensor = prim::GetAttr[name="weight"](%4401)
      %4427 : Tensor? = prim::GetAttr[name="bias"](%4401)
      %4428 : int[] = prim::ListConstruct(%27, %27)
      %4429 : int[] = prim::ListConstruct(%24, %24)
      %4430 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.153 : Tensor = aten::conv2d(%result.153, %4426, %4427, %4428, %4429, %4430, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.153)
  %4432 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1902)
  %4433 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1902)
  %4434 : int = aten::dim(%bottleneck_output.152) # torch/nn/modules/batchnorm.py:276:11
  %4435 : bool = aten::ne(%4434, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4435) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4436 : bool = prim::GetAttr[name="training"](%4433)
   = prim::If(%4436) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4437 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4433)
      %4438 : Tensor = aten::add(%4437, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4433, %4438)
      -> ()
    block1():
      -> ()
  %4439 : bool = prim::GetAttr[name="training"](%4433)
  %4440 : Tensor = prim::GetAttr[name="running_mean"](%4433)
  %4441 : Tensor = prim::GetAttr[name="running_var"](%4433)
  %4442 : Tensor = prim::GetAttr[name="weight"](%4433)
  %4443 : Tensor = prim::GetAttr[name="bias"](%4433)
   = prim::If(%4439) # torch/nn/functional.py:2011:4
    block0():
      %4444 : int[] = aten::size(%bottleneck_output.152) # torch/nn/functional.py:2012:27
      %size_prods.628 : int = aten::__getitem__(%4444, %24) # torch/nn/functional.py:1991:17
      %4446 : int = aten::len(%4444) # torch/nn/functional.py:1992:19
      %4447 : int = aten::sub(%4446, %26) # torch/nn/functional.py:1992:19
      %size_prods.629 : int = prim::Loop(%4447, %25, %size_prods.628) # torch/nn/functional.py:1992:4
        block0(%i.158 : int, %size_prods.630 : int):
          %4451 : int = aten::add(%i.158, %26) # torch/nn/functional.py:1993:27
          %4452 : int = aten::__getitem__(%4444, %4451) # torch/nn/functional.py:1993:22
          %size_prods.631 : int = aten::mul(%size_prods.630, %4452) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.631)
      %4454 : bool = aten::eq(%size_prods.629, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4454) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4455 : Tensor = aten::batch_norm(%bottleneck_output.152, %4442, %4443, %4440, %4441, %4439, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.154 : Tensor = aten::relu_(%4455) # torch/nn/functional.py:1117:17
  %4457 : Tensor = prim::GetAttr[name="weight"](%4432)
  %4458 : Tensor? = prim::GetAttr[name="bias"](%4432)
  %4459 : int[] = prim::ListConstruct(%27, %27)
  %4460 : int[] = prim::ListConstruct(%27, %27)
  %4461 : int[] = prim::ListConstruct(%27, %27)
  %new_features.153 : Tensor = aten::conv2d(%result.154, %4457, %4458, %4459, %4460, %4461, %27) # torch/nn/modules/conv.py:415:15
  %4463 : float = prim::GetAttr[name="drop_rate"](%1902)
  %4464 : bool = aten::gt(%4463, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.154 : Tensor = prim::If(%4464) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4466 : float = prim::GetAttr[name="drop_rate"](%1902)
      %4467 : bool = prim::GetAttr[name="training"](%1902)
      %4468 : bool = aten::lt(%4466, %16) # torch/nn/functional.py:968:7
      %4469 : bool = prim::If(%4468) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4470 : bool = aten::gt(%4466, %17) # torch/nn/functional.py:968:17
          -> (%4470)
       = prim::If(%4469) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4471 : Tensor = aten::dropout(%new_features.153, %4466, %4467) # torch/nn/functional.py:973:17
      -> (%4471)
    block1():
      -> (%new_features.153)
  %4472 : Tensor[] = aten::append(%features.4, %new_features.154) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4473 : Tensor = prim::Uninitialized()
  %4474 : bool = prim::GetAttr[name="memory_efficient"](%1903)
  %4475 : bool = prim::If(%4474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4476 : bool = prim::Uninitialized()
      %4477 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4478 : bool = aten::gt(%4477, %24)
      %4479 : bool, %4480 : bool, %4481 : int = prim::Loop(%18, %4478, %19, %4476, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4482 : int, %4483 : bool, %4484 : bool, %4485 : int):
          %tensor.78 : Tensor = aten::__getitem__(%features.4, %4485) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4487 : bool = prim::requires_grad(%tensor.78)
          %4488 : bool, %4489 : bool = prim::If(%4487) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4476)
          %4490 : int = aten::add(%4485, %27)
          %4491 : bool = aten::lt(%4490, %4477)
          %4492 : bool = aten::__and__(%4491, %4488)
          -> (%4492, %4487, %4489, %4490)
      %4493 : bool = prim::If(%4479)
        block0():
          -> (%4480)
        block1():
          -> (%19)
      -> (%4493)
    block1():
      -> (%19)
  %bottleneck_output.154 : Tensor = prim::If(%4475) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4473)
    block1():
      %concated_features.78 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4496 : __torch__.torch.nn.modules.conv.___torch_mangle_307.Conv2d = prim::GetAttr[name="conv1"](%1903)
      %4497 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="norm1"](%1903)
      %4498 : int = aten::dim(%concated_features.78) # torch/nn/modules/batchnorm.py:276:11
      %4499 : bool = aten::ne(%4498, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4499) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4500 : bool = prim::GetAttr[name="training"](%4497)
       = prim::If(%4500) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4501 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4497)
          %4502 : Tensor = aten::add(%4501, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4497, %4502)
          -> ()
        block1():
          -> ()
      %4503 : bool = prim::GetAttr[name="training"](%4497)
      %4504 : Tensor = prim::GetAttr[name="running_mean"](%4497)
      %4505 : Tensor = prim::GetAttr[name="running_var"](%4497)
      %4506 : Tensor = prim::GetAttr[name="weight"](%4497)
      %4507 : Tensor = prim::GetAttr[name="bias"](%4497)
       = prim::If(%4503) # torch/nn/functional.py:2011:4
        block0():
          %4508 : int[] = aten::size(%concated_features.78) # torch/nn/functional.py:2012:27
          %size_prods.632 : int = aten::__getitem__(%4508, %24) # torch/nn/functional.py:1991:17
          %4510 : int = aten::len(%4508) # torch/nn/functional.py:1992:19
          %4511 : int = aten::sub(%4510, %26) # torch/nn/functional.py:1992:19
          %size_prods.633 : int = prim::Loop(%4511, %25, %size_prods.632) # torch/nn/functional.py:1992:4
            block0(%i.159 : int, %size_prods.634 : int):
              %4515 : int = aten::add(%i.159, %26) # torch/nn/functional.py:1993:27
              %4516 : int = aten::__getitem__(%4508, %4515) # torch/nn/functional.py:1993:22
              %size_prods.635 : int = aten::mul(%size_prods.634, %4516) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.635)
          %4518 : bool = aten::eq(%size_prods.633, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4518) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4519 : Tensor = aten::batch_norm(%concated_features.78, %4506, %4507, %4504, %4505, %4503, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.155 : Tensor = aten::relu_(%4519) # torch/nn/functional.py:1117:17
      %4521 : Tensor = prim::GetAttr[name="weight"](%4496)
      %4522 : Tensor? = prim::GetAttr[name="bias"](%4496)
      %4523 : int[] = prim::ListConstruct(%27, %27)
      %4524 : int[] = prim::ListConstruct(%24, %24)
      %4525 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.155 : Tensor = aten::conv2d(%result.155, %4521, %4522, %4523, %4524, %4525, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.155)
  %4527 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1903)
  %4528 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1903)
  %4529 : int = aten::dim(%bottleneck_output.154) # torch/nn/modules/batchnorm.py:276:11
  %4530 : bool = aten::ne(%4529, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4530) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4531 : bool = prim::GetAttr[name="training"](%4528)
   = prim::If(%4531) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4532 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4528)
      %4533 : Tensor = aten::add(%4532, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4528, %4533)
      -> ()
    block1():
      -> ()
  %4534 : bool = prim::GetAttr[name="training"](%4528)
  %4535 : Tensor = prim::GetAttr[name="running_mean"](%4528)
  %4536 : Tensor = prim::GetAttr[name="running_var"](%4528)
  %4537 : Tensor = prim::GetAttr[name="weight"](%4528)
  %4538 : Tensor = prim::GetAttr[name="bias"](%4528)
   = prim::If(%4534) # torch/nn/functional.py:2011:4
    block0():
      %4539 : int[] = aten::size(%bottleneck_output.154) # torch/nn/functional.py:2012:27
      %size_prods.636 : int = aten::__getitem__(%4539, %24) # torch/nn/functional.py:1991:17
      %4541 : int = aten::len(%4539) # torch/nn/functional.py:1992:19
      %4542 : int = aten::sub(%4541, %26) # torch/nn/functional.py:1992:19
      %size_prods.637 : int = prim::Loop(%4542, %25, %size_prods.636) # torch/nn/functional.py:1992:4
        block0(%i.160 : int, %size_prods.638 : int):
          %4546 : int = aten::add(%i.160, %26) # torch/nn/functional.py:1993:27
          %4547 : int = aten::__getitem__(%4539, %4546) # torch/nn/functional.py:1993:22
          %size_prods.639 : int = aten::mul(%size_prods.638, %4547) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.639)
      %4549 : bool = aten::eq(%size_prods.637, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4549) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4550 : Tensor = aten::batch_norm(%bottleneck_output.154, %4537, %4538, %4535, %4536, %4534, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.156 : Tensor = aten::relu_(%4550) # torch/nn/functional.py:1117:17
  %4552 : Tensor = prim::GetAttr[name="weight"](%4527)
  %4553 : Tensor? = prim::GetAttr[name="bias"](%4527)
  %4554 : int[] = prim::ListConstruct(%27, %27)
  %4555 : int[] = prim::ListConstruct(%27, %27)
  %4556 : int[] = prim::ListConstruct(%27, %27)
  %new_features.155 : Tensor = aten::conv2d(%result.156, %4552, %4553, %4554, %4555, %4556, %27) # torch/nn/modules/conv.py:415:15
  %4558 : float = prim::GetAttr[name="drop_rate"](%1903)
  %4559 : bool = aten::gt(%4558, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.156 : Tensor = prim::If(%4559) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4561 : float = prim::GetAttr[name="drop_rate"](%1903)
      %4562 : bool = prim::GetAttr[name="training"](%1903)
      %4563 : bool = aten::lt(%4561, %16) # torch/nn/functional.py:968:7
      %4564 : bool = prim::If(%4563) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4565 : bool = aten::gt(%4561, %17) # torch/nn/functional.py:968:17
          -> (%4565)
       = prim::If(%4564) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4566 : Tensor = aten::dropout(%new_features.155, %4561, %4562) # torch/nn/functional.py:973:17
      -> (%4566)
    block1():
      -> (%new_features.155)
  %4567 : Tensor[] = aten::append(%features.4, %new_features.156) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4568 : Tensor = prim::Uninitialized()
  %4569 : bool = prim::GetAttr[name="memory_efficient"](%1904)
  %4570 : bool = prim::If(%4569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4571 : bool = prim::Uninitialized()
      %4572 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4573 : bool = aten::gt(%4572, %24)
      %4574 : bool, %4575 : bool, %4576 : int = prim::Loop(%18, %4573, %19, %4571, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4577 : int, %4578 : bool, %4579 : bool, %4580 : int):
          %tensor.79 : Tensor = aten::__getitem__(%features.4, %4580) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4582 : bool = prim::requires_grad(%tensor.79)
          %4583 : bool, %4584 : bool = prim::If(%4582) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4571)
          %4585 : int = aten::add(%4580, %27)
          %4586 : bool = aten::lt(%4585, %4572)
          %4587 : bool = aten::__and__(%4586, %4583)
          -> (%4587, %4582, %4584, %4585)
      %4588 : bool = prim::If(%4574)
        block0():
          -> (%4575)
        block1():
          -> (%19)
      -> (%4588)
    block1():
      -> (%19)
  %bottleneck_output.156 : Tensor = prim::If(%4570) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4568)
    block1():
      %concated_features.79 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4591 : __torch__.torch.nn.modules.conv.___torch_mangle_309.Conv2d = prim::GetAttr[name="conv1"](%1904)
      %4592 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%1904)
      %4593 : int = aten::dim(%concated_features.79) # torch/nn/modules/batchnorm.py:276:11
      %4594 : bool = aten::ne(%4593, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4594) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4595 : bool = prim::GetAttr[name="training"](%4592)
       = prim::If(%4595) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4596 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4592)
          %4597 : Tensor = aten::add(%4596, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4592, %4597)
          -> ()
        block1():
          -> ()
      %4598 : bool = prim::GetAttr[name="training"](%4592)
      %4599 : Tensor = prim::GetAttr[name="running_mean"](%4592)
      %4600 : Tensor = prim::GetAttr[name="running_var"](%4592)
      %4601 : Tensor = prim::GetAttr[name="weight"](%4592)
      %4602 : Tensor = prim::GetAttr[name="bias"](%4592)
       = prim::If(%4598) # torch/nn/functional.py:2011:4
        block0():
          %4603 : int[] = aten::size(%concated_features.79) # torch/nn/functional.py:2012:27
          %size_prods.640 : int = aten::__getitem__(%4603, %24) # torch/nn/functional.py:1991:17
          %4605 : int = aten::len(%4603) # torch/nn/functional.py:1992:19
          %4606 : int = aten::sub(%4605, %26) # torch/nn/functional.py:1992:19
          %size_prods.641 : int = prim::Loop(%4606, %25, %size_prods.640) # torch/nn/functional.py:1992:4
            block0(%i.161 : int, %size_prods.642 : int):
              %4610 : int = aten::add(%i.161, %26) # torch/nn/functional.py:1993:27
              %4611 : int = aten::__getitem__(%4603, %4610) # torch/nn/functional.py:1993:22
              %size_prods.643 : int = aten::mul(%size_prods.642, %4611) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.643)
          %4613 : bool = aten::eq(%size_prods.641, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4613) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4614 : Tensor = aten::batch_norm(%concated_features.79, %4601, %4602, %4599, %4600, %4598, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.157 : Tensor = aten::relu_(%4614) # torch/nn/functional.py:1117:17
      %4616 : Tensor = prim::GetAttr[name="weight"](%4591)
      %4617 : Tensor? = prim::GetAttr[name="bias"](%4591)
      %4618 : int[] = prim::ListConstruct(%27, %27)
      %4619 : int[] = prim::ListConstruct(%24, %24)
      %4620 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.157 : Tensor = aten::conv2d(%result.157, %4616, %4617, %4618, %4619, %4620, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.157)
  %4622 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1904)
  %4623 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1904)
  %4624 : int = aten::dim(%bottleneck_output.156) # torch/nn/modules/batchnorm.py:276:11
  %4625 : bool = aten::ne(%4624, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4625) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4626 : bool = prim::GetAttr[name="training"](%4623)
   = prim::If(%4626) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4627 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4623)
      %4628 : Tensor = aten::add(%4627, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4623, %4628)
      -> ()
    block1():
      -> ()
  %4629 : bool = prim::GetAttr[name="training"](%4623)
  %4630 : Tensor = prim::GetAttr[name="running_mean"](%4623)
  %4631 : Tensor = prim::GetAttr[name="running_var"](%4623)
  %4632 : Tensor = prim::GetAttr[name="weight"](%4623)
  %4633 : Tensor = prim::GetAttr[name="bias"](%4623)
   = prim::If(%4629) # torch/nn/functional.py:2011:4
    block0():
      %4634 : int[] = aten::size(%bottleneck_output.156) # torch/nn/functional.py:2012:27
      %size_prods.644 : int = aten::__getitem__(%4634, %24) # torch/nn/functional.py:1991:17
      %4636 : int = aten::len(%4634) # torch/nn/functional.py:1992:19
      %4637 : int = aten::sub(%4636, %26) # torch/nn/functional.py:1992:19
      %size_prods.645 : int = prim::Loop(%4637, %25, %size_prods.644) # torch/nn/functional.py:1992:4
        block0(%i.162 : int, %size_prods.646 : int):
          %4641 : int = aten::add(%i.162, %26) # torch/nn/functional.py:1993:27
          %4642 : int = aten::__getitem__(%4634, %4641) # torch/nn/functional.py:1993:22
          %size_prods.647 : int = aten::mul(%size_prods.646, %4642) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.647)
      %4644 : bool = aten::eq(%size_prods.645, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4644) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4645 : Tensor = aten::batch_norm(%bottleneck_output.156, %4632, %4633, %4630, %4631, %4629, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.158 : Tensor = aten::relu_(%4645) # torch/nn/functional.py:1117:17
  %4647 : Tensor = prim::GetAttr[name="weight"](%4622)
  %4648 : Tensor? = prim::GetAttr[name="bias"](%4622)
  %4649 : int[] = prim::ListConstruct(%27, %27)
  %4650 : int[] = prim::ListConstruct(%27, %27)
  %4651 : int[] = prim::ListConstruct(%27, %27)
  %new_features.157 : Tensor = aten::conv2d(%result.158, %4647, %4648, %4649, %4650, %4651, %27) # torch/nn/modules/conv.py:415:15
  %4653 : float = prim::GetAttr[name="drop_rate"](%1904)
  %4654 : bool = aten::gt(%4653, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.158 : Tensor = prim::If(%4654) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4656 : float = prim::GetAttr[name="drop_rate"](%1904)
      %4657 : bool = prim::GetAttr[name="training"](%1904)
      %4658 : bool = aten::lt(%4656, %16) # torch/nn/functional.py:968:7
      %4659 : bool = prim::If(%4658) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4660 : bool = aten::gt(%4656, %17) # torch/nn/functional.py:968:17
          -> (%4660)
       = prim::If(%4659) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4661 : Tensor = aten::dropout(%new_features.157, %4656, %4657) # torch/nn/functional.py:973:17
      -> (%4661)
    block1():
      -> (%new_features.157)
  %4662 : Tensor[] = aten::append(%features.4, %new_features.158) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4663 : Tensor = prim::Uninitialized()
  %4664 : bool = prim::GetAttr[name="memory_efficient"](%1905)
  %4665 : bool = prim::If(%4664) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4666 : bool = prim::Uninitialized()
      %4667 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4668 : bool = aten::gt(%4667, %24)
      %4669 : bool, %4670 : bool, %4671 : int = prim::Loop(%18, %4668, %19, %4666, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4672 : int, %4673 : bool, %4674 : bool, %4675 : int):
          %tensor.80 : Tensor = aten::__getitem__(%features.4, %4675) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4677 : bool = prim::requires_grad(%tensor.80)
          %4678 : bool, %4679 : bool = prim::If(%4677) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4666)
          %4680 : int = aten::add(%4675, %27)
          %4681 : bool = aten::lt(%4680, %4667)
          %4682 : bool = aten::__and__(%4681, %4678)
          -> (%4682, %4677, %4679, %4680)
      %4683 : bool = prim::If(%4669)
        block0():
          -> (%4670)
        block1():
          -> (%19)
      -> (%4683)
    block1():
      -> (%19)
  %bottleneck_output.158 : Tensor = prim::If(%4665) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4663)
    block1():
      %concated_features.80 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4686 : __torch__.torch.nn.modules.conv.___torch_mangle_312.Conv2d = prim::GetAttr[name="conv1"](%1905)
      %4687 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_311.BatchNorm2d = prim::GetAttr[name="norm1"](%1905)
      %4688 : int = aten::dim(%concated_features.80) # torch/nn/modules/batchnorm.py:276:11
      %4689 : bool = aten::ne(%4688, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4689) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4690 : bool = prim::GetAttr[name="training"](%4687)
       = prim::If(%4690) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4691 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4687)
          %4692 : Tensor = aten::add(%4691, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4687, %4692)
          -> ()
        block1():
          -> ()
      %4693 : bool = prim::GetAttr[name="training"](%4687)
      %4694 : Tensor = prim::GetAttr[name="running_mean"](%4687)
      %4695 : Tensor = prim::GetAttr[name="running_var"](%4687)
      %4696 : Tensor = prim::GetAttr[name="weight"](%4687)
      %4697 : Tensor = prim::GetAttr[name="bias"](%4687)
       = prim::If(%4693) # torch/nn/functional.py:2011:4
        block0():
          %4698 : int[] = aten::size(%concated_features.80) # torch/nn/functional.py:2012:27
          %size_prods.648 : int = aten::__getitem__(%4698, %24) # torch/nn/functional.py:1991:17
          %4700 : int = aten::len(%4698) # torch/nn/functional.py:1992:19
          %4701 : int = aten::sub(%4700, %26) # torch/nn/functional.py:1992:19
          %size_prods.649 : int = prim::Loop(%4701, %25, %size_prods.648) # torch/nn/functional.py:1992:4
            block0(%i.163 : int, %size_prods.650 : int):
              %4705 : int = aten::add(%i.163, %26) # torch/nn/functional.py:1993:27
              %4706 : int = aten::__getitem__(%4698, %4705) # torch/nn/functional.py:1993:22
              %size_prods.651 : int = aten::mul(%size_prods.650, %4706) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.651)
          %4708 : bool = aten::eq(%size_prods.649, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4708) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4709 : Tensor = aten::batch_norm(%concated_features.80, %4696, %4697, %4694, %4695, %4693, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.159 : Tensor = aten::relu_(%4709) # torch/nn/functional.py:1117:17
      %4711 : Tensor = prim::GetAttr[name="weight"](%4686)
      %4712 : Tensor? = prim::GetAttr[name="bias"](%4686)
      %4713 : int[] = prim::ListConstruct(%27, %27)
      %4714 : int[] = prim::ListConstruct(%24, %24)
      %4715 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.159 : Tensor = aten::conv2d(%result.159, %4711, %4712, %4713, %4714, %4715, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.159)
  %4717 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1905)
  %4718 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1905)
  %4719 : int = aten::dim(%bottleneck_output.158) # torch/nn/modules/batchnorm.py:276:11
  %4720 : bool = aten::ne(%4719, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4720) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4721 : bool = prim::GetAttr[name="training"](%4718)
   = prim::If(%4721) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4722 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4718)
      %4723 : Tensor = aten::add(%4722, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4718, %4723)
      -> ()
    block1():
      -> ()
  %4724 : bool = prim::GetAttr[name="training"](%4718)
  %4725 : Tensor = prim::GetAttr[name="running_mean"](%4718)
  %4726 : Tensor = prim::GetAttr[name="running_var"](%4718)
  %4727 : Tensor = prim::GetAttr[name="weight"](%4718)
  %4728 : Tensor = prim::GetAttr[name="bias"](%4718)
   = prim::If(%4724) # torch/nn/functional.py:2011:4
    block0():
      %4729 : int[] = aten::size(%bottleneck_output.158) # torch/nn/functional.py:2012:27
      %size_prods.652 : int = aten::__getitem__(%4729, %24) # torch/nn/functional.py:1991:17
      %4731 : int = aten::len(%4729) # torch/nn/functional.py:1992:19
      %4732 : int = aten::sub(%4731, %26) # torch/nn/functional.py:1992:19
      %size_prods.653 : int = prim::Loop(%4732, %25, %size_prods.652) # torch/nn/functional.py:1992:4
        block0(%i.164 : int, %size_prods.654 : int):
          %4736 : int = aten::add(%i.164, %26) # torch/nn/functional.py:1993:27
          %4737 : int = aten::__getitem__(%4729, %4736) # torch/nn/functional.py:1993:22
          %size_prods.655 : int = aten::mul(%size_prods.654, %4737) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.655)
      %4739 : bool = aten::eq(%size_prods.653, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4739) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4740 : Tensor = aten::batch_norm(%bottleneck_output.158, %4727, %4728, %4725, %4726, %4724, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.160 : Tensor = aten::relu_(%4740) # torch/nn/functional.py:1117:17
  %4742 : Tensor = prim::GetAttr[name="weight"](%4717)
  %4743 : Tensor? = prim::GetAttr[name="bias"](%4717)
  %4744 : int[] = prim::ListConstruct(%27, %27)
  %4745 : int[] = prim::ListConstruct(%27, %27)
  %4746 : int[] = prim::ListConstruct(%27, %27)
  %new_features.159 : Tensor = aten::conv2d(%result.160, %4742, %4743, %4744, %4745, %4746, %27) # torch/nn/modules/conv.py:415:15
  %4748 : float = prim::GetAttr[name="drop_rate"](%1905)
  %4749 : bool = aten::gt(%4748, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.160 : Tensor = prim::If(%4749) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4751 : float = prim::GetAttr[name="drop_rate"](%1905)
      %4752 : bool = prim::GetAttr[name="training"](%1905)
      %4753 : bool = aten::lt(%4751, %16) # torch/nn/functional.py:968:7
      %4754 : bool = prim::If(%4753) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4755 : bool = aten::gt(%4751, %17) # torch/nn/functional.py:968:17
          -> (%4755)
       = prim::If(%4754) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4756 : Tensor = aten::dropout(%new_features.159, %4751, %4752) # torch/nn/functional.py:973:17
      -> (%4756)
    block1():
      -> (%new_features.159)
  %4757 : Tensor[] = aten::append(%features.4, %new_features.160) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4758 : Tensor = prim::Uninitialized()
  %4759 : bool = prim::GetAttr[name="memory_efficient"](%1906)
  %4760 : bool = prim::If(%4759) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4761 : bool = prim::Uninitialized()
      %4762 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4763 : bool = aten::gt(%4762, %24)
      %4764 : bool, %4765 : bool, %4766 : int = prim::Loop(%18, %4763, %19, %4761, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4767 : int, %4768 : bool, %4769 : bool, %4770 : int):
          %tensor.81 : Tensor = aten::__getitem__(%features.4, %4770) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4772 : bool = prim::requires_grad(%tensor.81)
          %4773 : bool, %4774 : bool = prim::If(%4772) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4761)
          %4775 : int = aten::add(%4770, %27)
          %4776 : bool = aten::lt(%4775, %4762)
          %4777 : bool = aten::__and__(%4776, %4773)
          -> (%4777, %4772, %4774, %4775)
      %4778 : bool = prim::If(%4764)
        block0():
          -> (%4765)
        block1():
          -> (%19)
      -> (%4778)
    block1():
      -> (%19)
  %bottleneck_output.160 : Tensor = prim::If(%4760) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4758)
    block1():
      %concated_features.81 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4781 : __torch__.torch.nn.modules.conv.___torch_mangle_315.Conv2d = prim::GetAttr[name="conv1"](%1906)
      %4782 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_314.BatchNorm2d = prim::GetAttr[name="norm1"](%1906)
      %4783 : int = aten::dim(%concated_features.81) # torch/nn/modules/batchnorm.py:276:11
      %4784 : bool = aten::ne(%4783, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4784) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4785 : bool = prim::GetAttr[name="training"](%4782)
       = prim::If(%4785) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4786 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4782)
          %4787 : Tensor = aten::add(%4786, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4782, %4787)
          -> ()
        block1():
          -> ()
      %4788 : bool = prim::GetAttr[name="training"](%4782)
      %4789 : Tensor = prim::GetAttr[name="running_mean"](%4782)
      %4790 : Tensor = prim::GetAttr[name="running_var"](%4782)
      %4791 : Tensor = prim::GetAttr[name="weight"](%4782)
      %4792 : Tensor = prim::GetAttr[name="bias"](%4782)
       = prim::If(%4788) # torch/nn/functional.py:2011:4
        block0():
          %4793 : int[] = aten::size(%concated_features.81) # torch/nn/functional.py:2012:27
          %size_prods.656 : int = aten::__getitem__(%4793, %24) # torch/nn/functional.py:1991:17
          %4795 : int = aten::len(%4793) # torch/nn/functional.py:1992:19
          %4796 : int = aten::sub(%4795, %26) # torch/nn/functional.py:1992:19
          %size_prods.657 : int = prim::Loop(%4796, %25, %size_prods.656) # torch/nn/functional.py:1992:4
            block0(%i.165 : int, %size_prods.658 : int):
              %4800 : int = aten::add(%i.165, %26) # torch/nn/functional.py:1993:27
              %4801 : int = aten::__getitem__(%4793, %4800) # torch/nn/functional.py:1993:22
              %size_prods.659 : int = aten::mul(%size_prods.658, %4801) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.659)
          %4803 : bool = aten::eq(%size_prods.657, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4803) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4804 : Tensor = aten::batch_norm(%concated_features.81, %4791, %4792, %4789, %4790, %4788, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.161 : Tensor = aten::relu_(%4804) # torch/nn/functional.py:1117:17
      %4806 : Tensor = prim::GetAttr[name="weight"](%4781)
      %4807 : Tensor? = prim::GetAttr[name="bias"](%4781)
      %4808 : int[] = prim::ListConstruct(%27, %27)
      %4809 : int[] = prim::ListConstruct(%24, %24)
      %4810 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.161 : Tensor = aten::conv2d(%result.161, %4806, %4807, %4808, %4809, %4810, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.161)
  %4812 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1906)
  %4813 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1906)
  %4814 : int = aten::dim(%bottleneck_output.160) # torch/nn/modules/batchnorm.py:276:11
  %4815 : bool = aten::ne(%4814, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4815) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4816 : bool = prim::GetAttr[name="training"](%4813)
   = prim::If(%4816) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4817 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4813)
      %4818 : Tensor = aten::add(%4817, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4813, %4818)
      -> ()
    block1():
      -> ()
  %4819 : bool = prim::GetAttr[name="training"](%4813)
  %4820 : Tensor = prim::GetAttr[name="running_mean"](%4813)
  %4821 : Tensor = prim::GetAttr[name="running_var"](%4813)
  %4822 : Tensor = prim::GetAttr[name="weight"](%4813)
  %4823 : Tensor = prim::GetAttr[name="bias"](%4813)
   = prim::If(%4819) # torch/nn/functional.py:2011:4
    block0():
      %4824 : int[] = aten::size(%bottleneck_output.160) # torch/nn/functional.py:2012:27
      %size_prods.660 : int = aten::__getitem__(%4824, %24) # torch/nn/functional.py:1991:17
      %4826 : int = aten::len(%4824) # torch/nn/functional.py:1992:19
      %4827 : int = aten::sub(%4826, %26) # torch/nn/functional.py:1992:19
      %size_prods.661 : int = prim::Loop(%4827, %25, %size_prods.660) # torch/nn/functional.py:1992:4
        block0(%i.166 : int, %size_prods.662 : int):
          %4831 : int = aten::add(%i.166, %26) # torch/nn/functional.py:1993:27
          %4832 : int = aten::__getitem__(%4824, %4831) # torch/nn/functional.py:1993:22
          %size_prods.663 : int = aten::mul(%size_prods.662, %4832) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.663)
      %4834 : bool = aten::eq(%size_prods.661, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4834) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4835 : Tensor = aten::batch_norm(%bottleneck_output.160, %4822, %4823, %4820, %4821, %4819, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.162 : Tensor = aten::relu_(%4835) # torch/nn/functional.py:1117:17
  %4837 : Tensor = prim::GetAttr[name="weight"](%4812)
  %4838 : Tensor? = prim::GetAttr[name="bias"](%4812)
  %4839 : int[] = prim::ListConstruct(%27, %27)
  %4840 : int[] = prim::ListConstruct(%27, %27)
  %4841 : int[] = prim::ListConstruct(%27, %27)
  %new_features.161 : Tensor = aten::conv2d(%result.162, %4837, %4838, %4839, %4840, %4841, %27) # torch/nn/modules/conv.py:415:15
  %4843 : float = prim::GetAttr[name="drop_rate"](%1906)
  %4844 : bool = aten::gt(%4843, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.162 : Tensor = prim::If(%4844) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4846 : float = prim::GetAttr[name="drop_rate"](%1906)
      %4847 : bool = prim::GetAttr[name="training"](%1906)
      %4848 : bool = aten::lt(%4846, %16) # torch/nn/functional.py:968:7
      %4849 : bool = prim::If(%4848) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4850 : bool = aten::gt(%4846, %17) # torch/nn/functional.py:968:17
          -> (%4850)
       = prim::If(%4849) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4851 : Tensor = aten::dropout(%new_features.161, %4846, %4847) # torch/nn/functional.py:973:17
      -> (%4851)
    block1():
      -> (%new_features.161)
  %4852 : Tensor[] = aten::append(%features.4, %new_features.162) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4853 : Tensor = prim::Uninitialized()
  %4854 : bool = prim::GetAttr[name="memory_efficient"](%1907)
  %4855 : bool = prim::If(%4854) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4856 : bool = prim::Uninitialized()
      %4857 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4858 : bool = aten::gt(%4857, %24)
      %4859 : bool, %4860 : bool, %4861 : int = prim::Loop(%18, %4858, %19, %4856, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4862 : int, %4863 : bool, %4864 : bool, %4865 : int):
          %tensor.82 : Tensor = aten::__getitem__(%features.4, %4865) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4867 : bool = prim::requires_grad(%tensor.82)
          %4868 : bool, %4869 : bool = prim::If(%4867) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4856)
          %4870 : int = aten::add(%4865, %27)
          %4871 : bool = aten::lt(%4870, %4857)
          %4872 : bool = aten::__and__(%4871, %4868)
          -> (%4872, %4867, %4869, %4870)
      %4873 : bool = prim::If(%4859)
        block0():
          -> (%4860)
        block1():
          -> (%19)
      -> (%4873)
    block1():
      -> (%19)
  %bottleneck_output.162 : Tensor = prim::If(%4855) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4853)
    block1():
      %concated_features.82 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4876 : __torch__.torch.nn.modules.conv.___torch_mangle_317.Conv2d = prim::GetAttr[name="conv1"](%1907)
      %4877 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%1907)
      %4878 : int = aten::dim(%concated_features.82) # torch/nn/modules/batchnorm.py:276:11
      %4879 : bool = aten::ne(%4878, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4879) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4880 : bool = prim::GetAttr[name="training"](%4877)
       = prim::If(%4880) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4881 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4877)
          %4882 : Tensor = aten::add(%4881, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4877, %4882)
          -> ()
        block1():
          -> ()
      %4883 : bool = prim::GetAttr[name="training"](%4877)
      %4884 : Tensor = prim::GetAttr[name="running_mean"](%4877)
      %4885 : Tensor = prim::GetAttr[name="running_var"](%4877)
      %4886 : Tensor = prim::GetAttr[name="weight"](%4877)
      %4887 : Tensor = prim::GetAttr[name="bias"](%4877)
       = prim::If(%4883) # torch/nn/functional.py:2011:4
        block0():
          %4888 : int[] = aten::size(%concated_features.82) # torch/nn/functional.py:2012:27
          %size_prods.664 : int = aten::__getitem__(%4888, %24) # torch/nn/functional.py:1991:17
          %4890 : int = aten::len(%4888) # torch/nn/functional.py:1992:19
          %4891 : int = aten::sub(%4890, %26) # torch/nn/functional.py:1992:19
          %size_prods.665 : int = prim::Loop(%4891, %25, %size_prods.664) # torch/nn/functional.py:1992:4
            block0(%i.167 : int, %size_prods.666 : int):
              %4895 : int = aten::add(%i.167, %26) # torch/nn/functional.py:1993:27
              %4896 : int = aten::__getitem__(%4888, %4895) # torch/nn/functional.py:1993:22
              %size_prods.667 : int = aten::mul(%size_prods.666, %4896) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.667)
          %4898 : bool = aten::eq(%size_prods.665, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4898) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4899 : Tensor = aten::batch_norm(%concated_features.82, %4886, %4887, %4884, %4885, %4883, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.163 : Tensor = aten::relu_(%4899) # torch/nn/functional.py:1117:17
      %4901 : Tensor = prim::GetAttr[name="weight"](%4876)
      %4902 : Tensor? = prim::GetAttr[name="bias"](%4876)
      %4903 : int[] = prim::ListConstruct(%27, %27)
      %4904 : int[] = prim::ListConstruct(%24, %24)
      %4905 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.163 : Tensor = aten::conv2d(%result.163, %4901, %4902, %4903, %4904, %4905, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.163)
  %4907 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%1907)
  %4908 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%1907)
  %4909 : int = aten::dim(%bottleneck_output.162) # torch/nn/modules/batchnorm.py:276:11
  %4910 : bool = aten::ne(%4909, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4910) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4911 : bool = prim::GetAttr[name="training"](%4908)
   = prim::If(%4911) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4912 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4908)
      %4913 : Tensor = aten::add(%4912, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4908, %4913)
      -> ()
    block1():
      -> ()
  %4914 : bool = prim::GetAttr[name="training"](%4908)
  %4915 : Tensor = prim::GetAttr[name="running_mean"](%4908)
  %4916 : Tensor = prim::GetAttr[name="running_var"](%4908)
  %4917 : Tensor = prim::GetAttr[name="weight"](%4908)
  %4918 : Tensor = prim::GetAttr[name="bias"](%4908)
   = prim::If(%4914) # torch/nn/functional.py:2011:4
    block0():
      %4919 : int[] = aten::size(%bottleneck_output.162) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%4919, %24) # torch/nn/functional.py:1991:17
      %4921 : int = aten::len(%4919) # torch/nn/functional.py:1992:19
      %4922 : int = aten::sub(%4921, %26) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%4922, %25, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %4926 : int = aten::add(%i.104, %26) # torch/nn/functional.py:1993:27
          %4927 : int = aten::__getitem__(%4919, %4926) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %4927) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.415)
      %4929 : bool = aten::eq(%size_prods.413, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4929) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4930 : Tensor = aten::batch_norm(%bottleneck_output.162, %4917, %4918, %4915, %4916, %4914, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.164 : Tensor = aten::relu_(%4930) # torch/nn/functional.py:1117:17
  %4932 : Tensor = prim::GetAttr[name="weight"](%4907)
  %4933 : Tensor? = prim::GetAttr[name="bias"](%4907)
  %4934 : int[] = prim::ListConstruct(%27, %27)
  %4935 : int[] = prim::ListConstruct(%27, %27)
  %4936 : int[] = prim::ListConstruct(%27, %27)
  %new_features.163 : Tensor = aten::conv2d(%result.164, %4932, %4933, %4934, %4935, %4936, %27) # torch/nn/modules/conv.py:415:15
  %4938 : float = prim::GetAttr[name="drop_rate"](%1907)
  %4939 : bool = aten::gt(%4938, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.164 : Tensor = prim::If(%4939) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4941 : float = prim::GetAttr[name="drop_rate"](%1907)
      %4942 : bool = prim::GetAttr[name="training"](%1907)
      %4943 : bool = aten::lt(%4941, %16) # torch/nn/functional.py:968:7
      %4944 : bool = prim::If(%4943) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4945 : bool = aten::gt(%4941, %17) # torch/nn/functional.py:968:17
          -> (%4945)
       = prim::If(%4944) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4946 : Tensor = aten::dropout(%new_features.163, %4941, %4942) # torch/nn/functional.py:973:17
      -> (%4946)
    block1():
      -> (%new_features.163)
  %4947 : Tensor[] = aten::append(%features.4, %new_features.164) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.19 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %4949 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_320.BatchNorm2d = prim::GetAttr[name="norm"](%36)
  %4950 : __torch__.torch.nn.modules.conv.___torch_mangle_321.Conv2d = prim::GetAttr[name="conv"](%36)
  %4951 : int = aten::dim(%input.19) # torch/nn/modules/batchnorm.py:276:11
  %4952 : bool = aten::ne(%4951, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4952) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4953 : bool = prim::GetAttr[name="training"](%4949)
   = prim::If(%4953) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4954 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4949)
      %4955 : Tensor = aten::add(%4954, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4949, %4955)
      -> ()
    block1():
      -> ()
  %4956 : bool = prim::GetAttr[name="training"](%4949)
  %4957 : Tensor = prim::GetAttr[name="running_mean"](%4949)
  %4958 : Tensor = prim::GetAttr[name="running_var"](%4949)
  %4959 : Tensor = prim::GetAttr[name="weight"](%4949)
  %4960 : Tensor = prim::GetAttr[name="bias"](%4949)
   = prim::If(%4956) # torch/nn/functional.py:2011:4
    block0():
      %4961 : int[] = aten::size(%input.19) # torch/nn/functional.py:2012:27
      %size_prods.668 : int = aten::__getitem__(%4961, %24) # torch/nn/functional.py:1991:17
      %4963 : int = aten::len(%4961) # torch/nn/functional.py:1992:19
      %4964 : int = aten::sub(%4963, %26) # torch/nn/functional.py:1992:19
      %size_prods.669 : int = prim::Loop(%4964, %25, %size_prods.668) # torch/nn/functional.py:1992:4
        block0(%i.168 : int, %size_prods.670 : int):
          %4968 : int = aten::add(%i.168, %26) # torch/nn/functional.py:1993:27
          %4969 : int = aten::__getitem__(%4961, %4968) # torch/nn/functional.py:1993:22
          %size_prods.671 : int = aten::mul(%size_prods.670, %4969) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.671)
      %4971 : bool = aten::eq(%size_prods.669, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4971) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.3 : Tensor = aten::batch_norm(%input.19, %4959, %4960, %4957, %4958, %4956, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.5 : Tensor = aten::relu_(%input.3) # torch/nn/functional.py:1117:17
  %4974 : Tensor = prim::GetAttr[name="weight"](%4950)
  %4975 : Tensor? = prim::GetAttr[name="bias"](%4950)
  %4976 : int[] = prim::ListConstruct(%27, %27)
  %4977 : int[] = prim::ListConstruct(%24, %24)
  %4978 : int[] = prim::ListConstruct(%27, %27)
  %input.7 : Tensor = aten::conv2d(%input.5, %4974, %4975, %4976, %4977, %4978, %27) # torch/nn/modules/conv.py:415:15
  %4980 : int[] = prim::ListConstruct(%26, %26)
  %4981 : int[] = prim::ListConstruct(%26, %26)
  %4982 : int[] = prim::ListConstruct(%24, %24)
  %input.21 : Tensor = aten::avg_pool2d(%input.7, %4980, %4981, %4982, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.1 : Tensor[] = prim::ListConstruct(%input.21)
  %4985 : __torch__.torchvision.models.densenet.___torch_mangle_123._DenseLayer = prim::GetAttr[name="denselayer1"](%37)
  %4986 : __torch__.torchvision.models.densenet.___torch_mangle_126._DenseLayer = prim::GetAttr[name="denselayer2"](%37)
  %4987 : __torch__.torchvision.models.densenet.___torch_mangle_129._DenseLayer = prim::GetAttr[name="denselayer3"](%37)
  %4988 : __torch__.torchvision.models.densenet.___torch_mangle_132._DenseLayer = prim::GetAttr[name="denselayer4"](%37)
  %4989 : __torch__.torchvision.models.densenet.___torch_mangle_135._DenseLayer = prim::GetAttr[name="denselayer5"](%37)
  %4990 : __torch__.torchvision.models.densenet.___torch_mangle_138._DenseLayer = prim::GetAttr[name="denselayer6"](%37)
  %4991 : __torch__.torchvision.models.densenet.___torch_mangle_141._DenseLayer = prim::GetAttr[name="denselayer7"](%37)
  %4992 : __torch__.torchvision.models.densenet.___torch_mangle_144._DenseLayer = prim::GetAttr[name="denselayer8"](%37)
  %4993 : __torch__.torchvision.models.densenet.___torch_mangle_147._DenseLayer = prim::GetAttr[name="denselayer9"](%37)
  %4994 : __torch__.torchvision.models.densenet.___torch_mangle_150._DenseLayer = prim::GetAttr[name="denselayer10"](%37)
  %4995 : __torch__.torchvision.models.densenet.___torch_mangle_153._DenseLayer = prim::GetAttr[name="denselayer11"](%37)
  %4996 : __torch__.torchvision.models.densenet.___torch_mangle_156._DenseLayer = prim::GetAttr[name="denselayer12"](%37)
  %4997 : __torch__.torchvision.models.densenet.___torch_mangle_300._DenseLayer = prim::GetAttr[name="denselayer13"](%37)
  %4998 : __torch__.torchvision.models.densenet.___torch_mangle_302._DenseLayer = prim::GetAttr[name="denselayer14"](%37)
  %4999 : __torch__.torchvision.models.densenet.___torch_mangle_305._DenseLayer = prim::GetAttr[name="denselayer15"](%37)
  %5000 : __torch__.torchvision.models.densenet.___torch_mangle_308._DenseLayer = prim::GetAttr[name="denselayer16"](%37)
  %5001 : __torch__.torchvision.models.densenet.___torch_mangle_310._DenseLayer = prim::GetAttr[name="denselayer17"](%37)
  %5002 : __torch__.torchvision.models.densenet.___torch_mangle_313._DenseLayer = prim::GetAttr[name="denselayer18"](%37)
  %5003 : __torch__.torchvision.models.densenet.___torch_mangle_316._DenseLayer = prim::GetAttr[name="denselayer19"](%37)
  %5004 : __torch__.torchvision.models.densenet.___torch_mangle_318._DenseLayer = prim::GetAttr[name="denselayer20"](%37)
  %5005 : __torch__.torchvision.models.densenet.___torch_mangle_324._DenseLayer = prim::GetAttr[name="denselayer21"](%37)
  %5006 : __torch__.torchvision.models.densenet.___torch_mangle_327._DenseLayer = prim::GetAttr[name="denselayer22"](%37)
  %5007 : __torch__.torchvision.models.densenet.___torch_mangle_329._DenseLayer = prim::GetAttr[name="denselayer23"](%37)
  %5008 : __torch__.torchvision.models.densenet.___torch_mangle_332._DenseLayer = prim::GetAttr[name="denselayer24"](%37)
  %5009 : __torch__.torchvision.models.densenet.___torch_mangle_335._DenseLayer = prim::GetAttr[name="denselayer25"](%37)
  %5010 : __torch__.torchvision.models.densenet.___torch_mangle_337._DenseLayer = prim::GetAttr[name="denselayer26"](%37)
  %5011 : __torch__.torchvision.models.densenet.___torch_mangle_340._DenseLayer = prim::GetAttr[name="denselayer27"](%37)
  %5012 : __torch__.torchvision.models.densenet.___torch_mangle_343._DenseLayer = prim::GetAttr[name="denselayer28"](%37)
  %5013 : __torch__.torchvision.models.densenet.___torch_mangle_345._DenseLayer = prim::GetAttr[name="denselayer29"](%37)
  %5014 : __torch__.torchvision.models.densenet.___torch_mangle_348._DenseLayer = prim::GetAttr[name="denselayer30"](%37)
  %5015 : __torch__.torchvision.models.densenet.___torch_mangle_351._DenseLayer = prim::GetAttr[name="denselayer31"](%37)
  %5016 : __torch__.torchvision.models.densenet.___torch_mangle_353._DenseLayer = prim::GetAttr[name="denselayer32"](%37)
  %5017 : Tensor = prim::Uninitialized()
  %5018 : bool = prim::GetAttr[name="memory_efficient"](%4985)
  %5019 : bool = prim::If(%5018) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5020 : bool = prim::Uninitialized()
      %5021 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5022 : bool = aten::gt(%5021, %24)
      %5023 : bool, %5024 : bool, %5025 : int = prim::Loop(%18, %5022, %19, %5020, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5026 : int, %5027 : bool, %5028 : bool, %5029 : int):
          %tensor.2 : Tensor = aten::__getitem__(%features.1, %5029) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5031 : bool = prim::requires_grad(%tensor.2)
          %5032 : bool, %5033 : bool = prim::If(%5031) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5020)
          %5034 : int = aten::add(%5029, %27)
          %5035 : bool = aten::lt(%5034, %5021)
          %5036 : bool = aten::__and__(%5035, %5032)
          -> (%5036, %5031, %5033, %5034)
      %5037 : bool = prim::If(%5023)
        block0():
          -> (%5024)
        block1():
          -> (%19)
      -> (%5037)
    block1():
      -> (%19)
  %bottleneck_output.1 : Tensor = prim::If(%5019) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5017)
    block1():
      %concated_features.2 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5040 : __torch__.torch.nn.modules.conv.___torch_mangle_122.Conv2d = prim::GetAttr[name="conv1"](%4985)
      %5041 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_121.BatchNorm2d = prim::GetAttr[name="norm1"](%4985)
      %5042 : int = aten::dim(%concated_features.2) # torch/nn/modules/batchnorm.py:276:11
      %5043 : bool = aten::ne(%5042, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5043) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5044 : bool = prim::GetAttr[name="training"](%5041)
       = prim::If(%5044) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5045 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5041)
          %5046 : Tensor = aten::add(%5045, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5041, %5046)
          -> ()
        block1():
          -> ()
      %5047 : bool = prim::GetAttr[name="training"](%5041)
      %5048 : Tensor = prim::GetAttr[name="running_mean"](%5041)
      %5049 : Tensor = prim::GetAttr[name="running_var"](%5041)
      %5050 : Tensor = prim::GetAttr[name="weight"](%5041)
      %5051 : Tensor = prim::GetAttr[name="bias"](%5041)
       = prim::If(%5047) # torch/nn/functional.py:2011:4
        block0():
          %5052 : int[] = aten::size(%concated_features.2) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%5052, %24) # torch/nn/functional.py:1991:17
          %5054 : int = aten::len(%5052) # torch/nn/functional.py:1992:19
          %5055 : int = aten::sub(%5054, %26) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%5055, %25, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %5059 : int = aten::add(%i.3, %26) # torch/nn/functional.py:1993:27
              %5060 : int = aten::__getitem__(%5052, %5059) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %5060) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.11)
          %5062 : bool = aten::eq(%size_prods.9, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5062) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5063 : Tensor = aten::batch_norm(%concated_features.2, %5050, %5051, %5048, %5049, %5047, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.3 : Tensor = aten::relu_(%5063) # torch/nn/functional.py:1117:17
      %5065 : Tensor = prim::GetAttr[name="weight"](%5040)
      %5066 : Tensor? = prim::GetAttr[name="bias"](%5040)
      %5067 : int[] = prim::ListConstruct(%27, %27)
      %5068 : int[] = prim::ListConstruct(%24, %24)
      %5069 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.3 : Tensor = aten::conv2d(%result.3, %5065, %5066, %5067, %5068, %5069, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.3)
  %5071 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4985)
  %5072 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4985)
  %5073 : int = aten::dim(%bottleneck_output.1) # torch/nn/modules/batchnorm.py:276:11
  %5074 : bool = aten::ne(%5073, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5074) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5075 : bool = prim::GetAttr[name="training"](%5072)
   = prim::If(%5075) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5076 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5072)
      %5077 : Tensor = aten::add(%5076, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5072, %5077)
      -> ()
    block1():
      -> ()
  %5078 : bool = prim::GetAttr[name="training"](%5072)
  %5079 : Tensor = prim::GetAttr[name="running_mean"](%5072)
  %5080 : Tensor = prim::GetAttr[name="running_var"](%5072)
  %5081 : Tensor = prim::GetAttr[name="weight"](%5072)
  %5082 : Tensor = prim::GetAttr[name="bias"](%5072)
   = prim::If(%5078) # torch/nn/functional.py:2011:4
    block0():
      %5083 : int[] = aten::size(%bottleneck_output.1) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%5083, %24) # torch/nn/functional.py:1991:17
      %5085 : int = aten::len(%5083) # torch/nn/functional.py:1992:19
      %5086 : int = aten::sub(%5085, %26) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%5086, %25, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %5090 : int = aten::add(%i.4, %26) # torch/nn/functional.py:1993:27
          %5091 : int = aten::__getitem__(%5083, %5090) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %5091) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.15)
      %5093 : bool = aten::eq(%size_prods.13, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5093) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5094 : Tensor = aten::batch_norm(%bottleneck_output.1, %5081, %5082, %5079, %5080, %5078, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.4 : Tensor = aten::relu_(%5094) # torch/nn/functional.py:1117:17
  %5096 : Tensor = prim::GetAttr[name="weight"](%5071)
  %5097 : Tensor? = prim::GetAttr[name="bias"](%5071)
  %5098 : int[] = prim::ListConstruct(%27, %27)
  %5099 : int[] = prim::ListConstruct(%27, %27)
  %5100 : int[] = prim::ListConstruct(%27, %27)
  %new_features.4 : Tensor = aten::conv2d(%result.4, %5096, %5097, %5098, %5099, %5100, %27) # torch/nn/modules/conv.py:415:15
  %5102 : float = prim::GetAttr[name="drop_rate"](%4985)
  %5103 : bool = aten::gt(%5102, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.2 : Tensor = prim::If(%5103) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5105 : float = prim::GetAttr[name="drop_rate"](%4985)
      %5106 : bool = prim::GetAttr[name="training"](%4985)
      %5107 : bool = aten::lt(%5105, %16) # torch/nn/functional.py:968:7
      %5108 : bool = prim::If(%5107) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5109 : bool = aten::gt(%5105, %17) # torch/nn/functional.py:968:17
          -> (%5109)
       = prim::If(%5108) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5110 : Tensor = aten::dropout(%new_features.4, %5105, %5106) # torch/nn/functional.py:973:17
      -> (%5110)
    block1():
      -> (%new_features.4)
  %5111 : Tensor[] = aten::append(%features.1, %new_features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5112 : Tensor = prim::Uninitialized()
  %5113 : bool = prim::GetAttr[name="memory_efficient"](%4986)
  %5114 : bool = prim::If(%5113) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5115 : bool = prim::Uninitialized()
      %5116 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5117 : bool = aten::gt(%5116, %24)
      %5118 : bool, %5119 : bool, %5120 : int = prim::Loop(%18, %5117, %19, %5115, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5121 : int, %5122 : bool, %5123 : bool, %5124 : int):
          %tensor.3 : Tensor = aten::__getitem__(%features.1, %5124) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5126 : bool = prim::requires_grad(%tensor.3)
          %5127 : bool, %5128 : bool = prim::If(%5126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5115)
          %5129 : int = aten::add(%5124, %27)
          %5130 : bool = aten::lt(%5129, %5116)
          %5131 : bool = aten::__and__(%5130, %5127)
          -> (%5131, %5126, %5128, %5129)
      %5132 : bool = prim::If(%5118)
        block0():
          -> (%5119)
        block1():
          -> (%19)
      -> (%5132)
    block1():
      -> (%19)
  %bottleneck_output.4 : Tensor = prim::If(%5114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5112)
    block1():
      %concated_features.3 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5135 : __torch__.torch.nn.modules.conv.___torch_mangle_125.Conv2d = prim::GetAttr[name="conv1"](%4986)
      %5136 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%4986)
      %5137 : int = aten::dim(%concated_features.3) # torch/nn/modules/batchnorm.py:276:11
      %5138 : bool = aten::ne(%5137, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5138) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5139 : bool = prim::GetAttr[name="training"](%5136)
       = prim::If(%5139) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5140 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5136)
          %5141 : Tensor = aten::add(%5140, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5136, %5141)
          -> ()
        block1():
          -> ()
      %5142 : bool = prim::GetAttr[name="training"](%5136)
      %5143 : Tensor = prim::GetAttr[name="running_mean"](%5136)
      %5144 : Tensor = prim::GetAttr[name="running_var"](%5136)
      %5145 : Tensor = prim::GetAttr[name="weight"](%5136)
      %5146 : Tensor = prim::GetAttr[name="bias"](%5136)
       = prim::If(%5142) # torch/nn/functional.py:2011:4
        block0():
          %5147 : int[] = aten::size(%concated_features.3) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%5147, %24) # torch/nn/functional.py:1991:17
          %5149 : int = aten::len(%5147) # torch/nn/functional.py:1992:19
          %5150 : int = aten::sub(%5149, %26) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%5150, %25, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %5154 : int = aten::add(%i.5, %26) # torch/nn/functional.py:1993:27
              %5155 : int = aten::__getitem__(%5147, %5154) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %5155) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.19)
          %5157 : bool = aten::eq(%size_prods.17, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5157) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5158 : Tensor = aten::batch_norm(%concated_features.3, %5145, %5146, %5143, %5144, %5142, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.5 : Tensor = aten::relu_(%5158) # torch/nn/functional.py:1117:17
      %5160 : Tensor = prim::GetAttr[name="weight"](%5135)
      %5161 : Tensor? = prim::GetAttr[name="bias"](%5135)
      %5162 : int[] = prim::ListConstruct(%27, %27)
      %5163 : int[] = prim::ListConstruct(%24, %24)
      %5164 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.5 : Tensor = aten::conv2d(%result.5, %5160, %5161, %5162, %5163, %5164, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.5)
  %5166 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4986)
  %5167 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4986)
  %5168 : int = aten::dim(%bottleneck_output.4) # torch/nn/modules/batchnorm.py:276:11
  %5169 : bool = aten::ne(%5168, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5169) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5170 : bool = prim::GetAttr[name="training"](%5167)
   = prim::If(%5170) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5171 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5167)
      %5172 : Tensor = aten::add(%5171, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5167, %5172)
      -> ()
    block1():
      -> ()
  %5173 : bool = prim::GetAttr[name="training"](%5167)
  %5174 : Tensor = prim::GetAttr[name="running_mean"](%5167)
  %5175 : Tensor = prim::GetAttr[name="running_var"](%5167)
  %5176 : Tensor = prim::GetAttr[name="weight"](%5167)
  %5177 : Tensor = prim::GetAttr[name="bias"](%5167)
   = prim::If(%5173) # torch/nn/functional.py:2011:4
    block0():
      %5178 : int[] = aten::size(%bottleneck_output.4) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%5178, %24) # torch/nn/functional.py:1991:17
      %5180 : int = aten::len(%5178) # torch/nn/functional.py:1992:19
      %5181 : int = aten::sub(%5180, %26) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%5181, %25, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %5185 : int = aten::add(%i.6, %26) # torch/nn/functional.py:1993:27
          %5186 : int = aten::__getitem__(%5178, %5185) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %5186) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.23)
      %5188 : bool = aten::eq(%size_prods.21, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5188) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5189 : Tensor = aten::batch_norm(%bottleneck_output.4, %5176, %5177, %5174, %5175, %5173, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.6 : Tensor = aten::relu_(%5189) # torch/nn/functional.py:1117:17
  %5191 : Tensor = prim::GetAttr[name="weight"](%5166)
  %5192 : Tensor? = prim::GetAttr[name="bias"](%5166)
  %5193 : int[] = prim::ListConstruct(%27, %27)
  %5194 : int[] = prim::ListConstruct(%27, %27)
  %5195 : int[] = prim::ListConstruct(%27, %27)
  %new_features.6 : Tensor = aten::conv2d(%result.6, %5191, %5192, %5193, %5194, %5195, %27) # torch/nn/modules/conv.py:415:15
  %5197 : float = prim::GetAttr[name="drop_rate"](%4986)
  %5198 : bool = aten::gt(%5197, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.3 : Tensor = prim::If(%5198) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5200 : float = prim::GetAttr[name="drop_rate"](%4986)
      %5201 : bool = prim::GetAttr[name="training"](%4986)
      %5202 : bool = aten::lt(%5200, %16) # torch/nn/functional.py:968:7
      %5203 : bool = prim::If(%5202) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5204 : bool = aten::gt(%5200, %17) # torch/nn/functional.py:968:17
          -> (%5204)
       = prim::If(%5203) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5205 : Tensor = aten::dropout(%new_features.6, %5200, %5201) # torch/nn/functional.py:973:17
      -> (%5205)
    block1():
      -> (%new_features.6)
  %5206 : Tensor[] = aten::append(%features.1, %new_features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5207 : Tensor = prim::Uninitialized()
  %5208 : bool = prim::GetAttr[name="memory_efficient"](%4987)
  %5209 : bool = prim::If(%5208) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5210 : bool = prim::Uninitialized()
      %5211 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5212 : bool = aten::gt(%5211, %24)
      %5213 : bool, %5214 : bool, %5215 : int = prim::Loop(%18, %5212, %19, %5210, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5216 : int, %5217 : bool, %5218 : bool, %5219 : int):
          %tensor.4 : Tensor = aten::__getitem__(%features.1, %5219) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5221 : bool = prim::requires_grad(%tensor.4)
          %5222 : bool, %5223 : bool = prim::If(%5221) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5210)
          %5224 : int = aten::add(%5219, %27)
          %5225 : bool = aten::lt(%5224, %5211)
          %5226 : bool = aten::__and__(%5225, %5222)
          -> (%5226, %5221, %5223, %5224)
      %5227 : bool = prim::If(%5213)
        block0():
          -> (%5214)
        block1():
          -> (%19)
      -> (%5227)
    block1():
      -> (%19)
  %bottleneck_output.6 : Tensor = prim::If(%5209) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5207)
    block1():
      %concated_features.4 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5230 : __torch__.torch.nn.modules.conv.___torch_mangle_128.Conv2d = prim::GetAttr[name="conv1"](%4987)
      %5231 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_127.BatchNorm2d = prim::GetAttr[name="norm1"](%4987)
      %5232 : int = aten::dim(%concated_features.4) # torch/nn/modules/batchnorm.py:276:11
      %5233 : bool = aten::ne(%5232, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5233) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5234 : bool = prim::GetAttr[name="training"](%5231)
       = prim::If(%5234) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5235 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5231)
          %5236 : Tensor = aten::add(%5235, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5231, %5236)
          -> ()
        block1():
          -> ()
      %5237 : bool = prim::GetAttr[name="training"](%5231)
      %5238 : Tensor = prim::GetAttr[name="running_mean"](%5231)
      %5239 : Tensor = prim::GetAttr[name="running_var"](%5231)
      %5240 : Tensor = prim::GetAttr[name="weight"](%5231)
      %5241 : Tensor = prim::GetAttr[name="bias"](%5231)
       = prim::If(%5237) # torch/nn/functional.py:2011:4
        block0():
          %5242 : int[] = aten::size(%concated_features.4) # torch/nn/functional.py:2012:27
          %size_prods.24 : int = aten::__getitem__(%5242, %24) # torch/nn/functional.py:1991:17
          %5244 : int = aten::len(%5242) # torch/nn/functional.py:1992:19
          %5245 : int = aten::sub(%5244, %26) # torch/nn/functional.py:1992:19
          %size_prods.25 : int = prim::Loop(%5245, %25, %size_prods.24) # torch/nn/functional.py:1992:4
            block0(%i.7 : int, %size_prods.26 : int):
              %5249 : int = aten::add(%i.7, %26) # torch/nn/functional.py:1993:27
              %5250 : int = aten::__getitem__(%5242, %5249) # torch/nn/functional.py:1993:22
              %size_prods.27 : int = aten::mul(%size_prods.26, %5250) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.27)
          %5252 : bool = aten::eq(%size_prods.25, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5252) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5253 : Tensor = aten::batch_norm(%concated_features.4, %5240, %5241, %5238, %5239, %5237, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.7 : Tensor = aten::relu_(%5253) # torch/nn/functional.py:1117:17
      %5255 : Tensor = prim::GetAttr[name="weight"](%5230)
      %5256 : Tensor? = prim::GetAttr[name="bias"](%5230)
      %5257 : int[] = prim::ListConstruct(%27, %27)
      %5258 : int[] = prim::ListConstruct(%24, %24)
      %5259 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.7 : Tensor = aten::conv2d(%result.7, %5255, %5256, %5257, %5258, %5259, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.7)
  %5261 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4987)
  %5262 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4987)
  %5263 : int = aten::dim(%bottleneck_output.6) # torch/nn/modules/batchnorm.py:276:11
  %5264 : bool = aten::ne(%5263, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5264) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5265 : bool = prim::GetAttr[name="training"](%5262)
   = prim::If(%5265) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5266 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5262)
      %5267 : Tensor = aten::add(%5266, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5262, %5267)
      -> ()
    block1():
      -> ()
  %5268 : bool = prim::GetAttr[name="training"](%5262)
  %5269 : Tensor = prim::GetAttr[name="running_mean"](%5262)
  %5270 : Tensor = prim::GetAttr[name="running_var"](%5262)
  %5271 : Tensor = prim::GetAttr[name="weight"](%5262)
  %5272 : Tensor = prim::GetAttr[name="bias"](%5262)
   = prim::If(%5268) # torch/nn/functional.py:2011:4
    block0():
      %5273 : int[] = aten::size(%bottleneck_output.6) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%5273, %24) # torch/nn/functional.py:1991:17
      %5275 : int = aten::len(%5273) # torch/nn/functional.py:1992:19
      %5276 : int = aten::sub(%5275, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%5276, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %5280 : int = aten::add(%i.8, %26) # torch/nn/functional.py:1993:27
          %5281 : int = aten::__getitem__(%5273, %5280) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %5281) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %5283 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5283) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5284 : Tensor = aten::batch_norm(%bottleneck_output.6, %5271, %5272, %5269, %5270, %5268, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.8 : Tensor = aten::relu_(%5284) # torch/nn/functional.py:1117:17
  %5286 : Tensor = prim::GetAttr[name="weight"](%5261)
  %5287 : Tensor? = prim::GetAttr[name="bias"](%5261)
  %5288 : int[] = prim::ListConstruct(%27, %27)
  %5289 : int[] = prim::ListConstruct(%27, %27)
  %5290 : int[] = prim::ListConstruct(%27, %27)
  %new_features.8 : Tensor = aten::conv2d(%result.8, %5286, %5287, %5288, %5289, %5290, %27) # torch/nn/modules/conv.py:415:15
  %5292 : float = prim::GetAttr[name="drop_rate"](%4987)
  %5293 : bool = aten::gt(%5292, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.5 : Tensor = prim::If(%5293) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5295 : float = prim::GetAttr[name="drop_rate"](%4987)
      %5296 : bool = prim::GetAttr[name="training"](%4987)
      %5297 : bool = aten::lt(%5295, %16) # torch/nn/functional.py:968:7
      %5298 : bool = prim::If(%5297) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5299 : bool = aten::gt(%5295, %17) # torch/nn/functional.py:968:17
          -> (%5299)
       = prim::If(%5298) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5300 : Tensor = aten::dropout(%new_features.8, %5295, %5296) # torch/nn/functional.py:973:17
      -> (%5300)
    block1():
      -> (%new_features.8)
  %5301 : Tensor[] = aten::append(%features.1, %new_features.5) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5302 : Tensor = prim::Uninitialized()
  %5303 : bool = prim::GetAttr[name="memory_efficient"](%4988)
  %5304 : bool = prim::If(%5303) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5305 : bool = prim::Uninitialized()
      %5306 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5307 : bool = aten::gt(%5306, %24)
      %5308 : bool, %5309 : bool, %5310 : int = prim::Loop(%18, %5307, %19, %5305, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5311 : int, %5312 : bool, %5313 : bool, %5314 : int):
          %tensor.5 : Tensor = aten::__getitem__(%features.1, %5314) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5316 : bool = prim::requires_grad(%tensor.5)
          %5317 : bool, %5318 : bool = prim::If(%5316) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5305)
          %5319 : int = aten::add(%5314, %27)
          %5320 : bool = aten::lt(%5319, %5306)
          %5321 : bool = aten::__and__(%5320, %5317)
          -> (%5321, %5316, %5318, %5319)
      %5322 : bool = prim::If(%5308)
        block0():
          -> (%5309)
        block1():
          -> (%19)
      -> (%5322)
    block1():
      -> (%19)
  %bottleneck_output.8 : Tensor = prim::If(%5304) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5302)
    block1():
      %concated_features.5 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5325 : __torch__.torch.nn.modules.conv.___torch_mangle_131.Conv2d = prim::GetAttr[name="conv1"](%4988)
      %5326 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_130.BatchNorm2d = prim::GetAttr[name="norm1"](%4988)
      %5327 : int = aten::dim(%concated_features.5) # torch/nn/modules/batchnorm.py:276:11
      %5328 : bool = aten::ne(%5327, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5328) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5329 : bool = prim::GetAttr[name="training"](%5326)
       = prim::If(%5329) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5330 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5326)
          %5331 : Tensor = aten::add(%5330, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5326, %5331)
          -> ()
        block1():
          -> ()
      %5332 : bool = prim::GetAttr[name="training"](%5326)
      %5333 : Tensor = prim::GetAttr[name="running_mean"](%5326)
      %5334 : Tensor = prim::GetAttr[name="running_var"](%5326)
      %5335 : Tensor = prim::GetAttr[name="weight"](%5326)
      %5336 : Tensor = prim::GetAttr[name="bias"](%5326)
       = prim::If(%5332) # torch/nn/functional.py:2011:4
        block0():
          %5337 : int[] = aten::size(%concated_features.5) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%5337, %24) # torch/nn/functional.py:1991:17
          %5339 : int = aten::len(%5337) # torch/nn/functional.py:1992:19
          %5340 : int = aten::sub(%5339, %26) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%5340, %25, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %5344 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
              %5345 : int = aten::__getitem__(%5337, %5344) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %5345) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.35)
          %5347 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5347) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5348 : Tensor = aten::batch_norm(%concated_features.5, %5335, %5336, %5333, %5334, %5332, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.9 : Tensor = aten::relu_(%5348) # torch/nn/functional.py:1117:17
      %5350 : Tensor = prim::GetAttr[name="weight"](%5325)
      %5351 : Tensor? = prim::GetAttr[name="bias"](%5325)
      %5352 : int[] = prim::ListConstruct(%27, %27)
      %5353 : int[] = prim::ListConstruct(%24, %24)
      %5354 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.9 : Tensor = aten::conv2d(%result.9, %5350, %5351, %5352, %5353, %5354, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.9)
  %5356 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4988)
  %5357 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4988)
  %5358 : int = aten::dim(%bottleneck_output.8) # torch/nn/modules/batchnorm.py:276:11
  %5359 : bool = aten::ne(%5358, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5359) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5360 : bool = prim::GetAttr[name="training"](%5357)
   = prim::If(%5360) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5361 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5357)
      %5362 : Tensor = aten::add(%5361, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5357, %5362)
      -> ()
    block1():
      -> ()
  %5363 : bool = prim::GetAttr[name="training"](%5357)
  %5364 : Tensor = prim::GetAttr[name="running_mean"](%5357)
  %5365 : Tensor = prim::GetAttr[name="running_var"](%5357)
  %5366 : Tensor = prim::GetAttr[name="weight"](%5357)
  %5367 : Tensor = prim::GetAttr[name="bias"](%5357)
   = prim::If(%5363) # torch/nn/functional.py:2011:4
    block0():
      %5368 : int[] = aten::size(%bottleneck_output.8) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%5368, %24) # torch/nn/functional.py:1991:17
      %5370 : int = aten::len(%5368) # torch/nn/functional.py:1992:19
      %5371 : int = aten::sub(%5370, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%5371, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %5375 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %5376 : int = aten::__getitem__(%5368, %5375) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %5376) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %5378 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5378) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5379 : Tensor = aten::batch_norm(%bottleneck_output.8, %5366, %5367, %5364, %5365, %5363, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.10 : Tensor = aten::relu_(%5379) # torch/nn/functional.py:1117:17
  %5381 : Tensor = prim::GetAttr[name="weight"](%5356)
  %5382 : Tensor? = prim::GetAttr[name="bias"](%5356)
  %5383 : int[] = prim::ListConstruct(%27, %27)
  %5384 : int[] = prim::ListConstruct(%27, %27)
  %5385 : int[] = prim::ListConstruct(%27, %27)
  %new_features.10 : Tensor = aten::conv2d(%result.10, %5381, %5382, %5383, %5384, %5385, %27) # torch/nn/modules/conv.py:415:15
  %5387 : float = prim::GetAttr[name="drop_rate"](%4988)
  %5388 : bool = aten::gt(%5387, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.7 : Tensor = prim::If(%5388) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5390 : float = prim::GetAttr[name="drop_rate"](%4988)
      %5391 : bool = prim::GetAttr[name="training"](%4988)
      %5392 : bool = aten::lt(%5390, %16) # torch/nn/functional.py:968:7
      %5393 : bool = prim::If(%5392) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5394 : bool = aten::gt(%5390, %17) # torch/nn/functional.py:968:17
          -> (%5394)
       = prim::If(%5393) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5395 : Tensor = aten::dropout(%new_features.10, %5390, %5391) # torch/nn/functional.py:973:17
      -> (%5395)
    block1():
      -> (%new_features.10)
  %5396 : Tensor[] = aten::append(%features.1, %new_features.7) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5397 : Tensor = prim::Uninitialized()
  %5398 : bool = prim::GetAttr[name="memory_efficient"](%4989)
  %5399 : bool = prim::If(%5398) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5400 : bool = prim::Uninitialized()
      %5401 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5402 : bool = aten::gt(%5401, %24)
      %5403 : bool, %5404 : bool, %5405 : int = prim::Loop(%18, %5402, %19, %5400, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5406 : int, %5407 : bool, %5408 : bool, %5409 : int):
          %tensor.6 : Tensor = aten::__getitem__(%features.1, %5409) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5411 : bool = prim::requires_grad(%tensor.6)
          %5412 : bool, %5413 : bool = prim::If(%5411) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5400)
          %5414 : int = aten::add(%5409, %27)
          %5415 : bool = aten::lt(%5414, %5401)
          %5416 : bool = aten::__and__(%5415, %5412)
          -> (%5416, %5411, %5413, %5414)
      %5417 : bool = prim::If(%5403)
        block0():
          -> (%5404)
        block1():
          -> (%19)
      -> (%5417)
    block1():
      -> (%19)
  %bottleneck_output.10 : Tensor = prim::If(%5399) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5397)
    block1():
      %concated_features.6 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5420 : __torch__.torch.nn.modules.conv.___torch_mangle_134.Conv2d = prim::GetAttr[name="conv1"](%4989)
      %5421 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%4989)
      %5422 : int = aten::dim(%concated_features.6) # torch/nn/modules/batchnorm.py:276:11
      %5423 : bool = aten::ne(%5422, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5423) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5424 : bool = prim::GetAttr[name="training"](%5421)
       = prim::If(%5424) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5425 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5421)
          %5426 : Tensor = aten::add(%5425, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5421, %5426)
          -> ()
        block1():
          -> ()
      %5427 : bool = prim::GetAttr[name="training"](%5421)
      %5428 : Tensor = prim::GetAttr[name="running_mean"](%5421)
      %5429 : Tensor = prim::GetAttr[name="running_var"](%5421)
      %5430 : Tensor = prim::GetAttr[name="weight"](%5421)
      %5431 : Tensor = prim::GetAttr[name="bias"](%5421)
       = prim::If(%5427) # torch/nn/functional.py:2011:4
        block0():
          %5432 : int[] = aten::size(%concated_features.6) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%5432, %24) # torch/nn/functional.py:1991:17
          %5434 : int = aten::len(%5432) # torch/nn/functional.py:1992:19
          %5435 : int = aten::sub(%5434, %26) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%5435, %25, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %5439 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
              %5440 : int = aten::__getitem__(%5432, %5439) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %5440) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.43)
          %5442 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5442) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5443 : Tensor = aten::batch_norm(%concated_features.6, %5430, %5431, %5428, %5429, %5427, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.11 : Tensor = aten::relu_(%5443) # torch/nn/functional.py:1117:17
      %5445 : Tensor = prim::GetAttr[name="weight"](%5420)
      %5446 : Tensor? = prim::GetAttr[name="bias"](%5420)
      %5447 : int[] = prim::ListConstruct(%27, %27)
      %5448 : int[] = prim::ListConstruct(%24, %24)
      %5449 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.11 : Tensor = aten::conv2d(%result.11, %5445, %5446, %5447, %5448, %5449, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.11)
  %5451 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4989)
  %5452 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4989)
  %5453 : int = aten::dim(%bottleneck_output.10) # torch/nn/modules/batchnorm.py:276:11
  %5454 : bool = aten::ne(%5453, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5454) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5455 : bool = prim::GetAttr[name="training"](%5452)
   = prim::If(%5455) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5456 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5452)
      %5457 : Tensor = aten::add(%5456, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5452, %5457)
      -> ()
    block1():
      -> ()
  %5458 : bool = prim::GetAttr[name="training"](%5452)
  %5459 : Tensor = prim::GetAttr[name="running_mean"](%5452)
  %5460 : Tensor = prim::GetAttr[name="running_var"](%5452)
  %5461 : Tensor = prim::GetAttr[name="weight"](%5452)
  %5462 : Tensor = prim::GetAttr[name="bias"](%5452)
   = prim::If(%5458) # torch/nn/functional.py:2011:4
    block0():
      %5463 : int[] = aten::size(%bottleneck_output.10) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%5463, %24) # torch/nn/functional.py:1991:17
      %5465 : int = aten::len(%5463) # torch/nn/functional.py:1992:19
      %5466 : int = aten::sub(%5465, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%5466, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %5470 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %5471 : int = aten::__getitem__(%5463, %5470) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %5471) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %5473 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5473) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5474 : Tensor = aten::batch_norm(%bottleneck_output.10, %5461, %5462, %5459, %5460, %5458, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.12 : Tensor = aten::relu_(%5474) # torch/nn/functional.py:1117:17
  %5476 : Tensor = prim::GetAttr[name="weight"](%5451)
  %5477 : Tensor? = prim::GetAttr[name="bias"](%5451)
  %5478 : int[] = prim::ListConstruct(%27, %27)
  %5479 : int[] = prim::ListConstruct(%27, %27)
  %5480 : int[] = prim::ListConstruct(%27, %27)
  %new_features.12 : Tensor = aten::conv2d(%result.12, %5476, %5477, %5478, %5479, %5480, %27) # torch/nn/modules/conv.py:415:15
  %5482 : float = prim::GetAttr[name="drop_rate"](%4989)
  %5483 : bool = aten::gt(%5482, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.9 : Tensor = prim::If(%5483) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5485 : float = prim::GetAttr[name="drop_rate"](%4989)
      %5486 : bool = prim::GetAttr[name="training"](%4989)
      %5487 : bool = aten::lt(%5485, %16) # torch/nn/functional.py:968:7
      %5488 : bool = prim::If(%5487) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5489 : bool = aten::gt(%5485, %17) # torch/nn/functional.py:968:17
          -> (%5489)
       = prim::If(%5488) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5490 : Tensor = aten::dropout(%new_features.12, %5485, %5486) # torch/nn/functional.py:973:17
      -> (%5490)
    block1():
      -> (%new_features.12)
  %5491 : Tensor[] = aten::append(%features.1, %new_features.9) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5492 : Tensor = prim::Uninitialized()
  %5493 : bool = prim::GetAttr[name="memory_efficient"](%4990)
  %5494 : bool = prim::If(%5493) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5495 : bool = prim::Uninitialized()
      %5496 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5497 : bool = aten::gt(%5496, %24)
      %5498 : bool, %5499 : bool, %5500 : int = prim::Loop(%18, %5497, %19, %5495, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5501 : int, %5502 : bool, %5503 : bool, %5504 : int):
          %tensor.7 : Tensor = aten::__getitem__(%features.1, %5504) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5506 : bool = prim::requires_grad(%tensor.7)
          %5507 : bool, %5508 : bool = prim::If(%5506) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5495)
          %5509 : int = aten::add(%5504, %27)
          %5510 : bool = aten::lt(%5509, %5496)
          %5511 : bool = aten::__and__(%5510, %5507)
          -> (%5511, %5506, %5508, %5509)
      %5512 : bool = prim::If(%5498)
        block0():
          -> (%5499)
        block1():
          -> (%19)
      -> (%5512)
    block1():
      -> (%19)
  %bottleneck_output.12 : Tensor = prim::If(%5494) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5492)
    block1():
      %concated_features.7 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5515 : __torch__.torch.nn.modules.conv.___torch_mangle_137.Conv2d = prim::GetAttr[name="conv1"](%4990)
      %5516 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_136.BatchNorm2d = prim::GetAttr[name="norm1"](%4990)
      %5517 : int = aten::dim(%concated_features.7) # torch/nn/modules/batchnorm.py:276:11
      %5518 : bool = aten::ne(%5517, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5518) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5519 : bool = prim::GetAttr[name="training"](%5516)
       = prim::If(%5519) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5520 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5516)
          %5521 : Tensor = aten::add(%5520, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5516, %5521)
          -> ()
        block1():
          -> ()
      %5522 : bool = prim::GetAttr[name="training"](%5516)
      %5523 : Tensor = prim::GetAttr[name="running_mean"](%5516)
      %5524 : Tensor = prim::GetAttr[name="running_var"](%5516)
      %5525 : Tensor = prim::GetAttr[name="weight"](%5516)
      %5526 : Tensor = prim::GetAttr[name="bias"](%5516)
       = prim::If(%5522) # torch/nn/functional.py:2011:4
        block0():
          %5527 : int[] = aten::size(%concated_features.7) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%5527, %24) # torch/nn/functional.py:1991:17
          %5529 : int = aten::len(%5527) # torch/nn/functional.py:1992:19
          %5530 : int = aten::sub(%5529, %26) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%5530, %25, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %5534 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
              %5535 : int = aten::__getitem__(%5527, %5534) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %5535) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.51)
          %5537 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5537) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5538 : Tensor = aten::batch_norm(%concated_features.7, %5525, %5526, %5523, %5524, %5522, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.13 : Tensor = aten::relu_(%5538) # torch/nn/functional.py:1117:17
      %5540 : Tensor = prim::GetAttr[name="weight"](%5515)
      %5541 : Tensor? = prim::GetAttr[name="bias"](%5515)
      %5542 : int[] = prim::ListConstruct(%27, %27)
      %5543 : int[] = prim::ListConstruct(%24, %24)
      %5544 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.13 : Tensor = aten::conv2d(%result.13, %5540, %5541, %5542, %5543, %5544, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.13)
  %5546 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4990)
  %5547 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4990)
  %5548 : int = aten::dim(%bottleneck_output.12) # torch/nn/modules/batchnorm.py:276:11
  %5549 : bool = aten::ne(%5548, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5549) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5550 : bool = prim::GetAttr[name="training"](%5547)
   = prim::If(%5550) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5551 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5547)
      %5552 : Tensor = aten::add(%5551, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5547, %5552)
      -> ()
    block1():
      -> ()
  %5553 : bool = prim::GetAttr[name="training"](%5547)
  %5554 : Tensor = prim::GetAttr[name="running_mean"](%5547)
  %5555 : Tensor = prim::GetAttr[name="running_var"](%5547)
  %5556 : Tensor = prim::GetAttr[name="weight"](%5547)
  %5557 : Tensor = prim::GetAttr[name="bias"](%5547)
   = prim::If(%5553) # torch/nn/functional.py:2011:4
    block0():
      %5558 : int[] = aten::size(%bottleneck_output.12) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%5558, %24) # torch/nn/functional.py:1991:17
      %5560 : int = aten::len(%5558) # torch/nn/functional.py:1992:19
      %5561 : int = aten::sub(%5560, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%5561, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %5565 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %5566 : int = aten::__getitem__(%5558, %5565) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %5566) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %5568 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5568) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5569 : Tensor = aten::batch_norm(%bottleneck_output.12, %5556, %5557, %5554, %5555, %5553, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.14 : Tensor = aten::relu_(%5569) # torch/nn/functional.py:1117:17
  %5571 : Tensor = prim::GetAttr[name="weight"](%5546)
  %5572 : Tensor? = prim::GetAttr[name="bias"](%5546)
  %5573 : int[] = prim::ListConstruct(%27, %27)
  %5574 : int[] = prim::ListConstruct(%27, %27)
  %5575 : int[] = prim::ListConstruct(%27, %27)
  %new_features.14 : Tensor = aten::conv2d(%result.14, %5571, %5572, %5573, %5574, %5575, %27) # torch/nn/modules/conv.py:415:15
  %5577 : float = prim::GetAttr[name="drop_rate"](%4990)
  %5578 : bool = aten::gt(%5577, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.11 : Tensor = prim::If(%5578) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5580 : float = prim::GetAttr[name="drop_rate"](%4990)
      %5581 : bool = prim::GetAttr[name="training"](%4990)
      %5582 : bool = aten::lt(%5580, %16) # torch/nn/functional.py:968:7
      %5583 : bool = prim::If(%5582) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5584 : bool = aten::gt(%5580, %17) # torch/nn/functional.py:968:17
          -> (%5584)
       = prim::If(%5583) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5585 : Tensor = aten::dropout(%new_features.14, %5580, %5581) # torch/nn/functional.py:973:17
      -> (%5585)
    block1():
      -> (%new_features.14)
  %5586 : Tensor[] = aten::append(%features.1, %new_features.11) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5587 : Tensor = prim::Uninitialized()
  %5588 : bool = prim::GetAttr[name="memory_efficient"](%4991)
  %5589 : bool = prim::If(%5588) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5590 : bool = prim::Uninitialized()
      %5591 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5592 : bool = aten::gt(%5591, %24)
      %5593 : bool, %5594 : bool, %5595 : int = prim::Loop(%18, %5592, %19, %5590, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5596 : int, %5597 : bool, %5598 : bool, %5599 : int):
          %tensor.8 : Tensor = aten::__getitem__(%features.1, %5599) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5601 : bool = prim::requires_grad(%tensor.8)
          %5602 : bool, %5603 : bool = prim::If(%5601) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5590)
          %5604 : int = aten::add(%5599, %27)
          %5605 : bool = aten::lt(%5604, %5591)
          %5606 : bool = aten::__and__(%5605, %5602)
          -> (%5606, %5601, %5603, %5604)
      %5607 : bool = prim::If(%5593)
        block0():
          -> (%5594)
        block1():
          -> (%19)
      -> (%5607)
    block1():
      -> (%19)
  %bottleneck_output.14 : Tensor = prim::If(%5589) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5587)
    block1():
      %concated_features.8 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5610 : __torch__.torch.nn.modules.conv.___torch_mangle_140.Conv2d = prim::GetAttr[name="conv1"](%4991)
      %5611 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_139.BatchNorm2d = prim::GetAttr[name="norm1"](%4991)
      %5612 : int = aten::dim(%concated_features.8) # torch/nn/modules/batchnorm.py:276:11
      %5613 : bool = aten::ne(%5612, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5613) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5614 : bool = prim::GetAttr[name="training"](%5611)
       = prim::If(%5614) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5615 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5611)
          %5616 : Tensor = aten::add(%5615, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5611, %5616)
          -> ()
        block1():
          -> ()
      %5617 : bool = prim::GetAttr[name="training"](%5611)
      %5618 : Tensor = prim::GetAttr[name="running_mean"](%5611)
      %5619 : Tensor = prim::GetAttr[name="running_var"](%5611)
      %5620 : Tensor = prim::GetAttr[name="weight"](%5611)
      %5621 : Tensor = prim::GetAttr[name="bias"](%5611)
       = prim::If(%5617) # torch/nn/functional.py:2011:4
        block0():
          %5622 : int[] = aten::size(%concated_features.8) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%5622, %24) # torch/nn/functional.py:1991:17
          %5624 : int = aten::len(%5622) # torch/nn/functional.py:1992:19
          %5625 : int = aten::sub(%5624, %26) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%5625, %25, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %5629 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
              %5630 : int = aten::__getitem__(%5622, %5629) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %5630) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.59)
          %5632 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5632) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5633 : Tensor = aten::batch_norm(%concated_features.8, %5620, %5621, %5618, %5619, %5617, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.15 : Tensor = aten::relu_(%5633) # torch/nn/functional.py:1117:17
      %5635 : Tensor = prim::GetAttr[name="weight"](%5610)
      %5636 : Tensor? = prim::GetAttr[name="bias"](%5610)
      %5637 : int[] = prim::ListConstruct(%27, %27)
      %5638 : int[] = prim::ListConstruct(%24, %24)
      %5639 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.15 : Tensor = aten::conv2d(%result.15, %5635, %5636, %5637, %5638, %5639, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.15)
  %5641 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4991)
  %5642 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4991)
  %5643 : int = aten::dim(%bottleneck_output.14) # torch/nn/modules/batchnorm.py:276:11
  %5644 : bool = aten::ne(%5643, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5644) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5645 : bool = prim::GetAttr[name="training"](%5642)
   = prim::If(%5645) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5646 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5642)
      %5647 : Tensor = aten::add(%5646, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5642, %5647)
      -> ()
    block1():
      -> ()
  %5648 : bool = prim::GetAttr[name="training"](%5642)
  %5649 : Tensor = prim::GetAttr[name="running_mean"](%5642)
  %5650 : Tensor = prim::GetAttr[name="running_var"](%5642)
  %5651 : Tensor = prim::GetAttr[name="weight"](%5642)
  %5652 : Tensor = prim::GetAttr[name="bias"](%5642)
   = prim::If(%5648) # torch/nn/functional.py:2011:4
    block0():
      %5653 : int[] = aten::size(%bottleneck_output.14) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%5653, %24) # torch/nn/functional.py:1991:17
      %5655 : int = aten::len(%5653) # torch/nn/functional.py:1992:19
      %5656 : int = aten::sub(%5655, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%5656, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %5660 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %5661 : int = aten::__getitem__(%5653, %5660) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %5661) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %5663 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5663) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5664 : Tensor = aten::batch_norm(%bottleneck_output.14, %5651, %5652, %5649, %5650, %5648, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.16 : Tensor = aten::relu_(%5664) # torch/nn/functional.py:1117:17
  %5666 : Tensor = prim::GetAttr[name="weight"](%5641)
  %5667 : Tensor? = prim::GetAttr[name="bias"](%5641)
  %5668 : int[] = prim::ListConstruct(%27, %27)
  %5669 : int[] = prim::ListConstruct(%27, %27)
  %5670 : int[] = prim::ListConstruct(%27, %27)
  %new_features.16 : Tensor = aten::conv2d(%result.16, %5666, %5667, %5668, %5669, %5670, %27) # torch/nn/modules/conv.py:415:15
  %5672 : float = prim::GetAttr[name="drop_rate"](%4991)
  %5673 : bool = aten::gt(%5672, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.13 : Tensor = prim::If(%5673) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5675 : float = prim::GetAttr[name="drop_rate"](%4991)
      %5676 : bool = prim::GetAttr[name="training"](%4991)
      %5677 : bool = aten::lt(%5675, %16) # torch/nn/functional.py:968:7
      %5678 : bool = prim::If(%5677) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5679 : bool = aten::gt(%5675, %17) # torch/nn/functional.py:968:17
          -> (%5679)
       = prim::If(%5678) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5680 : Tensor = aten::dropout(%new_features.16, %5675, %5676) # torch/nn/functional.py:973:17
      -> (%5680)
    block1():
      -> (%new_features.16)
  %5681 : Tensor[] = aten::append(%features.1, %new_features.13) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5682 : Tensor = prim::Uninitialized()
  %5683 : bool = prim::GetAttr[name="memory_efficient"](%4992)
  %5684 : bool = prim::If(%5683) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5685 : bool = prim::Uninitialized()
      %5686 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5687 : bool = aten::gt(%5686, %24)
      %5688 : bool, %5689 : bool, %5690 : int = prim::Loop(%18, %5687, %19, %5685, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5691 : int, %5692 : bool, %5693 : bool, %5694 : int):
          %tensor.9 : Tensor = aten::__getitem__(%features.1, %5694) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5696 : bool = prim::requires_grad(%tensor.9)
          %5697 : bool, %5698 : bool = prim::If(%5696) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5685)
          %5699 : int = aten::add(%5694, %27)
          %5700 : bool = aten::lt(%5699, %5686)
          %5701 : bool = aten::__and__(%5700, %5697)
          -> (%5701, %5696, %5698, %5699)
      %5702 : bool = prim::If(%5688)
        block0():
          -> (%5689)
        block1():
          -> (%19)
      -> (%5702)
    block1():
      -> (%19)
  %bottleneck_output.16 : Tensor = prim::If(%5684) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5682)
    block1():
      %concated_features.9 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5705 : __torch__.torch.nn.modules.conv.___torch_mangle_143.Conv2d = prim::GetAttr[name="conv1"](%4992)
      %5706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%4992)
      %5707 : int = aten::dim(%concated_features.9) # torch/nn/modules/batchnorm.py:276:11
      %5708 : bool = aten::ne(%5707, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5708) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5709 : bool = prim::GetAttr[name="training"](%5706)
       = prim::If(%5709) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5710 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5706)
          %5711 : Tensor = aten::add(%5710, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5706, %5711)
          -> ()
        block1():
          -> ()
      %5712 : bool = prim::GetAttr[name="training"](%5706)
      %5713 : Tensor = prim::GetAttr[name="running_mean"](%5706)
      %5714 : Tensor = prim::GetAttr[name="running_var"](%5706)
      %5715 : Tensor = prim::GetAttr[name="weight"](%5706)
      %5716 : Tensor = prim::GetAttr[name="bias"](%5706)
       = prim::If(%5712) # torch/nn/functional.py:2011:4
        block0():
          %5717 : int[] = aten::size(%concated_features.9) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%5717, %24) # torch/nn/functional.py:1991:17
          %5719 : int = aten::len(%5717) # torch/nn/functional.py:1992:19
          %5720 : int = aten::sub(%5719, %26) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%5720, %25, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %5724 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
              %5725 : int = aten::__getitem__(%5717, %5724) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %5725) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.67)
          %5727 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5727) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5728 : Tensor = aten::batch_norm(%concated_features.9, %5715, %5716, %5713, %5714, %5712, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.17 : Tensor = aten::relu_(%5728) # torch/nn/functional.py:1117:17
      %5730 : Tensor = prim::GetAttr[name="weight"](%5705)
      %5731 : Tensor? = prim::GetAttr[name="bias"](%5705)
      %5732 : int[] = prim::ListConstruct(%27, %27)
      %5733 : int[] = prim::ListConstruct(%24, %24)
      %5734 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.17 : Tensor = aten::conv2d(%result.17, %5730, %5731, %5732, %5733, %5734, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.17)
  %5736 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4992)
  %5737 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4992)
  %5738 : int = aten::dim(%bottleneck_output.16) # torch/nn/modules/batchnorm.py:276:11
  %5739 : bool = aten::ne(%5738, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5739) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5740 : bool = prim::GetAttr[name="training"](%5737)
   = prim::If(%5740) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5741 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5737)
      %5742 : Tensor = aten::add(%5741, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5737, %5742)
      -> ()
    block1():
      -> ()
  %5743 : bool = prim::GetAttr[name="training"](%5737)
  %5744 : Tensor = prim::GetAttr[name="running_mean"](%5737)
  %5745 : Tensor = prim::GetAttr[name="running_var"](%5737)
  %5746 : Tensor = prim::GetAttr[name="weight"](%5737)
  %5747 : Tensor = prim::GetAttr[name="bias"](%5737)
   = prim::If(%5743) # torch/nn/functional.py:2011:4
    block0():
      %5748 : int[] = aten::size(%bottleneck_output.16) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%5748, %24) # torch/nn/functional.py:1991:17
      %5750 : int = aten::len(%5748) # torch/nn/functional.py:1992:19
      %5751 : int = aten::sub(%5750, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%5751, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %5755 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %5756 : int = aten::__getitem__(%5748, %5755) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %5756) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %5758 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5758) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5759 : Tensor = aten::batch_norm(%bottleneck_output.16, %5746, %5747, %5744, %5745, %5743, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.18 : Tensor = aten::relu_(%5759) # torch/nn/functional.py:1117:17
  %5761 : Tensor = prim::GetAttr[name="weight"](%5736)
  %5762 : Tensor? = prim::GetAttr[name="bias"](%5736)
  %5763 : int[] = prim::ListConstruct(%27, %27)
  %5764 : int[] = prim::ListConstruct(%27, %27)
  %5765 : int[] = prim::ListConstruct(%27, %27)
  %new_features.18 : Tensor = aten::conv2d(%result.18, %5761, %5762, %5763, %5764, %5765, %27) # torch/nn/modules/conv.py:415:15
  %5767 : float = prim::GetAttr[name="drop_rate"](%4992)
  %5768 : bool = aten::gt(%5767, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.15 : Tensor = prim::If(%5768) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5770 : float = prim::GetAttr[name="drop_rate"](%4992)
      %5771 : bool = prim::GetAttr[name="training"](%4992)
      %5772 : bool = aten::lt(%5770, %16) # torch/nn/functional.py:968:7
      %5773 : bool = prim::If(%5772) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5774 : bool = aten::gt(%5770, %17) # torch/nn/functional.py:968:17
          -> (%5774)
       = prim::If(%5773) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5775 : Tensor = aten::dropout(%new_features.18, %5770, %5771) # torch/nn/functional.py:973:17
      -> (%5775)
    block1():
      -> (%new_features.18)
  %5776 : Tensor[] = aten::append(%features.1, %new_features.15) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5777 : Tensor = prim::Uninitialized()
  %5778 : bool = prim::GetAttr[name="memory_efficient"](%4993)
  %5779 : bool = prim::If(%5778) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5780 : bool = prim::Uninitialized()
      %5781 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5782 : bool = aten::gt(%5781, %24)
      %5783 : bool, %5784 : bool, %5785 : int = prim::Loop(%18, %5782, %19, %5780, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5786 : int, %5787 : bool, %5788 : bool, %5789 : int):
          %tensor.10 : Tensor = aten::__getitem__(%features.1, %5789) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5791 : bool = prim::requires_grad(%tensor.10)
          %5792 : bool, %5793 : bool = prim::If(%5791) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5780)
          %5794 : int = aten::add(%5789, %27)
          %5795 : bool = aten::lt(%5794, %5781)
          %5796 : bool = aten::__and__(%5795, %5792)
          -> (%5796, %5791, %5793, %5794)
      %5797 : bool = prim::If(%5783)
        block0():
          -> (%5784)
        block1():
          -> (%19)
      -> (%5797)
    block1():
      -> (%19)
  %bottleneck_output.18 : Tensor = prim::If(%5779) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5777)
    block1():
      %concated_features.10 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5800 : __torch__.torch.nn.modules.conv.___torch_mangle_146.Conv2d = prim::GetAttr[name="conv1"](%4993)
      %5801 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_145.BatchNorm2d = prim::GetAttr[name="norm1"](%4993)
      %5802 : int = aten::dim(%concated_features.10) # torch/nn/modules/batchnorm.py:276:11
      %5803 : bool = aten::ne(%5802, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5803) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5804 : bool = prim::GetAttr[name="training"](%5801)
       = prim::If(%5804) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5805 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5801)
          %5806 : Tensor = aten::add(%5805, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5801, %5806)
          -> ()
        block1():
          -> ()
      %5807 : bool = prim::GetAttr[name="training"](%5801)
      %5808 : Tensor = prim::GetAttr[name="running_mean"](%5801)
      %5809 : Tensor = prim::GetAttr[name="running_var"](%5801)
      %5810 : Tensor = prim::GetAttr[name="weight"](%5801)
      %5811 : Tensor = prim::GetAttr[name="bias"](%5801)
       = prim::If(%5807) # torch/nn/functional.py:2011:4
        block0():
          %5812 : int[] = aten::size(%concated_features.10) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%5812, %24) # torch/nn/functional.py:1991:17
          %5814 : int = aten::len(%5812) # torch/nn/functional.py:1992:19
          %5815 : int = aten::sub(%5814, %26) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%5815, %25, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %5819 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
              %5820 : int = aten::__getitem__(%5812, %5819) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %5820) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.75)
          %5822 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5822) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5823 : Tensor = aten::batch_norm(%concated_features.10, %5810, %5811, %5808, %5809, %5807, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.19 : Tensor = aten::relu_(%5823) # torch/nn/functional.py:1117:17
      %5825 : Tensor = prim::GetAttr[name="weight"](%5800)
      %5826 : Tensor? = prim::GetAttr[name="bias"](%5800)
      %5827 : int[] = prim::ListConstruct(%27, %27)
      %5828 : int[] = prim::ListConstruct(%24, %24)
      %5829 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.19 : Tensor = aten::conv2d(%result.19, %5825, %5826, %5827, %5828, %5829, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.19)
  %5831 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4993)
  %5832 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4993)
  %5833 : int = aten::dim(%bottleneck_output.18) # torch/nn/modules/batchnorm.py:276:11
  %5834 : bool = aten::ne(%5833, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5834) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5835 : bool = prim::GetAttr[name="training"](%5832)
   = prim::If(%5835) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5836 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5832)
      %5837 : Tensor = aten::add(%5836, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5832, %5837)
      -> ()
    block1():
      -> ()
  %5838 : bool = prim::GetAttr[name="training"](%5832)
  %5839 : Tensor = prim::GetAttr[name="running_mean"](%5832)
  %5840 : Tensor = prim::GetAttr[name="running_var"](%5832)
  %5841 : Tensor = prim::GetAttr[name="weight"](%5832)
  %5842 : Tensor = prim::GetAttr[name="bias"](%5832)
   = prim::If(%5838) # torch/nn/functional.py:2011:4
    block0():
      %5843 : int[] = aten::size(%bottleneck_output.18) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%5843, %24) # torch/nn/functional.py:1991:17
      %5845 : int = aten::len(%5843) # torch/nn/functional.py:1992:19
      %5846 : int = aten::sub(%5845, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%5846, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %5850 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %5851 : int = aten::__getitem__(%5843, %5850) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %5851) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %5853 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5853) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5854 : Tensor = aten::batch_norm(%bottleneck_output.18, %5841, %5842, %5839, %5840, %5838, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.20 : Tensor = aten::relu_(%5854) # torch/nn/functional.py:1117:17
  %5856 : Tensor = prim::GetAttr[name="weight"](%5831)
  %5857 : Tensor? = prim::GetAttr[name="bias"](%5831)
  %5858 : int[] = prim::ListConstruct(%27, %27)
  %5859 : int[] = prim::ListConstruct(%27, %27)
  %5860 : int[] = prim::ListConstruct(%27, %27)
  %new_features.20 : Tensor = aten::conv2d(%result.20, %5856, %5857, %5858, %5859, %5860, %27) # torch/nn/modules/conv.py:415:15
  %5862 : float = prim::GetAttr[name="drop_rate"](%4993)
  %5863 : bool = aten::gt(%5862, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.17 : Tensor = prim::If(%5863) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5865 : float = prim::GetAttr[name="drop_rate"](%4993)
      %5866 : bool = prim::GetAttr[name="training"](%4993)
      %5867 : bool = aten::lt(%5865, %16) # torch/nn/functional.py:968:7
      %5868 : bool = prim::If(%5867) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5869 : bool = aten::gt(%5865, %17) # torch/nn/functional.py:968:17
          -> (%5869)
       = prim::If(%5868) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5870 : Tensor = aten::dropout(%new_features.20, %5865, %5866) # torch/nn/functional.py:973:17
      -> (%5870)
    block1():
      -> (%new_features.20)
  %5871 : Tensor[] = aten::append(%features.1, %new_features.17) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5872 : Tensor = prim::Uninitialized()
  %5873 : bool = prim::GetAttr[name="memory_efficient"](%4994)
  %5874 : bool = prim::If(%5873) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5875 : bool = prim::Uninitialized()
      %5876 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5877 : bool = aten::gt(%5876, %24)
      %5878 : bool, %5879 : bool, %5880 : int = prim::Loop(%18, %5877, %19, %5875, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5881 : int, %5882 : bool, %5883 : bool, %5884 : int):
          %tensor.11 : Tensor = aten::__getitem__(%features.1, %5884) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5886 : bool = prim::requires_grad(%tensor.11)
          %5887 : bool, %5888 : bool = prim::If(%5886) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5875)
          %5889 : int = aten::add(%5884, %27)
          %5890 : bool = aten::lt(%5889, %5876)
          %5891 : bool = aten::__and__(%5890, %5887)
          -> (%5891, %5886, %5888, %5889)
      %5892 : bool = prim::If(%5878)
        block0():
          -> (%5879)
        block1():
          -> (%19)
      -> (%5892)
    block1():
      -> (%19)
  %bottleneck_output.20 : Tensor = prim::If(%5874) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5872)
    block1():
      %concated_features.11 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5895 : __torch__.torch.nn.modules.conv.___torch_mangle_149.Conv2d = prim::GetAttr[name="conv1"](%4994)
      %5896 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_148.BatchNorm2d = prim::GetAttr[name="norm1"](%4994)
      %5897 : int = aten::dim(%concated_features.11) # torch/nn/modules/batchnorm.py:276:11
      %5898 : bool = aten::ne(%5897, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5898) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5899 : bool = prim::GetAttr[name="training"](%5896)
       = prim::If(%5899) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5900 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5896)
          %5901 : Tensor = aten::add(%5900, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5896, %5901)
          -> ()
        block1():
          -> ()
      %5902 : bool = prim::GetAttr[name="training"](%5896)
      %5903 : Tensor = prim::GetAttr[name="running_mean"](%5896)
      %5904 : Tensor = prim::GetAttr[name="running_var"](%5896)
      %5905 : Tensor = prim::GetAttr[name="weight"](%5896)
      %5906 : Tensor = prim::GetAttr[name="bias"](%5896)
       = prim::If(%5902) # torch/nn/functional.py:2011:4
        block0():
          %5907 : int[] = aten::size(%concated_features.11) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%5907, %24) # torch/nn/functional.py:1991:17
          %5909 : int = aten::len(%5907) # torch/nn/functional.py:1992:19
          %5910 : int = aten::sub(%5909, %26) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%5910, %25, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %5914 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
              %5915 : int = aten::__getitem__(%5907, %5914) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %5915) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.83)
          %5917 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5917) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5918 : Tensor = aten::batch_norm(%concated_features.11, %5905, %5906, %5903, %5904, %5902, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.21 : Tensor = aten::relu_(%5918) # torch/nn/functional.py:1117:17
      %5920 : Tensor = prim::GetAttr[name="weight"](%5895)
      %5921 : Tensor? = prim::GetAttr[name="bias"](%5895)
      %5922 : int[] = prim::ListConstruct(%27, %27)
      %5923 : int[] = prim::ListConstruct(%24, %24)
      %5924 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.21 : Tensor = aten::conv2d(%result.21, %5920, %5921, %5922, %5923, %5924, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.21)
  %5926 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4994)
  %5927 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4994)
  %5928 : int = aten::dim(%bottleneck_output.20) # torch/nn/modules/batchnorm.py:276:11
  %5929 : bool = aten::ne(%5928, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5929) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5930 : bool = prim::GetAttr[name="training"](%5927)
   = prim::If(%5930) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5931 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5927)
      %5932 : Tensor = aten::add(%5931, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5927, %5932)
      -> ()
    block1():
      -> ()
  %5933 : bool = prim::GetAttr[name="training"](%5927)
  %5934 : Tensor = prim::GetAttr[name="running_mean"](%5927)
  %5935 : Tensor = prim::GetAttr[name="running_var"](%5927)
  %5936 : Tensor = prim::GetAttr[name="weight"](%5927)
  %5937 : Tensor = prim::GetAttr[name="bias"](%5927)
   = prim::If(%5933) # torch/nn/functional.py:2011:4
    block0():
      %5938 : int[] = aten::size(%bottleneck_output.20) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%5938, %24) # torch/nn/functional.py:1991:17
      %5940 : int = aten::len(%5938) # torch/nn/functional.py:1992:19
      %5941 : int = aten::sub(%5940, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%5941, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %5945 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %5946 : int = aten::__getitem__(%5938, %5945) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %5946) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %5948 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5948) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5949 : Tensor = aten::batch_norm(%bottleneck_output.20, %5936, %5937, %5934, %5935, %5933, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.22 : Tensor = aten::relu_(%5949) # torch/nn/functional.py:1117:17
  %5951 : Tensor = prim::GetAttr[name="weight"](%5926)
  %5952 : Tensor? = prim::GetAttr[name="bias"](%5926)
  %5953 : int[] = prim::ListConstruct(%27, %27)
  %5954 : int[] = prim::ListConstruct(%27, %27)
  %5955 : int[] = prim::ListConstruct(%27, %27)
  %new_features.22 : Tensor = aten::conv2d(%result.22, %5951, %5952, %5953, %5954, %5955, %27) # torch/nn/modules/conv.py:415:15
  %5957 : float = prim::GetAttr[name="drop_rate"](%4994)
  %5958 : bool = aten::gt(%5957, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.19 : Tensor = prim::If(%5958) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5960 : float = prim::GetAttr[name="drop_rate"](%4994)
      %5961 : bool = prim::GetAttr[name="training"](%4994)
      %5962 : bool = aten::lt(%5960, %16) # torch/nn/functional.py:968:7
      %5963 : bool = prim::If(%5962) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5964 : bool = aten::gt(%5960, %17) # torch/nn/functional.py:968:17
          -> (%5964)
       = prim::If(%5963) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5965 : Tensor = aten::dropout(%new_features.22, %5960, %5961) # torch/nn/functional.py:973:17
      -> (%5965)
    block1():
      -> (%new_features.22)
  %5966 : Tensor[] = aten::append(%features.1, %new_features.19) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5967 : Tensor = prim::Uninitialized()
  %5968 : bool = prim::GetAttr[name="memory_efficient"](%4995)
  %5969 : bool = prim::If(%5968) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5970 : bool = prim::Uninitialized()
      %5971 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5972 : bool = aten::gt(%5971, %24)
      %5973 : bool, %5974 : bool, %5975 : int = prim::Loop(%18, %5972, %19, %5970, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5976 : int, %5977 : bool, %5978 : bool, %5979 : int):
          %tensor.12 : Tensor = aten::__getitem__(%features.1, %5979) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5981 : bool = prim::requires_grad(%tensor.12)
          %5982 : bool, %5983 : bool = prim::If(%5981) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5970)
          %5984 : int = aten::add(%5979, %27)
          %5985 : bool = aten::lt(%5984, %5971)
          %5986 : bool = aten::__and__(%5985, %5982)
          -> (%5986, %5981, %5983, %5984)
      %5987 : bool = prim::If(%5973)
        block0():
          -> (%5974)
        block1():
          -> (%19)
      -> (%5987)
    block1():
      -> (%19)
  %bottleneck_output.22 : Tensor = prim::If(%5969) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5967)
    block1():
      %concated_features.12 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5990 : __torch__.torch.nn.modules.conv.___torch_mangle_152.Conv2d = prim::GetAttr[name="conv1"](%4995)
      %5991 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%4995)
      %5992 : int = aten::dim(%concated_features.12) # torch/nn/modules/batchnorm.py:276:11
      %5993 : bool = aten::ne(%5992, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5993) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5994 : bool = prim::GetAttr[name="training"](%5991)
       = prim::If(%5994) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5995 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5991)
          %5996 : Tensor = aten::add(%5995, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5991, %5996)
          -> ()
        block1():
          -> ()
      %5997 : bool = prim::GetAttr[name="training"](%5991)
      %5998 : Tensor = prim::GetAttr[name="running_mean"](%5991)
      %5999 : Tensor = prim::GetAttr[name="running_var"](%5991)
      %6000 : Tensor = prim::GetAttr[name="weight"](%5991)
      %6001 : Tensor = prim::GetAttr[name="bias"](%5991)
       = prim::If(%5997) # torch/nn/functional.py:2011:4
        block0():
          %6002 : int[] = aten::size(%concated_features.12) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%6002, %24) # torch/nn/functional.py:1991:17
          %6004 : int = aten::len(%6002) # torch/nn/functional.py:1992:19
          %6005 : int = aten::sub(%6004, %26) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%6005, %25, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %6009 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
              %6010 : int = aten::__getitem__(%6002, %6009) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %6010) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.91)
          %6012 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6012) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6013 : Tensor = aten::batch_norm(%concated_features.12, %6000, %6001, %5998, %5999, %5997, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.23 : Tensor = aten::relu_(%6013) # torch/nn/functional.py:1117:17
      %6015 : Tensor = prim::GetAttr[name="weight"](%5990)
      %6016 : Tensor? = prim::GetAttr[name="bias"](%5990)
      %6017 : int[] = prim::ListConstruct(%27, %27)
      %6018 : int[] = prim::ListConstruct(%24, %24)
      %6019 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.23 : Tensor = aten::conv2d(%result.23, %6015, %6016, %6017, %6018, %6019, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.23)
  %6021 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4995)
  %6022 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4995)
  %6023 : int = aten::dim(%bottleneck_output.22) # torch/nn/modules/batchnorm.py:276:11
  %6024 : bool = aten::ne(%6023, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6024) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6025 : bool = prim::GetAttr[name="training"](%6022)
   = prim::If(%6025) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6026 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6022)
      %6027 : Tensor = aten::add(%6026, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6022, %6027)
      -> ()
    block1():
      -> ()
  %6028 : bool = prim::GetAttr[name="training"](%6022)
  %6029 : Tensor = prim::GetAttr[name="running_mean"](%6022)
  %6030 : Tensor = prim::GetAttr[name="running_var"](%6022)
  %6031 : Tensor = prim::GetAttr[name="weight"](%6022)
  %6032 : Tensor = prim::GetAttr[name="bias"](%6022)
   = prim::If(%6028) # torch/nn/functional.py:2011:4
    block0():
      %6033 : int[] = aten::size(%bottleneck_output.22) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%6033, %24) # torch/nn/functional.py:1991:17
      %6035 : int = aten::len(%6033) # torch/nn/functional.py:1992:19
      %6036 : int = aten::sub(%6035, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%6036, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %6040 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %6041 : int = aten::__getitem__(%6033, %6040) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %6041) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %6043 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6043) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6044 : Tensor = aten::batch_norm(%bottleneck_output.22, %6031, %6032, %6029, %6030, %6028, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.24 : Tensor = aten::relu_(%6044) # torch/nn/functional.py:1117:17
  %6046 : Tensor = prim::GetAttr[name="weight"](%6021)
  %6047 : Tensor? = prim::GetAttr[name="bias"](%6021)
  %6048 : int[] = prim::ListConstruct(%27, %27)
  %6049 : int[] = prim::ListConstruct(%27, %27)
  %6050 : int[] = prim::ListConstruct(%27, %27)
  %new_features.24 : Tensor = aten::conv2d(%result.24, %6046, %6047, %6048, %6049, %6050, %27) # torch/nn/modules/conv.py:415:15
  %6052 : float = prim::GetAttr[name="drop_rate"](%4995)
  %6053 : bool = aten::gt(%6052, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.21 : Tensor = prim::If(%6053) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6055 : float = prim::GetAttr[name="drop_rate"](%4995)
      %6056 : bool = prim::GetAttr[name="training"](%4995)
      %6057 : bool = aten::lt(%6055, %16) # torch/nn/functional.py:968:7
      %6058 : bool = prim::If(%6057) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6059 : bool = aten::gt(%6055, %17) # torch/nn/functional.py:968:17
          -> (%6059)
       = prim::If(%6058) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6060 : Tensor = aten::dropout(%new_features.24, %6055, %6056) # torch/nn/functional.py:973:17
      -> (%6060)
    block1():
      -> (%new_features.24)
  %6061 : Tensor[] = aten::append(%features.1, %new_features.21) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6062 : Tensor = prim::Uninitialized()
  %6063 : bool = prim::GetAttr[name="memory_efficient"](%4996)
  %6064 : bool = prim::If(%6063) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6065 : bool = prim::Uninitialized()
      %6066 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6067 : bool = aten::gt(%6066, %24)
      %6068 : bool, %6069 : bool, %6070 : int = prim::Loop(%18, %6067, %19, %6065, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6071 : int, %6072 : bool, %6073 : bool, %6074 : int):
          %tensor.13 : Tensor = aten::__getitem__(%features.1, %6074) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6076 : bool = prim::requires_grad(%tensor.13)
          %6077 : bool, %6078 : bool = prim::If(%6076) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6065)
          %6079 : int = aten::add(%6074, %27)
          %6080 : bool = aten::lt(%6079, %6066)
          %6081 : bool = aten::__and__(%6080, %6077)
          -> (%6081, %6076, %6078, %6079)
      %6082 : bool = prim::If(%6068)
        block0():
          -> (%6069)
        block1():
          -> (%19)
      -> (%6082)
    block1():
      -> (%19)
  %bottleneck_output.24 : Tensor = prim::If(%6064) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6062)
    block1():
      %concated_features.13 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6085 : __torch__.torch.nn.modules.conv.___torch_mangle_155.Conv2d = prim::GetAttr[name="conv1"](%4996)
      %6086 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_154.BatchNorm2d = prim::GetAttr[name="norm1"](%4996)
      %6087 : int = aten::dim(%concated_features.13) # torch/nn/modules/batchnorm.py:276:11
      %6088 : bool = aten::ne(%6087, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6088) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6089 : bool = prim::GetAttr[name="training"](%6086)
       = prim::If(%6089) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6090 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6086)
          %6091 : Tensor = aten::add(%6090, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6086, %6091)
          -> ()
        block1():
          -> ()
      %6092 : bool = prim::GetAttr[name="training"](%6086)
      %6093 : Tensor = prim::GetAttr[name="running_mean"](%6086)
      %6094 : Tensor = prim::GetAttr[name="running_var"](%6086)
      %6095 : Tensor = prim::GetAttr[name="weight"](%6086)
      %6096 : Tensor = prim::GetAttr[name="bias"](%6086)
       = prim::If(%6092) # torch/nn/functional.py:2011:4
        block0():
          %6097 : int[] = aten::size(%concated_features.13) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%6097, %24) # torch/nn/functional.py:1991:17
          %6099 : int = aten::len(%6097) # torch/nn/functional.py:1992:19
          %6100 : int = aten::sub(%6099, %26) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%6100, %25, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %6104 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
              %6105 : int = aten::__getitem__(%6097, %6104) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %6105) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.99)
          %6107 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6107) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6108 : Tensor = aten::batch_norm(%concated_features.13, %6095, %6096, %6093, %6094, %6092, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.25 : Tensor = aten::relu_(%6108) # torch/nn/functional.py:1117:17
      %6110 : Tensor = prim::GetAttr[name="weight"](%6085)
      %6111 : Tensor? = prim::GetAttr[name="bias"](%6085)
      %6112 : int[] = prim::ListConstruct(%27, %27)
      %6113 : int[] = prim::ListConstruct(%24, %24)
      %6114 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.25 : Tensor = aten::conv2d(%result.25, %6110, %6111, %6112, %6113, %6114, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.25)
  %6116 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4996)
  %6117 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4996)
  %6118 : int = aten::dim(%bottleneck_output.24) # torch/nn/modules/batchnorm.py:276:11
  %6119 : bool = aten::ne(%6118, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6119) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6120 : bool = prim::GetAttr[name="training"](%6117)
   = prim::If(%6120) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6121 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6117)
      %6122 : Tensor = aten::add(%6121, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6117, %6122)
      -> ()
    block1():
      -> ()
  %6123 : bool = prim::GetAttr[name="training"](%6117)
  %6124 : Tensor = prim::GetAttr[name="running_mean"](%6117)
  %6125 : Tensor = prim::GetAttr[name="running_var"](%6117)
  %6126 : Tensor = prim::GetAttr[name="weight"](%6117)
  %6127 : Tensor = prim::GetAttr[name="bias"](%6117)
   = prim::If(%6123) # torch/nn/functional.py:2011:4
    block0():
      %6128 : int[] = aten::size(%bottleneck_output.24) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%6128, %24) # torch/nn/functional.py:1991:17
      %6130 : int = aten::len(%6128) # torch/nn/functional.py:1992:19
      %6131 : int = aten::sub(%6130, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%6131, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %6135 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %6136 : int = aten::__getitem__(%6128, %6135) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %6136) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %6138 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6138) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6139 : Tensor = aten::batch_norm(%bottleneck_output.24, %6126, %6127, %6124, %6125, %6123, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.26 : Tensor = aten::relu_(%6139) # torch/nn/functional.py:1117:17
  %6141 : Tensor = prim::GetAttr[name="weight"](%6116)
  %6142 : Tensor? = prim::GetAttr[name="bias"](%6116)
  %6143 : int[] = prim::ListConstruct(%27, %27)
  %6144 : int[] = prim::ListConstruct(%27, %27)
  %6145 : int[] = prim::ListConstruct(%27, %27)
  %new_features.26 : Tensor = aten::conv2d(%result.26, %6141, %6142, %6143, %6144, %6145, %27) # torch/nn/modules/conv.py:415:15
  %6147 : float = prim::GetAttr[name="drop_rate"](%4996)
  %6148 : bool = aten::gt(%6147, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.23 : Tensor = prim::If(%6148) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6150 : float = prim::GetAttr[name="drop_rate"](%4996)
      %6151 : bool = prim::GetAttr[name="training"](%4996)
      %6152 : bool = aten::lt(%6150, %16) # torch/nn/functional.py:968:7
      %6153 : bool = prim::If(%6152) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6154 : bool = aten::gt(%6150, %17) # torch/nn/functional.py:968:17
          -> (%6154)
       = prim::If(%6153) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6155 : Tensor = aten::dropout(%new_features.26, %6150, %6151) # torch/nn/functional.py:973:17
      -> (%6155)
    block1():
      -> (%new_features.26)
  %6156 : Tensor[] = aten::append(%features.1, %new_features.23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6157 : Tensor = prim::Uninitialized()
  %6158 : bool = prim::GetAttr[name="memory_efficient"](%4997)
  %6159 : bool = prim::If(%6158) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6160 : bool = prim::Uninitialized()
      %6161 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6162 : bool = aten::gt(%6161, %24)
      %6163 : bool, %6164 : bool, %6165 : int = prim::Loop(%18, %6162, %19, %6160, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6166 : int, %6167 : bool, %6168 : bool, %6169 : int):
          %tensor.14 : Tensor = aten::__getitem__(%features.1, %6169) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6171 : bool = prim::requires_grad(%tensor.14)
          %6172 : bool, %6173 : bool = prim::If(%6171) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6160)
          %6174 : int = aten::add(%6169, %27)
          %6175 : bool = aten::lt(%6174, %6161)
          %6176 : bool = aten::__and__(%6175, %6172)
          -> (%6176, %6171, %6173, %6174)
      %6177 : bool = prim::If(%6163)
        block0():
          -> (%6164)
        block1():
          -> (%19)
      -> (%6177)
    block1():
      -> (%19)
  %bottleneck_output.26 : Tensor = prim::If(%6159) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6157)
    block1():
      %concated_features.14 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6180 : __torch__.torch.nn.modules.conv.___torch_mangle_299.Conv2d = prim::GetAttr[name="conv1"](%4997)
      %6181 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="norm1"](%4997)
      %6182 : int = aten::dim(%concated_features.14) # torch/nn/modules/batchnorm.py:276:11
      %6183 : bool = aten::ne(%6182, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6183) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6184 : bool = prim::GetAttr[name="training"](%6181)
       = prim::If(%6184) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6185 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6181)
          %6186 : Tensor = aten::add(%6185, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6181, %6186)
          -> ()
        block1():
          -> ()
      %6187 : bool = prim::GetAttr[name="training"](%6181)
      %6188 : Tensor = prim::GetAttr[name="running_mean"](%6181)
      %6189 : Tensor = prim::GetAttr[name="running_var"](%6181)
      %6190 : Tensor = prim::GetAttr[name="weight"](%6181)
      %6191 : Tensor = prim::GetAttr[name="bias"](%6181)
       = prim::If(%6187) # torch/nn/functional.py:2011:4
        block0():
          %6192 : int[] = aten::size(%concated_features.14) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%6192, %24) # torch/nn/functional.py:1991:17
          %6194 : int = aten::len(%6192) # torch/nn/functional.py:1992:19
          %6195 : int = aten::sub(%6194, %26) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%6195, %25, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %6199 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
              %6200 : int = aten::__getitem__(%6192, %6199) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %6200) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.107)
          %6202 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6202) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6203 : Tensor = aten::batch_norm(%concated_features.14, %6190, %6191, %6188, %6189, %6187, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.27 : Tensor = aten::relu_(%6203) # torch/nn/functional.py:1117:17
      %6205 : Tensor = prim::GetAttr[name="weight"](%6180)
      %6206 : Tensor? = prim::GetAttr[name="bias"](%6180)
      %6207 : int[] = prim::ListConstruct(%27, %27)
      %6208 : int[] = prim::ListConstruct(%24, %24)
      %6209 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.27 : Tensor = aten::conv2d(%result.27, %6205, %6206, %6207, %6208, %6209, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.27)
  %6211 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4997)
  %6212 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4997)
  %6213 : int = aten::dim(%bottleneck_output.26) # torch/nn/modules/batchnorm.py:276:11
  %6214 : bool = aten::ne(%6213, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6214) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6215 : bool = prim::GetAttr[name="training"](%6212)
   = prim::If(%6215) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6216 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6212)
      %6217 : Tensor = aten::add(%6216, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6212, %6217)
      -> ()
    block1():
      -> ()
  %6218 : bool = prim::GetAttr[name="training"](%6212)
  %6219 : Tensor = prim::GetAttr[name="running_mean"](%6212)
  %6220 : Tensor = prim::GetAttr[name="running_var"](%6212)
  %6221 : Tensor = prim::GetAttr[name="weight"](%6212)
  %6222 : Tensor = prim::GetAttr[name="bias"](%6212)
   = prim::If(%6218) # torch/nn/functional.py:2011:4
    block0():
      %6223 : int[] = aten::size(%bottleneck_output.26) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%6223, %24) # torch/nn/functional.py:1991:17
      %6225 : int = aten::len(%6223) # torch/nn/functional.py:1992:19
      %6226 : int = aten::sub(%6225, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%6226, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %6230 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %6231 : int = aten::__getitem__(%6223, %6230) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %6231) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %6233 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6233) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6234 : Tensor = aten::batch_norm(%bottleneck_output.26, %6221, %6222, %6219, %6220, %6218, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.28 : Tensor = aten::relu_(%6234) # torch/nn/functional.py:1117:17
  %6236 : Tensor = prim::GetAttr[name="weight"](%6211)
  %6237 : Tensor? = prim::GetAttr[name="bias"](%6211)
  %6238 : int[] = prim::ListConstruct(%27, %27)
  %6239 : int[] = prim::ListConstruct(%27, %27)
  %6240 : int[] = prim::ListConstruct(%27, %27)
  %new_features.28 : Tensor = aten::conv2d(%result.28, %6236, %6237, %6238, %6239, %6240, %27) # torch/nn/modules/conv.py:415:15
  %6242 : float = prim::GetAttr[name="drop_rate"](%4997)
  %6243 : bool = aten::gt(%6242, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.25 : Tensor = prim::If(%6243) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6245 : float = prim::GetAttr[name="drop_rate"](%4997)
      %6246 : bool = prim::GetAttr[name="training"](%4997)
      %6247 : bool = aten::lt(%6245, %16) # torch/nn/functional.py:968:7
      %6248 : bool = prim::If(%6247) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6249 : bool = aten::gt(%6245, %17) # torch/nn/functional.py:968:17
          -> (%6249)
       = prim::If(%6248) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6250 : Tensor = aten::dropout(%new_features.28, %6245, %6246) # torch/nn/functional.py:973:17
      -> (%6250)
    block1():
      -> (%new_features.28)
  %6251 : Tensor[] = aten::append(%features.1, %new_features.25) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6252 : Tensor = prim::Uninitialized()
  %6253 : bool = prim::GetAttr[name="memory_efficient"](%4998)
  %6254 : bool = prim::If(%6253) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6255 : bool = prim::Uninitialized()
      %6256 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6257 : bool = aten::gt(%6256, %24)
      %6258 : bool, %6259 : bool, %6260 : int = prim::Loop(%18, %6257, %19, %6255, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6261 : int, %6262 : bool, %6263 : bool, %6264 : int):
          %tensor.15 : Tensor = aten::__getitem__(%features.1, %6264) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6266 : bool = prim::requires_grad(%tensor.15)
          %6267 : bool, %6268 : bool = prim::If(%6266) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6255)
          %6269 : int = aten::add(%6264, %27)
          %6270 : bool = aten::lt(%6269, %6256)
          %6271 : bool = aten::__and__(%6270, %6267)
          -> (%6271, %6266, %6268, %6269)
      %6272 : bool = prim::If(%6258)
        block0():
          -> (%6259)
        block1():
          -> (%19)
      -> (%6272)
    block1():
      -> (%19)
  %bottleneck_output.28 : Tensor = prim::If(%6254) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6252)
    block1():
      %concated_features.15 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6275 : __torch__.torch.nn.modules.conv.___torch_mangle_301.Conv2d = prim::GetAttr[name="conv1"](%4998)
      %6276 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%4998)
      %6277 : int = aten::dim(%concated_features.15) # torch/nn/modules/batchnorm.py:276:11
      %6278 : bool = aten::ne(%6277, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6278) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6279 : bool = prim::GetAttr[name="training"](%6276)
       = prim::If(%6279) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6280 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6276)
          %6281 : Tensor = aten::add(%6280, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6276, %6281)
          -> ()
        block1():
          -> ()
      %6282 : bool = prim::GetAttr[name="training"](%6276)
      %6283 : Tensor = prim::GetAttr[name="running_mean"](%6276)
      %6284 : Tensor = prim::GetAttr[name="running_var"](%6276)
      %6285 : Tensor = prim::GetAttr[name="weight"](%6276)
      %6286 : Tensor = prim::GetAttr[name="bias"](%6276)
       = prim::If(%6282) # torch/nn/functional.py:2011:4
        block0():
          %6287 : int[] = aten::size(%concated_features.15) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%6287, %24) # torch/nn/functional.py:1991:17
          %6289 : int = aten::len(%6287) # torch/nn/functional.py:1992:19
          %6290 : int = aten::sub(%6289, %26) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%6290, %25, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %6294 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
              %6295 : int = aten::__getitem__(%6287, %6294) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %6295) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.115)
          %6297 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6297) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6298 : Tensor = aten::batch_norm(%concated_features.15, %6285, %6286, %6283, %6284, %6282, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.29 : Tensor = aten::relu_(%6298) # torch/nn/functional.py:1117:17
      %6300 : Tensor = prim::GetAttr[name="weight"](%6275)
      %6301 : Tensor? = prim::GetAttr[name="bias"](%6275)
      %6302 : int[] = prim::ListConstruct(%27, %27)
      %6303 : int[] = prim::ListConstruct(%24, %24)
      %6304 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.29 : Tensor = aten::conv2d(%result.29, %6300, %6301, %6302, %6303, %6304, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.29)
  %6306 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4998)
  %6307 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4998)
  %6308 : int = aten::dim(%bottleneck_output.28) # torch/nn/modules/batchnorm.py:276:11
  %6309 : bool = aten::ne(%6308, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6309) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6310 : bool = prim::GetAttr[name="training"](%6307)
   = prim::If(%6310) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6311 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6307)
      %6312 : Tensor = aten::add(%6311, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6307, %6312)
      -> ()
    block1():
      -> ()
  %6313 : bool = prim::GetAttr[name="training"](%6307)
  %6314 : Tensor = prim::GetAttr[name="running_mean"](%6307)
  %6315 : Tensor = prim::GetAttr[name="running_var"](%6307)
  %6316 : Tensor = prim::GetAttr[name="weight"](%6307)
  %6317 : Tensor = prim::GetAttr[name="bias"](%6307)
   = prim::If(%6313) # torch/nn/functional.py:2011:4
    block0():
      %6318 : int[] = aten::size(%bottleneck_output.28) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%6318, %24) # torch/nn/functional.py:1991:17
      %6320 : int = aten::len(%6318) # torch/nn/functional.py:1992:19
      %6321 : int = aten::sub(%6320, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%6321, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %6325 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %6326 : int = aten::__getitem__(%6318, %6325) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %6326) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %6328 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6328) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6329 : Tensor = aten::batch_norm(%bottleneck_output.28, %6316, %6317, %6314, %6315, %6313, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.30 : Tensor = aten::relu_(%6329) # torch/nn/functional.py:1117:17
  %6331 : Tensor = prim::GetAttr[name="weight"](%6306)
  %6332 : Tensor? = prim::GetAttr[name="bias"](%6306)
  %6333 : int[] = prim::ListConstruct(%27, %27)
  %6334 : int[] = prim::ListConstruct(%27, %27)
  %6335 : int[] = prim::ListConstruct(%27, %27)
  %new_features.30 : Tensor = aten::conv2d(%result.30, %6331, %6332, %6333, %6334, %6335, %27) # torch/nn/modules/conv.py:415:15
  %6337 : float = prim::GetAttr[name="drop_rate"](%4998)
  %6338 : bool = aten::gt(%6337, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.27 : Tensor = prim::If(%6338) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6340 : float = prim::GetAttr[name="drop_rate"](%4998)
      %6341 : bool = prim::GetAttr[name="training"](%4998)
      %6342 : bool = aten::lt(%6340, %16) # torch/nn/functional.py:968:7
      %6343 : bool = prim::If(%6342) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6344 : bool = aten::gt(%6340, %17) # torch/nn/functional.py:968:17
          -> (%6344)
       = prim::If(%6343) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6345 : Tensor = aten::dropout(%new_features.30, %6340, %6341) # torch/nn/functional.py:973:17
      -> (%6345)
    block1():
      -> (%new_features.30)
  %6346 : Tensor[] = aten::append(%features.1, %new_features.27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6347 : Tensor = prim::Uninitialized()
  %6348 : bool = prim::GetAttr[name="memory_efficient"](%4999)
  %6349 : bool = prim::If(%6348) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6350 : bool = prim::Uninitialized()
      %6351 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6352 : bool = aten::gt(%6351, %24)
      %6353 : bool, %6354 : bool, %6355 : int = prim::Loop(%18, %6352, %19, %6350, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6356 : int, %6357 : bool, %6358 : bool, %6359 : int):
          %tensor.16 : Tensor = aten::__getitem__(%features.1, %6359) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6361 : bool = prim::requires_grad(%tensor.16)
          %6362 : bool, %6363 : bool = prim::If(%6361) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6350)
          %6364 : int = aten::add(%6359, %27)
          %6365 : bool = aten::lt(%6364, %6351)
          %6366 : bool = aten::__and__(%6365, %6362)
          -> (%6366, %6361, %6363, %6364)
      %6367 : bool = prim::If(%6353)
        block0():
          -> (%6354)
        block1():
          -> (%19)
      -> (%6367)
    block1():
      -> (%19)
  %bottleneck_output.30 : Tensor = prim::If(%6349) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6347)
    block1():
      %concated_features.16 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6370 : __torch__.torch.nn.modules.conv.___torch_mangle_304.Conv2d = prim::GetAttr[name="conv1"](%4999)
      %6371 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_303.BatchNorm2d = prim::GetAttr[name="norm1"](%4999)
      %6372 : int = aten::dim(%concated_features.16) # torch/nn/modules/batchnorm.py:276:11
      %6373 : bool = aten::ne(%6372, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6373) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6374 : bool = prim::GetAttr[name="training"](%6371)
       = prim::If(%6374) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6375 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6371)
          %6376 : Tensor = aten::add(%6375, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6371, %6376)
          -> ()
        block1():
          -> ()
      %6377 : bool = prim::GetAttr[name="training"](%6371)
      %6378 : Tensor = prim::GetAttr[name="running_mean"](%6371)
      %6379 : Tensor = prim::GetAttr[name="running_var"](%6371)
      %6380 : Tensor = prim::GetAttr[name="weight"](%6371)
      %6381 : Tensor = prim::GetAttr[name="bias"](%6371)
       = prim::If(%6377) # torch/nn/functional.py:2011:4
        block0():
          %6382 : int[] = aten::size(%concated_features.16) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%6382, %24) # torch/nn/functional.py:1991:17
          %6384 : int = aten::len(%6382) # torch/nn/functional.py:1992:19
          %6385 : int = aten::sub(%6384, %26) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%6385, %25, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %6389 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
              %6390 : int = aten::__getitem__(%6382, %6389) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %6390) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.123)
          %6392 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6392) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6393 : Tensor = aten::batch_norm(%concated_features.16, %6380, %6381, %6378, %6379, %6377, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.31 : Tensor = aten::relu_(%6393) # torch/nn/functional.py:1117:17
      %6395 : Tensor = prim::GetAttr[name="weight"](%6370)
      %6396 : Tensor? = prim::GetAttr[name="bias"](%6370)
      %6397 : int[] = prim::ListConstruct(%27, %27)
      %6398 : int[] = prim::ListConstruct(%24, %24)
      %6399 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.31 : Tensor = aten::conv2d(%result.31, %6395, %6396, %6397, %6398, %6399, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.31)
  %6401 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%4999)
  %6402 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%4999)
  %6403 : int = aten::dim(%bottleneck_output.30) # torch/nn/modules/batchnorm.py:276:11
  %6404 : bool = aten::ne(%6403, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6404) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6405 : bool = prim::GetAttr[name="training"](%6402)
   = prim::If(%6405) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6406 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6402)
      %6407 : Tensor = aten::add(%6406, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6402, %6407)
      -> ()
    block1():
      -> ()
  %6408 : bool = prim::GetAttr[name="training"](%6402)
  %6409 : Tensor = prim::GetAttr[name="running_mean"](%6402)
  %6410 : Tensor = prim::GetAttr[name="running_var"](%6402)
  %6411 : Tensor = prim::GetAttr[name="weight"](%6402)
  %6412 : Tensor = prim::GetAttr[name="bias"](%6402)
   = prim::If(%6408) # torch/nn/functional.py:2011:4
    block0():
      %6413 : int[] = aten::size(%bottleneck_output.30) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%6413, %24) # torch/nn/functional.py:1991:17
      %6415 : int = aten::len(%6413) # torch/nn/functional.py:1992:19
      %6416 : int = aten::sub(%6415, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%6416, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %6420 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %6421 : int = aten::__getitem__(%6413, %6420) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %6421) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %6423 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6423) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6424 : Tensor = aten::batch_norm(%bottleneck_output.30, %6411, %6412, %6409, %6410, %6408, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.32 : Tensor = aten::relu_(%6424) # torch/nn/functional.py:1117:17
  %6426 : Tensor = prim::GetAttr[name="weight"](%6401)
  %6427 : Tensor? = prim::GetAttr[name="bias"](%6401)
  %6428 : int[] = prim::ListConstruct(%27, %27)
  %6429 : int[] = prim::ListConstruct(%27, %27)
  %6430 : int[] = prim::ListConstruct(%27, %27)
  %new_features.32 : Tensor = aten::conv2d(%result.32, %6426, %6427, %6428, %6429, %6430, %27) # torch/nn/modules/conv.py:415:15
  %6432 : float = prim::GetAttr[name="drop_rate"](%4999)
  %6433 : bool = aten::gt(%6432, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.29 : Tensor = prim::If(%6433) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6435 : float = prim::GetAttr[name="drop_rate"](%4999)
      %6436 : bool = prim::GetAttr[name="training"](%4999)
      %6437 : bool = aten::lt(%6435, %16) # torch/nn/functional.py:968:7
      %6438 : bool = prim::If(%6437) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6439 : bool = aten::gt(%6435, %17) # torch/nn/functional.py:968:17
          -> (%6439)
       = prim::If(%6438) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6440 : Tensor = aten::dropout(%new_features.32, %6435, %6436) # torch/nn/functional.py:973:17
      -> (%6440)
    block1():
      -> (%new_features.32)
  %6441 : Tensor[] = aten::append(%features.1, %new_features.29) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6442 : Tensor = prim::Uninitialized()
  %6443 : bool = prim::GetAttr[name="memory_efficient"](%5000)
  %6444 : bool = prim::If(%6443) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6445 : bool = prim::Uninitialized()
      %6446 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6447 : bool = aten::gt(%6446, %24)
      %6448 : bool, %6449 : bool, %6450 : int = prim::Loop(%18, %6447, %19, %6445, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6451 : int, %6452 : bool, %6453 : bool, %6454 : int):
          %tensor.17 : Tensor = aten::__getitem__(%features.1, %6454) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6456 : bool = prim::requires_grad(%tensor.17)
          %6457 : bool, %6458 : bool = prim::If(%6456) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6445)
          %6459 : int = aten::add(%6454, %27)
          %6460 : bool = aten::lt(%6459, %6446)
          %6461 : bool = aten::__and__(%6460, %6457)
          -> (%6461, %6456, %6458, %6459)
      %6462 : bool = prim::If(%6448)
        block0():
          -> (%6449)
        block1():
          -> (%19)
      -> (%6462)
    block1():
      -> (%19)
  %bottleneck_output.32 : Tensor = prim::If(%6444) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6442)
    block1():
      %concated_features.17 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6465 : __torch__.torch.nn.modules.conv.___torch_mangle_307.Conv2d = prim::GetAttr[name="conv1"](%5000)
      %6466 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_306.BatchNorm2d = prim::GetAttr[name="norm1"](%5000)
      %6467 : int = aten::dim(%concated_features.17) # torch/nn/modules/batchnorm.py:276:11
      %6468 : bool = aten::ne(%6467, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6468) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6469 : bool = prim::GetAttr[name="training"](%6466)
       = prim::If(%6469) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6470 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6466)
          %6471 : Tensor = aten::add(%6470, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6466, %6471)
          -> ()
        block1():
          -> ()
      %6472 : bool = prim::GetAttr[name="training"](%6466)
      %6473 : Tensor = prim::GetAttr[name="running_mean"](%6466)
      %6474 : Tensor = prim::GetAttr[name="running_var"](%6466)
      %6475 : Tensor = prim::GetAttr[name="weight"](%6466)
      %6476 : Tensor = prim::GetAttr[name="bias"](%6466)
       = prim::If(%6472) # torch/nn/functional.py:2011:4
        block0():
          %6477 : int[] = aten::size(%concated_features.17) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%6477, %24) # torch/nn/functional.py:1991:17
          %6479 : int = aten::len(%6477) # torch/nn/functional.py:1992:19
          %6480 : int = aten::sub(%6479, %26) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%6480, %25, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %6484 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
              %6485 : int = aten::__getitem__(%6477, %6484) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %6485) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.131)
          %6487 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6487) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6488 : Tensor = aten::batch_norm(%concated_features.17, %6475, %6476, %6473, %6474, %6472, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.33 : Tensor = aten::relu_(%6488) # torch/nn/functional.py:1117:17
      %6490 : Tensor = prim::GetAttr[name="weight"](%6465)
      %6491 : Tensor? = prim::GetAttr[name="bias"](%6465)
      %6492 : int[] = prim::ListConstruct(%27, %27)
      %6493 : int[] = prim::ListConstruct(%24, %24)
      %6494 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.33 : Tensor = aten::conv2d(%result.33, %6490, %6491, %6492, %6493, %6494, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.33)
  %6496 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5000)
  %6497 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5000)
  %6498 : int = aten::dim(%bottleneck_output.32) # torch/nn/modules/batchnorm.py:276:11
  %6499 : bool = aten::ne(%6498, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6499) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6500 : bool = prim::GetAttr[name="training"](%6497)
   = prim::If(%6500) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6501 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6497)
      %6502 : Tensor = aten::add(%6501, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6497, %6502)
      -> ()
    block1():
      -> ()
  %6503 : bool = prim::GetAttr[name="training"](%6497)
  %6504 : Tensor = prim::GetAttr[name="running_mean"](%6497)
  %6505 : Tensor = prim::GetAttr[name="running_var"](%6497)
  %6506 : Tensor = prim::GetAttr[name="weight"](%6497)
  %6507 : Tensor = prim::GetAttr[name="bias"](%6497)
   = prim::If(%6503) # torch/nn/functional.py:2011:4
    block0():
      %6508 : int[] = aten::size(%bottleneck_output.32) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%6508, %24) # torch/nn/functional.py:1991:17
      %6510 : int = aten::len(%6508) # torch/nn/functional.py:1992:19
      %6511 : int = aten::sub(%6510, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%6511, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %6515 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %6516 : int = aten::__getitem__(%6508, %6515) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %6516) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %6518 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6518) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6519 : Tensor = aten::batch_norm(%bottleneck_output.32, %6506, %6507, %6504, %6505, %6503, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.34 : Tensor = aten::relu_(%6519) # torch/nn/functional.py:1117:17
  %6521 : Tensor = prim::GetAttr[name="weight"](%6496)
  %6522 : Tensor? = prim::GetAttr[name="bias"](%6496)
  %6523 : int[] = prim::ListConstruct(%27, %27)
  %6524 : int[] = prim::ListConstruct(%27, %27)
  %6525 : int[] = prim::ListConstruct(%27, %27)
  %new_features.34 : Tensor = aten::conv2d(%result.34, %6521, %6522, %6523, %6524, %6525, %27) # torch/nn/modules/conv.py:415:15
  %6527 : float = prim::GetAttr[name="drop_rate"](%5000)
  %6528 : bool = aten::gt(%6527, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.31 : Tensor = prim::If(%6528) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6530 : float = prim::GetAttr[name="drop_rate"](%5000)
      %6531 : bool = prim::GetAttr[name="training"](%5000)
      %6532 : bool = aten::lt(%6530, %16) # torch/nn/functional.py:968:7
      %6533 : bool = prim::If(%6532) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6534 : bool = aten::gt(%6530, %17) # torch/nn/functional.py:968:17
          -> (%6534)
       = prim::If(%6533) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6535 : Tensor = aten::dropout(%new_features.34, %6530, %6531) # torch/nn/functional.py:973:17
      -> (%6535)
    block1():
      -> (%new_features.34)
  %6536 : Tensor[] = aten::append(%features.1, %new_features.31) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6537 : Tensor = prim::Uninitialized()
  %6538 : bool = prim::GetAttr[name="memory_efficient"](%5001)
  %6539 : bool = prim::If(%6538) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6540 : bool = prim::Uninitialized()
      %6541 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6542 : bool = aten::gt(%6541, %24)
      %6543 : bool, %6544 : bool, %6545 : int = prim::Loop(%18, %6542, %19, %6540, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6546 : int, %6547 : bool, %6548 : bool, %6549 : int):
          %tensor.18 : Tensor = aten::__getitem__(%features.1, %6549) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6551 : bool = prim::requires_grad(%tensor.18)
          %6552 : bool, %6553 : bool = prim::If(%6551) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6540)
          %6554 : int = aten::add(%6549, %27)
          %6555 : bool = aten::lt(%6554, %6541)
          %6556 : bool = aten::__and__(%6555, %6552)
          -> (%6556, %6551, %6553, %6554)
      %6557 : bool = prim::If(%6543)
        block0():
          -> (%6544)
        block1():
          -> (%19)
      -> (%6557)
    block1():
      -> (%19)
  %bottleneck_output.34 : Tensor = prim::If(%6539) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6537)
    block1():
      %concated_features.18 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6560 : __torch__.torch.nn.modules.conv.___torch_mangle_309.Conv2d = prim::GetAttr[name="conv1"](%5001)
      %6561 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%5001)
      %6562 : int = aten::dim(%concated_features.18) # torch/nn/modules/batchnorm.py:276:11
      %6563 : bool = aten::ne(%6562, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6563) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6564 : bool = prim::GetAttr[name="training"](%6561)
       = prim::If(%6564) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6565 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6561)
          %6566 : Tensor = aten::add(%6565, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6561, %6566)
          -> ()
        block1():
          -> ()
      %6567 : bool = prim::GetAttr[name="training"](%6561)
      %6568 : Tensor = prim::GetAttr[name="running_mean"](%6561)
      %6569 : Tensor = prim::GetAttr[name="running_var"](%6561)
      %6570 : Tensor = prim::GetAttr[name="weight"](%6561)
      %6571 : Tensor = prim::GetAttr[name="bias"](%6561)
       = prim::If(%6567) # torch/nn/functional.py:2011:4
        block0():
          %6572 : int[] = aten::size(%concated_features.18) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%6572, %24) # torch/nn/functional.py:1991:17
          %6574 : int = aten::len(%6572) # torch/nn/functional.py:1992:19
          %6575 : int = aten::sub(%6574, %26) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%6575, %25, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %6579 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
              %6580 : int = aten::__getitem__(%6572, %6579) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %6580) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.139)
          %6582 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6582) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6583 : Tensor = aten::batch_norm(%concated_features.18, %6570, %6571, %6568, %6569, %6567, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.35 : Tensor = aten::relu_(%6583) # torch/nn/functional.py:1117:17
      %6585 : Tensor = prim::GetAttr[name="weight"](%6560)
      %6586 : Tensor? = prim::GetAttr[name="bias"](%6560)
      %6587 : int[] = prim::ListConstruct(%27, %27)
      %6588 : int[] = prim::ListConstruct(%24, %24)
      %6589 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.35 : Tensor = aten::conv2d(%result.35, %6585, %6586, %6587, %6588, %6589, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.35)
  %6591 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5001)
  %6592 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5001)
  %6593 : int = aten::dim(%bottleneck_output.34) # torch/nn/modules/batchnorm.py:276:11
  %6594 : bool = aten::ne(%6593, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6594) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6595 : bool = prim::GetAttr[name="training"](%6592)
   = prim::If(%6595) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6596 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6592)
      %6597 : Tensor = aten::add(%6596, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6592, %6597)
      -> ()
    block1():
      -> ()
  %6598 : bool = prim::GetAttr[name="training"](%6592)
  %6599 : Tensor = prim::GetAttr[name="running_mean"](%6592)
  %6600 : Tensor = prim::GetAttr[name="running_var"](%6592)
  %6601 : Tensor = prim::GetAttr[name="weight"](%6592)
  %6602 : Tensor = prim::GetAttr[name="bias"](%6592)
   = prim::If(%6598) # torch/nn/functional.py:2011:4
    block0():
      %6603 : int[] = aten::size(%bottleneck_output.34) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%6603, %24) # torch/nn/functional.py:1991:17
      %6605 : int = aten::len(%6603) # torch/nn/functional.py:1992:19
      %6606 : int = aten::sub(%6605, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%6606, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %6610 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %6611 : int = aten::__getitem__(%6603, %6610) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %6611) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %6613 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6613) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6614 : Tensor = aten::batch_norm(%bottleneck_output.34, %6601, %6602, %6599, %6600, %6598, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.36 : Tensor = aten::relu_(%6614) # torch/nn/functional.py:1117:17
  %6616 : Tensor = prim::GetAttr[name="weight"](%6591)
  %6617 : Tensor? = prim::GetAttr[name="bias"](%6591)
  %6618 : int[] = prim::ListConstruct(%27, %27)
  %6619 : int[] = prim::ListConstruct(%27, %27)
  %6620 : int[] = prim::ListConstruct(%27, %27)
  %new_features.36 : Tensor = aten::conv2d(%result.36, %6616, %6617, %6618, %6619, %6620, %27) # torch/nn/modules/conv.py:415:15
  %6622 : float = prim::GetAttr[name="drop_rate"](%5001)
  %6623 : bool = aten::gt(%6622, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.33 : Tensor = prim::If(%6623) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6625 : float = prim::GetAttr[name="drop_rate"](%5001)
      %6626 : bool = prim::GetAttr[name="training"](%5001)
      %6627 : bool = aten::lt(%6625, %16) # torch/nn/functional.py:968:7
      %6628 : bool = prim::If(%6627) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6629 : bool = aten::gt(%6625, %17) # torch/nn/functional.py:968:17
          -> (%6629)
       = prim::If(%6628) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6630 : Tensor = aten::dropout(%new_features.36, %6625, %6626) # torch/nn/functional.py:973:17
      -> (%6630)
    block1():
      -> (%new_features.36)
  %6631 : Tensor[] = aten::append(%features.1, %new_features.33) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6632 : Tensor = prim::Uninitialized()
  %6633 : bool = prim::GetAttr[name="memory_efficient"](%5002)
  %6634 : bool = prim::If(%6633) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6635 : bool = prim::Uninitialized()
      %6636 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6637 : bool = aten::gt(%6636, %24)
      %6638 : bool, %6639 : bool, %6640 : int = prim::Loop(%18, %6637, %19, %6635, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6641 : int, %6642 : bool, %6643 : bool, %6644 : int):
          %tensor.19 : Tensor = aten::__getitem__(%features.1, %6644) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6646 : bool = prim::requires_grad(%tensor.19)
          %6647 : bool, %6648 : bool = prim::If(%6646) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6635)
          %6649 : int = aten::add(%6644, %27)
          %6650 : bool = aten::lt(%6649, %6636)
          %6651 : bool = aten::__and__(%6650, %6647)
          -> (%6651, %6646, %6648, %6649)
      %6652 : bool = prim::If(%6638)
        block0():
          -> (%6639)
        block1():
          -> (%19)
      -> (%6652)
    block1():
      -> (%19)
  %bottleneck_output.36 : Tensor = prim::If(%6634) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6632)
    block1():
      %concated_features.19 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6655 : __torch__.torch.nn.modules.conv.___torch_mangle_312.Conv2d = prim::GetAttr[name="conv1"](%5002)
      %6656 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_311.BatchNorm2d = prim::GetAttr[name="norm1"](%5002)
      %6657 : int = aten::dim(%concated_features.19) # torch/nn/modules/batchnorm.py:276:11
      %6658 : bool = aten::ne(%6657, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6658) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6659 : bool = prim::GetAttr[name="training"](%6656)
       = prim::If(%6659) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6660 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6656)
          %6661 : Tensor = aten::add(%6660, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6656, %6661)
          -> ()
        block1():
          -> ()
      %6662 : bool = prim::GetAttr[name="training"](%6656)
      %6663 : Tensor = prim::GetAttr[name="running_mean"](%6656)
      %6664 : Tensor = prim::GetAttr[name="running_var"](%6656)
      %6665 : Tensor = prim::GetAttr[name="weight"](%6656)
      %6666 : Tensor = prim::GetAttr[name="bias"](%6656)
       = prim::If(%6662) # torch/nn/functional.py:2011:4
        block0():
          %6667 : int[] = aten::size(%concated_features.19) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%6667, %24) # torch/nn/functional.py:1991:17
          %6669 : int = aten::len(%6667) # torch/nn/functional.py:1992:19
          %6670 : int = aten::sub(%6669, %26) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%6670, %25, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %6674 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
              %6675 : int = aten::__getitem__(%6667, %6674) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %6675) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.147)
          %6677 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6677) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6678 : Tensor = aten::batch_norm(%concated_features.19, %6665, %6666, %6663, %6664, %6662, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.37 : Tensor = aten::relu_(%6678) # torch/nn/functional.py:1117:17
      %6680 : Tensor = prim::GetAttr[name="weight"](%6655)
      %6681 : Tensor? = prim::GetAttr[name="bias"](%6655)
      %6682 : int[] = prim::ListConstruct(%27, %27)
      %6683 : int[] = prim::ListConstruct(%24, %24)
      %6684 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.37 : Tensor = aten::conv2d(%result.37, %6680, %6681, %6682, %6683, %6684, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.37)
  %6686 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5002)
  %6687 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5002)
  %6688 : int = aten::dim(%bottleneck_output.36) # torch/nn/modules/batchnorm.py:276:11
  %6689 : bool = aten::ne(%6688, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6689) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6690 : bool = prim::GetAttr[name="training"](%6687)
   = prim::If(%6690) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6691 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6687)
      %6692 : Tensor = aten::add(%6691, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6687, %6692)
      -> ()
    block1():
      -> ()
  %6693 : bool = prim::GetAttr[name="training"](%6687)
  %6694 : Tensor = prim::GetAttr[name="running_mean"](%6687)
  %6695 : Tensor = prim::GetAttr[name="running_var"](%6687)
  %6696 : Tensor = prim::GetAttr[name="weight"](%6687)
  %6697 : Tensor = prim::GetAttr[name="bias"](%6687)
   = prim::If(%6693) # torch/nn/functional.py:2011:4
    block0():
      %6698 : int[] = aten::size(%bottleneck_output.36) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%6698, %24) # torch/nn/functional.py:1991:17
      %6700 : int = aten::len(%6698) # torch/nn/functional.py:1992:19
      %6701 : int = aten::sub(%6700, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%6701, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %6705 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %6706 : int = aten::__getitem__(%6698, %6705) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %6706) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %6708 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6708) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6709 : Tensor = aten::batch_norm(%bottleneck_output.36, %6696, %6697, %6694, %6695, %6693, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.38 : Tensor = aten::relu_(%6709) # torch/nn/functional.py:1117:17
  %6711 : Tensor = prim::GetAttr[name="weight"](%6686)
  %6712 : Tensor? = prim::GetAttr[name="bias"](%6686)
  %6713 : int[] = prim::ListConstruct(%27, %27)
  %6714 : int[] = prim::ListConstruct(%27, %27)
  %6715 : int[] = prim::ListConstruct(%27, %27)
  %new_features.38 : Tensor = aten::conv2d(%result.38, %6711, %6712, %6713, %6714, %6715, %27) # torch/nn/modules/conv.py:415:15
  %6717 : float = prim::GetAttr[name="drop_rate"](%5002)
  %6718 : bool = aten::gt(%6717, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.35 : Tensor = prim::If(%6718) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6720 : float = prim::GetAttr[name="drop_rate"](%5002)
      %6721 : bool = prim::GetAttr[name="training"](%5002)
      %6722 : bool = aten::lt(%6720, %16) # torch/nn/functional.py:968:7
      %6723 : bool = prim::If(%6722) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6724 : bool = aten::gt(%6720, %17) # torch/nn/functional.py:968:17
          -> (%6724)
       = prim::If(%6723) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6725 : Tensor = aten::dropout(%new_features.38, %6720, %6721) # torch/nn/functional.py:973:17
      -> (%6725)
    block1():
      -> (%new_features.38)
  %6726 : Tensor[] = aten::append(%features.1, %new_features.35) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6727 : Tensor = prim::Uninitialized()
  %6728 : bool = prim::GetAttr[name="memory_efficient"](%5003)
  %6729 : bool = prim::If(%6728) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6730 : bool = prim::Uninitialized()
      %6731 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6732 : bool = aten::gt(%6731, %24)
      %6733 : bool, %6734 : bool, %6735 : int = prim::Loop(%18, %6732, %19, %6730, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6736 : int, %6737 : bool, %6738 : bool, %6739 : int):
          %tensor.20 : Tensor = aten::__getitem__(%features.1, %6739) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6741 : bool = prim::requires_grad(%tensor.20)
          %6742 : bool, %6743 : bool = prim::If(%6741) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6730)
          %6744 : int = aten::add(%6739, %27)
          %6745 : bool = aten::lt(%6744, %6731)
          %6746 : bool = aten::__and__(%6745, %6742)
          -> (%6746, %6741, %6743, %6744)
      %6747 : bool = prim::If(%6733)
        block0():
          -> (%6734)
        block1():
          -> (%19)
      -> (%6747)
    block1():
      -> (%19)
  %bottleneck_output.38 : Tensor = prim::If(%6729) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6727)
    block1():
      %concated_features.20 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6750 : __torch__.torch.nn.modules.conv.___torch_mangle_315.Conv2d = prim::GetAttr[name="conv1"](%5003)
      %6751 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_314.BatchNorm2d = prim::GetAttr[name="norm1"](%5003)
      %6752 : int = aten::dim(%concated_features.20) # torch/nn/modules/batchnorm.py:276:11
      %6753 : bool = aten::ne(%6752, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6753) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6754 : bool = prim::GetAttr[name="training"](%6751)
       = prim::If(%6754) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6755 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6751)
          %6756 : Tensor = aten::add(%6755, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6751, %6756)
          -> ()
        block1():
          -> ()
      %6757 : bool = prim::GetAttr[name="training"](%6751)
      %6758 : Tensor = prim::GetAttr[name="running_mean"](%6751)
      %6759 : Tensor = prim::GetAttr[name="running_var"](%6751)
      %6760 : Tensor = prim::GetAttr[name="weight"](%6751)
      %6761 : Tensor = prim::GetAttr[name="bias"](%6751)
       = prim::If(%6757) # torch/nn/functional.py:2011:4
        block0():
          %6762 : int[] = aten::size(%concated_features.20) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%6762, %24) # torch/nn/functional.py:1991:17
          %6764 : int = aten::len(%6762) # torch/nn/functional.py:1992:19
          %6765 : int = aten::sub(%6764, %26) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%6765, %25, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %6769 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
              %6770 : int = aten::__getitem__(%6762, %6769) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %6770) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.155)
          %6772 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6772) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6773 : Tensor = aten::batch_norm(%concated_features.20, %6760, %6761, %6758, %6759, %6757, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.39 : Tensor = aten::relu_(%6773) # torch/nn/functional.py:1117:17
      %6775 : Tensor = prim::GetAttr[name="weight"](%6750)
      %6776 : Tensor? = prim::GetAttr[name="bias"](%6750)
      %6777 : int[] = prim::ListConstruct(%27, %27)
      %6778 : int[] = prim::ListConstruct(%24, %24)
      %6779 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.39 : Tensor = aten::conv2d(%result.39, %6775, %6776, %6777, %6778, %6779, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.39)
  %6781 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5003)
  %6782 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5003)
  %6783 : int = aten::dim(%bottleneck_output.38) # torch/nn/modules/batchnorm.py:276:11
  %6784 : bool = aten::ne(%6783, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6784) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6785 : bool = prim::GetAttr[name="training"](%6782)
   = prim::If(%6785) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6786 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6782)
      %6787 : Tensor = aten::add(%6786, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6782, %6787)
      -> ()
    block1():
      -> ()
  %6788 : bool = prim::GetAttr[name="training"](%6782)
  %6789 : Tensor = prim::GetAttr[name="running_mean"](%6782)
  %6790 : Tensor = prim::GetAttr[name="running_var"](%6782)
  %6791 : Tensor = prim::GetAttr[name="weight"](%6782)
  %6792 : Tensor = prim::GetAttr[name="bias"](%6782)
   = prim::If(%6788) # torch/nn/functional.py:2011:4
    block0():
      %6793 : int[] = aten::size(%bottleneck_output.38) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%6793, %24) # torch/nn/functional.py:1991:17
      %6795 : int = aten::len(%6793) # torch/nn/functional.py:1992:19
      %6796 : int = aten::sub(%6795, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%6796, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %6800 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %6801 : int = aten::__getitem__(%6793, %6800) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %6801) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %6803 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6803) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6804 : Tensor = aten::batch_norm(%bottleneck_output.38, %6791, %6792, %6789, %6790, %6788, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.40 : Tensor = aten::relu_(%6804) # torch/nn/functional.py:1117:17
  %6806 : Tensor = prim::GetAttr[name="weight"](%6781)
  %6807 : Tensor? = prim::GetAttr[name="bias"](%6781)
  %6808 : int[] = prim::ListConstruct(%27, %27)
  %6809 : int[] = prim::ListConstruct(%27, %27)
  %6810 : int[] = prim::ListConstruct(%27, %27)
  %new_features.40 : Tensor = aten::conv2d(%result.40, %6806, %6807, %6808, %6809, %6810, %27) # torch/nn/modules/conv.py:415:15
  %6812 : float = prim::GetAttr[name="drop_rate"](%5003)
  %6813 : bool = aten::gt(%6812, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.37 : Tensor = prim::If(%6813) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6815 : float = prim::GetAttr[name="drop_rate"](%5003)
      %6816 : bool = prim::GetAttr[name="training"](%5003)
      %6817 : bool = aten::lt(%6815, %16) # torch/nn/functional.py:968:7
      %6818 : bool = prim::If(%6817) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6819 : bool = aten::gt(%6815, %17) # torch/nn/functional.py:968:17
          -> (%6819)
       = prim::If(%6818) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6820 : Tensor = aten::dropout(%new_features.40, %6815, %6816) # torch/nn/functional.py:973:17
      -> (%6820)
    block1():
      -> (%new_features.40)
  %6821 : Tensor[] = aten::append(%features.1, %new_features.37) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6822 : Tensor = prim::Uninitialized()
  %6823 : bool = prim::GetAttr[name="memory_efficient"](%5004)
  %6824 : bool = prim::If(%6823) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6825 : bool = prim::Uninitialized()
      %6826 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6827 : bool = aten::gt(%6826, %24)
      %6828 : bool, %6829 : bool, %6830 : int = prim::Loop(%18, %6827, %19, %6825, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6831 : int, %6832 : bool, %6833 : bool, %6834 : int):
          %tensor.21 : Tensor = aten::__getitem__(%features.1, %6834) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6836 : bool = prim::requires_grad(%tensor.21)
          %6837 : bool, %6838 : bool = prim::If(%6836) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6825)
          %6839 : int = aten::add(%6834, %27)
          %6840 : bool = aten::lt(%6839, %6826)
          %6841 : bool = aten::__and__(%6840, %6837)
          -> (%6841, %6836, %6838, %6839)
      %6842 : bool = prim::If(%6828)
        block0():
          -> (%6829)
        block1():
          -> (%19)
      -> (%6842)
    block1():
      -> (%19)
  %bottleneck_output.40 : Tensor = prim::If(%6824) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6822)
    block1():
      %concated_features.21 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6845 : __torch__.torch.nn.modules.conv.___torch_mangle_317.Conv2d = prim::GetAttr[name="conv1"](%5004)
      %6846 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%5004)
      %6847 : int = aten::dim(%concated_features.21) # torch/nn/modules/batchnorm.py:276:11
      %6848 : bool = aten::ne(%6847, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6848) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6849 : bool = prim::GetAttr[name="training"](%6846)
       = prim::If(%6849) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6850 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6846)
          %6851 : Tensor = aten::add(%6850, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6846, %6851)
          -> ()
        block1():
          -> ()
      %6852 : bool = prim::GetAttr[name="training"](%6846)
      %6853 : Tensor = prim::GetAttr[name="running_mean"](%6846)
      %6854 : Tensor = prim::GetAttr[name="running_var"](%6846)
      %6855 : Tensor = prim::GetAttr[name="weight"](%6846)
      %6856 : Tensor = prim::GetAttr[name="bias"](%6846)
       = prim::If(%6852) # torch/nn/functional.py:2011:4
        block0():
          %6857 : int[] = aten::size(%concated_features.21) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%6857, %24) # torch/nn/functional.py:1991:17
          %6859 : int = aten::len(%6857) # torch/nn/functional.py:1992:19
          %6860 : int = aten::sub(%6859, %26) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%6860, %25, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %6864 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
              %6865 : int = aten::__getitem__(%6857, %6864) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %6865) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.163)
          %6867 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6867) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6868 : Tensor = aten::batch_norm(%concated_features.21, %6855, %6856, %6853, %6854, %6852, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.41 : Tensor = aten::relu_(%6868) # torch/nn/functional.py:1117:17
      %6870 : Tensor = prim::GetAttr[name="weight"](%6845)
      %6871 : Tensor? = prim::GetAttr[name="bias"](%6845)
      %6872 : int[] = prim::ListConstruct(%27, %27)
      %6873 : int[] = prim::ListConstruct(%24, %24)
      %6874 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.41 : Tensor = aten::conv2d(%result.41, %6870, %6871, %6872, %6873, %6874, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.41)
  %6876 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5004)
  %6877 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5004)
  %6878 : int = aten::dim(%bottleneck_output.40) # torch/nn/modules/batchnorm.py:276:11
  %6879 : bool = aten::ne(%6878, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6879) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6880 : bool = prim::GetAttr[name="training"](%6877)
   = prim::If(%6880) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6881 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6877)
      %6882 : Tensor = aten::add(%6881, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6877, %6882)
      -> ()
    block1():
      -> ()
  %6883 : bool = prim::GetAttr[name="training"](%6877)
  %6884 : Tensor = prim::GetAttr[name="running_mean"](%6877)
  %6885 : Tensor = prim::GetAttr[name="running_var"](%6877)
  %6886 : Tensor = prim::GetAttr[name="weight"](%6877)
  %6887 : Tensor = prim::GetAttr[name="bias"](%6877)
   = prim::If(%6883) # torch/nn/functional.py:2011:4
    block0():
      %6888 : int[] = aten::size(%bottleneck_output.40) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%6888, %24) # torch/nn/functional.py:1991:17
      %6890 : int = aten::len(%6888) # torch/nn/functional.py:1992:19
      %6891 : int = aten::sub(%6890, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%6891, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %6895 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %6896 : int = aten::__getitem__(%6888, %6895) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %6896) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %6898 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6898) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6899 : Tensor = aten::batch_norm(%bottleneck_output.40, %6886, %6887, %6884, %6885, %6883, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.42 : Tensor = aten::relu_(%6899) # torch/nn/functional.py:1117:17
  %6901 : Tensor = prim::GetAttr[name="weight"](%6876)
  %6902 : Tensor? = prim::GetAttr[name="bias"](%6876)
  %6903 : int[] = prim::ListConstruct(%27, %27)
  %6904 : int[] = prim::ListConstruct(%27, %27)
  %6905 : int[] = prim::ListConstruct(%27, %27)
  %new_features.42 : Tensor = aten::conv2d(%result.42, %6901, %6902, %6903, %6904, %6905, %27) # torch/nn/modules/conv.py:415:15
  %6907 : float = prim::GetAttr[name="drop_rate"](%5004)
  %6908 : bool = aten::gt(%6907, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.39 : Tensor = prim::If(%6908) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6910 : float = prim::GetAttr[name="drop_rate"](%5004)
      %6911 : bool = prim::GetAttr[name="training"](%5004)
      %6912 : bool = aten::lt(%6910, %16) # torch/nn/functional.py:968:7
      %6913 : bool = prim::If(%6912) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6914 : bool = aten::gt(%6910, %17) # torch/nn/functional.py:968:17
          -> (%6914)
       = prim::If(%6913) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6915 : Tensor = aten::dropout(%new_features.42, %6910, %6911) # torch/nn/functional.py:973:17
      -> (%6915)
    block1():
      -> (%new_features.42)
  %6916 : Tensor[] = aten::append(%features.1, %new_features.39) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6917 : Tensor = prim::Uninitialized()
  %6918 : bool = prim::GetAttr[name="memory_efficient"](%5005)
  %6919 : bool = prim::If(%6918) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6920 : bool = prim::Uninitialized()
      %6921 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6922 : bool = aten::gt(%6921, %24)
      %6923 : bool, %6924 : bool, %6925 : int = prim::Loop(%18, %6922, %19, %6920, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6926 : int, %6927 : bool, %6928 : bool, %6929 : int):
          %tensor.22 : Tensor = aten::__getitem__(%features.1, %6929) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6931 : bool = prim::requires_grad(%tensor.22)
          %6932 : bool, %6933 : bool = prim::If(%6931) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6920)
          %6934 : int = aten::add(%6929, %27)
          %6935 : bool = aten::lt(%6934, %6921)
          %6936 : bool = aten::__and__(%6935, %6932)
          -> (%6936, %6931, %6933, %6934)
      %6937 : bool = prim::If(%6923)
        block0():
          -> (%6924)
        block1():
          -> (%19)
      -> (%6937)
    block1():
      -> (%19)
  %bottleneck_output.42 : Tensor = prim::If(%6919) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6917)
    block1():
      %concated_features.22 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6940 : __torch__.torch.nn.modules.conv.___torch_mangle_323.Conv2d = prim::GetAttr[name="conv1"](%5005)
      %6941 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_320.BatchNorm2d = prim::GetAttr[name="norm1"](%5005)
      %6942 : int = aten::dim(%concated_features.22) # torch/nn/modules/batchnorm.py:276:11
      %6943 : bool = aten::ne(%6942, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6943) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6944 : bool = prim::GetAttr[name="training"](%6941)
       = prim::If(%6944) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6945 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6941)
          %6946 : Tensor = aten::add(%6945, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6941, %6946)
          -> ()
        block1():
          -> ()
      %6947 : bool = prim::GetAttr[name="training"](%6941)
      %6948 : Tensor = prim::GetAttr[name="running_mean"](%6941)
      %6949 : Tensor = prim::GetAttr[name="running_var"](%6941)
      %6950 : Tensor = prim::GetAttr[name="weight"](%6941)
      %6951 : Tensor = prim::GetAttr[name="bias"](%6941)
       = prim::If(%6947) # torch/nn/functional.py:2011:4
        block0():
          %6952 : int[] = aten::size(%concated_features.22) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%6952, %24) # torch/nn/functional.py:1991:17
          %6954 : int = aten::len(%6952) # torch/nn/functional.py:1992:19
          %6955 : int = aten::sub(%6954, %26) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%6955, %25, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %6959 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
              %6960 : int = aten::__getitem__(%6952, %6959) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %6960) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.171)
          %6962 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6962) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6963 : Tensor = aten::batch_norm(%concated_features.22, %6950, %6951, %6948, %6949, %6947, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.43 : Tensor = aten::relu_(%6963) # torch/nn/functional.py:1117:17
      %6965 : Tensor = prim::GetAttr[name="weight"](%6940)
      %6966 : Tensor? = prim::GetAttr[name="bias"](%6940)
      %6967 : int[] = prim::ListConstruct(%27, %27)
      %6968 : int[] = prim::ListConstruct(%24, %24)
      %6969 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.43 : Tensor = aten::conv2d(%result.43, %6965, %6966, %6967, %6968, %6969, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.43)
  %6971 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5005)
  %6972 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5005)
  %6973 : int = aten::dim(%bottleneck_output.42) # torch/nn/modules/batchnorm.py:276:11
  %6974 : bool = aten::ne(%6973, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6974) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6975 : bool = prim::GetAttr[name="training"](%6972)
   = prim::If(%6975) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6976 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6972)
      %6977 : Tensor = aten::add(%6976, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6972, %6977)
      -> ()
    block1():
      -> ()
  %6978 : bool = prim::GetAttr[name="training"](%6972)
  %6979 : Tensor = prim::GetAttr[name="running_mean"](%6972)
  %6980 : Tensor = prim::GetAttr[name="running_var"](%6972)
  %6981 : Tensor = prim::GetAttr[name="weight"](%6972)
  %6982 : Tensor = prim::GetAttr[name="bias"](%6972)
   = prim::If(%6978) # torch/nn/functional.py:2011:4
    block0():
      %6983 : int[] = aten::size(%bottleneck_output.42) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%6983, %24) # torch/nn/functional.py:1991:17
      %6985 : int = aten::len(%6983) # torch/nn/functional.py:1992:19
      %6986 : int = aten::sub(%6985, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%6986, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %6990 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %6991 : int = aten::__getitem__(%6983, %6990) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %6991) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %6993 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6993) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6994 : Tensor = aten::batch_norm(%bottleneck_output.42, %6981, %6982, %6979, %6980, %6978, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.44 : Tensor = aten::relu_(%6994) # torch/nn/functional.py:1117:17
  %6996 : Tensor = prim::GetAttr[name="weight"](%6971)
  %6997 : Tensor? = prim::GetAttr[name="bias"](%6971)
  %6998 : int[] = prim::ListConstruct(%27, %27)
  %6999 : int[] = prim::ListConstruct(%27, %27)
  %7000 : int[] = prim::ListConstruct(%27, %27)
  %new_features.44 : Tensor = aten::conv2d(%result.44, %6996, %6997, %6998, %6999, %7000, %27) # torch/nn/modules/conv.py:415:15
  %7002 : float = prim::GetAttr[name="drop_rate"](%5005)
  %7003 : bool = aten::gt(%7002, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.41 : Tensor = prim::If(%7003) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7005 : float = prim::GetAttr[name="drop_rate"](%5005)
      %7006 : bool = prim::GetAttr[name="training"](%5005)
      %7007 : bool = aten::lt(%7005, %16) # torch/nn/functional.py:968:7
      %7008 : bool = prim::If(%7007) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7009 : bool = aten::gt(%7005, %17) # torch/nn/functional.py:968:17
          -> (%7009)
       = prim::If(%7008) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7010 : Tensor = aten::dropout(%new_features.44, %7005, %7006) # torch/nn/functional.py:973:17
      -> (%7010)
    block1():
      -> (%new_features.44)
  %7011 : Tensor[] = aten::append(%features.1, %new_features.41) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7012 : Tensor = prim::Uninitialized()
  %7013 : bool = prim::GetAttr[name="memory_efficient"](%5006)
  %7014 : bool = prim::If(%7013) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7015 : bool = prim::Uninitialized()
      %7016 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7017 : bool = aten::gt(%7016, %24)
      %7018 : bool, %7019 : bool, %7020 : int = prim::Loop(%18, %7017, %19, %7015, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7021 : int, %7022 : bool, %7023 : bool, %7024 : int):
          %tensor.23 : Tensor = aten::__getitem__(%features.1, %7024) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7026 : bool = prim::requires_grad(%tensor.23)
          %7027 : bool, %7028 : bool = prim::If(%7026) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7015)
          %7029 : int = aten::add(%7024, %27)
          %7030 : bool = aten::lt(%7029, %7016)
          %7031 : bool = aten::__and__(%7030, %7027)
          -> (%7031, %7026, %7028, %7029)
      %7032 : bool = prim::If(%7018)
        block0():
          -> (%7019)
        block1():
          -> (%19)
      -> (%7032)
    block1():
      -> (%19)
  %bottleneck_output.44 : Tensor = prim::If(%7014) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7012)
    block1():
      %concated_features.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7035 : __torch__.torch.nn.modules.conv.___torch_mangle_326.Conv2d = prim::GetAttr[name="conv1"](%5006)
      %7036 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_325.BatchNorm2d = prim::GetAttr[name="norm1"](%5006)
      %7037 : int = aten::dim(%concated_features.23) # torch/nn/modules/batchnorm.py:276:11
      %7038 : bool = aten::ne(%7037, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7038) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7039 : bool = prim::GetAttr[name="training"](%7036)
       = prim::If(%7039) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7040 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7036)
          %7041 : Tensor = aten::add(%7040, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7036, %7041)
          -> ()
        block1():
          -> ()
      %7042 : bool = prim::GetAttr[name="training"](%7036)
      %7043 : Tensor = prim::GetAttr[name="running_mean"](%7036)
      %7044 : Tensor = prim::GetAttr[name="running_var"](%7036)
      %7045 : Tensor = prim::GetAttr[name="weight"](%7036)
      %7046 : Tensor = prim::GetAttr[name="bias"](%7036)
       = prim::If(%7042) # torch/nn/functional.py:2011:4
        block0():
          %7047 : int[] = aten::size(%concated_features.23) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%7047, %24) # torch/nn/functional.py:1991:17
          %7049 : int = aten::len(%7047) # torch/nn/functional.py:1992:19
          %7050 : int = aten::sub(%7049, %26) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%7050, %25, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %7054 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
              %7055 : int = aten::__getitem__(%7047, %7054) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %7055) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.179)
          %7057 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7057) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7058 : Tensor = aten::batch_norm(%concated_features.23, %7045, %7046, %7043, %7044, %7042, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.45 : Tensor = aten::relu_(%7058) # torch/nn/functional.py:1117:17
      %7060 : Tensor = prim::GetAttr[name="weight"](%7035)
      %7061 : Tensor? = prim::GetAttr[name="bias"](%7035)
      %7062 : int[] = prim::ListConstruct(%27, %27)
      %7063 : int[] = prim::ListConstruct(%24, %24)
      %7064 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.45 : Tensor = aten::conv2d(%result.45, %7060, %7061, %7062, %7063, %7064, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.45)
  %7066 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5006)
  %7067 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5006)
  %7068 : int = aten::dim(%bottleneck_output.44) # torch/nn/modules/batchnorm.py:276:11
  %7069 : bool = aten::ne(%7068, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7069) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7070 : bool = prim::GetAttr[name="training"](%7067)
   = prim::If(%7070) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7071 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7067)
      %7072 : Tensor = aten::add(%7071, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7067, %7072)
      -> ()
    block1():
      -> ()
  %7073 : bool = prim::GetAttr[name="training"](%7067)
  %7074 : Tensor = prim::GetAttr[name="running_mean"](%7067)
  %7075 : Tensor = prim::GetAttr[name="running_var"](%7067)
  %7076 : Tensor = prim::GetAttr[name="weight"](%7067)
  %7077 : Tensor = prim::GetAttr[name="bias"](%7067)
   = prim::If(%7073) # torch/nn/functional.py:2011:4
    block0():
      %7078 : int[] = aten::size(%bottleneck_output.44) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%7078, %24) # torch/nn/functional.py:1991:17
      %7080 : int = aten::len(%7078) # torch/nn/functional.py:1992:19
      %7081 : int = aten::sub(%7080, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%7081, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %7085 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %7086 : int = aten::__getitem__(%7078, %7085) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %7086) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %7088 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7088) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7089 : Tensor = aten::batch_norm(%bottleneck_output.44, %7076, %7077, %7074, %7075, %7073, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.46 : Tensor = aten::relu_(%7089) # torch/nn/functional.py:1117:17
  %7091 : Tensor = prim::GetAttr[name="weight"](%7066)
  %7092 : Tensor? = prim::GetAttr[name="bias"](%7066)
  %7093 : int[] = prim::ListConstruct(%27, %27)
  %7094 : int[] = prim::ListConstruct(%27, %27)
  %7095 : int[] = prim::ListConstruct(%27, %27)
  %new_features.46 : Tensor = aten::conv2d(%result.46, %7091, %7092, %7093, %7094, %7095, %27) # torch/nn/modules/conv.py:415:15
  %7097 : float = prim::GetAttr[name="drop_rate"](%5006)
  %7098 : bool = aten::gt(%7097, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.43 : Tensor = prim::If(%7098) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7100 : float = prim::GetAttr[name="drop_rate"](%5006)
      %7101 : bool = prim::GetAttr[name="training"](%5006)
      %7102 : bool = aten::lt(%7100, %16) # torch/nn/functional.py:968:7
      %7103 : bool = prim::If(%7102) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7104 : bool = aten::gt(%7100, %17) # torch/nn/functional.py:968:17
          -> (%7104)
       = prim::If(%7103) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7105 : Tensor = aten::dropout(%new_features.46, %7100, %7101) # torch/nn/functional.py:973:17
      -> (%7105)
    block1():
      -> (%new_features.46)
  %7106 : Tensor[] = aten::append(%features.1, %new_features.43) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7107 : Tensor = prim::Uninitialized()
  %7108 : bool = prim::GetAttr[name="memory_efficient"](%5007)
  %7109 : bool = prim::If(%7108) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7110 : bool = prim::Uninitialized()
      %7111 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7112 : bool = aten::gt(%7111, %24)
      %7113 : bool, %7114 : bool, %7115 : int = prim::Loop(%18, %7112, %19, %7110, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7116 : int, %7117 : bool, %7118 : bool, %7119 : int):
          %tensor.24 : Tensor = aten::__getitem__(%features.1, %7119) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7121 : bool = prim::requires_grad(%tensor.24)
          %7122 : bool, %7123 : bool = prim::If(%7121) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7110)
          %7124 : int = aten::add(%7119, %27)
          %7125 : bool = aten::lt(%7124, %7111)
          %7126 : bool = aten::__and__(%7125, %7122)
          -> (%7126, %7121, %7123, %7124)
      %7127 : bool = prim::If(%7113)
        block0():
          -> (%7114)
        block1():
          -> (%19)
      -> (%7127)
    block1():
      -> (%19)
  %bottleneck_output.46 : Tensor = prim::If(%7109) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7107)
    block1():
      %concated_features.24 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7130 : __torch__.torch.nn.modules.conv.___torch_mangle_328.Conv2d = prim::GetAttr[name="conv1"](%5007)
      %7131 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_237.BatchNorm2d = prim::GetAttr[name="norm1"](%5007)
      %7132 : int = aten::dim(%concated_features.24) # torch/nn/modules/batchnorm.py:276:11
      %7133 : bool = aten::ne(%7132, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7133) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7134 : bool = prim::GetAttr[name="training"](%7131)
       = prim::If(%7134) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7135 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7131)
          %7136 : Tensor = aten::add(%7135, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7131, %7136)
          -> ()
        block1():
          -> ()
      %7137 : bool = prim::GetAttr[name="training"](%7131)
      %7138 : Tensor = prim::GetAttr[name="running_mean"](%7131)
      %7139 : Tensor = prim::GetAttr[name="running_var"](%7131)
      %7140 : Tensor = prim::GetAttr[name="weight"](%7131)
      %7141 : Tensor = prim::GetAttr[name="bias"](%7131)
       = prim::If(%7137) # torch/nn/functional.py:2011:4
        block0():
          %7142 : int[] = aten::size(%concated_features.24) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%7142, %24) # torch/nn/functional.py:1991:17
          %7144 : int = aten::len(%7142) # torch/nn/functional.py:1992:19
          %7145 : int = aten::sub(%7144, %26) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%7145, %25, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %7149 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
              %7150 : int = aten::__getitem__(%7142, %7149) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %7150) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.187)
          %7152 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7152) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7153 : Tensor = aten::batch_norm(%concated_features.24, %7140, %7141, %7138, %7139, %7137, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.47 : Tensor = aten::relu_(%7153) # torch/nn/functional.py:1117:17
      %7155 : Tensor = prim::GetAttr[name="weight"](%7130)
      %7156 : Tensor? = prim::GetAttr[name="bias"](%7130)
      %7157 : int[] = prim::ListConstruct(%27, %27)
      %7158 : int[] = prim::ListConstruct(%24, %24)
      %7159 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.47 : Tensor = aten::conv2d(%result.47, %7155, %7156, %7157, %7158, %7159, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.47)
  %7161 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5007)
  %7162 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5007)
  %7163 : int = aten::dim(%bottleneck_output.46) # torch/nn/modules/batchnorm.py:276:11
  %7164 : bool = aten::ne(%7163, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7164) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7165 : bool = prim::GetAttr[name="training"](%7162)
   = prim::If(%7165) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7166 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7162)
      %7167 : Tensor = aten::add(%7166, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7162, %7167)
      -> ()
    block1():
      -> ()
  %7168 : bool = prim::GetAttr[name="training"](%7162)
  %7169 : Tensor = prim::GetAttr[name="running_mean"](%7162)
  %7170 : Tensor = prim::GetAttr[name="running_var"](%7162)
  %7171 : Tensor = prim::GetAttr[name="weight"](%7162)
  %7172 : Tensor = prim::GetAttr[name="bias"](%7162)
   = prim::If(%7168) # torch/nn/functional.py:2011:4
    block0():
      %7173 : int[] = aten::size(%bottleneck_output.46) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%7173, %24) # torch/nn/functional.py:1991:17
      %7175 : int = aten::len(%7173) # torch/nn/functional.py:1992:19
      %7176 : int = aten::sub(%7175, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%7176, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %7180 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %7181 : int = aten::__getitem__(%7173, %7180) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %7181) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %7183 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7183) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7184 : Tensor = aten::batch_norm(%bottleneck_output.46, %7171, %7172, %7169, %7170, %7168, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.48 : Tensor = aten::relu_(%7184) # torch/nn/functional.py:1117:17
  %7186 : Tensor = prim::GetAttr[name="weight"](%7161)
  %7187 : Tensor? = prim::GetAttr[name="bias"](%7161)
  %7188 : int[] = prim::ListConstruct(%27, %27)
  %7189 : int[] = prim::ListConstruct(%27, %27)
  %7190 : int[] = prim::ListConstruct(%27, %27)
  %new_features.48 : Tensor = aten::conv2d(%result.48, %7186, %7187, %7188, %7189, %7190, %27) # torch/nn/modules/conv.py:415:15
  %7192 : float = prim::GetAttr[name="drop_rate"](%5007)
  %7193 : bool = aten::gt(%7192, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.45 : Tensor = prim::If(%7193) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7195 : float = prim::GetAttr[name="drop_rate"](%5007)
      %7196 : bool = prim::GetAttr[name="training"](%5007)
      %7197 : bool = aten::lt(%7195, %16) # torch/nn/functional.py:968:7
      %7198 : bool = prim::If(%7197) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7199 : bool = aten::gt(%7195, %17) # torch/nn/functional.py:968:17
          -> (%7199)
       = prim::If(%7198) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7200 : Tensor = aten::dropout(%new_features.48, %7195, %7196) # torch/nn/functional.py:973:17
      -> (%7200)
    block1():
      -> (%new_features.48)
  %7201 : Tensor[] = aten::append(%features.1, %new_features.45) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7202 : Tensor = prim::Uninitialized()
  %7203 : bool = prim::GetAttr[name="memory_efficient"](%5008)
  %7204 : bool = prim::If(%7203) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7205 : bool = prim::Uninitialized()
      %7206 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7207 : bool = aten::gt(%7206, %24)
      %7208 : bool, %7209 : bool, %7210 : int = prim::Loop(%18, %7207, %19, %7205, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7211 : int, %7212 : bool, %7213 : bool, %7214 : int):
          %tensor.25 : Tensor = aten::__getitem__(%features.1, %7214) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7216 : bool = prim::requires_grad(%tensor.25)
          %7217 : bool, %7218 : bool = prim::If(%7216) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7205)
          %7219 : int = aten::add(%7214, %27)
          %7220 : bool = aten::lt(%7219, %7206)
          %7221 : bool = aten::__and__(%7220, %7217)
          -> (%7221, %7216, %7218, %7219)
      %7222 : bool = prim::If(%7208)
        block0():
          -> (%7209)
        block1():
          -> (%19)
      -> (%7222)
    block1():
      -> (%19)
  %bottleneck_output.48 : Tensor = prim::If(%7204) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7202)
    block1():
      %concated_features.25 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7225 : __torch__.torch.nn.modules.conv.___torch_mangle_331.Conv2d = prim::GetAttr[name="conv1"](%5008)
      %7226 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_330.BatchNorm2d = prim::GetAttr[name="norm1"](%5008)
      %7227 : int = aten::dim(%concated_features.25) # torch/nn/modules/batchnorm.py:276:11
      %7228 : bool = aten::ne(%7227, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7228) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7229 : bool = prim::GetAttr[name="training"](%7226)
       = prim::If(%7229) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7230 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7226)
          %7231 : Tensor = aten::add(%7230, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7226, %7231)
          -> ()
        block1():
          -> ()
      %7232 : bool = prim::GetAttr[name="training"](%7226)
      %7233 : Tensor = prim::GetAttr[name="running_mean"](%7226)
      %7234 : Tensor = prim::GetAttr[name="running_var"](%7226)
      %7235 : Tensor = prim::GetAttr[name="weight"](%7226)
      %7236 : Tensor = prim::GetAttr[name="bias"](%7226)
       = prim::If(%7232) # torch/nn/functional.py:2011:4
        block0():
          %7237 : int[] = aten::size(%concated_features.25) # torch/nn/functional.py:2012:27
          %size_prods.192 : int = aten::__getitem__(%7237, %24) # torch/nn/functional.py:1991:17
          %7239 : int = aten::len(%7237) # torch/nn/functional.py:1992:19
          %7240 : int = aten::sub(%7239, %26) # torch/nn/functional.py:1992:19
          %size_prods.193 : int = prim::Loop(%7240, %25, %size_prods.192) # torch/nn/functional.py:1992:4
            block0(%i.49 : int, %size_prods.194 : int):
              %7244 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
              %7245 : int = aten::__getitem__(%7237, %7244) # torch/nn/functional.py:1993:22
              %size_prods.195 : int = aten::mul(%size_prods.194, %7245) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.195)
          %7247 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7247) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7248 : Tensor = aten::batch_norm(%concated_features.25, %7235, %7236, %7233, %7234, %7232, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.49 : Tensor = aten::relu_(%7248) # torch/nn/functional.py:1117:17
      %7250 : Tensor = prim::GetAttr[name="weight"](%7225)
      %7251 : Tensor? = prim::GetAttr[name="bias"](%7225)
      %7252 : int[] = prim::ListConstruct(%27, %27)
      %7253 : int[] = prim::ListConstruct(%24, %24)
      %7254 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.49 : Tensor = aten::conv2d(%result.49, %7250, %7251, %7252, %7253, %7254, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.49)
  %7256 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5008)
  %7257 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5008)
  %7258 : int = aten::dim(%bottleneck_output.48) # torch/nn/modules/batchnorm.py:276:11
  %7259 : bool = aten::ne(%7258, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7259) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7260 : bool = prim::GetAttr[name="training"](%7257)
   = prim::If(%7260) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7261 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7257)
      %7262 : Tensor = aten::add(%7261, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7257, %7262)
      -> ()
    block1():
      -> ()
  %7263 : bool = prim::GetAttr[name="training"](%7257)
  %7264 : Tensor = prim::GetAttr[name="running_mean"](%7257)
  %7265 : Tensor = prim::GetAttr[name="running_var"](%7257)
  %7266 : Tensor = prim::GetAttr[name="weight"](%7257)
  %7267 : Tensor = prim::GetAttr[name="bias"](%7257)
   = prim::If(%7263) # torch/nn/functional.py:2011:4
    block0():
      %7268 : int[] = aten::size(%bottleneck_output.48) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%7268, %24) # torch/nn/functional.py:1991:17
      %7270 : int = aten::len(%7268) # torch/nn/functional.py:1992:19
      %7271 : int = aten::sub(%7270, %26) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%7271, %25, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %7275 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
          %7276 : int = aten::__getitem__(%7268, %7275) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %7276) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.199)
      %7278 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7278) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7279 : Tensor = aten::batch_norm(%bottleneck_output.48, %7266, %7267, %7264, %7265, %7263, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.50 : Tensor = aten::relu_(%7279) # torch/nn/functional.py:1117:17
  %7281 : Tensor = prim::GetAttr[name="weight"](%7256)
  %7282 : Tensor? = prim::GetAttr[name="bias"](%7256)
  %7283 : int[] = prim::ListConstruct(%27, %27)
  %7284 : int[] = prim::ListConstruct(%27, %27)
  %7285 : int[] = prim::ListConstruct(%27, %27)
  %new_features.50 : Tensor = aten::conv2d(%result.50, %7281, %7282, %7283, %7284, %7285, %27) # torch/nn/modules/conv.py:415:15
  %7287 : float = prim::GetAttr[name="drop_rate"](%5008)
  %7288 : bool = aten::gt(%7287, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.47 : Tensor = prim::If(%7288) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7290 : float = prim::GetAttr[name="drop_rate"](%5008)
      %7291 : bool = prim::GetAttr[name="training"](%5008)
      %7292 : bool = aten::lt(%7290, %16) # torch/nn/functional.py:968:7
      %7293 : bool = prim::If(%7292) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7294 : bool = aten::gt(%7290, %17) # torch/nn/functional.py:968:17
          -> (%7294)
       = prim::If(%7293) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7295 : Tensor = aten::dropout(%new_features.50, %7290, %7291) # torch/nn/functional.py:973:17
      -> (%7295)
    block1():
      -> (%new_features.50)
  %7296 : Tensor[] = aten::append(%features.1, %new_features.47) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7297 : Tensor = prim::Uninitialized()
  %7298 : bool = prim::GetAttr[name="memory_efficient"](%5009)
  %7299 : bool = prim::If(%7298) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7300 : bool = prim::Uninitialized()
      %7301 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7302 : bool = aten::gt(%7301, %24)
      %7303 : bool, %7304 : bool, %7305 : int = prim::Loop(%18, %7302, %19, %7300, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7306 : int, %7307 : bool, %7308 : bool, %7309 : int):
          %tensor.26 : Tensor = aten::__getitem__(%features.1, %7309) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7311 : bool = prim::requires_grad(%tensor.26)
          %7312 : bool, %7313 : bool = prim::If(%7311) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7300)
          %7314 : int = aten::add(%7309, %27)
          %7315 : bool = aten::lt(%7314, %7301)
          %7316 : bool = aten::__and__(%7315, %7312)
          -> (%7316, %7311, %7313, %7314)
      %7317 : bool = prim::If(%7303)
        block0():
          -> (%7304)
        block1():
          -> (%19)
      -> (%7317)
    block1():
      -> (%19)
  %bottleneck_output.50 : Tensor = prim::If(%7299) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7297)
    block1():
      %concated_features.26 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7320 : __torch__.torch.nn.modules.conv.___torch_mangle_334.Conv2d = prim::GetAttr[name="conv1"](%5009)
      %7321 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_333.BatchNorm2d = prim::GetAttr[name="norm1"](%5009)
      %7322 : int = aten::dim(%concated_features.26) # torch/nn/modules/batchnorm.py:276:11
      %7323 : bool = aten::ne(%7322, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7323) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7324 : bool = prim::GetAttr[name="training"](%7321)
       = prim::If(%7324) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7325 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7321)
          %7326 : Tensor = aten::add(%7325, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7321, %7326)
          -> ()
        block1():
          -> ()
      %7327 : bool = prim::GetAttr[name="training"](%7321)
      %7328 : Tensor = prim::GetAttr[name="running_mean"](%7321)
      %7329 : Tensor = prim::GetAttr[name="running_var"](%7321)
      %7330 : Tensor = prim::GetAttr[name="weight"](%7321)
      %7331 : Tensor = prim::GetAttr[name="bias"](%7321)
       = prim::If(%7327) # torch/nn/functional.py:2011:4
        block0():
          %7332 : int[] = aten::size(%concated_features.26) # torch/nn/functional.py:2012:27
          %size_prods.200 : int = aten::__getitem__(%7332, %24) # torch/nn/functional.py:1991:17
          %7334 : int = aten::len(%7332) # torch/nn/functional.py:1992:19
          %7335 : int = aten::sub(%7334, %26) # torch/nn/functional.py:1992:19
          %size_prods.201 : int = prim::Loop(%7335, %25, %size_prods.200) # torch/nn/functional.py:1992:4
            block0(%i.51 : int, %size_prods.202 : int):
              %7339 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
              %7340 : int = aten::__getitem__(%7332, %7339) # torch/nn/functional.py:1993:22
              %size_prods.203 : int = aten::mul(%size_prods.202, %7340) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.203)
          %7342 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7342) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7343 : Tensor = aten::batch_norm(%concated_features.26, %7330, %7331, %7328, %7329, %7327, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.51 : Tensor = aten::relu_(%7343) # torch/nn/functional.py:1117:17
      %7345 : Tensor = prim::GetAttr[name="weight"](%7320)
      %7346 : Tensor? = prim::GetAttr[name="bias"](%7320)
      %7347 : int[] = prim::ListConstruct(%27, %27)
      %7348 : int[] = prim::ListConstruct(%24, %24)
      %7349 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.51 : Tensor = aten::conv2d(%result.51, %7345, %7346, %7347, %7348, %7349, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.51)
  %7351 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5009)
  %7352 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5009)
  %7353 : int = aten::dim(%bottleneck_output.50) # torch/nn/modules/batchnorm.py:276:11
  %7354 : bool = aten::ne(%7353, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7354) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7355 : bool = prim::GetAttr[name="training"](%7352)
   = prim::If(%7355) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7356 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7352)
      %7357 : Tensor = aten::add(%7356, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7352, %7357)
      -> ()
    block1():
      -> ()
  %7358 : bool = prim::GetAttr[name="training"](%7352)
  %7359 : Tensor = prim::GetAttr[name="running_mean"](%7352)
  %7360 : Tensor = prim::GetAttr[name="running_var"](%7352)
  %7361 : Tensor = prim::GetAttr[name="weight"](%7352)
  %7362 : Tensor = prim::GetAttr[name="bias"](%7352)
   = prim::If(%7358) # torch/nn/functional.py:2011:4
    block0():
      %7363 : int[] = aten::size(%bottleneck_output.50) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%7363, %24) # torch/nn/functional.py:1991:17
      %7365 : int = aten::len(%7363) # torch/nn/functional.py:1992:19
      %7366 : int = aten::sub(%7365, %26) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%7366, %25, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %7370 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
          %7371 : int = aten::__getitem__(%7363, %7370) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %7371) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.207)
      %7373 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7373) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7374 : Tensor = aten::batch_norm(%bottleneck_output.50, %7361, %7362, %7359, %7360, %7358, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.52 : Tensor = aten::relu_(%7374) # torch/nn/functional.py:1117:17
  %7376 : Tensor = prim::GetAttr[name="weight"](%7351)
  %7377 : Tensor? = prim::GetAttr[name="bias"](%7351)
  %7378 : int[] = prim::ListConstruct(%27, %27)
  %7379 : int[] = prim::ListConstruct(%27, %27)
  %7380 : int[] = prim::ListConstruct(%27, %27)
  %new_features.52 : Tensor = aten::conv2d(%result.52, %7376, %7377, %7378, %7379, %7380, %27) # torch/nn/modules/conv.py:415:15
  %7382 : float = prim::GetAttr[name="drop_rate"](%5009)
  %7383 : bool = aten::gt(%7382, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.49 : Tensor = prim::If(%7383) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7385 : float = prim::GetAttr[name="drop_rate"](%5009)
      %7386 : bool = prim::GetAttr[name="training"](%5009)
      %7387 : bool = aten::lt(%7385, %16) # torch/nn/functional.py:968:7
      %7388 : bool = prim::If(%7387) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7389 : bool = aten::gt(%7385, %17) # torch/nn/functional.py:968:17
          -> (%7389)
       = prim::If(%7388) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7390 : Tensor = aten::dropout(%new_features.52, %7385, %7386) # torch/nn/functional.py:973:17
      -> (%7390)
    block1():
      -> (%new_features.52)
  %7391 : Tensor[] = aten::append(%features.1, %new_features.49) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7392 : Tensor = prim::Uninitialized()
  %7393 : bool = prim::GetAttr[name="memory_efficient"](%5010)
  %7394 : bool = prim::If(%7393) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7395 : bool = prim::Uninitialized()
      %7396 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7397 : bool = aten::gt(%7396, %24)
      %7398 : bool, %7399 : bool, %7400 : int = prim::Loop(%18, %7397, %19, %7395, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7401 : int, %7402 : bool, %7403 : bool, %7404 : int):
          %tensor.27 : Tensor = aten::__getitem__(%features.1, %7404) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7406 : bool = prim::requires_grad(%tensor.27)
          %7407 : bool, %7408 : bool = prim::If(%7406) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7395)
          %7409 : int = aten::add(%7404, %27)
          %7410 : bool = aten::lt(%7409, %7396)
          %7411 : bool = aten::__and__(%7410, %7407)
          -> (%7411, %7406, %7408, %7409)
      %7412 : bool = prim::If(%7398)
        block0():
          -> (%7399)
        block1():
          -> (%19)
      -> (%7412)
    block1():
      -> (%19)
  %bottleneck_output.52 : Tensor = prim::If(%7394) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7392)
    block1():
      %concated_features.27 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7415 : __torch__.torch.nn.modules.conv.___torch_mangle_336.Conv2d = prim::GetAttr[name="conv1"](%5010)
      %7416 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_243.BatchNorm2d = prim::GetAttr[name="norm1"](%5010)
      %7417 : int = aten::dim(%concated_features.27) # torch/nn/modules/batchnorm.py:276:11
      %7418 : bool = aten::ne(%7417, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7418) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7419 : bool = prim::GetAttr[name="training"](%7416)
       = prim::If(%7419) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7420 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7416)
          %7421 : Tensor = aten::add(%7420, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7416, %7421)
          -> ()
        block1():
          -> ()
      %7422 : bool = prim::GetAttr[name="training"](%7416)
      %7423 : Tensor = prim::GetAttr[name="running_mean"](%7416)
      %7424 : Tensor = prim::GetAttr[name="running_var"](%7416)
      %7425 : Tensor = prim::GetAttr[name="weight"](%7416)
      %7426 : Tensor = prim::GetAttr[name="bias"](%7416)
       = prim::If(%7422) # torch/nn/functional.py:2011:4
        block0():
          %7427 : int[] = aten::size(%concated_features.27) # torch/nn/functional.py:2012:27
          %size_prods.208 : int = aten::__getitem__(%7427, %24) # torch/nn/functional.py:1991:17
          %7429 : int = aten::len(%7427) # torch/nn/functional.py:1992:19
          %7430 : int = aten::sub(%7429, %26) # torch/nn/functional.py:1992:19
          %size_prods.209 : int = prim::Loop(%7430, %25, %size_prods.208) # torch/nn/functional.py:1992:4
            block0(%i.53 : int, %size_prods.210 : int):
              %7434 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
              %7435 : int = aten::__getitem__(%7427, %7434) # torch/nn/functional.py:1993:22
              %size_prods.211 : int = aten::mul(%size_prods.210, %7435) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.211)
          %7437 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7437) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7438 : Tensor = aten::batch_norm(%concated_features.27, %7425, %7426, %7423, %7424, %7422, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.53 : Tensor = aten::relu_(%7438) # torch/nn/functional.py:1117:17
      %7440 : Tensor = prim::GetAttr[name="weight"](%7415)
      %7441 : Tensor? = prim::GetAttr[name="bias"](%7415)
      %7442 : int[] = prim::ListConstruct(%27, %27)
      %7443 : int[] = prim::ListConstruct(%24, %24)
      %7444 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.53 : Tensor = aten::conv2d(%result.53, %7440, %7441, %7442, %7443, %7444, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.53)
  %7446 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5010)
  %7447 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5010)
  %7448 : int = aten::dim(%bottleneck_output.52) # torch/nn/modules/batchnorm.py:276:11
  %7449 : bool = aten::ne(%7448, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7449) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7450 : bool = prim::GetAttr[name="training"](%7447)
   = prim::If(%7450) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7451 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7447)
      %7452 : Tensor = aten::add(%7451, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7447, %7452)
      -> ()
    block1():
      -> ()
  %7453 : bool = prim::GetAttr[name="training"](%7447)
  %7454 : Tensor = prim::GetAttr[name="running_mean"](%7447)
  %7455 : Tensor = prim::GetAttr[name="running_var"](%7447)
  %7456 : Tensor = prim::GetAttr[name="weight"](%7447)
  %7457 : Tensor = prim::GetAttr[name="bias"](%7447)
   = prim::If(%7453) # torch/nn/functional.py:2011:4
    block0():
      %7458 : int[] = aten::size(%bottleneck_output.52) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%7458, %24) # torch/nn/functional.py:1991:17
      %7460 : int = aten::len(%7458) # torch/nn/functional.py:1992:19
      %7461 : int = aten::sub(%7460, %26) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%7461, %25, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %7465 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
          %7466 : int = aten::__getitem__(%7458, %7465) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %7466) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.215)
      %7468 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7468) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7469 : Tensor = aten::batch_norm(%bottleneck_output.52, %7456, %7457, %7454, %7455, %7453, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.54 : Tensor = aten::relu_(%7469) # torch/nn/functional.py:1117:17
  %7471 : Tensor = prim::GetAttr[name="weight"](%7446)
  %7472 : Tensor? = prim::GetAttr[name="bias"](%7446)
  %7473 : int[] = prim::ListConstruct(%27, %27)
  %7474 : int[] = prim::ListConstruct(%27, %27)
  %7475 : int[] = prim::ListConstruct(%27, %27)
  %new_features.54 : Tensor = aten::conv2d(%result.54, %7471, %7472, %7473, %7474, %7475, %27) # torch/nn/modules/conv.py:415:15
  %7477 : float = prim::GetAttr[name="drop_rate"](%5010)
  %7478 : bool = aten::gt(%7477, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.51 : Tensor = prim::If(%7478) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7480 : float = prim::GetAttr[name="drop_rate"](%5010)
      %7481 : bool = prim::GetAttr[name="training"](%5010)
      %7482 : bool = aten::lt(%7480, %16) # torch/nn/functional.py:968:7
      %7483 : bool = prim::If(%7482) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7484 : bool = aten::gt(%7480, %17) # torch/nn/functional.py:968:17
          -> (%7484)
       = prim::If(%7483) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7485 : Tensor = aten::dropout(%new_features.54, %7480, %7481) # torch/nn/functional.py:973:17
      -> (%7485)
    block1():
      -> (%new_features.54)
  %7486 : Tensor[] = aten::append(%features.1, %new_features.51) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7487 : Tensor = prim::Uninitialized()
  %7488 : bool = prim::GetAttr[name="memory_efficient"](%5011)
  %7489 : bool = prim::If(%7488) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7490 : bool = prim::Uninitialized()
      %7491 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7492 : bool = aten::gt(%7491, %24)
      %7493 : bool, %7494 : bool, %7495 : int = prim::Loop(%18, %7492, %19, %7490, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7496 : int, %7497 : bool, %7498 : bool, %7499 : int):
          %tensor.28 : Tensor = aten::__getitem__(%features.1, %7499) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7501 : bool = prim::requires_grad(%tensor.28)
          %7502 : bool, %7503 : bool = prim::If(%7501) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7490)
          %7504 : int = aten::add(%7499, %27)
          %7505 : bool = aten::lt(%7504, %7491)
          %7506 : bool = aten::__and__(%7505, %7502)
          -> (%7506, %7501, %7503, %7504)
      %7507 : bool = prim::If(%7493)
        block0():
          -> (%7494)
        block1():
          -> (%19)
      -> (%7507)
    block1():
      -> (%19)
  %bottleneck_output.54 : Tensor = prim::If(%7489) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7487)
    block1():
      %concated_features.28 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7510 : __torch__.torch.nn.modules.conv.___torch_mangle_339.Conv2d = prim::GetAttr[name="conv1"](%5011)
      %7511 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_338.BatchNorm2d = prim::GetAttr[name="norm1"](%5011)
      %7512 : int = aten::dim(%concated_features.28) # torch/nn/modules/batchnorm.py:276:11
      %7513 : bool = aten::ne(%7512, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7513) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7514 : bool = prim::GetAttr[name="training"](%7511)
       = prim::If(%7514) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7515 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7511)
          %7516 : Tensor = aten::add(%7515, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7511, %7516)
          -> ()
        block1():
          -> ()
      %7517 : bool = prim::GetAttr[name="training"](%7511)
      %7518 : Tensor = prim::GetAttr[name="running_mean"](%7511)
      %7519 : Tensor = prim::GetAttr[name="running_var"](%7511)
      %7520 : Tensor = prim::GetAttr[name="weight"](%7511)
      %7521 : Tensor = prim::GetAttr[name="bias"](%7511)
       = prim::If(%7517) # torch/nn/functional.py:2011:4
        block0():
          %7522 : int[] = aten::size(%concated_features.28) # torch/nn/functional.py:2012:27
          %size_prods.216 : int = aten::__getitem__(%7522, %24) # torch/nn/functional.py:1991:17
          %7524 : int = aten::len(%7522) # torch/nn/functional.py:1992:19
          %7525 : int = aten::sub(%7524, %26) # torch/nn/functional.py:1992:19
          %size_prods.217 : int = prim::Loop(%7525, %25, %size_prods.216) # torch/nn/functional.py:1992:4
            block0(%i.55 : int, %size_prods.218 : int):
              %7529 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
              %7530 : int = aten::__getitem__(%7522, %7529) # torch/nn/functional.py:1993:22
              %size_prods.219 : int = aten::mul(%size_prods.218, %7530) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.219)
          %7532 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7532) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7533 : Tensor = aten::batch_norm(%concated_features.28, %7520, %7521, %7518, %7519, %7517, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.55 : Tensor = aten::relu_(%7533) # torch/nn/functional.py:1117:17
      %7535 : Tensor = prim::GetAttr[name="weight"](%7510)
      %7536 : Tensor? = prim::GetAttr[name="bias"](%7510)
      %7537 : int[] = prim::ListConstruct(%27, %27)
      %7538 : int[] = prim::ListConstruct(%24, %24)
      %7539 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.55 : Tensor = aten::conv2d(%result.55, %7535, %7536, %7537, %7538, %7539, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.55)
  %7541 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5011)
  %7542 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5011)
  %7543 : int = aten::dim(%bottleneck_output.54) # torch/nn/modules/batchnorm.py:276:11
  %7544 : bool = aten::ne(%7543, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7544) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7545 : bool = prim::GetAttr[name="training"](%7542)
   = prim::If(%7545) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7546 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7542)
      %7547 : Tensor = aten::add(%7546, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7542, %7547)
      -> ()
    block1():
      -> ()
  %7548 : bool = prim::GetAttr[name="training"](%7542)
  %7549 : Tensor = prim::GetAttr[name="running_mean"](%7542)
  %7550 : Tensor = prim::GetAttr[name="running_var"](%7542)
  %7551 : Tensor = prim::GetAttr[name="weight"](%7542)
  %7552 : Tensor = prim::GetAttr[name="bias"](%7542)
   = prim::If(%7548) # torch/nn/functional.py:2011:4
    block0():
      %7553 : int[] = aten::size(%bottleneck_output.54) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%7553, %24) # torch/nn/functional.py:1991:17
      %7555 : int = aten::len(%7553) # torch/nn/functional.py:1992:19
      %7556 : int = aten::sub(%7555, %26) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%7556, %25, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %7560 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
          %7561 : int = aten::__getitem__(%7553, %7560) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %7561) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.223)
      %7563 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7563) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7564 : Tensor = aten::batch_norm(%bottleneck_output.54, %7551, %7552, %7549, %7550, %7548, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.56 : Tensor = aten::relu_(%7564) # torch/nn/functional.py:1117:17
  %7566 : Tensor = prim::GetAttr[name="weight"](%7541)
  %7567 : Tensor? = prim::GetAttr[name="bias"](%7541)
  %7568 : int[] = prim::ListConstruct(%27, %27)
  %7569 : int[] = prim::ListConstruct(%27, %27)
  %7570 : int[] = prim::ListConstruct(%27, %27)
  %new_features.56 : Tensor = aten::conv2d(%result.56, %7566, %7567, %7568, %7569, %7570, %27) # torch/nn/modules/conv.py:415:15
  %7572 : float = prim::GetAttr[name="drop_rate"](%5011)
  %7573 : bool = aten::gt(%7572, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.53 : Tensor = prim::If(%7573) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7575 : float = prim::GetAttr[name="drop_rate"](%5011)
      %7576 : bool = prim::GetAttr[name="training"](%5011)
      %7577 : bool = aten::lt(%7575, %16) # torch/nn/functional.py:968:7
      %7578 : bool = prim::If(%7577) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7579 : bool = aten::gt(%7575, %17) # torch/nn/functional.py:968:17
          -> (%7579)
       = prim::If(%7578) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7580 : Tensor = aten::dropout(%new_features.56, %7575, %7576) # torch/nn/functional.py:973:17
      -> (%7580)
    block1():
      -> (%new_features.56)
  %7581 : Tensor[] = aten::append(%features.1, %new_features.53) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7582 : Tensor = prim::Uninitialized()
  %7583 : bool = prim::GetAttr[name="memory_efficient"](%5012)
  %7584 : bool = prim::If(%7583) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7585 : bool = prim::Uninitialized()
      %7586 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7587 : bool = aten::gt(%7586, %24)
      %7588 : bool, %7589 : bool, %7590 : int = prim::Loop(%18, %7587, %19, %7585, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7591 : int, %7592 : bool, %7593 : bool, %7594 : int):
          %tensor.29 : Tensor = aten::__getitem__(%features.1, %7594) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7596 : bool = prim::requires_grad(%tensor.29)
          %7597 : bool, %7598 : bool = prim::If(%7596) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7585)
          %7599 : int = aten::add(%7594, %27)
          %7600 : bool = aten::lt(%7599, %7586)
          %7601 : bool = aten::__and__(%7600, %7597)
          -> (%7601, %7596, %7598, %7599)
      %7602 : bool = prim::If(%7588)
        block0():
          -> (%7589)
        block1():
          -> (%19)
      -> (%7602)
    block1():
      -> (%19)
  %bottleneck_output.56 : Tensor = prim::If(%7584) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7582)
    block1():
      %concated_features.29 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7605 : __torch__.torch.nn.modules.conv.___torch_mangle_342.Conv2d = prim::GetAttr[name="conv1"](%5012)
      %7606 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_341.BatchNorm2d = prim::GetAttr[name="norm1"](%5012)
      %7607 : int = aten::dim(%concated_features.29) # torch/nn/modules/batchnorm.py:276:11
      %7608 : bool = aten::ne(%7607, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7608) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7609 : bool = prim::GetAttr[name="training"](%7606)
       = prim::If(%7609) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7610 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7606)
          %7611 : Tensor = aten::add(%7610, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7606, %7611)
          -> ()
        block1():
          -> ()
      %7612 : bool = prim::GetAttr[name="training"](%7606)
      %7613 : Tensor = prim::GetAttr[name="running_mean"](%7606)
      %7614 : Tensor = prim::GetAttr[name="running_var"](%7606)
      %7615 : Tensor = prim::GetAttr[name="weight"](%7606)
      %7616 : Tensor = prim::GetAttr[name="bias"](%7606)
       = prim::If(%7612) # torch/nn/functional.py:2011:4
        block0():
          %7617 : int[] = aten::size(%concated_features.29) # torch/nn/functional.py:2012:27
          %size_prods.224 : int = aten::__getitem__(%7617, %24) # torch/nn/functional.py:1991:17
          %7619 : int = aten::len(%7617) # torch/nn/functional.py:1992:19
          %7620 : int = aten::sub(%7619, %26) # torch/nn/functional.py:1992:19
          %size_prods.225 : int = prim::Loop(%7620, %25, %size_prods.224) # torch/nn/functional.py:1992:4
            block0(%i.57 : int, %size_prods.226 : int):
              %7624 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
              %7625 : int = aten::__getitem__(%7617, %7624) # torch/nn/functional.py:1993:22
              %size_prods.227 : int = aten::mul(%size_prods.226, %7625) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.227)
          %7627 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7627) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7628 : Tensor = aten::batch_norm(%concated_features.29, %7615, %7616, %7613, %7614, %7612, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.57 : Tensor = aten::relu_(%7628) # torch/nn/functional.py:1117:17
      %7630 : Tensor = prim::GetAttr[name="weight"](%7605)
      %7631 : Tensor? = prim::GetAttr[name="bias"](%7605)
      %7632 : int[] = prim::ListConstruct(%27, %27)
      %7633 : int[] = prim::ListConstruct(%24, %24)
      %7634 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.57 : Tensor = aten::conv2d(%result.57, %7630, %7631, %7632, %7633, %7634, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.57)
  %7636 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5012)
  %7637 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5012)
  %7638 : int = aten::dim(%bottleneck_output.56) # torch/nn/modules/batchnorm.py:276:11
  %7639 : bool = aten::ne(%7638, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7639) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7640 : bool = prim::GetAttr[name="training"](%7637)
   = prim::If(%7640) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7641 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7637)
      %7642 : Tensor = aten::add(%7641, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7637, %7642)
      -> ()
    block1():
      -> ()
  %7643 : bool = prim::GetAttr[name="training"](%7637)
  %7644 : Tensor = prim::GetAttr[name="running_mean"](%7637)
  %7645 : Tensor = prim::GetAttr[name="running_var"](%7637)
  %7646 : Tensor = prim::GetAttr[name="weight"](%7637)
  %7647 : Tensor = prim::GetAttr[name="bias"](%7637)
   = prim::If(%7643) # torch/nn/functional.py:2011:4
    block0():
      %7648 : int[] = aten::size(%bottleneck_output.56) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%7648, %24) # torch/nn/functional.py:1991:17
      %7650 : int = aten::len(%7648) # torch/nn/functional.py:1992:19
      %7651 : int = aten::sub(%7650, %26) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%7651, %25, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %7655 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
          %7656 : int = aten::__getitem__(%7648, %7655) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %7656) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.231)
      %7658 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7658) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7659 : Tensor = aten::batch_norm(%bottleneck_output.56, %7646, %7647, %7644, %7645, %7643, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.58 : Tensor = aten::relu_(%7659) # torch/nn/functional.py:1117:17
  %7661 : Tensor = prim::GetAttr[name="weight"](%7636)
  %7662 : Tensor? = prim::GetAttr[name="bias"](%7636)
  %7663 : int[] = prim::ListConstruct(%27, %27)
  %7664 : int[] = prim::ListConstruct(%27, %27)
  %7665 : int[] = prim::ListConstruct(%27, %27)
  %new_features.58 : Tensor = aten::conv2d(%result.58, %7661, %7662, %7663, %7664, %7665, %27) # torch/nn/modules/conv.py:415:15
  %7667 : float = prim::GetAttr[name="drop_rate"](%5012)
  %7668 : bool = aten::gt(%7667, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.55 : Tensor = prim::If(%7668) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7670 : float = prim::GetAttr[name="drop_rate"](%5012)
      %7671 : bool = prim::GetAttr[name="training"](%5012)
      %7672 : bool = aten::lt(%7670, %16) # torch/nn/functional.py:968:7
      %7673 : bool = prim::If(%7672) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7674 : bool = aten::gt(%7670, %17) # torch/nn/functional.py:968:17
          -> (%7674)
       = prim::If(%7673) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7675 : Tensor = aten::dropout(%new_features.58, %7670, %7671) # torch/nn/functional.py:973:17
      -> (%7675)
    block1():
      -> (%new_features.58)
  %7676 : Tensor[] = aten::append(%features.1, %new_features.55) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7677 : Tensor = prim::Uninitialized()
  %7678 : bool = prim::GetAttr[name="memory_efficient"](%5013)
  %7679 : bool = prim::If(%7678) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7680 : bool = prim::Uninitialized()
      %7681 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7682 : bool = aten::gt(%7681, %24)
      %7683 : bool, %7684 : bool, %7685 : int = prim::Loop(%18, %7682, %19, %7680, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7686 : int, %7687 : bool, %7688 : bool, %7689 : int):
          %tensor.30 : Tensor = aten::__getitem__(%features.1, %7689) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7691 : bool = prim::requires_grad(%tensor.30)
          %7692 : bool, %7693 : bool = prim::If(%7691) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7680)
          %7694 : int = aten::add(%7689, %27)
          %7695 : bool = aten::lt(%7694, %7681)
          %7696 : bool = aten::__and__(%7695, %7692)
          -> (%7696, %7691, %7693, %7694)
      %7697 : bool = prim::If(%7683)
        block0():
          -> (%7684)
        block1():
          -> (%19)
      -> (%7697)
    block1():
      -> (%19)
  %bottleneck_output.58 : Tensor = prim::If(%7679) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7677)
    block1():
      %concated_features.30 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7700 : __torch__.torch.nn.modules.conv.___torch_mangle_344.Conv2d = prim::GetAttr[name="conv1"](%5013)
      %7701 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_249.BatchNorm2d = prim::GetAttr[name="norm1"](%5013)
      %7702 : int = aten::dim(%concated_features.30) # torch/nn/modules/batchnorm.py:276:11
      %7703 : bool = aten::ne(%7702, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7703) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7704 : bool = prim::GetAttr[name="training"](%7701)
       = prim::If(%7704) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7705 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7701)
          %7706 : Tensor = aten::add(%7705, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7701, %7706)
          -> ()
        block1():
          -> ()
      %7707 : bool = prim::GetAttr[name="training"](%7701)
      %7708 : Tensor = prim::GetAttr[name="running_mean"](%7701)
      %7709 : Tensor = prim::GetAttr[name="running_var"](%7701)
      %7710 : Tensor = prim::GetAttr[name="weight"](%7701)
      %7711 : Tensor = prim::GetAttr[name="bias"](%7701)
       = prim::If(%7707) # torch/nn/functional.py:2011:4
        block0():
          %7712 : int[] = aten::size(%concated_features.30) # torch/nn/functional.py:2012:27
          %size_prods.232 : int = aten::__getitem__(%7712, %24) # torch/nn/functional.py:1991:17
          %7714 : int = aten::len(%7712) # torch/nn/functional.py:1992:19
          %7715 : int = aten::sub(%7714, %26) # torch/nn/functional.py:1992:19
          %size_prods.233 : int = prim::Loop(%7715, %25, %size_prods.232) # torch/nn/functional.py:1992:4
            block0(%i.59 : int, %size_prods.234 : int):
              %7719 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
              %7720 : int = aten::__getitem__(%7712, %7719) # torch/nn/functional.py:1993:22
              %size_prods.235 : int = aten::mul(%size_prods.234, %7720) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.235)
          %7722 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7722) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7723 : Tensor = aten::batch_norm(%concated_features.30, %7710, %7711, %7708, %7709, %7707, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.59 : Tensor = aten::relu_(%7723) # torch/nn/functional.py:1117:17
      %7725 : Tensor = prim::GetAttr[name="weight"](%7700)
      %7726 : Tensor? = prim::GetAttr[name="bias"](%7700)
      %7727 : int[] = prim::ListConstruct(%27, %27)
      %7728 : int[] = prim::ListConstruct(%24, %24)
      %7729 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.59 : Tensor = aten::conv2d(%result.59, %7725, %7726, %7727, %7728, %7729, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.59)
  %7731 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5013)
  %7732 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5013)
  %7733 : int = aten::dim(%bottleneck_output.58) # torch/nn/modules/batchnorm.py:276:11
  %7734 : bool = aten::ne(%7733, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7734) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7735 : bool = prim::GetAttr[name="training"](%7732)
   = prim::If(%7735) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7736 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7732)
      %7737 : Tensor = aten::add(%7736, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7732, %7737)
      -> ()
    block1():
      -> ()
  %7738 : bool = prim::GetAttr[name="training"](%7732)
  %7739 : Tensor = prim::GetAttr[name="running_mean"](%7732)
  %7740 : Tensor = prim::GetAttr[name="running_var"](%7732)
  %7741 : Tensor = prim::GetAttr[name="weight"](%7732)
  %7742 : Tensor = prim::GetAttr[name="bias"](%7732)
   = prim::If(%7738) # torch/nn/functional.py:2011:4
    block0():
      %7743 : int[] = aten::size(%bottleneck_output.58) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%7743, %24) # torch/nn/functional.py:1991:17
      %7745 : int = aten::len(%7743) # torch/nn/functional.py:1992:19
      %7746 : int = aten::sub(%7745, %26) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%7746, %25, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %7750 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
          %7751 : int = aten::__getitem__(%7743, %7750) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %7751) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.239)
      %7753 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7753) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7754 : Tensor = aten::batch_norm(%bottleneck_output.58, %7741, %7742, %7739, %7740, %7738, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.60 : Tensor = aten::relu_(%7754) # torch/nn/functional.py:1117:17
  %7756 : Tensor = prim::GetAttr[name="weight"](%7731)
  %7757 : Tensor? = prim::GetAttr[name="bias"](%7731)
  %7758 : int[] = prim::ListConstruct(%27, %27)
  %7759 : int[] = prim::ListConstruct(%27, %27)
  %7760 : int[] = prim::ListConstruct(%27, %27)
  %new_features.60 : Tensor = aten::conv2d(%result.60, %7756, %7757, %7758, %7759, %7760, %27) # torch/nn/modules/conv.py:415:15
  %7762 : float = prim::GetAttr[name="drop_rate"](%5013)
  %7763 : bool = aten::gt(%7762, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.57 : Tensor = prim::If(%7763) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7765 : float = prim::GetAttr[name="drop_rate"](%5013)
      %7766 : bool = prim::GetAttr[name="training"](%5013)
      %7767 : bool = aten::lt(%7765, %16) # torch/nn/functional.py:968:7
      %7768 : bool = prim::If(%7767) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7769 : bool = aten::gt(%7765, %17) # torch/nn/functional.py:968:17
          -> (%7769)
       = prim::If(%7768) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7770 : Tensor = aten::dropout(%new_features.60, %7765, %7766) # torch/nn/functional.py:973:17
      -> (%7770)
    block1():
      -> (%new_features.60)
  %7771 : Tensor[] = aten::append(%features.1, %new_features.57) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7772 : Tensor = prim::Uninitialized()
  %7773 : bool = prim::GetAttr[name="memory_efficient"](%5014)
  %7774 : bool = prim::If(%7773) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7775 : bool = prim::Uninitialized()
      %7776 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7777 : bool = aten::gt(%7776, %24)
      %7778 : bool, %7779 : bool, %7780 : int = prim::Loop(%18, %7777, %19, %7775, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7781 : int, %7782 : bool, %7783 : bool, %7784 : int):
          %tensor.31 : Tensor = aten::__getitem__(%features.1, %7784) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7786 : bool = prim::requires_grad(%tensor.31)
          %7787 : bool, %7788 : bool = prim::If(%7786) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7775)
          %7789 : int = aten::add(%7784, %27)
          %7790 : bool = aten::lt(%7789, %7776)
          %7791 : bool = aten::__and__(%7790, %7787)
          -> (%7791, %7786, %7788, %7789)
      %7792 : bool = prim::If(%7778)
        block0():
          -> (%7779)
        block1():
          -> (%19)
      -> (%7792)
    block1():
      -> (%19)
  %bottleneck_output.60 : Tensor = prim::If(%7774) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7772)
    block1():
      %concated_features.31 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7795 : __torch__.torch.nn.modules.conv.___torch_mangle_347.Conv2d = prim::GetAttr[name="conv1"](%5014)
      %7796 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_346.BatchNorm2d = prim::GetAttr[name="norm1"](%5014)
      %7797 : int = aten::dim(%concated_features.31) # torch/nn/modules/batchnorm.py:276:11
      %7798 : bool = aten::ne(%7797, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7798) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7799 : bool = prim::GetAttr[name="training"](%7796)
       = prim::If(%7799) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7800 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7796)
          %7801 : Tensor = aten::add(%7800, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7796, %7801)
          -> ()
        block1():
          -> ()
      %7802 : bool = prim::GetAttr[name="training"](%7796)
      %7803 : Tensor = prim::GetAttr[name="running_mean"](%7796)
      %7804 : Tensor = prim::GetAttr[name="running_var"](%7796)
      %7805 : Tensor = prim::GetAttr[name="weight"](%7796)
      %7806 : Tensor = prim::GetAttr[name="bias"](%7796)
       = prim::If(%7802) # torch/nn/functional.py:2011:4
        block0():
          %7807 : int[] = aten::size(%concated_features.31) # torch/nn/functional.py:2012:27
          %size_prods.240 : int = aten::__getitem__(%7807, %24) # torch/nn/functional.py:1991:17
          %7809 : int = aten::len(%7807) # torch/nn/functional.py:1992:19
          %7810 : int = aten::sub(%7809, %26) # torch/nn/functional.py:1992:19
          %size_prods.241 : int = prim::Loop(%7810, %25, %size_prods.240) # torch/nn/functional.py:1992:4
            block0(%i.61 : int, %size_prods.242 : int):
              %7814 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
              %7815 : int = aten::__getitem__(%7807, %7814) # torch/nn/functional.py:1993:22
              %size_prods.243 : int = aten::mul(%size_prods.242, %7815) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.243)
          %7817 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7817) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7818 : Tensor = aten::batch_norm(%concated_features.31, %7805, %7806, %7803, %7804, %7802, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.61 : Tensor = aten::relu_(%7818) # torch/nn/functional.py:1117:17
      %7820 : Tensor = prim::GetAttr[name="weight"](%7795)
      %7821 : Tensor? = prim::GetAttr[name="bias"](%7795)
      %7822 : int[] = prim::ListConstruct(%27, %27)
      %7823 : int[] = prim::ListConstruct(%24, %24)
      %7824 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.61 : Tensor = aten::conv2d(%result.61, %7820, %7821, %7822, %7823, %7824, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.61)
  %7826 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5014)
  %7827 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5014)
  %7828 : int = aten::dim(%bottleneck_output.60) # torch/nn/modules/batchnorm.py:276:11
  %7829 : bool = aten::ne(%7828, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7829) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7830 : bool = prim::GetAttr[name="training"](%7827)
   = prim::If(%7830) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7831 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7827)
      %7832 : Tensor = aten::add(%7831, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7827, %7832)
      -> ()
    block1():
      -> ()
  %7833 : bool = prim::GetAttr[name="training"](%7827)
  %7834 : Tensor = prim::GetAttr[name="running_mean"](%7827)
  %7835 : Tensor = prim::GetAttr[name="running_var"](%7827)
  %7836 : Tensor = prim::GetAttr[name="weight"](%7827)
  %7837 : Tensor = prim::GetAttr[name="bias"](%7827)
   = prim::If(%7833) # torch/nn/functional.py:2011:4
    block0():
      %7838 : int[] = aten::size(%bottleneck_output.60) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%7838, %24) # torch/nn/functional.py:1991:17
      %7840 : int = aten::len(%7838) # torch/nn/functional.py:1992:19
      %7841 : int = aten::sub(%7840, %26) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%7841, %25, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %7845 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
          %7846 : int = aten::__getitem__(%7838, %7845) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %7846) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.247)
      %7848 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7848) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7849 : Tensor = aten::batch_norm(%bottleneck_output.60, %7836, %7837, %7834, %7835, %7833, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.62 : Tensor = aten::relu_(%7849) # torch/nn/functional.py:1117:17
  %7851 : Tensor = prim::GetAttr[name="weight"](%7826)
  %7852 : Tensor? = prim::GetAttr[name="bias"](%7826)
  %7853 : int[] = prim::ListConstruct(%27, %27)
  %7854 : int[] = prim::ListConstruct(%27, %27)
  %7855 : int[] = prim::ListConstruct(%27, %27)
  %new_features.62 : Tensor = aten::conv2d(%result.62, %7851, %7852, %7853, %7854, %7855, %27) # torch/nn/modules/conv.py:415:15
  %7857 : float = prim::GetAttr[name="drop_rate"](%5014)
  %7858 : bool = aten::gt(%7857, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.59 : Tensor = prim::If(%7858) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7860 : float = prim::GetAttr[name="drop_rate"](%5014)
      %7861 : bool = prim::GetAttr[name="training"](%5014)
      %7862 : bool = aten::lt(%7860, %16) # torch/nn/functional.py:968:7
      %7863 : bool = prim::If(%7862) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7864 : bool = aten::gt(%7860, %17) # torch/nn/functional.py:968:17
          -> (%7864)
       = prim::If(%7863) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7865 : Tensor = aten::dropout(%new_features.62, %7860, %7861) # torch/nn/functional.py:973:17
      -> (%7865)
    block1():
      -> (%new_features.62)
  %7866 : Tensor[] = aten::append(%features.1, %new_features.59) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7867 : Tensor = prim::Uninitialized()
  %7868 : bool = prim::GetAttr[name="memory_efficient"](%5015)
  %7869 : bool = prim::If(%7868) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7870 : bool = prim::Uninitialized()
      %7871 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7872 : bool = aten::gt(%7871, %24)
      %7873 : bool, %7874 : bool, %7875 : int = prim::Loop(%18, %7872, %19, %7870, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7876 : int, %7877 : bool, %7878 : bool, %7879 : int):
          %tensor.32 : Tensor = aten::__getitem__(%features.1, %7879) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7881 : bool = prim::requires_grad(%tensor.32)
          %7882 : bool, %7883 : bool = prim::If(%7881) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7870)
          %7884 : int = aten::add(%7879, %27)
          %7885 : bool = aten::lt(%7884, %7871)
          %7886 : bool = aten::__and__(%7885, %7882)
          -> (%7886, %7881, %7883, %7884)
      %7887 : bool = prim::If(%7873)
        block0():
          -> (%7874)
        block1():
          -> (%19)
      -> (%7887)
    block1():
      -> (%19)
  %bottleneck_output.62 : Tensor = prim::If(%7869) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7867)
    block1():
      %concated_features.32 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7890 : __torch__.torch.nn.modules.conv.___torch_mangle_350.Conv2d = prim::GetAttr[name="conv1"](%5015)
      %7891 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_349.BatchNorm2d = prim::GetAttr[name="norm1"](%5015)
      %7892 : int = aten::dim(%concated_features.32) # torch/nn/modules/batchnorm.py:276:11
      %7893 : bool = aten::ne(%7892, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7893) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7894 : bool = prim::GetAttr[name="training"](%7891)
       = prim::If(%7894) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7895 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7891)
          %7896 : Tensor = aten::add(%7895, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7891, %7896)
          -> ()
        block1():
          -> ()
      %7897 : bool = prim::GetAttr[name="training"](%7891)
      %7898 : Tensor = prim::GetAttr[name="running_mean"](%7891)
      %7899 : Tensor = prim::GetAttr[name="running_var"](%7891)
      %7900 : Tensor = prim::GetAttr[name="weight"](%7891)
      %7901 : Tensor = prim::GetAttr[name="bias"](%7891)
       = prim::If(%7897) # torch/nn/functional.py:2011:4
        block0():
          %7902 : int[] = aten::size(%concated_features.32) # torch/nn/functional.py:2012:27
          %size_prods.248 : int = aten::__getitem__(%7902, %24) # torch/nn/functional.py:1991:17
          %7904 : int = aten::len(%7902) # torch/nn/functional.py:1992:19
          %7905 : int = aten::sub(%7904, %26) # torch/nn/functional.py:1992:19
          %size_prods.249 : int = prim::Loop(%7905, %25, %size_prods.248) # torch/nn/functional.py:1992:4
            block0(%i.63 : int, %size_prods.250 : int):
              %7909 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
              %7910 : int = aten::__getitem__(%7902, %7909) # torch/nn/functional.py:1993:22
              %size_prods.251 : int = aten::mul(%size_prods.250, %7910) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.251)
          %7912 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7912) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7913 : Tensor = aten::batch_norm(%concated_features.32, %7900, %7901, %7898, %7899, %7897, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.63 : Tensor = aten::relu_(%7913) # torch/nn/functional.py:1117:17
      %7915 : Tensor = prim::GetAttr[name="weight"](%7890)
      %7916 : Tensor? = prim::GetAttr[name="bias"](%7890)
      %7917 : int[] = prim::ListConstruct(%27, %27)
      %7918 : int[] = prim::ListConstruct(%24, %24)
      %7919 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.63 : Tensor = aten::conv2d(%result.63, %7915, %7916, %7917, %7918, %7919, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.63)
  %7921 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5015)
  %7922 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5015)
  %7923 : int = aten::dim(%bottleneck_output.62) # torch/nn/modules/batchnorm.py:276:11
  %7924 : bool = aten::ne(%7923, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7924) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7925 : bool = prim::GetAttr[name="training"](%7922)
   = prim::If(%7925) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7926 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7922)
      %7927 : Tensor = aten::add(%7926, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7922, %7927)
      -> ()
    block1():
      -> ()
  %7928 : bool = prim::GetAttr[name="training"](%7922)
  %7929 : Tensor = prim::GetAttr[name="running_mean"](%7922)
  %7930 : Tensor = prim::GetAttr[name="running_var"](%7922)
  %7931 : Tensor = prim::GetAttr[name="weight"](%7922)
  %7932 : Tensor = prim::GetAttr[name="bias"](%7922)
   = prim::If(%7928) # torch/nn/functional.py:2011:4
    block0():
      %7933 : int[] = aten::size(%bottleneck_output.62) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%7933, %24) # torch/nn/functional.py:1991:17
      %7935 : int = aten::len(%7933) # torch/nn/functional.py:1992:19
      %7936 : int = aten::sub(%7935, %26) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%7936, %25, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %7940 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
          %7941 : int = aten::__getitem__(%7933, %7940) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %7941) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.255)
      %7943 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7943) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7944 : Tensor = aten::batch_norm(%bottleneck_output.62, %7931, %7932, %7929, %7930, %7928, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.64 : Tensor = aten::relu_(%7944) # torch/nn/functional.py:1117:17
  %7946 : Tensor = prim::GetAttr[name="weight"](%7921)
  %7947 : Tensor? = prim::GetAttr[name="bias"](%7921)
  %7948 : int[] = prim::ListConstruct(%27, %27)
  %7949 : int[] = prim::ListConstruct(%27, %27)
  %7950 : int[] = prim::ListConstruct(%27, %27)
  %new_features.64 : Tensor = aten::conv2d(%result.64, %7946, %7947, %7948, %7949, %7950, %27) # torch/nn/modules/conv.py:415:15
  %7952 : float = prim::GetAttr[name="drop_rate"](%5015)
  %7953 : bool = aten::gt(%7952, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.61 : Tensor = prim::If(%7953) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7955 : float = prim::GetAttr[name="drop_rate"](%5015)
      %7956 : bool = prim::GetAttr[name="training"](%5015)
      %7957 : bool = aten::lt(%7955, %16) # torch/nn/functional.py:968:7
      %7958 : bool = prim::If(%7957) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7959 : bool = aten::gt(%7955, %17) # torch/nn/functional.py:968:17
          -> (%7959)
       = prim::If(%7958) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7960 : Tensor = aten::dropout(%new_features.64, %7955, %7956) # torch/nn/functional.py:973:17
      -> (%7960)
    block1():
      -> (%new_features.64)
  %7961 : Tensor[] = aten::append(%features.1, %new_features.61) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7962 : Tensor = prim::Uninitialized()
  %7963 : bool = prim::GetAttr[name="memory_efficient"](%5016)
  %7964 : bool = prim::If(%7963) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7965 : bool = prim::Uninitialized()
      %7966 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7967 : bool = aten::gt(%7966, %24)
      %7968 : bool, %7969 : bool, %7970 : int = prim::Loop(%18, %7967, %19, %7965, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7971 : int, %7972 : bool, %7973 : bool, %7974 : int):
          %tensor.1 : Tensor = aten::__getitem__(%features.1, %7974) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7976 : bool = prim::requires_grad(%tensor.1)
          %7977 : bool, %7978 : bool = prim::If(%7976) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7965)
          %7979 : int = aten::add(%7974, %27)
          %7980 : bool = aten::lt(%7979, %7966)
          %7981 : bool = aten::__and__(%7980, %7977)
          -> (%7981, %7976, %7978, %7979)
      %7982 : bool = prim::If(%7968)
        block0():
          -> (%7969)
        block1():
          -> (%19)
      -> (%7982)
    block1():
      -> (%19)
  %bottleneck_output : Tensor = prim::If(%7964) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7962)
    block1():
      %concated_features.1 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7985 : __torch__.torch.nn.modules.conv.___torch_mangle_352.Conv2d = prim::GetAttr[name="conv1"](%5016)
      %7986 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_255.BatchNorm2d = prim::GetAttr[name="norm1"](%5016)
      %7987 : int = aten::dim(%concated_features.1) # torch/nn/modules/batchnorm.py:276:11
      %7988 : bool = aten::ne(%7987, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7988) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7989 : bool = prim::GetAttr[name="training"](%7986)
       = prim::If(%7989) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7990 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7986)
          %7991 : Tensor = aten::add(%7990, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7986, %7991)
          -> ()
        block1():
          -> ()
      %7992 : bool = prim::GetAttr[name="training"](%7986)
      %7993 : Tensor = prim::GetAttr[name="running_mean"](%7986)
      %7994 : Tensor = prim::GetAttr[name="running_var"](%7986)
      %7995 : Tensor = prim::GetAttr[name="weight"](%7986)
      %7996 : Tensor = prim::GetAttr[name="bias"](%7986)
       = prim::If(%7992) # torch/nn/functional.py:2011:4
        block0():
          %7997 : int[] = aten::size(%concated_features.1) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%7997, %24) # torch/nn/functional.py:1991:17
          %7999 : int = aten::len(%7997) # torch/nn/functional.py:1992:19
          %8000 : int = aten::sub(%7999, %26) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%8000, %25, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %8004 : int = aten::add(%i.2, %26) # torch/nn/functional.py:1993:27
              %8005 : int = aten::__getitem__(%7997, %8004) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %8005) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.5)
          %8007 : bool = aten::eq(%size_prods.4, %27) # torch/nn/functional.py:1994:7
           = prim::If(%8007) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %8008 : Tensor = aten::batch_norm(%concated_features.1, %7995, %7996, %7993, %7994, %7992, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.166 : Tensor = aten::relu_(%8008) # torch/nn/functional.py:1117:17
      %8010 : Tensor = prim::GetAttr[name="weight"](%7985)
      %8011 : Tensor? = prim::GetAttr[name="bias"](%7985)
      %8012 : int[] = prim::ListConstruct(%27, %27)
      %8013 : int[] = prim::ListConstruct(%24, %24)
      %8014 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.2 : Tensor = aten::conv2d(%result.166, %8010, %8011, %8012, %8013, %8014, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.2)
  %8016 : __torch__.torch.nn.modules.conv.___torch_mangle_72.Conv2d = prim::GetAttr[name="conv2"](%5016)
  %8017 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="norm2"](%5016)
  %8018 : int = aten::dim(%bottleneck_output) # torch/nn/modules/batchnorm.py:276:11
  %8019 : bool = aten::ne(%8018, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8019) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8020 : bool = prim::GetAttr[name="training"](%8017)
   = prim::If(%8020) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8021 : Tensor = prim::GetAttr[name="num_batches_tracked"](%8017)
      %8022 : Tensor = aten::add(%8021, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%8017, %8022)
      -> ()
    block1():
      -> ()
  %8023 : bool = prim::GetAttr[name="training"](%8017)
  %8024 : Tensor = prim::GetAttr[name="running_mean"](%8017)
  %8025 : Tensor = prim::GetAttr[name="running_var"](%8017)
  %8026 : Tensor = prim::GetAttr[name="weight"](%8017)
  %8027 : Tensor = prim::GetAttr[name="bias"](%8017)
   = prim::If(%8023) # torch/nn/functional.py:2011:4
    block0():
      %8028 : int[] = aten::size(%bottleneck_output) # torch/nn/functional.py:2012:27
      %size_prods.672 : int = aten::__getitem__(%8028, %24) # torch/nn/functional.py:1991:17
      %8030 : int = aten::len(%8028) # torch/nn/functional.py:1992:19
      %8031 : int = aten::sub(%8030, %26) # torch/nn/functional.py:1992:19
      %size_prods.673 : int = prim::Loop(%8031, %25, %size_prods.672) # torch/nn/functional.py:1992:4
        block0(%i.169 : int, %size_prods.674 : int):
          %8035 : int = aten::add(%i.169, %26) # torch/nn/functional.py:1993:27
          %8036 : int = aten::__getitem__(%8028, %8035) # torch/nn/functional.py:1993:22
          %size_prods.675 : int = aten::mul(%size_prods.674, %8036) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.675)
      %8038 : bool = aten::eq(%size_prods.673, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8038) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %8039 : Tensor = aten::batch_norm(%bottleneck_output, %8026, %8027, %8024, %8025, %8023, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.165 : Tensor = aten::relu_(%8039) # torch/nn/functional.py:1117:17
  %8041 : Tensor = prim::GetAttr[name="weight"](%8016)
  %8042 : Tensor? = prim::GetAttr[name="bias"](%8016)
  %8043 : int[] = prim::ListConstruct(%27, %27)
  %8044 : int[] = prim::ListConstruct(%27, %27)
  %8045 : int[] = prim::ListConstruct(%27, %27)
  %new_features.1 : Tensor = aten::conv2d(%result.165, %8041, %8042, %8043, %8044, %8045, %27) # torch/nn/modules/conv.py:415:15
  %8047 : float = prim::GetAttr[name="drop_rate"](%5016)
  %8048 : bool = aten::gt(%8047, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.63 : Tensor = prim::If(%8048) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %8050 : float = prim::GetAttr[name="drop_rate"](%5016)
      %8051 : bool = prim::GetAttr[name="training"](%5016)
      %8052 : bool = aten::lt(%8050, %16) # torch/nn/functional.py:968:7
      %8053 : bool = prim::If(%8052) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %8054 : bool = aten::gt(%8050, %17) # torch/nn/functional.py:968:17
          -> (%8054)
       = prim::If(%8053) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %8055 : Tensor = aten::dropout(%new_features.1, %8050, %8051) # torch/nn/functional.py:973:17
      -> (%8055)
    block1():
      -> (%new_features.1)
  %8056 : Tensor[] = aten::append(%features.1, %new_features.63) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %8058 : int = aten::dim(%input.23) # torch/nn/modules/batchnorm.py:276:11
  %8059 : bool = aten::ne(%8058, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%8059) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %8060 : bool = prim::GetAttr[name="training"](%38)
   = prim::If(%8060) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %8061 : Tensor = prim::GetAttr[name="num_batches_tracked"](%38)
      %8062 : Tensor = aten::add(%8061, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%38, %8062)
      -> ()
    block1():
      -> ()
  %8063 : bool = prim::GetAttr[name="training"](%38)
  %8064 : Tensor = prim::GetAttr[name="running_mean"](%38)
  %8065 : Tensor = prim::GetAttr[name="running_var"](%38)
  %8066 : Tensor = prim::GetAttr[name="weight"](%38)
  %8067 : Tensor = prim::GetAttr[name="bias"](%38)
   = prim::If(%8063) # torch/nn/functional.py:2011:4
    block0():
      %8068 : int[] = aten::size(%input.23) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%8068, %24) # torch/nn/functional.py:1991:17
      %8070 : int = aten::len(%8068) # torch/nn/functional.py:1992:19
      %8071 : int = aten::sub(%8070, %26) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%8071, %25, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %8075 : int = aten::add(%i.1, %26) # torch/nn/functional.py:1993:27
          %8076 : int = aten::__getitem__(%8068, %8075) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %8076) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.3)
      %8078 : bool = aten::eq(%size_prods, %27) # torch/nn/functional.py:1994:7
       = prim::If(%8078) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %features.5 : Tensor = aten::batch_norm(%input.23, %8066, %8067, %8064, %8065, %8063, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %out.1 : Tensor = prim::If(%5) # torch/nn/functional.py:1116:4
    block0():
      %result.1 : Tensor = aten::relu_(%features.5) # torch/nn/functional.py:1117:17
      -> (%result.1)
    block1():
      %result.2 : Tensor = aten::relu(%features.5) # torch/nn/functional.py:1119:17
      -> (%result.2)
  %10 : int[] = prim::ListConstruct(%6, %6)
  %8083 : str = prim::Constant[value="Exception"]() # <string>:5:2
  %8084 : int[] = aten::size(%out.1) # torch/nn/functional.py:925:51
  %8085 : int = aten::len(%8084) # <string>:5:9
  %8086 : int = aten::len(%10) # <string>:5:25
  %8087 : bool = aten::gt(%8085, %8086) # <string>:5:9
   = prim::If(%8087) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%8083) # <string>:5:2
      -> ()
  %out.3 : Tensor = aten::adaptive_avg_pool2d(%out.1, %10) # torch/nn/functional.py:926:11
  %out.5 : Tensor = aten::flatten(%out.3, %6, %2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:195:14
  %13 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name="classifier"](%self)
  %8089 : int = prim::Constant[value=1]()
  %8090 : int = prim::Constant[value=2]() # torch/nn/functional.py:1672:22
  %8091 : Tensor = prim::GetAttr[name="weight"](%13)
  %8092 : Tensor = prim::GetAttr[name="bias"](%13)
  %8093 : int = aten::dim(%out.5) # torch/nn/functional.py:1672:7
  %8094 : bool = aten::eq(%8093, %8090) # torch/nn/functional.py:1672:7
  %out.7 : Tensor = prim::If(%8094) # torch/nn/functional.py:1672:4
    block0():
      %8096 : Tensor = aten::t(%8091) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%8092, %out.5, %8096, %8089, %8089) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %8098 : Tensor = aten::t(%8091) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%out.5, %8098) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %8092, %8089) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%out.7)
