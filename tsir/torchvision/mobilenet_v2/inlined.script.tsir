graph(%self : __torch__.torchvision.models.mobilenet.MobileNetV2,
      %x.1 : Tensor):
  %3 : float = prim::Constant[value=0.20000000000000001]() # torch/nn/modules/dropout.py:58:32
  %4 : float = prim::Constant[value=6.]() # torch/nn/modules/activation.py:237:47
  %5 : float = prim::Constant[value=0.]() # torch/nn/modules/activation.py:237:33
  %6 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %8 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %9 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %10 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %11 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %12 : int = prim::Constant[value=32]() # torch/nn/modules/conv.py:414:53
  %13 : int = prim::Constant[value=96]() # torch/nn/modules/conv.py:414:53
  %14 : int = prim::Constant[value=144]() # torch/nn/modules/conv.py:414:53
  %15 : int = prim::Constant[value=192]() # torch/nn/modules/conv.py:414:53
  %16 : int = prim::Constant[value=384]() # torch/nn/modules/conv.py:414:53
  %17 : int = prim::Constant[value=576]() # torch/nn/modules/conv.py:414:53
  %18 : int = prim::Constant[value=960]() # torch/nn/modules/conv.py:414:53
  %19 : int = prim::Constant[value=0]() # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:166:68
  %20 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:166:49
  %21 : int = prim::Constant[value=-1]() # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:166:72
  %22 : __torch__.torch.nn.modules.container.___torch_mangle_936.Sequential = prim::GetAttr[name="features"](%self)
  %23 : __torch__.torchvision.models.mobilenet.ConvBNReLU = prim::GetAttr[name="0"](%22)
  %24 : __torch__.torchvision.models.mobilenet.InvertedResidual = prim::GetAttr[name="1"](%22)
  %25 : __torch__.torchvision.models.mobilenet.___torch_mangle_889.InvertedResidual = prim::GetAttr[name="2"](%22)
  %26 : __torch__.torchvision.models.mobilenet.___torch_mangle_895.InvertedResidual = prim::GetAttr[name="3"](%22)
  %27 : __torch__.torchvision.models.mobilenet.___torch_mangle_900.InvertedResidual = prim::GetAttr[name="4"](%22)
  %28 : __torch__.torchvision.models.mobilenet.___torch_mangle_905.InvertedResidual = prim::GetAttr[name="5"](%22)
  %29 : __torch__.torchvision.models.mobilenet.___torch_mangle_905.InvertedResidual = prim::GetAttr[name="6"](%22)
  %30 : __torch__.torchvision.models.mobilenet.___torch_mangle_909.InvertedResidual = prim::GetAttr[name="7"](%22)
  %31 : __torch__.torchvision.models.mobilenet.___torch_mangle_913.InvertedResidual = prim::GetAttr[name="8"](%22)
  %32 : __torch__.torchvision.models.mobilenet.___torch_mangle_913.InvertedResidual = prim::GetAttr[name="9"](%22)
  %33 : __torch__.torchvision.models.mobilenet.___torch_mangle_913.InvertedResidual = prim::GetAttr[name="10"](%22)
  %34 : __torch__.torchvision.models.mobilenet.___torch_mangle_916.InvertedResidual = prim::GetAttr[name="11"](%22)
  %35 : __torch__.torchvision.models.mobilenet.___torch_mangle_920.InvertedResidual = prim::GetAttr[name="12"](%22)
  %36 : __torch__.torchvision.models.mobilenet.___torch_mangle_920.InvertedResidual = prim::GetAttr[name="13"](%22)
  %37 : __torch__.torchvision.models.mobilenet.___torch_mangle_924.InvertedResidual = prim::GetAttr[name="14"](%22)
  %38 : __torch__.torchvision.models.mobilenet.___torch_mangle_931.InvertedResidual = prim::GetAttr[name="15"](%22)
  %39 : __torch__.torchvision.models.mobilenet.___torch_mangle_931.InvertedResidual = prim::GetAttr[name="16"](%22)
  %40 : __torch__.torchvision.models.mobilenet.___torch_mangle_934.InvertedResidual = prim::GetAttr[name="17"](%22)
  %41 : __torch__.torchvision.models.mobilenet.___torch_mangle_935.ConvBNReLU = prim::GetAttr[name="18"](%22)
  %42 : __torch__.torch.nn.modules.conv.___torch_mangle_761.Conv2d = prim::GetAttr[name="0"](%23)
  %43 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="1"](%23)
  %44 : Tensor = prim::GetAttr[name="weight"](%42)
  %45 : Tensor? = prim::GetAttr[name="bias"](%42)
  %46 : int[] = prim::ListConstruct(%11, %11)
  %47 : int[] = prim::ListConstruct(%20, %20)
  %48 : int[] = prim::ListConstruct(%20, %20)
  %input.41 : Tensor = aten::conv2d(%x.1, %44, %45, %46, %47, %48, %20) # torch/nn/modules/conv.py:415:15
  %50 : int = aten::dim(%input.41) # torch/nn/modules/batchnorm.py:276:11
  %51 : bool = aten::ne(%50, %8) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%51) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %52 : bool = prim::GetAttr[name="training"](%43)
   = prim::If(%52) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %53 : Tensor = prim::GetAttr[name="num_batches_tracked"](%43)
      %54 : Tensor = aten::add(%53, %20, %20) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%43, %54)
      -> ()
    block1():
      -> ()
  %55 : bool = prim::GetAttr[name="training"](%43)
  %56 : Tensor = prim::GetAttr[name="running_mean"](%43)
  %57 : Tensor = prim::GetAttr[name="running_var"](%43)
  %58 : Tensor = prim::GetAttr[name="weight"](%43)
  %59 : Tensor = prim::GetAttr[name="bias"](%43)
   = prim::If(%55) # torch/nn/functional.py:2011:4
    block0():
      %60 : int[] = aten::size(%input.41) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%60, %19) # torch/nn/functional.py:1991:17
      %62 : int = aten::len(%60) # torch/nn/functional.py:1992:19
      %63 : int = aten::sub(%62, %11) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%63, %10, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %67 : int = aten::add(%i.7, %11) # torch/nn/functional.py:1993:27
          %68 : int = aten::__getitem__(%60, %67) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %68) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.27)
      %70 : bool = aten::eq(%size_prods.25, %20) # torch/nn/functional.py:1994:7
       = prim::If(%70) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.20 : Tensor = aten::batch_norm(%input.41, %58, %59, %56, %57, %55, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
  %input.22 : Tensor = aten::hardtanh_(%input.20, %5, %4) # torch/nn/functional.py:1171:17
  %73 : bool = prim::GetAttr[name="use_res_connect"](%24)
  %input.24 : Tensor = prim::If(%73) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %75 : __torch__.torch.nn.modules.container.___torch_mangle_881.Sequential = prim::GetAttr[name="conv"](%24)
      %76 : __torch__.torchvision.models.mobilenet.___torch_mangle_879.ConvBNReLU = prim::GetAttr[name="0"](%75)
      %77 : __torch__.torch.nn.modules.conv.___torch_mangle_763.Conv2d = prim::GetAttr[name="1"](%75)
      %78 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_880.BatchNorm2d = prim::GetAttr[name="2"](%75)
      %79 : __torch__.torch.nn.modules.conv.___torch_mangle_762.Conv2d = prim::GetAttr[name="0"](%76)
      %80 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="1"](%76)
      %81 : Tensor = prim::GetAttr[name="weight"](%79)
      %82 : Tensor? = prim::GetAttr[name="bias"](%79)
      %83 : int[] = prim::ListConstruct(%20, %20)
      %84 : int[] = prim::ListConstruct(%20, %20)
      %85 : int[] = prim::ListConstruct(%20, %20)
      %input.32 : Tensor = aten::conv2d(%input.22, %81, %82, %83, %84, %85, %12) # torch/nn/modules/conv.py:415:15
      %87 : int = aten::dim(%input.32) # torch/nn/modules/batchnorm.py:276:11
      %88 : bool = aten::ne(%87, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%88) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %89 : bool = prim::GetAttr[name="training"](%80)
       = prim::If(%89) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %90 : Tensor = prim::GetAttr[name="num_batches_tracked"](%80)
          %91 : Tensor = aten::add(%90, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%80, %91)
          -> ()
        block1():
          -> ()
      %92 : bool = prim::GetAttr[name="training"](%80)
      %93 : Tensor = prim::GetAttr[name="running_mean"](%80)
      %94 : Tensor = prim::GetAttr[name="running_var"](%80)
      %95 : Tensor = prim::GetAttr[name="weight"](%80)
      %96 : Tensor = prim::GetAttr[name="bias"](%80)
       = prim::If(%92) # torch/nn/functional.py:2011:4
        block0():
          %97 : int[] = aten::size(%input.32) # torch/nn/functional.py:2012:27
          %size_prods.36 : int = aten::__getitem__(%97, %19) # torch/nn/functional.py:1991:17
          %99 : int = aten::len(%97) # torch/nn/functional.py:1992:19
          %100 : int = aten::sub(%99, %11) # torch/nn/functional.py:1992:19
          %size_prods.37 : int = prim::Loop(%100, %10, %size_prods.36) # torch/nn/functional.py:1992:4
            block0(%i.10 : int, %size_prods.38 : int):
              %104 : int = aten::add(%i.10, %11) # torch/nn/functional.py:1993:27
              %105 : int = aten::__getitem__(%97, %104) # torch/nn/functional.py:1993:22
              %size_prods.39 : int = aten::mul(%size_prods.38, %105) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.39)
          %107 : bool = aten::eq(%size_prods.37, %20) # torch/nn/functional.py:1994:7
           = prim::If(%107) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.49 : Tensor = aten::batch_norm(%input.32, %95, %96, %93, %94, %92, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.43 : Tensor = aten::hardtanh_(%input.49, %5, %4) # torch/nn/functional.py:1171:17
      %110 : Tensor = prim::GetAttr[name="weight"](%77)
      %111 : Tensor? = prim::GetAttr[name="bias"](%77)
      %112 : int[] = prim::ListConstruct(%20, %20)
      %113 : int[] = prim::ListConstruct(%19, %19)
      %114 : int[] = prim::ListConstruct(%20, %20)
      %input.42 : Tensor = aten::conv2d(%input.43, %110, %111, %112, %113, %114, %20) # torch/nn/modules/conv.py:415:15
      %116 : int = aten::dim(%input.42) # torch/nn/modules/batchnorm.py:276:11
      %117 : bool = aten::ne(%116, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%117) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %118 : bool = prim::GetAttr[name="training"](%78)
       = prim::If(%118) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %119 : Tensor = prim::GetAttr[name="num_batches_tracked"](%78)
          %120 : Tensor = aten::add(%119, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%78, %120)
          -> ()
        block1():
          -> ()
      %121 : bool = prim::GetAttr[name="training"](%78)
      %122 : Tensor = prim::GetAttr[name="running_mean"](%78)
      %123 : Tensor = prim::GetAttr[name="running_var"](%78)
      %124 : Tensor = prim::GetAttr[name="weight"](%78)
      %125 : Tensor = prim::GetAttr[name="bias"](%78)
       = prim::If(%121) # torch/nn/functional.py:2011:4
        block0():
          %126 : int[] = aten::size(%input.42) # torch/nn/functional.py:2012:27
          %size_prods.28 : int = aten::__getitem__(%126, %19) # torch/nn/functional.py:1991:17
          %128 : int = aten::len(%126) # torch/nn/functional.py:1992:19
          %129 : int = aten::sub(%128, %11) # torch/nn/functional.py:1992:19
          %size_prods.29 : int = prim::Loop(%129, %10, %size_prods.28) # torch/nn/functional.py:1992:4
            block0(%i.8 : int, %size_prods.30 : int):
              %133 : int = aten::add(%i.8, %11) # torch/nn/functional.py:1993:27
              %134 : int = aten::__getitem__(%126, %133) # torch/nn/functional.py:1993:22
              %size_prods.31 : int = aten::mul(%size_prods.30, %134) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.31)
          %136 : bool = aten::eq(%size_prods.29, %20) # torch/nn/functional.py:1994:7
           = prim::If(%136) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.28 : Tensor = aten::batch_norm(%input.42, %124, %125, %122, %123, %121, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %138 : Tensor = aten::add(%input.22, %input.28, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%138)
    block1():
      %139 : __torch__.torch.nn.modules.container.___torch_mangle_881.Sequential = prim::GetAttr[name="conv"](%24)
      %140 : __torch__.torchvision.models.mobilenet.___torch_mangle_879.ConvBNReLU = prim::GetAttr[name="0"](%139)
      %141 : __torch__.torch.nn.modules.conv.___torch_mangle_763.Conv2d = prim::GetAttr[name="1"](%139)
      %142 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_880.BatchNorm2d = prim::GetAttr[name="2"](%139)
      %143 : __torch__.torch.nn.modules.conv.___torch_mangle_762.Conv2d = prim::GetAttr[name="0"](%140)
      %144 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="1"](%140)
      %145 : Tensor = prim::GetAttr[name="weight"](%143)
      %146 : Tensor? = prim::GetAttr[name="bias"](%143)
      %147 : int[] = prim::ListConstruct(%20, %20)
      %148 : int[] = prim::ListConstruct(%20, %20)
      %149 : int[] = prim::ListConstruct(%20, %20)
      %input.45 : Tensor = aten::conv2d(%input.22, %145, %146, %147, %148, %149, %12) # torch/nn/modules/conv.py:415:15
      %151 : int = aten::dim(%input.45) # torch/nn/modules/batchnorm.py:276:11
      %152 : bool = aten::ne(%151, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%152) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %153 : bool = prim::GetAttr[name="training"](%144)
       = prim::If(%153) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %154 : Tensor = prim::GetAttr[name="num_batches_tracked"](%144)
          %155 : Tensor = aten::add(%154, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%144, %155)
          -> ()
        block1():
          -> ()
      %156 : bool = prim::GetAttr[name="training"](%144)
      %157 : Tensor = prim::GetAttr[name="running_mean"](%144)
      %158 : Tensor = prim::GetAttr[name="running_var"](%144)
      %159 : Tensor = prim::GetAttr[name="weight"](%144)
      %160 : Tensor = prim::GetAttr[name="bias"](%144)
       = prim::If(%156) # torch/nn/functional.py:2011:4
        block0():
          %161 : int[] = aten::size(%input.45) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%161, %19) # torch/nn/functional.py:1991:17
          %163 : int = aten::len(%161) # torch/nn/functional.py:1992:19
          %164 : int = aten::sub(%163, %11) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%164, %10, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %168 : int = aten::add(%i.9, %11) # torch/nn/functional.py:1993:27
              %169 : int = aten::__getitem__(%161, %168) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %169) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.35)
          %171 : bool = aten::eq(%size_prods.33, %20) # torch/nn/functional.py:1994:7
           = prim::If(%171) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.46 : Tensor = aten::batch_norm(%input.45, %159, %160, %157, %158, %156, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.44 : Tensor = aten::hardtanh_(%input.46, %5, %4) # torch/nn/functional.py:1171:17
      %174 : Tensor = prim::GetAttr[name="weight"](%141)
      %175 : Tensor? = prim::GetAttr[name="bias"](%141)
      %176 : int[] = prim::ListConstruct(%20, %20)
      %177 : int[] = prim::ListConstruct(%19, %19)
      %178 : int[] = prim::ListConstruct(%20, %20)
      %input.47 : Tensor = aten::conv2d(%input.44, %174, %175, %176, %177, %178, %20) # torch/nn/modules/conv.py:415:15
      %180 : int = aten::dim(%input.47) # torch/nn/modules/batchnorm.py:276:11
      %181 : bool = aten::ne(%180, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%181) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %182 : bool = prim::GetAttr[name="training"](%142)
       = prim::If(%182) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %183 : Tensor = prim::GetAttr[name="num_batches_tracked"](%142)
          %184 : Tensor = aten::add(%183, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%142, %184)
          -> ()
        block1():
          -> ()
      %185 : bool = prim::GetAttr[name="training"](%142)
      %186 : Tensor = prim::GetAttr[name="running_mean"](%142)
      %187 : Tensor = prim::GetAttr[name="running_var"](%142)
      %188 : Tensor = prim::GetAttr[name="weight"](%142)
      %189 : Tensor = prim::GetAttr[name="bias"](%142)
       = prim::If(%185) # torch/nn/functional.py:2011:4
        block0():
          %190 : int[] = aten::size(%input.47) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%190, %19) # torch/nn/functional.py:1991:17
          %192 : int = aten::len(%190) # torch/nn/functional.py:1992:19
          %193 : int = aten::sub(%192, %11) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%193, %10, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %197 : int = aten::add(%i.11, %11) # torch/nn/functional.py:1993:27
              %198 : int = aten::__getitem__(%190, %197) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %198) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.43)
          %200 : bool = aten::eq(%size_prods.41, %20) # torch/nn/functional.py:1994:7
           = prim::If(%200) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.48 : Tensor = aten::batch_norm(%input.47, %188, %189, %186, %187, %185, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.48)
  %202 : bool = prim::GetAttr[name="use_res_connect"](%25)
  %input.26 : Tensor = prim::If(%202) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %204 : __torch__.torch.nn.modules.container.___torch_mangle_888.Sequential = prim::GetAttr[name="conv"](%25)
      %205 : __torch__.torchvision.models.mobilenet.___torch_mangle_883.ConvBNReLU = prim::GetAttr[name="0"](%204)
      %206 : __torch__.torchvision.models.mobilenet.___torch_mangle_885.ConvBNReLU = prim::GetAttr[name="1"](%204)
      %207 : __torch__.torch.nn.modules.conv.___torch_mangle_886.Conv2d = prim::GetAttr[name="2"](%204)
      %208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_887.BatchNorm2d = prim::GetAttr[name="3"](%204)
      %209 : __torch__.torch.nn.modules.conv.___torch_mangle_882.Conv2d = prim::GetAttr[name="0"](%205)
      %210 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="1"](%205)
      %211 : Tensor = prim::GetAttr[name="weight"](%209)
      %212 : Tensor? = prim::GetAttr[name="bias"](%209)
      %213 : int[] = prim::ListConstruct(%20, %20)
      %214 : int[] = prim::ListConstruct(%19, %19)
      %215 : int[] = prim::ListConstruct(%20, %20)
      %input.50 : Tensor = aten::conv2d(%input.24, %211, %212, %213, %214, %215, %20) # torch/nn/modules/conv.py:415:15
      %217 : int = aten::dim(%input.50) # torch/nn/modules/batchnorm.py:276:11
      %218 : bool = aten::ne(%217, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%218) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %219 : bool = prim::GetAttr[name="training"](%210)
       = prim::If(%219) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %220 : Tensor = prim::GetAttr[name="num_batches_tracked"](%210)
          %221 : Tensor = aten::add(%220, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%210, %221)
          -> ()
        block1():
          -> ()
      %222 : bool = prim::GetAttr[name="training"](%210)
      %223 : Tensor = prim::GetAttr[name="running_mean"](%210)
      %224 : Tensor = prim::GetAttr[name="running_var"](%210)
      %225 : Tensor = prim::GetAttr[name="weight"](%210)
      %226 : Tensor = prim::GetAttr[name="bias"](%210)
       = prim::If(%222) # torch/nn/functional.py:2011:4
        block0():
          %227 : int[] = aten::size(%input.50) # torch/nn/functional.py:2012:27
          %size_prods.44 : int = aten::__getitem__(%227, %19) # torch/nn/functional.py:1991:17
          %229 : int = aten::len(%227) # torch/nn/functional.py:1992:19
          %230 : int = aten::sub(%229, %11) # torch/nn/functional.py:1992:19
          %size_prods.45 : int = prim::Loop(%230, %10, %size_prods.44) # torch/nn/functional.py:1992:4
            block0(%i.12 : int, %size_prods.46 : int):
              %234 : int = aten::add(%i.12, %11) # torch/nn/functional.py:1993:27
              %235 : int = aten::__getitem__(%227, %234) # torch/nn/functional.py:1993:22
              %size_prods.47 : int = aten::mul(%size_prods.46, %235) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.47)
          %237 : bool = aten::eq(%size_prods.45, %20) # torch/nn/functional.py:1994:7
           = prim::If(%237) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.51 : Tensor = aten::batch_norm(%input.50, %225, %226, %223, %224, %222, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.52 : Tensor = aten::hardtanh_(%input.51, %5, %4) # torch/nn/functional.py:1171:17
      %240 : __torch__.torch.nn.modules.conv.___torch_mangle_884.Conv2d = prim::GetAttr[name="0"](%206)
      %241 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="1"](%206)
      %242 : Tensor = prim::GetAttr[name="weight"](%240)
      %243 : Tensor? = prim::GetAttr[name="bias"](%240)
      %244 : int[] = prim::ListConstruct(%11, %11)
      %245 : int[] = prim::ListConstruct(%20, %20)
      %246 : int[] = prim::ListConstruct(%20, %20)
      %input.53 : Tensor = aten::conv2d(%input.52, %242, %243, %244, %245, %246, %13) # torch/nn/modules/conv.py:415:15
      %248 : int = aten::dim(%input.53) # torch/nn/modules/batchnorm.py:276:11
      %249 : bool = aten::ne(%248, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%249) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %250 : bool = prim::GetAttr[name="training"](%241)
       = prim::If(%250) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %251 : Tensor = prim::GetAttr[name="num_batches_tracked"](%241)
          %252 : Tensor = aten::add(%251, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%241, %252)
          -> ()
        block1():
          -> ()
      %253 : bool = prim::GetAttr[name="training"](%241)
      %254 : Tensor = prim::GetAttr[name="running_mean"](%241)
      %255 : Tensor = prim::GetAttr[name="running_var"](%241)
      %256 : Tensor = prim::GetAttr[name="weight"](%241)
      %257 : Tensor = prim::GetAttr[name="bias"](%241)
       = prim::If(%253) # torch/nn/functional.py:2011:4
        block0():
          %258 : int[] = aten::size(%input.53) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%258, %19) # torch/nn/functional.py:1991:17
          %260 : int = aten::len(%258) # torch/nn/functional.py:1992:19
          %261 : int = aten::sub(%260, %11) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%261, %10, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %265 : int = aten::add(%i.13, %11) # torch/nn/functional.py:1993:27
              %266 : int = aten::__getitem__(%258, %265) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %266) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.51)
          %268 : bool = aten::eq(%size_prods.49, %20) # torch/nn/functional.py:1994:7
           = prim::If(%268) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.54 : Tensor = aten::batch_norm(%input.53, %256, %257, %254, %255, %253, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.55 : Tensor = aten::hardtanh_(%input.54, %5, %4) # torch/nn/functional.py:1171:17
      %271 : Tensor = prim::GetAttr[name="weight"](%207)
      %272 : Tensor? = prim::GetAttr[name="bias"](%207)
      %273 : int[] = prim::ListConstruct(%20, %20)
      %274 : int[] = prim::ListConstruct(%19, %19)
      %275 : int[] = prim::ListConstruct(%20, %20)
      %input.56 : Tensor = aten::conv2d(%input.55, %271, %272, %273, %274, %275, %20) # torch/nn/modules/conv.py:415:15
      %277 : int = aten::dim(%input.56) # torch/nn/modules/batchnorm.py:276:11
      %278 : bool = aten::ne(%277, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%278) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %279 : bool = prim::GetAttr[name="training"](%208)
       = prim::If(%279) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %280 : Tensor = prim::GetAttr[name="num_batches_tracked"](%208)
          %281 : Tensor = aten::add(%280, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%208, %281)
          -> ()
        block1():
          -> ()
      %282 : bool = prim::GetAttr[name="training"](%208)
      %283 : Tensor = prim::GetAttr[name="running_mean"](%208)
      %284 : Tensor = prim::GetAttr[name="running_var"](%208)
      %285 : Tensor = prim::GetAttr[name="weight"](%208)
      %286 : Tensor = prim::GetAttr[name="bias"](%208)
       = prim::If(%282) # torch/nn/functional.py:2011:4
        block0():
          %287 : int[] = aten::size(%input.56) # torch/nn/functional.py:2012:27
          %size_prods.52 : int = aten::__getitem__(%287, %19) # torch/nn/functional.py:1991:17
          %289 : int = aten::len(%287) # torch/nn/functional.py:1992:19
          %290 : int = aten::sub(%289, %11) # torch/nn/functional.py:1992:19
          %size_prods.53 : int = prim::Loop(%290, %10, %size_prods.52) # torch/nn/functional.py:1992:4
            block0(%i.14 : int, %size_prods.54 : int):
              %294 : int = aten::add(%i.14, %11) # torch/nn/functional.py:1993:27
              %295 : int = aten::__getitem__(%287, %294) # torch/nn/functional.py:1993:22
              %size_prods.55 : int = aten::mul(%size_prods.54, %295) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.55)
          %297 : bool = aten::eq(%size_prods.53, %20) # torch/nn/functional.py:1994:7
           = prim::If(%297) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.57 : Tensor = aten::batch_norm(%input.56, %285, %286, %283, %284, %282, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %299 : Tensor = aten::add(%input.24, %input.57, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%299)
    block1():
      %300 : __torch__.torch.nn.modules.container.___torch_mangle_888.Sequential = prim::GetAttr[name="conv"](%25)
      %301 : __torch__.torchvision.models.mobilenet.___torch_mangle_883.ConvBNReLU = prim::GetAttr[name="0"](%300)
      %302 : __torch__.torchvision.models.mobilenet.___torch_mangle_885.ConvBNReLU = prim::GetAttr[name="1"](%300)
      %303 : __torch__.torch.nn.modules.conv.___torch_mangle_886.Conv2d = prim::GetAttr[name="2"](%300)
      %304 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_887.BatchNorm2d = prim::GetAttr[name="3"](%300)
      %305 : __torch__.torch.nn.modules.conv.___torch_mangle_882.Conv2d = prim::GetAttr[name="0"](%301)
      %306 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="1"](%301)
      %307 : Tensor = prim::GetAttr[name="weight"](%305)
      %308 : Tensor? = prim::GetAttr[name="bias"](%305)
      %309 : int[] = prim::ListConstruct(%20, %20)
      %310 : int[] = prim::ListConstruct(%19, %19)
      %311 : int[] = prim::ListConstruct(%20, %20)
      %input.58 : Tensor = aten::conv2d(%input.24, %307, %308, %309, %310, %311, %20) # torch/nn/modules/conv.py:415:15
      %313 : int = aten::dim(%input.58) # torch/nn/modules/batchnorm.py:276:11
      %314 : bool = aten::ne(%313, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%314) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %315 : bool = prim::GetAttr[name="training"](%306)
       = prim::If(%315) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %316 : Tensor = prim::GetAttr[name="num_batches_tracked"](%306)
          %317 : Tensor = aten::add(%316, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%306, %317)
          -> ()
        block1():
          -> ()
      %318 : bool = prim::GetAttr[name="training"](%306)
      %319 : Tensor = prim::GetAttr[name="running_mean"](%306)
      %320 : Tensor = prim::GetAttr[name="running_var"](%306)
      %321 : Tensor = prim::GetAttr[name="weight"](%306)
      %322 : Tensor = prim::GetAttr[name="bias"](%306)
       = prim::If(%318) # torch/nn/functional.py:2011:4
        block0():
          %323 : int[] = aten::size(%input.58) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%323, %19) # torch/nn/functional.py:1991:17
          %325 : int = aten::len(%323) # torch/nn/functional.py:1992:19
          %326 : int = aten::sub(%325, %11) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%326, %10, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %330 : int = aten::add(%i.15, %11) # torch/nn/functional.py:1993:27
              %331 : int = aten::__getitem__(%323, %330) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %331) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.59)
          %333 : bool = aten::eq(%size_prods.57, %20) # torch/nn/functional.py:1994:7
           = prim::If(%333) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.59 : Tensor = aten::batch_norm(%input.58, %321, %322, %319, %320, %318, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.60 : Tensor = aten::hardtanh_(%input.59, %5, %4) # torch/nn/functional.py:1171:17
      %336 : __torch__.torch.nn.modules.conv.___torch_mangle_884.Conv2d = prim::GetAttr[name="0"](%302)
      %337 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="1"](%302)
      %338 : Tensor = prim::GetAttr[name="weight"](%336)
      %339 : Tensor? = prim::GetAttr[name="bias"](%336)
      %340 : int[] = prim::ListConstruct(%11, %11)
      %341 : int[] = prim::ListConstruct(%20, %20)
      %342 : int[] = prim::ListConstruct(%20, %20)
      %input.61 : Tensor = aten::conv2d(%input.60, %338, %339, %340, %341, %342, %13) # torch/nn/modules/conv.py:415:15
      %344 : int = aten::dim(%input.61) # torch/nn/modules/batchnorm.py:276:11
      %345 : bool = aten::ne(%344, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%345) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %346 : bool = prim::GetAttr[name="training"](%337)
       = prim::If(%346) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %347 : Tensor = prim::GetAttr[name="num_batches_tracked"](%337)
          %348 : Tensor = aten::add(%347, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%337, %348)
          -> ()
        block1():
          -> ()
      %349 : bool = prim::GetAttr[name="training"](%337)
      %350 : Tensor = prim::GetAttr[name="running_mean"](%337)
      %351 : Tensor = prim::GetAttr[name="running_var"](%337)
      %352 : Tensor = prim::GetAttr[name="weight"](%337)
      %353 : Tensor = prim::GetAttr[name="bias"](%337)
       = prim::If(%349) # torch/nn/functional.py:2011:4
        block0():
          %354 : int[] = aten::size(%input.61) # torch/nn/functional.py:2012:27
          %size_prods.60 : int = aten::__getitem__(%354, %19) # torch/nn/functional.py:1991:17
          %356 : int = aten::len(%354) # torch/nn/functional.py:1992:19
          %357 : int = aten::sub(%356, %11) # torch/nn/functional.py:1992:19
          %size_prods.61 : int = prim::Loop(%357, %10, %size_prods.60) # torch/nn/functional.py:1992:4
            block0(%i.16 : int, %size_prods.62 : int):
              %361 : int = aten::add(%i.16, %11) # torch/nn/functional.py:1993:27
              %362 : int = aten::__getitem__(%354, %361) # torch/nn/functional.py:1993:22
              %size_prods.63 : int = aten::mul(%size_prods.62, %362) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.63)
          %364 : bool = aten::eq(%size_prods.61, %20) # torch/nn/functional.py:1994:7
           = prim::If(%364) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.62 : Tensor = aten::batch_norm(%input.61, %352, %353, %350, %351, %349, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.63 : Tensor = aten::hardtanh_(%input.62, %5, %4) # torch/nn/functional.py:1171:17
      %367 : Tensor = prim::GetAttr[name="weight"](%303)
      %368 : Tensor? = prim::GetAttr[name="bias"](%303)
      %369 : int[] = prim::ListConstruct(%20, %20)
      %370 : int[] = prim::ListConstruct(%19, %19)
      %371 : int[] = prim::ListConstruct(%20, %20)
      %input.64 : Tensor = aten::conv2d(%input.63, %367, %368, %369, %370, %371, %20) # torch/nn/modules/conv.py:415:15
      %373 : int = aten::dim(%input.64) # torch/nn/modules/batchnorm.py:276:11
      %374 : bool = aten::ne(%373, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%374) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %375 : bool = prim::GetAttr[name="training"](%304)
       = prim::If(%375) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %376 : Tensor = prim::GetAttr[name="num_batches_tracked"](%304)
          %377 : Tensor = aten::add(%376, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%304, %377)
          -> ()
        block1():
          -> ()
      %378 : bool = prim::GetAttr[name="training"](%304)
      %379 : Tensor = prim::GetAttr[name="running_mean"](%304)
      %380 : Tensor = prim::GetAttr[name="running_var"](%304)
      %381 : Tensor = prim::GetAttr[name="weight"](%304)
      %382 : Tensor = prim::GetAttr[name="bias"](%304)
       = prim::If(%378) # torch/nn/functional.py:2011:4
        block0():
          %383 : int[] = aten::size(%input.64) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%383, %19) # torch/nn/functional.py:1991:17
          %385 : int = aten::len(%383) # torch/nn/functional.py:1992:19
          %386 : int = aten::sub(%385, %11) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%386, %10, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %390 : int = aten::add(%i.17, %11) # torch/nn/functional.py:1993:27
              %391 : int = aten::__getitem__(%383, %390) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %391) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.67)
          %393 : bool = aten::eq(%size_prods.65, %20) # torch/nn/functional.py:1994:7
           = prim::If(%393) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.65 : Tensor = aten::batch_norm(%input.64, %381, %382, %379, %380, %378, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.65)
  %395 : bool = prim::GetAttr[name="use_res_connect"](%26)
  %input.38 : Tensor = prim::If(%395) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %397 : __torch__.torch.nn.modules.container.___torch_mangle_894.Sequential = prim::GetAttr[name="conv"](%26)
      %398 : __torch__.torchvision.models.mobilenet.___torch_mangle_890.ConvBNReLU = prim::GetAttr[name="0"](%397)
      %399 : __torch__.torchvision.models.mobilenet.___torch_mangle_892.ConvBNReLU = prim::GetAttr[name="1"](%397)
      %400 : __torch__.torch.nn.modules.conv.___torch_mangle_893.Conv2d = prim::GetAttr[name="2"](%397)
      %401 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_887.BatchNorm2d = prim::GetAttr[name="3"](%397)
      %402 : __torch__.torch.nn.modules.conv.___torch_mangle_650.Conv2d = prim::GetAttr[name="0"](%398)
      %403 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%398)
      %404 : Tensor = prim::GetAttr[name="weight"](%402)
      %405 : Tensor? = prim::GetAttr[name="bias"](%402)
      %406 : int[] = prim::ListConstruct(%20, %20)
      %407 : int[] = prim::ListConstruct(%19, %19)
      %408 : int[] = prim::ListConstruct(%20, %20)
      %input.66 : Tensor = aten::conv2d(%input.26, %404, %405, %406, %407, %408, %20) # torch/nn/modules/conv.py:415:15
      %410 : int = aten::dim(%input.66) # torch/nn/modules/batchnorm.py:276:11
      %411 : bool = aten::ne(%410, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%411) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %412 : bool = prim::GetAttr[name="training"](%403)
       = prim::If(%412) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %413 : Tensor = prim::GetAttr[name="num_batches_tracked"](%403)
          %414 : Tensor = aten::add(%413, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%403, %414)
          -> ()
        block1():
          -> ()
      %415 : bool = prim::GetAttr[name="training"](%403)
      %416 : Tensor = prim::GetAttr[name="running_mean"](%403)
      %417 : Tensor = prim::GetAttr[name="running_var"](%403)
      %418 : Tensor = prim::GetAttr[name="weight"](%403)
      %419 : Tensor = prim::GetAttr[name="bias"](%403)
       = prim::If(%415) # torch/nn/functional.py:2011:4
        block0():
          %420 : int[] = aten::size(%input.66) # torch/nn/functional.py:2012:27
          %size_prods.68 : int = aten::__getitem__(%420, %19) # torch/nn/functional.py:1991:17
          %422 : int = aten::len(%420) # torch/nn/functional.py:1992:19
          %423 : int = aten::sub(%422, %11) # torch/nn/functional.py:1992:19
          %size_prods.69 : int = prim::Loop(%423, %10, %size_prods.68) # torch/nn/functional.py:1992:4
            block0(%i.18 : int, %size_prods.70 : int):
              %427 : int = aten::add(%i.18, %11) # torch/nn/functional.py:1993:27
              %428 : int = aten::__getitem__(%420, %427) # torch/nn/functional.py:1993:22
              %size_prods.71 : int = aten::mul(%size_prods.70, %428) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.71)
          %430 : bool = aten::eq(%size_prods.69, %20) # torch/nn/functional.py:1994:7
           = prim::If(%430) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.67 : Tensor = aten::batch_norm(%input.66, %418, %419, %416, %417, %415, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.68 : Tensor = aten::hardtanh_(%input.67, %5, %4) # torch/nn/functional.py:1171:17
      %433 : __torch__.torch.nn.modules.conv.___torch_mangle_891.Conv2d = prim::GetAttr[name="0"](%399)
      %434 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%399)
      %435 : Tensor = prim::GetAttr[name="weight"](%433)
      %436 : Tensor? = prim::GetAttr[name="bias"](%433)
      %437 : int[] = prim::ListConstruct(%20, %20)
      %438 : int[] = prim::ListConstruct(%20, %20)
      %439 : int[] = prim::ListConstruct(%20, %20)
      %input.69 : Tensor = aten::conv2d(%input.68, %435, %436, %437, %438, %439, %14) # torch/nn/modules/conv.py:415:15
      %441 : int = aten::dim(%input.69) # torch/nn/modules/batchnorm.py:276:11
      %442 : bool = aten::ne(%441, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%442) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %443 : bool = prim::GetAttr[name="training"](%434)
       = prim::If(%443) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %444 : Tensor = prim::GetAttr[name="num_batches_tracked"](%434)
          %445 : Tensor = aten::add(%444, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%434, %445)
          -> ()
        block1():
          -> ()
      %446 : bool = prim::GetAttr[name="training"](%434)
      %447 : Tensor = prim::GetAttr[name="running_mean"](%434)
      %448 : Tensor = prim::GetAttr[name="running_var"](%434)
      %449 : Tensor = prim::GetAttr[name="weight"](%434)
      %450 : Tensor = prim::GetAttr[name="bias"](%434)
       = prim::If(%446) # torch/nn/functional.py:2011:4
        block0():
          %451 : int[] = aten::size(%input.69) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%451, %19) # torch/nn/functional.py:1991:17
          %453 : int = aten::len(%451) # torch/nn/functional.py:1992:19
          %454 : int = aten::sub(%453, %11) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%454, %10, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %458 : int = aten::add(%i.19, %11) # torch/nn/functional.py:1993:27
              %459 : int = aten::__getitem__(%451, %458) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %459) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.75)
          %461 : bool = aten::eq(%size_prods.73, %20) # torch/nn/functional.py:1994:7
           = prim::If(%461) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.70 : Tensor = aten::batch_norm(%input.69, %449, %450, %447, %448, %446, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.71 : Tensor = aten::hardtanh_(%input.70, %5, %4) # torch/nn/functional.py:1171:17
      %464 : Tensor = prim::GetAttr[name="weight"](%400)
      %465 : Tensor? = prim::GetAttr[name="bias"](%400)
      %466 : int[] = prim::ListConstruct(%20, %20)
      %467 : int[] = prim::ListConstruct(%19, %19)
      %468 : int[] = prim::ListConstruct(%20, %20)
      %input.72 : Tensor = aten::conv2d(%input.71, %464, %465, %466, %467, %468, %20) # torch/nn/modules/conv.py:415:15
      %470 : int = aten::dim(%input.72) # torch/nn/modules/batchnorm.py:276:11
      %471 : bool = aten::ne(%470, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%471) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %472 : bool = prim::GetAttr[name="training"](%401)
       = prim::If(%472) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %473 : Tensor = prim::GetAttr[name="num_batches_tracked"](%401)
          %474 : Tensor = aten::add(%473, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%401, %474)
          -> ()
        block1():
          -> ()
      %475 : bool = prim::GetAttr[name="training"](%401)
      %476 : Tensor = prim::GetAttr[name="running_mean"](%401)
      %477 : Tensor = prim::GetAttr[name="running_var"](%401)
      %478 : Tensor = prim::GetAttr[name="weight"](%401)
      %479 : Tensor = prim::GetAttr[name="bias"](%401)
       = prim::If(%475) # torch/nn/functional.py:2011:4
        block0():
          %480 : int[] = aten::size(%input.72) # torch/nn/functional.py:2012:27
          %size_prods.76 : int = aten::__getitem__(%480, %19) # torch/nn/functional.py:1991:17
          %482 : int = aten::len(%480) # torch/nn/functional.py:1992:19
          %483 : int = aten::sub(%482, %11) # torch/nn/functional.py:1992:19
          %size_prods.77 : int = prim::Loop(%483, %10, %size_prods.76) # torch/nn/functional.py:1992:4
            block0(%i.20 : int, %size_prods.78 : int):
              %487 : int = aten::add(%i.20, %11) # torch/nn/functional.py:1993:27
              %488 : int = aten::__getitem__(%480, %487) # torch/nn/functional.py:1993:22
              %size_prods.79 : int = aten::mul(%size_prods.78, %488) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.79)
          %490 : bool = aten::eq(%size_prods.77, %20) # torch/nn/functional.py:1994:7
           = prim::If(%490) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.73 : Tensor = aten::batch_norm(%input.72, %478, %479, %476, %477, %475, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %492 : Tensor = aten::add(%input.26, %input.73, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%492)
    block1():
      %493 : __torch__.torch.nn.modules.container.___torch_mangle_894.Sequential = prim::GetAttr[name="conv"](%26)
      %494 : __torch__.torchvision.models.mobilenet.___torch_mangle_890.ConvBNReLU = prim::GetAttr[name="0"](%493)
      %495 : __torch__.torchvision.models.mobilenet.___torch_mangle_892.ConvBNReLU = prim::GetAttr[name="1"](%493)
      %496 : __torch__.torch.nn.modules.conv.___torch_mangle_893.Conv2d = prim::GetAttr[name="2"](%493)
      %497 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_887.BatchNorm2d = prim::GetAttr[name="3"](%493)
      %498 : __torch__.torch.nn.modules.conv.___torch_mangle_650.Conv2d = prim::GetAttr[name="0"](%494)
      %499 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%494)
      %500 : Tensor = prim::GetAttr[name="weight"](%498)
      %501 : Tensor? = prim::GetAttr[name="bias"](%498)
      %502 : int[] = prim::ListConstruct(%20, %20)
      %503 : int[] = prim::ListConstruct(%19, %19)
      %504 : int[] = prim::ListConstruct(%20, %20)
      %input.74 : Tensor = aten::conv2d(%input.26, %500, %501, %502, %503, %504, %20) # torch/nn/modules/conv.py:415:15
      %506 : int = aten::dim(%input.74) # torch/nn/modules/batchnorm.py:276:11
      %507 : bool = aten::ne(%506, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%507) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %508 : bool = prim::GetAttr[name="training"](%499)
       = prim::If(%508) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %509 : Tensor = prim::GetAttr[name="num_batches_tracked"](%499)
          %510 : Tensor = aten::add(%509, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%499, %510)
          -> ()
        block1():
          -> ()
      %511 : bool = prim::GetAttr[name="training"](%499)
      %512 : Tensor = prim::GetAttr[name="running_mean"](%499)
      %513 : Tensor = prim::GetAttr[name="running_var"](%499)
      %514 : Tensor = prim::GetAttr[name="weight"](%499)
      %515 : Tensor = prim::GetAttr[name="bias"](%499)
       = prim::If(%511) # torch/nn/functional.py:2011:4
        block0():
          %516 : int[] = aten::size(%input.74) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%516, %19) # torch/nn/functional.py:1991:17
          %518 : int = aten::len(%516) # torch/nn/functional.py:1992:19
          %519 : int = aten::sub(%518, %11) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%519, %10, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %523 : int = aten::add(%i.21, %11) # torch/nn/functional.py:1993:27
              %524 : int = aten::__getitem__(%516, %523) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %524) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.83)
          %526 : bool = aten::eq(%size_prods.81, %20) # torch/nn/functional.py:1994:7
           = prim::If(%526) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.75 : Tensor = aten::batch_norm(%input.74, %514, %515, %512, %513, %511, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.76 : Tensor = aten::hardtanh_(%input.75, %5, %4) # torch/nn/functional.py:1171:17
      %529 : __torch__.torch.nn.modules.conv.___torch_mangle_891.Conv2d = prim::GetAttr[name="0"](%495)
      %530 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%495)
      %531 : Tensor = prim::GetAttr[name="weight"](%529)
      %532 : Tensor? = prim::GetAttr[name="bias"](%529)
      %533 : int[] = prim::ListConstruct(%20, %20)
      %534 : int[] = prim::ListConstruct(%20, %20)
      %535 : int[] = prim::ListConstruct(%20, %20)
      %input.77 : Tensor = aten::conv2d(%input.76, %531, %532, %533, %534, %535, %14) # torch/nn/modules/conv.py:415:15
      %537 : int = aten::dim(%input.77) # torch/nn/modules/batchnorm.py:276:11
      %538 : bool = aten::ne(%537, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%538) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %539 : bool = prim::GetAttr[name="training"](%530)
       = prim::If(%539) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %540 : Tensor = prim::GetAttr[name="num_batches_tracked"](%530)
          %541 : Tensor = aten::add(%540, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%530, %541)
          -> ()
        block1():
          -> ()
      %542 : bool = prim::GetAttr[name="training"](%530)
      %543 : Tensor = prim::GetAttr[name="running_mean"](%530)
      %544 : Tensor = prim::GetAttr[name="running_var"](%530)
      %545 : Tensor = prim::GetAttr[name="weight"](%530)
      %546 : Tensor = prim::GetAttr[name="bias"](%530)
       = prim::If(%542) # torch/nn/functional.py:2011:4
        block0():
          %547 : int[] = aten::size(%input.77) # torch/nn/functional.py:2012:27
          %size_prods.84 : int = aten::__getitem__(%547, %19) # torch/nn/functional.py:1991:17
          %549 : int = aten::len(%547) # torch/nn/functional.py:1992:19
          %550 : int = aten::sub(%549, %11) # torch/nn/functional.py:1992:19
          %size_prods.85 : int = prim::Loop(%550, %10, %size_prods.84) # torch/nn/functional.py:1992:4
            block0(%i.22 : int, %size_prods.86 : int):
              %554 : int = aten::add(%i.22, %11) # torch/nn/functional.py:1993:27
              %555 : int = aten::__getitem__(%547, %554) # torch/nn/functional.py:1993:22
              %size_prods.87 : int = aten::mul(%size_prods.86, %555) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.87)
          %557 : bool = aten::eq(%size_prods.85, %20) # torch/nn/functional.py:1994:7
           = prim::If(%557) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.78 : Tensor = aten::batch_norm(%input.77, %545, %546, %543, %544, %542, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.79 : Tensor = aten::hardtanh_(%input.78, %5, %4) # torch/nn/functional.py:1171:17
      %560 : Tensor = prim::GetAttr[name="weight"](%496)
      %561 : Tensor? = prim::GetAttr[name="bias"](%496)
      %562 : int[] = prim::ListConstruct(%20, %20)
      %563 : int[] = prim::ListConstruct(%19, %19)
      %564 : int[] = prim::ListConstruct(%20, %20)
      %input.80 : Tensor = aten::conv2d(%input.79, %560, %561, %562, %563, %564, %20) # torch/nn/modules/conv.py:415:15
      %566 : int = aten::dim(%input.80) # torch/nn/modules/batchnorm.py:276:11
      %567 : bool = aten::ne(%566, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%567) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %568 : bool = prim::GetAttr[name="training"](%497)
       = prim::If(%568) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %569 : Tensor = prim::GetAttr[name="num_batches_tracked"](%497)
          %570 : Tensor = aten::add(%569, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%497, %570)
          -> ()
        block1():
          -> ()
      %571 : bool = prim::GetAttr[name="training"](%497)
      %572 : Tensor = prim::GetAttr[name="running_mean"](%497)
      %573 : Tensor = prim::GetAttr[name="running_var"](%497)
      %574 : Tensor = prim::GetAttr[name="weight"](%497)
      %575 : Tensor = prim::GetAttr[name="bias"](%497)
       = prim::If(%571) # torch/nn/functional.py:2011:4
        block0():
          %576 : int[] = aten::size(%input.80) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%576, %19) # torch/nn/functional.py:1991:17
          %578 : int = aten::len(%576) # torch/nn/functional.py:1992:19
          %579 : int = aten::sub(%578, %11) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%579, %10, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %583 : int = aten::add(%i.23, %11) # torch/nn/functional.py:1993:27
              %584 : int = aten::__getitem__(%576, %583) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %584) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.91)
          %586 : bool = aten::eq(%size_prods.89, %20) # torch/nn/functional.py:1994:7
           = prim::If(%586) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.81 : Tensor = aten::batch_norm(%input.80, %574, %575, %572, %573, %571, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.81)
  %588 : bool = prim::GetAttr[name="use_res_connect"](%27)
  %input.34 : Tensor = prim::If(%588) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %590 : __torch__.torch.nn.modules.container.___torch_mangle_899.Sequential = prim::GetAttr[name="conv"](%27)
      %591 : __torch__.torchvision.models.mobilenet.___torch_mangle_890.ConvBNReLU = prim::GetAttr[name="0"](%590)
      %592 : __torch__.torchvision.models.mobilenet.___torch_mangle_897.ConvBNReLU = prim::GetAttr[name="1"](%590)
      %593 : __torch__.torch.nn.modules.conv.___torch_mangle_898.Conv2d = prim::GetAttr[name="2"](%590)
      %594 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%590)
      %595 : __torch__.torch.nn.modules.conv.___torch_mangle_650.Conv2d = prim::GetAttr[name="0"](%591)
      %596 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%591)
      %597 : Tensor = prim::GetAttr[name="weight"](%595)
      %598 : Tensor? = prim::GetAttr[name="bias"](%595)
      %599 : int[] = prim::ListConstruct(%20, %20)
      %600 : int[] = prim::ListConstruct(%19, %19)
      %601 : int[] = prim::ListConstruct(%20, %20)
      %input.82 : Tensor = aten::conv2d(%input.38, %597, %598, %599, %600, %601, %20) # torch/nn/modules/conv.py:415:15
      %603 : int = aten::dim(%input.82) # torch/nn/modules/batchnorm.py:276:11
      %604 : bool = aten::ne(%603, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%604) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %605 : bool = prim::GetAttr[name="training"](%596)
       = prim::If(%605) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %606 : Tensor = prim::GetAttr[name="num_batches_tracked"](%596)
          %607 : Tensor = aten::add(%606, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%596, %607)
          -> ()
        block1():
          -> ()
      %608 : bool = prim::GetAttr[name="training"](%596)
      %609 : Tensor = prim::GetAttr[name="running_mean"](%596)
      %610 : Tensor = prim::GetAttr[name="running_var"](%596)
      %611 : Tensor = prim::GetAttr[name="weight"](%596)
      %612 : Tensor = prim::GetAttr[name="bias"](%596)
       = prim::If(%608) # torch/nn/functional.py:2011:4
        block0():
          %613 : int[] = aten::size(%input.82) # torch/nn/functional.py:2012:27
          %size_prods.92 : int = aten::__getitem__(%613, %19) # torch/nn/functional.py:1991:17
          %615 : int = aten::len(%613) # torch/nn/functional.py:1992:19
          %616 : int = aten::sub(%615, %11) # torch/nn/functional.py:1992:19
          %size_prods.93 : int = prim::Loop(%616, %10, %size_prods.92) # torch/nn/functional.py:1992:4
            block0(%i.24 : int, %size_prods.94 : int):
              %620 : int = aten::add(%i.24, %11) # torch/nn/functional.py:1993:27
              %621 : int = aten::__getitem__(%613, %620) # torch/nn/functional.py:1993:22
              %size_prods.95 : int = aten::mul(%size_prods.94, %621) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.95)
          %623 : bool = aten::eq(%size_prods.93, %20) # torch/nn/functional.py:1994:7
           = prim::If(%623) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.83 : Tensor = aten::batch_norm(%input.82, %611, %612, %609, %610, %608, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.84 : Tensor = aten::hardtanh_(%input.83, %5, %4) # torch/nn/functional.py:1171:17
      %626 : __torch__.torch.nn.modules.conv.___torch_mangle_896.Conv2d = prim::GetAttr[name="0"](%592)
      %627 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%592)
      %628 : Tensor = prim::GetAttr[name="weight"](%626)
      %629 : Tensor? = prim::GetAttr[name="bias"](%626)
      %630 : int[] = prim::ListConstruct(%11, %11)
      %631 : int[] = prim::ListConstruct(%20, %20)
      %632 : int[] = prim::ListConstruct(%20, %20)
      %input.85 : Tensor = aten::conv2d(%input.84, %628, %629, %630, %631, %632, %14) # torch/nn/modules/conv.py:415:15
      %634 : int = aten::dim(%input.85) # torch/nn/modules/batchnorm.py:276:11
      %635 : bool = aten::ne(%634, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%635) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %636 : bool = prim::GetAttr[name="training"](%627)
       = prim::If(%636) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %637 : Tensor = prim::GetAttr[name="num_batches_tracked"](%627)
          %638 : Tensor = aten::add(%637, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%627, %638)
          -> ()
        block1():
          -> ()
      %639 : bool = prim::GetAttr[name="training"](%627)
      %640 : Tensor = prim::GetAttr[name="running_mean"](%627)
      %641 : Tensor = prim::GetAttr[name="running_var"](%627)
      %642 : Tensor = prim::GetAttr[name="weight"](%627)
      %643 : Tensor = prim::GetAttr[name="bias"](%627)
       = prim::If(%639) # torch/nn/functional.py:2011:4
        block0():
          %644 : int[] = aten::size(%input.85) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%644, %19) # torch/nn/functional.py:1991:17
          %646 : int = aten::len(%644) # torch/nn/functional.py:1992:19
          %647 : int = aten::sub(%646, %11) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%647, %10, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %651 : int = aten::add(%i.25, %11) # torch/nn/functional.py:1993:27
              %652 : int = aten::__getitem__(%644, %651) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %652) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.99)
          %654 : bool = aten::eq(%size_prods.97, %20) # torch/nn/functional.py:1994:7
           = prim::If(%654) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.86 : Tensor = aten::batch_norm(%input.85, %642, %643, %640, %641, %639, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.87 : Tensor = aten::hardtanh_(%input.86, %5, %4) # torch/nn/functional.py:1171:17
      %657 : Tensor = prim::GetAttr[name="weight"](%593)
      %658 : Tensor? = prim::GetAttr[name="bias"](%593)
      %659 : int[] = prim::ListConstruct(%20, %20)
      %660 : int[] = prim::ListConstruct(%19, %19)
      %661 : int[] = prim::ListConstruct(%20, %20)
      %input.88 : Tensor = aten::conv2d(%input.87, %657, %658, %659, %660, %661, %20) # torch/nn/modules/conv.py:415:15
      %663 : int = aten::dim(%input.88) # torch/nn/modules/batchnorm.py:276:11
      %664 : bool = aten::ne(%663, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%664) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %665 : bool = prim::GetAttr[name="training"](%594)
       = prim::If(%665) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %666 : Tensor = prim::GetAttr[name="num_batches_tracked"](%594)
          %667 : Tensor = aten::add(%666, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%594, %667)
          -> ()
        block1():
          -> ()
      %668 : bool = prim::GetAttr[name="training"](%594)
      %669 : Tensor = prim::GetAttr[name="running_mean"](%594)
      %670 : Tensor = prim::GetAttr[name="running_var"](%594)
      %671 : Tensor = prim::GetAttr[name="weight"](%594)
      %672 : Tensor = prim::GetAttr[name="bias"](%594)
       = prim::If(%668) # torch/nn/functional.py:2011:4
        block0():
          %673 : int[] = aten::size(%input.88) # torch/nn/functional.py:2012:27
          %size_prods.100 : int = aten::__getitem__(%673, %19) # torch/nn/functional.py:1991:17
          %675 : int = aten::len(%673) # torch/nn/functional.py:1992:19
          %676 : int = aten::sub(%675, %11) # torch/nn/functional.py:1992:19
          %size_prods.101 : int = prim::Loop(%676, %10, %size_prods.100) # torch/nn/functional.py:1992:4
            block0(%i.26 : int, %size_prods.102 : int):
              %680 : int = aten::add(%i.26, %11) # torch/nn/functional.py:1993:27
              %681 : int = aten::__getitem__(%673, %680) # torch/nn/functional.py:1993:22
              %size_prods.103 : int = aten::mul(%size_prods.102, %681) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.103)
          %683 : bool = aten::eq(%size_prods.101, %20) # torch/nn/functional.py:1994:7
           = prim::If(%683) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.89 : Tensor = aten::batch_norm(%input.88, %671, %672, %669, %670, %668, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %685 : Tensor = aten::add(%input.38, %input.89, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%685)
    block1():
      %686 : __torch__.torch.nn.modules.container.___torch_mangle_899.Sequential = prim::GetAttr[name="conv"](%27)
      %687 : __torch__.torchvision.models.mobilenet.___torch_mangle_890.ConvBNReLU = prim::GetAttr[name="0"](%686)
      %688 : __torch__.torchvision.models.mobilenet.___torch_mangle_897.ConvBNReLU = prim::GetAttr[name="1"](%686)
      %689 : __torch__.torch.nn.modules.conv.___torch_mangle_898.Conv2d = prim::GetAttr[name="2"](%686)
      %690 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%686)
      %691 : __torch__.torch.nn.modules.conv.___torch_mangle_650.Conv2d = prim::GetAttr[name="0"](%687)
      %692 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%687)
      %693 : Tensor = prim::GetAttr[name="weight"](%691)
      %694 : Tensor? = prim::GetAttr[name="bias"](%691)
      %695 : int[] = prim::ListConstruct(%20, %20)
      %696 : int[] = prim::ListConstruct(%19, %19)
      %697 : int[] = prim::ListConstruct(%20, %20)
      %input.90 : Tensor = aten::conv2d(%input.38, %693, %694, %695, %696, %697, %20) # torch/nn/modules/conv.py:415:15
      %699 : int = aten::dim(%input.90) # torch/nn/modules/batchnorm.py:276:11
      %700 : bool = aten::ne(%699, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%700) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %701 : bool = prim::GetAttr[name="training"](%692)
       = prim::If(%701) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %702 : Tensor = prim::GetAttr[name="num_batches_tracked"](%692)
          %703 : Tensor = aten::add(%702, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%692, %703)
          -> ()
        block1():
          -> ()
      %704 : bool = prim::GetAttr[name="training"](%692)
      %705 : Tensor = prim::GetAttr[name="running_mean"](%692)
      %706 : Tensor = prim::GetAttr[name="running_var"](%692)
      %707 : Tensor = prim::GetAttr[name="weight"](%692)
      %708 : Tensor = prim::GetAttr[name="bias"](%692)
       = prim::If(%704) # torch/nn/functional.py:2011:4
        block0():
          %709 : int[] = aten::size(%input.90) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%709, %19) # torch/nn/functional.py:1991:17
          %711 : int = aten::len(%709) # torch/nn/functional.py:1992:19
          %712 : int = aten::sub(%711, %11) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%712, %10, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %716 : int = aten::add(%i.27, %11) # torch/nn/functional.py:1993:27
              %717 : int = aten::__getitem__(%709, %716) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %717) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.107)
          %719 : bool = aten::eq(%size_prods.105, %20) # torch/nn/functional.py:1994:7
           = prim::If(%719) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.91 : Tensor = aten::batch_norm(%input.90, %707, %708, %705, %706, %704, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.92 : Tensor = aten::hardtanh_(%input.91, %5, %4) # torch/nn/functional.py:1171:17
      %722 : __torch__.torch.nn.modules.conv.___torch_mangle_896.Conv2d = prim::GetAttr[name="0"](%688)
      %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="1"](%688)
      %724 : Tensor = prim::GetAttr[name="weight"](%722)
      %725 : Tensor? = prim::GetAttr[name="bias"](%722)
      %726 : int[] = prim::ListConstruct(%11, %11)
      %727 : int[] = prim::ListConstruct(%20, %20)
      %728 : int[] = prim::ListConstruct(%20, %20)
      %input.93 : Tensor = aten::conv2d(%input.92, %724, %725, %726, %727, %728, %14) # torch/nn/modules/conv.py:415:15
      %730 : int = aten::dim(%input.93) # torch/nn/modules/batchnorm.py:276:11
      %731 : bool = aten::ne(%730, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%731) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %732 : bool = prim::GetAttr[name="training"](%723)
       = prim::If(%732) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %733 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
          %734 : Tensor = aten::add(%733, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%723, %734)
          -> ()
        block1():
          -> ()
      %735 : bool = prim::GetAttr[name="training"](%723)
      %736 : Tensor = prim::GetAttr[name="running_mean"](%723)
      %737 : Tensor = prim::GetAttr[name="running_var"](%723)
      %738 : Tensor = prim::GetAttr[name="weight"](%723)
      %739 : Tensor = prim::GetAttr[name="bias"](%723)
       = prim::If(%735) # torch/nn/functional.py:2011:4
        block0():
          %740 : int[] = aten::size(%input.93) # torch/nn/functional.py:2012:27
          %size_prods.108 : int = aten::__getitem__(%740, %19) # torch/nn/functional.py:1991:17
          %742 : int = aten::len(%740) # torch/nn/functional.py:1992:19
          %743 : int = aten::sub(%742, %11) # torch/nn/functional.py:1992:19
          %size_prods.109 : int = prim::Loop(%743, %10, %size_prods.108) # torch/nn/functional.py:1992:4
            block0(%i.28 : int, %size_prods.110 : int):
              %747 : int = aten::add(%i.28, %11) # torch/nn/functional.py:1993:27
              %748 : int = aten::__getitem__(%740, %747) # torch/nn/functional.py:1993:22
              %size_prods.111 : int = aten::mul(%size_prods.110, %748) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.111)
          %750 : bool = aten::eq(%size_prods.109, %20) # torch/nn/functional.py:1994:7
           = prim::If(%750) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.94 : Tensor = aten::batch_norm(%input.93, %738, %739, %736, %737, %735, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.95 : Tensor = aten::hardtanh_(%input.94, %5, %4) # torch/nn/functional.py:1171:17
      %753 : Tensor = prim::GetAttr[name="weight"](%689)
      %754 : Tensor? = prim::GetAttr[name="bias"](%689)
      %755 : int[] = prim::ListConstruct(%20, %20)
      %756 : int[] = prim::ListConstruct(%19, %19)
      %757 : int[] = prim::ListConstruct(%20, %20)
      %input.96 : Tensor = aten::conv2d(%input.95, %753, %754, %755, %756, %757, %20) # torch/nn/modules/conv.py:415:15
      %759 : int = aten::dim(%input.96) # torch/nn/modules/batchnorm.py:276:11
      %760 : bool = aten::ne(%759, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%760) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %761 : bool = prim::GetAttr[name="training"](%690)
       = prim::If(%761) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %762 : Tensor = prim::GetAttr[name="num_batches_tracked"](%690)
          %763 : Tensor = aten::add(%762, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%690, %763)
          -> ()
        block1():
          -> ()
      %764 : bool = prim::GetAttr[name="training"](%690)
      %765 : Tensor = prim::GetAttr[name="running_mean"](%690)
      %766 : Tensor = prim::GetAttr[name="running_var"](%690)
      %767 : Tensor = prim::GetAttr[name="weight"](%690)
      %768 : Tensor = prim::GetAttr[name="bias"](%690)
       = prim::If(%764) # torch/nn/functional.py:2011:4
        block0():
          %769 : int[] = aten::size(%input.96) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%769, %19) # torch/nn/functional.py:1991:17
          %771 : int = aten::len(%769) # torch/nn/functional.py:1992:19
          %772 : int = aten::sub(%771, %11) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%772, %10, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %776 : int = aten::add(%i.29, %11) # torch/nn/functional.py:1993:27
              %777 : int = aten::__getitem__(%769, %776) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %777) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.115)
          %779 : bool = aten::eq(%size_prods.113, %20) # torch/nn/functional.py:1994:7
           = prim::If(%779) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.97 : Tensor = aten::batch_norm(%input.96, %767, %768, %765, %766, %764, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.97)
  %781 : bool = prim::GetAttr[name="use_res_connect"](%28)
  %input.30 : Tensor = prim::If(%781) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %783 : __torch__.torch.nn.modules.container.___torch_mangle_904.Sequential = prim::GetAttr[name="conv"](%28)
      %784 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%783)
      %785 : __torch__.torchvision.models.mobilenet.___torch_mangle_903.ConvBNReLU = prim::GetAttr[name="1"](%783)
      %786 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="2"](%783)
      %787 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%783)
      %788 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%784)
      %789 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%784)
      %790 : Tensor = prim::GetAttr[name="weight"](%788)
      %791 : Tensor? = prim::GetAttr[name="bias"](%788)
      %792 : int[] = prim::ListConstruct(%20, %20)
      %793 : int[] = prim::ListConstruct(%19, %19)
      %794 : int[] = prim::ListConstruct(%20, %20)
      %input.98 : Tensor = aten::conv2d(%input.34, %790, %791, %792, %793, %794, %20) # torch/nn/modules/conv.py:415:15
      %796 : int = aten::dim(%input.98) # torch/nn/modules/batchnorm.py:276:11
      %797 : bool = aten::ne(%796, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%797) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %798 : bool = prim::GetAttr[name="training"](%789)
       = prim::If(%798) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %799 : Tensor = prim::GetAttr[name="num_batches_tracked"](%789)
          %800 : Tensor = aten::add(%799, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%789, %800)
          -> ()
        block1():
          -> ()
      %801 : bool = prim::GetAttr[name="training"](%789)
      %802 : Tensor = prim::GetAttr[name="running_mean"](%789)
      %803 : Tensor = prim::GetAttr[name="running_var"](%789)
      %804 : Tensor = prim::GetAttr[name="weight"](%789)
      %805 : Tensor = prim::GetAttr[name="bias"](%789)
       = prim::If(%801) # torch/nn/functional.py:2011:4
        block0():
          %806 : int[] = aten::size(%input.98) # torch/nn/functional.py:2012:27
          %size_prods.116 : int = aten::__getitem__(%806, %19) # torch/nn/functional.py:1991:17
          %808 : int = aten::len(%806) # torch/nn/functional.py:1992:19
          %809 : int = aten::sub(%808, %11) # torch/nn/functional.py:1992:19
          %size_prods.117 : int = prim::Loop(%809, %10, %size_prods.116) # torch/nn/functional.py:1992:4
            block0(%i.30 : int, %size_prods.118 : int):
              %813 : int = aten::add(%i.30, %11) # torch/nn/functional.py:1993:27
              %814 : int = aten::__getitem__(%806, %813) # torch/nn/functional.py:1993:22
              %size_prods.119 : int = aten::mul(%size_prods.118, %814) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.119)
          %816 : bool = aten::eq(%size_prods.117, %20) # torch/nn/functional.py:1994:7
           = prim::If(%816) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.99 : Tensor = aten::batch_norm(%input.98, %804, %805, %802, %803, %801, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.100 : Tensor = aten::hardtanh_(%input.99, %5, %4) # torch/nn/functional.py:1171:17
      %819 : __torch__.torch.nn.modules.conv.___torch_mangle_902.Conv2d = prim::GetAttr[name="0"](%785)
      %820 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%785)
      %821 : Tensor = prim::GetAttr[name="weight"](%819)
      %822 : Tensor? = prim::GetAttr[name="bias"](%819)
      %823 : int[] = prim::ListConstruct(%20, %20)
      %824 : int[] = prim::ListConstruct(%20, %20)
      %825 : int[] = prim::ListConstruct(%20, %20)
      %input.101 : Tensor = aten::conv2d(%input.100, %821, %822, %823, %824, %825, %15) # torch/nn/modules/conv.py:415:15
      %827 : int = aten::dim(%input.101) # torch/nn/modules/batchnorm.py:276:11
      %828 : bool = aten::ne(%827, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%828) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %829 : bool = prim::GetAttr[name="training"](%820)
       = prim::If(%829) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %830 : Tensor = prim::GetAttr[name="num_batches_tracked"](%820)
          %831 : Tensor = aten::add(%830, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%820, %831)
          -> ()
        block1():
          -> ()
      %832 : bool = prim::GetAttr[name="training"](%820)
      %833 : Tensor = prim::GetAttr[name="running_mean"](%820)
      %834 : Tensor = prim::GetAttr[name="running_var"](%820)
      %835 : Tensor = prim::GetAttr[name="weight"](%820)
      %836 : Tensor = prim::GetAttr[name="bias"](%820)
       = prim::If(%832) # torch/nn/functional.py:2011:4
        block0():
          %837 : int[] = aten::size(%input.101) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%837, %19) # torch/nn/functional.py:1991:17
          %839 : int = aten::len(%837) # torch/nn/functional.py:1992:19
          %840 : int = aten::sub(%839, %11) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%840, %10, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %844 : int = aten::add(%i.31, %11) # torch/nn/functional.py:1993:27
              %845 : int = aten::__getitem__(%837, %844) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %845) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.123)
          %847 : bool = aten::eq(%size_prods.121, %20) # torch/nn/functional.py:1994:7
           = prim::If(%847) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.102 : Tensor = aten::batch_norm(%input.101, %835, %836, %833, %834, %832, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.103 : Tensor = aten::hardtanh_(%input.102, %5, %4) # torch/nn/functional.py:1171:17
      %850 : Tensor = prim::GetAttr[name="weight"](%786)
      %851 : Tensor? = prim::GetAttr[name="bias"](%786)
      %852 : int[] = prim::ListConstruct(%20, %20)
      %853 : int[] = prim::ListConstruct(%19, %19)
      %854 : int[] = prim::ListConstruct(%20, %20)
      %input.104 : Tensor = aten::conv2d(%input.103, %850, %851, %852, %853, %854, %20) # torch/nn/modules/conv.py:415:15
      %856 : int = aten::dim(%input.104) # torch/nn/modules/batchnorm.py:276:11
      %857 : bool = aten::ne(%856, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%857) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %858 : bool = prim::GetAttr[name="training"](%787)
       = prim::If(%858) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %859 : Tensor = prim::GetAttr[name="num_batches_tracked"](%787)
          %860 : Tensor = aten::add(%859, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%787, %860)
          -> ()
        block1():
          -> ()
      %861 : bool = prim::GetAttr[name="training"](%787)
      %862 : Tensor = prim::GetAttr[name="running_mean"](%787)
      %863 : Tensor = prim::GetAttr[name="running_var"](%787)
      %864 : Tensor = prim::GetAttr[name="weight"](%787)
      %865 : Tensor = prim::GetAttr[name="bias"](%787)
       = prim::If(%861) # torch/nn/functional.py:2011:4
        block0():
          %866 : int[] = aten::size(%input.104) # torch/nn/functional.py:2012:27
          %size_prods.124 : int = aten::__getitem__(%866, %19) # torch/nn/functional.py:1991:17
          %868 : int = aten::len(%866) # torch/nn/functional.py:1992:19
          %869 : int = aten::sub(%868, %11) # torch/nn/functional.py:1992:19
          %size_prods.125 : int = prim::Loop(%869, %10, %size_prods.124) # torch/nn/functional.py:1992:4
            block0(%i.32 : int, %size_prods.126 : int):
              %873 : int = aten::add(%i.32, %11) # torch/nn/functional.py:1993:27
              %874 : int = aten::__getitem__(%866, %873) # torch/nn/functional.py:1993:22
              %size_prods.127 : int = aten::mul(%size_prods.126, %874) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.127)
          %876 : bool = aten::eq(%size_prods.125, %20) # torch/nn/functional.py:1994:7
           = prim::If(%876) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.105 : Tensor = aten::batch_norm(%input.104, %864, %865, %862, %863, %861, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %878 : Tensor = aten::add(%input.34, %input.105, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%878)
    block1():
      %879 : __torch__.torch.nn.modules.container.___torch_mangle_904.Sequential = prim::GetAttr[name="conv"](%28)
      %880 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%879)
      %881 : __torch__.torchvision.models.mobilenet.___torch_mangle_903.ConvBNReLU = prim::GetAttr[name="1"](%879)
      %882 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="2"](%879)
      %883 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%879)
      %884 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%880)
      %885 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%880)
      %886 : Tensor = prim::GetAttr[name="weight"](%884)
      %887 : Tensor? = prim::GetAttr[name="bias"](%884)
      %888 : int[] = prim::ListConstruct(%20, %20)
      %889 : int[] = prim::ListConstruct(%19, %19)
      %890 : int[] = prim::ListConstruct(%20, %20)
      %input.106 : Tensor = aten::conv2d(%input.34, %886, %887, %888, %889, %890, %20) # torch/nn/modules/conv.py:415:15
      %892 : int = aten::dim(%input.106) # torch/nn/modules/batchnorm.py:276:11
      %893 : bool = aten::ne(%892, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%893) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %894 : bool = prim::GetAttr[name="training"](%885)
       = prim::If(%894) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %895 : Tensor = prim::GetAttr[name="num_batches_tracked"](%885)
          %896 : Tensor = aten::add(%895, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%885, %896)
          -> ()
        block1():
          -> ()
      %897 : bool = prim::GetAttr[name="training"](%885)
      %898 : Tensor = prim::GetAttr[name="running_mean"](%885)
      %899 : Tensor = prim::GetAttr[name="running_var"](%885)
      %900 : Tensor = prim::GetAttr[name="weight"](%885)
      %901 : Tensor = prim::GetAttr[name="bias"](%885)
       = prim::If(%897) # torch/nn/functional.py:2011:4
        block0():
          %902 : int[] = aten::size(%input.106) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%902, %19) # torch/nn/functional.py:1991:17
          %904 : int = aten::len(%902) # torch/nn/functional.py:1992:19
          %905 : int = aten::sub(%904, %11) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%905, %10, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %909 : int = aten::add(%i.33, %11) # torch/nn/functional.py:1993:27
              %910 : int = aten::__getitem__(%902, %909) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %910) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.131)
          %912 : bool = aten::eq(%size_prods.129, %20) # torch/nn/functional.py:1994:7
           = prim::If(%912) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.107 : Tensor = aten::batch_norm(%input.106, %900, %901, %898, %899, %897, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.108 : Tensor = aten::hardtanh_(%input.107, %5, %4) # torch/nn/functional.py:1171:17
      %915 : __torch__.torch.nn.modules.conv.___torch_mangle_902.Conv2d = prim::GetAttr[name="0"](%881)
      %916 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%881)
      %917 : Tensor = prim::GetAttr[name="weight"](%915)
      %918 : Tensor? = prim::GetAttr[name="bias"](%915)
      %919 : int[] = prim::ListConstruct(%20, %20)
      %920 : int[] = prim::ListConstruct(%20, %20)
      %921 : int[] = prim::ListConstruct(%20, %20)
      %input.109 : Tensor = aten::conv2d(%input.108, %917, %918, %919, %920, %921, %15) # torch/nn/modules/conv.py:415:15
      %923 : int = aten::dim(%input.109) # torch/nn/modules/batchnorm.py:276:11
      %924 : bool = aten::ne(%923, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%924) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %925 : bool = prim::GetAttr[name="training"](%916)
       = prim::If(%925) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %926 : Tensor = prim::GetAttr[name="num_batches_tracked"](%916)
          %927 : Tensor = aten::add(%926, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%916, %927)
          -> ()
        block1():
          -> ()
      %928 : bool = prim::GetAttr[name="training"](%916)
      %929 : Tensor = prim::GetAttr[name="running_mean"](%916)
      %930 : Tensor = prim::GetAttr[name="running_var"](%916)
      %931 : Tensor = prim::GetAttr[name="weight"](%916)
      %932 : Tensor = prim::GetAttr[name="bias"](%916)
       = prim::If(%928) # torch/nn/functional.py:2011:4
        block0():
          %933 : int[] = aten::size(%input.109) # torch/nn/functional.py:2012:27
          %size_prods.132 : int = aten::__getitem__(%933, %19) # torch/nn/functional.py:1991:17
          %935 : int = aten::len(%933) # torch/nn/functional.py:1992:19
          %936 : int = aten::sub(%935, %11) # torch/nn/functional.py:1992:19
          %size_prods.133 : int = prim::Loop(%936, %10, %size_prods.132) # torch/nn/functional.py:1992:4
            block0(%i.34 : int, %size_prods.134 : int):
              %940 : int = aten::add(%i.34, %11) # torch/nn/functional.py:1993:27
              %941 : int = aten::__getitem__(%933, %940) # torch/nn/functional.py:1993:22
              %size_prods.135 : int = aten::mul(%size_prods.134, %941) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.135)
          %943 : bool = aten::eq(%size_prods.133, %20) # torch/nn/functional.py:1994:7
           = prim::If(%943) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.110 : Tensor = aten::batch_norm(%input.109, %931, %932, %929, %930, %928, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.111 : Tensor = aten::hardtanh_(%input.110, %5, %4) # torch/nn/functional.py:1171:17
      %946 : Tensor = prim::GetAttr[name="weight"](%882)
      %947 : Tensor? = prim::GetAttr[name="bias"](%882)
      %948 : int[] = prim::ListConstruct(%20, %20)
      %949 : int[] = prim::ListConstruct(%19, %19)
      %950 : int[] = prim::ListConstruct(%20, %20)
      %input.112 : Tensor = aten::conv2d(%input.111, %946, %947, %948, %949, %950, %20) # torch/nn/modules/conv.py:415:15
      %952 : int = aten::dim(%input.112) # torch/nn/modules/batchnorm.py:276:11
      %953 : bool = aten::ne(%952, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%953) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %954 : bool = prim::GetAttr[name="training"](%883)
       = prim::If(%954) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %955 : Tensor = prim::GetAttr[name="num_batches_tracked"](%883)
          %956 : Tensor = aten::add(%955, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%883, %956)
          -> ()
        block1():
          -> ()
      %957 : bool = prim::GetAttr[name="training"](%883)
      %958 : Tensor = prim::GetAttr[name="running_mean"](%883)
      %959 : Tensor = prim::GetAttr[name="running_var"](%883)
      %960 : Tensor = prim::GetAttr[name="weight"](%883)
      %961 : Tensor = prim::GetAttr[name="bias"](%883)
       = prim::If(%957) # torch/nn/functional.py:2011:4
        block0():
          %962 : int[] = aten::size(%input.112) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%962, %19) # torch/nn/functional.py:1991:17
          %964 : int = aten::len(%962) # torch/nn/functional.py:1992:19
          %965 : int = aten::sub(%964, %11) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%965, %10, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %969 : int = aten::add(%i.35, %11) # torch/nn/functional.py:1993:27
              %970 : int = aten::__getitem__(%962, %969) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %970) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.139)
          %972 : bool = aten::eq(%size_prods.137, %20) # torch/nn/functional.py:1994:7
           = prim::If(%972) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.113 : Tensor = aten::batch_norm(%input.112, %960, %961, %958, %959, %957, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.113)
  %974 : bool = prim::GetAttr[name="use_res_connect"](%29)
  %input.36 : Tensor = prim::If(%974) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %976 : __torch__.torch.nn.modules.container.___torch_mangle_904.Sequential = prim::GetAttr[name="conv"](%29)
      %977 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%976)
      %978 : __torch__.torchvision.models.mobilenet.___torch_mangle_903.ConvBNReLU = prim::GetAttr[name="1"](%976)
      %979 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="2"](%976)
      %980 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%976)
      %981 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%977)
      %982 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%977)
      %983 : Tensor = prim::GetAttr[name="weight"](%981)
      %984 : Tensor? = prim::GetAttr[name="bias"](%981)
      %985 : int[] = prim::ListConstruct(%20, %20)
      %986 : int[] = prim::ListConstruct(%19, %19)
      %987 : int[] = prim::ListConstruct(%20, %20)
      %input.114 : Tensor = aten::conv2d(%input.30, %983, %984, %985, %986, %987, %20) # torch/nn/modules/conv.py:415:15
      %989 : int = aten::dim(%input.114) # torch/nn/modules/batchnorm.py:276:11
      %990 : bool = aten::ne(%989, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%990) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %991 : bool = prim::GetAttr[name="training"](%982)
       = prim::If(%991) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %992 : Tensor = prim::GetAttr[name="num_batches_tracked"](%982)
          %993 : Tensor = aten::add(%992, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%982, %993)
          -> ()
        block1():
          -> ()
      %994 : bool = prim::GetAttr[name="training"](%982)
      %995 : Tensor = prim::GetAttr[name="running_mean"](%982)
      %996 : Tensor = prim::GetAttr[name="running_var"](%982)
      %997 : Tensor = prim::GetAttr[name="weight"](%982)
      %998 : Tensor = prim::GetAttr[name="bias"](%982)
       = prim::If(%994) # torch/nn/functional.py:2011:4
        block0():
          %999 : int[] = aten::size(%input.114) # torch/nn/functional.py:2012:27
          %size_prods.140 : int = aten::__getitem__(%999, %19) # torch/nn/functional.py:1991:17
          %1001 : int = aten::len(%999) # torch/nn/functional.py:1992:19
          %1002 : int = aten::sub(%1001, %11) # torch/nn/functional.py:1992:19
          %size_prods.141 : int = prim::Loop(%1002, %10, %size_prods.140) # torch/nn/functional.py:1992:4
            block0(%i.36 : int, %size_prods.142 : int):
              %1006 : int = aten::add(%i.36, %11) # torch/nn/functional.py:1993:27
              %1007 : int = aten::__getitem__(%999, %1006) # torch/nn/functional.py:1993:22
              %size_prods.143 : int = aten::mul(%size_prods.142, %1007) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.143)
          %1009 : bool = aten::eq(%size_prods.141, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1009) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.115 : Tensor = aten::batch_norm(%input.114, %997, %998, %995, %996, %994, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.116 : Tensor = aten::hardtanh_(%input.115, %5, %4) # torch/nn/functional.py:1171:17
      %1012 : __torch__.torch.nn.modules.conv.___torch_mangle_902.Conv2d = prim::GetAttr[name="0"](%978)
      %1013 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%978)
      %1014 : Tensor = prim::GetAttr[name="weight"](%1012)
      %1015 : Tensor? = prim::GetAttr[name="bias"](%1012)
      %1016 : int[] = prim::ListConstruct(%20, %20)
      %1017 : int[] = prim::ListConstruct(%20, %20)
      %1018 : int[] = prim::ListConstruct(%20, %20)
      %input.117 : Tensor = aten::conv2d(%input.116, %1014, %1015, %1016, %1017, %1018, %15) # torch/nn/modules/conv.py:415:15
      %1020 : int = aten::dim(%input.117) # torch/nn/modules/batchnorm.py:276:11
      %1021 : bool = aten::ne(%1020, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1021) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1022 : bool = prim::GetAttr[name="training"](%1013)
       = prim::If(%1022) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1023 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1013)
          %1024 : Tensor = aten::add(%1023, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1013, %1024)
          -> ()
        block1():
          -> ()
      %1025 : bool = prim::GetAttr[name="training"](%1013)
      %1026 : Tensor = prim::GetAttr[name="running_mean"](%1013)
      %1027 : Tensor = prim::GetAttr[name="running_var"](%1013)
      %1028 : Tensor = prim::GetAttr[name="weight"](%1013)
      %1029 : Tensor = prim::GetAttr[name="bias"](%1013)
       = prim::If(%1025) # torch/nn/functional.py:2011:4
        block0():
          %1030 : int[] = aten::size(%input.117) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%1030, %19) # torch/nn/functional.py:1991:17
          %1032 : int = aten::len(%1030) # torch/nn/functional.py:1992:19
          %1033 : int = aten::sub(%1032, %11) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%1033, %10, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %1037 : int = aten::add(%i.37, %11) # torch/nn/functional.py:1993:27
              %1038 : int = aten::__getitem__(%1030, %1037) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %1038) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.147)
          %1040 : bool = aten::eq(%size_prods.145, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1040) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.118 : Tensor = aten::batch_norm(%input.117, %1028, %1029, %1026, %1027, %1025, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.119 : Tensor = aten::hardtanh_(%input.118, %5, %4) # torch/nn/functional.py:1171:17
      %1043 : Tensor = prim::GetAttr[name="weight"](%979)
      %1044 : Tensor? = prim::GetAttr[name="bias"](%979)
      %1045 : int[] = prim::ListConstruct(%20, %20)
      %1046 : int[] = prim::ListConstruct(%19, %19)
      %1047 : int[] = prim::ListConstruct(%20, %20)
      %input.120 : Tensor = aten::conv2d(%input.119, %1043, %1044, %1045, %1046, %1047, %20) # torch/nn/modules/conv.py:415:15
      %1049 : int = aten::dim(%input.120) # torch/nn/modules/batchnorm.py:276:11
      %1050 : bool = aten::ne(%1049, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1050) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1051 : bool = prim::GetAttr[name="training"](%980)
       = prim::If(%1051) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1052 : Tensor = prim::GetAttr[name="num_batches_tracked"](%980)
          %1053 : Tensor = aten::add(%1052, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%980, %1053)
          -> ()
        block1():
          -> ()
      %1054 : bool = prim::GetAttr[name="training"](%980)
      %1055 : Tensor = prim::GetAttr[name="running_mean"](%980)
      %1056 : Tensor = prim::GetAttr[name="running_var"](%980)
      %1057 : Tensor = prim::GetAttr[name="weight"](%980)
      %1058 : Tensor = prim::GetAttr[name="bias"](%980)
       = prim::If(%1054) # torch/nn/functional.py:2011:4
        block0():
          %1059 : int[] = aten::size(%input.120) # torch/nn/functional.py:2012:27
          %size_prods.148 : int = aten::__getitem__(%1059, %19) # torch/nn/functional.py:1991:17
          %1061 : int = aten::len(%1059) # torch/nn/functional.py:1992:19
          %1062 : int = aten::sub(%1061, %11) # torch/nn/functional.py:1992:19
          %size_prods.149 : int = prim::Loop(%1062, %10, %size_prods.148) # torch/nn/functional.py:1992:4
            block0(%i.38 : int, %size_prods.150 : int):
              %1066 : int = aten::add(%i.38, %11) # torch/nn/functional.py:1993:27
              %1067 : int = aten::__getitem__(%1059, %1066) # torch/nn/functional.py:1993:22
              %size_prods.151 : int = aten::mul(%size_prods.150, %1067) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.151)
          %1069 : bool = aten::eq(%size_prods.149, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1069) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.121 : Tensor = aten::batch_norm(%input.120, %1057, %1058, %1055, %1056, %1054, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %1071 : Tensor = aten::add(%input.30, %input.121, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%1071)
    block1():
      %1072 : __torch__.torch.nn.modules.container.___torch_mangle_904.Sequential = prim::GetAttr[name="conv"](%29)
      %1073 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%1072)
      %1074 : __torch__.torchvision.models.mobilenet.___torch_mangle_903.ConvBNReLU = prim::GetAttr[name="1"](%1072)
      %1075 : __torch__.torch.nn.modules.conv.___torch_mangle_414.Conv2d = prim::GetAttr[name="2"](%1072)
      %1076 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_878.BatchNorm2d = prim::GetAttr[name="3"](%1072)
      %1077 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%1073)
      %1078 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1073)
      %1079 : Tensor = prim::GetAttr[name="weight"](%1077)
      %1080 : Tensor? = prim::GetAttr[name="bias"](%1077)
      %1081 : int[] = prim::ListConstruct(%20, %20)
      %1082 : int[] = prim::ListConstruct(%19, %19)
      %1083 : int[] = prim::ListConstruct(%20, %20)
      %input.122 : Tensor = aten::conv2d(%input.30, %1079, %1080, %1081, %1082, %1083, %20) # torch/nn/modules/conv.py:415:15
      %1085 : int = aten::dim(%input.122) # torch/nn/modules/batchnorm.py:276:11
      %1086 : bool = aten::ne(%1085, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1086) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1087 : bool = prim::GetAttr[name="training"](%1078)
       = prim::If(%1087) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1088 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1078)
          %1089 : Tensor = aten::add(%1088, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1078, %1089)
          -> ()
        block1():
          -> ()
      %1090 : bool = prim::GetAttr[name="training"](%1078)
      %1091 : Tensor = prim::GetAttr[name="running_mean"](%1078)
      %1092 : Tensor = prim::GetAttr[name="running_var"](%1078)
      %1093 : Tensor = prim::GetAttr[name="weight"](%1078)
      %1094 : Tensor = prim::GetAttr[name="bias"](%1078)
       = prim::If(%1090) # torch/nn/functional.py:2011:4
        block0():
          %1095 : int[] = aten::size(%input.122) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%1095, %19) # torch/nn/functional.py:1991:17
          %1097 : int = aten::len(%1095) # torch/nn/functional.py:1992:19
          %1098 : int = aten::sub(%1097, %11) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%1098, %10, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %1102 : int = aten::add(%i.39, %11) # torch/nn/functional.py:1993:27
              %1103 : int = aten::__getitem__(%1095, %1102) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %1103) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.155)
          %1105 : bool = aten::eq(%size_prods.153, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1105) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.123 : Tensor = aten::batch_norm(%input.122, %1093, %1094, %1091, %1092, %1090, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.124 : Tensor = aten::hardtanh_(%input.123, %5, %4) # torch/nn/functional.py:1171:17
      %1108 : __torch__.torch.nn.modules.conv.___torch_mangle_902.Conv2d = prim::GetAttr[name="0"](%1074)
      %1109 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1074)
      %1110 : Tensor = prim::GetAttr[name="weight"](%1108)
      %1111 : Tensor? = prim::GetAttr[name="bias"](%1108)
      %1112 : int[] = prim::ListConstruct(%20, %20)
      %1113 : int[] = prim::ListConstruct(%20, %20)
      %1114 : int[] = prim::ListConstruct(%20, %20)
      %input.125 : Tensor = aten::conv2d(%input.124, %1110, %1111, %1112, %1113, %1114, %15) # torch/nn/modules/conv.py:415:15
      %1116 : int = aten::dim(%input.125) # torch/nn/modules/batchnorm.py:276:11
      %1117 : bool = aten::ne(%1116, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1117) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1118 : bool = prim::GetAttr[name="training"](%1109)
       = prim::If(%1118) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1119 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1109)
          %1120 : Tensor = aten::add(%1119, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1109, %1120)
          -> ()
        block1():
          -> ()
      %1121 : bool = prim::GetAttr[name="training"](%1109)
      %1122 : Tensor = prim::GetAttr[name="running_mean"](%1109)
      %1123 : Tensor = prim::GetAttr[name="running_var"](%1109)
      %1124 : Tensor = prim::GetAttr[name="weight"](%1109)
      %1125 : Tensor = prim::GetAttr[name="bias"](%1109)
       = prim::If(%1121) # torch/nn/functional.py:2011:4
        block0():
          %1126 : int[] = aten::size(%input.125) # torch/nn/functional.py:2012:27
          %size_prods.156 : int = aten::__getitem__(%1126, %19) # torch/nn/functional.py:1991:17
          %1128 : int = aten::len(%1126) # torch/nn/functional.py:1992:19
          %1129 : int = aten::sub(%1128, %11) # torch/nn/functional.py:1992:19
          %size_prods.157 : int = prim::Loop(%1129, %10, %size_prods.156) # torch/nn/functional.py:1992:4
            block0(%i.40 : int, %size_prods.158 : int):
              %1133 : int = aten::add(%i.40, %11) # torch/nn/functional.py:1993:27
              %1134 : int = aten::__getitem__(%1126, %1133) # torch/nn/functional.py:1993:22
              %size_prods.159 : int = aten::mul(%size_prods.158, %1134) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.159)
          %1136 : bool = aten::eq(%size_prods.157, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1136) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.126 : Tensor = aten::batch_norm(%input.125, %1124, %1125, %1122, %1123, %1121, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.127 : Tensor = aten::hardtanh_(%input.126, %5, %4) # torch/nn/functional.py:1171:17
      %1139 : Tensor = prim::GetAttr[name="weight"](%1075)
      %1140 : Tensor? = prim::GetAttr[name="bias"](%1075)
      %1141 : int[] = prim::ListConstruct(%20, %20)
      %1142 : int[] = prim::ListConstruct(%19, %19)
      %1143 : int[] = prim::ListConstruct(%20, %20)
      %input.128 : Tensor = aten::conv2d(%input.127, %1139, %1140, %1141, %1142, %1143, %20) # torch/nn/modules/conv.py:415:15
      %1145 : int = aten::dim(%input.128) # torch/nn/modules/batchnorm.py:276:11
      %1146 : bool = aten::ne(%1145, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1146) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1147 : bool = prim::GetAttr[name="training"](%1076)
       = prim::If(%1147) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1148 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1076)
          %1149 : Tensor = aten::add(%1148, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1076, %1149)
          -> ()
        block1():
          -> ()
      %1150 : bool = prim::GetAttr[name="training"](%1076)
      %1151 : Tensor = prim::GetAttr[name="running_mean"](%1076)
      %1152 : Tensor = prim::GetAttr[name="running_var"](%1076)
      %1153 : Tensor = prim::GetAttr[name="weight"](%1076)
      %1154 : Tensor = prim::GetAttr[name="bias"](%1076)
       = prim::If(%1150) # torch/nn/functional.py:2011:4
        block0():
          %1155 : int[] = aten::size(%input.128) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%1155, %19) # torch/nn/functional.py:1991:17
          %1157 : int = aten::len(%1155) # torch/nn/functional.py:1992:19
          %1158 : int = aten::sub(%1157, %11) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%1158, %10, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %1162 : int = aten::add(%i.41, %11) # torch/nn/functional.py:1993:27
              %1163 : int = aten::__getitem__(%1155, %1162) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %1163) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.163)
          %1165 : bool = aten::eq(%size_prods.161, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1165) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.129 : Tensor = aten::batch_norm(%input.128, %1153, %1154, %1151, %1152, %1150, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.129)
  %1167 : bool = prim::GetAttr[name="use_res_connect"](%30)
  %input.40 : Tensor = prim::If(%1167) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %1169 : __torch__.torch.nn.modules.container.___torch_mangle_908.Sequential = prim::GetAttr[name="conv"](%30)
      %1170 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%1169)
      %1171 : __torch__.torchvision.models.mobilenet.___torch_mangle_907.ConvBNReLU = prim::GetAttr[name="1"](%1169)
      %1172 : __torch__.torch.nn.modules.conv.___torch_mangle_397.Conv2d = prim::GetAttr[name="2"](%1169)
      %1173 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1169)
      %1174 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%1170)
      %1175 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1170)
      %1176 : Tensor = prim::GetAttr[name="weight"](%1174)
      %1177 : Tensor? = prim::GetAttr[name="bias"](%1174)
      %1178 : int[] = prim::ListConstruct(%20, %20)
      %1179 : int[] = prim::ListConstruct(%19, %19)
      %1180 : int[] = prim::ListConstruct(%20, %20)
      %input.130 : Tensor = aten::conv2d(%input.36, %1176, %1177, %1178, %1179, %1180, %20) # torch/nn/modules/conv.py:415:15
      %1182 : int = aten::dim(%input.130) # torch/nn/modules/batchnorm.py:276:11
      %1183 : bool = aten::ne(%1182, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1183) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1184 : bool = prim::GetAttr[name="training"](%1175)
       = prim::If(%1184) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1185 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1175)
          %1186 : Tensor = aten::add(%1185, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1175, %1186)
          -> ()
        block1():
          -> ()
      %1187 : bool = prim::GetAttr[name="training"](%1175)
      %1188 : Tensor = prim::GetAttr[name="running_mean"](%1175)
      %1189 : Tensor = prim::GetAttr[name="running_var"](%1175)
      %1190 : Tensor = prim::GetAttr[name="weight"](%1175)
      %1191 : Tensor = prim::GetAttr[name="bias"](%1175)
       = prim::If(%1187) # torch/nn/functional.py:2011:4
        block0():
          %1192 : int[] = aten::size(%input.130) # torch/nn/functional.py:2012:27
          %size_prods.164 : int = aten::__getitem__(%1192, %19) # torch/nn/functional.py:1991:17
          %1194 : int = aten::len(%1192) # torch/nn/functional.py:1992:19
          %1195 : int = aten::sub(%1194, %11) # torch/nn/functional.py:1992:19
          %size_prods.165 : int = prim::Loop(%1195, %10, %size_prods.164) # torch/nn/functional.py:1992:4
            block0(%i.42 : int, %size_prods.166 : int):
              %1199 : int = aten::add(%i.42, %11) # torch/nn/functional.py:1993:27
              %1200 : int = aten::__getitem__(%1192, %1199) # torch/nn/functional.py:1993:22
              %size_prods.167 : int = aten::mul(%size_prods.166, %1200) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.167)
          %1202 : bool = aten::eq(%size_prods.165, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1202) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.131 : Tensor = aten::batch_norm(%input.130, %1190, %1191, %1188, %1189, %1187, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.132 : Tensor = aten::hardtanh_(%input.131, %5, %4) # torch/nn/functional.py:1171:17
      %1205 : __torch__.torch.nn.modules.conv.___torch_mangle_906.Conv2d = prim::GetAttr[name="0"](%1171)
      %1206 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1171)
      %1207 : Tensor = prim::GetAttr[name="weight"](%1205)
      %1208 : Tensor? = prim::GetAttr[name="bias"](%1205)
      %1209 : int[] = prim::ListConstruct(%11, %11)
      %1210 : int[] = prim::ListConstruct(%20, %20)
      %1211 : int[] = prim::ListConstruct(%20, %20)
      %input.133 : Tensor = aten::conv2d(%input.132, %1207, %1208, %1209, %1210, %1211, %15) # torch/nn/modules/conv.py:415:15
      %1213 : int = aten::dim(%input.133) # torch/nn/modules/batchnorm.py:276:11
      %1214 : bool = aten::ne(%1213, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1214) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1215 : bool = prim::GetAttr[name="training"](%1206)
       = prim::If(%1215) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1216 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1206)
          %1217 : Tensor = aten::add(%1216, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1206, %1217)
          -> ()
        block1():
          -> ()
      %1218 : bool = prim::GetAttr[name="training"](%1206)
      %1219 : Tensor = prim::GetAttr[name="running_mean"](%1206)
      %1220 : Tensor = prim::GetAttr[name="running_var"](%1206)
      %1221 : Tensor = prim::GetAttr[name="weight"](%1206)
      %1222 : Tensor = prim::GetAttr[name="bias"](%1206)
       = prim::If(%1218) # torch/nn/functional.py:2011:4
        block0():
          %1223 : int[] = aten::size(%input.133) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%1223, %19) # torch/nn/functional.py:1991:17
          %1225 : int = aten::len(%1223) # torch/nn/functional.py:1992:19
          %1226 : int = aten::sub(%1225, %11) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%1226, %10, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %1230 : int = aten::add(%i.43, %11) # torch/nn/functional.py:1993:27
              %1231 : int = aten::__getitem__(%1223, %1230) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %1231) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.171)
          %1233 : bool = aten::eq(%size_prods.169, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1233) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.134 : Tensor = aten::batch_norm(%input.133, %1221, %1222, %1219, %1220, %1218, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.135 : Tensor = aten::hardtanh_(%input.134, %5, %4) # torch/nn/functional.py:1171:17
      %1236 : Tensor = prim::GetAttr[name="weight"](%1172)
      %1237 : Tensor? = prim::GetAttr[name="bias"](%1172)
      %1238 : int[] = prim::ListConstruct(%20, %20)
      %1239 : int[] = prim::ListConstruct(%19, %19)
      %1240 : int[] = prim::ListConstruct(%20, %20)
      %input.136 : Tensor = aten::conv2d(%input.135, %1236, %1237, %1238, %1239, %1240, %20) # torch/nn/modules/conv.py:415:15
      %1242 : int = aten::dim(%input.136) # torch/nn/modules/batchnorm.py:276:11
      %1243 : bool = aten::ne(%1242, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1243) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1244 : bool = prim::GetAttr[name="training"](%1173)
       = prim::If(%1244) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1245 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1173)
          %1246 : Tensor = aten::add(%1245, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1173, %1246)
          -> ()
        block1():
          -> ()
      %1247 : bool = prim::GetAttr[name="training"](%1173)
      %1248 : Tensor = prim::GetAttr[name="running_mean"](%1173)
      %1249 : Tensor = prim::GetAttr[name="running_var"](%1173)
      %1250 : Tensor = prim::GetAttr[name="weight"](%1173)
      %1251 : Tensor = prim::GetAttr[name="bias"](%1173)
       = prim::If(%1247) # torch/nn/functional.py:2011:4
        block0():
          %1252 : int[] = aten::size(%input.136) # torch/nn/functional.py:2012:27
          %size_prods.172 : int = aten::__getitem__(%1252, %19) # torch/nn/functional.py:1991:17
          %1254 : int = aten::len(%1252) # torch/nn/functional.py:1992:19
          %1255 : int = aten::sub(%1254, %11) # torch/nn/functional.py:1992:19
          %size_prods.173 : int = prim::Loop(%1255, %10, %size_prods.172) # torch/nn/functional.py:1992:4
            block0(%i.44 : int, %size_prods.174 : int):
              %1259 : int = aten::add(%i.44, %11) # torch/nn/functional.py:1993:27
              %1260 : int = aten::__getitem__(%1252, %1259) # torch/nn/functional.py:1993:22
              %size_prods.175 : int = aten::mul(%size_prods.174, %1260) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.175)
          %1262 : bool = aten::eq(%size_prods.173, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1262) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.137 : Tensor = aten::batch_norm(%input.136, %1250, %1251, %1248, %1249, %1247, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %1264 : Tensor = aten::add(%input.36, %input.137, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%1264)
    block1():
      %1265 : __torch__.torch.nn.modules.container.___torch_mangle_908.Sequential = prim::GetAttr[name="conv"](%30)
      %1266 : __torch__.torchvision.models.mobilenet.___torch_mangle_901.ConvBNReLU = prim::GetAttr[name="0"](%1265)
      %1267 : __torch__.torchvision.models.mobilenet.___torch_mangle_907.ConvBNReLU = prim::GetAttr[name="1"](%1265)
      %1268 : __torch__.torch.nn.modules.conv.___torch_mangle_397.Conv2d = prim::GetAttr[name="2"](%1265)
      %1269 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1265)
      %1270 : __torch__.torch.nn.modules.conv.___torch_mangle_718.Conv2d = prim::GetAttr[name="0"](%1266)
      %1271 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1266)
      %1272 : Tensor = prim::GetAttr[name="weight"](%1270)
      %1273 : Tensor? = prim::GetAttr[name="bias"](%1270)
      %1274 : int[] = prim::ListConstruct(%20, %20)
      %1275 : int[] = prim::ListConstruct(%19, %19)
      %1276 : int[] = prim::ListConstruct(%20, %20)
      %input.138 : Tensor = aten::conv2d(%input.36, %1272, %1273, %1274, %1275, %1276, %20) # torch/nn/modules/conv.py:415:15
      %1278 : int = aten::dim(%input.138) # torch/nn/modules/batchnorm.py:276:11
      %1279 : bool = aten::ne(%1278, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1279) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1280 : bool = prim::GetAttr[name="training"](%1271)
       = prim::If(%1280) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1281 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1271)
          %1282 : Tensor = aten::add(%1281, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1271, %1282)
          -> ()
        block1():
          -> ()
      %1283 : bool = prim::GetAttr[name="training"](%1271)
      %1284 : Tensor = prim::GetAttr[name="running_mean"](%1271)
      %1285 : Tensor = prim::GetAttr[name="running_var"](%1271)
      %1286 : Tensor = prim::GetAttr[name="weight"](%1271)
      %1287 : Tensor = prim::GetAttr[name="bias"](%1271)
       = prim::If(%1283) # torch/nn/functional.py:2011:4
        block0():
          %1288 : int[] = aten::size(%input.138) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%1288, %19) # torch/nn/functional.py:1991:17
          %1290 : int = aten::len(%1288) # torch/nn/functional.py:1992:19
          %1291 : int = aten::sub(%1290, %11) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%1291, %10, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %1295 : int = aten::add(%i.45, %11) # torch/nn/functional.py:1993:27
              %1296 : int = aten::__getitem__(%1288, %1295) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %1296) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.179)
          %1298 : bool = aten::eq(%size_prods.177, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1298) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.139 : Tensor = aten::batch_norm(%input.138, %1286, %1287, %1284, %1285, %1283, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.140 : Tensor = aten::hardtanh_(%input.139, %5, %4) # torch/nn/functional.py:1171:17
      %1301 : __torch__.torch.nn.modules.conv.___torch_mangle_906.Conv2d = prim::GetAttr[name="0"](%1267)
      %1302 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="1"](%1267)
      %1303 : Tensor = prim::GetAttr[name="weight"](%1301)
      %1304 : Tensor? = prim::GetAttr[name="bias"](%1301)
      %1305 : int[] = prim::ListConstruct(%11, %11)
      %1306 : int[] = prim::ListConstruct(%20, %20)
      %1307 : int[] = prim::ListConstruct(%20, %20)
      %input.141 : Tensor = aten::conv2d(%input.140, %1303, %1304, %1305, %1306, %1307, %15) # torch/nn/modules/conv.py:415:15
      %1309 : int = aten::dim(%input.141) # torch/nn/modules/batchnorm.py:276:11
      %1310 : bool = aten::ne(%1309, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1310) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1311 : bool = prim::GetAttr[name="training"](%1302)
       = prim::If(%1311) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1312 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1302)
          %1313 : Tensor = aten::add(%1312, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1302, %1313)
          -> ()
        block1():
          -> ()
      %1314 : bool = prim::GetAttr[name="training"](%1302)
      %1315 : Tensor = prim::GetAttr[name="running_mean"](%1302)
      %1316 : Tensor = prim::GetAttr[name="running_var"](%1302)
      %1317 : Tensor = prim::GetAttr[name="weight"](%1302)
      %1318 : Tensor = prim::GetAttr[name="bias"](%1302)
       = prim::If(%1314) # torch/nn/functional.py:2011:4
        block0():
          %1319 : int[] = aten::size(%input.141) # torch/nn/functional.py:2012:27
          %size_prods.180 : int = aten::__getitem__(%1319, %19) # torch/nn/functional.py:1991:17
          %1321 : int = aten::len(%1319) # torch/nn/functional.py:1992:19
          %1322 : int = aten::sub(%1321, %11) # torch/nn/functional.py:1992:19
          %size_prods.181 : int = prim::Loop(%1322, %10, %size_prods.180) # torch/nn/functional.py:1992:4
            block0(%i.46 : int, %size_prods.182 : int):
              %1326 : int = aten::add(%i.46, %11) # torch/nn/functional.py:1993:27
              %1327 : int = aten::__getitem__(%1319, %1326) # torch/nn/functional.py:1993:22
              %size_prods.183 : int = aten::mul(%size_prods.182, %1327) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.183)
          %1329 : bool = aten::eq(%size_prods.181, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1329) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.142 : Tensor = aten::batch_norm(%input.141, %1317, %1318, %1315, %1316, %1314, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.143 : Tensor = aten::hardtanh_(%input.142, %5, %4) # torch/nn/functional.py:1171:17
      %1332 : Tensor = prim::GetAttr[name="weight"](%1268)
      %1333 : Tensor? = prim::GetAttr[name="bias"](%1268)
      %1334 : int[] = prim::ListConstruct(%20, %20)
      %1335 : int[] = prim::ListConstruct(%19, %19)
      %1336 : int[] = prim::ListConstruct(%20, %20)
      %input.144 : Tensor = aten::conv2d(%input.143, %1332, %1333, %1334, %1335, %1336, %20) # torch/nn/modules/conv.py:415:15
      %1338 : int = aten::dim(%input.144) # torch/nn/modules/batchnorm.py:276:11
      %1339 : bool = aten::ne(%1338, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1339) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1340 : bool = prim::GetAttr[name="training"](%1269)
       = prim::If(%1340) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1341 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1269)
          %1342 : Tensor = aten::add(%1341, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1269, %1342)
          -> ()
        block1():
          -> ()
      %1343 : bool = prim::GetAttr[name="training"](%1269)
      %1344 : Tensor = prim::GetAttr[name="running_mean"](%1269)
      %1345 : Tensor = prim::GetAttr[name="running_var"](%1269)
      %1346 : Tensor = prim::GetAttr[name="weight"](%1269)
      %1347 : Tensor = prim::GetAttr[name="bias"](%1269)
       = prim::If(%1343) # torch/nn/functional.py:2011:4
        block0():
          %1348 : int[] = aten::size(%input.144) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%1348, %19) # torch/nn/functional.py:1991:17
          %1350 : int = aten::len(%1348) # torch/nn/functional.py:1992:19
          %1351 : int = aten::sub(%1350, %11) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%1351, %10, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %1355 : int = aten::add(%i.47, %11) # torch/nn/functional.py:1993:27
              %1356 : int = aten::__getitem__(%1348, %1355) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %1356) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.187)
          %1358 : bool = aten::eq(%size_prods.185, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1358) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.145 : Tensor = aten::batch_norm(%input.144, %1346, %1347, %1344, %1345, %1343, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.145)
  %1360 : bool = prim::GetAttr[name="use_res_connect"](%31)
  %input.19 : Tensor = prim::If(%1360) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %1362 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%31)
      %1363 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1362)
      %1364 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1362)
      %1365 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1362)
      %1366 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1362)
      %1367 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1363)
      %1368 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1363)
      %1369 : Tensor = prim::GetAttr[name="weight"](%1367)
      %1370 : Tensor? = prim::GetAttr[name="bias"](%1367)
      %1371 : int[] = prim::ListConstruct(%20, %20)
      %1372 : int[] = prim::ListConstruct(%19, %19)
      %1373 : int[] = prim::ListConstruct(%20, %20)
      %input.146 : Tensor = aten::conv2d(%input.40, %1369, %1370, %1371, %1372, %1373, %20) # torch/nn/modules/conv.py:415:15
      %1375 : int = aten::dim(%input.146) # torch/nn/modules/batchnorm.py:276:11
      %1376 : bool = aten::ne(%1375, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1376) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1377 : bool = prim::GetAttr[name="training"](%1368)
       = prim::If(%1377) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1378 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1368)
          %1379 : Tensor = aten::add(%1378, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1368, %1379)
          -> ()
        block1():
          -> ()
      %1380 : bool = prim::GetAttr[name="training"](%1368)
      %1381 : Tensor = prim::GetAttr[name="running_mean"](%1368)
      %1382 : Tensor = prim::GetAttr[name="running_var"](%1368)
      %1383 : Tensor = prim::GetAttr[name="weight"](%1368)
      %1384 : Tensor = prim::GetAttr[name="bias"](%1368)
       = prim::If(%1380) # torch/nn/functional.py:2011:4
        block0():
          %1385 : int[] = aten::size(%input.146) # torch/nn/functional.py:2012:27
          %size_prods.188 : int = aten::__getitem__(%1385, %19) # torch/nn/functional.py:1991:17
          %1387 : int = aten::len(%1385) # torch/nn/functional.py:1992:19
          %1388 : int = aten::sub(%1387, %11) # torch/nn/functional.py:1992:19
          %size_prods.189 : int = prim::Loop(%1388, %10, %size_prods.188) # torch/nn/functional.py:1992:4
            block0(%i.48 : int, %size_prods.190 : int):
              %1392 : int = aten::add(%i.48, %11) # torch/nn/functional.py:1993:27
              %1393 : int = aten::__getitem__(%1385, %1392) # torch/nn/functional.py:1993:22
              %size_prods.191 : int = aten::mul(%size_prods.190, %1393) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.191)
          %1395 : bool = aten::eq(%size_prods.189, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1395) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.147 : Tensor = aten::batch_norm(%input.146, %1383, %1384, %1381, %1382, %1380, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.148 : Tensor = aten::hardtanh_(%input.147, %5, %4) # torch/nn/functional.py:1171:17
      %1398 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1364)
      %1399 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1364)
      %1400 : Tensor = prim::GetAttr[name="weight"](%1398)
      %1401 : Tensor? = prim::GetAttr[name="bias"](%1398)
      %1402 : int[] = prim::ListConstruct(%20, %20)
      %1403 : int[] = prim::ListConstruct(%20, %20)
      %1404 : int[] = prim::ListConstruct(%20, %20)
      %input.149 : Tensor = aten::conv2d(%input.148, %1400, %1401, %1402, %1403, %1404, %16) # torch/nn/modules/conv.py:415:15
      %1406 : int = aten::dim(%input.149) # torch/nn/modules/batchnorm.py:276:11
      %1407 : bool = aten::ne(%1406, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1407) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1408 : bool = prim::GetAttr[name="training"](%1399)
       = prim::If(%1408) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1409 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1399)
          %1410 : Tensor = aten::add(%1409, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1399, %1410)
          -> ()
        block1():
          -> ()
      %1411 : bool = prim::GetAttr[name="training"](%1399)
      %1412 : Tensor = prim::GetAttr[name="running_mean"](%1399)
      %1413 : Tensor = prim::GetAttr[name="running_var"](%1399)
      %1414 : Tensor = prim::GetAttr[name="weight"](%1399)
      %1415 : Tensor = prim::GetAttr[name="bias"](%1399)
       = prim::If(%1411) # torch/nn/functional.py:2011:4
        block0():
          %1416 : int[] = aten::size(%input.149) # torch/nn/functional.py:2012:27
          %size_prods.192 : int = aten::__getitem__(%1416, %19) # torch/nn/functional.py:1991:17
          %1418 : int = aten::len(%1416) # torch/nn/functional.py:1992:19
          %1419 : int = aten::sub(%1418, %11) # torch/nn/functional.py:1992:19
          %size_prods.193 : int = prim::Loop(%1419, %10, %size_prods.192) # torch/nn/functional.py:1992:4
            block0(%i.49 : int, %size_prods.194 : int):
              %1423 : int = aten::add(%i.49, %11) # torch/nn/functional.py:1993:27
              %1424 : int = aten::__getitem__(%1416, %1423) # torch/nn/functional.py:1993:22
              %size_prods.195 : int = aten::mul(%size_prods.194, %1424) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.195)
          %1426 : bool = aten::eq(%size_prods.193, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1426) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.150 : Tensor = aten::batch_norm(%input.149, %1414, %1415, %1412, %1413, %1411, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.151 : Tensor = aten::hardtanh_(%input.150, %5, %4) # torch/nn/functional.py:1171:17
      %1429 : Tensor = prim::GetAttr[name="weight"](%1365)
      %1430 : Tensor? = prim::GetAttr[name="bias"](%1365)
      %1431 : int[] = prim::ListConstruct(%20, %20)
      %1432 : int[] = prim::ListConstruct(%19, %19)
      %1433 : int[] = prim::ListConstruct(%20, %20)
      %input.152 : Tensor = aten::conv2d(%input.151, %1429, %1430, %1431, %1432, %1433, %20) # torch/nn/modules/conv.py:415:15
      %1435 : int = aten::dim(%input.152) # torch/nn/modules/batchnorm.py:276:11
      %1436 : bool = aten::ne(%1435, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1436) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1437 : bool = prim::GetAttr[name="training"](%1366)
       = prim::If(%1437) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1438 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1366)
          %1439 : Tensor = aten::add(%1438, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1366, %1439)
          -> ()
        block1():
          -> ()
      %1440 : bool = prim::GetAttr[name="training"](%1366)
      %1441 : Tensor = prim::GetAttr[name="running_mean"](%1366)
      %1442 : Tensor = prim::GetAttr[name="running_var"](%1366)
      %1443 : Tensor = prim::GetAttr[name="weight"](%1366)
      %1444 : Tensor = prim::GetAttr[name="bias"](%1366)
       = prim::If(%1440) # torch/nn/functional.py:2011:4
        block0():
          %1445 : int[] = aten::size(%input.152) # torch/nn/functional.py:2012:27
          %size_prods.196 : int = aten::__getitem__(%1445, %19) # torch/nn/functional.py:1991:17
          %1447 : int = aten::len(%1445) # torch/nn/functional.py:1992:19
          %1448 : int = aten::sub(%1447, %11) # torch/nn/functional.py:1992:19
          %size_prods.197 : int = prim::Loop(%1448, %10, %size_prods.196) # torch/nn/functional.py:1992:4
            block0(%i.50 : int, %size_prods.198 : int):
              %1452 : int = aten::add(%i.50, %11) # torch/nn/functional.py:1993:27
              %1453 : int = aten::__getitem__(%1445, %1452) # torch/nn/functional.py:1993:22
              %size_prods.199 : int = aten::mul(%size_prods.198, %1453) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.199)
          %1455 : bool = aten::eq(%size_prods.197, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1455) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.153 : Tensor = aten::batch_norm(%input.152, %1443, %1444, %1441, %1442, %1440, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %1457 : Tensor = aten::add(%input.40, %input.153, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%1457)
    block1():
      %1458 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%31)
      %1459 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1458)
      %1460 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1458)
      %1461 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1458)
      %1462 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1458)
      %1463 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1459)
      %1464 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1459)
      %1465 : Tensor = prim::GetAttr[name="weight"](%1463)
      %1466 : Tensor? = prim::GetAttr[name="bias"](%1463)
      %1467 : int[] = prim::ListConstruct(%20, %20)
      %1468 : int[] = prim::ListConstruct(%19, %19)
      %1469 : int[] = prim::ListConstruct(%20, %20)
      %input.154 : Tensor = aten::conv2d(%input.40, %1465, %1466, %1467, %1468, %1469, %20) # torch/nn/modules/conv.py:415:15
      %1471 : int = aten::dim(%input.154) # torch/nn/modules/batchnorm.py:276:11
      %1472 : bool = aten::ne(%1471, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1472) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1473 : bool = prim::GetAttr[name="training"](%1464)
       = prim::If(%1473) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1474 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1464)
          %1475 : Tensor = aten::add(%1474, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1464, %1475)
          -> ()
        block1():
          -> ()
      %1476 : bool = prim::GetAttr[name="training"](%1464)
      %1477 : Tensor = prim::GetAttr[name="running_mean"](%1464)
      %1478 : Tensor = prim::GetAttr[name="running_var"](%1464)
      %1479 : Tensor = prim::GetAttr[name="weight"](%1464)
      %1480 : Tensor = prim::GetAttr[name="bias"](%1464)
       = prim::If(%1476) # torch/nn/functional.py:2011:4
        block0():
          %1481 : int[] = aten::size(%input.154) # torch/nn/functional.py:2012:27
          %size_prods.200 : int = aten::__getitem__(%1481, %19) # torch/nn/functional.py:1991:17
          %1483 : int = aten::len(%1481) # torch/nn/functional.py:1992:19
          %1484 : int = aten::sub(%1483, %11) # torch/nn/functional.py:1992:19
          %size_prods.201 : int = prim::Loop(%1484, %10, %size_prods.200) # torch/nn/functional.py:1992:4
            block0(%i.51 : int, %size_prods.202 : int):
              %1488 : int = aten::add(%i.51, %11) # torch/nn/functional.py:1993:27
              %1489 : int = aten::__getitem__(%1481, %1488) # torch/nn/functional.py:1993:22
              %size_prods.203 : int = aten::mul(%size_prods.202, %1489) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.203)
          %1491 : bool = aten::eq(%size_prods.201, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1491) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.155 : Tensor = aten::batch_norm(%input.154, %1479, %1480, %1477, %1478, %1476, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.156 : Tensor = aten::hardtanh_(%input.155, %5, %4) # torch/nn/functional.py:1171:17
      %1494 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1460)
      %1495 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1460)
      %1496 : Tensor = prim::GetAttr[name="weight"](%1494)
      %1497 : Tensor? = prim::GetAttr[name="bias"](%1494)
      %1498 : int[] = prim::ListConstruct(%20, %20)
      %1499 : int[] = prim::ListConstruct(%20, %20)
      %1500 : int[] = prim::ListConstruct(%20, %20)
      %input.157 : Tensor = aten::conv2d(%input.156, %1496, %1497, %1498, %1499, %1500, %16) # torch/nn/modules/conv.py:415:15
      %1502 : int = aten::dim(%input.157) # torch/nn/modules/batchnorm.py:276:11
      %1503 : bool = aten::ne(%1502, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1503) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1504 : bool = prim::GetAttr[name="training"](%1495)
       = prim::If(%1504) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1505 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1495)
          %1506 : Tensor = aten::add(%1505, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1495, %1506)
          -> ()
        block1():
          -> ()
      %1507 : bool = prim::GetAttr[name="training"](%1495)
      %1508 : Tensor = prim::GetAttr[name="running_mean"](%1495)
      %1509 : Tensor = prim::GetAttr[name="running_var"](%1495)
      %1510 : Tensor = prim::GetAttr[name="weight"](%1495)
      %1511 : Tensor = prim::GetAttr[name="bias"](%1495)
       = prim::If(%1507) # torch/nn/functional.py:2011:4
        block0():
          %1512 : int[] = aten::size(%input.157) # torch/nn/functional.py:2012:27
          %size_prods.204 : int = aten::__getitem__(%1512, %19) # torch/nn/functional.py:1991:17
          %1514 : int = aten::len(%1512) # torch/nn/functional.py:1992:19
          %1515 : int = aten::sub(%1514, %11) # torch/nn/functional.py:1992:19
          %size_prods.205 : int = prim::Loop(%1515, %10, %size_prods.204) # torch/nn/functional.py:1992:4
            block0(%i.52 : int, %size_prods.206 : int):
              %1519 : int = aten::add(%i.52, %11) # torch/nn/functional.py:1993:27
              %1520 : int = aten::__getitem__(%1512, %1519) # torch/nn/functional.py:1993:22
              %size_prods.207 : int = aten::mul(%size_prods.206, %1520) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.207)
          %1522 : bool = aten::eq(%size_prods.205, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1522) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.158 : Tensor = aten::batch_norm(%input.157, %1510, %1511, %1508, %1509, %1507, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.159 : Tensor = aten::hardtanh_(%input.158, %5, %4) # torch/nn/functional.py:1171:17
      %1525 : Tensor = prim::GetAttr[name="weight"](%1461)
      %1526 : Tensor? = prim::GetAttr[name="bias"](%1461)
      %1527 : int[] = prim::ListConstruct(%20, %20)
      %1528 : int[] = prim::ListConstruct(%19, %19)
      %1529 : int[] = prim::ListConstruct(%20, %20)
      %input.160 : Tensor = aten::conv2d(%input.159, %1525, %1526, %1527, %1528, %1529, %20) # torch/nn/modules/conv.py:415:15
      %1531 : int = aten::dim(%input.160) # torch/nn/modules/batchnorm.py:276:11
      %1532 : bool = aten::ne(%1531, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1532) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1533 : bool = prim::GetAttr[name="training"](%1462)
       = prim::If(%1533) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1534 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1462)
          %1535 : Tensor = aten::add(%1534, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1462, %1535)
          -> ()
        block1():
          -> ()
      %1536 : bool = prim::GetAttr[name="training"](%1462)
      %1537 : Tensor = prim::GetAttr[name="running_mean"](%1462)
      %1538 : Tensor = prim::GetAttr[name="running_var"](%1462)
      %1539 : Tensor = prim::GetAttr[name="weight"](%1462)
      %1540 : Tensor = prim::GetAttr[name="bias"](%1462)
       = prim::If(%1536) # torch/nn/functional.py:2011:4
        block0():
          %1541 : int[] = aten::size(%input.160) # torch/nn/functional.py:2012:27
          %size_prods.208 : int = aten::__getitem__(%1541, %19) # torch/nn/functional.py:1991:17
          %1543 : int = aten::len(%1541) # torch/nn/functional.py:1992:19
          %1544 : int = aten::sub(%1543, %11) # torch/nn/functional.py:1992:19
          %size_prods.209 : int = prim::Loop(%1544, %10, %size_prods.208) # torch/nn/functional.py:1992:4
            block0(%i.53 : int, %size_prods.210 : int):
              %1548 : int = aten::add(%i.53, %11) # torch/nn/functional.py:1993:27
              %1549 : int = aten::__getitem__(%1541, %1548) # torch/nn/functional.py:1993:22
              %size_prods.211 : int = aten::mul(%size_prods.210, %1549) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.211)
          %1551 : bool = aten::eq(%size_prods.209, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1551) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.161 : Tensor = aten::batch_norm(%input.160, %1539, %1540, %1537, %1538, %1536, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.161)
  %1553 : bool = prim::GetAttr[name="use_res_connect"](%32)
  %input.21 : Tensor = prim::If(%1553) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %1555 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%32)
      %1556 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1555)
      %1557 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1555)
      %1558 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1555)
      %1559 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1555)
      %1560 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1556)
      %1561 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1556)
      %1562 : Tensor = prim::GetAttr[name="weight"](%1560)
      %1563 : Tensor? = prim::GetAttr[name="bias"](%1560)
      %1564 : int[] = prim::ListConstruct(%20, %20)
      %1565 : int[] = prim::ListConstruct(%19, %19)
      %1566 : int[] = prim::ListConstruct(%20, %20)
      %input.162 : Tensor = aten::conv2d(%input.19, %1562, %1563, %1564, %1565, %1566, %20) # torch/nn/modules/conv.py:415:15
      %1568 : int = aten::dim(%input.162) # torch/nn/modules/batchnorm.py:276:11
      %1569 : bool = aten::ne(%1568, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1569) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1570 : bool = prim::GetAttr[name="training"](%1561)
       = prim::If(%1570) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1571 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1561)
          %1572 : Tensor = aten::add(%1571, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1561, %1572)
          -> ()
        block1():
          -> ()
      %1573 : bool = prim::GetAttr[name="training"](%1561)
      %1574 : Tensor = prim::GetAttr[name="running_mean"](%1561)
      %1575 : Tensor = prim::GetAttr[name="running_var"](%1561)
      %1576 : Tensor = prim::GetAttr[name="weight"](%1561)
      %1577 : Tensor = prim::GetAttr[name="bias"](%1561)
       = prim::If(%1573) # torch/nn/functional.py:2011:4
        block0():
          %1578 : int[] = aten::size(%input.162) # torch/nn/functional.py:2012:27
          %size_prods.212 : int = aten::__getitem__(%1578, %19) # torch/nn/functional.py:1991:17
          %1580 : int = aten::len(%1578) # torch/nn/functional.py:1992:19
          %1581 : int = aten::sub(%1580, %11) # torch/nn/functional.py:1992:19
          %size_prods.213 : int = prim::Loop(%1581, %10, %size_prods.212) # torch/nn/functional.py:1992:4
            block0(%i.54 : int, %size_prods.214 : int):
              %1585 : int = aten::add(%i.54, %11) # torch/nn/functional.py:1993:27
              %1586 : int = aten::__getitem__(%1578, %1585) # torch/nn/functional.py:1993:22
              %size_prods.215 : int = aten::mul(%size_prods.214, %1586) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.215)
          %1588 : bool = aten::eq(%size_prods.213, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1588) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.163 : Tensor = aten::batch_norm(%input.162, %1576, %1577, %1574, %1575, %1573, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.164 : Tensor = aten::hardtanh_(%input.163, %5, %4) # torch/nn/functional.py:1171:17
      %1591 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1557)
      %1592 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1557)
      %1593 : Tensor = prim::GetAttr[name="weight"](%1591)
      %1594 : Tensor? = prim::GetAttr[name="bias"](%1591)
      %1595 : int[] = prim::ListConstruct(%20, %20)
      %1596 : int[] = prim::ListConstruct(%20, %20)
      %1597 : int[] = prim::ListConstruct(%20, %20)
      %input.165 : Tensor = aten::conv2d(%input.164, %1593, %1594, %1595, %1596, %1597, %16) # torch/nn/modules/conv.py:415:15
      %1599 : int = aten::dim(%input.165) # torch/nn/modules/batchnorm.py:276:11
      %1600 : bool = aten::ne(%1599, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1600) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1601 : bool = prim::GetAttr[name="training"](%1592)
       = prim::If(%1601) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1602 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1592)
          %1603 : Tensor = aten::add(%1602, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1592, %1603)
          -> ()
        block1():
          -> ()
      %1604 : bool = prim::GetAttr[name="training"](%1592)
      %1605 : Tensor = prim::GetAttr[name="running_mean"](%1592)
      %1606 : Tensor = prim::GetAttr[name="running_var"](%1592)
      %1607 : Tensor = prim::GetAttr[name="weight"](%1592)
      %1608 : Tensor = prim::GetAttr[name="bias"](%1592)
       = prim::If(%1604) # torch/nn/functional.py:2011:4
        block0():
          %1609 : int[] = aten::size(%input.165) # torch/nn/functional.py:2012:27
          %size_prods.216 : int = aten::__getitem__(%1609, %19) # torch/nn/functional.py:1991:17
          %1611 : int = aten::len(%1609) # torch/nn/functional.py:1992:19
          %1612 : int = aten::sub(%1611, %11) # torch/nn/functional.py:1992:19
          %size_prods.217 : int = prim::Loop(%1612, %10, %size_prods.216) # torch/nn/functional.py:1992:4
            block0(%i.55 : int, %size_prods.218 : int):
              %1616 : int = aten::add(%i.55, %11) # torch/nn/functional.py:1993:27
              %1617 : int = aten::__getitem__(%1609, %1616) # torch/nn/functional.py:1993:22
              %size_prods.219 : int = aten::mul(%size_prods.218, %1617) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.219)
          %1619 : bool = aten::eq(%size_prods.217, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1619) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.166 : Tensor = aten::batch_norm(%input.165, %1607, %1608, %1605, %1606, %1604, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.167 : Tensor = aten::hardtanh_(%input.166, %5, %4) # torch/nn/functional.py:1171:17
      %1622 : Tensor = prim::GetAttr[name="weight"](%1558)
      %1623 : Tensor? = prim::GetAttr[name="bias"](%1558)
      %1624 : int[] = prim::ListConstruct(%20, %20)
      %1625 : int[] = prim::ListConstruct(%19, %19)
      %1626 : int[] = prim::ListConstruct(%20, %20)
      %input.168 : Tensor = aten::conv2d(%input.167, %1622, %1623, %1624, %1625, %1626, %20) # torch/nn/modules/conv.py:415:15
      %1628 : int = aten::dim(%input.168) # torch/nn/modules/batchnorm.py:276:11
      %1629 : bool = aten::ne(%1628, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1629) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1630 : bool = prim::GetAttr[name="training"](%1559)
       = prim::If(%1630) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1631 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1559)
          %1632 : Tensor = aten::add(%1631, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1559, %1632)
          -> ()
        block1():
          -> ()
      %1633 : bool = prim::GetAttr[name="training"](%1559)
      %1634 : Tensor = prim::GetAttr[name="running_mean"](%1559)
      %1635 : Tensor = prim::GetAttr[name="running_var"](%1559)
      %1636 : Tensor = prim::GetAttr[name="weight"](%1559)
      %1637 : Tensor = prim::GetAttr[name="bias"](%1559)
       = prim::If(%1633) # torch/nn/functional.py:2011:4
        block0():
          %1638 : int[] = aten::size(%input.168) # torch/nn/functional.py:2012:27
          %size_prods.220 : int = aten::__getitem__(%1638, %19) # torch/nn/functional.py:1991:17
          %1640 : int = aten::len(%1638) # torch/nn/functional.py:1992:19
          %1641 : int = aten::sub(%1640, %11) # torch/nn/functional.py:1992:19
          %size_prods.221 : int = prim::Loop(%1641, %10, %size_prods.220) # torch/nn/functional.py:1992:4
            block0(%i.56 : int, %size_prods.222 : int):
              %1645 : int = aten::add(%i.56, %11) # torch/nn/functional.py:1993:27
              %1646 : int = aten::__getitem__(%1638, %1645) # torch/nn/functional.py:1993:22
              %size_prods.223 : int = aten::mul(%size_prods.222, %1646) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.223)
          %1648 : bool = aten::eq(%size_prods.221, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1648) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.169 : Tensor = aten::batch_norm(%input.168, %1636, %1637, %1634, %1635, %1633, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %1650 : Tensor = aten::add(%input.19, %input.169, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%1650)
    block1():
      %1651 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%32)
      %1652 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1651)
      %1653 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1651)
      %1654 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1651)
      %1655 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1651)
      %1656 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1652)
      %1657 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1652)
      %1658 : Tensor = prim::GetAttr[name="weight"](%1656)
      %1659 : Tensor? = prim::GetAttr[name="bias"](%1656)
      %1660 : int[] = prim::ListConstruct(%20, %20)
      %1661 : int[] = prim::ListConstruct(%19, %19)
      %1662 : int[] = prim::ListConstruct(%20, %20)
      %input.170 : Tensor = aten::conv2d(%input.19, %1658, %1659, %1660, %1661, %1662, %20) # torch/nn/modules/conv.py:415:15
      %1664 : int = aten::dim(%input.170) # torch/nn/modules/batchnorm.py:276:11
      %1665 : bool = aten::ne(%1664, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1665) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1666 : bool = prim::GetAttr[name="training"](%1657)
       = prim::If(%1666) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1667 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1657)
          %1668 : Tensor = aten::add(%1667, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1657, %1668)
          -> ()
        block1():
          -> ()
      %1669 : bool = prim::GetAttr[name="training"](%1657)
      %1670 : Tensor = prim::GetAttr[name="running_mean"](%1657)
      %1671 : Tensor = prim::GetAttr[name="running_var"](%1657)
      %1672 : Tensor = prim::GetAttr[name="weight"](%1657)
      %1673 : Tensor = prim::GetAttr[name="bias"](%1657)
       = prim::If(%1669) # torch/nn/functional.py:2011:4
        block0():
          %1674 : int[] = aten::size(%input.170) # torch/nn/functional.py:2012:27
          %size_prods.224 : int = aten::__getitem__(%1674, %19) # torch/nn/functional.py:1991:17
          %1676 : int = aten::len(%1674) # torch/nn/functional.py:1992:19
          %1677 : int = aten::sub(%1676, %11) # torch/nn/functional.py:1992:19
          %size_prods.225 : int = prim::Loop(%1677, %10, %size_prods.224) # torch/nn/functional.py:1992:4
            block0(%i.57 : int, %size_prods.226 : int):
              %1681 : int = aten::add(%i.57, %11) # torch/nn/functional.py:1993:27
              %1682 : int = aten::__getitem__(%1674, %1681) # torch/nn/functional.py:1993:22
              %size_prods.227 : int = aten::mul(%size_prods.226, %1682) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.227)
          %1684 : bool = aten::eq(%size_prods.225, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1684) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.171 : Tensor = aten::batch_norm(%input.170, %1672, %1673, %1670, %1671, %1669, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.172 : Tensor = aten::hardtanh_(%input.171, %5, %4) # torch/nn/functional.py:1171:17
      %1687 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1653)
      %1688 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1653)
      %1689 : Tensor = prim::GetAttr[name="weight"](%1687)
      %1690 : Tensor? = prim::GetAttr[name="bias"](%1687)
      %1691 : int[] = prim::ListConstruct(%20, %20)
      %1692 : int[] = prim::ListConstruct(%20, %20)
      %1693 : int[] = prim::ListConstruct(%20, %20)
      %input.173 : Tensor = aten::conv2d(%input.172, %1689, %1690, %1691, %1692, %1693, %16) # torch/nn/modules/conv.py:415:15
      %1695 : int = aten::dim(%input.173) # torch/nn/modules/batchnorm.py:276:11
      %1696 : bool = aten::ne(%1695, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1696) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1697 : bool = prim::GetAttr[name="training"](%1688)
       = prim::If(%1697) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1698 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1688)
          %1699 : Tensor = aten::add(%1698, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1688, %1699)
          -> ()
        block1():
          -> ()
      %1700 : bool = prim::GetAttr[name="training"](%1688)
      %1701 : Tensor = prim::GetAttr[name="running_mean"](%1688)
      %1702 : Tensor = prim::GetAttr[name="running_var"](%1688)
      %1703 : Tensor = prim::GetAttr[name="weight"](%1688)
      %1704 : Tensor = prim::GetAttr[name="bias"](%1688)
       = prim::If(%1700) # torch/nn/functional.py:2011:4
        block0():
          %1705 : int[] = aten::size(%input.173) # torch/nn/functional.py:2012:27
          %size_prods.228 : int = aten::__getitem__(%1705, %19) # torch/nn/functional.py:1991:17
          %1707 : int = aten::len(%1705) # torch/nn/functional.py:1992:19
          %1708 : int = aten::sub(%1707, %11) # torch/nn/functional.py:1992:19
          %size_prods.229 : int = prim::Loop(%1708, %10, %size_prods.228) # torch/nn/functional.py:1992:4
            block0(%i.58 : int, %size_prods.230 : int):
              %1712 : int = aten::add(%i.58, %11) # torch/nn/functional.py:1993:27
              %1713 : int = aten::__getitem__(%1705, %1712) # torch/nn/functional.py:1993:22
              %size_prods.231 : int = aten::mul(%size_prods.230, %1713) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.231)
          %1715 : bool = aten::eq(%size_prods.229, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1715) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.174 : Tensor = aten::batch_norm(%input.173, %1703, %1704, %1701, %1702, %1700, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.175 : Tensor = aten::hardtanh_(%input.174, %5, %4) # torch/nn/functional.py:1171:17
      %1718 : Tensor = prim::GetAttr[name="weight"](%1654)
      %1719 : Tensor? = prim::GetAttr[name="bias"](%1654)
      %1720 : int[] = prim::ListConstruct(%20, %20)
      %1721 : int[] = prim::ListConstruct(%19, %19)
      %1722 : int[] = prim::ListConstruct(%20, %20)
      %input.176 : Tensor = aten::conv2d(%input.175, %1718, %1719, %1720, %1721, %1722, %20) # torch/nn/modules/conv.py:415:15
      %1724 : int = aten::dim(%input.176) # torch/nn/modules/batchnorm.py:276:11
      %1725 : bool = aten::ne(%1724, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1725) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1726 : bool = prim::GetAttr[name="training"](%1655)
       = prim::If(%1726) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1655)
          %1728 : Tensor = aten::add(%1727, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1655, %1728)
          -> ()
        block1():
          -> ()
      %1729 : bool = prim::GetAttr[name="training"](%1655)
      %1730 : Tensor = prim::GetAttr[name="running_mean"](%1655)
      %1731 : Tensor = prim::GetAttr[name="running_var"](%1655)
      %1732 : Tensor = prim::GetAttr[name="weight"](%1655)
      %1733 : Tensor = prim::GetAttr[name="bias"](%1655)
       = prim::If(%1729) # torch/nn/functional.py:2011:4
        block0():
          %1734 : int[] = aten::size(%input.176) # torch/nn/functional.py:2012:27
          %size_prods.232 : int = aten::__getitem__(%1734, %19) # torch/nn/functional.py:1991:17
          %1736 : int = aten::len(%1734) # torch/nn/functional.py:1992:19
          %1737 : int = aten::sub(%1736, %11) # torch/nn/functional.py:1992:19
          %size_prods.233 : int = prim::Loop(%1737, %10, %size_prods.232) # torch/nn/functional.py:1992:4
            block0(%i.59 : int, %size_prods.234 : int):
              %1741 : int = aten::add(%i.59, %11) # torch/nn/functional.py:1993:27
              %1742 : int = aten::__getitem__(%1734, %1741) # torch/nn/functional.py:1993:22
              %size_prods.235 : int = aten::mul(%size_prods.234, %1742) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.235)
          %1744 : bool = aten::eq(%size_prods.233, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1744) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.177 : Tensor = aten::batch_norm(%input.176, %1732, %1733, %1730, %1731, %1729, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.177)
  %1746 : bool = prim::GetAttr[name="use_res_connect"](%33)
  %input.23 : Tensor = prim::If(%1746) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %1748 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%33)
      %1749 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1748)
      %1750 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1748)
      %1751 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1748)
      %1752 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1748)
      %1753 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1749)
      %1754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1749)
      %1755 : Tensor = prim::GetAttr[name="weight"](%1753)
      %1756 : Tensor? = prim::GetAttr[name="bias"](%1753)
      %1757 : int[] = prim::ListConstruct(%20, %20)
      %1758 : int[] = prim::ListConstruct(%19, %19)
      %1759 : int[] = prim::ListConstruct(%20, %20)
      %input.178 : Tensor = aten::conv2d(%input.21, %1755, %1756, %1757, %1758, %1759, %20) # torch/nn/modules/conv.py:415:15
      %1761 : int = aten::dim(%input.178) # torch/nn/modules/batchnorm.py:276:11
      %1762 : bool = aten::ne(%1761, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1762) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1763 : bool = prim::GetAttr[name="training"](%1754)
       = prim::If(%1763) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1764 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1754)
          %1765 : Tensor = aten::add(%1764, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1754, %1765)
          -> ()
        block1():
          -> ()
      %1766 : bool = prim::GetAttr[name="training"](%1754)
      %1767 : Tensor = prim::GetAttr[name="running_mean"](%1754)
      %1768 : Tensor = prim::GetAttr[name="running_var"](%1754)
      %1769 : Tensor = prim::GetAttr[name="weight"](%1754)
      %1770 : Tensor = prim::GetAttr[name="bias"](%1754)
       = prim::If(%1766) # torch/nn/functional.py:2011:4
        block0():
          %1771 : int[] = aten::size(%input.178) # torch/nn/functional.py:2012:27
          %size_prods.236 : int = aten::__getitem__(%1771, %19) # torch/nn/functional.py:1991:17
          %1773 : int = aten::len(%1771) # torch/nn/functional.py:1992:19
          %1774 : int = aten::sub(%1773, %11) # torch/nn/functional.py:1992:19
          %size_prods.237 : int = prim::Loop(%1774, %10, %size_prods.236) # torch/nn/functional.py:1992:4
            block0(%i.60 : int, %size_prods.238 : int):
              %1778 : int = aten::add(%i.60, %11) # torch/nn/functional.py:1993:27
              %1779 : int = aten::__getitem__(%1771, %1778) # torch/nn/functional.py:1993:22
              %size_prods.239 : int = aten::mul(%size_prods.238, %1779) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.239)
          %1781 : bool = aten::eq(%size_prods.237, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1781) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.179 : Tensor = aten::batch_norm(%input.178, %1769, %1770, %1767, %1768, %1766, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.180 : Tensor = aten::hardtanh_(%input.179, %5, %4) # torch/nn/functional.py:1171:17
      %1784 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1750)
      %1785 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1750)
      %1786 : Tensor = prim::GetAttr[name="weight"](%1784)
      %1787 : Tensor? = prim::GetAttr[name="bias"](%1784)
      %1788 : int[] = prim::ListConstruct(%20, %20)
      %1789 : int[] = prim::ListConstruct(%20, %20)
      %1790 : int[] = prim::ListConstruct(%20, %20)
      %input.181 : Tensor = aten::conv2d(%input.180, %1786, %1787, %1788, %1789, %1790, %16) # torch/nn/modules/conv.py:415:15
      %1792 : int = aten::dim(%input.181) # torch/nn/modules/batchnorm.py:276:11
      %1793 : bool = aten::ne(%1792, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1793) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1794 : bool = prim::GetAttr[name="training"](%1785)
       = prim::If(%1794) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1795 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1785)
          %1796 : Tensor = aten::add(%1795, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1785, %1796)
          -> ()
        block1():
          -> ()
      %1797 : bool = prim::GetAttr[name="training"](%1785)
      %1798 : Tensor = prim::GetAttr[name="running_mean"](%1785)
      %1799 : Tensor = prim::GetAttr[name="running_var"](%1785)
      %1800 : Tensor = prim::GetAttr[name="weight"](%1785)
      %1801 : Tensor = prim::GetAttr[name="bias"](%1785)
       = prim::If(%1797) # torch/nn/functional.py:2011:4
        block0():
          %1802 : int[] = aten::size(%input.181) # torch/nn/functional.py:2012:27
          %size_prods.240 : int = aten::__getitem__(%1802, %19) # torch/nn/functional.py:1991:17
          %1804 : int = aten::len(%1802) # torch/nn/functional.py:1992:19
          %1805 : int = aten::sub(%1804, %11) # torch/nn/functional.py:1992:19
          %size_prods.241 : int = prim::Loop(%1805, %10, %size_prods.240) # torch/nn/functional.py:1992:4
            block0(%i.61 : int, %size_prods.242 : int):
              %1809 : int = aten::add(%i.61, %11) # torch/nn/functional.py:1993:27
              %1810 : int = aten::__getitem__(%1802, %1809) # torch/nn/functional.py:1993:22
              %size_prods.243 : int = aten::mul(%size_prods.242, %1810) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.243)
          %1812 : bool = aten::eq(%size_prods.241, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1812) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.182 : Tensor = aten::batch_norm(%input.181, %1800, %1801, %1798, %1799, %1797, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.183 : Tensor = aten::hardtanh_(%input.182, %5, %4) # torch/nn/functional.py:1171:17
      %1815 : Tensor = prim::GetAttr[name="weight"](%1751)
      %1816 : Tensor? = prim::GetAttr[name="bias"](%1751)
      %1817 : int[] = prim::ListConstruct(%20, %20)
      %1818 : int[] = prim::ListConstruct(%19, %19)
      %1819 : int[] = prim::ListConstruct(%20, %20)
      %input.184 : Tensor = aten::conv2d(%input.183, %1815, %1816, %1817, %1818, %1819, %20) # torch/nn/modules/conv.py:415:15
      %1821 : int = aten::dim(%input.184) # torch/nn/modules/batchnorm.py:276:11
      %1822 : bool = aten::ne(%1821, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1822) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1823 : bool = prim::GetAttr[name="training"](%1752)
       = prim::If(%1823) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1824 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1752)
          %1825 : Tensor = aten::add(%1824, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1752, %1825)
          -> ()
        block1():
          -> ()
      %1826 : bool = prim::GetAttr[name="training"](%1752)
      %1827 : Tensor = prim::GetAttr[name="running_mean"](%1752)
      %1828 : Tensor = prim::GetAttr[name="running_var"](%1752)
      %1829 : Tensor = prim::GetAttr[name="weight"](%1752)
      %1830 : Tensor = prim::GetAttr[name="bias"](%1752)
       = prim::If(%1826) # torch/nn/functional.py:2011:4
        block0():
          %1831 : int[] = aten::size(%input.184) # torch/nn/functional.py:2012:27
          %size_prods.244 : int = aten::__getitem__(%1831, %19) # torch/nn/functional.py:1991:17
          %1833 : int = aten::len(%1831) # torch/nn/functional.py:1992:19
          %1834 : int = aten::sub(%1833, %11) # torch/nn/functional.py:1992:19
          %size_prods.245 : int = prim::Loop(%1834, %10, %size_prods.244) # torch/nn/functional.py:1992:4
            block0(%i.62 : int, %size_prods.246 : int):
              %1838 : int = aten::add(%i.62, %11) # torch/nn/functional.py:1993:27
              %1839 : int = aten::__getitem__(%1831, %1838) # torch/nn/functional.py:1993:22
              %size_prods.247 : int = aten::mul(%size_prods.246, %1839) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.247)
          %1841 : bool = aten::eq(%size_prods.245, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1841) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.185 : Tensor = aten::batch_norm(%input.184, %1829, %1830, %1827, %1828, %1826, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %1843 : Tensor = aten::add(%input.21, %input.185, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%1843)
    block1():
      %1844 : __torch__.torch.nn.modules.container.___torch_mangle_912.Sequential = prim::GetAttr[name="conv"](%33)
      %1845 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1844)
      %1846 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1844)
      %1847 : __torch__.torch.nn.modules.conv.___torch_mangle_727.Conv2d = prim::GetAttr[name="2"](%1844)
      %1848 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="3"](%1844)
      %1849 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1845)
      %1850 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1845)
      %1851 : Tensor = prim::GetAttr[name="weight"](%1849)
      %1852 : Tensor? = prim::GetAttr[name="bias"](%1849)
      %1853 : int[] = prim::ListConstruct(%20, %20)
      %1854 : int[] = prim::ListConstruct(%19, %19)
      %1855 : int[] = prim::ListConstruct(%20, %20)
      %input.186 : Tensor = aten::conv2d(%input.21, %1851, %1852, %1853, %1854, %1855, %20) # torch/nn/modules/conv.py:415:15
      %1857 : int = aten::dim(%input.186) # torch/nn/modules/batchnorm.py:276:11
      %1858 : bool = aten::ne(%1857, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1858) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1859 : bool = prim::GetAttr[name="training"](%1850)
       = prim::If(%1859) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1860 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1850)
          %1861 : Tensor = aten::add(%1860, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1850, %1861)
          -> ()
        block1():
          -> ()
      %1862 : bool = prim::GetAttr[name="training"](%1850)
      %1863 : Tensor = prim::GetAttr[name="running_mean"](%1850)
      %1864 : Tensor = prim::GetAttr[name="running_var"](%1850)
      %1865 : Tensor = prim::GetAttr[name="weight"](%1850)
      %1866 : Tensor = prim::GetAttr[name="bias"](%1850)
       = prim::If(%1862) # torch/nn/functional.py:2011:4
        block0():
          %1867 : int[] = aten::size(%input.186) # torch/nn/functional.py:2012:27
          %size_prods.248 : int = aten::__getitem__(%1867, %19) # torch/nn/functional.py:1991:17
          %1869 : int = aten::len(%1867) # torch/nn/functional.py:1992:19
          %1870 : int = aten::sub(%1869, %11) # torch/nn/functional.py:1992:19
          %size_prods.249 : int = prim::Loop(%1870, %10, %size_prods.248) # torch/nn/functional.py:1992:4
            block0(%i.63 : int, %size_prods.250 : int):
              %1874 : int = aten::add(%i.63, %11) # torch/nn/functional.py:1993:27
              %1875 : int = aten::__getitem__(%1867, %1874) # torch/nn/functional.py:1993:22
              %size_prods.251 : int = aten::mul(%size_prods.250, %1875) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.251)
          %1877 : bool = aten::eq(%size_prods.249, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1877) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.187 : Tensor = aten::batch_norm(%input.186, %1865, %1866, %1863, %1864, %1862, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.188 : Tensor = aten::hardtanh_(%input.187, %5, %4) # torch/nn/functional.py:1171:17
      %1880 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1846)
      %1881 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1846)
      %1882 : Tensor = prim::GetAttr[name="weight"](%1880)
      %1883 : Tensor? = prim::GetAttr[name="bias"](%1880)
      %1884 : int[] = prim::ListConstruct(%20, %20)
      %1885 : int[] = prim::ListConstruct(%20, %20)
      %1886 : int[] = prim::ListConstruct(%20, %20)
      %input.189 : Tensor = aten::conv2d(%input.188, %1882, %1883, %1884, %1885, %1886, %16) # torch/nn/modules/conv.py:415:15
      %1888 : int = aten::dim(%input.189) # torch/nn/modules/batchnorm.py:276:11
      %1889 : bool = aten::ne(%1888, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1889) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1890 : bool = prim::GetAttr[name="training"](%1881)
       = prim::If(%1890) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1891 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1881)
          %1892 : Tensor = aten::add(%1891, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1881, %1892)
          -> ()
        block1():
          -> ()
      %1893 : bool = prim::GetAttr[name="training"](%1881)
      %1894 : Tensor = prim::GetAttr[name="running_mean"](%1881)
      %1895 : Tensor = prim::GetAttr[name="running_var"](%1881)
      %1896 : Tensor = prim::GetAttr[name="weight"](%1881)
      %1897 : Tensor = prim::GetAttr[name="bias"](%1881)
       = prim::If(%1893) # torch/nn/functional.py:2011:4
        block0():
          %1898 : int[] = aten::size(%input.189) # torch/nn/functional.py:2012:27
          %size_prods.252 : int = aten::__getitem__(%1898, %19) # torch/nn/functional.py:1991:17
          %1900 : int = aten::len(%1898) # torch/nn/functional.py:1992:19
          %1901 : int = aten::sub(%1900, %11) # torch/nn/functional.py:1992:19
          %size_prods.253 : int = prim::Loop(%1901, %10, %size_prods.252) # torch/nn/functional.py:1992:4
            block0(%i.64 : int, %size_prods.254 : int):
              %1905 : int = aten::add(%i.64, %11) # torch/nn/functional.py:1993:27
              %1906 : int = aten::__getitem__(%1898, %1905) # torch/nn/functional.py:1993:22
              %size_prods.255 : int = aten::mul(%size_prods.254, %1906) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.255)
          %1908 : bool = aten::eq(%size_prods.253, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1908) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.190 : Tensor = aten::batch_norm(%input.189, %1896, %1897, %1894, %1895, %1893, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.191 : Tensor = aten::hardtanh_(%input.190, %5, %4) # torch/nn/functional.py:1171:17
      %1911 : Tensor = prim::GetAttr[name="weight"](%1847)
      %1912 : Tensor? = prim::GetAttr[name="bias"](%1847)
      %1913 : int[] = prim::ListConstruct(%20, %20)
      %1914 : int[] = prim::ListConstruct(%19, %19)
      %1915 : int[] = prim::ListConstruct(%20, %20)
      %input.192 : Tensor = aten::conv2d(%input.191, %1911, %1912, %1913, %1914, %1915, %20) # torch/nn/modules/conv.py:415:15
      %1917 : int = aten::dim(%input.192) # torch/nn/modules/batchnorm.py:276:11
      %1918 : bool = aten::ne(%1917, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1918) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1919 : bool = prim::GetAttr[name="training"](%1848)
       = prim::If(%1919) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1920 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1848)
          %1921 : Tensor = aten::add(%1920, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1848, %1921)
          -> ()
        block1():
          -> ()
      %1922 : bool = prim::GetAttr[name="training"](%1848)
      %1923 : Tensor = prim::GetAttr[name="running_mean"](%1848)
      %1924 : Tensor = prim::GetAttr[name="running_var"](%1848)
      %1925 : Tensor = prim::GetAttr[name="weight"](%1848)
      %1926 : Tensor = prim::GetAttr[name="bias"](%1848)
       = prim::If(%1922) # torch/nn/functional.py:2011:4
        block0():
          %1927 : int[] = aten::size(%input.192) # torch/nn/functional.py:2012:27
          %size_prods.256 : int = aten::__getitem__(%1927, %19) # torch/nn/functional.py:1991:17
          %1929 : int = aten::len(%1927) # torch/nn/functional.py:1992:19
          %1930 : int = aten::sub(%1929, %11) # torch/nn/functional.py:1992:19
          %size_prods.257 : int = prim::Loop(%1930, %10, %size_prods.256) # torch/nn/functional.py:1992:4
            block0(%i.65 : int, %size_prods.258 : int):
              %1934 : int = aten::add(%i.65, %11) # torch/nn/functional.py:1993:27
              %1935 : int = aten::__getitem__(%1927, %1934) # torch/nn/functional.py:1993:22
              %size_prods.259 : int = aten::mul(%size_prods.258, %1935) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.259)
          %1937 : bool = aten::eq(%size_prods.257, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1937) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.193 : Tensor = aten::batch_norm(%input.192, %1925, %1926, %1923, %1924, %1922, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.193)
  %1939 : bool = prim::GetAttr[name="use_res_connect"](%34)
  %input.25 : Tensor = prim::If(%1939) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %1941 : __torch__.torch.nn.modules.container.___torch_mangle_915.Sequential = prim::GetAttr[name="conv"](%34)
      %1942 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%1941)
      %1943 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%1941)
      %1944 : __torch__.torch.nn.modules.conv.___torch_mangle_914.Conv2d = prim::GetAttr[name="2"](%1941)
      %1945 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%1941)
      %1946 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%1942)
      %1947 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1942)
      %1948 : Tensor = prim::GetAttr[name="weight"](%1946)
      %1949 : Tensor? = prim::GetAttr[name="bias"](%1946)
      %1950 : int[] = prim::ListConstruct(%20, %20)
      %1951 : int[] = prim::ListConstruct(%19, %19)
      %1952 : int[] = prim::ListConstruct(%20, %20)
      %input.194 : Tensor = aten::conv2d(%input.23, %1948, %1949, %1950, %1951, %1952, %20) # torch/nn/modules/conv.py:415:15
      %1954 : int = aten::dim(%input.194) # torch/nn/modules/batchnorm.py:276:11
      %1955 : bool = aten::ne(%1954, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1955) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1956 : bool = prim::GetAttr[name="training"](%1947)
       = prim::If(%1956) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1957 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1947)
          %1958 : Tensor = aten::add(%1957, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1947, %1958)
          -> ()
        block1():
          -> ()
      %1959 : bool = prim::GetAttr[name="training"](%1947)
      %1960 : Tensor = prim::GetAttr[name="running_mean"](%1947)
      %1961 : Tensor = prim::GetAttr[name="running_var"](%1947)
      %1962 : Tensor = prim::GetAttr[name="weight"](%1947)
      %1963 : Tensor = prim::GetAttr[name="bias"](%1947)
       = prim::If(%1959) # torch/nn/functional.py:2011:4
        block0():
          %1964 : int[] = aten::size(%input.194) # torch/nn/functional.py:2012:27
          %size_prods.260 : int = aten::__getitem__(%1964, %19) # torch/nn/functional.py:1991:17
          %1966 : int = aten::len(%1964) # torch/nn/functional.py:1992:19
          %1967 : int = aten::sub(%1966, %11) # torch/nn/functional.py:1992:19
          %size_prods.261 : int = prim::Loop(%1967, %10, %size_prods.260) # torch/nn/functional.py:1992:4
            block0(%i.66 : int, %size_prods.262 : int):
              %1971 : int = aten::add(%i.66, %11) # torch/nn/functional.py:1993:27
              %1972 : int = aten::__getitem__(%1964, %1971) # torch/nn/functional.py:1993:22
              %size_prods.263 : int = aten::mul(%size_prods.262, %1972) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.263)
          %1974 : bool = aten::eq(%size_prods.261, %20) # torch/nn/functional.py:1994:7
           = prim::If(%1974) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.195 : Tensor = aten::batch_norm(%input.194, %1962, %1963, %1960, %1961, %1959, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.196 : Tensor = aten::hardtanh_(%input.195, %5, %4) # torch/nn/functional.py:1171:17
      %1977 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%1943)
      %1978 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%1943)
      %1979 : Tensor = prim::GetAttr[name="weight"](%1977)
      %1980 : Tensor? = prim::GetAttr[name="bias"](%1977)
      %1981 : int[] = prim::ListConstruct(%20, %20)
      %1982 : int[] = prim::ListConstruct(%20, %20)
      %1983 : int[] = prim::ListConstruct(%20, %20)
      %input.197 : Tensor = aten::conv2d(%input.196, %1979, %1980, %1981, %1982, %1983, %16) # torch/nn/modules/conv.py:415:15
      %1985 : int = aten::dim(%input.197) # torch/nn/modules/batchnorm.py:276:11
      %1986 : bool = aten::ne(%1985, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1986) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1987 : bool = prim::GetAttr[name="training"](%1978)
       = prim::If(%1987) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1988 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1978)
          %1989 : Tensor = aten::add(%1988, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1978, %1989)
          -> ()
        block1():
          -> ()
      %1990 : bool = prim::GetAttr[name="training"](%1978)
      %1991 : Tensor = prim::GetAttr[name="running_mean"](%1978)
      %1992 : Tensor = prim::GetAttr[name="running_var"](%1978)
      %1993 : Tensor = prim::GetAttr[name="weight"](%1978)
      %1994 : Tensor = prim::GetAttr[name="bias"](%1978)
       = prim::If(%1990) # torch/nn/functional.py:2011:4
        block0():
          %1995 : int[] = aten::size(%input.197) # torch/nn/functional.py:2012:27
          %size_prods.264 : int = aten::__getitem__(%1995, %19) # torch/nn/functional.py:1991:17
          %1997 : int = aten::len(%1995) # torch/nn/functional.py:1992:19
          %1998 : int = aten::sub(%1997, %11) # torch/nn/functional.py:1992:19
          %size_prods.265 : int = prim::Loop(%1998, %10, %size_prods.264) # torch/nn/functional.py:1992:4
            block0(%i.67 : int, %size_prods.266 : int):
              %2002 : int = aten::add(%i.67, %11) # torch/nn/functional.py:1993:27
              %2003 : int = aten::__getitem__(%1995, %2002) # torch/nn/functional.py:1993:22
              %size_prods.267 : int = aten::mul(%size_prods.266, %2003) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.267)
          %2005 : bool = aten::eq(%size_prods.265, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2005) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.198 : Tensor = aten::batch_norm(%input.197, %1993, %1994, %1991, %1992, %1990, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.199 : Tensor = aten::hardtanh_(%input.198, %5, %4) # torch/nn/functional.py:1171:17
      %2008 : Tensor = prim::GetAttr[name="weight"](%1944)
      %2009 : Tensor? = prim::GetAttr[name="bias"](%1944)
      %2010 : int[] = prim::ListConstruct(%20, %20)
      %2011 : int[] = prim::ListConstruct(%19, %19)
      %2012 : int[] = prim::ListConstruct(%20, %20)
      %input.200 : Tensor = aten::conv2d(%input.199, %2008, %2009, %2010, %2011, %2012, %20) # torch/nn/modules/conv.py:415:15
      %2014 : int = aten::dim(%input.200) # torch/nn/modules/batchnorm.py:276:11
      %2015 : bool = aten::ne(%2014, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2015) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2016 : bool = prim::GetAttr[name="training"](%1945)
       = prim::If(%2016) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2017 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1945)
          %2018 : Tensor = aten::add(%2017, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1945, %2018)
          -> ()
        block1():
          -> ()
      %2019 : bool = prim::GetAttr[name="training"](%1945)
      %2020 : Tensor = prim::GetAttr[name="running_mean"](%1945)
      %2021 : Tensor = prim::GetAttr[name="running_var"](%1945)
      %2022 : Tensor = prim::GetAttr[name="weight"](%1945)
      %2023 : Tensor = prim::GetAttr[name="bias"](%1945)
       = prim::If(%2019) # torch/nn/functional.py:2011:4
        block0():
          %2024 : int[] = aten::size(%input.200) # torch/nn/functional.py:2012:27
          %size_prods.268 : int = aten::__getitem__(%2024, %19) # torch/nn/functional.py:1991:17
          %2026 : int = aten::len(%2024) # torch/nn/functional.py:1992:19
          %2027 : int = aten::sub(%2026, %11) # torch/nn/functional.py:1992:19
          %size_prods.269 : int = prim::Loop(%2027, %10, %size_prods.268) # torch/nn/functional.py:1992:4
            block0(%i.68 : int, %size_prods.270 : int):
              %2031 : int = aten::add(%i.68, %11) # torch/nn/functional.py:1993:27
              %2032 : int = aten::__getitem__(%2024, %2031) # torch/nn/functional.py:1993:22
              %size_prods.271 : int = aten::mul(%size_prods.270, %2032) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.271)
          %2034 : bool = aten::eq(%size_prods.269, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2034) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.201 : Tensor = aten::batch_norm(%input.200, %2022, %2023, %2020, %2021, %2019, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %2036 : Tensor = aten::add(%input.23, %input.201, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%2036)
    block1():
      %2037 : __torch__.torch.nn.modules.container.___torch_mangle_915.Sequential = prim::GetAttr[name="conv"](%34)
      %2038 : __torch__.torchvision.models.mobilenet.___torch_mangle_910.ConvBNReLU = prim::GetAttr[name="0"](%2037)
      %2039 : __torch__.torchvision.models.mobilenet.___torch_mangle_911.ConvBNReLU = prim::GetAttr[name="1"](%2037)
      %2040 : __torch__.torch.nn.modules.conv.___torch_mangle_914.Conv2d = prim::GetAttr[name="2"](%2037)
      %2041 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%2037)
      %2042 : __torch__.torch.nn.modules.conv.___torch_mangle_724.Conv2d = prim::GetAttr[name="0"](%2038)
      %2043 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%2038)
      %2044 : Tensor = prim::GetAttr[name="weight"](%2042)
      %2045 : Tensor? = prim::GetAttr[name="bias"](%2042)
      %2046 : int[] = prim::ListConstruct(%20, %20)
      %2047 : int[] = prim::ListConstruct(%19, %19)
      %2048 : int[] = prim::ListConstruct(%20, %20)
      %input.202 : Tensor = aten::conv2d(%input.23, %2044, %2045, %2046, %2047, %2048, %20) # torch/nn/modules/conv.py:415:15
      %2050 : int = aten::dim(%input.202) # torch/nn/modules/batchnorm.py:276:11
      %2051 : bool = aten::ne(%2050, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2051) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2052 : bool = prim::GetAttr[name="training"](%2043)
       = prim::If(%2052) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2053 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2043)
          %2054 : Tensor = aten::add(%2053, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2043, %2054)
          -> ()
        block1():
          -> ()
      %2055 : bool = prim::GetAttr[name="training"](%2043)
      %2056 : Tensor = prim::GetAttr[name="running_mean"](%2043)
      %2057 : Tensor = prim::GetAttr[name="running_var"](%2043)
      %2058 : Tensor = prim::GetAttr[name="weight"](%2043)
      %2059 : Tensor = prim::GetAttr[name="bias"](%2043)
       = prim::If(%2055) # torch/nn/functional.py:2011:4
        block0():
          %2060 : int[] = aten::size(%input.202) # torch/nn/functional.py:2012:27
          %size_prods.272 : int = aten::__getitem__(%2060, %19) # torch/nn/functional.py:1991:17
          %2062 : int = aten::len(%2060) # torch/nn/functional.py:1992:19
          %2063 : int = aten::sub(%2062, %11) # torch/nn/functional.py:1992:19
          %size_prods.273 : int = prim::Loop(%2063, %10, %size_prods.272) # torch/nn/functional.py:1992:4
            block0(%i.69 : int, %size_prods.274 : int):
              %2067 : int = aten::add(%i.69, %11) # torch/nn/functional.py:1993:27
              %2068 : int = aten::__getitem__(%2060, %2067) # torch/nn/functional.py:1993:22
              %size_prods.275 : int = aten::mul(%size_prods.274, %2068) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.275)
          %2070 : bool = aten::eq(%size_prods.273, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2070) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.203 : Tensor = aten::batch_norm(%input.202, %2058, %2059, %2056, %2057, %2055, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.204 : Tensor = aten::hardtanh_(%input.203, %5, %4) # torch/nn/functional.py:1171:17
      %2073 : __torch__.torch.nn.modules.conv.___torch_mangle_731.Conv2d = prim::GetAttr[name="0"](%2039)
      %2074 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="1"](%2039)
      %2075 : Tensor = prim::GetAttr[name="weight"](%2073)
      %2076 : Tensor? = prim::GetAttr[name="bias"](%2073)
      %2077 : int[] = prim::ListConstruct(%20, %20)
      %2078 : int[] = prim::ListConstruct(%20, %20)
      %2079 : int[] = prim::ListConstruct(%20, %20)
      %input.205 : Tensor = aten::conv2d(%input.204, %2075, %2076, %2077, %2078, %2079, %16) # torch/nn/modules/conv.py:415:15
      %2081 : int = aten::dim(%input.205) # torch/nn/modules/batchnorm.py:276:11
      %2082 : bool = aten::ne(%2081, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2082) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2083 : bool = prim::GetAttr[name="training"](%2074)
       = prim::If(%2083) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2084 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2074)
          %2085 : Tensor = aten::add(%2084, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2074, %2085)
          -> ()
        block1():
          -> ()
      %2086 : bool = prim::GetAttr[name="training"](%2074)
      %2087 : Tensor = prim::GetAttr[name="running_mean"](%2074)
      %2088 : Tensor = prim::GetAttr[name="running_var"](%2074)
      %2089 : Tensor = prim::GetAttr[name="weight"](%2074)
      %2090 : Tensor = prim::GetAttr[name="bias"](%2074)
       = prim::If(%2086) # torch/nn/functional.py:2011:4
        block0():
          %2091 : int[] = aten::size(%input.205) # torch/nn/functional.py:2012:27
          %size_prods.276 : int = aten::__getitem__(%2091, %19) # torch/nn/functional.py:1991:17
          %2093 : int = aten::len(%2091) # torch/nn/functional.py:1992:19
          %2094 : int = aten::sub(%2093, %11) # torch/nn/functional.py:1992:19
          %size_prods.277 : int = prim::Loop(%2094, %10, %size_prods.276) # torch/nn/functional.py:1992:4
            block0(%i.70 : int, %size_prods.278 : int):
              %2098 : int = aten::add(%i.70, %11) # torch/nn/functional.py:1993:27
              %2099 : int = aten::__getitem__(%2091, %2098) # torch/nn/functional.py:1993:22
              %size_prods.279 : int = aten::mul(%size_prods.278, %2099) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.279)
          %2101 : bool = aten::eq(%size_prods.277, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2101) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.206 : Tensor = aten::batch_norm(%input.205, %2089, %2090, %2087, %2088, %2086, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.207 : Tensor = aten::hardtanh_(%input.206, %5, %4) # torch/nn/functional.py:1171:17
      %2104 : Tensor = prim::GetAttr[name="weight"](%2040)
      %2105 : Tensor? = prim::GetAttr[name="bias"](%2040)
      %2106 : int[] = prim::ListConstruct(%20, %20)
      %2107 : int[] = prim::ListConstruct(%19, %19)
      %2108 : int[] = prim::ListConstruct(%20, %20)
      %input.208 : Tensor = aten::conv2d(%input.207, %2104, %2105, %2106, %2107, %2108, %20) # torch/nn/modules/conv.py:415:15
      %2110 : int = aten::dim(%input.208) # torch/nn/modules/batchnorm.py:276:11
      %2111 : bool = aten::ne(%2110, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2111) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2112 : bool = prim::GetAttr[name="training"](%2041)
       = prim::If(%2112) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2113 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2041)
          %2114 : Tensor = aten::add(%2113, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2041, %2114)
          -> ()
        block1():
          -> ()
      %2115 : bool = prim::GetAttr[name="training"](%2041)
      %2116 : Tensor = prim::GetAttr[name="running_mean"](%2041)
      %2117 : Tensor = prim::GetAttr[name="running_var"](%2041)
      %2118 : Tensor = prim::GetAttr[name="weight"](%2041)
      %2119 : Tensor = prim::GetAttr[name="bias"](%2041)
       = prim::If(%2115) # torch/nn/functional.py:2011:4
        block0():
          %2120 : int[] = aten::size(%input.208) # torch/nn/functional.py:2012:27
          %size_prods.280 : int = aten::__getitem__(%2120, %19) # torch/nn/functional.py:1991:17
          %2122 : int = aten::len(%2120) # torch/nn/functional.py:1992:19
          %2123 : int = aten::sub(%2122, %11) # torch/nn/functional.py:1992:19
          %size_prods.281 : int = prim::Loop(%2123, %10, %size_prods.280) # torch/nn/functional.py:1992:4
            block0(%i.71 : int, %size_prods.282 : int):
              %2127 : int = aten::add(%i.71, %11) # torch/nn/functional.py:1993:27
              %2128 : int = aten::__getitem__(%2120, %2127) # torch/nn/functional.py:1993:22
              %size_prods.283 : int = aten::mul(%size_prods.282, %2128) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.283)
          %2130 : bool = aten::eq(%size_prods.281, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2130) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.209 : Tensor = aten::batch_norm(%input.208, %2118, %2119, %2116, %2117, %2115, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.209)
  %2132 : bool = prim::GetAttr[name="use_res_connect"](%35)
  %input.27 : Tensor = prim::If(%2132) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %2134 : __torch__.torch.nn.modules.container.___torch_mangle_919.Sequential = prim::GetAttr[name="conv"](%35)
      %2135 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2134)
      %2136 : __torch__.torchvision.models.mobilenet.___torch_mangle_918.ConvBNReLU = prim::GetAttr[name="1"](%2134)
      %2137 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="2"](%2134)
      %2138 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%2134)
      %2139 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2135)
      %2140 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2135)
      %2141 : Tensor = prim::GetAttr[name="weight"](%2139)
      %2142 : Tensor? = prim::GetAttr[name="bias"](%2139)
      %2143 : int[] = prim::ListConstruct(%20, %20)
      %2144 : int[] = prim::ListConstruct(%19, %19)
      %2145 : int[] = prim::ListConstruct(%20, %20)
      %input.210 : Tensor = aten::conv2d(%input.25, %2141, %2142, %2143, %2144, %2145, %20) # torch/nn/modules/conv.py:415:15
      %2147 : int = aten::dim(%input.210) # torch/nn/modules/batchnorm.py:276:11
      %2148 : bool = aten::ne(%2147, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2148) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2149 : bool = prim::GetAttr[name="training"](%2140)
       = prim::If(%2149) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2150 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2140)
          %2151 : Tensor = aten::add(%2150, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2140, %2151)
          -> ()
        block1():
          -> ()
      %2152 : bool = prim::GetAttr[name="training"](%2140)
      %2153 : Tensor = prim::GetAttr[name="running_mean"](%2140)
      %2154 : Tensor = prim::GetAttr[name="running_var"](%2140)
      %2155 : Tensor = prim::GetAttr[name="weight"](%2140)
      %2156 : Tensor = prim::GetAttr[name="bias"](%2140)
       = prim::If(%2152) # torch/nn/functional.py:2011:4
        block0():
          %2157 : int[] = aten::size(%input.210) # torch/nn/functional.py:2012:27
          %size_prods.284 : int = aten::__getitem__(%2157, %19) # torch/nn/functional.py:1991:17
          %2159 : int = aten::len(%2157) # torch/nn/functional.py:1992:19
          %2160 : int = aten::sub(%2159, %11) # torch/nn/functional.py:1992:19
          %size_prods.285 : int = prim::Loop(%2160, %10, %size_prods.284) # torch/nn/functional.py:1992:4
            block0(%i.72 : int, %size_prods.286 : int):
              %2164 : int = aten::add(%i.72, %11) # torch/nn/functional.py:1993:27
              %2165 : int = aten::__getitem__(%2157, %2164) # torch/nn/functional.py:1993:22
              %size_prods.287 : int = aten::mul(%size_prods.286, %2165) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.287)
          %2167 : bool = aten::eq(%size_prods.285, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2167) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.211 : Tensor = aten::batch_norm(%input.210, %2155, %2156, %2153, %2154, %2152, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.212 : Tensor = aten::hardtanh_(%input.211, %5, %4) # torch/nn/functional.py:1171:17
      %2170 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="0"](%2136)
      %2171 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2136)
      %2172 : Tensor = prim::GetAttr[name="weight"](%2170)
      %2173 : Tensor? = prim::GetAttr[name="bias"](%2170)
      %2174 : int[] = prim::ListConstruct(%20, %20)
      %2175 : int[] = prim::ListConstruct(%20, %20)
      %2176 : int[] = prim::ListConstruct(%20, %20)
      %input.213 : Tensor = aten::conv2d(%input.212, %2172, %2173, %2174, %2175, %2176, %17) # torch/nn/modules/conv.py:415:15
      %2178 : int = aten::dim(%input.213) # torch/nn/modules/batchnorm.py:276:11
      %2179 : bool = aten::ne(%2178, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2179) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2180 : bool = prim::GetAttr[name="training"](%2171)
       = prim::If(%2180) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2181 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2171)
          %2182 : Tensor = aten::add(%2181, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2171, %2182)
          -> ()
        block1():
          -> ()
      %2183 : bool = prim::GetAttr[name="training"](%2171)
      %2184 : Tensor = prim::GetAttr[name="running_mean"](%2171)
      %2185 : Tensor = prim::GetAttr[name="running_var"](%2171)
      %2186 : Tensor = prim::GetAttr[name="weight"](%2171)
      %2187 : Tensor = prim::GetAttr[name="bias"](%2171)
       = prim::If(%2183) # torch/nn/functional.py:2011:4
        block0():
          %2188 : int[] = aten::size(%input.213) # torch/nn/functional.py:2012:27
          %size_prods.288 : int = aten::__getitem__(%2188, %19) # torch/nn/functional.py:1991:17
          %2190 : int = aten::len(%2188) # torch/nn/functional.py:1992:19
          %2191 : int = aten::sub(%2190, %11) # torch/nn/functional.py:1992:19
          %size_prods.289 : int = prim::Loop(%2191, %10, %size_prods.288) # torch/nn/functional.py:1992:4
            block0(%i.73 : int, %size_prods.290 : int):
              %2195 : int = aten::add(%i.73, %11) # torch/nn/functional.py:1993:27
              %2196 : int = aten::__getitem__(%2188, %2195) # torch/nn/functional.py:1993:22
              %size_prods.291 : int = aten::mul(%size_prods.290, %2196) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.291)
          %2198 : bool = aten::eq(%size_prods.289, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2198) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.214 : Tensor = aten::batch_norm(%input.213, %2186, %2187, %2184, %2185, %2183, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.215 : Tensor = aten::hardtanh_(%input.214, %5, %4) # torch/nn/functional.py:1171:17
      %2201 : Tensor = prim::GetAttr[name="weight"](%2137)
      %2202 : Tensor? = prim::GetAttr[name="bias"](%2137)
      %2203 : int[] = prim::ListConstruct(%20, %20)
      %2204 : int[] = prim::ListConstruct(%19, %19)
      %2205 : int[] = prim::ListConstruct(%20, %20)
      %input.216 : Tensor = aten::conv2d(%input.215, %2201, %2202, %2203, %2204, %2205, %20) # torch/nn/modules/conv.py:415:15
      %2207 : int = aten::dim(%input.216) # torch/nn/modules/batchnorm.py:276:11
      %2208 : bool = aten::ne(%2207, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2208) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2209 : bool = prim::GetAttr[name="training"](%2138)
       = prim::If(%2209) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2210 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2138)
          %2211 : Tensor = aten::add(%2210, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2138, %2211)
          -> ()
        block1():
          -> ()
      %2212 : bool = prim::GetAttr[name="training"](%2138)
      %2213 : Tensor = prim::GetAttr[name="running_mean"](%2138)
      %2214 : Tensor = prim::GetAttr[name="running_var"](%2138)
      %2215 : Tensor = prim::GetAttr[name="weight"](%2138)
      %2216 : Tensor = prim::GetAttr[name="bias"](%2138)
       = prim::If(%2212) # torch/nn/functional.py:2011:4
        block0():
          %2217 : int[] = aten::size(%input.216) # torch/nn/functional.py:2012:27
          %size_prods.292 : int = aten::__getitem__(%2217, %19) # torch/nn/functional.py:1991:17
          %2219 : int = aten::len(%2217) # torch/nn/functional.py:1992:19
          %2220 : int = aten::sub(%2219, %11) # torch/nn/functional.py:1992:19
          %size_prods.293 : int = prim::Loop(%2220, %10, %size_prods.292) # torch/nn/functional.py:1992:4
            block0(%i.74 : int, %size_prods.294 : int):
              %2224 : int = aten::add(%i.74, %11) # torch/nn/functional.py:1993:27
              %2225 : int = aten::__getitem__(%2217, %2224) # torch/nn/functional.py:1993:22
              %size_prods.295 : int = aten::mul(%size_prods.294, %2225) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.295)
          %2227 : bool = aten::eq(%size_prods.293, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2227) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.217 : Tensor = aten::batch_norm(%input.216, %2215, %2216, %2213, %2214, %2212, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %2229 : Tensor = aten::add(%input.25, %input.217, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%2229)
    block1():
      %2230 : __torch__.torch.nn.modules.container.___torch_mangle_919.Sequential = prim::GetAttr[name="conv"](%35)
      %2231 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2230)
      %2232 : __torch__.torchvision.models.mobilenet.___torch_mangle_918.ConvBNReLU = prim::GetAttr[name="1"](%2230)
      %2233 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="2"](%2230)
      %2234 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%2230)
      %2235 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2231)
      %2236 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2231)
      %2237 : Tensor = prim::GetAttr[name="weight"](%2235)
      %2238 : Tensor? = prim::GetAttr[name="bias"](%2235)
      %2239 : int[] = prim::ListConstruct(%20, %20)
      %2240 : int[] = prim::ListConstruct(%19, %19)
      %2241 : int[] = prim::ListConstruct(%20, %20)
      %input.218 : Tensor = aten::conv2d(%input.25, %2237, %2238, %2239, %2240, %2241, %20) # torch/nn/modules/conv.py:415:15
      %2243 : int = aten::dim(%input.218) # torch/nn/modules/batchnorm.py:276:11
      %2244 : bool = aten::ne(%2243, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2244) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2245 : bool = prim::GetAttr[name="training"](%2236)
       = prim::If(%2245) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2246 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2236)
          %2247 : Tensor = aten::add(%2246, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2236, %2247)
          -> ()
        block1():
          -> ()
      %2248 : bool = prim::GetAttr[name="training"](%2236)
      %2249 : Tensor = prim::GetAttr[name="running_mean"](%2236)
      %2250 : Tensor = prim::GetAttr[name="running_var"](%2236)
      %2251 : Tensor = prim::GetAttr[name="weight"](%2236)
      %2252 : Tensor = prim::GetAttr[name="bias"](%2236)
       = prim::If(%2248) # torch/nn/functional.py:2011:4
        block0():
          %2253 : int[] = aten::size(%input.218) # torch/nn/functional.py:2012:27
          %size_prods.296 : int = aten::__getitem__(%2253, %19) # torch/nn/functional.py:1991:17
          %2255 : int = aten::len(%2253) # torch/nn/functional.py:1992:19
          %2256 : int = aten::sub(%2255, %11) # torch/nn/functional.py:1992:19
          %size_prods.297 : int = prim::Loop(%2256, %10, %size_prods.296) # torch/nn/functional.py:1992:4
            block0(%i.75 : int, %size_prods.298 : int):
              %2260 : int = aten::add(%i.75, %11) # torch/nn/functional.py:1993:27
              %2261 : int = aten::__getitem__(%2253, %2260) # torch/nn/functional.py:1993:22
              %size_prods.299 : int = aten::mul(%size_prods.298, %2261) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.299)
          %2263 : bool = aten::eq(%size_prods.297, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2263) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.219 : Tensor = aten::batch_norm(%input.218, %2251, %2252, %2249, %2250, %2248, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.220 : Tensor = aten::hardtanh_(%input.219, %5, %4) # torch/nn/functional.py:1171:17
      %2266 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="0"](%2232)
      %2267 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2232)
      %2268 : Tensor = prim::GetAttr[name="weight"](%2266)
      %2269 : Tensor? = prim::GetAttr[name="bias"](%2266)
      %2270 : int[] = prim::ListConstruct(%20, %20)
      %2271 : int[] = prim::ListConstruct(%20, %20)
      %2272 : int[] = prim::ListConstruct(%20, %20)
      %input.221 : Tensor = aten::conv2d(%input.220, %2268, %2269, %2270, %2271, %2272, %17) # torch/nn/modules/conv.py:415:15
      %2274 : int = aten::dim(%input.221) # torch/nn/modules/batchnorm.py:276:11
      %2275 : bool = aten::ne(%2274, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2275) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2276 : bool = prim::GetAttr[name="training"](%2267)
       = prim::If(%2276) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2277 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2267)
          %2278 : Tensor = aten::add(%2277, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2267, %2278)
          -> ()
        block1():
          -> ()
      %2279 : bool = prim::GetAttr[name="training"](%2267)
      %2280 : Tensor = prim::GetAttr[name="running_mean"](%2267)
      %2281 : Tensor = prim::GetAttr[name="running_var"](%2267)
      %2282 : Tensor = prim::GetAttr[name="weight"](%2267)
      %2283 : Tensor = prim::GetAttr[name="bias"](%2267)
       = prim::If(%2279) # torch/nn/functional.py:2011:4
        block0():
          %2284 : int[] = aten::size(%input.221) # torch/nn/functional.py:2012:27
          %size_prods.300 : int = aten::__getitem__(%2284, %19) # torch/nn/functional.py:1991:17
          %2286 : int = aten::len(%2284) # torch/nn/functional.py:1992:19
          %2287 : int = aten::sub(%2286, %11) # torch/nn/functional.py:1992:19
          %size_prods.301 : int = prim::Loop(%2287, %10, %size_prods.300) # torch/nn/functional.py:1992:4
            block0(%i.76 : int, %size_prods.302 : int):
              %2291 : int = aten::add(%i.76, %11) # torch/nn/functional.py:1993:27
              %2292 : int = aten::__getitem__(%2284, %2291) # torch/nn/functional.py:1993:22
              %size_prods.303 : int = aten::mul(%size_prods.302, %2292) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.303)
          %2294 : bool = aten::eq(%size_prods.301, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2294) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.222 : Tensor = aten::batch_norm(%input.221, %2282, %2283, %2280, %2281, %2279, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.223 : Tensor = aten::hardtanh_(%input.222, %5, %4) # torch/nn/functional.py:1171:17
      %2297 : Tensor = prim::GetAttr[name="weight"](%2233)
      %2298 : Tensor? = prim::GetAttr[name="bias"](%2233)
      %2299 : int[] = prim::ListConstruct(%20, %20)
      %2300 : int[] = prim::ListConstruct(%19, %19)
      %2301 : int[] = prim::ListConstruct(%20, %20)
      %input.224 : Tensor = aten::conv2d(%input.223, %2297, %2298, %2299, %2300, %2301, %20) # torch/nn/modules/conv.py:415:15
      %2303 : int = aten::dim(%input.224) # torch/nn/modules/batchnorm.py:276:11
      %2304 : bool = aten::ne(%2303, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2304) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2305 : bool = prim::GetAttr[name="training"](%2234)
       = prim::If(%2305) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2306 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2234)
          %2307 : Tensor = aten::add(%2306, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2234, %2307)
          -> ()
        block1():
          -> ()
      %2308 : bool = prim::GetAttr[name="training"](%2234)
      %2309 : Tensor = prim::GetAttr[name="running_mean"](%2234)
      %2310 : Tensor = prim::GetAttr[name="running_var"](%2234)
      %2311 : Tensor = prim::GetAttr[name="weight"](%2234)
      %2312 : Tensor = prim::GetAttr[name="bias"](%2234)
       = prim::If(%2308) # torch/nn/functional.py:2011:4
        block0():
          %2313 : int[] = aten::size(%input.224) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%2313, %19) # torch/nn/functional.py:1991:17
          %2315 : int = aten::len(%2313) # torch/nn/functional.py:1992:19
          %2316 : int = aten::sub(%2315, %11) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%2316, %10, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %2320 : int = aten::add(%i.77, %11) # torch/nn/functional.py:1993:27
              %2321 : int = aten::__getitem__(%2313, %2320) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %2321) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.307)
          %2323 : bool = aten::eq(%size_prods.305, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2323) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.225 : Tensor = aten::batch_norm(%input.224, %2311, %2312, %2309, %2310, %2308, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.225)
  %2325 : bool = prim::GetAttr[name="use_res_connect"](%36)
  %input.29 : Tensor = prim::If(%2325) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %2327 : __torch__.torch.nn.modules.container.___torch_mangle_919.Sequential = prim::GetAttr[name="conv"](%36)
      %2328 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2327)
      %2329 : __torch__.torchvision.models.mobilenet.___torch_mangle_918.ConvBNReLU = prim::GetAttr[name="1"](%2327)
      %2330 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="2"](%2327)
      %2331 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%2327)
      %2332 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2328)
      %2333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2328)
      %2334 : Tensor = prim::GetAttr[name="weight"](%2332)
      %2335 : Tensor? = prim::GetAttr[name="bias"](%2332)
      %2336 : int[] = prim::ListConstruct(%20, %20)
      %2337 : int[] = prim::ListConstruct(%19, %19)
      %2338 : int[] = prim::ListConstruct(%20, %20)
      %input.226 : Tensor = aten::conv2d(%input.27, %2334, %2335, %2336, %2337, %2338, %20) # torch/nn/modules/conv.py:415:15
      %2340 : int = aten::dim(%input.226) # torch/nn/modules/batchnorm.py:276:11
      %2341 : bool = aten::ne(%2340, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2341) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2342 : bool = prim::GetAttr[name="training"](%2333)
       = prim::If(%2342) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2343 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2333)
          %2344 : Tensor = aten::add(%2343, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2333, %2344)
          -> ()
        block1():
          -> ()
      %2345 : bool = prim::GetAttr[name="training"](%2333)
      %2346 : Tensor = prim::GetAttr[name="running_mean"](%2333)
      %2347 : Tensor = prim::GetAttr[name="running_var"](%2333)
      %2348 : Tensor = prim::GetAttr[name="weight"](%2333)
      %2349 : Tensor = prim::GetAttr[name="bias"](%2333)
       = prim::If(%2345) # torch/nn/functional.py:2011:4
        block0():
          %2350 : int[] = aten::size(%input.226) # torch/nn/functional.py:2012:27
          %size_prods.308 : int = aten::__getitem__(%2350, %19) # torch/nn/functional.py:1991:17
          %2352 : int = aten::len(%2350) # torch/nn/functional.py:1992:19
          %2353 : int = aten::sub(%2352, %11) # torch/nn/functional.py:1992:19
          %size_prods.309 : int = prim::Loop(%2353, %10, %size_prods.308) # torch/nn/functional.py:1992:4
            block0(%i.78 : int, %size_prods.310 : int):
              %2357 : int = aten::add(%i.78, %11) # torch/nn/functional.py:1993:27
              %2358 : int = aten::__getitem__(%2350, %2357) # torch/nn/functional.py:1993:22
              %size_prods.311 : int = aten::mul(%size_prods.310, %2358) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.311)
          %2360 : bool = aten::eq(%size_prods.309, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2360) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.227 : Tensor = aten::batch_norm(%input.226, %2348, %2349, %2346, %2347, %2345, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.228 : Tensor = aten::hardtanh_(%input.227, %5, %4) # torch/nn/functional.py:1171:17
      %2363 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="0"](%2329)
      %2364 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2329)
      %2365 : Tensor = prim::GetAttr[name="weight"](%2363)
      %2366 : Tensor? = prim::GetAttr[name="bias"](%2363)
      %2367 : int[] = prim::ListConstruct(%20, %20)
      %2368 : int[] = prim::ListConstruct(%20, %20)
      %2369 : int[] = prim::ListConstruct(%20, %20)
      %input.229 : Tensor = aten::conv2d(%input.228, %2365, %2366, %2367, %2368, %2369, %17) # torch/nn/modules/conv.py:415:15
      %2371 : int = aten::dim(%input.229) # torch/nn/modules/batchnorm.py:276:11
      %2372 : bool = aten::ne(%2371, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2372) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2373 : bool = prim::GetAttr[name="training"](%2364)
       = prim::If(%2373) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2374 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2364)
          %2375 : Tensor = aten::add(%2374, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2364, %2375)
          -> ()
        block1():
          -> ()
      %2376 : bool = prim::GetAttr[name="training"](%2364)
      %2377 : Tensor = prim::GetAttr[name="running_mean"](%2364)
      %2378 : Tensor = prim::GetAttr[name="running_var"](%2364)
      %2379 : Tensor = prim::GetAttr[name="weight"](%2364)
      %2380 : Tensor = prim::GetAttr[name="bias"](%2364)
       = prim::If(%2376) # torch/nn/functional.py:2011:4
        block0():
          %2381 : int[] = aten::size(%input.229) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%2381, %19) # torch/nn/functional.py:1991:17
          %2383 : int = aten::len(%2381) # torch/nn/functional.py:1992:19
          %2384 : int = aten::sub(%2383, %11) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%2384, %10, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %2388 : int = aten::add(%i.79, %11) # torch/nn/functional.py:1993:27
              %2389 : int = aten::__getitem__(%2381, %2388) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %2389) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.315)
          %2391 : bool = aten::eq(%size_prods.313, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2391) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.230 : Tensor = aten::batch_norm(%input.229, %2379, %2380, %2377, %2378, %2376, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.231 : Tensor = aten::hardtanh_(%input.230, %5, %4) # torch/nn/functional.py:1171:17
      %2394 : Tensor = prim::GetAttr[name="weight"](%2330)
      %2395 : Tensor? = prim::GetAttr[name="bias"](%2330)
      %2396 : int[] = prim::ListConstruct(%20, %20)
      %2397 : int[] = prim::ListConstruct(%19, %19)
      %2398 : int[] = prim::ListConstruct(%20, %20)
      %input.232 : Tensor = aten::conv2d(%input.231, %2394, %2395, %2396, %2397, %2398, %20) # torch/nn/modules/conv.py:415:15
      %2400 : int = aten::dim(%input.232) # torch/nn/modules/batchnorm.py:276:11
      %2401 : bool = aten::ne(%2400, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2401) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2402 : bool = prim::GetAttr[name="training"](%2331)
       = prim::If(%2402) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2403 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2331)
          %2404 : Tensor = aten::add(%2403, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2331, %2404)
          -> ()
        block1():
          -> ()
      %2405 : bool = prim::GetAttr[name="training"](%2331)
      %2406 : Tensor = prim::GetAttr[name="running_mean"](%2331)
      %2407 : Tensor = prim::GetAttr[name="running_var"](%2331)
      %2408 : Tensor = prim::GetAttr[name="weight"](%2331)
      %2409 : Tensor = prim::GetAttr[name="bias"](%2331)
       = prim::If(%2405) # torch/nn/functional.py:2011:4
        block0():
          %2410 : int[] = aten::size(%input.232) # torch/nn/functional.py:2012:27
          %size_prods.316 : int = aten::__getitem__(%2410, %19) # torch/nn/functional.py:1991:17
          %2412 : int = aten::len(%2410) # torch/nn/functional.py:1992:19
          %2413 : int = aten::sub(%2412, %11) # torch/nn/functional.py:1992:19
          %size_prods.317 : int = prim::Loop(%2413, %10, %size_prods.316) # torch/nn/functional.py:1992:4
            block0(%i.80 : int, %size_prods.318 : int):
              %2417 : int = aten::add(%i.80, %11) # torch/nn/functional.py:1993:27
              %2418 : int = aten::__getitem__(%2410, %2417) # torch/nn/functional.py:1993:22
              %size_prods.319 : int = aten::mul(%size_prods.318, %2418) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.319)
          %2420 : bool = aten::eq(%size_prods.317, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2420) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.233 : Tensor = aten::batch_norm(%input.232, %2408, %2409, %2406, %2407, %2405, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %2422 : Tensor = aten::add(%input.27, %input.233, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%2422)
    block1():
      %2423 : __torch__.torch.nn.modules.container.___torch_mangle_919.Sequential = prim::GetAttr[name="conv"](%36)
      %2424 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2423)
      %2425 : __torch__.torchvision.models.mobilenet.___torch_mangle_918.ConvBNReLU = prim::GetAttr[name="1"](%2423)
      %2426 : __torch__.torch.nn.modules.conv.___torch_mangle_682.Conv2d = prim::GetAttr[name="2"](%2423)
      %2427 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="3"](%2423)
      %2428 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2424)
      %2429 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2424)
      %2430 : Tensor = prim::GetAttr[name="weight"](%2428)
      %2431 : Tensor? = prim::GetAttr[name="bias"](%2428)
      %2432 : int[] = prim::ListConstruct(%20, %20)
      %2433 : int[] = prim::ListConstruct(%19, %19)
      %2434 : int[] = prim::ListConstruct(%20, %20)
      %input.234 : Tensor = aten::conv2d(%input.27, %2430, %2431, %2432, %2433, %2434, %20) # torch/nn/modules/conv.py:415:15
      %2436 : int = aten::dim(%input.234) # torch/nn/modules/batchnorm.py:276:11
      %2437 : bool = aten::ne(%2436, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2437) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2438 : bool = prim::GetAttr[name="training"](%2429)
       = prim::If(%2438) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2439 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2429)
          %2440 : Tensor = aten::add(%2439, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2429, %2440)
          -> ()
        block1():
          -> ()
      %2441 : bool = prim::GetAttr[name="training"](%2429)
      %2442 : Tensor = prim::GetAttr[name="running_mean"](%2429)
      %2443 : Tensor = prim::GetAttr[name="running_var"](%2429)
      %2444 : Tensor = prim::GetAttr[name="weight"](%2429)
      %2445 : Tensor = prim::GetAttr[name="bias"](%2429)
       = prim::If(%2441) # torch/nn/functional.py:2011:4
        block0():
          %2446 : int[] = aten::size(%input.234) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%2446, %19) # torch/nn/functional.py:1991:17
          %2448 : int = aten::len(%2446) # torch/nn/functional.py:1992:19
          %2449 : int = aten::sub(%2448, %11) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%2449, %10, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %2453 : int = aten::add(%i.81, %11) # torch/nn/functional.py:1993:27
              %2454 : int = aten::__getitem__(%2446, %2453) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %2454) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.323)
          %2456 : bool = aten::eq(%size_prods.321, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2456) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.235 : Tensor = aten::batch_norm(%input.234, %2444, %2445, %2442, %2443, %2441, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.236 : Tensor = aten::hardtanh_(%input.235, %5, %4) # torch/nn/functional.py:1171:17
      %2459 : __torch__.torch.nn.modules.conv.___torch_mangle_686.Conv2d = prim::GetAttr[name="0"](%2425)
      %2460 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2425)
      %2461 : Tensor = prim::GetAttr[name="weight"](%2459)
      %2462 : Tensor? = prim::GetAttr[name="bias"](%2459)
      %2463 : int[] = prim::ListConstruct(%20, %20)
      %2464 : int[] = prim::ListConstruct(%20, %20)
      %2465 : int[] = prim::ListConstruct(%20, %20)
      %input.237 : Tensor = aten::conv2d(%input.236, %2461, %2462, %2463, %2464, %2465, %17) # torch/nn/modules/conv.py:415:15
      %2467 : int = aten::dim(%input.237) # torch/nn/modules/batchnorm.py:276:11
      %2468 : bool = aten::ne(%2467, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2468) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2469 : bool = prim::GetAttr[name="training"](%2460)
       = prim::If(%2469) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2470 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2460)
          %2471 : Tensor = aten::add(%2470, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2460, %2471)
          -> ()
        block1():
          -> ()
      %2472 : bool = prim::GetAttr[name="training"](%2460)
      %2473 : Tensor = prim::GetAttr[name="running_mean"](%2460)
      %2474 : Tensor = prim::GetAttr[name="running_var"](%2460)
      %2475 : Tensor = prim::GetAttr[name="weight"](%2460)
      %2476 : Tensor = prim::GetAttr[name="bias"](%2460)
       = prim::If(%2472) # torch/nn/functional.py:2011:4
        block0():
          %2477 : int[] = aten::size(%input.237) # torch/nn/functional.py:2012:27
          %size_prods.324 : int = aten::__getitem__(%2477, %19) # torch/nn/functional.py:1991:17
          %2479 : int = aten::len(%2477) # torch/nn/functional.py:1992:19
          %2480 : int = aten::sub(%2479, %11) # torch/nn/functional.py:1992:19
          %size_prods.325 : int = prim::Loop(%2480, %10, %size_prods.324) # torch/nn/functional.py:1992:4
            block0(%i.82 : int, %size_prods.326 : int):
              %2484 : int = aten::add(%i.82, %11) # torch/nn/functional.py:1993:27
              %2485 : int = aten::__getitem__(%2477, %2484) # torch/nn/functional.py:1993:22
              %size_prods.327 : int = aten::mul(%size_prods.326, %2485) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.327)
          %2487 : bool = aten::eq(%size_prods.325, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2487) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.238 : Tensor = aten::batch_norm(%input.237, %2475, %2476, %2473, %2474, %2472, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.239 : Tensor = aten::hardtanh_(%input.238, %5, %4) # torch/nn/functional.py:1171:17
      %2490 : Tensor = prim::GetAttr[name="weight"](%2426)
      %2491 : Tensor? = prim::GetAttr[name="bias"](%2426)
      %2492 : int[] = prim::ListConstruct(%20, %20)
      %2493 : int[] = prim::ListConstruct(%19, %19)
      %2494 : int[] = prim::ListConstruct(%20, %20)
      %input.240 : Tensor = aten::conv2d(%input.239, %2490, %2491, %2492, %2493, %2494, %20) # torch/nn/modules/conv.py:415:15
      %2496 : int = aten::dim(%input.240) # torch/nn/modules/batchnorm.py:276:11
      %2497 : bool = aten::ne(%2496, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2497) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2498 : bool = prim::GetAttr[name="training"](%2427)
       = prim::If(%2498) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2499 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2427)
          %2500 : Tensor = aten::add(%2499, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2427, %2500)
          -> ()
        block1():
          -> ()
      %2501 : bool = prim::GetAttr[name="training"](%2427)
      %2502 : Tensor = prim::GetAttr[name="running_mean"](%2427)
      %2503 : Tensor = prim::GetAttr[name="running_var"](%2427)
      %2504 : Tensor = prim::GetAttr[name="weight"](%2427)
      %2505 : Tensor = prim::GetAttr[name="bias"](%2427)
       = prim::If(%2501) # torch/nn/functional.py:2011:4
        block0():
          %2506 : int[] = aten::size(%input.240) # torch/nn/functional.py:2012:27
          %size_prods.328 : int = aten::__getitem__(%2506, %19) # torch/nn/functional.py:1991:17
          %2508 : int = aten::len(%2506) # torch/nn/functional.py:1992:19
          %2509 : int = aten::sub(%2508, %11) # torch/nn/functional.py:1992:19
          %size_prods.329 : int = prim::Loop(%2509, %10, %size_prods.328) # torch/nn/functional.py:1992:4
            block0(%i.83 : int, %size_prods.330 : int):
              %2513 : int = aten::add(%i.83, %11) # torch/nn/functional.py:1993:27
              %2514 : int = aten::__getitem__(%2506, %2513) # torch/nn/functional.py:1993:22
              %size_prods.331 : int = aten::mul(%size_prods.330, %2514) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.331)
          %2516 : bool = aten::eq(%size_prods.329, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2516) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.241 : Tensor = aten::batch_norm(%input.240, %2504, %2505, %2502, %2503, %2501, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.241)
  %2518 : bool = prim::GetAttr[name="use_res_connect"](%37)
  %input.31 : Tensor = prim::If(%2518) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %2520 : __torch__.torch.nn.modules.container.___torch_mangle_923.Sequential = prim::GetAttr[name="conv"](%37)
      %2521 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2520)
      %2522 : __torch__.torchvision.models.mobilenet.___torch_mangle_922.ConvBNReLU = prim::GetAttr[name="1"](%2520)
      %2523 : __torch__.torch.nn.modules.conv.___torch_mangle_687.Conv2d = prim::GetAttr[name="2"](%2520)
      %2524 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%2520)
      %2525 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2521)
      %2526 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2521)
      %2527 : Tensor = prim::GetAttr[name="weight"](%2525)
      %2528 : Tensor? = prim::GetAttr[name="bias"](%2525)
      %2529 : int[] = prim::ListConstruct(%20, %20)
      %2530 : int[] = prim::ListConstruct(%19, %19)
      %2531 : int[] = prim::ListConstruct(%20, %20)
      %input.242 : Tensor = aten::conv2d(%input.29, %2527, %2528, %2529, %2530, %2531, %20) # torch/nn/modules/conv.py:415:15
      %2533 : int = aten::dim(%input.242) # torch/nn/modules/batchnorm.py:276:11
      %2534 : bool = aten::ne(%2533, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2534) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2535 : bool = prim::GetAttr[name="training"](%2526)
       = prim::If(%2535) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2536 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2526)
          %2537 : Tensor = aten::add(%2536, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2526, %2537)
          -> ()
        block1():
          -> ()
      %2538 : bool = prim::GetAttr[name="training"](%2526)
      %2539 : Tensor = prim::GetAttr[name="running_mean"](%2526)
      %2540 : Tensor = prim::GetAttr[name="running_var"](%2526)
      %2541 : Tensor = prim::GetAttr[name="weight"](%2526)
      %2542 : Tensor = prim::GetAttr[name="bias"](%2526)
       = prim::If(%2538) # torch/nn/functional.py:2011:4
        block0():
          %2543 : int[] = aten::size(%input.242) # torch/nn/functional.py:2012:27
          %size_prods.332 : int = aten::__getitem__(%2543, %19) # torch/nn/functional.py:1991:17
          %2545 : int = aten::len(%2543) # torch/nn/functional.py:1992:19
          %2546 : int = aten::sub(%2545, %11) # torch/nn/functional.py:1992:19
          %size_prods.333 : int = prim::Loop(%2546, %10, %size_prods.332) # torch/nn/functional.py:1992:4
            block0(%i.84 : int, %size_prods.334 : int):
              %2550 : int = aten::add(%i.84, %11) # torch/nn/functional.py:1993:27
              %2551 : int = aten::__getitem__(%2543, %2550) # torch/nn/functional.py:1993:22
              %size_prods.335 : int = aten::mul(%size_prods.334, %2551) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.335)
          %2553 : bool = aten::eq(%size_prods.333, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2553) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.243 : Tensor = aten::batch_norm(%input.242, %2541, %2542, %2539, %2540, %2538, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.244 : Tensor = aten::hardtanh_(%input.243, %5, %4) # torch/nn/functional.py:1171:17
      %2556 : __torch__.torch.nn.modules.conv.___torch_mangle_921.Conv2d = prim::GetAttr[name="0"](%2522)
      %2557 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2522)
      %2558 : Tensor = prim::GetAttr[name="weight"](%2556)
      %2559 : Tensor? = prim::GetAttr[name="bias"](%2556)
      %2560 : int[] = prim::ListConstruct(%11, %11)
      %2561 : int[] = prim::ListConstruct(%20, %20)
      %2562 : int[] = prim::ListConstruct(%20, %20)
      %input.245 : Tensor = aten::conv2d(%input.244, %2558, %2559, %2560, %2561, %2562, %17) # torch/nn/modules/conv.py:415:15
      %2564 : int = aten::dim(%input.245) # torch/nn/modules/batchnorm.py:276:11
      %2565 : bool = aten::ne(%2564, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2565) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2566 : bool = prim::GetAttr[name="training"](%2557)
       = prim::If(%2566) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2567 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2557)
          %2568 : Tensor = aten::add(%2567, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2557, %2568)
          -> ()
        block1():
          -> ()
      %2569 : bool = prim::GetAttr[name="training"](%2557)
      %2570 : Tensor = prim::GetAttr[name="running_mean"](%2557)
      %2571 : Tensor = prim::GetAttr[name="running_var"](%2557)
      %2572 : Tensor = prim::GetAttr[name="weight"](%2557)
      %2573 : Tensor = prim::GetAttr[name="bias"](%2557)
       = prim::If(%2569) # torch/nn/functional.py:2011:4
        block0():
          %2574 : int[] = aten::size(%input.245) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%2574, %19) # torch/nn/functional.py:1991:17
          %2576 : int = aten::len(%2574) # torch/nn/functional.py:1992:19
          %2577 : int = aten::sub(%2576, %11) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%2577, %10, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %2581 : int = aten::add(%i.85, %11) # torch/nn/functional.py:1993:27
              %2582 : int = aten::__getitem__(%2574, %2581) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %2582) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.339)
          %2584 : bool = aten::eq(%size_prods.337, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2584) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.246 : Tensor = aten::batch_norm(%input.245, %2572, %2573, %2570, %2571, %2569, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.247 : Tensor = aten::hardtanh_(%input.246, %5, %4) # torch/nn/functional.py:1171:17
      %2587 : Tensor = prim::GetAttr[name="weight"](%2523)
      %2588 : Tensor? = prim::GetAttr[name="bias"](%2523)
      %2589 : int[] = prim::ListConstruct(%20, %20)
      %2590 : int[] = prim::ListConstruct(%19, %19)
      %2591 : int[] = prim::ListConstruct(%20, %20)
      %input.248 : Tensor = aten::conv2d(%input.247, %2587, %2588, %2589, %2590, %2591, %20) # torch/nn/modules/conv.py:415:15
      %2593 : int = aten::dim(%input.248) # torch/nn/modules/batchnorm.py:276:11
      %2594 : bool = aten::ne(%2593, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2594) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2595 : bool = prim::GetAttr[name="training"](%2524)
       = prim::If(%2595) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2596 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2524)
          %2597 : Tensor = aten::add(%2596, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2524, %2597)
          -> ()
        block1():
          -> ()
      %2598 : bool = prim::GetAttr[name="training"](%2524)
      %2599 : Tensor = prim::GetAttr[name="running_mean"](%2524)
      %2600 : Tensor = prim::GetAttr[name="running_var"](%2524)
      %2601 : Tensor = prim::GetAttr[name="weight"](%2524)
      %2602 : Tensor = prim::GetAttr[name="bias"](%2524)
       = prim::If(%2598) # torch/nn/functional.py:2011:4
        block0():
          %2603 : int[] = aten::size(%input.248) # torch/nn/functional.py:2012:27
          %size_prods.340 : int = aten::__getitem__(%2603, %19) # torch/nn/functional.py:1991:17
          %2605 : int = aten::len(%2603) # torch/nn/functional.py:1992:19
          %2606 : int = aten::sub(%2605, %11) # torch/nn/functional.py:1992:19
          %size_prods.341 : int = prim::Loop(%2606, %10, %size_prods.340) # torch/nn/functional.py:1992:4
            block0(%i.86 : int, %size_prods.342 : int):
              %2610 : int = aten::add(%i.86, %11) # torch/nn/functional.py:1993:27
              %2611 : int = aten::__getitem__(%2603, %2610) # torch/nn/functional.py:1993:22
              %size_prods.343 : int = aten::mul(%size_prods.342, %2611) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.343)
          %2613 : bool = aten::eq(%size_prods.341, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2613) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.249 : Tensor = aten::batch_norm(%input.248, %2601, %2602, %2599, %2600, %2598, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %2615 : Tensor = aten::add(%input.29, %input.249, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%2615)
    block1():
      %2616 : __torch__.torch.nn.modules.container.___torch_mangle_923.Sequential = prim::GetAttr[name="conv"](%37)
      %2617 : __torch__.torchvision.models.mobilenet.___torch_mangle_917.ConvBNReLU = prim::GetAttr[name="0"](%2616)
      %2618 : __torch__.torchvision.models.mobilenet.___torch_mangle_922.ConvBNReLU = prim::GetAttr[name="1"](%2616)
      %2619 : __torch__.torch.nn.modules.conv.___torch_mangle_687.Conv2d = prim::GetAttr[name="2"](%2616)
      %2620 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%2616)
      %2621 : __torch__.torch.nn.modules.conv.___torch_mangle_679.Conv2d = prim::GetAttr[name="0"](%2617)
      %2622 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2617)
      %2623 : Tensor = prim::GetAttr[name="weight"](%2621)
      %2624 : Tensor? = prim::GetAttr[name="bias"](%2621)
      %2625 : int[] = prim::ListConstruct(%20, %20)
      %2626 : int[] = prim::ListConstruct(%19, %19)
      %2627 : int[] = prim::ListConstruct(%20, %20)
      %input.250 : Tensor = aten::conv2d(%input.29, %2623, %2624, %2625, %2626, %2627, %20) # torch/nn/modules/conv.py:415:15
      %2629 : int = aten::dim(%input.250) # torch/nn/modules/batchnorm.py:276:11
      %2630 : bool = aten::ne(%2629, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2630) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2631 : bool = prim::GetAttr[name="training"](%2622)
       = prim::If(%2631) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2632 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2622)
          %2633 : Tensor = aten::add(%2632, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2622, %2633)
          -> ()
        block1():
          -> ()
      %2634 : bool = prim::GetAttr[name="training"](%2622)
      %2635 : Tensor = prim::GetAttr[name="running_mean"](%2622)
      %2636 : Tensor = prim::GetAttr[name="running_var"](%2622)
      %2637 : Tensor = prim::GetAttr[name="weight"](%2622)
      %2638 : Tensor = prim::GetAttr[name="bias"](%2622)
       = prim::If(%2634) # torch/nn/functional.py:2011:4
        block0():
          %2639 : int[] = aten::size(%input.250) # torch/nn/functional.py:2012:27
          %size_prods.344 : int = aten::__getitem__(%2639, %19) # torch/nn/functional.py:1991:17
          %2641 : int = aten::len(%2639) # torch/nn/functional.py:1992:19
          %2642 : int = aten::sub(%2641, %11) # torch/nn/functional.py:1992:19
          %size_prods.345 : int = prim::Loop(%2642, %10, %size_prods.344) # torch/nn/functional.py:1992:4
            block0(%i.87 : int, %size_prods.346 : int):
              %2646 : int = aten::add(%i.87, %11) # torch/nn/functional.py:1993:27
              %2647 : int = aten::__getitem__(%2639, %2646) # torch/nn/functional.py:1993:22
              %size_prods.347 : int = aten::mul(%size_prods.346, %2647) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.347)
          %2649 : bool = aten::eq(%size_prods.345, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2649) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.251 : Tensor = aten::batch_norm(%input.250, %2637, %2638, %2635, %2636, %2634, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.252 : Tensor = aten::hardtanh_(%input.251, %5, %4) # torch/nn/functional.py:1171:17
      %2652 : __torch__.torch.nn.modules.conv.___torch_mangle_921.Conv2d = prim::GetAttr[name="0"](%2618)
      %2653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="1"](%2618)
      %2654 : Tensor = prim::GetAttr[name="weight"](%2652)
      %2655 : Tensor? = prim::GetAttr[name="bias"](%2652)
      %2656 : int[] = prim::ListConstruct(%11, %11)
      %2657 : int[] = prim::ListConstruct(%20, %20)
      %2658 : int[] = prim::ListConstruct(%20, %20)
      %input.253 : Tensor = aten::conv2d(%input.252, %2654, %2655, %2656, %2657, %2658, %17) # torch/nn/modules/conv.py:415:15
      %2660 : int = aten::dim(%input.253) # torch/nn/modules/batchnorm.py:276:11
      %2661 : bool = aten::ne(%2660, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2661) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2662 : bool = prim::GetAttr[name="training"](%2653)
       = prim::If(%2662) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2663 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2653)
          %2664 : Tensor = aten::add(%2663, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2653, %2664)
          -> ()
        block1():
          -> ()
      %2665 : bool = prim::GetAttr[name="training"](%2653)
      %2666 : Tensor = prim::GetAttr[name="running_mean"](%2653)
      %2667 : Tensor = prim::GetAttr[name="running_var"](%2653)
      %2668 : Tensor = prim::GetAttr[name="weight"](%2653)
      %2669 : Tensor = prim::GetAttr[name="bias"](%2653)
       = prim::If(%2665) # torch/nn/functional.py:2011:4
        block0():
          %2670 : int[] = aten::size(%input.253) # torch/nn/functional.py:2012:27
          %size_prods.348 : int = aten::__getitem__(%2670, %19) # torch/nn/functional.py:1991:17
          %2672 : int = aten::len(%2670) # torch/nn/functional.py:1992:19
          %2673 : int = aten::sub(%2672, %11) # torch/nn/functional.py:1992:19
          %size_prods.349 : int = prim::Loop(%2673, %10, %size_prods.348) # torch/nn/functional.py:1992:4
            block0(%i.88 : int, %size_prods.350 : int):
              %2677 : int = aten::add(%i.88, %11) # torch/nn/functional.py:1993:27
              %2678 : int = aten::__getitem__(%2670, %2677) # torch/nn/functional.py:1993:22
              %size_prods.351 : int = aten::mul(%size_prods.350, %2678) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.351)
          %2680 : bool = aten::eq(%size_prods.349, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2680) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.254 : Tensor = aten::batch_norm(%input.253, %2668, %2669, %2666, %2667, %2665, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.255 : Tensor = aten::hardtanh_(%input.254, %5, %4) # torch/nn/functional.py:1171:17
      %2683 : Tensor = prim::GetAttr[name="weight"](%2619)
      %2684 : Tensor? = prim::GetAttr[name="bias"](%2619)
      %2685 : int[] = prim::ListConstruct(%20, %20)
      %2686 : int[] = prim::ListConstruct(%19, %19)
      %2687 : int[] = prim::ListConstruct(%20, %20)
      %input.256 : Tensor = aten::conv2d(%input.255, %2683, %2684, %2685, %2686, %2687, %20) # torch/nn/modules/conv.py:415:15
      %2689 : int = aten::dim(%input.256) # torch/nn/modules/batchnorm.py:276:11
      %2690 : bool = aten::ne(%2689, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2690) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2691 : bool = prim::GetAttr[name="training"](%2620)
       = prim::If(%2691) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2692 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2620)
          %2693 : Tensor = aten::add(%2692, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2620, %2693)
          -> ()
        block1():
          -> ()
      %2694 : bool = prim::GetAttr[name="training"](%2620)
      %2695 : Tensor = prim::GetAttr[name="running_mean"](%2620)
      %2696 : Tensor = prim::GetAttr[name="running_var"](%2620)
      %2697 : Tensor = prim::GetAttr[name="weight"](%2620)
      %2698 : Tensor = prim::GetAttr[name="bias"](%2620)
       = prim::If(%2694) # torch/nn/functional.py:2011:4
        block0():
          %2699 : int[] = aten::size(%input.256) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%2699, %19) # torch/nn/functional.py:1991:17
          %2701 : int = aten::len(%2699) # torch/nn/functional.py:1992:19
          %2702 : int = aten::sub(%2701, %11) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%2702, %10, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %2706 : int = aten::add(%i.89, %11) # torch/nn/functional.py:1993:27
              %2707 : int = aten::__getitem__(%2699, %2706) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %2707) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.355)
          %2709 : bool = aten::eq(%size_prods.353, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2709) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.257 : Tensor = aten::batch_norm(%input.256, %2697, %2698, %2695, %2696, %2694, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.257)
  %2711 : bool = prim::GetAttr[name="use_res_connect"](%38)
  %input.33 : Tensor = prim::If(%2711) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %2713 : __torch__.torch.nn.modules.container.___torch_mangle_930.Sequential = prim::GetAttr[name="conv"](%38)
      %2714 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%2713)
      %2715 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%2713)
      %2716 : __torch__.torch.nn.modules.conv.___torch_mangle_929.Conv2d = prim::GetAttr[name="2"](%2713)
      %2717 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%2713)
      %2718 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%2714)
      %2719 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2714)
      %2720 : Tensor = prim::GetAttr[name="weight"](%2718)
      %2721 : Tensor? = prim::GetAttr[name="bias"](%2718)
      %2722 : int[] = prim::ListConstruct(%20, %20)
      %2723 : int[] = prim::ListConstruct(%19, %19)
      %2724 : int[] = prim::ListConstruct(%20, %20)
      %input.258 : Tensor = aten::conv2d(%input.31, %2720, %2721, %2722, %2723, %2724, %20) # torch/nn/modules/conv.py:415:15
      %2726 : int = aten::dim(%input.258) # torch/nn/modules/batchnorm.py:276:11
      %2727 : bool = aten::ne(%2726, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2727) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2728 : bool = prim::GetAttr[name="training"](%2719)
       = prim::If(%2728) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2729 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2719)
          %2730 : Tensor = aten::add(%2729, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2719, %2730)
          -> ()
        block1():
          -> ()
      %2731 : bool = prim::GetAttr[name="training"](%2719)
      %2732 : Tensor = prim::GetAttr[name="running_mean"](%2719)
      %2733 : Tensor = prim::GetAttr[name="running_var"](%2719)
      %2734 : Tensor = prim::GetAttr[name="weight"](%2719)
      %2735 : Tensor = prim::GetAttr[name="bias"](%2719)
       = prim::If(%2731) # torch/nn/functional.py:2011:4
        block0():
          %2736 : int[] = aten::size(%input.258) # torch/nn/functional.py:2012:27
          %size_prods.356 : int = aten::__getitem__(%2736, %19) # torch/nn/functional.py:1991:17
          %2738 : int = aten::len(%2736) # torch/nn/functional.py:1992:19
          %2739 : int = aten::sub(%2738, %11) # torch/nn/functional.py:1992:19
          %size_prods.357 : int = prim::Loop(%2739, %10, %size_prods.356) # torch/nn/functional.py:1992:4
            block0(%i.90 : int, %size_prods.358 : int):
              %2743 : int = aten::add(%i.90, %11) # torch/nn/functional.py:1993:27
              %2744 : int = aten::__getitem__(%2736, %2743) # torch/nn/functional.py:1993:22
              %size_prods.359 : int = aten::mul(%size_prods.358, %2744) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.359)
          %2746 : bool = aten::eq(%size_prods.357, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2746) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.259 : Tensor = aten::batch_norm(%input.258, %2734, %2735, %2732, %2733, %2731, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.260 : Tensor = aten::hardtanh_(%input.259, %5, %4) # torch/nn/functional.py:1171:17
      %2749 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%2715)
      %2750 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2715)
      %2751 : Tensor = prim::GetAttr[name="weight"](%2749)
      %2752 : Tensor? = prim::GetAttr[name="bias"](%2749)
      %2753 : int[] = prim::ListConstruct(%20, %20)
      %2754 : int[] = prim::ListConstruct(%20, %20)
      %2755 : int[] = prim::ListConstruct(%20, %20)
      %input.261 : Tensor = aten::conv2d(%input.260, %2751, %2752, %2753, %2754, %2755, %18) # torch/nn/modules/conv.py:415:15
      %2757 : int = aten::dim(%input.261) # torch/nn/modules/batchnorm.py:276:11
      %2758 : bool = aten::ne(%2757, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2758) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2759 : bool = prim::GetAttr[name="training"](%2750)
       = prim::If(%2759) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2760 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2750)
          %2761 : Tensor = aten::add(%2760, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2750, %2761)
          -> ()
        block1():
          -> ()
      %2762 : bool = prim::GetAttr[name="training"](%2750)
      %2763 : Tensor = prim::GetAttr[name="running_mean"](%2750)
      %2764 : Tensor = prim::GetAttr[name="running_var"](%2750)
      %2765 : Tensor = prim::GetAttr[name="weight"](%2750)
      %2766 : Tensor = prim::GetAttr[name="bias"](%2750)
       = prim::If(%2762) # torch/nn/functional.py:2011:4
        block0():
          %2767 : int[] = aten::size(%input.261) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%2767, %19) # torch/nn/functional.py:1991:17
          %2769 : int = aten::len(%2767) # torch/nn/functional.py:1992:19
          %2770 : int = aten::sub(%2769, %11) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%2770, %10, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %2774 : int = aten::add(%i.91, %11) # torch/nn/functional.py:1993:27
              %2775 : int = aten::__getitem__(%2767, %2774) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %2775) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.363)
          %2777 : bool = aten::eq(%size_prods.361, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2777) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.262 : Tensor = aten::batch_norm(%input.261, %2765, %2766, %2763, %2764, %2762, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.263 : Tensor = aten::hardtanh_(%input.262, %5, %4) # torch/nn/functional.py:1171:17
      %2780 : Tensor = prim::GetAttr[name="weight"](%2716)
      %2781 : Tensor? = prim::GetAttr[name="bias"](%2716)
      %2782 : int[] = prim::ListConstruct(%20, %20)
      %2783 : int[] = prim::ListConstruct(%19, %19)
      %2784 : int[] = prim::ListConstruct(%20, %20)
      %input.264 : Tensor = aten::conv2d(%input.263, %2780, %2781, %2782, %2783, %2784, %20) # torch/nn/modules/conv.py:415:15
      %2786 : int = aten::dim(%input.264) # torch/nn/modules/batchnorm.py:276:11
      %2787 : bool = aten::ne(%2786, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2787) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2788 : bool = prim::GetAttr[name="training"](%2717)
       = prim::If(%2788) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2789 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2717)
          %2790 : Tensor = aten::add(%2789, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2717, %2790)
          -> ()
        block1():
          -> ()
      %2791 : bool = prim::GetAttr[name="training"](%2717)
      %2792 : Tensor = prim::GetAttr[name="running_mean"](%2717)
      %2793 : Tensor = prim::GetAttr[name="running_var"](%2717)
      %2794 : Tensor = prim::GetAttr[name="weight"](%2717)
      %2795 : Tensor = prim::GetAttr[name="bias"](%2717)
       = prim::If(%2791) # torch/nn/functional.py:2011:4
        block0():
          %2796 : int[] = aten::size(%input.264) # torch/nn/functional.py:2012:27
          %size_prods.364 : int = aten::__getitem__(%2796, %19) # torch/nn/functional.py:1991:17
          %2798 : int = aten::len(%2796) # torch/nn/functional.py:1992:19
          %2799 : int = aten::sub(%2798, %11) # torch/nn/functional.py:1992:19
          %size_prods.365 : int = prim::Loop(%2799, %10, %size_prods.364) # torch/nn/functional.py:1992:4
            block0(%i.92 : int, %size_prods.366 : int):
              %2803 : int = aten::add(%i.92, %11) # torch/nn/functional.py:1993:27
              %2804 : int = aten::__getitem__(%2796, %2803) # torch/nn/functional.py:1993:22
              %size_prods.367 : int = aten::mul(%size_prods.366, %2804) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.367)
          %2806 : bool = aten::eq(%size_prods.365, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2806) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.265 : Tensor = aten::batch_norm(%input.264, %2794, %2795, %2792, %2793, %2791, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %2808 : Tensor = aten::add(%input.31, %input.265, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%2808)
    block1():
      %2809 : __torch__.torch.nn.modules.container.___torch_mangle_930.Sequential = prim::GetAttr[name="conv"](%38)
      %2810 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%2809)
      %2811 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%2809)
      %2812 : __torch__.torch.nn.modules.conv.___torch_mangle_929.Conv2d = prim::GetAttr[name="2"](%2809)
      %2813 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%2809)
      %2814 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%2810)
      %2815 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2810)
      %2816 : Tensor = prim::GetAttr[name="weight"](%2814)
      %2817 : Tensor? = prim::GetAttr[name="bias"](%2814)
      %2818 : int[] = prim::ListConstruct(%20, %20)
      %2819 : int[] = prim::ListConstruct(%19, %19)
      %2820 : int[] = prim::ListConstruct(%20, %20)
      %input.266 : Tensor = aten::conv2d(%input.31, %2816, %2817, %2818, %2819, %2820, %20) # torch/nn/modules/conv.py:415:15
      %2822 : int = aten::dim(%input.266) # torch/nn/modules/batchnorm.py:276:11
      %2823 : bool = aten::ne(%2822, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2823) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2824 : bool = prim::GetAttr[name="training"](%2815)
       = prim::If(%2824) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2825 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2815)
          %2826 : Tensor = aten::add(%2825, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2815, %2826)
          -> ()
        block1():
          -> ()
      %2827 : bool = prim::GetAttr[name="training"](%2815)
      %2828 : Tensor = prim::GetAttr[name="running_mean"](%2815)
      %2829 : Tensor = prim::GetAttr[name="running_var"](%2815)
      %2830 : Tensor = prim::GetAttr[name="weight"](%2815)
      %2831 : Tensor = prim::GetAttr[name="bias"](%2815)
       = prim::If(%2827) # torch/nn/functional.py:2011:4
        block0():
          %2832 : int[] = aten::size(%input.266) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%2832, %19) # torch/nn/functional.py:1991:17
          %2834 : int = aten::len(%2832) # torch/nn/functional.py:1992:19
          %2835 : int = aten::sub(%2834, %11) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%2835, %10, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %2839 : int = aten::add(%i.93, %11) # torch/nn/functional.py:1993:27
              %2840 : int = aten::__getitem__(%2832, %2839) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %2840) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.371)
          %2842 : bool = aten::eq(%size_prods.369, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2842) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.267 : Tensor = aten::batch_norm(%input.266, %2830, %2831, %2828, %2829, %2827, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.268 : Tensor = aten::hardtanh_(%input.267, %5, %4) # torch/nn/functional.py:1171:17
      %2845 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%2811)
      %2846 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2811)
      %2847 : Tensor = prim::GetAttr[name="weight"](%2845)
      %2848 : Tensor? = prim::GetAttr[name="bias"](%2845)
      %2849 : int[] = prim::ListConstruct(%20, %20)
      %2850 : int[] = prim::ListConstruct(%20, %20)
      %2851 : int[] = prim::ListConstruct(%20, %20)
      %input.269 : Tensor = aten::conv2d(%input.268, %2847, %2848, %2849, %2850, %2851, %18) # torch/nn/modules/conv.py:415:15
      %2853 : int = aten::dim(%input.269) # torch/nn/modules/batchnorm.py:276:11
      %2854 : bool = aten::ne(%2853, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2854) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2855 : bool = prim::GetAttr[name="training"](%2846)
       = prim::If(%2855) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2856 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2846)
          %2857 : Tensor = aten::add(%2856, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2846, %2857)
          -> ()
        block1():
          -> ()
      %2858 : bool = prim::GetAttr[name="training"](%2846)
      %2859 : Tensor = prim::GetAttr[name="running_mean"](%2846)
      %2860 : Tensor = prim::GetAttr[name="running_var"](%2846)
      %2861 : Tensor = prim::GetAttr[name="weight"](%2846)
      %2862 : Tensor = prim::GetAttr[name="bias"](%2846)
       = prim::If(%2858) # torch/nn/functional.py:2011:4
        block0():
          %2863 : int[] = aten::size(%input.269) # torch/nn/functional.py:2012:27
          %size_prods.372 : int = aten::__getitem__(%2863, %19) # torch/nn/functional.py:1991:17
          %2865 : int = aten::len(%2863) # torch/nn/functional.py:1992:19
          %2866 : int = aten::sub(%2865, %11) # torch/nn/functional.py:1992:19
          %size_prods.373 : int = prim::Loop(%2866, %10, %size_prods.372) # torch/nn/functional.py:1992:4
            block0(%i.94 : int, %size_prods.374 : int):
              %2870 : int = aten::add(%i.94, %11) # torch/nn/functional.py:1993:27
              %2871 : int = aten::__getitem__(%2863, %2870) # torch/nn/functional.py:1993:22
              %size_prods.375 : int = aten::mul(%size_prods.374, %2871) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.375)
          %2873 : bool = aten::eq(%size_prods.373, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2873) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.270 : Tensor = aten::batch_norm(%input.269, %2861, %2862, %2859, %2860, %2858, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.271 : Tensor = aten::hardtanh_(%input.270, %5, %4) # torch/nn/functional.py:1171:17
      %2876 : Tensor = prim::GetAttr[name="weight"](%2812)
      %2877 : Tensor? = prim::GetAttr[name="bias"](%2812)
      %2878 : int[] = prim::ListConstruct(%20, %20)
      %2879 : int[] = prim::ListConstruct(%19, %19)
      %2880 : int[] = prim::ListConstruct(%20, %20)
      %input.272 : Tensor = aten::conv2d(%input.271, %2876, %2877, %2878, %2879, %2880, %20) # torch/nn/modules/conv.py:415:15
      %2882 : int = aten::dim(%input.272) # torch/nn/modules/batchnorm.py:276:11
      %2883 : bool = aten::ne(%2882, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2883) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2884 : bool = prim::GetAttr[name="training"](%2813)
       = prim::If(%2884) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2885 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2813)
          %2886 : Tensor = aten::add(%2885, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2813, %2886)
          -> ()
        block1():
          -> ()
      %2887 : bool = prim::GetAttr[name="training"](%2813)
      %2888 : Tensor = prim::GetAttr[name="running_mean"](%2813)
      %2889 : Tensor = prim::GetAttr[name="running_var"](%2813)
      %2890 : Tensor = prim::GetAttr[name="weight"](%2813)
      %2891 : Tensor = prim::GetAttr[name="bias"](%2813)
       = prim::If(%2887) # torch/nn/functional.py:2011:4
        block0():
          %2892 : int[] = aten::size(%input.272) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%2892, %19) # torch/nn/functional.py:1991:17
          %2894 : int = aten::len(%2892) # torch/nn/functional.py:1992:19
          %2895 : int = aten::sub(%2894, %11) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%2895, %10, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %2899 : int = aten::add(%i.95, %11) # torch/nn/functional.py:1993:27
              %2900 : int = aten::__getitem__(%2892, %2899) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %2900) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.379)
          %2902 : bool = aten::eq(%size_prods.377, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2902) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.273 : Tensor = aten::batch_norm(%input.272, %2890, %2891, %2888, %2889, %2887, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.273)
  %2904 : bool = prim::GetAttr[name="use_res_connect"](%39)
  %input.35 : Tensor = prim::If(%2904) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %2906 : __torch__.torch.nn.modules.container.___torch_mangle_930.Sequential = prim::GetAttr[name="conv"](%39)
      %2907 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%2906)
      %2908 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%2906)
      %2909 : __torch__.torch.nn.modules.conv.___torch_mangle_929.Conv2d = prim::GetAttr[name="2"](%2906)
      %2910 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%2906)
      %2911 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%2907)
      %2912 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2907)
      %2913 : Tensor = prim::GetAttr[name="weight"](%2911)
      %2914 : Tensor? = prim::GetAttr[name="bias"](%2911)
      %2915 : int[] = prim::ListConstruct(%20, %20)
      %2916 : int[] = prim::ListConstruct(%19, %19)
      %2917 : int[] = prim::ListConstruct(%20, %20)
      %input.274 : Tensor = aten::conv2d(%input.33, %2913, %2914, %2915, %2916, %2917, %20) # torch/nn/modules/conv.py:415:15
      %2919 : int = aten::dim(%input.274) # torch/nn/modules/batchnorm.py:276:11
      %2920 : bool = aten::ne(%2919, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2920) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2921 : bool = prim::GetAttr[name="training"](%2912)
       = prim::If(%2921) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2922 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2912)
          %2923 : Tensor = aten::add(%2922, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2912, %2923)
          -> ()
        block1():
          -> ()
      %2924 : bool = prim::GetAttr[name="training"](%2912)
      %2925 : Tensor = prim::GetAttr[name="running_mean"](%2912)
      %2926 : Tensor = prim::GetAttr[name="running_var"](%2912)
      %2927 : Tensor = prim::GetAttr[name="weight"](%2912)
      %2928 : Tensor = prim::GetAttr[name="bias"](%2912)
       = prim::If(%2924) # torch/nn/functional.py:2011:4
        block0():
          %2929 : int[] = aten::size(%input.274) # torch/nn/functional.py:2012:27
          %size_prods.380 : int = aten::__getitem__(%2929, %19) # torch/nn/functional.py:1991:17
          %2931 : int = aten::len(%2929) # torch/nn/functional.py:1992:19
          %2932 : int = aten::sub(%2931, %11) # torch/nn/functional.py:1992:19
          %size_prods.381 : int = prim::Loop(%2932, %10, %size_prods.380) # torch/nn/functional.py:1992:4
            block0(%i.96 : int, %size_prods.382 : int):
              %2936 : int = aten::add(%i.96, %11) # torch/nn/functional.py:1993:27
              %2937 : int = aten::__getitem__(%2929, %2936) # torch/nn/functional.py:1993:22
              %size_prods.383 : int = aten::mul(%size_prods.382, %2937) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.383)
          %2939 : bool = aten::eq(%size_prods.381, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2939) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.275 : Tensor = aten::batch_norm(%input.274, %2927, %2928, %2925, %2926, %2924, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.276 : Tensor = aten::hardtanh_(%input.275, %5, %4) # torch/nn/functional.py:1171:17
      %2942 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%2908)
      %2943 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%2908)
      %2944 : Tensor = prim::GetAttr[name="weight"](%2942)
      %2945 : Tensor? = prim::GetAttr[name="bias"](%2942)
      %2946 : int[] = prim::ListConstruct(%20, %20)
      %2947 : int[] = prim::ListConstruct(%20, %20)
      %2948 : int[] = prim::ListConstruct(%20, %20)
      %input.277 : Tensor = aten::conv2d(%input.276, %2944, %2945, %2946, %2947, %2948, %18) # torch/nn/modules/conv.py:415:15
      %2950 : int = aten::dim(%input.277) # torch/nn/modules/batchnorm.py:276:11
      %2951 : bool = aten::ne(%2950, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2951) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2952 : bool = prim::GetAttr[name="training"](%2943)
       = prim::If(%2952) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2953 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2943)
          %2954 : Tensor = aten::add(%2953, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2943, %2954)
          -> ()
        block1():
          -> ()
      %2955 : bool = prim::GetAttr[name="training"](%2943)
      %2956 : Tensor = prim::GetAttr[name="running_mean"](%2943)
      %2957 : Tensor = prim::GetAttr[name="running_var"](%2943)
      %2958 : Tensor = prim::GetAttr[name="weight"](%2943)
      %2959 : Tensor = prim::GetAttr[name="bias"](%2943)
       = prim::If(%2955) # torch/nn/functional.py:2011:4
        block0():
          %2960 : int[] = aten::size(%input.277) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%2960, %19) # torch/nn/functional.py:1991:17
          %2962 : int = aten::len(%2960) # torch/nn/functional.py:1992:19
          %2963 : int = aten::sub(%2962, %11) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%2963, %10, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %2967 : int = aten::add(%i.97, %11) # torch/nn/functional.py:1993:27
              %2968 : int = aten::__getitem__(%2960, %2967) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %2968) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.387)
          %2970 : bool = aten::eq(%size_prods.385, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2970) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.278 : Tensor = aten::batch_norm(%input.277, %2958, %2959, %2956, %2957, %2955, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.279 : Tensor = aten::hardtanh_(%input.278, %5, %4) # torch/nn/functional.py:1171:17
      %2973 : Tensor = prim::GetAttr[name="weight"](%2909)
      %2974 : Tensor? = prim::GetAttr[name="bias"](%2909)
      %2975 : int[] = prim::ListConstruct(%20, %20)
      %2976 : int[] = prim::ListConstruct(%19, %19)
      %2977 : int[] = prim::ListConstruct(%20, %20)
      %input.280 : Tensor = aten::conv2d(%input.279, %2973, %2974, %2975, %2976, %2977, %20) # torch/nn/modules/conv.py:415:15
      %2979 : int = aten::dim(%input.280) # torch/nn/modules/batchnorm.py:276:11
      %2980 : bool = aten::ne(%2979, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2980) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2981 : bool = prim::GetAttr[name="training"](%2910)
       = prim::If(%2981) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2982 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2910)
          %2983 : Tensor = aten::add(%2982, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2910, %2983)
          -> ()
        block1():
          -> ()
      %2984 : bool = prim::GetAttr[name="training"](%2910)
      %2985 : Tensor = prim::GetAttr[name="running_mean"](%2910)
      %2986 : Tensor = prim::GetAttr[name="running_var"](%2910)
      %2987 : Tensor = prim::GetAttr[name="weight"](%2910)
      %2988 : Tensor = prim::GetAttr[name="bias"](%2910)
       = prim::If(%2984) # torch/nn/functional.py:2011:4
        block0():
          %2989 : int[] = aten::size(%input.280) # torch/nn/functional.py:2012:27
          %size_prods.388 : int = aten::__getitem__(%2989, %19) # torch/nn/functional.py:1991:17
          %2991 : int = aten::len(%2989) # torch/nn/functional.py:1992:19
          %2992 : int = aten::sub(%2991, %11) # torch/nn/functional.py:1992:19
          %size_prods.389 : int = prim::Loop(%2992, %10, %size_prods.388) # torch/nn/functional.py:1992:4
            block0(%i.98 : int, %size_prods.390 : int):
              %2996 : int = aten::add(%i.98, %11) # torch/nn/functional.py:1993:27
              %2997 : int = aten::__getitem__(%2989, %2996) # torch/nn/functional.py:1993:22
              %size_prods.391 : int = aten::mul(%size_prods.390, %2997) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.391)
          %2999 : bool = aten::eq(%size_prods.389, %20) # torch/nn/functional.py:1994:7
           = prim::If(%2999) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.281 : Tensor = aten::batch_norm(%input.280, %2987, %2988, %2985, %2986, %2984, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %3001 : Tensor = aten::add(%input.33, %input.281, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%3001)
    block1():
      %3002 : __torch__.torch.nn.modules.container.___torch_mangle_930.Sequential = prim::GetAttr[name="conv"](%39)
      %3003 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%3002)
      %3004 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%3002)
      %3005 : __torch__.torch.nn.modules.conv.___torch_mangle_929.Conv2d = prim::GetAttr[name="2"](%3002)
      %3006 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_78.BatchNorm2d = prim::GetAttr[name="3"](%3002)
      %3007 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%3003)
      %3008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3003)
      %3009 : Tensor = prim::GetAttr[name="weight"](%3007)
      %3010 : Tensor? = prim::GetAttr[name="bias"](%3007)
      %3011 : int[] = prim::ListConstruct(%20, %20)
      %3012 : int[] = prim::ListConstruct(%19, %19)
      %3013 : int[] = prim::ListConstruct(%20, %20)
      %input.282 : Tensor = aten::conv2d(%input.33, %3009, %3010, %3011, %3012, %3013, %20) # torch/nn/modules/conv.py:415:15
      %3015 : int = aten::dim(%input.282) # torch/nn/modules/batchnorm.py:276:11
      %3016 : bool = aten::ne(%3015, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3016) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3017 : bool = prim::GetAttr[name="training"](%3008)
       = prim::If(%3017) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3018 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3008)
          %3019 : Tensor = aten::add(%3018, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3008, %3019)
          -> ()
        block1():
          -> ()
      %3020 : bool = prim::GetAttr[name="training"](%3008)
      %3021 : Tensor = prim::GetAttr[name="running_mean"](%3008)
      %3022 : Tensor = prim::GetAttr[name="running_var"](%3008)
      %3023 : Tensor = prim::GetAttr[name="weight"](%3008)
      %3024 : Tensor = prim::GetAttr[name="bias"](%3008)
       = prim::If(%3020) # torch/nn/functional.py:2011:4
        block0():
          %3025 : int[] = aten::size(%input.282) # torch/nn/functional.py:2012:27
          %size_prods.392 : int = aten::__getitem__(%3025, %19) # torch/nn/functional.py:1991:17
          %3027 : int = aten::len(%3025) # torch/nn/functional.py:1992:19
          %3028 : int = aten::sub(%3027, %11) # torch/nn/functional.py:1992:19
          %size_prods.393 : int = prim::Loop(%3028, %10, %size_prods.392) # torch/nn/functional.py:1992:4
            block0(%i.99 : int, %size_prods.394 : int):
              %3032 : int = aten::add(%i.99, %11) # torch/nn/functional.py:1993:27
              %3033 : int = aten::__getitem__(%3025, %3032) # torch/nn/functional.py:1993:22
              %size_prods.395 : int = aten::mul(%size_prods.394, %3033) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.395)
          %3035 : bool = aten::eq(%size_prods.393, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3035) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.283 : Tensor = aten::batch_norm(%input.282, %3023, %3024, %3021, %3022, %3020, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.284 : Tensor = aten::hardtanh_(%input.283, %5, %4) # torch/nn/functional.py:1171:17
      %3038 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%3004)
      %3039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3004)
      %3040 : Tensor = prim::GetAttr[name="weight"](%3038)
      %3041 : Tensor? = prim::GetAttr[name="bias"](%3038)
      %3042 : int[] = prim::ListConstruct(%20, %20)
      %3043 : int[] = prim::ListConstruct(%20, %20)
      %3044 : int[] = prim::ListConstruct(%20, %20)
      %input.285 : Tensor = aten::conv2d(%input.284, %3040, %3041, %3042, %3043, %3044, %18) # torch/nn/modules/conv.py:415:15
      %3046 : int = aten::dim(%input.285) # torch/nn/modules/batchnorm.py:276:11
      %3047 : bool = aten::ne(%3046, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3047) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3048 : bool = prim::GetAttr[name="training"](%3039)
       = prim::If(%3048) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3049 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3039)
          %3050 : Tensor = aten::add(%3049, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3039, %3050)
          -> ()
        block1():
          -> ()
      %3051 : bool = prim::GetAttr[name="training"](%3039)
      %3052 : Tensor = prim::GetAttr[name="running_mean"](%3039)
      %3053 : Tensor = prim::GetAttr[name="running_var"](%3039)
      %3054 : Tensor = prim::GetAttr[name="weight"](%3039)
      %3055 : Tensor = prim::GetAttr[name="bias"](%3039)
       = prim::If(%3051) # torch/nn/functional.py:2011:4
        block0():
          %3056 : int[] = aten::size(%input.285) # torch/nn/functional.py:2012:27
          %size_prods.396 : int = aten::__getitem__(%3056, %19) # torch/nn/functional.py:1991:17
          %3058 : int = aten::len(%3056) # torch/nn/functional.py:1992:19
          %3059 : int = aten::sub(%3058, %11) # torch/nn/functional.py:1992:19
          %size_prods.397 : int = prim::Loop(%3059, %10, %size_prods.396) # torch/nn/functional.py:1992:4
            block0(%i.100 : int, %size_prods.398 : int):
              %3063 : int = aten::add(%i.100, %11) # torch/nn/functional.py:1993:27
              %3064 : int = aten::__getitem__(%3056, %3063) # torch/nn/functional.py:1993:22
              %size_prods.399 : int = aten::mul(%size_prods.398, %3064) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.399)
          %3066 : bool = aten::eq(%size_prods.397, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3066) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.286 : Tensor = aten::batch_norm(%input.285, %3054, %3055, %3052, %3053, %3051, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.287 : Tensor = aten::hardtanh_(%input.286, %5, %4) # torch/nn/functional.py:1171:17
      %3069 : Tensor = prim::GetAttr[name="weight"](%3005)
      %3070 : Tensor? = prim::GetAttr[name="bias"](%3005)
      %3071 : int[] = prim::ListConstruct(%20, %20)
      %3072 : int[] = prim::ListConstruct(%19, %19)
      %3073 : int[] = prim::ListConstruct(%20, %20)
      %input.288 : Tensor = aten::conv2d(%input.287, %3069, %3070, %3071, %3072, %3073, %20) # torch/nn/modules/conv.py:415:15
      %3075 : int = aten::dim(%input.288) # torch/nn/modules/batchnorm.py:276:11
      %3076 : bool = aten::ne(%3075, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3076) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3077 : bool = prim::GetAttr[name="training"](%3006)
       = prim::If(%3077) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3078 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3006)
          %3079 : Tensor = aten::add(%3078, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3006, %3079)
          -> ()
        block1():
          -> ()
      %3080 : bool = prim::GetAttr[name="training"](%3006)
      %3081 : Tensor = prim::GetAttr[name="running_mean"](%3006)
      %3082 : Tensor = prim::GetAttr[name="running_var"](%3006)
      %3083 : Tensor = prim::GetAttr[name="weight"](%3006)
      %3084 : Tensor = prim::GetAttr[name="bias"](%3006)
       = prim::If(%3080) # torch/nn/functional.py:2011:4
        block0():
          %3085 : int[] = aten::size(%input.288) # torch/nn/functional.py:2012:27
          %size_prods.400 : int = aten::__getitem__(%3085, %19) # torch/nn/functional.py:1991:17
          %3087 : int = aten::len(%3085) # torch/nn/functional.py:1992:19
          %3088 : int = aten::sub(%3087, %11) # torch/nn/functional.py:1992:19
          %size_prods.401 : int = prim::Loop(%3088, %10, %size_prods.400) # torch/nn/functional.py:1992:4
            block0(%i.101 : int, %size_prods.402 : int):
              %3092 : int = aten::add(%i.101, %11) # torch/nn/functional.py:1993:27
              %3093 : int = aten::__getitem__(%3085, %3092) # torch/nn/functional.py:1993:22
              %size_prods.403 : int = aten::mul(%size_prods.402, %3093) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.403)
          %3095 : bool = aten::eq(%size_prods.401, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3095) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.289 : Tensor = aten::batch_norm(%input.288, %3083, %3084, %3081, %3082, %3080, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.289)
  %3097 : bool = prim::GetAttr[name="use_res_connect"](%40)
  %input.37 : Tensor = prim::If(%3097) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:71:8
    block0():
      %3099 : __torch__.torch.nn.modules.container.___torch_mangle_933.Sequential = prim::GetAttr[name="conv"](%40)
      %3100 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%3099)
      %3101 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%3099)
      %3102 : __torch__.torch.nn.modules.conv.___torch_mangle_932.Conv2d = prim::GetAttr[name="2"](%3099)
      %3103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="3"](%3099)
      %3104 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%3100)
      %3105 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3100)
      %3106 : Tensor = prim::GetAttr[name="weight"](%3104)
      %3107 : Tensor? = prim::GetAttr[name="bias"](%3104)
      %3108 : int[] = prim::ListConstruct(%20, %20)
      %3109 : int[] = prim::ListConstruct(%19, %19)
      %3110 : int[] = prim::ListConstruct(%20, %20)
      %input.12 : Tensor = aten::conv2d(%input.35, %3106, %3107, %3108, %3109, %3110, %20) # torch/nn/modules/conv.py:415:15
      %3112 : int = aten::dim(%input.12) # torch/nn/modules/batchnorm.py:276:11
      %3113 : bool = aten::ne(%3112, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3113) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3114 : bool = prim::GetAttr[name="training"](%3105)
       = prim::If(%3114) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3115 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3105)
          %3116 : Tensor = aten::add(%3115, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3105, %3116)
          -> ()
        block1():
          -> ()
      %3117 : bool = prim::GetAttr[name="training"](%3105)
      %3118 : Tensor = prim::GetAttr[name="running_mean"](%3105)
      %3119 : Tensor = prim::GetAttr[name="running_var"](%3105)
      %3120 : Tensor = prim::GetAttr[name="weight"](%3105)
      %3121 : Tensor = prim::GetAttr[name="bias"](%3105)
       = prim::If(%3117) # torch/nn/functional.py:2011:4
        block0():
          %3122 : int[] = aten::size(%input.12) # torch/nn/functional.py:2012:27
          %size_prods.12 : int = aten::__getitem__(%3122, %19) # torch/nn/functional.py:1991:17
          %3124 : int = aten::len(%3122) # torch/nn/functional.py:1992:19
          %3125 : int = aten::sub(%3124, %11) # torch/nn/functional.py:1992:19
          %size_prods.13 : int = prim::Loop(%3125, %10, %size_prods.12) # torch/nn/functional.py:1992:4
            block0(%i.4 : int, %size_prods.14 : int):
              %3129 : int = aten::add(%i.4, %11) # torch/nn/functional.py:1993:27
              %3130 : int = aten::__getitem__(%3122, %3129) # torch/nn/functional.py:1993:22
              %size_prods.15 : int = aten::mul(%size_prods.14, %3130) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.15)
          %3132 : bool = aten::eq(%size_prods.13, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3132) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.13 : Tensor = aten::batch_norm(%input.12, %3120, %3121, %3118, %3119, %3117, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.292 : Tensor = aten::hardtanh_(%input.13, %5, %4) # torch/nn/functional.py:1171:17
      %3135 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%3101)
      %3136 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3101)
      %3137 : Tensor = prim::GetAttr[name="weight"](%3135)
      %3138 : Tensor? = prim::GetAttr[name="bias"](%3135)
      %3139 : int[] = prim::ListConstruct(%20, %20)
      %3140 : int[] = prim::ListConstruct(%20, %20)
      %3141 : int[] = prim::ListConstruct(%20, %20)
      %input.14 : Tensor = aten::conv2d(%input.292, %3137, %3138, %3139, %3140, %3141, %18) # torch/nn/modules/conv.py:415:15
      %3143 : int = aten::dim(%input.14) # torch/nn/modules/batchnorm.py:276:11
      %3144 : bool = aten::ne(%3143, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3144) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3145 : bool = prim::GetAttr[name="training"](%3136)
       = prim::If(%3145) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3146 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3136)
          %3147 : Tensor = aten::add(%3146, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3136, %3147)
          -> ()
        block1():
          -> ()
      %3148 : bool = prim::GetAttr[name="training"](%3136)
      %3149 : Tensor = prim::GetAttr[name="running_mean"](%3136)
      %3150 : Tensor = prim::GetAttr[name="running_var"](%3136)
      %3151 : Tensor = prim::GetAttr[name="weight"](%3136)
      %3152 : Tensor = prim::GetAttr[name="bias"](%3136)
       = prim::If(%3148) # torch/nn/functional.py:2011:4
        block0():
          %3153 : int[] = aten::size(%input.14) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%3153, %19) # torch/nn/functional.py:1991:17
          %3155 : int = aten::len(%3153) # torch/nn/functional.py:1992:19
          %3156 : int = aten::sub(%3155, %11) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%3156, %10, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %3160 : int = aten::add(%i.5, %11) # torch/nn/functional.py:1993:27
              %3161 : int = aten::__getitem__(%3153, %3160) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %3161) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.19)
          %3163 : bool = aten::eq(%size_prods.17, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3163) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.15 : Tensor = aten::batch_norm(%input.14, %3151, %3152, %3149, %3150, %3148, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.16 : Tensor = aten::hardtanh_(%input.15, %5, %4) # torch/nn/functional.py:1171:17
      %3166 : Tensor = prim::GetAttr[name="weight"](%3102)
      %3167 : Tensor? = prim::GetAttr[name="bias"](%3102)
      %3168 : int[] = prim::ListConstruct(%20, %20)
      %3169 : int[] = prim::ListConstruct(%19, %19)
      %3170 : int[] = prim::ListConstruct(%20, %20)
      %input.17 : Tensor = aten::conv2d(%input.16, %3166, %3167, %3168, %3169, %3170, %20) # torch/nn/modules/conv.py:415:15
      %3172 : int = aten::dim(%input.17) # torch/nn/modules/batchnorm.py:276:11
      %3173 : bool = aten::ne(%3172, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3173) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3174 : bool = prim::GetAttr[name="training"](%3103)
       = prim::If(%3174) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3175 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3103)
          %3176 : Tensor = aten::add(%3175, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3103, %3176)
          -> ()
        block1():
          -> ()
      %3177 : bool = prim::GetAttr[name="training"](%3103)
      %3178 : Tensor = prim::GetAttr[name="running_mean"](%3103)
      %3179 : Tensor = prim::GetAttr[name="running_var"](%3103)
      %3180 : Tensor = prim::GetAttr[name="weight"](%3103)
      %3181 : Tensor = prim::GetAttr[name="bias"](%3103)
       = prim::If(%3177) # torch/nn/functional.py:2011:4
        block0():
          %3182 : int[] = aten::size(%input.17) # torch/nn/functional.py:2012:27
          %size_prods.20 : int = aten::__getitem__(%3182, %19) # torch/nn/functional.py:1991:17
          %3184 : int = aten::len(%3182) # torch/nn/functional.py:1992:19
          %3185 : int = aten::sub(%3184, %11) # torch/nn/functional.py:1992:19
          %size_prods.21 : int = prim::Loop(%3185, %10, %size_prods.20) # torch/nn/functional.py:1992:4
            block0(%i.6 : int, %size_prods.22 : int):
              %3189 : int = aten::add(%i.6, %11) # torch/nn/functional.py:1993:27
              %3190 : int = aten::__getitem__(%3182, %3189) # torch/nn/functional.py:1993:22
              %size_prods.23 : int = aten::mul(%size_prods.22, %3190) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.23)
          %3192 : bool = aten::eq(%size_prods.21, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3192) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.18 : Tensor = aten::batch_norm(%input.17, %3180, %3181, %3178, %3179, %3177, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %3194 : Tensor = aten::add(%input.35, %input.18, %20) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:72:19
      -> (%3194)
    block1():
      %3195 : __torch__.torch.nn.modules.container.___torch_mangle_933.Sequential = prim::GetAttr[name="conv"](%40)
      %3196 : __torch__.torchvision.models.mobilenet.___torch_mangle_926.ConvBNReLU = prim::GetAttr[name="0"](%3195)
      %3197 : __torch__.torchvision.models.mobilenet.___torch_mangle_928.ConvBNReLU = prim::GetAttr[name="1"](%3195)
      %3198 : __torch__.torch.nn.modules.conv.___torch_mangle_932.Conv2d = prim::GetAttr[name="2"](%3195)
      %3199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_91.BatchNorm2d = prim::GetAttr[name="3"](%3195)
      %3200 : __torch__.torch.nn.modules.conv.___torch_mangle_925.Conv2d = prim::GetAttr[name="0"](%3196)
      %3201 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3196)
      %3202 : Tensor = prim::GetAttr[name="weight"](%3200)
      %3203 : Tensor? = prim::GetAttr[name="bias"](%3200)
      %3204 : int[] = prim::ListConstruct(%20, %20)
      %3205 : int[] = prim::ListConstruct(%19, %19)
      %3206 : int[] = prim::ListConstruct(%20, %20)
      %input.11 : Tensor = aten::conv2d(%input.35, %3202, %3203, %3204, %3205, %3206, %20) # torch/nn/modules/conv.py:415:15
      %3208 : int = aten::dim(%input.11) # torch/nn/modules/batchnorm.py:276:11
      %3209 : bool = aten::ne(%3208, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3209) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3210 : bool = prim::GetAttr[name="training"](%3201)
       = prim::If(%3210) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3211 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3201)
          %3212 : Tensor = aten::add(%3211, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3201, %3212)
          -> ()
        block1():
          -> ()
      %3213 : bool = prim::GetAttr[name="training"](%3201)
      %3214 : Tensor = prim::GetAttr[name="running_mean"](%3201)
      %3215 : Tensor = prim::GetAttr[name="running_var"](%3201)
      %3216 : Tensor = prim::GetAttr[name="weight"](%3201)
      %3217 : Tensor = prim::GetAttr[name="bias"](%3201)
       = prim::If(%3213) # torch/nn/functional.py:2011:4
        block0():
          %3218 : int[] = aten::size(%input.11) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%3218, %19) # torch/nn/functional.py:1991:17
          %3220 : int = aten::len(%3218) # torch/nn/functional.py:1992:19
          %3221 : int = aten::sub(%3220, %11) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%3221, %10, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %3225 : int = aten::add(%i.2, %11) # torch/nn/functional.py:1993:27
              %3226 : int = aten::__getitem__(%3218, %3225) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %3226) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.5)
          %3228 : bool = aten::eq(%size_prods.4, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3228) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.10 : Tensor = aten::batch_norm(%input.11, %3216, %3217, %3214, %3215, %3213, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.4 : Tensor = aten::hardtanh_(%input.10, %5, %4) # torch/nn/functional.py:1171:17
      %3231 : __torch__.torch.nn.modules.conv.___torch_mangle_927.Conv2d = prim::GetAttr[name="0"](%3197)
      %3232 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="1"](%3197)
      %3233 : Tensor = prim::GetAttr[name="weight"](%3231)
      %3234 : Tensor? = prim::GetAttr[name="bias"](%3231)
      %3235 : int[] = prim::ListConstruct(%20, %20)
      %3236 : int[] = prim::ListConstruct(%20, %20)
      %3237 : int[] = prim::ListConstruct(%20, %20)
      %input.290 : Tensor = aten::conv2d(%input.4, %3233, %3234, %3235, %3236, %3237, %18) # torch/nn/modules/conv.py:415:15
      %3239 : int = aten::dim(%input.290) # torch/nn/modules/batchnorm.py:276:11
      %3240 : bool = aten::ne(%3239, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3240) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3241 : bool = prim::GetAttr[name="training"](%3232)
       = prim::If(%3241) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3242 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3232)
          %3243 : Tensor = aten::add(%3242, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3232, %3243)
          -> ()
        block1():
          -> ()
      %3244 : bool = prim::GetAttr[name="training"](%3232)
      %3245 : Tensor = prim::GetAttr[name="running_mean"](%3232)
      %3246 : Tensor = prim::GetAttr[name="running_var"](%3232)
      %3247 : Tensor = prim::GetAttr[name="weight"](%3232)
      %3248 : Tensor = prim::GetAttr[name="bias"](%3232)
       = prim::If(%3244) # torch/nn/functional.py:2011:4
        block0():
          %3249 : int[] = aten::size(%input.290) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%3249, %19) # torch/nn/functional.py:1991:17
          %3251 : int = aten::len(%3249) # torch/nn/functional.py:1992:19
          %3252 : int = aten::sub(%3251, %11) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%3252, %10, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %3256 : int = aten::add(%i.3, %11) # torch/nn/functional.py:1993:27
              %3257 : int = aten::__getitem__(%3249, %3256) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %3257) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.11)
          %3259 : bool = aten::eq(%size_prods.9, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3259) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.291 : Tensor = aten::batch_norm(%input.290, %3247, %3248, %3245, %3246, %3244, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      %input.6 : Tensor = aten::hardtanh_(%input.291, %5, %4) # torch/nn/functional.py:1171:17
      %3262 : Tensor = prim::GetAttr[name="weight"](%3198)
      %3263 : Tensor? = prim::GetAttr[name="bias"](%3198)
      %3264 : int[] = prim::ListConstruct(%20, %20)
      %3265 : int[] = prim::ListConstruct(%19, %19)
      %3266 : int[] = prim::ListConstruct(%20, %20)
      %input.8 : Tensor = aten::conv2d(%input.6, %3262, %3263, %3264, %3265, %3266, %20) # torch/nn/modules/conv.py:415:15
      %3268 : int = aten::dim(%input.8) # torch/nn/modules/batchnorm.py:276:11
      %3269 : bool = aten::ne(%3268, %8) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3269) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3270 : bool = prim::GetAttr[name="training"](%3199)
       = prim::If(%3270) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3271 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3199)
          %3272 : Tensor = aten::add(%3271, %20, %20) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3199, %3272)
          -> ()
        block1():
          -> ()
      %3273 : bool = prim::GetAttr[name="training"](%3199)
      %3274 : Tensor = prim::GetAttr[name="running_mean"](%3199)
      %3275 : Tensor = prim::GetAttr[name="running_var"](%3199)
      %3276 : Tensor = prim::GetAttr[name="weight"](%3199)
      %3277 : Tensor = prim::GetAttr[name="bias"](%3199)
       = prim::If(%3273) # torch/nn/functional.py:2011:4
        block0():
          %3278 : int[] = aten::size(%input.8) # torch/nn/functional.py:2012:27
          %size_prods.404 : int = aten::__getitem__(%3278, %19) # torch/nn/functional.py:1991:17
          %3280 : int = aten::len(%3278) # torch/nn/functional.py:1992:19
          %3281 : int = aten::sub(%3280, %11) # torch/nn/functional.py:1992:19
          %size_prods.405 : int = prim::Loop(%3281, %10, %size_prods.404) # torch/nn/functional.py:1992:4
            block0(%i.102 : int, %size_prods.406 : int):
              %3285 : int = aten::add(%i.102, %11) # torch/nn/functional.py:1993:27
              %3286 : int = aten::__getitem__(%3278, %3285) # torch/nn/functional.py:1993:22
              %size_prods.407 : int = aten::mul(%size_prods.406, %3286) # torch/nn/functional.py:1993:8
              -> (%10, %size_prods.407)
          %3288 : bool = aten::eq(%size_prods.405, %20) # torch/nn/functional.py:1994:7
           = prim::If(%3288) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %input.9 : Tensor = aten::batch_norm(%input.8, %3276, %3277, %3274, %3275, %3273, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
      -> (%input.9)
  %3290 : __torch__.torch.nn.modules.conv.___torch_mangle_807.Conv2d = prim::GetAttr[name="0"](%41)
  %3291 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_320.BatchNorm2d = prim::GetAttr[name="1"](%41)
  %3292 : Tensor = prim::GetAttr[name="weight"](%3290)
  %3293 : Tensor? = prim::GetAttr[name="bias"](%3290)
  %3294 : int[] = prim::ListConstruct(%20, %20)
  %3295 : int[] = prim::ListConstruct(%19, %19)
  %3296 : int[] = prim::ListConstruct(%20, %20)
  %input.7 : Tensor = aten::conv2d(%input.37, %3292, %3293, %3294, %3295, %3296, %20) # torch/nn/modules/conv.py:415:15
  %3298 : int = aten::dim(%input.7) # torch/nn/modules/batchnorm.py:276:11
  %3299 : bool = aten::ne(%3298, %8) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3299) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%9) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3300 : bool = prim::GetAttr[name="training"](%3291)
   = prim::If(%3300) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3301 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3291)
      %3302 : Tensor = aten::add(%3301, %20, %20) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3291, %3302)
      -> ()
    block1():
      -> ()
  %3303 : bool = prim::GetAttr[name="training"](%3291)
  %3304 : Tensor = prim::GetAttr[name="running_mean"](%3291)
  %3305 : Tensor = prim::GetAttr[name="running_var"](%3291)
  %3306 : Tensor = prim::GetAttr[name="weight"](%3291)
  %3307 : Tensor = prim::GetAttr[name="bias"](%3291)
   = prim::If(%3303) # torch/nn/functional.py:2011:4
    block0():
      %3308 : int[] = aten::size(%input.7) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%3308, %19) # torch/nn/functional.py:1991:17
      %3310 : int = aten::len(%3308) # torch/nn/functional.py:1992:19
      %3311 : int = aten::sub(%3310, %11) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%3311, %10, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %3315 : int = aten::add(%i.1, %11) # torch/nn/functional.py:1993:27
          %3316 : int = aten::__getitem__(%3308, %3315) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %3316) # torch/nn/functional.py:1993:8
          -> (%10, %size_prods.3)
      %3318 : bool = aten::eq(%size_prods, %20) # torch/nn/functional.py:1994:7
       = prim::If(%3318) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%9) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.39 : Tensor = aten::batch_norm(%input.7, %3306, %3307, %3304, %3305, %3303, %exponential_average_factor.2, %6, %10) # torch/nn/functional.py:2014:11
  %x.3 : Tensor = aten::hardtanh_(%input.39, %5, %4) # torch/nn/functional.py:1171:17
  %3321 : int[] = prim::ListConstruct(%20, %20)
  %3322 : int[] = aten::size(%x.3) # torch/nn/functional.py:925:51
  %3323 : int = aten::len(%3322) # <string>:5:9
  %3324 : bool = aten::gt(%3323, %11) # <string>:5:9
   = prim::If(%3324) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%9) # <string>:5:2
      -> ()
  %3325 : Tensor = aten::adaptive_avg_pool2d(%x.3, %3321) # torch/nn/functional.py:926:11
  %3326 : int[] = aten::size(%x.3) # <string>:7:9
  %3327 : int = aten::__getitem__(%3326, %19) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:166:60
  %3328 : int[] = prim::ListConstruct(%3327, %21)
  %x.6 : Tensor = aten::reshape(%3325, %3328) # torch/hub/pytorch_vision_master/torchvision/models/mobilenet.py:166:12
  %3330 : __torch__.torch.nn.modules.container.___torch_mangle_937.Sequential = prim::GetAttr[name="classifier"](%self)
  %3331 : __torch__.torch.nn.modules.dropout.___torch_mangle_530.Dropout = prim::GetAttr[name="0"](%3330)
  %3332 : __torch__.torch.nn.modules.linear.___torch_mangle_696.Linear = prim::GetAttr[name="1"](%3330)
  %3333 : bool = prim::GetAttr[name="training"](%3331)
  %input.3 : Tensor = aten::dropout(%x.6, %3, %3333) # torch/nn/functional.py:973:17
  %3335 : Tensor = prim::GetAttr[name="weight"](%3332)
  %3336 : Tensor = prim::GetAttr[name="bias"](%3332)
  %3337 : int = aten::dim(%input.3) # torch/nn/functional.py:1672:7
  %3338 : bool = aten::eq(%3337, %11) # torch/nn/functional.py:1672:7
  %x.8 : Tensor = prim::If(%3338) # torch/nn/functional.py:1672:4
    block0():
      %3340 : Tensor = aten::t(%3335) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%3336, %input.3, %3340, %20, %20) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %3342 : Tensor = aten::t(%3335) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%input.3, %3342) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %3336, %20) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%x.8)
