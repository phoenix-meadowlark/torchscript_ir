graph(%self : __torch__.torchvision.models.resnet.___torch_mangle_953.ResNet,
      %x.1 : Tensor):
  %3 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %4 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.1 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %6 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %7 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %8 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %9 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %10 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %11 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %12 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:29
  %13 : int = prim::Constant[value=-1]()
  %14 : __torch__.torch.nn.modules.conv.___torch_mangle_7.Conv2d = prim::GetAttr[name="conv1"](%self)
  %15 : Tensor = prim::GetAttr[name="weight"](%14)
  %16 : Tensor? = prim::GetAttr[name="bias"](%14)
  %17 : int[] = prim::ListConstruct(%10, %10)
  %18 : int[] = prim::ListConstruct(%11, %11)
  %19 : int[] = prim::ListConstruct(%12, %12)
  %x.3 : Tensor = aten::conv2d(%x.1, %15, %16, %17, %18, %19, %12) # torch/nn/modules/conv.py:415:15
  %21 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%self)
  %22 : int = aten::dim(%x.3) # torch/nn/modules/batchnorm.py:276:11
  %23 : bool = aten::ne(%22, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%23) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %24 : bool = prim::GetAttr[name="training"](%21)
   = prim::If(%24) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %25 : Tensor = prim::GetAttr[name="num_batches_tracked"](%21)
      %26 : Tensor = aten::add(%25, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%21, %26)
      -> ()
    block1():
      -> ()
  %27 : bool = prim::GetAttr[name="training"](%21)
  %28 : Tensor = prim::GetAttr[name="running_mean"](%21)
  %29 : Tensor = prim::GetAttr[name="running_var"](%21)
  %30 : Tensor = prim::GetAttr[name="weight"](%21)
  %31 : Tensor = prim::GetAttr[name="bias"](%21)
   = prim::If(%27) # torch/nn/functional.py:2011:4
    block0():
      %32 : int[] = aten::size(%x.3) # torch/nn/functional.py:2012:27
      %size_prods.500 : int = aten::__getitem__(%32, %8) # torch/nn/functional.py:1991:17
      %34 : int = aten::len(%32) # torch/nn/functional.py:1992:19
      %35 : int = aten::sub(%34, %10) # torch/nn/functional.py:1992:19
      %size_prods.501 : int = prim::Loop(%35, %9, %size_prods.500) # torch/nn/functional.py:1992:4
        block0(%i.126 : int, %size_prods.502 : int):
          %39 : int = aten::add(%i.126, %10) # torch/nn/functional.py:1993:27
          %40 : int = aten::__getitem__(%32, %39) # torch/nn/functional.py:1993:22
          %size_prods.503 : int = aten::mul(%size_prods.502, %40) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.503)
      %42 : bool = aten::eq(%size_prods.501, %12) # torch/nn/functional.py:1994:7
       = prim::If(%42) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %x.5 : Tensor = aten::batch_norm(%x.3, %30, %31, %28, %29, %27, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %x.7 : Tensor = aten::relu_(%x.5) # torch/nn/functional.py:1117:17
  %45 : int[] = prim::ListConstruct(%11, %11)
  %46 : int[] = prim::ListConstruct(%10, %10)
  %47 : int[] = prim::ListConstruct(%12, %12)
  %48 : int[] = prim::ListConstruct(%12, %12)
  %x.9 : Tensor = aten::max_pool2d(%x.7, %45, %46, %47, %48, %3) # torch/nn/functional.py:575:11
  %50 : __torch__.torch.nn.modules.container.___torch_mangle_16.Sequential = prim::GetAttr[name="layer1"](%self)
  %51 : __torch__.torchvision.models.resnet.Bottleneck = prim::GetAttr[name="0"](%50)
  %52 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="1"](%50)
  %53 : __torch__.torchvision.models.resnet.___torch_mangle_15.Bottleneck = prim::GetAttr[name="2"](%50)
  %54 : __torch__.torch.nn.modules.conv.___torch_mangle_9.Conv2d = prim::GetAttr[name="conv1"](%51)
  %55 : Tensor = prim::GetAttr[name="weight"](%54)
  %56 : Tensor? = prim::GetAttr[name="bias"](%54)
  %57 : int[] = prim::ListConstruct(%12, %12)
  %58 : int[] = prim::ListConstruct(%8, %8)
  %59 : int[] = prim::ListConstruct(%12, %12)
  %out.19 : Tensor = aten::conv2d(%x.9, %55, %56, %57, %58, %59, %12) # torch/nn/modules/conv.py:415:15
  %61 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%51)
  %62 : int = aten::dim(%out.19) # torch/nn/modules/batchnorm.py:276:11
  %63 : bool = aten::ne(%62, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%63) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %64 : bool = prim::GetAttr[name="training"](%61)
   = prim::If(%64) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %65 : Tensor = prim::GetAttr[name="num_batches_tracked"](%61)
      %66 : Tensor = aten::add(%65, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%61, %66)
      -> ()
    block1():
      -> ()
  %67 : bool = prim::GetAttr[name="training"](%61)
  %68 : Tensor = prim::GetAttr[name="running_mean"](%61)
  %69 : Tensor = prim::GetAttr[name="running_var"](%61)
  %70 : Tensor = prim::GetAttr[name="weight"](%61)
  %71 : Tensor = prim::GetAttr[name="bias"](%61)
   = prim::If(%67) # torch/nn/functional.py:2011:4
    block0():
      %72 : int[] = aten::size(%out.19) # torch/nn/functional.py:2012:27
      %size_prods.504 : int = aten::__getitem__(%72, %8) # torch/nn/functional.py:1991:17
      %74 : int = aten::len(%72) # torch/nn/functional.py:1992:19
      %75 : int = aten::sub(%74, %10) # torch/nn/functional.py:1992:19
      %size_prods.505 : int = prim::Loop(%75, %9, %size_prods.504) # torch/nn/functional.py:1992:4
        block0(%i.127 : int, %size_prods.506 : int):
          %79 : int = aten::add(%i.127, %10) # torch/nn/functional.py:1993:27
          %80 : int = aten::__getitem__(%72, %79) # torch/nn/functional.py:1993:22
          %size_prods.507 : int = aten::mul(%size_prods.506, %80) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.507)
      %82 : bool = aten::eq(%size_prods.505, %12) # torch/nn/functional.py:1994:7
       = prim::If(%82) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.353 : Tensor = aten::batch_norm(%out.19, %70, %71, %68, %69, %67, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.354 : Tensor = aten::relu_(%out.353) # torch/nn/functional.py:1117:17
  %85 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%51)
  %86 : Tensor = prim::GetAttr[name="weight"](%85)
  %87 : Tensor? = prim::GetAttr[name="bias"](%85)
  %88 : int[] = prim::ListConstruct(%12, %12)
  %89 : int[] = prim::ListConstruct(%12, %12)
  %90 : int[] = prim::ListConstruct(%12, %12)
  %out.355 : Tensor = aten::conv2d(%out.354, %86, %87, %88, %89, %90, %12) # torch/nn/modules/conv.py:415:15
  %92 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%51)
  %93 : int = aten::dim(%out.355) # torch/nn/modules/batchnorm.py:276:11
  %94 : bool = aten::ne(%93, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%94) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %95 : bool = prim::GetAttr[name="training"](%92)
   = prim::If(%95) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %96 : Tensor = prim::GetAttr[name="num_batches_tracked"](%92)
      %97 : Tensor = aten::add(%96, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%92, %97)
      -> ()
    block1():
      -> ()
  %98 : bool = prim::GetAttr[name="training"](%92)
  %99 : Tensor = prim::GetAttr[name="running_mean"](%92)
  %100 : Tensor = prim::GetAttr[name="running_var"](%92)
  %101 : Tensor = prim::GetAttr[name="weight"](%92)
  %102 : Tensor = prim::GetAttr[name="bias"](%92)
   = prim::If(%98) # torch/nn/functional.py:2011:4
    block0():
      %103 : int[] = aten::size(%out.355) # torch/nn/functional.py:2012:27
      %size_prods.508 : int = aten::__getitem__(%103, %8) # torch/nn/functional.py:1991:17
      %105 : int = aten::len(%103) # torch/nn/functional.py:1992:19
      %106 : int = aten::sub(%105, %10) # torch/nn/functional.py:1992:19
      %size_prods.509 : int = prim::Loop(%106, %9, %size_prods.508) # torch/nn/functional.py:1992:4
        block0(%i.128 : int, %size_prods.510 : int):
          %110 : int = aten::add(%i.128, %10) # torch/nn/functional.py:1993:27
          %111 : int = aten::__getitem__(%103, %110) # torch/nn/functional.py:1993:22
          %size_prods.511 : int = aten::mul(%size_prods.510, %111) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.511)
      %113 : bool = aten::eq(%size_prods.509, %12) # torch/nn/functional.py:1994:7
       = prim::If(%113) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.356 : Tensor = aten::batch_norm(%out.355, %101, %102, %99, %100, %98, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.357 : Tensor = aten::relu_(%out.356) # torch/nn/functional.py:1117:17
  %116 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%51)
  %117 : Tensor = prim::GetAttr[name="weight"](%116)
  %118 : Tensor? = prim::GetAttr[name="bias"](%116)
  %119 : int[] = prim::ListConstruct(%12, %12)
  %120 : int[] = prim::ListConstruct(%8, %8)
  %121 : int[] = prim::ListConstruct(%12, %12)
  %out.358 : Tensor = aten::conv2d(%out.357, %117, %118, %119, %120, %121, %12) # torch/nn/modules/conv.py:415:15
  %123 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%51)
  %124 : int = aten::dim(%out.358) # torch/nn/modules/batchnorm.py:276:11
  %125 : bool = aten::ne(%124, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%125) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %126 : bool = prim::GetAttr[name="training"](%123)
   = prim::If(%126) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %127 : Tensor = prim::GetAttr[name="num_batches_tracked"](%123)
      %128 : Tensor = aten::add(%127, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%123, %128)
      -> ()
    block1():
      -> ()
  %129 : bool = prim::GetAttr[name="training"](%123)
  %130 : Tensor = prim::GetAttr[name="running_mean"](%123)
  %131 : Tensor = prim::GetAttr[name="running_var"](%123)
  %132 : Tensor = prim::GetAttr[name="weight"](%123)
  %133 : Tensor = prim::GetAttr[name="bias"](%123)
   = prim::If(%129) # torch/nn/functional.py:2011:4
    block0():
      %134 : int[] = aten::size(%out.358) # torch/nn/functional.py:2012:27
      %size_prods.512 : int = aten::__getitem__(%134, %8) # torch/nn/functional.py:1991:17
      %136 : int = aten::len(%134) # torch/nn/functional.py:1992:19
      %137 : int = aten::sub(%136, %10) # torch/nn/functional.py:1992:19
      %size_prods.513 : int = prim::Loop(%137, %9, %size_prods.512) # torch/nn/functional.py:1992:4
        block0(%i.129 : int, %size_prods.514 : int):
          %141 : int = aten::add(%i.129, %10) # torch/nn/functional.py:1993:27
          %142 : int = aten::__getitem__(%134, %141) # torch/nn/functional.py:1993:22
          %size_prods.515 : int = aten::mul(%size_prods.514, %142) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.515)
      %144 : bool = aten::eq(%size_prods.513, %12) # torch/nn/functional.py:1994:7
       = prim::If(%144) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.359 : Tensor = aten::batch_norm(%out.358, %132, %133, %130, %131, %129, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %146 : __torch__.torch.nn.modules.container.___torch_mangle_13.Sequential = prim::GetAttr[name="downsample"](%51)
  %147 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="0"](%146)
  %148 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="1"](%146)
  %149 : Tensor = prim::GetAttr[name="weight"](%147)
  %150 : Tensor? = prim::GetAttr[name="bias"](%147)
  %151 : int[] = prim::ListConstruct(%12, %12)
  %152 : int[] = prim::ListConstruct(%8, %8)
  %153 : int[] = prim::ListConstruct(%12, %12)
  %input.6 : Tensor = aten::conv2d(%x.9, %149, %150, %151, %152, %153, %12) # torch/nn/modules/conv.py:415:15
  %155 : int = aten::dim(%input.6) # torch/nn/modules/batchnorm.py:276:11
  %156 : bool = aten::ne(%155, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%156) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %157 : bool = prim::GetAttr[name="training"](%148)
   = prim::If(%157) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %158 : Tensor = prim::GetAttr[name="num_batches_tracked"](%148)
      %159 : Tensor = aten::add(%158, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%148, %159)
      -> ()
    block1():
      -> ()
  %160 : bool = prim::GetAttr[name="training"](%148)
  %161 : Tensor = prim::GetAttr[name="running_mean"](%148)
  %162 : Tensor = prim::GetAttr[name="running_var"](%148)
  %163 : Tensor = prim::GetAttr[name="weight"](%148)
  %164 : Tensor = prim::GetAttr[name="bias"](%148)
   = prim::If(%160) # torch/nn/functional.py:2011:4
    block0():
      %165 : int[] = aten::size(%input.6) # torch/nn/functional.py:2012:27
      %size_prods.516 : int = aten::__getitem__(%165, %8) # torch/nn/functional.py:1991:17
      %167 : int = aten::len(%165) # torch/nn/functional.py:1992:19
      %168 : int = aten::sub(%167, %10) # torch/nn/functional.py:1992:19
      %size_prods.517 : int = prim::Loop(%168, %9, %size_prods.516) # torch/nn/functional.py:1992:4
        block0(%i.130 : int, %size_prods.518 : int):
          %172 : int = aten::add(%i.130, %10) # torch/nn/functional.py:1993:27
          %173 : int = aten::__getitem__(%165, %172) # torch/nn/functional.py:1993:22
          %size_prods.519 : int = aten::mul(%size_prods.518, %173) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.519)
      %175 : bool = aten::eq(%size_prods.517, %12) # torch/nn/functional.py:1994:7
       = prim::If(%175) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.2 : Tensor = aten::batch_norm(%input.6, %163, %164, %161, %162, %160, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.360 : Tensor = aten::add_(%out.359, %identity.2, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.10 : Tensor = aten::relu_(%out.360) # torch/nn/functional.py:1117:17
  %179 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%52)
  %180 : Tensor = prim::GetAttr[name="weight"](%179)
  %181 : Tensor? = prim::GetAttr[name="bias"](%179)
  %182 : int[] = prim::ListConstruct(%12, %12)
  %183 : int[] = prim::ListConstruct(%8, %8)
  %184 : int[] = prim::ListConstruct(%12, %12)
  %out.406 : Tensor = aten::conv2d(%input.10, %180, %181, %182, %183, %184, %12) # torch/nn/modules/conv.py:415:15
  %186 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%52)
  %187 : int = aten::dim(%out.406) # torch/nn/modules/batchnorm.py:276:11
  %188 : bool = aten::ne(%187, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%188) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %189 : bool = prim::GetAttr[name="training"](%186)
   = prim::If(%189) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %190 : Tensor = prim::GetAttr[name="num_batches_tracked"](%186)
      %191 : Tensor = aten::add(%190, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%186, %191)
      -> ()
    block1():
      -> ()
  %192 : bool = prim::GetAttr[name="training"](%186)
  %193 : Tensor = prim::GetAttr[name="running_mean"](%186)
  %194 : Tensor = prim::GetAttr[name="running_var"](%186)
  %195 : Tensor = prim::GetAttr[name="weight"](%186)
  %196 : Tensor = prim::GetAttr[name="bias"](%186)
   = prim::If(%192) # torch/nn/functional.py:2011:4
    block0():
      %197 : int[] = aten::size(%out.406) # torch/nn/functional.py:2012:27
      %size_prods.520 : int = aten::__getitem__(%197, %8) # torch/nn/functional.py:1991:17
      %199 : int = aten::len(%197) # torch/nn/functional.py:1992:19
      %200 : int = aten::sub(%199, %10) # torch/nn/functional.py:1992:19
      %size_prods.521 : int = prim::Loop(%200, %9, %size_prods.520) # torch/nn/functional.py:1992:4
        block0(%i.131 : int, %size_prods.522 : int):
          %204 : int = aten::add(%i.131, %10) # torch/nn/functional.py:1993:27
          %205 : int = aten::__getitem__(%197, %204) # torch/nn/functional.py:1993:22
          %size_prods.523 : int = aten::mul(%size_prods.522, %205) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.523)
      %207 : bool = aten::eq(%size_prods.521, %12) # torch/nn/functional.py:1994:7
       = prim::If(%207) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.344 : Tensor = aten::batch_norm(%out.406, %195, %196, %193, %194, %192, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.345 : Tensor = aten::relu_(%out.344) # torch/nn/functional.py:1117:17
  %210 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%52)
  %211 : Tensor = prim::GetAttr[name="weight"](%210)
  %212 : Tensor? = prim::GetAttr[name="bias"](%210)
  %213 : int[] = prim::ListConstruct(%12, %12)
  %214 : int[] = prim::ListConstruct(%12, %12)
  %215 : int[] = prim::ListConstruct(%12, %12)
  %out.346 : Tensor = aten::conv2d(%out.345, %211, %212, %213, %214, %215, %12) # torch/nn/modules/conv.py:415:15
  %217 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%52)
  %218 : int = aten::dim(%out.346) # torch/nn/modules/batchnorm.py:276:11
  %219 : bool = aten::ne(%218, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%219) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %220 : bool = prim::GetAttr[name="training"](%217)
   = prim::If(%220) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %221 : Tensor = prim::GetAttr[name="num_batches_tracked"](%217)
      %222 : Tensor = aten::add(%221, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%217, %222)
      -> ()
    block1():
      -> ()
  %223 : bool = prim::GetAttr[name="training"](%217)
  %224 : Tensor = prim::GetAttr[name="running_mean"](%217)
  %225 : Tensor = prim::GetAttr[name="running_var"](%217)
  %226 : Tensor = prim::GetAttr[name="weight"](%217)
  %227 : Tensor = prim::GetAttr[name="bias"](%217)
   = prim::If(%223) # torch/nn/functional.py:2011:4
    block0():
      %228 : int[] = aten::size(%out.346) # torch/nn/functional.py:2012:27
      %size_prods.524 : int = aten::__getitem__(%228, %8) # torch/nn/functional.py:1991:17
      %230 : int = aten::len(%228) # torch/nn/functional.py:1992:19
      %231 : int = aten::sub(%230, %10) # torch/nn/functional.py:1992:19
      %size_prods.525 : int = prim::Loop(%231, %9, %size_prods.524) # torch/nn/functional.py:1992:4
        block0(%i.132 : int, %size_prods.526 : int):
          %235 : int = aten::add(%i.132, %10) # torch/nn/functional.py:1993:27
          %236 : int = aten::__getitem__(%228, %235) # torch/nn/functional.py:1993:22
          %size_prods.527 : int = aten::mul(%size_prods.526, %236) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.527)
      %238 : bool = aten::eq(%size_prods.525, %12) # torch/nn/functional.py:1994:7
       = prim::If(%238) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.347 : Tensor = aten::batch_norm(%out.346, %226, %227, %224, %225, %223, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.348 : Tensor = aten::relu_(%out.347) # torch/nn/functional.py:1117:17
  %241 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%52)
  %242 : Tensor = prim::GetAttr[name="weight"](%241)
  %243 : Tensor? = prim::GetAttr[name="bias"](%241)
  %244 : int[] = prim::ListConstruct(%12, %12)
  %245 : int[] = prim::ListConstruct(%8, %8)
  %246 : int[] = prim::ListConstruct(%12, %12)
  %out.349 : Tensor = aten::conv2d(%out.348, %242, %243, %244, %245, %246, %12) # torch/nn/modules/conv.py:415:15
  %248 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%52)
  %249 : int = aten::dim(%out.349) # torch/nn/modules/batchnorm.py:276:11
  %250 : bool = aten::ne(%249, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%250) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %251 : bool = prim::GetAttr[name="training"](%248)
   = prim::If(%251) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %252 : Tensor = prim::GetAttr[name="num_batches_tracked"](%248)
      %253 : Tensor = aten::add(%252, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%248, %253)
      -> ()
    block1():
      -> ()
  %254 : bool = prim::GetAttr[name="training"](%248)
  %255 : Tensor = prim::GetAttr[name="running_mean"](%248)
  %256 : Tensor = prim::GetAttr[name="running_var"](%248)
  %257 : Tensor = prim::GetAttr[name="weight"](%248)
  %258 : Tensor = prim::GetAttr[name="bias"](%248)
   = prim::If(%254) # torch/nn/functional.py:2011:4
    block0():
      %259 : int[] = aten::size(%out.349) # torch/nn/functional.py:2012:27
      %size_prods.496 : int = aten::__getitem__(%259, %8) # torch/nn/functional.py:1991:17
      %261 : int = aten::len(%259) # torch/nn/functional.py:1992:19
      %262 : int = aten::sub(%261, %10) # torch/nn/functional.py:1992:19
      %size_prods.497 : int = prim::Loop(%262, %9, %size_prods.496) # torch/nn/functional.py:1992:4
        block0(%i.125 : int, %size_prods.498 : int):
          %266 : int = aten::add(%i.125, %10) # torch/nn/functional.py:1993:27
          %267 : int = aten::__getitem__(%259, %266) # torch/nn/functional.py:1993:22
          %size_prods.499 : int = aten::mul(%size_prods.498, %267) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.499)
      %269 : bool = aten::eq(%size_prods.497, %12) # torch/nn/functional.py:1994:7
       = prim::If(%269) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.350 : Tensor = aten::batch_norm(%out.349, %257, %258, %255, %256, %254, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.351 : Tensor = aten::add_(%out.350, %input.10, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.8 : Tensor = aten::relu_(%out.351) # torch/nn/functional.py:1117:17
  %273 : __torch__.torch.nn.modules.conv.___torch_mangle_14.Conv2d = prim::GetAttr[name="conv1"](%53)
  %274 : Tensor = prim::GetAttr[name="weight"](%273)
  %275 : Tensor? = prim::GetAttr[name="bias"](%273)
  %276 : int[] = prim::ListConstruct(%12, %12)
  %277 : int[] = prim::ListConstruct(%8, %8)
  %278 : int[] = prim::ListConstruct(%12, %12)
  %out.334 : Tensor = aten::conv2d(%input.8, %274, %275, %276, %277, %278, %12) # torch/nn/modules/conv.py:415:15
  %280 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn1"](%53)
  %281 : int = aten::dim(%out.334) # torch/nn/modules/batchnorm.py:276:11
  %282 : bool = aten::ne(%281, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%282) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %283 : bool = prim::GetAttr[name="training"](%280)
   = prim::If(%283) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %284 : Tensor = prim::GetAttr[name="num_batches_tracked"](%280)
      %285 : Tensor = aten::add(%284, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%280, %285)
      -> ()
    block1():
      -> ()
  %286 : bool = prim::GetAttr[name="training"](%280)
  %287 : Tensor = prim::GetAttr[name="running_mean"](%280)
  %288 : Tensor = prim::GetAttr[name="running_var"](%280)
  %289 : Tensor = prim::GetAttr[name="weight"](%280)
  %290 : Tensor = prim::GetAttr[name="bias"](%280)
   = prim::If(%286) # torch/nn/functional.py:2011:4
    block0():
      %291 : int[] = aten::size(%out.334) # torch/nn/functional.py:2012:27
      %size_prods.528 : int = aten::__getitem__(%291, %8) # torch/nn/functional.py:1991:17
      %293 : int = aten::len(%291) # torch/nn/functional.py:1992:19
      %294 : int = aten::sub(%293, %10) # torch/nn/functional.py:1992:19
      %size_prods.529 : int = prim::Loop(%294, %9, %size_prods.528) # torch/nn/functional.py:1992:4
        block0(%i.133 : int, %size_prods.530 : int):
          %298 : int = aten::add(%i.133, %10) # torch/nn/functional.py:1993:27
          %299 : int = aten::__getitem__(%291, %298) # torch/nn/functional.py:1993:22
          %size_prods.531 : int = aten::mul(%size_prods.530, %299) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.531)
      %301 : bool = aten::eq(%size_prods.529, %12) # torch/nn/functional.py:1994:7
       = prim::If(%301) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.362 : Tensor = aten::batch_norm(%out.334, %289, %290, %287, %288, %286, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.363 : Tensor = aten::relu_(%out.362) # torch/nn/functional.py:1117:17
  %304 : __torch__.torch.nn.modules.conv.___torch_mangle_10.Conv2d = prim::GetAttr[name="conv2"](%53)
  %305 : Tensor = prim::GetAttr[name="weight"](%304)
  %306 : Tensor? = prim::GetAttr[name="bias"](%304)
  %307 : int[] = prim::ListConstruct(%12, %12)
  %308 : int[] = prim::ListConstruct(%12, %12)
  %309 : int[] = prim::ListConstruct(%12, %12)
  %out.364 : Tensor = aten::conv2d(%out.363, %305, %306, %307, %308, %309, %12) # torch/nn/modules/conv.py:415:15
  %311 : __torch__.torch.nn.modules.batchnorm.BatchNorm2d = prim::GetAttr[name="bn2"](%53)
  %312 : int = aten::dim(%out.364) # torch/nn/modules/batchnorm.py:276:11
  %313 : bool = aten::ne(%312, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%313) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %314 : bool = prim::GetAttr[name="training"](%311)
   = prim::If(%314) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%311)
      %316 : Tensor = aten::add(%315, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%311, %316)
      -> ()
    block1():
      -> ()
  %317 : bool = prim::GetAttr[name="training"](%311)
  %318 : Tensor = prim::GetAttr[name="running_mean"](%311)
  %319 : Tensor = prim::GetAttr[name="running_var"](%311)
  %320 : Tensor = prim::GetAttr[name="weight"](%311)
  %321 : Tensor = prim::GetAttr[name="bias"](%311)
   = prim::If(%317) # torch/nn/functional.py:2011:4
    block0():
      %322 : int[] = aten::size(%out.364) # torch/nn/functional.py:2012:27
      %size_prods.532 : int = aten::__getitem__(%322, %8) # torch/nn/functional.py:1991:17
      %324 : int = aten::len(%322) # torch/nn/functional.py:1992:19
      %325 : int = aten::sub(%324, %10) # torch/nn/functional.py:1992:19
      %size_prods.533 : int = prim::Loop(%325, %9, %size_prods.532) # torch/nn/functional.py:1992:4
        block0(%i.134 : int, %size_prods.534 : int):
          %329 : int = aten::add(%i.134, %10) # torch/nn/functional.py:1993:27
          %330 : int = aten::__getitem__(%322, %329) # torch/nn/functional.py:1993:22
          %size_prods.535 : int = aten::mul(%size_prods.534, %330) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.535)
      %332 : bool = aten::eq(%size_prods.533, %12) # torch/nn/functional.py:1994:7
       = prim::If(%332) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.365 : Tensor = aten::batch_norm(%out.364, %320, %321, %318, %319, %317, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.366 : Tensor = aten::relu_(%out.365) # torch/nn/functional.py:1117:17
  %335 : __torch__.torch.nn.modules.conv.___torch_mangle_11.Conv2d = prim::GetAttr[name="conv3"](%53)
  %336 : Tensor = prim::GetAttr[name="weight"](%335)
  %337 : Tensor? = prim::GetAttr[name="bias"](%335)
  %338 : int[] = prim::ListConstruct(%12, %12)
  %339 : int[] = prim::ListConstruct(%8, %8)
  %340 : int[] = prim::ListConstruct(%12, %12)
  %out.367 : Tensor = aten::conv2d(%out.366, %336, %337, %338, %339, %340, %12) # torch/nn/modules/conv.py:415:15
  %342 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn3"](%53)
  %343 : int = aten::dim(%out.367) # torch/nn/modules/batchnorm.py:276:11
  %344 : bool = aten::ne(%343, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%344) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %345 : bool = prim::GetAttr[name="training"](%342)
   = prim::If(%345) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %346 : Tensor = prim::GetAttr[name="num_batches_tracked"](%342)
      %347 : Tensor = aten::add(%346, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%342, %347)
      -> ()
    block1():
      -> ()
  %348 : bool = prim::GetAttr[name="training"](%342)
  %349 : Tensor = prim::GetAttr[name="running_mean"](%342)
  %350 : Tensor = prim::GetAttr[name="running_var"](%342)
  %351 : Tensor = prim::GetAttr[name="weight"](%342)
  %352 : Tensor = prim::GetAttr[name="bias"](%342)
   = prim::If(%348) # torch/nn/functional.py:2011:4
    block0():
      %353 : int[] = aten::size(%out.367) # torch/nn/functional.py:2012:27
      %size_prods.536 : int = aten::__getitem__(%353, %8) # torch/nn/functional.py:1991:17
      %355 : int = aten::len(%353) # torch/nn/functional.py:1992:19
      %356 : int = aten::sub(%355, %10) # torch/nn/functional.py:1992:19
      %size_prods.537 : int = prim::Loop(%356, %9, %size_prods.536) # torch/nn/functional.py:1992:4
        block0(%i.135 : int, %size_prods.538 : int):
          %360 : int = aten::add(%i.135, %10) # torch/nn/functional.py:1993:27
          %361 : int = aten::__getitem__(%353, %360) # torch/nn/functional.py:1993:22
          %size_prods.539 : int = aten::mul(%size_prods.538, %361) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.539)
      %363 : bool = aten::eq(%size_prods.537, %12) # torch/nn/functional.py:1994:7
       = prim::If(%363) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.368 : Tensor = aten::batch_norm(%out.367, %351, %352, %349, %350, %348, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.369 : Tensor = aten::add_(%out.368, %input.8, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.11 : Tensor = aten::relu_(%out.369) # torch/nn/functional.py:1117:17
  %367 : __torch__.torch.nn.modules.container.___torch_mangle_951.Sequential = prim::GetAttr[name="layer2"](%self)
  %368 : __torch__.torchvision.models.resnet.___torch_mangle_24.Bottleneck = prim::GetAttr[name="0"](%367)
  %369 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="1"](%367)
  %370 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="2"](%367)
  %371 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="3"](%367)
  %372 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="4"](%367)
  %373 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="5"](%367)
  %374 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="6"](%367)
  %375 : __torch__.torchvision.models.resnet.___torch_mangle_27.Bottleneck = prim::GetAttr[name="7"](%367)
  %376 : __torch__.torch.nn.modules.conv.___torch_mangle_17.Conv2d = prim::GetAttr[name="conv1"](%368)
  %377 : Tensor = prim::GetAttr[name="weight"](%376)
  %378 : Tensor? = prim::GetAttr[name="bias"](%376)
  %379 : int[] = prim::ListConstruct(%12, %12)
  %380 : int[] = prim::ListConstruct(%8, %8)
  %381 : int[] = prim::ListConstruct(%12, %12)
  %out.343 : Tensor = aten::conv2d(%x.11, %377, %378, %379, %380, %381, %12) # torch/nn/modules/conv.py:415:15
  %383 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%368)
  %384 : int = aten::dim(%out.343) # torch/nn/modules/batchnorm.py:276:11
  %385 : bool = aten::ne(%384, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%385) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %386 : bool = prim::GetAttr[name="training"](%383)
   = prim::If(%386) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %387 : Tensor = prim::GetAttr[name="num_batches_tracked"](%383)
      %388 : Tensor = aten::add(%387, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%383, %388)
      -> ()
    block1():
      -> ()
  %389 : bool = prim::GetAttr[name="training"](%383)
  %390 : Tensor = prim::GetAttr[name="running_mean"](%383)
  %391 : Tensor = prim::GetAttr[name="running_var"](%383)
  %392 : Tensor = prim::GetAttr[name="weight"](%383)
  %393 : Tensor = prim::GetAttr[name="bias"](%383)
   = prim::If(%389) # torch/nn/functional.py:2011:4
    block0():
      %394 : int[] = aten::size(%out.343) # torch/nn/functional.py:2012:27
      %size_prods.540 : int = aten::__getitem__(%394, %8) # torch/nn/functional.py:1991:17
      %396 : int = aten::len(%394) # torch/nn/functional.py:1992:19
      %397 : int = aten::sub(%396, %10) # torch/nn/functional.py:1992:19
      %size_prods.541 : int = prim::Loop(%397, %9, %size_prods.540) # torch/nn/functional.py:1992:4
        block0(%i.136 : int, %size_prods.542 : int):
          %401 : int = aten::add(%i.136, %10) # torch/nn/functional.py:1993:27
          %402 : int = aten::__getitem__(%394, %401) # torch/nn/functional.py:1993:22
          %size_prods.543 : int = aten::mul(%size_prods.542, %402) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.543)
      %404 : bool = aten::eq(%size_prods.541, %12) # torch/nn/functional.py:1994:7
       = prim::If(%404) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.371 : Tensor = aten::batch_norm(%out.343, %392, %393, %390, %391, %389, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.372 : Tensor = aten::relu_(%out.371) # torch/nn/functional.py:1117:17
  %407 : __torch__.torch.nn.modules.conv.___torch_mangle_19.Conv2d = prim::GetAttr[name="conv2"](%368)
  %408 : Tensor = prim::GetAttr[name="weight"](%407)
  %409 : Tensor? = prim::GetAttr[name="bias"](%407)
  %410 : int[] = prim::ListConstruct(%10, %10)
  %411 : int[] = prim::ListConstruct(%12, %12)
  %412 : int[] = prim::ListConstruct(%12, %12)
  %out.373 : Tensor = aten::conv2d(%out.372, %408, %409, %410, %411, %412, %12) # torch/nn/modules/conv.py:415:15
  %414 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%368)
  %415 : int = aten::dim(%out.373) # torch/nn/modules/batchnorm.py:276:11
  %416 : bool = aten::ne(%415, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%416) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %417 : bool = prim::GetAttr[name="training"](%414)
   = prim::If(%417) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %418 : Tensor = prim::GetAttr[name="num_batches_tracked"](%414)
      %419 : Tensor = aten::add(%418, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%414, %419)
      -> ()
    block1():
      -> ()
  %420 : bool = prim::GetAttr[name="training"](%414)
  %421 : Tensor = prim::GetAttr[name="running_mean"](%414)
  %422 : Tensor = prim::GetAttr[name="running_var"](%414)
  %423 : Tensor = prim::GetAttr[name="weight"](%414)
  %424 : Tensor = prim::GetAttr[name="bias"](%414)
   = prim::If(%420) # torch/nn/functional.py:2011:4
    block0():
      %425 : int[] = aten::size(%out.373) # torch/nn/functional.py:2012:27
      %size_prods.544 : int = aten::__getitem__(%425, %8) # torch/nn/functional.py:1991:17
      %427 : int = aten::len(%425) # torch/nn/functional.py:1992:19
      %428 : int = aten::sub(%427, %10) # torch/nn/functional.py:1992:19
      %size_prods.545 : int = prim::Loop(%428, %9, %size_prods.544) # torch/nn/functional.py:1992:4
        block0(%i.137 : int, %size_prods.546 : int):
          %432 : int = aten::add(%i.137, %10) # torch/nn/functional.py:1993:27
          %433 : int = aten::__getitem__(%425, %432) # torch/nn/functional.py:1993:22
          %size_prods.547 : int = aten::mul(%size_prods.546, %433) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.547)
      %435 : bool = aten::eq(%size_prods.545, %12) # torch/nn/functional.py:1994:7
       = prim::If(%435) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.374 : Tensor = aten::batch_norm(%out.373, %423, %424, %421, %422, %420, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.375 : Tensor = aten::relu_(%out.374) # torch/nn/functional.py:1117:17
  %438 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%368)
  %439 : Tensor = prim::GetAttr[name="weight"](%438)
  %440 : Tensor? = prim::GetAttr[name="bias"](%438)
  %441 : int[] = prim::ListConstruct(%12, %12)
  %442 : int[] = prim::ListConstruct(%8, %8)
  %443 : int[] = prim::ListConstruct(%12, %12)
  %out.376 : Tensor = aten::conv2d(%out.375, %439, %440, %441, %442, %443, %12) # torch/nn/modules/conv.py:415:15
  %445 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%368)
  %446 : int = aten::dim(%out.376) # torch/nn/modules/batchnorm.py:276:11
  %447 : bool = aten::ne(%446, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%447) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %448 : bool = prim::GetAttr[name="training"](%445)
   = prim::If(%448) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %449 : Tensor = prim::GetAttr[name="num_batches_tracked"](%445)
      %450 : Tensor = aten::add(%449, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%445, %450)
      -> ()
    block1():
      -> ()
  %451 : bool = prim::GetAttr[name="training"](%445)
  %452 : Tensor = prim::GetAttr[name="running_mean"](%445)
  %453 : Tensor = prim::GetAttr[name="running_var"](%445)
  %454 : Tensor = prim::GetAttr[name="weight"](%445)
  %455 : Tensor = prim::GetAttr[name="bias"](%445)
   = prim::If(%451) # torch/nn/functional.py:2011:4
    block0():
      %456 : int[] = aten::size(%out.376) # torch/nn/functional.py:2012:27
      %size_prods.548 : int = aten::__getitem__(%456, %8) # torch/nn/functional.py:1991:17
      %458 : int = aten::len(%456) # torch/nn/functional.py:1992:19
      %459 : int = aten::sub(%458, %10) # torch/nn/functional.py:1992:19
      %size_prods.549 : int = prim::Loop(%459, %9, %size_prods.548) # torch/nn/functional.py:1992:4
        block0(%i.138 : int, %size_prods.550 : int):
          %463 : int = aten::add(%i.138, %10) # torch/nn/functional.py:1993:27
          %464 : int = aten::__getitem__(%456, %463) # torch/nn/functional.py:1993:22
          %size_prods.551 : int = aten::mul(%size_prods.550, %464) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.551)
      %466 : bool = aten::eq(%size_prods.549, %12) # torch/nn/functional.py:1994:7
       = prim::If(%466) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.377 : Tensor = aten::batch_norm(%out.376, %454, %455, %452, %453, %451, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %468 : __torch__.torch.nn.modules.container.___torch_mangle_23.Sequential = prim::GetAttr[name="downsample"](%368)
  %469 : __torch__.torch.nn.modules.conv.___torch_mangle_22.Conv2d = prim::GetAttr[name="0"](%468)
  %470 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="1"](%468)
  %471 : Tensor = prim::GetAttr[name="weight"](%469)
  %472 : Tensor? = prim::GetAttr[name="bias"](%469)
  %473 : int[] = prim::ListConstruct(%10, %10)
  %474 : int[] = prim::ListConstruct(%8, %8)
  %475 : int[] = prim::ListConstruct(%12, %12)
  %input.12 : Tensor = aten::conv2d(%x.11, %471, %472, %473, %474, %475, %12) # torch/nn/modules/conv.py:415:15
  %477 : int = aten::dim(%input.12) # torch/nn/modules/batchnorm.py:276:11
  %478 : bool = aten::ne(%477, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%478) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %479 : bool = prim::GetAttr[name="training"](%470)
   = prim::If(%479) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %480 : Tensor = prim::GetAttr[name="num_batches_tracked"](%470)
      %481 : Tensor = aten::add(%480, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%470, %481)
      -> ()
    block1():
      -> ()
  %482 : bool = prim::GetAttr[name="training"](%470)
  %483 : Tensor = prim::GetAttr[name="running_mean"](%470)
  %484 : Tensor = prim::GetAttr[name="running_var"](%470)
  %485 : Tensor = prim::GetAttr[name="weight"](%470)
  %486 : Tensor = prim::GetAttr[name="bias"](%470)
   = prim::If(%482) # torch/nn/functional.py:2011:4
    block0():
      %487 : int[] = aten::size(%input.12) # torch/nn/functional.py:2012:27
      %size_prods.552 : int = aten::__getitem__(%487, %8) # torch/nn/functional.py:1991:17
      %489 : int = aten::len(%487) # torch/nn/functional.py:1992:19
      %490 : int = aten::sub(%489, %10) # torch/nn/functional.py:1992:19
      %size_prods.553 : int = prim::Loop(%490, %9, %size_prods.552) # torch/nn/functional.py:1992:4
        block0(%i.139 : int, %size_prods.554 : int):
          %494 : int = aten::add(%i.139, %10) # torch/nn/functional.py:1993:27
          %495 : int = aten::__getitem__(%487, %494) # torch/nn/functional.py:1993:22
          %size_prods.555 : int = aten::mul(%size_prods.554, %495) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.555)
      %497 : bool = aten::eq(%size_prods.553, %12) # torch/nn/functional.py:1994:7
       = prim::If(%497) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.3 : Tensor = aten::batch_norm(%input.12, %485, %486, %483, %484, %482, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.378 : Tensor = aten::add_(%out.377, %identity.3, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.14 : Tensor = aten::relu_(%out.378) # torch/nn/functional.py:1117:17
  %501 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%369)
  %502 : Tensor = prim::GetAttr[name="weight"](%501)
  %503 : Tensor? = prim::GetAttr[name="bias"](%501)
  %504 : int[] = prim::ListConstruct(%12, %12)
  %505 : int[] = prim::ListConstruct(%8, %8)
  %506 : int[] = prim::ListConstruct(%12, %12)
  %out.352 : Tensor = aten::conv2d(%input.14, %502, %503, %504, %505, %506, %12) # torch/nn/modules/conv.py:415:15
  %508 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%369)
  %509 : int = aten::dim(%out.352) # torch/nn/modules/batchnorm.py:276:11
  %510 : bool = aten::ne(%509, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%510) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %511 : bool = prim::GetAttr[name="training"](%508)
   = prim::If(%511) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %512 : Tensor = prim::GetAttr[name="num_batches_tracked"](%508)
      %513 : Tensor = aten::add(%512, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%508, %513)
      -> ()
    block1():
      -> ()
  %514 : bool = prim::GetAttr[name="training"](%508)
  %515 : Tensor = prim::GetAttr[name="running_mean"](%508)
  %516 : Tensor = prim::GetAttr[name="running_var"](%508)
  %517 : Tensor = prim::GetAttr[name="weight"](%508)
  %518 : Tensor = prim::GetAttr[name="bias"](%508)
   = prim::If(%514) # torch/nn/functional.py:2011:4
    block0():
      %519 : int[] = aten::size(%out.352) # torch/nn/functional.py:2012:27
      %size_prods.556 : int = aten::__getitem__(%519, %8) # torch/nn/functional.py:1991:17
      %521 : int = aten::len(%519) # torch/nn/functional.py:1992:19
      %522 : int = aten::sub(%521, %10) # torch/nn/functional.py:1992:19
      %size_prods.557 : int = prim::Loop(%522, %9, %size_prods.556) # torch/nn/functional.py:1992:4
        block0(%i.140 : int, %size_prods.558 : int):
          %526 : int = aten::add(%i.140, %10) # torch/nn/functional.py:1993:27
          %527 : int = aten::__getitem__(%519, %526) # torch/nn/functional.py:1993:22
          %size_prods.559 : int = aten::mul(%size_prods.558, %527) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.559)
      %529 : bool = aten::eq(%size_prods.557, %12) # torch/nn/functional.py:1994:7
       = prim::If(%529) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.380 : Tensor = aten::batch_norm(%out.352, %517, %518, %515, %516, %514, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.381 : Tensor = aten::relu_(%out.380) # torch/nn/functional.py:1117:17
  %532 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%369)
  %533 : Tensor = prim::GetAttr[name="weight"](%532)
  %534 : Tensor? = prim::GetAttr[name="bias"](%532)
  %535 : int[] = prim::ListConstruct(%12, %12)
  %536 : int[] = prim::ListConstruct(%12, %12)
  %537 : int[] = prim::ListConstruct(%12, %12)
  %out.382 : Tensor = aten::conv2d(%out.381, %533, %534, %535, %536, %537, %12) # torch/nn/modules/conv.py:415:15
  %539 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%369)
  %540 : int = aten::dim(%out.382) # torch/nn/modules/batchnorm.py:276:11
  %541 : bool = aten::ne(%540, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%541) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %542 : bool = prim::GetAttr[name="training"](%539)
   = prim::If(%542) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %543 : Tensor = prim::GetAttr[name="num_batches_tracked"](%539)
      %544 : Tensor = aten::add(%543, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%539, %544)
      -> ()
    block1():
      -> ()
  %545 : bool = prim::GetAttr[name="training"](%539)
  %546 : Tensor = prim::GetAttr[name="running_mean"](%539)
  %547 : Tensor = prim::GetAttr[name="running_var"](%539)
  %548 : Tensor = prim::GetAttr[name="weight"](%539)
  %549 : Tensor = prim::GetAttr[name="bias"](%539)
   = prim::If(%545) # torch/nn/functional.py:2011:4
    block0():
      %550 : int[] = aten::size(%out.382) # torch/nn/functional.py:2012:27
      %size_prods.560 : int = aten::__getitem__(%550, %8) # torch/nn/functional.py:1991:17
      %552 : int = aten::len(%550) # torch/nn/functional.py:1992:19
      %553 : int = aten::sub(%552, %10) # torch/nn/functional.py:1992:19
      %size_prods.561 : int = prim::Loop(%553, %9, %size_prods.560) # torch/nn/functional.py:1992:4
        block0(%i.141 : int, %size_prods.562 : int):
          %557 : int = aten::add(%i.141, %10) # torch/nn/functional.py:1993:27
          %558 : int = aten::__getitem__(%550, %557) # torch/nn/functional.py:1993:22
          %size_prods.563 : int = aten::mul(%size_prods.562, %558) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.563)
      %560 : bool = aten::eq(%size_prods.561, %12) # torch/nn/functional.py:1994:7
       = prim::If(%560) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.383 : Tensor = aten::batch_norm(%out.382, %548, %549, %546, %547, %545, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.384 : Tensor = aten::relu_(%out.383) # torch/nn/functional.py:1117:17
  %563 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%369)
  %564 : Tensor = prim::GetAttr[name="weight"](%563)
  %565 : Tensor? = prim::GetAttr[name="bias"](%563)
  %566 : int[] = prim::ListConstruct(%12, %12)
  %567 : int[] = prim::ListConstruct(%8, %8)
  %568 : int[] = prim::ListConstruct(%12, %12)
  %out.385 : Tensor = aten::conv2d(%out.384, %564, %565, %566, %567, %568, %12) # torch/nn/modules/conv.py:415:15
  %570 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%369)
  %571 : int = aten::dim(%out.385) # torch/nn/modules/batchnorm.py:276:11
  %572 : bool = aten::ne(%571, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%572) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %573 : bool = prim::GetAttr[name="training"](%570)
   = prim::If(%573) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %574 : Tensor = prim::GetAttr[name="num_batches_tracked"](%570)
      %575 : Tensor = aten::add(%574, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%570, %575)
      -> ()
    block1():
      -> ()
  %576 : bool = prim::GetAttr[name="training"](%570)
  %577 : Tensor = prim::GetAttr[name="running_mean"](%570)
  %578 : Tensor = prim::GetAttr[name="running_var"](%570)
  %579 : Tensor = prim::GetAttr[name="weight"](%570)
  %580 : Tensor = prim::GetAttr[name="bias"](%570)
   = prim::If(%576) # torch/nn/functional.py:2011:4
    block0():
      %581 : int[] = aten::size(%out.385) # torch/nn/functional.py:2012:27
      %size_prods.564 : int = aten::__getitem__(%581, %8) # torch/nn/functional.py:1991:17
      %583 : int = aten::len(%581) # torch/nn/functional.py:1992:19
      %584 : int = aten::sub(%583, %10) # torch/nn/functional.py:1992:19
      %size_prods.565 : int = prim::Loop(%584, %9, %size_prods.564) # torch/nn/functional.py:1992:4
        block0(%i.142 : int, %size_prods.566 : int):
          %588 : int = aten::add(%i.142, %10) # torch/nn/functional.py:1993:27
          %589 : int = aten::__getitem__(%581, %588) # torch/nn/functional.py:1993:22
          %size_prods.567 : int = aten::mul(%size_prods.566, %589) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.567)
      %591 : bool = aten::eq(%size_prods.565, %12) # torch/nn/functional.py:1994:7
       = prim::If(%591) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.386 : Tensor = aten::batch_norm(%out.385, %579, %580, %577, %578, %576, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.387 : Tensor = aten::add_(%out.386, %input.14, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.16 : Tensor = aten::relu_(%out.387) # torch/nn/functional.py:1117:17
  %595 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%370)
  %596 : Tensor = prim::GetAttr[name="weight"](%595)
  %597 : Tensor? = prim::GetAttr[name="bias"](%595)
  %598 : int[] = prim::ListConstruct(%12, %12)
  %599 : int[] = prim::ListConstruct(%8, %8)
  %600 : int[] = prim::ListConstruct(%12, %12)
  %out.361 : Tensor = aten::conv2d(%input.16, %596, %597, %598, %599, %600, %12) # torch/nn/modules/conv.py:415:15
  %602 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%370)
  %603 : int = aten::dim(%out.361) # torch/nn/modules/batchnorm.py:276:11
  %604 : bool = aten::ne(%603, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%604) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %605 : bool = prim::GetAttr[name="training"](%602)
   = prim::If(%605) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %606 : Tensor = prim::GetAttr[name="num_batches_tracked"](%602)
      %607 : Tensor = aten::add(%606, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%602, %607)
      -> ()
    block1():
      -> ()
  %608 : bool = prim::GetAttr[name="training"](%602)
  %609 : Tensor = prim::GetAttr[name="running_mean"](%602)
  %610 : Tensor = prim::GetAttr[name="running_var"](%602)
  %611 : Tensor = prim::GetAttr[name="weight"](%602)
  %612 : Tensor = prim::GetAttr[name="bias"](%602)
   = prim::If(%608) # torch/nn/functional.py:2011:4
    block0():
      %613 : int[] = aten::size(%out.361) # torch/nn/functional.py:2012:27
      %size_prods.436 : int = aten::__getitem__(%613, %8) # torch/nn/functional.py:1991:17
      %615 : int = aten::len(%613) # torch/nn/functional.py:1992:19
      %616 : int = aten::sub(%615, %10) # torch/nn/functional.py:1992:19
      %size_prods.437 : int = prim::Loop(%616, %9, %size_prods.436) # torch/nn/functional.py:1992:4
        block0(%i.110 : int, %size_prods.438 : int):
          %620 : int = aten::add(%i.110, %10) # torch/nn/functional.py:1993:27
          %621 : int = aten::__getitem__(%613, %620) # torch/nn/functional.py:1993:22
          %size_prods.439 : int = aten::mul(%size_prods.438, %621) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.439)
      %623 : bool = aten::eq(%size_prods.437, %12) # torch/nn/functional.py:1994:7
       = prim::If(%623) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.389 : Tensor = aten::batch_norm(%out.361, %611, %612, %609, %610, %608, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.390 : Tensor = aten::relu_(%out.389) # torch/nn/functional.py:1117:17
  %626 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%370)
  %627 : Tensor = prim::GetAttr[name="weight"](%626)
  %628 : Tensor? = prim::GetAttr[name="bias"](%626)
  %629 : int[] = prim::ListConstruct(%12, %12)
  %630 : int[] = prim::ListConstruct(%12, %12)
  %631 : int[] = prim::ListConstruct(%12, %12)
  %out.391 : Tensor = aten::conv2d(%out.390, %627, %628, %629, %630, %631, %12) # torch/nn/modules/conv.py:415:15
  %633 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%370)
  %634 : int = aten::dim(%out.391) # torch/nn/modules/batchnorm.py:276:11
  %635 : bool = aten::ne(%634, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%635) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %636 : bool = prim::GetAttr[name="training"](%633)
   = prim::If(%636) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %637 : Tensor = prim::GetAttr[name="num_batches_tracked"](%633)
      %638 : Tensor = aten::add(%637, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%633, %638)
      -> ()
    block1():
      -> ()
  %639 : bool = prim::GetAttr[name="training"](%633)
  %640 : Tensor = prim::GetAttr[name="running_mean"](%633)
  %641 : Tensor = prim::GetAttr[name="running_var"](%633)
  %642 : Tensor = prim::GetAttr[name="weight"](%633)
  %643 : Tensor = prim::GetAttr[name="bias"](%633)
   = prim::If(%639) # torch/nn/functional.py:2011:4
    block0():
      %644 : int[] = aten::size(%out.391) # torch/nn/functional.py:2012:27
      %size_prods.440 : int = aten::__getitem__(%644, %8) # torch/nn/functional.py:1991:17
      %646 : int = aten::len(%644) # torch/nn/functional.py:1992:19
      %647 : int = aten::sub(%646, %10) # torch/nn/functional.py:1992:19
      %size_prods.441 : int = prim::Loop(%647, %9, %size_prods.440) # torch/nn/functional.py:1992:4
        block0(%i.111 : int, %size_prods.442 : int):
          %651 : int = aten::add(%i.111, %10) # torch/nn/functional.py:1993:27
          %652 : int = aten::__getitem__(%644, %651) # torch/nn/functional.py:1993:22
          %size_prods.443 : int = aten::mul(%size_prods.442, %652) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.443)
      %654 : bool = aten::eq(%size_prods.441, %12) # torch/nn/functional.py:1994:7
       = prim::If(%654) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.392 : Tensor = aten::batch_norm(%out.391, %642, %643, %640, %641, %639, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.393 : Tensor = aten::relu_(%out.392) # torch/nn/functional.py:1117:17
  %657 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%370)
  %658 : Tensor = prim::GetAttr[name="weight"](%657)
  %659 : Tensor? = prim::GetAttr[name="bias"](%657)
  %660 : int[] = prim::ListConstruct(%12, %12)
  %661 : int[] = prim::ListConstruct(%8, %8)
  %662 : int[] = prim::ListConstruct(%12, %12)
  %out.394 : Tensor = aten::conv2d(%out.393, %658, %659, %660, %661, %662, %12) # torch/nn/modules/conv.py:415:15
  %664 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%370)
  %665 : int = aten::dim(%out.394) # torch/nn/modules/batchnorm.py:276:11
  %666 : bool = aten::ne(%665, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%666) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %667 : bool = prim::GetAttr[name="training"](%664)
   = prim::If(%667) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %668 : Tensor = prim::GetAttr[name="num_batches_tracked"](%664)
      %669 : Tensor = aten::add(%668, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%664, %669)
      -> ()
    block1():
      -> ()
  %670 : bool = prim::GetAttr[name="training"](%664)
  %671 : Tensor = prim::GetAttr[name="running_mean"](%664)
  %672 : Tensor = prim::GetAttr[name="running_var"](%664)
  %673 : Tensor = prim::GetAttr[name="weight"](%664)
  %674 : Tensor = prim::GetAttr[name="bias"](%664)
   = prim::If(%670) # torch/nn/functional.py:2011:4
    block0():
      %675 : int[] = aten::size(%out.394) # torch/nn/functional.py:2012:27
      %size_prods.444 : int = aten::__getitem__(%675, %8) # torch/nn/functional.py:1991:17
      %677 : int = aten::len(%675) # torch/nn/functional.py:1992:19
      %678 : int = aten::sub(%677, %10) # torch/nn/functional.py:1992:19
      %size_prods.445 : int = prim::Loop(%678, %9, %size_prods.444) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.446 : int):
          %682 : int = aten::add(%i.112, %10) # torch/nn/functional.py:1993:27
          %683 : int = aten::__getitem__(%675, %682) # torch/nn/functional.py:1993:22
          %size_prods.447 : int = aten::mul(%size_prods.446, %683) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.447)
      %685 : bool = aten::eq(%size_prods.445, %12) # torch/nn/functional.py:1994:7
       = prim::If(%685) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.395 : Tensor = aten::batch_norm(%out.394, %673, %674, %671, %672, %670, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.396 : Tensor = aten::add_(%out.395, %input.16, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.22 : Tensor = aten::relu_(%out.396) # torch/nn/functional.py:1117:17
  %689 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%371)
  %690 : Tensor = prim::GetAttr[name="weight"](%689)
  %691 : Tensor? = prim::GetAttr[name="bias"](%689)
  %692 : int[] = prim::ListConstruct(%12, %12)
  %693 : int[] = prim::ListConstruct(%8, %8)
  %694 : int[] = prim::ListConstruct(%12, %12)
  %out.370 : Tensor = aten::conv2d(%input.22, %690, %691, %692, %693, %694, %12) # torch/nn/modules/conv.py:415:15
  %696 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%371)
  %697 : int = aten::dim(%out.370) # torch/nn/modules/batchnorm.py:276:11
  %698 : bool = aten::ne(%697, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%698) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %699 : bool = prim::GetAttr[name="training"](%696)
   = prim::If(%699) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %700 : Tensor = prim::GetAttr[name="num_batches_tracked"](%696)
      %701 : Tensor = aten::add(%700, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%696, %701)
      -> ()
    block1():
      -> ()
  %702 : bool = prim::GetAttr[name="training"](%696)
  %703 : Tensor = prim::GetAttr[name="running_mean"](%696)
  %704 : Tensor = prim::GetAttr[name="running_var"](%696)
  %705 : Tensor = prim::GetAttr[name="weight"](%696)
  %706 : Tensor = prim::GetAttr[name="bias"](%696)
   = prim::If(%702) # torch/nn/functional.py:2011:4
    block0():
      %707 : int[] = aten::size(%out.370) # torch/nn/functional.py:2012:27
      %size_prods.448 : int = aten::__getitem__(%707, %8) # torch/nn/functional.py:1991:17
      %709 : int = aten::len(%707) # torch/nn/functional.py:1992:19
      %710 : int = aten::sub(%709, %10) # torch/nn/functional.py:1992:19
      %size_prods.449 : int = prim::Loop(%710, %9, %size_prods.448) # torch/nn/functional.py:1992:4
        block0(%i.113 : int, %size_prods.450 : int):
          %714 : int = aten::add(%i.113, %10) # torch/nn/functional.py:1993:27
          %715 : int = aten::__getitem__(%707, %714) # torch/nn/functional.py:1993:22
          %size_prods.451 : int = aten::mul(%size_prods.450, %715) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.451)
      %717 : bool = aten::eq(%size_prods.449, %12) # torch/nn/functional.py:1994:7
       = prim::If(%717) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.398 : Tensor = aten::batch_norm(%out.370, %705, %706, %703, %704, %702, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.399 : Tensor = aten::relu_(%out.398) # torch/nn/functional.py:1117:17
  %720 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%371)
  %721 : Tensor = prim::GetAttr[name="weight"](%720)
  %722 : Tensor? = prim::GetAttr[name="bias"](%720)
  %723 : int[] = prim::ListConstruct(%12, %12)
  %724 : int[] = prim::ListConstruct(%12, %12)
  %725 : int[] = prim::ListConstruct(%12, %12)
  %out.400 : Tensor = aten::conv2d(%out.399, %721, %722, %723, %724, %725, %12) # torch/nn/modules/conv.py:415:15
  %727 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%371)
  %728 : int = aten::dim(%out.400) # torch/nn/modules/batchnorm.py:276:11
  %729 : bool = aten::ne(%728, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%729) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %730 : bool = prim::GetAttr[name="training"](%727)
   = prim::If(%730) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %731 : Tensor = prim::GetAttr[name="num_batches_tracked"](%727)
      %732 : Tensor = aten::add(%731, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%727, %732)
      -> ()
    block1():
      -> ()
  %733 : bool = prim::GetAttr[name="training"](%727)
  %734 : Tensor = prim::GetAttr[name="running_mean"](%727)
  %735 : Tensor = prim::GetAttr[name="running_var"](%727)
  %736 : Tensor = prim::GetAttr[name="weight"](%727)
  %737 : Tensor = prim::GetAttr[name="bias"](%727)
   = prim::If(%733) # torch/nn/functional.py:2011:4
    block0():
      %738 : int[] = aten::size(%out.400) # torch/nn/functional.py:2012:27
      %size_prods.452 : int = aten::__getitem__(%738, %8) # torch/nn/functional.py:1991:17
      %740 : int = aten::len(%738) # torch/nn/functional.py:1992:19
      %741 : int = aten::sub(%740, %10) # torch/nn/functional.py:1992:19
      %size_prods.453 : int = prim::Loop(%741, %9, %size_prods.452) # torch/nn/functional.py:1992:4
        block0(%i.114 : int, %size_prods.454 : int):
          %745 : int = aten::add(%i.114, %10) # torch/nn/functional.py:1993:27
          %746 : int = aten::__getitem__(%738, %745) # torch/nn/functional.py:1993:22
          %size_prods.455 : int = aten::mul(%size_prods.454, %746) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.455)
      %748 : bool = aten::eq(%size_prods.453, %12) # torch/nn/functional.py:1994:7
       = prim::If(%748) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.401 : Tensor = aten::batch_norm(%out.400, %736, %737, %734, %735, %733, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.402 : Tensor = aten::relu_(%out.401) # torch/nn/functional.py:1117:17
  %751 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%371)
  %752 : Tensor = prim::GetAttr[name="weight"](%751)
  %753 : Tensor? = prim::GetAttr[name="bias"](%751)
  %754 : int[] = prim::ListConstruct(%12, %12)
  %755 : int[] = prim::ListConstruct(%8, %8)
  %756 : int[] = prim::ListConstruct(%12, %12)
  %out.403 : Tensor = aten::conv2d(%out.402, %752, %753, %754, %755, %756, %12) # torch/nn/modules/conv.py:415:15
  %758 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%371)
  %759 : int = aten::dim(%out.403) # torch/nn/modules/batchnorm.py:276:11
  %760 : bool = aten::ne(%759, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%760) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %761 : bool = prim::GetAttr[name="training"](%758)
   = prim::If(%761) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %762 : Tensor = prim::GetAttr[name="num_batches_tracked"](%758)
      %763 : Tensor = aten::add(%762, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%758, %763)
      -> ()
    block1():
      -> ()
  %764 : bool = prim::GetAttr[name="training"](%758)
  %765 : Tensor = prim::GetAttr[name="running_mean"](%758)
  %766 : Tensor = prim::GetAttr[name="running_var"](%758)
  %767 : Tensor = prim::GetAttr[name="weight"](%758)
  %768 : Tensor = prim::GetAttr[name="bias"](%758)
   = prim::If(%764) # torch/nn/functional.py:2011:4
    block0():
      %769 : int[] = aten::size(%out.403) # torch/nn/functional.py:2012:27
      %size_prods.456 : int = aten::__getitem__(%769, %8) # torch/nn/functional.py:1991:17
      %771 : int = aten::len(%769) # torch/nn/functional.py:1992:19
      %772 : int = aten::sub(%771, %10) # torch/nn/functional.py:1992:19
      %size_prods.457 : int = prim::Loop(%772, %9, %size_prods.456) # torch/nn/functional.py:1992:4
        block0(%i.115 : int, %size_prods.458 : int):
          %776 : int = aten::add(%i.115, %10) # torch/nn/functional.py:1993:27
          %777 : int = aten::__getitem__(%769, %776) # torch/nn/functional.py:1993:22
          %size_prods.459 : int = aten::mul(%size_prods.458, %777) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.459)
      %779 : bool = aten::eq(%size_prods.457, %12) # torch/nn/functional.py:1994:7
       = prim::If(%779) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.404 : Tensor = aten::batch_norm(%out.403, %767, %768, %765, %766, %764, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.405 : Tensor = aten::add_(%out.404, %input.22, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.18 : Tensor = aten::relu_(%out.405) # torch/nn/functional.py:1117:17
  %783 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%372)
  %784 : Tensor = prim::GetAttr[name="weight"](%783)
  %785 : Tensor? = prim::GetAttr[name="bias"](%783)
  %786 : int[] = prim::ListConstruct(%12, %12)
  %787 : int[] = prim::ListConstruct(%8, %8)
  %788 : int[] = prim::ListConstruct(%12, %12)
  %out.379 : Tensor = aten::conv2d(%input.18, %784, %785, %786, %787, %788, %12) # torch/nn/modules/conv.py:415:15
  %790 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%372)
  %791 : int = aten::dim(%out.379) # torch/nn/modules/batchnorm.py:276:11
  %792 : bool = aten::ne(%791, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%792) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %793 : bool = prim::GetAttr[name="training"](%790)
   = prim::If(%793) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %794 : Tensor = prim::GetAttr[name="num_batches_tracked"](%790)
      %795 : Tensor = aten::add(%794, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%790, %795)
      -> ()
    block1():
      -> ()
  %796 : bool = prim::GetAttr[name="training"](%790)
  %797 : Tensor = prim::GetAttr[name="running_mean"](%790)
  %798 : Tensor = prim::GetAttr[name="running_var"](%790)
  %799 : Tensor = prim::GetAttr[name="weight"](%790)
  %800 : Tensor = prim::GetAttr[name="bias"](%790)
   = prim::If(%796) # torch/nn/functional.py:2011:4
    block0():
      %801 : int[] = aten::size(%out.379) # torch/nn/functional.py:2012:27
      %size_prods.460 : int = aten::__getitem__(%801, %8) # torch/nn/functional.py:1991:17
      %803 : int = aten::len(%801) # torch/nn/functional.py:1992:19
      %804 : int = aten::sub(%803, %10) # torch/nn/functional.py:1992:19
      %size_prods.461 : int = prim::Loop(%804, %9, %size_prods.460) # torch/nn/functional.py:1992:4
        block0(%i.116 : int, %size_prods.462 : int):
          %808 : int = aten::add(%i.116, %10) # torch/nn/functional.py:1993:27
          %809 : int = aten::__getitem__(%801, %808) # torch/nn/functional.py:1993:22
          %size_prods.463 : int = aten::mul(%size_prods.462, %809) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.463)
      %811 : bool = aten::eq(%size_prods.461, %12) # torch/nn/functional.py:1994:7
       = prim::If(%811) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.407 : Tensor = aten::batch_norm(%out.379, %799, %800, %797, %798, %796, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.408 : Tensor = aten::relu_(%out.407) # torch/nn/functional.py:1117:17
  %814 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%372)
  %815 : Tensor = prim::GetAttr[name="weight"](%814)
  %816 : Tensor? = prim::GetAttr[name="bias"](%814)
  %817 : int[] = prim::ListConstruct(%12, %12)
  %818 : int[] = prim::ListConstruct(%12, %12)
  %819 : int[] = prim::ListConstruct(%12, %12)
  %out.409 : Tensor = aten::conv2d(%out.408, %815, %816, %817, %818, %819, %12) # torch/nn/modules/conv.py:415:15
  %821 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%372)
  %822 : int = aten::dim(%out.409) # torch/nn/modules/batchnorm.py:276:11
  %823 : bool = aten::ne(%822, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%823) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %824 : bool = prim::GetAttr[name="training"](%821)
   = prim::If(%824) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %825 : Tensor = prim::GetAttr[name="num_batches_tracked"](%821)
      %826 : Tensor = aten::add(%825, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%821, %826)
      -> ()
    block1():
      -> ()
  %827 : bool = prim::GetAttr[name="training"](%821)
  %828 : Tensor = prim::GetAttr[name="running_mean"](%821)
  %829 : Tensor = prim::GetAttr[name="running_var"](%821)
  %830 : Tensor = prim::GetAttr[name="weight"](%821)
  %831 : Tensor = prim::GetAttr[name="bias"](%821)
   = prim::If(%827) # torch/nn/functional.py:2011:4
    block0():
      %832 : int[] = aten::size(%out.409) # torch/nn/functional.py:2012:27
      %size_prods.464 : int = aten::__getitem__(%832, %8) # torch/nn/functional.py:1991:17
      %834 : int = aten::len(%832) # torch/nn/functional.py:1992:19
      %835 : int = aten::sub(%834, %10) # torch/nn/functional.py:1992:19
      %size_prods.465 : int = prim::Loop(%835, %9, %size_prods.464) # torch/nn/functional.py:1992:4
        block0(%i.117 : int, %size_prods.466 : int):
          %839 : int = aten::add(%i.117, %10) # torch/nn/functional.py:1993:27
          %840 : int = aten::__getitem__(%832, %839) # torch/nn/functional.py:1993:22
          %size_prods.467 : int = aten::mul(%size_prods.466, %840) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.467)
      %842 : bool = aten::eq(%size_prods.465, %12) # torch/nn/functional.py:1994:7
       = prim::If(%842) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.410 : Tensor = aten::batch_norm(%out.409, %830, %831, %828, %829, %827, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.411 : Tensor = aten::relu_(%out.410) # torch/nn/functional.py:1117:17
  %845 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%372)
  %846 : Tensor = prim::GetAttr[name="weight"](%845)
  %847 : Tensor? = prim::GetAttr[name="bias"](%845)
  %848 : int[] = prim::ListConstruct(%12, %12)
  %849 : int[] = prim::ListConstruct(%8, %8)
  %850 : int[] = prim::ListConstruct(%12, %12)
  %out.412 : Tensor = aten::conv2d(%out.411, %846, %847, %848, %849, %850, %12) # torch/nn/modules/conv.py:415:15
  %852 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%372)
  %853 : int = aten::dim(%out.412) # torch/nn/modules/batchnorm.py:276:11
  %854 : bool = aten::ne(%853, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%854) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %855 : bool = prim::GetAttr[name="training"](%852)
   = prim::If(%855) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %856 : Tensor = prim::GetAttr[name="num_batches_tracked"](%852)
      %857 : Tensor = aten::add(%856, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%852, %857)
      -> ()
    block1():
      -> ()
  %858 : bool = prim::GetAttr[name="training"](%852)
  %859 : Tensor = prim::GetAttr[name="running_mean"](%852)
  %860 : Tensor = prim::GetAttr[name="running_var"](%852)
  %861 : Tensor = prim::GetAttr[name="weight"](%852)
  %862 : Tensor = prim::GetAttr[name="bias"](%852)
   = prim::If(%858) # torch/nn/functional.py:2011:4
    block0():
      %863 : int[] = aten::size(%out.412) # torch/nn/functional.py:2012:27
      %size_prods.468 : int = aten::__getitem__(%863, %8) # torch/nn/functional.py:1991:17
      %865 : int = aten::len(%863) # torch/nn/functional.py:1992:19
      %866 : int = aten::sub(%865, %10) # torch/nn/functional.py:1992:19
      %size_prods.469 : int = prim::Loop(%866, %9, %size_prods.468) # torch/nn/functional.py:1992:4
        block0(%i.118 : int, %size_prods.470 : int):
          %870 : int = aten::add(%i.118, %10) # torch/nn/functional.py:1993:27
          %871 : int = aten::__getitem__(%863, %870) # torch/nn/functional.py:1993:22
          %size_prods.471 : int = aten::mul(%size_prods.470, %871) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.471)
      %873 : bool = aten::eq(%size_prods.469, %12) # torch/nn/functional.py:1994:7
       = prim::If(%873) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.413 : Tensor = aten::batch_norm(%out.412, %861, %862, %859, %860, %858, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.414 : Tensor = aten::add_(%out.413, %input.18, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.26 : Tensor = aten::relu_(%out.414) # torch/nn/functional.py:1117:17
  %877 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%373)
  %878 : Tensor = prim::GetAttr[name="weight"](%877)
  %879 : Tensor? = prim::GetAttr[name="bias"](%877)
  %880 : int[] = prim::ListConstruct(%12, %12)
  %881 : int[] = prim::ListConstruct(%8, %8)
  %882 : int[] = prim::ListConstruct(%12, %12)
  %out.388 : Tensor = aten::conv2d(%input.26, %878, %879, %880, %881, %882, %12) # torch/nn/modules/conv.py:415:15
  %884 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%373)
  %885 : int = aten::dim(%out.388) # torch/nn/modules/batchnorm.py:276:11
  %886 : bool = aten::ne(%885, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%886) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %887 : bool = prim::GetAttr[name="training"](%884)
   = prim::If(%887) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %888 : Tensor = prim::GetAttr[name="num_batches_tracked"](%884)
      %889 : Tensor = aten::add(%888, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%884, %889)
      -> ()
    block1():
      -> ()
  %890 : bool = prim::GetAttr[name="training"](%884)
  %891 : Tensor = prim::GetAttr[name="running_mean"](%884)
  %892 : Tensor = prim::GetAttr[name="running_var"](%884)
  %893 : Tensor = prim::GetAttr[name="weight"](%884)
  %894 : Tensor = prim::GetAttr[name="bias"](%884)
   = prim::If(%890) # torch/nn/functional.py:2011:4
    block0():
      %895 : int[] = aten::size(%out.388) # torch/nn/functional.py:2012:27
      %size_prods.472 : int = aten::__getitem__(%895, %8) # torch/nn/functional.py:1991:17
      %897 : int = aten::len(%895) # torch/nn/functional.py:1992:19
      %898 : int = aten::sub(%897, %10) # torch/nn/functional.py:1992:19
      %size_prods.473 : int = prim::Loop(%898, %9, %size_prods.472) # torch/nn/functional.py:1992:4
        block0(%i.119 : int, %size_prods.474 : int):
          %902 : int = aten::add(%i.119, %10) # torch/nn/functional.py:1993:27
          %903 : int = aten::__getitem__(%895, %902) # torch/nn/functional.py:1993:22
          %size_prods.475 : int = aten::mul(%size_prods.474, %903) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.475)
      %905 : bool = aten::eq(%size_prods.473, %12) # torch/nn/functional.py:1994:7
       = prim::If(%905) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.326 : Tensor = aten::batch_norm(%out.388, %893, %894, %891, %892, %890, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.327 : Tensor = aten::relu_(%out.326) # torch/nn/functional.py:1117:17
  %908 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%373)
  %909 : Tensor = prim::GetAttr[name="weight"](%908)
  %910 : Tensor? = prim::GetAttr[name="bias"](%908)
  %911 : int[] = prim::ListConstruct(%12, %12)
  %912 : int[] = prim::ListConstruct(%12, %12)
  %913 : int[] = prim::ListConstruct(%12, %12)
  %out.328 : Tensor = aten::conv2d(%out.327, %909, %910, %911, %912, %913, %12) # torch/nn/modules/conv.py:415:15
  %915 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%373)
  %916 : int = aten::dim(%out.328) # torch/nn/modules/batchnorm.py:276:11
  %917 : bool = aten::ne(%916, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%917) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %918 : bool = prim::GetAttr[name="training"](%915)
   = prim::If(%918) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %919 : Tensor = prim::GetAttr[name="num_batches_tracked"](%915)
      %920 : Tensor = aten::add(%919, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%915, %920)
      -> ()
    block1():
      -> ()
  %921 : bool = prim::GetAttr[name="training"](%915)
  %922 : Tensor = prim::GetAttr[name="running_mean"](%915)
  %923 : Tensor = prim::GetAttr[name="running_var"](%915)
  %924 : Tensor = prim::GetAttr[name="weight"](%915)
  %925 : Tensor = prim::GetAttr[name="bias"](%915)
   = prim::If(%921) # torch/nn/functional.py:2011:4
    block0():
      %926 : int[] = aten::size(%out.328) # torch/nn/functional.py:2012:27
      %size_prods.476 : int = aten::__getitem__(%926, %8) # torch/nn/functional.py:1991:17
      %928 : int = aten::len(%926) # torch/nn/functional.py:1992:19
      %929 : int = aten::sub(%928, %10) # torch/nn/functional.py:1992:19
      %size_prods.477 : int = prim::Loop(%929, %9, %size_prods.476) # torch/nn/functional.py:1992:4
        block0(%i.120 : int, %size_prods.478 : int):
          %933 : int = aten::add(%i.120, %10) # torch/nn/functional.py:1993:27
          %934 : int = aten::__getitem__(%926, %933) # torch/nn/functional.py:1993:22
          %size_prods.479 : int = aten::mul(%size_prods.478, %934) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.479)
      %936 : bool = aten::eq(%size_prods.477, %12) # torch/nn/functional.py:1994:7
       = prim::If(%936) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.329 : Tensor = aten::batch_norm(%out.328, %924, %925, %922, %923, %921, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.330 : Tensor = aten::relu_(%out.329) # torch/nn/functional.py:1117:17
  %939 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%373)
  %940 : Tensor = prim::GetAttr[name="weight"](%939)
  %941 : Tensor? = prim::GetAttr[name="bias"](%939)
  %942 : int[] = prim::ListConstruct(%12, %12)
  %943 : int[] = prim::ListConstruct(%8, %8)
  %944 : int[] = prim::ListConstruct(%12, %12)
  %out.331 : Tensor = aten::conv2d(%out.330, %940, %941, %942, %943, %944, %12) # torch/nn/modules/conv.py:415:15
  %946 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%373)
  %947 : int = aten::dim(%out.331) # torch/nn/modules/batchnorm.py:276:11
  %948 : bool = aten::ne(%947, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%948) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %949 : bool = prim::GetAttr[name="training"](%946)
   = prim::If(%949) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %950 : Tensor = prim::GetAttr[name="num_batches_tracked"](%946)
      %951 : Tensor = aten::add(%950, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%946, %951)
      -> ()
    block1():
      -> ()
  %952 : bool = prim::GetAttr[name="training"](%946)
  %953 : Tensor = prim::GetAttr[name="running_mean"](%946)
  %954 : Tensor = prim::GetAttr[name="running_var"](%946)
  %955 : Tensor = prim::GetAttr[name="weight"](%946)
  %956 : Tensor = prim::GetAttr[name="bias"](%946)
   = prim::If(%952) # torch/nn/functional.py:2011:4
    block0():
      %957 : int[] = aten::size(%out.331) # torch/nn/functional.py:2012:27
      %size_prods.480 : int = aten::__getitem__(%957, %8) # torch/nn/functional.py:1991:17
      %959 : int = aten::len(%957) # torch/nn/functional.py:1992:19
      %960 : int = aten::sub(%959, %10) # torch/nn/functional.py:1992:19
      %size_prods.481 : int = prim::Loop(%960, %9, %size_prods.480) # torch/nn/functional.py:1992:4
        block0(%i.121 : int, %size_prods.482 : int):
          %964 : int = aten::add(%i.121, %10) # torch/nn/functional.py:1993:27
          %965 : int = aten::__getitem__(%957, %964) # torch/nn/functional.py:1993:22
          %size_prods.483 : int = aten::mul(%size_prods.482, %965) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.483)
      %967 : bool = aten::eq(%size_prods.481, %12) # torch/nn/functional.py:1994:7
       = prim::If(%967) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.332 : Tensor = aten::batch_norm(%out.331, %955, %956, %953, %954, %952, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.333 : Tensor = aten::add_(%out.332, %input.26, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.20 : Tensor = aten::relu_(%out.333) # torch/nn/functional.py:1117:17
  %971 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%374)
  %972 : Tensor = prim::GetAttr[name="weight"](%971)
  %973 : Tensor? = prim::GetAttr[name="bias"](%971)
  %974 : int[] = prim::ListConstruct(%12, %12)
  %975 : int[] = prim::ListConstruct(%8, %8)
  %976 : int[] = prim::ListConstruct(%12, %12)
  %out.397 : Tensor = aten::conv2d(%input.20, %972, %973, %974, %975, %976, %12) # torch/nn/modules/conv.py:415:15
  %978 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%374)
  %979 : int = aten::dim(%out.397) # torch/nn/modules/batchnorm.py:276:11
  %980 : bool = aten::ne(%979, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%980) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %981 : bool = prim::GetAttr[name="training"](%978)
   = prim::If(%981) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %982 : Tensor = prim::GetAttr[name="num_batches_tracked"](%978)
      %983 : Tensor = aten::add(%982, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%978, %983)
      -> ()
    block1():
      -> ()
  %984 : bool = prim::GetAttr[name="training"](%978)
  %985 : Tensor = prim::GetAttr[name="running_mean"](%978)
  %986 : Tensor = prim::GetAttr[name="running_var"](%978)
  %987 : Tensor = prim::GetAttr[name="weight"](%978)
  %988 : Tensor = prim::GetAttr[name="bias"](%978)
   = prim::If(%984) # torch/nn/functional.py:2011:4
    block0():
      %989 : int[] = aten::size(%out.397) # torch/nn/functional.py:2012:27
      %size_prods.484 : int = aten::__getitem__(%989, %8) # torch/nn/functional.py:1991:17
      %991 : int = aten::len(%989) # torch/nn/functional.py:1992:19
      %992 : int = aten::sub(%991, %10) # torch/nn/functional.py:1992:19
      %size_prods.485 : int = prim::Loop(%992, %9, %size_prods.484) # torch/nn/functional.py:1992:4
        block0(%i.122 : int, %size_prods.486 : int):
          %996 : int = aten::add(%i.122, %10) # torch/nn/functional.py:1993:27
          %997 : int = aten::__getitem__(%989, %996) # torch/nn/functional.py:1993:22
          %size_prods.487 : int = aten::mul(%size_prods.486, %997) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.487)
      %999 : bool = aten::eq(%size_prods.485, %12) # torch/nn/functional.py:1994:7
       = prim::If(%999) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.335 : Tensor = aten::batch_norm(%out.397, %987, %988, %985, %986, %984, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.336 : Tensor = aten::relu_(%out.335) # torch/nn/functional.py:1117:17
  %1002 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%374)
  %1003 : Tensor = prim::GetAttr[name="weight"](%1002)
  %1004 : Tensor? = prim::GetAttr[name="bias"](%1002)
  %1005 : int[] = prim::ListConstruct(%12, %12)
  %1006 : int[] = prim::ListConstruct(%12, %12)
  %1007 : int[] = prim::ListConstruct(%12, %12)
  %out.337 : Tensor = aten::conv2d(%out.336, %1003, %1004, %1005, %1006, %1007, %12) # torch/nn/modules/conv.py:415:15
  %1009 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%374)
  %1010 : int = aten::dim(%out.337) # torch/nn/modules/batchnorm.py:276:11
  %1011 : bool = aten::ne(%1010, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1011) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1012 : bool = prim::GetAttr[name="training"](%1009)
   = prim::If(%1012) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1013 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1009)
      %1014 : Tensor = aten::add(%1013, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1009, %1014)
      -> ()
    block1():
      -> ()
  %1015 : bool = prim::GetAttr[name="training"](%1009)
  %1016 : Tensor = prim::GetAttr[name="running_mean"](%1009)
  %1017 : Tensor = prim::GetAttr[name="running_var"](%1009)
  %1018 : Tensor = prim::GetAttr[name="weight"](%1009)
  %1019 : Tensor = prim::GetAttr[name="bias"](%1009)
   = prim::If(%1015) # torch/nn/functional.py:2011:4
    block0():
      %1020 : int[] = aten::size(%out.337) # torch/nn/functional.py:2012:27
      %size_prods.488 : int = aten::__getitem__(%1020, %8) # torch/nn/functional.py:1991:17
      %1022 : int = aten::len(%1020) # torch/nn/functional.py:1992:19
      %1023 : int = aten::sub(%1022, %10) # torch/nn/functional.py:1992:19
      %size_prods.489 : int = prim::Loop(%1023, %9, %size_prods.488) # torch/nn/functional.py:1992:4
        block0(%i.123 : int, %size_prods.490 : int):
          %1027 : int = aten::add(%i.123, %10) # torch/nn/functional.py:1993:27
          %1028 : int = aten::__getitem__(%1020, %1027) # torch/nn/functional.py:1993:22
          %size_prods.491 : int = aten::mul(%size_prods.490, %1028) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.491)
      %1030 : bool = aten::eq(%size_prods.489, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1030) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.338 : Tensor = aten::batch_norm(%out.337, %1018, %1019, %1016, %1017, %1015, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.339 : Tensor = aten::relu_(%out.338) # torch/nn/functional.py:1117:17
  %1033 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%374)
  %1034 : Tensor = prim::GetAttr[name="weight"](%1033)
  %1035 : Tensor? = prim::GetAttr[name="bias"](%1033)
  %1036 : int[] = prim::ListConstruct(%12, %12)
  %1037 : int[] = prim::ListConstruct(%8, %8)
  %1038 : int[] = prim::ListConstruct(%12, %12)
  %out.340 : Tensor = aten::conv2d(%out.339, %1034, %1035, %1036, %1037, %1038, %12) # torch/nn/modules/conv.py:415:15
  %1040 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%374)
  %1041 : int = aten::dim(%out.340) # torch/nn/modules/batchnorm.py:276:11
  %1042 : bool = aten::ne(%1041, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1042) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1043 : bool = prim::GetAttr[name="training"](%1040)
   = prim::If(%1043) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1044 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1040)
      %1045 : Tensor = aten::add(%1044, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1040, %1045)
      -> ()
    block1():
      -> ()
  %1046 : bool = prim::GetAttr[name="training"](%1040)
  %1047 : Tensor = prim::GetAttr[name="running_mean"](%1040)
  %1048 : Tensor = prim::GetAttr[name="running_var"](%1040)
  %1049 : Tensor = prim::GetAttr[name="weight"](%1040)
  %1050 : Tensor = prim::GetAttr[name="bias"](%1040)
   = prim::If(%1046) # torch/nn/functional.py:2011:4
    block0():
      %1051 : int[] = aten::size(%out.340) # torch/nn/functional.py:2012:27
      %size_prods.492 : int = aten::__getitem__(%1051, %8) # torch/nn/functional.py:1991:17
      %1053 : int = aten::len(%1051) # torch/nn/functional.py:1992:19
      %1054 : int = aten::sub(%1053, %10) # torch/nn/functional.py:1992:19
      %size_prods.493 : int = prim::Loop(%1054, %9, %size_prods.492) # torch/nn/functional.py:1992:4
        block0(%i.124 : int, %size_prods.494 : int):
          %1058 : int = aten::add(%i.124, %10) # torch/nn/functional.py:1993:27
          %1059 : int = aten::__getitem__(%1051, %1058) # torch/nn/functional.py:1993:22
          %size_prods.495 : int = aten::mul(%size_prods.494, %1059) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.495)
      %1061 : bool = aten::eq(%size_prods.493, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1061) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.341 : Tensor = aten::batch_norm(%out.340, %1049, %1050, %1047, %1048, %1046, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.342 : Tensor = aten::add_(%out.341, %input.20, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.24 : Tensor = aten::relu_(%out.342) # torch/nn/functional.py:1117:17
  %1065 : __torch__.torch.nn.modules.conv.___torch_mangle_25.Conv2d = prim::GetAttr[name="conv1"](%375)
  %1066 : Tensor = prim::GetAttr[name="weight"](%1065)
  %1067 : Tensor? = prim::GetAttr[name="bias"](%1065)
  %1068 : int[] = prim::ListConstruct(%12, %12)
  %1069 : int[] = prim::ListConstruct(%8, %8)
  %1070 : int[] = prim::ListConstruct(%12, %12)
  %out.415 : Tensor = aten::conv2d(%input.24, %1066, %1067, %1068, %1069, %1070, %12) # torch/nn/modules/conv.py:415:15
  %1072 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn1"](%375)
  %1073 : int = aten::dim(%out.415) # torch/nn/modules/batchnorm.py:276:11
  %1074 : bool = aten::ne(%1073, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1074) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1075 : bool = prim::GetAttr[name="training"](%1072)
   = prim::If(%1075) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1076 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1072)
      %1077 : Tensor = aten::add(%1076, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1072, %1077)
      -> ()
    block1():
      -> ()
  %1078 : bool = prim::GetAttr[name="training"](%1072)
  %1079 : Tensor = prim::GetAttr[name="running_mean"](%1072)
  %1080 : Tensor = prim::GetAttr[name="running_var"](%1072)
  %1081 : Tensor = prim::GetAttr[name="weight"](%1072)
  %1082 : Tensor = prim::GetAttr[name="bias"](%1072)
   = prim::If(%1078) # torch/nn/functional.py:2011:4
    block0():
      %1083 : int[] = aten::size(%out.415) # torch/nn/functional.py:2012:27
      %size_prods.568 : int = aten::__getitem__(%1083, %8) # torch/nn/functional.py:1991:17
      %1085 : int = aten::len(%1083) # torch/nn/functional.py:1992:19
      %1086 : int = aten::sub(%1085, %10) # torch/nn/functional.py:1992:19
      %size_prods.569 : int = prim::Loop(%1086, %9, %size_prods.568) # torch/nn/functional.py:1992:4
        block0(%i.143 : int, %size_prods.570 : int):
          %1090 : int = aten::add(%i.143, %10) # torch/nn/functional.py:1993:27
          %1091 : int = aten::__getitem__(%1083, %1090) # torch/nn/functional.py:1993:22
          %size_prods.571 : int = aten::mul(%size_prods.570, %1091) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.571)
      %1093 : bool = aten::eq(%size_prods.569, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1093) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.416 : Tensor = aten::batch_norm(%out.415, %1081, %1082, %1079, %1080, %1078, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.417 : Tensor = aten::relu_(%out.416) # torch/nn/functional.py:1117:17
  %1096 : __torch__.torch.nn.modules.conv.___torch_mangle_26.Conv2d = prim::GetAttr[name="conv2"](%375)
  %1097 : Tensor = prim::GetAttr[name="weight"](%1096)
  %1098 : Tensor? = prim::GetAttr[name="bias"](%1096)
  %1099 : int[] = prim::ListConstruct(%12, %12)
  %1100 : int[] = prim::ListConstruct(%12, %12)
  %1101 : int[] = prim::ListConstruct(%12, %12)
  %out.418 : Tensor = aten::conv2d(%out.417, %1097, %1098, %1099, %1100, %1101, %12) # torch/nn/modules/conv.py:415:15
  %1103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_18.BatchNorm2d = prim::GetAttr[name="bn2"](%375)
  %1104 : int = aten::dim(%out.418) # torch/nn/modules/batchnorm.py:276:11
  %1105 : bool = aten::ne(%1104, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1105) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1106 : bool = prim::GetAttr[name="training"](%1103)
   = prim::If(%1106) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1103)
      %1108 : Tensor = aten::add(%1107, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1103, %1108)
      -> ()
    block1():
      -> ()
  %1109 : bool = prim::GetAttr[name="training"](%1103)
  %1110 : Tensor = prim::GetAttr[name="running_mean"](%1103)
  %1111 : Tensor = prim::GetAttr[name="running_var"](%1103)
  %1112 : Tensor = prim::GetAttr[name="weight"](%1103)
  %1113 : Tensor = prim::GetAttr[name="bias"](%1103)
   = prim::If(%1109) # torch/nn/functional.py:2011:4
    block0():
      %1114 : int[] = aten::size(%out.418) # torch/nn/functional.py:2012:27
      %size_prods.572 : int = aten::__getitem__(%1114, %8) # torch/nn/functional.py:1991:17
      %1116 : int = aten::len(%1114) # torch/nn/functional.py:1992:19
      %1117 : int = aten::sub(%1116, %10) # torch/nn/functional.py:1992:19
      %size_prods.573 : int = prim::Loop(%1117, %9, %size_prods.572) # torch/nn/functional.py:1992:4
        block0(%i.144 : int, %size_prods.574 : int):
          %1121 : int = aten::add(%i.144, %10) # torch/nn/functional.py:1993:27
          %1122 : int = aten::__getitem__(%1114, %1121) # torch/nn/functional.py:1993:22
          %size_prods.575 : int = aten::mul(%size_prods.574, %1122) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.575)
      %1124 : bool = aten::eq(%size_prods.573, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1124) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.419 : Tensor = aten::batch_norm(%out.418, %1112, %1113, %1110, %1111, %1109, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.420 : Tensor = aten::relu_(%out.419) # torch/nn/functional.py:1117:17
  %1127 : __torch__.torch.nn.modules.conv.___torch_mangle_20.Conv2d = prim::GetAttr[name="conv3"](%375)
  %1128 : Tensor = prim::GetAttr[name="weight"](%1127)
  %1129 : Tensor? = prim::GetAttr[name="bias"](%1127)
  %1130 : int[] = prim::ListConstruct(%12, %12)
  %1131 : int[] = prim::ListConstruct(%8, %8)
  %1132 : int[] = prim::ListConstruct(%12, %12)
  %out.421 : Tensor = aten::conv2d(%out.420, %1128, %1129, %1130, %1131, %1132, %12) # torch/nn/modules/conv.py:415:15
  %1134 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn3"](%375)
  %1135 : int = aten::dim(%out.421) # torch/nn/modules/batchnorm.py:276:11
  %1136 : bool = aten::ne(%1135, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1136) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1137 : bool = prim::GetAttr[name="training"](%1134)
   = prim::If(%1137) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1138 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1134)
      %1139 : Tensor = aten::add(%1138, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1134, %1139)
      -> ()
    block1():
      -> ()
  %1140 : bool = prim::GetAttr[name="training"](%1134)
  %1141 : Tensor = prim::GetAttr[name="running_mean"](%1134)
  %1142 : Tensor = prim::GetAttr[name="running_var"](%1134)
  %1143 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
   = prim::If(%1140) # torch/nn/functional.py:2011:4
    block0():
      %1145 : int[] = aten::size(%out.421) # torch/nn/functional.py:2012:27
      %size_prods.576 : int = aten::__getitem__(%1145, %8) # torch/nn/functional.py:1991:17
      %1147 : int = aten::len(%1145) # torch/nn/functional.py:1992:19
      %1148 : int = aten::sub(%1147, %10) # torch/nn/functional.py:1992:19
      %size_prods.577 : int = prim::Loop(%1148, %9, %size_prods.576) # torch/nn/functional.py:1992:4
        block0(%i.145 : int, %size_prods.578 : int):
          %1152 : int = aten::add(%i.145, %10) # torch/nn/functional.py:1993:27
          %1153 : int = aten::__getitem__(%1145, %1152) # torch/nn/functional.py:1993:22
          %size_prods.579 : int = aten::mul(%size_prods.578, %1153) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.579)
      %1155 : bool = aten::eq(%size_prods.577, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1155) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.422 : Tensor = aten::batch_norm(%out.421, %1143, %1144, %1141, %1142, %1140, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.423 : Tensor = aten::add_(%out.422, %input.24, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.13 : Tensor = aten::relu_(%out.423) # torch/nn/functional.py:1117:17
  %1159 : __torch__.torch.nn.modules.container.___torch_mangle_952.Sequential = prim::GetAttr[name="layer3"](%self)
  %1160 : __torch__.torchvision.models.resnet.___torch_mangle_941.Bottleneck = prim::GetAttr[name="0"](%1159)
  %1161 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="1"](%1159)
  %1162 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="2"](%1159)
  %1163 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="3"](%1159)
  %1164 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="4"](%1159)
  %1165 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="5"](%1159)
  %1166 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="6"](%1159)
  %1167 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="7"](%1159)
  %1168 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="8"](%1159)
  %1169 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="9"](%1159)
  %1170 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="10"](%1159)
  %1171 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="11"](%1159)
  %1172 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="12"](%1159)
  %1173 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="13"](%1159)
  %1174 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="14"](%1159)
  %1175 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="15"](%1159)
  %1176 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="16"](%1159)
  %1177 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="17"](%1159)
  %1178 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="18"](%1159)
  %1179 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="19"](%1159)
  %1180 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="20"](%1159)
  %1181 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="21"](%1159)
  %1182 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="22"](%1159)
  %1183 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="23"](%1159)
  %1184 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="24"](%1159)
  %1185 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="25"](%1159)
  %1186 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="26"](%1159)
  %1187 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="27"](%1159)
  %1188 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="28"](%1159)
  %1189 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="29"](%1159)
  %1190 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="30"](%1159)
  %1191 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="31"](%1159)
  %1192 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="32"](%1159)
  %1193 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="33"](%1159)
  %1194 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="34"](%1159)
  %1195 : __torch__.torchvision.models.resnet.___torch_mangle_942.Bottleneck = prim::GetAttr[name="35"](%1159)
  %1196 : __torch__.torch.nn.modules.conv.___torch_mangle_29.Conv2d = prim::GetAttr[name="conv1"](%1160)
  %1197 : Tensor = prim::GetAttr[name="weight"](%1196)
  %1198 : Tensor? = prim::GetAttr[name="bias"](%1196)
  %1199 : int[] = prim::ListConstruct(%12, %12)
  %1200 : int[] = prim::ListConstruct(%8, %8)
  %1201 : int[] = prim::ListConstruct(%12, %12)
  %out.424 : Tensor = aten::conv2d(%x.13, %1197, %1198, %1199, %1200, %1201, %12) # torch/nn/modules/conv.py:415:15
  %1203 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1160)
  %1204 : int = aten::dim(%out.424) # torch/nn/modules/batchnorm.py:276:11
  %1205 : bool = aten::ne(%1204, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1205) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1206 : bool = prim::GetAttr[name="training"](%1203)
   = prim::If(%1206) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1207 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1203)
      %1208 : Tensor = aten::add(%1207, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1203, %1208)
      -> ()
    block1():
      -> ()
  %1209 : bool = prim::GetAttr[name="training"](%1203)
  %1210 : Tensor = prim::GetAttr[name="running_mean"](%1203)
  %1211 : Tensor = prim::GetAttr[name="running_var"](%1203)
  %1212 : Tensor = prim::GetAttr[name="weight"](%1203)
  %1213 : Tensor = prim::GetAttr[name="bias"](%1203)
   = prim::If(%1209) # torch/nn/functional.py:2011:4
    block0():
      %1214 : int[] = aten::size(%out.424) # torch/nn/functional.py:2012:27
      %size_prods.580 : int = aten::__getitem__(%1214, %8) # torch/nn/functional.py:1991:17
      %1216 : int = aten::len(%1214) # torch/nn/functional.py:1992:19
      %1217 : int = aten::sub(%1216, %10) # torch/nn/functional.py:1992:19
      %size_prods.581 : int = prim::Loop(%1217, %9, %size_prods.580) # torch/nn/functional.py:1992:4
        block0(%i.146 : int, %size_prods.582 : int):
          %1221 : int = aten::add(%i.146, %10) # torch/nn/functional.py:1993:27
          %1222 : int = aten::__getitem__(%1214, %1221) # torch/nn/functional.py:1993:22
          %size_prods.583 : int = aten::mul(%size_prods.582, %1222) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.583)
      %1224 : bool = aten::eq(%size_prods.581, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1224) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.425 : Tensor = aten::batch_norm(%out.424, %1212, %1213, %1210, %1211, %1209, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.426 : Tensor = aten::relu_(%out.425) # torch/nn/functional.py:1117:17
  %1227 : __torch__.torch.nn.modules.conv.___torch_mangle_938.Conv2d = prim::GetAttr[name="conv2"](%1160)
  %1228 : Tensor = prim::GetAttr[name="weight"](%1227)
  %1229 : Tensor? = prim::GetAttr[name="bias"](%1227)
  %1230 : int[] = prim::ListConstruct(%10, %10)
  %1231 : int[] = prim::ListConstruct(%12, %12)
  %1232 : int[] = prim::ListConstruct(%12, %12)
  %out.427 : Tensor = aten::conv2d(%out.426, %1228, %1229, %1230, %1231, %1232, %12) # torch/nn/modules/conv.py:415:15
  %1234 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1160)
  %1235 : int = aten::dim(%out.427) # torch/nn/modules/batchnorm.py:276:11
  %1236 : bool = aten::ne(%1235, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1236) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1237 : bool = prim::GetAttr[name="training"](%1234)
   = prim::If(%1237) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1238 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1234)
      %1239 : Tensor = aten::add(%1238, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1234, %1239)
      -> ()
    block1():
      -> ()
  %1240 : bool = prim::GetAttr[name="training"](%1234)
  %1241 : Tensor = prim::GetAttr[name="running_mean"](%1234)
  %1242 : Tensor = prim::GetAttr[name="running_var"](%1234)
  %1243 : Tensor = prim::GetAttr[name="weight"](%1234)
  %1244 : Tensor = prim::GetAttr[name="bias"](%1234)
   = prim::If(%1240) # torch/nn/functional.py:2011:4
    block0():
      %1245 : int[] = aten::size(%out.427) # torch/nn/functional.py:2012:27
      %size_prods.584 : int = aten::__getitem__(%1245, %8) # torch/nn/functional.py:1991:17
      %1247 : int = aten::len(%1245) # torch/nn/functional.py:1992:19
      %1248 : int = aten::sub(%1247, %10) # torch/nn/functional.py:1992:19
      %size_prods.585 : int = prim::Loop(%1248, %9, %size_prods.584) # torch/nn/functional.py:1992:4
        block0(%i.147 : int, %size_prods.586 : int):
          %1252 : int = aten::add(%i.147, %10) # torch/nn/functional.py:1993:27
          %1253 : int = aten::__getitem__(%1245, %1252) # torch/nn/functional.py:1993:22
          %size_prods.587 : int = aten::mul(%size_prods.586, %1253) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.587)
      %1255 : bool = aten::eq(%size_prods.585, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1255) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.428 : Tensor = aten::batch_norm(%out.427, %1243, %1244, %1241, %1242, %1240, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.429 : Tensor = aten::relu_(%out.428) # torch/nn/functional.py:1117:17
  %1258 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1160)
  %1259 : Tensor = prim::GetAttr[name="weight"](%1258)
  %1260 : Tensor? = prim::GetAttr[name="bias"](%1258)
  %1261 : int[] = prim::ListConstruct(%12, %12)
  %1262 : int[] = prim::ListConstruct(%8, %8)
  %1263 : int[] = prim::ListConstruct(%12, %12)
  %out.430 : Tensor = aten::conv2d(%out.429, %1259, %1260, %1261, %1262, %1263, %12) # torch/nn/modules/conv.py:415:15
  %1265 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1160)
  %1266 : int = aten::dim(%out.430) # torch/nn/modules/batchnorm.py:276:11
  %1267 : bool = aten::ne(%1266, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1267) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1268 : bool = prim::GetAttr[name="training"](%1265)
   = prim::If(%1268) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1269 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1265)
      %1270 : Tensor = aten::add(%1269, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1265, %1270)
      -> ()
    block1():
      -> ()
  %1271 : bool = prim::GetAttr[name="training"](%1265)
  %1272 : Tensor = prim::GetAttr[name="running_mean"](%1265)
  %1273 : Tensor = prim::GetAttr[name="running_var"](%1265)
  %1274 : Tensor = prim::GetAttr[name="weight"](%1265)
  %1275 : Tensor = prim::GetAttr[name="bias"](%1265)
   = prim::If(%1271) # torch/nn/functional.py:2011:4
    block0():
      %1276 : int[] = aten::size(%out.430) # torch/nn/functional.py:2012:27
      %size_prods.588 : int = aten::__getitem__(%1276, %8) # torch/nn/functional.py:1991:17
      %1278 : int = aten::len(%1276) # torch/nn/functional.py:1992:19
      %1279 : int = aten::sub(%1278, %10) # torch/nn/functional.py:1992:19
      %size_prods.589 : int = prim::Loop(%1279, %9, %size_prods.588) # torch/nn/functional.py:1992:4
        block0(%i.148 : int, %size_prods.590 : int):
          %1283 : int = aten::add(%i.148, %10) # torch/nn/functional.py:1993:27
          %1284 : int = aten::__getitem__(%1276, %1283) # torch/nn/functional.py:1993:22
          %size_prods.591 : int = aten::mul(%size_prods.590, %1284) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.591)
      %1286 : bool = aten::eq(%size_prods.589, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1286) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.431 : Tensor = aten::batch_norm(%out.430, %1274, %1275, %1272, %1273, %1271, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %1288 : __torch__.torch.nn.modules.container.___torch_mangle_940.Sequential = prim::GetAttr[name="downsample"](%1160)
  %1289 : __torch__.torch.nn.modules.conv.___torch_mangle_939.Conv2d = prim::GetAttr[name="0"](%1288)
  %1290 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="1"](%1288)
  %1291 : Tensor = prim::GetAttr[name="weight"](%1289)
  %1292 : Tensor? = prim::GetAttr[name="bias"](%1289)
  %1293 : int[] = prim::ListConstruct(%10, %10)
  %1294 : int[] = prim::ListConstruct(%8, %8)
  %1295 : int[] = prim::ListConstruct(%12, %12)
  %input.28 : Tensor = aten::conv2d(%x.13, %1291, %1292, %1293, %1294, %1295, %12) # torch/nn/modules/conv.py:415:15
  %1297 : int = aten::dim(%input.28) # torch/nn/modules/batchnorm.py:276:11
  %1298 : bool = aten::ne(%1297, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1298) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1299 : bool = prim::GetAttr[name="training"](%1290)
   = prim::If(%1299) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1300 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1290)
      %1301 : Tensor = aten::add(%1300, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1290, %1301)
      -> ()
    block1():
      -> ()
  %1302 : bool = prim::GetAttr[name="training"](%1290)
  %1303 : Tensor = prim::GetAttr[name="running_mean"](%1290)
  %1304 : Tensor = prim::GetAttr[name="running_var"](%1290)
  %1305 : Tensor = prim::GetAttr[name="weight"](%1290)
  %1306 : Tensor = prim::GetAttr[name="bias"](%1290)
   = prim::If(%1302) # torch/nn/functional.py:2011:4
    block0():
      %1307 : int[] = aten::size(%input.28) # torch/nn/functional.py:2012:27
      %size_prods.592 : int = aten::__getitem__(%1307, %8) # torch/nn/functional.py:1991:17
      %1309 : int = aten::len(%1307) # torch/nn/functional.py:1992:19
      %1310 : int = aten::sub(%1309, %10) # torch/nn/functional.py:1992:19
      %size_prods.593 : int = prim::Loop(%1310, %9, %size_prods.592) # torch/nn/functional.py:1992:4
        block0(%i.149 : int, %size_prods.594 : int):
          %1314 : int = aten::add(%i.149, %10) # torch/nn/functional.py:1993:27
          %1315 : int = aten::__getitem__(%1307, %1314) # torch/nn/functional.py:1993:22
          %size_prods.595 : int = aten::mul(%size_prods.594, %1315) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.595)
      %1317 : bool = aten::eq(%size_prods.593, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1317) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.4 : Tensor = aten::batch_norm(%input.28, %1305, %1306, %1303, %1304, %1302, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.432 : Tensor = aten::add_(%out.431, %identity.4, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.30 : Tensor = aten::relu_(%out.432) # torch/nn/functional.py:1117:17
  %1321 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1161)
  %1322 : Tensor = prim::GetAttr[name="weight"](%1321)
  %1323 : Tensor? = prim::GetAttr[name="bias"](%1321)
  %1324 : int[] = prim::ListConstruct(%12, %12)
  %1325 : int[] = prim::ListConstruct(%8, %8)
  %1326 : int[] = prim::ListConstruct(%12, %12)
  %out.433 : Tensor = aten::conv2d(%input.30, %1322, %1323, %1324, %1325, %1326, %12) # torch/nn/modules/conv.py:415:15
  %1328 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1161)
  %1329 : int = aten::dim(%out.433) # torch/nn/modules/batchnorm.py:276:11
  %1330 : bool = aten::ne(%1329, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1330) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1331 : bool = prim::GetAttr[name="training"](%1328)
   = prim::If(%1331) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1332 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1328)
      %1333 : Tensor = aten::add(%1332, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1328, %1333)
      -> ()
    block1():
      -> ()
  %1334 : bool = prim::GetAttr[name="training"](%1328)
  %1335 : Tensor = prim::GetAttr[name="running_mean"](%1328)
  %1336 : Tensor = prim::GetAttr[name="running_var"](%1328)
  %1337 : Tensor = prim::GetAttr[name="weight"](%1328)
  %1338 : Tensor = prim::GetAttr[name="bias"](%1328)
   = prim::If(%1334) # torch/nn/functional.py:2011:4
    block0():
      %1339 : int[] = aten::size(%out.433) # torch/nn/functional.py:2012:27
      %size_prods.596 : int = aten::__getitem__(%1339, %8) # torch/nn/functional.py:1991:17
      %1341 : int = aten::len(%1339) # torch/nn/functional.py:1992:19
      %1342 : int = aten::sub(%1341, %10) # torch/nn/functional.py:1992:19
      %size_prods.597 : int = prim::Loop(%1342, %9, %size_prods.596) # torch/nn/functional.py:1992:4
        block0(%i.150 : int, %size_prods.598 : int):
          %1346 : int = aten::add(%i.150, %10) # torch/nn/functional.py:1993:27
          %1347 : int = aten::__getitem__(%1339, %1346) # torch/nn/functional.py:1993:22
          %size_prods.599 : int = aten::mul(%size_prods.598, %1347) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.599)
      %1349 : bool = aten::eq(%size_prods.597, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1349) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.434 : Tensor = aten::batch_norm(%out.433, %1337, %1338, %1335, %1336, %1334, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.435 : Tensor = aten::relu_(%out.434) # torch/nn/functional.py:1117:17
  %1352 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1161)
  %1353 : Tensor = prim::GetAttr[name="weight"](%1352)
  %1354 : Tensor? = prim::GetAttr[name="bias"](%1352)
  %1355 : int[] = prim::ListConstruct(%12, %12)
  %1356 : int[] = prim::ListConstruct(%12, %12)
  %1357 : int[] = prim::ListConstruct(%12, %12)
  %out.436 : Tensor = aten::conv2d(%out.435, %1353, %1354, %1355, %1356, %1357, %12) # torch/nn/modules/conv.py:415:15
  %1359 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1161)
  %1360 : int = aten::dim(%out.436) # torch/nn/modules/batchnorm.py:276:11
  %1361 : bool = aten::ne(%1360, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1361) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1362 : bool = prim::GetAttr[name="training"](%1359)
   = prim::If(%1362) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1363 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1359)
      %1364 : Tensor = aten::add(%1363, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1359, %1364)
      -> ()
    block1():
      -> ()
  %1365 : bool = prim::GetAttr[name="training"](%1359)
  %1366 : Tensor = prim::GetAttr[name="running_mean"](%1359)
  %1367 : Tensor = prim::GetAttr[name="running_var"](%1359)
  %1368 : Tensor = prim::GetAttr[name="weight"](%1359)
  %1369 : Tensor = prim::GetAttr[name="bias"](%1359)
   = prim::If(%1365) # torch/nn/functional.py:2011:4
    block0():
      %1370 : int[] = aten::size(%out.436) # torch/nn/functional.py:2012:27
      %size_prods.600 : int = aten::__getitem__(%1370, %8) # torch/nn/functional.py:1991:17
      %1372 : int = aten::len(%1370) # torch/nn/functional.py:1992:19
      %1373 : int = aten::sub(%1372, %10) # torch/nn/functional.py:1992:19
      %size_prods.601 : int = prim::Loop(%1373, %9, %size_prods.600) # torch/nn/functional.py:1992:4
        block0(%i.151 : int, %size_prods.602 : int):
          %1377 : int = aten::add(%i.151, %10) # torch/nn/functional.py:1993:27
          %1378 : int = aten::__getitem__(%1370, %1377) # torch/nn/functional.py:1993:22
          %size_prods.603 : int = aten::mul(%size_prods.602, %1378) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.603)
      %1380 : bool = aten::eq(%size_prods.601, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1380) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.437 : Tensor = aten::batch_norm(%out.436, %1368, %1369, %1366, %1367, %1365, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.438 : Tensor = aten::relu_(%out.437) # torch/nn/functional.py:1117:17
  %1383 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1161)
  %1384 : Tensor = prim::GetAttr[name="weight"](%1383)
  %1385 : Tensor? = prim::GetAttr[name="bias"](%1383)
  %1386 : int[] = prim::ListConstruct(%12, %12)
  %1387 : int[] = prim::ListConstruct(%8, %8)
  %1388 : int[] = prim::ListConstruct(%12, %12)
  %out.439 : Tensor = aten::conv2d(%out.438, %1384, %1385, %1386, %1387, %1388, %12) # torch/nn/modules/conv.py:415:15
  %1390 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1161)
  %1391 : int = aten::dim(%out.439) # torch/nn/modules/batchnorm.py:276:11
  %1392 : bool = aten::ne(%1391, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1392) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1393 : bool = prim::GetAttr[name="training"](%1390)
   = prim::If(%1393) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1394 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1390)
      %1395 : Tensor = aten::add(%1394, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1390, %1395)
      -> ()
    block1():
      -> ()
  %1396 : bool = prim::GetAttr[name="training"](%1390)
  %1397 : Tensor = prim::GetAttr[name="running_mean"](%1390)
  %1398 : Tensor = prim::GetAttr[name="running_var"](%1390)
  %1399 : Tensor = prim::GetAttr[name="weight"](%1390)
  %1400 : Tensor = prim::GetAttr[name="bias"](%1390)
   = prim::If(%1396) # torch/nn/functional.py:2011:4
    block0():
      %1401 : int[] = aten::size(%out.439) # torch/nn/functional.py:2012:27
      %size_prods.604 : int = aten::__getitem__(%1401, %8) # torch/nn/functional.py:1991:17
      %1403 : int = aten::len(%1401) # torch/nn/functional.py:1992:19
      %1404 : int = aten::sub(%1403, %10) # torch/nn/functional.py:1992:19
      %size_prods.605 : int = prim::Loop(%1404, %9, %size_prods.604) # torch/nn/functional.py:1992:4
        block0(%i.152 : int, %size_prods.606 : int):
          %1408 : int = aten::add(%i.152, %10) # torch/nn/functional.py:1993:27
          %1409 : int = aten::__getitem__(%1401, %1408) # torch/nn/functional.py:1993:22
          %size_prods.607 : int = aten::mul(%size_prods.606, %1409) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.607)
      %1411 : bool = aten::eq(%size_prods.605, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1411) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.440 : Tensor = aten::batch_norm(%out.439, %1399, %1400, %1397, %1398, %1396, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.441 : Tensor = aten::add_(%out.440, %input.30, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.32 : Tensor = aten::relu_(%out.441) # torch/nn/functional.py:1117:17
  %1415 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1162)
  %1416 : Tensor = prim::GetAttr[name="weight"](%1415)
  %1417 : Tensor? = prim::GetAttr[name="bias"](%1415)
  %1418 : int[] = prim::ListConstruct(%12, %12)
  %1419 : int[] = prim::ListConstruct(%8, %8)
  %1420 : int[] = prim::ListConstruct(%12, %12)
  %out.37 : Tensor = aten::conv2d(%input.32, %1416, %1417, %1418, %1419, %1420, %12) # torch/nn/modules/conv.py:415:15
  %1422 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1162)
  %1423 : int = aten::dim(%out.37) # torch/nn/modules/batchnorm.py:276:11
  %1424 : bool = aten::ne(%1423, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1424) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1422)
   = prim::If(%1425) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1426 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1422)
      %1427 : Tensor = aten::add(%1426, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1422, %1427)
      -> ()
    block1():
      -> ()
  %1428 : bool = prim::GetAttr[name="training"](%1422)
  %1429 : Tensor = prim::GetAttr[name="running_mean"](%1422)
  %1430 : Tensor = prim::GetAttr[name="running_var"](%1422)
  %1431 : Tensor = prim::GetAttr[name="weight"](%1422)
  %1432 : Tensor = prim::GetAttr[name="bias"](%1422)
   = prim::If(%1428) # torch/nn/functional.py:2011:4
    block0():
      %1433 : int[] = aten::size(%out.37) # torch/nn/functional.py:2012:27
      %size_prods.40 : int = aten::__getitem__(%1433, %8) # torch/nn/functional.py:1991:17
      %1435 : int = aten::len(%1433) # torch/nn/functional.py:1992:19
      %1436 : int = aten::sub(%1435, %10) # torch/nn/functional.py:1992:19
      %size_prods.41 : int = prim::Loop(%1436, %9, %size_prods.40) # torch/nn/functional.py:1992:4
        block0(%i.11 : int, %size_prods.42 : int):
          %1440 : int = aten::add(%i.11, %10) # torch/nn/functional.py:1993:27
          %1441 : int = aten::__getitem__(%1433, %1440) # torch/nn/functional.py:1993:22
          %size_prods.43 : int = aten::mul(%size_prods.42, %1441) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.43)
      %1443 : bool = aten::eq(%size_prods.41, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1443) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.29 : Tensor = aten::batch_norm(%out.37, %1431, %1432, %1429, %1430, %1428, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.30 : Tensor = aten::relu_(%out.29) # torch/nn/functional.py:1117:17
  %1446 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1162)
  %1447 : Tensor = prim::GetAttr[name="weight"](%1446)
  %1448 : Tensor? = prim::GetAttr[name="bias"](%1446)
  %1449 : int[] = prim::ListConstruct(%12, %12)
  %1450 : int[] = prim::ListConstruct(%12, %12)
  %1451 : int[] = prim::ListConstruct(%12, %12)
  %out.31 : Tensor = aten::conv2d(%out.30, %1447, %1448, %1449, %1450, %1451, %12) # torch/nn/modules/conv.py:415:15
  %1453 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1162)
  %1454 : int = aten::dim(%out.31) # torch/nn/modules/batchnorm.py:276:11
  %1455 : bool = aten::ne(%1454, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1455) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1456 : bool = prim::GetAttr[name="training"](%1453)
   = prim::If(%1456) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1457 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1453)
      %1458 : Tensor = aten::add(%1457, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1453, %1458)
      -> ()
    block1():
      -> ()
  %1459 : bool = prim::GetAttr[name="training"](%1453)
  %1460 : Tensor = prim::GetAttr[name="running_mean"](%1453)
  %1461 : Tensor = prim::GetAttr[name="running_var"](%1453)
  %1462 : Tensor = prim::GetAttr[name="weight"](%1453)
  %1463 : Tensor = prim::GetAttr[name="bias"](%1453)
   = prim::If(%1459) # torch/nn/functional.py:2011:4
    block0():
      %1464 : int[] = aten::size(%out.31) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%1464, %8) # torch/nn/functional.py:1991:17
      %1466 : int = aten::len(%1464) # torch/nn/functional.py:1992:19
      %1467 : int = aten::sub(%1466, %10) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%1467, %9, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %1471 : int = aten::add(%i.12, %10) # torch/nn/functional.py:1993:27
          %1472 : int = aten::__getitem__(%1464, %1471) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %1472) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.47)
      %1474 : bool = aten::eq(%size_prods.45, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1474) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.32 : Tensor = aten::batch_norm(%out.31, %1462, %1463, %1460, %1461, %1459, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.33 : Tensor = aten::relu_(%out.32) # torch/nn/functional.py:1117:17
  %1477 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1162)
  %1478 : Tensor = prim::GetAttr[name="weight"](%1477)
  %1479 : Tensor? = prim::GetAttr[name="bias"](%1477)
  %1480 : int[] = prim::ListConstruct(%12, %12)
  %1481 : int[] = prim::ListConstruct(%8, %8)
  %1482 : int[] = prim::ListConstruct(%12, %12)
  %out.34 : Tensor = aten::conv2d(%out.33, %1478, %1479, %1480, %1481, %1482, %12) # torch/nn/modules/conv.py:415:15
  %1484 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1162)
  %1485 : int = aten::dim(%out.34) # torch/nn/modules/batchnorm.py:276:11
  %1486 : bool = aten::ne(%1485, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1486) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1487 : bool = prim::GetAttr[name="training"](%1484)
   = prim::If(%1487) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1488 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1484)
      %1489 : Tensor = aten::add(%1488, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1484, %1489)
      -> ()
    block1():
      -> ()
  %1490 : bool = prim::GetAttr[name="training"](%1484)
  %1491 : Tensor = prim::GetAttr[name="running_mean"](%1484)
  %1492 : Tensor = prim::GetAttr[name="running_var"](%1484)
  %1493 : Tensor = prim::GetAttr[name="weight"](%1484)
  %1494 : Tensor = prim::GetAttr[name="bias"](%1484)
   = prim::If(%1490) # torch/nn/functional.py:2011:4
    block0():
      %1495 : int[] = aten::size(%out.34) # torch/nn/functional.py:2012:27
      %size_prods.48 : int = aten::__getitem__(%1495, %8) # torch/nn/functional.py:1991:17
      %1497 : int = aten::len(%1495) # torch/nn/functional.py:1992:19
      %1498 : int = aten::sub(%1497, %10) # torch/nn/functional.py:1992:19
      %size_prods.49 : int = prim::Loop(%1498, %9, %size_prods.48) # torch/nn/functional.py:1992:4
        block0(%i.13 : int, %size_prods.50 : int):
          %1502 : int = aten::add(%i.13, %10) # torch/nn/functional.py:1993:27
          %1503 : int = aten::__getitem__(%1495, %1502) # torch/nn/functional.py:1993:22
          %size_prods.51 : int = aten::mul(%size_prods.50, %1503) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.51)
      %1505 : bool = aten::eq(%size_prods.49, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1505) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.35 : Tensor = aten::batch_norm(%out.34, %1493, %1494, %1491, %1492, %1490, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.36 : Tensor = aten::add_(%out.35, %input.32, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.34 : Tensor = aten::relu_(%out.36) # torch/nn/functional.py:1117:17
  %1509 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1163)
  %1510 : Tensor = prim::GetAttr[name="weight"](%1509)
  %1511 : Tensor? = prim::GetAttr[name="bias"](%1509)
  %1512 : int[] = prim::ListConstruct(%12, %12)
  %1513 : int[] = prim::ListConstruct(%8, %8)
  %1514 : int[] = prim::ListConstruct(%12, %12)
  %out.46 : Tensor = aten::conv2d(%input.34, %1510, %1511, %1512, %1513, %1514, %12) # torch/nn/modules/conv.py:415:15
  %1516 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1163)
  %1517 : int = aten::dim(%out.46) # torch/nn/modules/batchnorm.py:276:11
  %1518 : bool = aten::ne(%1517, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1518) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1519 : bool = prim::GetAttr[name="training"](%1516)
   = prim::If(%1519) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1520 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1516)
      %1521 : Tensor = aten::add(%1520, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1516, %1521)
      -> ()
    block1():
      -> ()
  %1522 : bool = prim::GetAttr[name="training"](%1516)
  %1523 : Tensor = prim::GetAttr[name="running_mean"](%1516)
  %1524 : Tensor = prim::GetAttr[name="running_var"](%1516)
  %1525 : Tensor = prim::GetAttr[name="weight"](%1516)
  %1526 : Tensor = prim::GetAttr[name="bias"](%1516)
   = prim::If(%1522) # torch/nn/functional.py:2011:4
    block0():
      %1527 : int[] = aten::size(%out.46) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%1527, %8) # torch/nn/functional.py:1991:17
      %1529 : int = aten::len(%1527) # torch/nn/functional.py:1992:19
      %1530 : int = aten::sub(%1529, %10) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%1530, %9, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %1534 : int = aten::add(%i.14, %10) # torch/nn/functional.py:1993:27
          %1535 : int = aten::__getitem__(%1527, %1534) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %1535) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.55)
      %1537 : bool = aten::eq(%size_prods.53, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1537) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.38 : Tensor = aten::batch_norm(%out.46, %1525, %1526, %1523, %1524, %1522, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.39 : Tensor = aten::relu_(%out.38) # torch/nn/functional.py:1117:17
  %1540 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1163)
  %1541 : Tensor = prim::GetAttr[name="weight"](%1540)
  %1542 : Tensor? = prim::GetAttr[name="bias"](%1540)
  %1543 : int[] = prim::ListConstruct(%12, %12)
  %1544 : int[] = prim::ListConstruct(%12, %12)
  %1545 : int[] = prim::ListConstruct(%12, %12)
  %out.40 : Tensor = aten::conv2d(%out.39, %1541, %1542, %1543, %1544, %1545, %12) # torch/nn/modules/conv.py:415:15
  %1547 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1163)
  %1548 : int = aten::dim(%out.40) # torch/nn/modules/batchnorm.py:276:11
  %1549 : bool = aten::ne(%1548, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1549) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1550 : bool = prim::GetAttr[name="training"](%1547)
   = prim::If(%1550) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1551 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1547)
      %1552 : Tensor = aten::add(%1551, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1547, %1552)
      -> ()
    block1():
      -> ()
  %1553 : bool = prim::GetAttr[name="training"](%1547)
  %1554 : Tensor = prim::GetAttr[name="running_mean"](%1547)
  %1555 : Tensor = prim::GetAttr[name="running_var"](%1547)
  %1556 : Tensor = prim::GetAttr[name="weight"](%1547)
  %1557 : Tensor = prim::GetAttr[name="bias"](%1547)
   = prim::If(%1553) # torch/nn/functional.py:2011:4
    block0():
      %1558 : int[] = aten::size(%out.40) # torch/nn/functional.py:2012:27
      %size_prods.56 : int = aten::__getitem__(%1558, %8) # torch/nn/functional.py:1991:17
      %1560 : int = aten::len(%1558) # torch/nn/functional.py:1992:19
      %1561 : int = aten::sub(%1560, %10) # torch/nn/functional.py:1992:19
      %size_prods.57 : int = prim::Loop(%1561, %9, %size_prods.56) # torch/nn/functional.py:1992:4
        block0(%i.15 : int, %size_prods.58 : int):
          %1565 : int = aten::add(%i.15, %10) # torch/nn/functional.py:1993:27
          %1566 : int = aten::__getitem__(%1558, %1565) # torch/nn/functional.py:1993:22
          %size_prods.59 : int = aten::mul(%size_prods.58, %1566) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.59)
      %1568 : bool = aten::eq(%size_prods.57, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1568) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.41 : Tensor = aten::batch_norm(%out.40, %1556, %1557, %1554, %1555, %1553, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.42 : Tensor = aten::relu_(%out.41) # torch/nn/functional.py:1117:17
  %1571 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1163)
  %1572 : Tensor = prim::GetAttr[name="weight"](%1571)
  %1573 : Tensor? = prim::GetAttr[name="bias"](%1571)
  %1574 : int[] = prim::ListConstruct(%12, %12)
  %1575 : int[] = prim::ListConstruct(%8, %8)
  %1576 : int[] = prim::ListConstruct(%12, %12)
  %out.43 : Tensor = aten::conv2d(%out.42, %1572, %1573, %1574, %1575, %1576, %12) # torch/nn/modules/conv.py:415:15
  %1578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1163)
  %1579 : int = aten::dim(%out.43) # torch/nn/modules/batchnorm.py:276:11
  %1580 : bool = aten::ne(%1579, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1580) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1581 : bool = prim::GetAttr[name="training"](%1578)
   = prim::If(%1581) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1578)
      %1583 : Tensor = aten::add(%1582, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1578, %1583)
      -> ()
    block1():
      -> ()
  %1584 : bool = prim::GetAttr[name="training"](%1578)
  %1585 : Tensor = prim::GetAttr[name="running_mean"](%1578)
  %1586 : Tensor = prim::GetAttr[name="running_var"](%1578)
  %1587 : Tensor = prim::GetAttr[name="weight"](%1578)
  %1588 : Tensor = prim::GetAttr[name="bias"](%1578)
   = prim::If(%1584) # torch/nn/functional.py:2011:4
    block0():
      %1589 : int[] = aten::size(%out.43) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%1589, %8) # torch/nn/functional.py:1991:17
      %1591 : int = aten::len(%1589) # torch/nn/functional.py:1992:19
      %1592 : int = aten::sub(%1591, %10) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%1592, %9, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %1596 : int = aten::add(%i.16, %10) # torch/nn/functional.py:1993:27
          %1597 : int = aten::__getitem__(%1589, %1596) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %1597) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.63)
      %1599 : bool = aten::eq(%size_prods.61, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1599) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.44 : Tensor = aten::batch_norm(%out.43, %1587, %1588, %1585, %1586, %1584, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.45 : Tensor = aten::add_(%out.44, %input.34, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.9 : Tensor = aten::relu_(%out.45) # torch/nn/functional.py:1117:17
  %1603 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1164)
  %1604 : Tensor = prim::GetAttr[name="weight"](%1603)
  %1605 : Tensor? = prim::GetAttr[name="bias"](%1603)
  %1606 : int[] = prim::ListConstruct(%12, %12)
  %1607 : int[] = prim::ListConstruct(%8, %8)
  %1608 : int[] = prim::ListConstruct(%12, %12)
  %out.55 : Tensor = aten::conv2d(%input.9, %1604, %1605, %1606, %1607, %1608, %12) # torch/nn/modules/conv.py:415:15
  %1610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1164)
  %1611 : int = aten::dim(%out.55) # torch/nn/modules/batchnorm.py:276:11
  %1612 : bool = aten::ne(%1611, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1613 : bool = prim::GetAttr[name="training"](%1610)
   = prim::If(%1613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1610)
      %1615 : Tensor = aten::add(%1614, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1610, %1615)
      -> ()
    block1():
      -> ()
  %1616 : bool = prim::GetAttr[name="training"](%1610)
  %1617 : Tensor = prim::GetAttr[name="running_mean"](%1610)
  %1618 : Tensor = prim::GetAttr[name="running_var"](%1610)
  %1619 : Tensor = prim::GetAttr[name="weight"](%1610)
  %1620 : Tensor = prim::GetAttr[name="bias"](%1610)
   = prim::If(%1616) # torch/nn/functional.py:2011:4
    block0():
      %1621 : int[] = aten::size(%out.55) # torch/nn/functional.py:2012:27
      %size_prods.64 : int = aten::__getitem__(%1621, %8) # torch/nn/functional.py:1991:17
      %1623 : int = aten::len(%1621) # torch/nn/functional.py:1992:19
      %1624 : int = aten::sub(%1623, %10) # torch/nn/functional.py:1992:19
      %size_prods.65 : int = prim::Loop(%1624, %9, %size_prods.64) # torch/nn/functional.py:1992:4
        block0(%i.17 : int, %size_prods.66 : int):
          %1628 : int = aten::add(%i.17, %10) # torch/nn/functional.py:1993:27
          %1629 : int = aten::__getitem__(%1621, %1628) # torch/nn/functional.py:1993:22
          %size_prods.67 : int = aten::mul(%size_prods.66, %1629) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.67)
      %1631 : bool = aten::eq(%size_prods.65, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.47 : Tensor = aten::batch_norm(%out.55, %1619, %1620, %1617, %1618, %1616, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.48 : Tensor = aten::relu_(%out.47) # torch/nn/functional.py:1117:17
  %1634 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1164)
  %1635 : Tensor = prim::GetAttr[name="weight"](%1634)
  %1636 : Tensor? = prim::GetAttr[name="bias"](%1634)
  %1637 : int[] = prim::ListConstruct(%12, %12)
  %1638 : int[] = prim::ListConstruct(%12, %12)
  %1639 : int[] = prim::ListConstruct(%12, %12)
  %out.49 : Tensor = aten::conv2d(%out.48, %1635, %1636, %1637, %1638, %1639, %12) # torch/nn/modules/conv.py:415:15
  %1641 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1164)
  %1642 : int = aten::dim(%out.49) # torch/nn/modules/batchnorm.py:276:11
  %1643 : bool = aten::ne(%1642, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1643) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1644 : bool = prim::GetAttr[name="training"](%1641)
   = prim::If(%1644) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1645 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1641)
      %1646 : Tensor = aten::add(%1645, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1641, %1646)
      -> ()
    block1():
      -> ()
  %1647 : bool = prim::GetAttr[name="training"](%1641)
  %1648 : Tensor = prim::GetAttr[name="running_mean"](%1641)
  %1649 : Tensor = prim::GetAttr[name="running_var"](%1641)
  %1650 : Tensor = prim::GetAttr[name="weight"](%1641)
  %1651 : Tensor = prim::GetAttr[name="bias"](%1641)
   = prim::If(%1647) # torch/nn/functional.py:2011:4
    block0():
      %1652 : int[] = aten::size(%out.49) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%1652, %8) # torch/nn/functional.py:1991:17
      %1654 : int = aten::len(%1652) # torch/nn/functional.py:1992:19
      %1655 : int = aten::sub(%1654, %10) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%1655, %9, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %1659 : int = aten::add(%i.18, %10) # torch/nn/functional.py:1993:27
          %1660 : int = aten::__getitem__(%1652, %1659) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %1660) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.71)
      %1662 : bool = aten::eq(%size_prods.69, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1662) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.50 : Tensor = aten::batch_norm(%out.49, %1650, %1651, %1648, %1649, %1647, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.51 : Tensor = aten::relu_(%out.50) # torch/nn/functional.py:1117:17
  %1665 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1164)
  %1666 : Tensor = prim::GetAttr[name="weight"](%1665)
  %1667 : Tensor? = prim::GetAttr[name="bias"](%1665)
  %1668 : int[] = prim::ListConstruct(%12, %12)
  %1669 : int[] = prim::ListConstruct(%8, %8)
  %1670 : int[] = prim::ListConstruct(%12, %12)
  %out.52 : Tensor = aten::conv2d(%out.51, %1666, %1667, %1668, %1669, %1670, %12) # torch/nn/modules/conv.py:415:15
  %1672 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1164)
  %1673 : int = aten::dim(%out.52) # torch/nn/modules/batchnorm.py:276:11
  %1674 : bool = aten::ne(%1673, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1674) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1675 : bool = prim::GetAttr[name="training"](%1672)
   = prim::If(%1675) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1676 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1672)
      %1677 : Tensor = aten::add(%1676, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1672, %1677)
      -> ()
    block1():
      -> ()
  %1678 : bool = prim::GetAttr[name="training"](%1672)
  %1679 : Tensor = prim::GetAttr[name="running_mean"](%1672)
  %1680 : Tensor = prim::GetAttr[name="running_var"](%1672)
  %1681 : Tensor = prim::GetAttr[name="weight"](%1672)
  %1682 : Tensor = prim::GetAttr[name="bias"](%1672)
   = prim::If(%1678) # torch/nn/functional.py:2011:4
    block0():
      %1683 : int[] = aten::size(%out.52) # torch/nn/functional.py:2012:27
      %size_prods.72 : int = aten::__getitem__(%1683, %8) # torch/nn/functional.py:1991:17
      %1685 : int = aten::len(%1683) # torch/nn/functional.py:1992:19
      %1686 : int = aten::sub(%1685, %10) # torch/nn/functional.py:1992:19
      %size_prods.73 : int = prim::Loop(%1686, %9, %size_prods.72) # torch/nn/functional.py:1992:4
        block0(%i.19 : int, %size_prods.74 : int):
          %1690 : int = aten::add(%i.19, %10) # torch/nn/functional.py:1993:27
          %1691 : int = aten::__getitem__(%1683, %1690) # torch/nn/functional.py:1993:22
          %size_prods.75 : int = aten::mul(%size_prods.74, %1691) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.75)
      %1693 : bool = aten::eq(%size_prods.73, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1693) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.53 : Tensor = aten::batch_norm(%out.52, %1681, %1682, %1679, %1680, %1678, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.54 : Tensor = aten::add_(%out.53, %input.9, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.11 : Tensor = aten::relu_(%out.54) # torch/nn/functional.py:1117:17
  %1697 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1165)
  %1698 : Tensor = prim::GetAttr[name="weight"](%1697)
  %1699 : Tensor? = prim::GetAttr[name="bias"](%1697)
  %1700 : int[] = prim::ListConstruct(%12, %12)
  %1701 : int[] = prim::ListConstruct(%8, %8)
  %1702 : int[] = prim::ListConstruct(%12, %12)
  %out.64 : Tensor = aten::conv2d(%input.11, %1698, %1699, %1700, %1701, %1702, %12) # torch/nn/modules/conv.py:415:15
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1165)
  %1705 : int = aten::dim(%out.64) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%out.64) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%1715, %8) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %10) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%1718, %9, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %1722 : int = aten::add(%i.20, %10) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %1723) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.79)
      %1725 : bool = aten::eq(%size_prods.77, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.56 : Tensor = aten::batch_norm(%out.64, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.57 : Tensor = aten::relu_(%out.56) # torch/nn/functional.py:1117:17
  %1728 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1165)
  %1729 : Tensor = prim::GetAttr[name="weight"](%1728)
  %1730 : Tensor? = prim::GetAttr[name="bias"](%1728)
  %1731 : int[] = prim::ListConstruct(%12, %12)
  %1732 : int[] = prim::ListConstruct(%12, %12)
  %1733 : int[] = prim::ListConstruct(%12, %12)
  %out.58 : Tensor = aten::conv2d(%out.57, %1729, %1730, %1731, %1732, %1733, %12) # torch/nn/modules/conv.py:415:15
  %1735 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1165)
  %1736 : int = aten::dim(%out.58) # torch/nn/modules/batchnorm.py:276:11
  %1737 : bool = aten::ne(%1736, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1737) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1738 : bool = prim::GetAttr[name="training"](%1735)
   = prim::If(%1738) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1739 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1735)
      %1740 : Tensor = aten::add(%1739, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1735, %1740)
      -> ()
    block1():
      -> ()
  %1741 : bool = prim::GetAttr[name="training"](%1735)
  %1742 : Tensor = prim::GetAttr[name="running_mean"](%1735)
  %1743 : Tensor = prim::GetAttr[name="running_var"](%1735)
  %1744 : Tensor = prim::GetAttr[name="weight"](%1735)
  %1745 : Tensor = prim::GetAttr[name="bias"](%1735)
   = prim::If(%1741) # torch/nn/functional.py:2011:4
    block0():
      %1746 : int[] = aten::size(%out.58) # torch/nn/functional.py:2012:27
      %size_prods.80 : int = aten::__getitem__(%1746, %8) # torch/nn/functional.py:1991:17
      %1748 : int = aten::len(%1746) # torch/nn/functional.py:1992:19
      %1749 : int = aten::sub(%1748, %10) # torch/nn/functional.py:1992:19
      %size_prods.81 : int = prim::Loop(%1749, %9, %size_prods.80) # torch/nn/functional.py:1992:4
        block0(%i.21 : int, %size_prods.82 : int):
          %1753 : int = aten::add(%i.21, %10) # torch/nn/functional.py:1993:27
          %1754 : int = aten::__getitem__(%1746, %1753) # torch/nn/functional.py:1993:22
          %size_prods.83 : int = aten::mul(%size_prods.82, %1754) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.83)
      %1756 : bool = aten::eq(%size_prods.81, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1756) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.59 : Tensor = aten::batch_norm(%out.58, %1744, %1745, %1742, %1743, %1741, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.60 : Tensor = aten::relu_(%out.59) # torch/nn/functional.py:1117:17
  %1759 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1165)
  %1760 : Tensor = prim::GetAttr[name="weight"](%1759)
  %1761 : Tensor? = prim::GetAttr[name="bias"](%1759)
  %1762 : int[] = prim::ListConstruct(%12, %12)
  %1763 : int[] = prim::ListConstruct(%8, %8)
  %1764 : int[] = prim::ListConstruct(%12, %12)
  %out.61 : Tensor = aten::conv2d(%out.60, %1760, %1761, %1762, %1763, %1764, %12) # torch/nn/modules/conv.py:415:15
  %1766 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1165)
  %1767 : int = aten::dim(%out.61) # torch/nn/modules/batchnorm.py:276:11
  %1768 : bool = aten::ne(%1767, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1768) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1769 : bool = prim::GetAttr[name="training"](%1766)
   = prim::If(%1769) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1770 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1766)
      %1771 : Tensor = aten::add(%1770, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1766, %1771)
      -> ()
    block1():
      -> ()
  %1772 : bool = prim::GetAttr[name="training"](%1766)
  %1773 : Tensor = prim::GetAttr[name="running_mean"](%1766)
  %1774 : Tensor = prim::GetAttr[name="running_var"](%1766)
  %1775 : Tensor = prim::GetAttr[name="weight"](%1766)
  %1776 : Tensor = prim::GetAttr[name="bias"](%1766)
   = prim::If(%1772) # torch/nn/functional.py:2011:4
    block0():
      %1777 : int[] = aten::size(%out.61) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%1777, %8) # torch/nn/functional.py:1991:17
      %1779 : int = aten::len(%1777) # torch/nn/functional.py:1992:19
      %1780 : int = aten::sub(%1779, %10) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%1780, %9, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %1784 : int = aten::add(%i.22, %10) # torch/nn/functional.py:1993:27
          %1785 : int = aten::__getitem__(%1777, %1784) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %1785) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.87)
      %1787 : bool = aten::eq(%size_prods.85, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1787) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.62 : Tensor = aten::batch_norm(%out.61, %1775, %1776, %1773, %1774, %1772, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.63 : Tensor = aten::add_(%out.62, %input.11, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.13 : Tensor = aten::relu_(%out.63) # torch/nn/functional.py:1117:17
  %1791 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1166)
  %1792 : Tensor = prim::GetAttr[name="weight"](%1791)
  %1793 : Tensor? = prim::GetAttr[name="bias"](%1791)
  %1794 : int[] = prim::ListConstruct(%12, %12)
  %1795 : int[] = prim::ListConstruct(%8, %8)
  %1796 : int[] = prim::ListConstruct(%12, %12)
  %out.73 : Tensor = aten::conv2d(%input.13, %1792, %1793, %1794, %1795, %1796, %12) # torch/nn/modules/conv.py:415:15
  %1798 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1166)
  %1799 : int = aten::dim(%out.73) # torch/nn/modules/batchnorm.py:276:11
  %1800 : bool = aten::ne(%1799, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1800) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1801 : bool = prim::GetAttr[name="training"](%1798)
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1802 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1798)
      %1803 : Tensor = aten::add(%1802, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1798, %1803)
      -> ()
    block1():
      -> ()
  %1804 : bool = prim::GetAttr[name="training"](%1798)
  %1805 : Tensor = prim::GetAttr[name="running_mean"](%1798)
  %1806 : Tensor = prim::GetAttr[name="running_var"](%1798)
  %1807 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1808 : Tensor = prim::GetAttr[name="bias"](%1798)
   = prim::If(%1804) # torch/nn/functional.py:2011:4
    block0():
      %1809 : int[] = aten::size(%out.73) # torch/nn/functional.py:2012:27
      %size_prods.88 : int = aten::__getitem__(%1809, %8) # torch/nn/functional.py:1991:17
      %1811 : int = aten::len(%1809) # torch/nn/functional.py:1992:19
      %1812 : int = aten::sub(%1811, %10) # torch/nn/functional.py:1992:19
      %size_prods.89 : int = prim::Loop(%1812, %9, %size_prods.88) # torch/nn/functional.py:1992:4
        block0(%i.23 : int, %size_prods.90 : int):
          %1816 : int = aten::add(%i.23, %10) # torch/nn/functional.py:1993:27
          %1817 : int = aten::__getitem__(%1809, %1816) # torch/nn/functional.py:1993:22
          %size_prods.91 : int = aten::mul(%size_prods.90, %1817) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.91)
      %1819 : bool = aten::eq(%size_prods.89, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1819) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.65 : Tensor = aten::batch_norm(%out.73, %1807, %1808, %1805, %1806, %1804, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.66 : Tensor = aten::relu_(%out.65) # torch/nn/functional.py:1117:17
  %1822 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1166)
  %1823 : Tensor = prim::GetAttr[name="weight"](%1822)
  %1824 : Tensor? = prim::GetAttr[name="bias"](%1822)
  %1825 : int[] = prim::ListConstruct(%12, %12)
  %1826 : int[] = prim::ListConstruct(%12, %12)
  %1827 : int[] = prim::ListConstruct(%12, %12)
  %out.67 : Tensor = aten::conv2d(%out.66, %1823, %1824, %1825, %1826, %1827, %12) # torch/nn/modules/conv.py:415:15
  %1829 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1166)
  %1830 : int = aten::dim(%out.67) # torch/nn/modules/batchnorm.py:276:11
  %1831 : bool = aten::ne(%1830, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1831) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1832 : bool = prim::GetAttr[name="training"](%1829)
   = prim::If(%1832) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1833 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1829)
      %1834 : Tensor = aten::add(%1833, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1829, %1834)
      -> ()
    block1():
      -> ()
  %1835 : bool = prim::GetAttr[name="training"](%1829)
  %1836 : Tensor = prim::GetAttr[name="running_mean"](%1829)
  %1837 : Tensor = prim::GetAttr[name="running_var"](%1829)
  %1838 : Tensor = prim::GetAttr[name="weight"](%1829)
  %1839 : Tensor = prim::GetAttr[name="bias"](%1829)
   = prim::If(%1835) # torch/nn/functional.py:2011:4
    block0():
      %1840 : int[] = aten::size(%out.67) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%1840, %8) # torch/nn/functional.py:1991:17
      %1842 : int = aten::len(%1840) # torch/nn/functional.py:1992:19
      %1843 : int = aten::sub(%1842, %10) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%1843, %9, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %1847 : int = aten::add(%i.24, %10) # torch/nn/functional.py:1993:27
          %1848 : int = aten::__getitem__(%1840, %1847) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %1848) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.95)
      %1850 : bool = aten::eq(%size_prods.93, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1850) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.68 : Tensor = aten::batch_norm(%out.67, %1838, %1839, %1836, %1837, %1835, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.69 : Tensor = aten::relu_(%out.68) # torch/nn/functional.py:1117:17
  %1853 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1166)
  %1854 : Tensor = prim::GetAttr[name="weight"](%1853)
  %1855 : Tensor? = prim::GetAttr[name="bias"](%1853)
  %1856 : int[] = prim::ListConstruct(%12, %12)
  %1857 : int[] = prim::ListConstruct(%8, %8)
  %1858 : int[] = prim::ListConstruct(%12, %12)
  %out.70 : Tensor = aten::conv2d(%out.69, %1854, %1855, %1856, %1857, %1858, %12) # torch/nn/modules/conv.py:415:15
  %1860 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1166)
  %1861 : int = aten::dim(%out.70) # torch/nn/modules/batchnorm.py:276:11
  %1862 : bool = aten::ne(%1861, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1862) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1863 : bool = prim::GetAttr[name="training"](%1860)
   = prim::If(%1863) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1864 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1860)
      %1865 : Tensor = aten::add(%1864, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1860, %1865)
      -> ()
    block1():
      -> ()
  %1866 : bool = prim::GetAttr[name="training"](%1860)
  %1867 : Tensor = prim::GetAttr[name="running_mean"](%1860)
  %1868 : Tensor = prim::GetAttr[name="running_var"](%1860)
  %1869 : Tensor = prim::GetAttr[name="weight"](%1860)
  %1870 : Tensor = prim::GetAttr[name="bias"](%1860)
   = prim::If(%1866) # torch/nn/functional.py:2011:4
    block0():
      %1871 : int[] = aten::size(%out.70) # torch/nn/functional.py:2012:27
      %size_prods.96 : int = aten::__getitem__(%1871, %8) # torch/nn/functional.py:1991:17
      %1873 : int = aten::len(%1871) # torch/nn/functional.py:1992:19
      %1874 : int = aten::sub(%1873, %10) # torch/nn/functional.py:1992:19
      %size_prods.97 : int = prim::Loop(%1874, %9, %size_prods.96) # torch/nn/functional.py:1992:4
        block0(%i.25 : int, %size_prods.98 : int):
          %1878 : int = aten::add(%i.25, %10) # torch/nn/functional.py:1993:27
          %1879 : int = aten::__getitem__(%1871, %1878) # torch/nn/functional.py:1993:22
          %size_prods.99 : int = aten::mul(%size_prods.98, %1879) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.99)
      %1881 : bool = aten::eq(%size_prods.97, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1881) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.71 : Tensor = aten::batch_norm(%out.70, %1869, %1870, %1867, %1868, %1866, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.72 : Tensor = aten::add_(%out.71, %input.13, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.15 : Tensor = aten::relu_(%out.72) # torch/nn/functional.py:1117:17
  %1885 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1167)
  %1886 : Tensor = prim::GetAttr[name="weight"](%1885)
  %1887 : Tensor? = prim::GetAttr[name="bias"](%1885)
  %1888 : int[] = prim::ListConstruct(%12, %12)
  %1889 : int[] = prim::ListConstruct(%8, %8)
  %1890 : int[] = prim::ListConstruct(%12, %12)
  %out.82 : Tensor = aten::conv2d(%input.15, %1886, %1887, %1888, %1889, %1890, %12) # torch/nn/modules/conv.py:415:15
  %1892 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1167)
  %1893 : int = aten::dim(%out.82) # torch/nn/modules/batchnorm.py:276:11
  %1894 : bool = aten::ne(%1893, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1894) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1895 : bool = prim::GetAttr[name="training"](%1892)
   = prim::If(%1895) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1896 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1892)
      %1897 : Tensor = aten::add(%1896, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1892, %1897)
      -> ()
    block1():
      -> ()
  %1898 : bool = prim::GetAttr[name="training"](%1892)
  %1899 : Tensor = prim::GetAttr[name="running_mean"](%1892)
  %1900 : Tensor = prim::GetAttr[name="running_var"](%1892)
  %1901 : Tensor = prim::GetAttr[name="weight"](%1892)
  %1902 : Tensor = prim::GetAttr[name="bias"](%1892)
   = prim::If(%1898) # torch/nn/functional.py:2011:4
    block0():
      %1903 : int[] = aten::size(%out.82) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%1903, %8) # torch/nn/functional.py:1991:17
      %1905 : int = aten::len(%1903) # torch/nn/functional.py:1992:19
      %1906 : int = aten::sub(%1905, %10) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%1906, %9, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %1910 : int = aten::add(%i.26, %10) # torch/nn/functional.py:1993:27
          %1911 : int = aten::__getitem__(%1903, %1910) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %1911) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.103)
      %1913 : bool = aten::eq(%size_prods.101, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1913) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.74 : Tensor = aten::batch_norm(%out.82, %1901, %1902, %1899, %1900, %1898, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.75 : Tensor = aten::relu_(%out.74) # torch/nn/functional.py:1117:17
  %1916 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1167)
  %1917 : Tensor = prim::GetAttr[name="weight"](%1916)
  %1918 : Tensor? = prim::GetAttr[name="bias"](%1916)
  %1919 : int[] = prim::ListConstruct(%12, %12)
  %1920 : int[] = prim::ListConstruct(%12, %12)
  %1921 : int[] = prim::ListConstruct(%12, %12)
  %out.76 : Tensor = aten::conv2d(%out.75, %1917, %1918, %1919, %1920, %1921, %12) # torch/nn/modules/conv.py:415:15
  %1923 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1167)
  %1924 : int = aten::dim(%out.76) # torch/nn/modules/batchnorm.py:276:11
  %1925 : bool = aten::ne(%1924, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1925) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1926 : bool = prim::GetAttr[name="training"](%1923)
   = prim::If(%1926) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1927 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1923)
      %1928 : Tensor = aten::add(%1927, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1923, %1928)
      -> ()
    block1():
      -> ()
  %1929 : bool = prim::GetAttr[name="training"](%1923)
  %1930 : Tensor = prim::GetAttr[name="running_mean"](%1923)
  %1931 : Tensor = prim::GetAttr[name="running_var"](%1923)
  %1932 : Tensor = prim::GetAttr[name="weight"](%1923)
  %1933 : Tensor = prim::GetAttr[name="bias"](%1923)
   = prim::If(%1929) # torch/nn/functional.py:2011:4
    block0():
      %1934 : int[] = aten::size(%out.76) # torch/nn/functional.py:2012:27
      %size_prods.104 : int = aten::__getitem__(%1934, %8) # torch/nn/functional.py:1991:17
      %1936 : int = aten::len(%1934) # torch/nn/functional.py:1992:19
      %1937 : int = aten::sub(%1936, %10) # torch/nn/functional.py:1992:19
      %size_prods.105 : int = prim::Loop(%1937, %9, %size_prods.104) # torch/nn/functional.py:1992:4
        block0(%i.27 : int, %size_prods.106 : int):
          %1941 : int = aten::add(%i.27, %10) # torch/nn/functional.py:1993:27
          %1942 : int = aten::__getitem__(%1934, %1941) # torch/nn/functional.py:1993:22
          %size_prods.107 : int = aten::mul(%size_prods.106, %1942) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.107)
      %1944 : bool = aten::eq(%size_prods.105, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1944) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.77 : Tensor = aten::batch_norm(%out.76, %1932, %1933, %1930, %1931, %1929, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.78 : Tensor = aten::relu_(%out.77) # torch/nn/functional.py:1117:17
  %1947 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1167)
  %1948 : Tensor = prim::GetAttr[name="weight"](%1947)
  %1949 : Tensor? = prim::GetAttr[name="bias"](%1947)
  %1950 : int[] = prim::ListConstruct(%12, %12)
  %1951 : int[] = prim::ListConstruct(%8, %8)
  %1952 : int[] = prim::ListConstruct(%12, %12)
  %out.79 : Tensor = aten::conv2d(%out.78, %1948, %1949, %1950, %1951, %1952, %12) # torch/nn/modules/conv.py:415:15
  %1954 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1167)
  %1955 : int = aten::dim(%out.79) # torch/nn/modules/batchnorm.py:276:11
  %1956 : bool = aten::ne(%1955, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1956) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1957 : bool = prim::GetAttr[name="training"](%1954)
   = prim::If(%1957) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1958 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1954)
      %1959 : Tensor = aten::add(%1958, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1954, %1959)
      -> ()
    block1():
      -> ()
  %1960 : bool = prim::GetAttr[name="training"](%1954)
  %1961 : Tensor = prim::GetAttr[name="running_mean"](%1954)
  %1962 : Tensor = prim::GetAttr[name="running_var"](%1954)
  %1963 : Tensor = prim::GetAttr[name="weight"](%1954)
  %1964 : Tensor = prim::GetAttr[name="bias"](%1954)
   = prim::If(%1960) # torch/nn/functional.py:2011:4
    block0():
      %1965 : int[] = aten::size(%out.79) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%1965, %8) # torch/nn/functional.py:1991:17
      %1967 : int = aten::len(%1965) # torch/nn/functional.py:1992:19
      %1968 : int = aten::sub(%1967, %10) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%1968, %9, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %1972 : int = aten::add(%i.28, %10) # torch/nn/functional.py:1993:27
          %1973 : int = aten::__getitem__(%1965, %1972) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %1973) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.111)
      %1975 : bool = aten::eq(%size_prods.109, %12) # torch/nn/functional.py:1994:7
       = prim::If(%1975) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.80 : Tensor = aten::batch_norm(%out.79, %1963, %1964, %1961, %1962, %1960, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.81 : Tensor = aten::add_(%out.80, %input.15, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.17 : Tensor = aten::relu_(%out.81) # torch/nn/functional.py:1117:17
  %1979 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1168)
  %1980 : Tensor = prim::GetAttr[name="weight"](%1979)
  %1981 : Tensor? = prim::GetAttr[name="bias"](%1979)
  %1982 : int[] = prim::ListConstruct(%12, %12)
  %1983 : int[] = prim::ListConstruct(%8, %8)
  %1984 : int[] = prim::ListConstruct(%12, %12)
  %out.91 : Tensor = aten::conv2d(%input.17, %1980, %1981, %1982, %1983, %1984, %12) # torch/nn/modules/conv.py:415:15
  %1986 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1168)
  %1987 : int = aten::dim(%out.91) # torch/nn/modules/batchnorm.py:276:11
  %1988 : bool = aten::ne(%1987, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1988) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1989 : bool = prim::GetAttr[name="training"](%1986)
   = prim::If(%1989) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1990 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1986)
      %1991 : Tensor = aten::add(%1990, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1986, %1991)
      -> ()
    block1():
      -> ()
  %1992 : bool = prim::GetAttr[name="training"](%1986)
  %1993 : Tensor = prim::GetAttr[name="running_mean"](%1986)
  %1994 : Tensor = prim::GetAttr[name="running_var"](%1986)
  %1995 : Tensor = prim::GetAttr[name="weight"](%1986)
  %1996 : Tensor = prim::GetAttr[name="bias"](%1986)
   = prim::If(%1992) # torch/nn/functional.py:2011:4
    block0():
      %1997 : int[] = aten::size(%out.91) # torch/nn/functional.py:2012:27
      %size_prods.112 : int = aten::__getitem__(%1997, %8) # torch/nn/functional.py:1991:17
      %1999 : int = aten::len(%1997) # torch/nn/functional.py:1992:19
      %2000 : int = aten::sub(%1999, %10) # torch/nn/functional.py:1992:19
      %size_prods.113 : int = prim::Loop(%2000, %9, %size_prods.112) # torch/nn/functional.py:1992:4
        block0(%i.29 : int, %size_prods.114 : int):
          %2004 : int = aten::add(%i.29, %10) # torch/nn/functional.py:1993:27
          %2005 : int = aten::__getitem__(%1997, %2004) # torch/nn/functional.py:1993:22
          %size_prods.115 : int = aten::mul(%size_prods.114, %2005) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.115)
      %2007 : bool = aten::eq(%size_prods.113, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2007) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.83 : Tensor = aten::batch_norm(%out.91, %1995, %1996, %1993, %1994, %1992, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.84 : Tensor = aten::relu_(%out.83) # torch/nn/functional.py:1117:17
  %2010 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1168)
  %2011 : Tensor = prim::GetAttr[name="weight"](%2010)
  %2012 : Tensor? = prim::GetAttr[name="bias"](%2010)
  %2013 : int[] = prim::ListConstruct(%12, %12)
  %2014 : int[] = prim::ListConstruct(%12, %12)
  %2015 : int[] = prim::ListConstruct(%12, %12)
  %out.85 : Tensor = aten::conv2d(%out.84, %2011, %2012, %2013, %2014, %2015, %12) # torch/nn/modules/conv.py:415:15
  %2017 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1168)
  %2018 : int = aten::dim(%out.85) # torch/nn/modules/batchnorm.py:276:11
  %2019 : bool = aten::ne(%2018, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2019) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2020 : bool = prim::GetAttr[name="training"](%2017)
   = prim::If(%2020) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2021 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2017)
      %2022 : Tensor = aten::add(%2021, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2017, %2022)
      -> ()
    block1():
      -> ()
  %2023 : bool = prim::GetAttr[name="training"](%2017)
  %2024 : Tensor = prim::GetAttr[name="running_mean"](%2017)
  %2025 : Tensor = prim::GetAttr[name="running_var"](%2017)
  %2026 : Tensor = prim::GetAttr[name="weight"](%2017)
  %2027 : Tensor = prim::GetAttr[name="bias"](%2017)
   = prim::If(%2023) # torch/nn/functional.py:2011:4
    block0():
      %2028 : int[] = aten::size(%out.85) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%2028, %8) # torch/nn/functional.py:1991:17
      %2030 : int = aten::len(%2028) # torch/nn/functional.py:1992:19
      %2031 : int = aten::sub(%2030, %10) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%2031, %9, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %2035 : int = aten::add(%i.30, %10) # torch/nn/functional.py:1993:27
          %2036 : int = aten::__getitem__(%2028, %2035) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %2036) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.119)
      %2038 : bool = aten::eq(%size_prods.117, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2038) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.86 : Tensor = aten::batch_norm(%out.85, %2026, %2027, %2024, %2025, %2023, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.87 : Tensor = aten::relu_(%out.86) # torch/nn/functional.py:1117:17
  %2041 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1168)
  %2042 : Tensor = prim::GetAttr[name="weight"](%2041)
  %2043 : Tensor? = prim::GetAttr[name="bias"](%2041)
  %2044 : int[] = prim::ListConstruct(%12, %12)
  %2045 : int[] = prim::ListConstruct(%8, %8)
  %2046 : int[] = prim::ListConstruct(%12, %12)
  %out.88 : Tensor = aten::conv2d(%out.87, %2042, %2043, %2044, %2045, %2046, %12) # torch/nn/modules/conv.py:415:15
  %2048 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1168)
  %2049 : int = aten::dim(%out.88) # torch/nn/modules/batchnorm.py:276:11
  %2050 : bool = aten::ne(%2049, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2050) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2051 : bool = prim::GetAttr[name="training"](%2048)
   = prim::If(%2051) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2052 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2048)
      %2053 : Tensor = aten::add(%2052, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2048, %2053)
      -> ()
    block1():
      -> ()
  %2054 : bool = prim::GetAttr[name="training"](%2048)
  %2055 : Tensor = prim::GetAttr[name="running_mean"](%2048)
  %2056 : Tensor = prim::GetAttr[name="running_var"](%2048)
  %2057 : Tensor = prim::GetAttr[name="weight"](%2048)
  %2058 : Tensor = prim::GetAttr[name="bias"](%2048)
   = prim::If(%2054) # torch/nn/functional.py:2011:4
    block0():
      %2059 : int[] = aten::size(%out.88) # torch/nn/functional.py:2012:27
      %size_prods.120 : int = aten::__getitem__(%2059, %8) # torch/nn/functional.py:1991:17
      %2061 : int = aten::len(%2059) # torch/nn/functional.py:1992:19
      %2062 : int = aten::sub(%2061, %10) # torch/nn/functional.py:1992:19
      %size_prods.121 : int = prim::Loop(%2062, %9, %size_prods.120) # torch/nn/functional.py:1992:4
        block0(%i.31 : int, %size_prods.122 : int):
          %2066 : int = aten::add(%i.31, %10) # torch/nn/functional.py:1993:27
          %2067 : int = aten::__getitem__(%2059, %2066) # torch/nn/functional.py:1993:22
          %size_prods.123 : int = aten::mul(%size_prods.122, %2067) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.123)
      %2069 : bool = aten::eq(%size_prods.121, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2069) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.89 : Tensor = aten::batch_norm(%out.88, %2057, %2058, %2055, %2056, %2054, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.90 : Tensor = aten::add_(%out.89, %input.17, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.19 : Tensor = aten::relu_(%out.90) # torch/nn/functional.py:1117:17
  %2073 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1169)
  %2074 : Tensor = prim::GetAttr[name="weight"](%2073)
  %2075 : Tensor? = prim::GetAttr[name="bias"](%2073)
  %2076 : int[] = prim::ListConstruct(%12, %12)
  %2077 : int[] = prim::ListConstruct(%8, %8)
  %2078 : int[] = prim::ListConstruct(%12, %12)
  %out.100 : Tensor = aten::conv2d(%input.19, %2074, %2075, %2076, %2077, %2078, %12) # torch/nn/modules/conv.py:415:15
  %2080 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1169)
  %2081 : int = aten::dim(%out.100) # torch/nn/modules/batchnorm.py:276:11
  %2082 : bool = aten::ne(%2081, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2082) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2083 : bool = prim::GetAttr[name="training"](%2080)
   = prim::If(%2083) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2084 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2080)
      %2085 : Tensor = aten::add(%2084, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2080, %2085)
      -> ()
    block1():
      -> ()
  %2086 : bool = prim::GetAttr[name="training"](%2080)
  %2087 : Tensor = prim::GetAttr[name="running_mean"](%2080)
  %2088 : Tensor = prim::GetAttr[name="running_var"](%2080)
  %2089 : Tensor = prim::GetAttr[name="weight"](%2080)
  %2090 : Tensor = prim::GetAttr[name="bias"](%2080)
   = prim::If(%2086) # torch/nn/functional.py:2011:4
    block0():
      %2091 : int[] = aten::size(%out.100) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%2091, %8) # torch/nn/functional.py:1991:17
      %2093 : int = aten::len(%2091) # torch/nn/functional.py:1992:19
      %2094 : int = aten::sub(%2093, %10) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%2094, %9, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %2098 : int = aten::add(%i.32, %10) # torch/nn/functional.py:1993:27
          %2099 : int = aten::__getitem__(%2091, %2098) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %2099) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.127)
      %2101 : bool = aten::eq(%size_prods.125, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2101) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.92 : Tensor = aten::batch_norm(%out.100, %2089, %2090, %2087, %2088, %2086, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.93 : Tensor = aten::relu_(%out.92) # torch/nn/functional.py:1117:17
  %2104 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1169)
  %2105 : Tensor = prim::GetAttr[name="weight"](%2104)
  %2106 : Tensor? = prim::GetAttr[name="bias"](%2104)
  %2107 : int[] = prim::ListConstruct(%12, %12)
  %2108 : int[] = prim::ListConstruct(%12, %12)
  %2109 : int[] = prim::ListConstruct(%12, %12)
  %out.94 : Tensor = aten::conv2d(%out.93, %2105, %2106, %2107, %2108, %2109, %12) # torch/nn/modules/conv.py:415:15
  %2111 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1169)
  %2112 : int = aten::dim(%out.94) # torch/nn/modules/batchnorm.py:276:11
  %2113 : bool = aten::ne(%2112, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2113) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2114 : bool = prim::GetAttr[name="training"](%2111)
   = prim::If(%2114) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2115 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2111)
      %2116 : Tensor = aten::add(%2115, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2111, %2116)
      -> ()
    block1():
      -> ()
  %2117 : bool = prim::GetAttr[name="training"](%2111)
  %2118 : Tensor = prim::GetAttr[name="running_mean"](%2111)
  %2119 : Tensor = prim::GetAttr[name="running_var"](%2111)
  %2120 : Tensor = prim::GetAttr[name="weight"](%2111)
  %2121 : Tensor = prim::GetAttr[name="bias"](%2111)
   = prim::If(%2117) # torch/nn/functional.py:2011:4
    block0():
      %2122 : int[] = aten::size(%out.94) # torch/nn/functional.py:2012:27
      %size_prods.128 : int = aten::__getitem__(%2122, %8) # torch/nn/functional.py:1991:17
      %2124 : int = aten::len(%2122) # torch/nn/functional.py:1992:19
      %2125 : int = aten::sub(%2124, %10) # torch/nn/functional.py:1992:19
      %size_prods.129 : int = prim::Loop(%2125, %9, %size_prods.128) # torch/nn/functional.py:1992:4
        block0(%i.33 : int, %size_prods.130 : int):
          %2129 : int = aten::add(%i.33, %10) # torch/nn/functional.py:1993:27
          %2130 : int = aten::__getitem__(%2122, %2129) # torch/nn/functional.py:1993:22
          %size_prods.131 : int = aten::mul(%size_prods.130, %2130) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.131)
      %2132 : bool = aten::eq(%size_prods.129, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2132) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.95 : Tensor = aten::batch_norm(%out.94, %2120, %2121, %2118, %2119, %2117, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.96 : Tensor = aten::relu_(%out.95) # torch/nn/functional.py:1117:17
  %2135 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1169)
  %2136 : Tensor = prim::GetAttr[name="weight"](%2135)
  %2137 : Tensor? = prim::GetAttr[name="bias"](%2135)
  %2138 : int[] = prim::ListConstruct(%12, %12)
  %2139 : int[] = prim::ListConstruct(%8, %8)
  %2140 : int[] = prim::ListConstruct(%12, %12)
  %out.97 : Tensor = aten::conv2d(%out.96, %2136, %2137, %2138, %2139, %2140, %12) # torch/nn/modules/conv.py:415:15
  %2142 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1169)
  %2143 : int = aten::dim(%out.97) # torch/nn/modules/batchnorm.py:276:11
  %2144 : bool = aten::ne(%2143, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2144) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2145 : bool = prim::GetAttr[name="training"](%2142)
   = prim::If(%2145) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2146 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2142)
      %2147 : Tensor = aten::add(%2146, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2142, %2147)
      -> ()
    block1():
      -> ()
  %2148 : bool = prim::GetAttr[name="training"](%2142)
  %2149 : Tensor = prim::GetAttr[name="running_mean"](%2142)
  %2150 : Tensor = prim::GetAttr[name="running_var"](%2142)
  %2151 : Tensor = prim::GetAttr[name="weight"](%2142)
  %2152 : Tensor = prim::GetAttr[name="bias"](%2142)
   = prim::If(%2148) # torch/nn/functional.py:2011:4
    block0():
      %2153 : int[] = aten::size(%out.97) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%2153, %8) # torch/nn/functional.py:1991:17
      %2155 : int = aten::len(%2153) # torch/nn/functional.py:1992:19
      %2156 : int = aten::sub(%2155, %10) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%2156, %9, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %2160 : int = aten::add(%i.34, %10) # torch/nn/functional.py:1993:27
          %2161 : int = aten::__getitem__(%2153, %2160) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %2161) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.135)
      %2163 : bool = aten::eq(%size_prods.133, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2163) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.98 : Tensor = aten::batch_norm(%out.97, %2151, %2152, %2149, %2150, %2148, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.99 : Tensor = aten::add_(%out.98, %input.19, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.21 : Tensor = aten::relu_(%out.99) # torch/nn/functional.py:1117:17
  %2167 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1170)
  %2168 : Tensor = prim::GetAttr[name="weight"](%2167)
  %2169 : Tensor? = prim::GetAttr[name="bias"](%2167)
  %2170 : int[] = prim::ListConstruct(%12, %12)
  %2171 : int[] = prim::ListConstruct(%8, %8)
  %2172 : int[] = prim::ListConstruct(%12, %12)
  %out.109 : Tensor = aten::conv2d(%input.21, %2168, %2169, %2170, %2171, %2172, %12) # torch/nn/modules/conv.py:415:15
  %2174 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1170)
  %2175 : int = aten::dim(%out.109) # torch/nn/modules/batchnorm.py:276:11
  %2176 : bool = aten::ne(%2175, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2176) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2177 : bool = prim::GetAttr[name="training"](%2174)
   = prim::If(%2177) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2178 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2174)
      %2179 : Tensor = aten::add(%2178, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2174, %2179)
      -> ()
    block1():
      -> ()
  %2180 : bool = prim::GetAttr[name="training"](%2174)
  %2181 : Tensor = prim::GetAttr[name="running_mean"](%2174)
  %2182 : Tensor = prim::GetAttr[name="running_var"](%2174)
  %2183 : Tensor = prim::GetAttr[name="weight"](%2174)
  %2184 : Tensor = prim::GetAttr[name="bias"](%2174)
   = prim::If(%2180) # torch/nn/functional.py:2011:4
    block0():
      %2185 : int[] = aten::size(%out.109) # torch/nn/functional.py:2012:27
      %size_prods.136 : int = aten::__getitem__(%2185, %8) # torch/nn/functional.py:1991:17
      %2187 : int = aten::len(%2185) # torch/nn/functional.py:1992:19
      %2188 : int = aten::sub(%2187, %10) # torch/nn/functional.py:1992:19
      %size_prods.137 : int = prim::Loop(%2188, %9, %size_prods.136) # torch/nn/functional.py:1992:4
        block0(%i.35 : int, %size_prods.138 : int):
          %2192 : int = aten::add(%i.35, %10) # torch/nn/functional.py:1993:27
          %2193 : int = aten::__getitem__(%2185, %2192) # torch/nn/functional.py:1993:22
          %size_prods.139 : int = aten::mul(%size_prods.138, %2193) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.139)
      %2195 : bool = aten::eq(%size_prods.137, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2195) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.101 : Tensor = aten::batch_norm(%out.109, %2183, %2184, %2181, %2182, %2180, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.102 : Tensor = aten::relu_(%out.101) # torch/nn/functional.py:1117:17
  %2198 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1170)
  %2199 : Tensor = prim::GetAttr[name="weight"](%2198)
  %2200 : Tensor? = prim::GetAttr[name="bias"](%2198)
  %2201 : int[] = prim::ListConstruct(%12, %12)
  %2202 : int[] = prim::ListConstruct(%12, %12)
  %2203 : int[] = prim::ListConstruct(%12, %12)
  %out.103 : Tensor = aten::conv2d(%out.102, %2199, %2200, %2201, %2202, %2203, %12) # torch/nn/modules/conv.py:415:15
  %2205 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1170)
  %2206 : int = aten::dim(%out.103) # torch/nn/modules/batchnorm.py:276:11
  %2207 : bool = aten::ne(%2206, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2207) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2208 : bool = prim::GetAttr[name="training"](%2205)
   = prim::If(%2208) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2209 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2205)
      %2210 : Tensor = aten::add(%2209, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2205, %2210)
      -> ()
    block1():
      -> ()
  %2211 : bool = prim::GetAttr[name="training"](%2205)
  %2212 : Tensor = prim::GetAttr[name="running_mean"](%2205)
  %2213 : Tensor = prim::GetAttr[name="running_var"](%2205)
  %2214 : Tensor = prim::GetAttr[name="weight"](%2205)
  %2215 : Tensor = prim::GetAttr[name="bias"](%2205)
   = prim::If(%2211) # torch/nn/functional.py:2011:4
    block0():
      %2216 : int[] = aten::size(%out.103) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%2216, %8) # torch/nn/functional.py:1991:17
      %2218 : int = aten::len(%2216) # torch/nn/functional.py:1992:19
      %2219 : int = aten::sub(%2218, %10) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%2219, %9, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %2223 : int = aten::add(%i.36, %10) # torch/nn/functional.py:1993:27
          %2224 : int = aten::__getitem__(%2216, %2223) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %2224) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.143)
      %2226 : bool = aten::eq(%size_prods.141, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2226) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.104 : Tensor = aten::batch_norm(%out.103, %2214, %2215, %2212, %2213, %2211, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.105 : Tensor = aten::relu_(%out.104) # torch/nn/functional.py:1117:17
  %2229 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1170)
  %2230 : Tensor = prim::GetAttr[name="weight"](%2229)
  %2231 : Tensor? = prim::GetAttr[name="bias"](%2229)
  %2232 : int[] = prim::ListConstruct(%12, %12)
  %2233 : int[] = prim::ListConstruct(%8, %8)
  %2234 : int[] = prim::ListConstruct(%12, %12)
  %out.106 : Tensor = aten::conv2d(%out.105, %2230, %2231, %2232, %2233, %2234, %12) # torch/nn/modules/conv.py:415:15
  %2236 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1170)
  %2237 : int = aten::dim(%out.106) # torch/nn/modules/batchnorm.py:276:11
  %2238 : bool = aten::ne(%2237, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2238) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2239 : bool = prim::GetAttr[name="training"](%2236)
   = prim::If(%2239) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2240 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2236)
      %2241 : Tensor = aten::add(%2240, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2236, %2241)
      -> ()
    block1():
      -> ()
  %2242 : bool = prim::GetAttr[name="training"](%2236)
  %2243 : Tensor = prim::GetAttr[name="running_mean"](%2236)
  %2244 : Tensor = prim::GetAttr[name="running_var"](%2236)
  %2245 : Tensor = prim::GetAttr[name="weight"](%2236)
  %2246 : Tensor = prim::GetAttr[name="bias"](%2236)
   = prim::If(%2242) # torch/nn/functional.py:2011:4
    block0():
      %2247 : int[] = aten::size(%out.106) # torch/nn/functional.py:2012:27
      %size_prods.144 : int = aten::__getitem__(%2247, %8) # torch/nn/functional.py:1991:17
      %2249 : int = aten::len(%2247) # torch/nn/functional.py:1992:19
      %2250 : int = aten::sub(%2249, %10) # torch/nn/functional.py:1992:19
      %size_prods.145 : int = prim::Loop(%2250, %9, %size_prods.144) # torch/nn/functional.py:1992:4
        block0(%i.37 : int, %size_prods.146 : int):
          %2254 : int = aten::add(%i.37, %10) # torch/nn/functional.py:1993:27
          %2255 : int = aten::__getitem__(%2247, %2254) # torch/nn/functional.py:1993:22
          %size_prods.147 : int = aten::mul(%size_prods.146, %2255) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.147)
      %2257 : bool = aten::eq(%size_prods.145, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2257) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.107 : Tensor = aten::batch_norm(%out.106, %2245, %2246, %2243, %2244, %2242, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.108 : Tensor = aten::add_(%out.107, %input.21, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.23 : Tensor = aten::relu_(%out.108) # torch/nn/functional.py:1117:17
  %2261 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1171)
  %2262 : Tensor = prim::GetAttr[name="weight"](%2261)
  %2263 : Tensor? = prim::GetAttr[name="bias"](%2261)
  %2264 : int[] = prim::ListConstruct(%12, %12)
  %2265 : int[] = prim::ListConstruct(%8, %8)
  %2266 : int[] = prim::ListConstruct(%12, %12)
  %out.118 : Tensor = aten::conv2d(%input.23, %2262, %2263, %2264, %2265, %2266, %12) # torch/nn/modules/conv.py:415:15
  %2268 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1171)
  %2269 : int = aten::dim(%out.118) # torch/nn/modules/batchnorm.py:276:11
  %2270 : bool = aten::ne(%2269, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2270) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2271 : bool = prim::GetAttr[name="training"](%2268)
   = prim::If(%2271) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2272 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2268)
      %2273 : Tensor = aten::add(%2272, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2268, %2273)
      -> ()
    block1():
      -> ()
  %2274 : bool = prim::GetAttr[name="training"](%2268)
  %2275 : Tensor = prim::GetAttr[name="running_mean"](%2268)
  %2276 : Tensor = prim::GetAttr[name="running_var"](%2268)
  %2277 : Tensor = prim::GetAttr[name="weight"](%2268)
  %2278 : Tensor = prim::GetAttr[name="bias"](%2268)
   = prim::If(%2274) # torch/nn/functional.py:2011:4
    block0():
      %2279 : int[] = aten::size(%out.118) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%2279, %8) # torch/nn/functional.py:1991:17
      %2281 : int = aten::len(%2279) # torch/nn/functional.py:1992:19
      %2282 : int = aten::sub(%2281, %10) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%2282, %9, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %2286 : int = aten::add(%i.38, %10) # torch/nn/functional.py:1993:27
          %2287 : int = aten::__getitem__(%2279, %2286) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %2287) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.151)
      %2289 : bool = aten::eq(%size_prods.149, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2289) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.110 : Tensor = aten::batch_norm(%out.118, %2277, %2278, %2275, %2276, %2274, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.111 : Tensor = aten::relu_(%out.110) # torch/nn/functional.py:1117:17
  %2292 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1171)
  %2293 : Tensor = prim::GetAttr[name="weight"](%2292)
  %2294 : Tensor? = prim::GetAttr[name="bias"](%2292)
  %2295 : int[] = prim::ListConstruct(%12, %12)
  %2296 : int[] = prim::ListConstruct(%12, %12)
  %2297 : int[] = prim::ListConstruct(%12, %12)
  %out.112 : Tensor = aten::conv2d(%out.111, %2293, %2294, %2295, %2296, %2297, %12) # torch/nn/modules/conv.py:415:15
  %2299 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1171)
  %2300 : int = aten::dim(%out.112) # torch/nn/modules/batchnorm.py:276:11
  %2301 : bool = aten::ne(%2300, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2301) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2302 : bool = prim::GetAttr[name="training"](%2299)
   = prim::If(%2302) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2303 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2299)
      %2304 : Tensor = aten::add(%2303, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2299, %2304)
      -> ()
    block1():
      -> ()
  %2305 : bool = prim::GetAttr[name="training"](%2299)
  %2306 : Tensor = prim::GetAttr[name="running_mean"](%2299)
  %2307 : Tensor = prim::GetAttr[name="running_var"](%2299)
  %2308 : Tensor = prim::GetAttr[name="weight"](%2299)
  %2309 : Tensor = prim::GetAttr[name="bias"](%2299)
   = prim::If(%2305) # torch/nn/functional.py:2011:4
    block0():
      %2310 : int[] = aten::size(%out.112) # torch/nn/functional.py:2012:27
      %size_prods.152 : int = aten::__getitem__(%2310, %8) # torch/nn/functional.py:1991:17
      %2312 : int = aten::len(%2310) # torch/nn/functional.py:1992:19
      %2313 : int = aten::sub(%2312, %10) # torch/nn/functional.py:1992:19
      %size_prods.153 : int = prim::Loop(%2313, %9, %size_prods.152) # torch/nn/functional.py:1992:4
        block0(%i.39 : int, %size_prods.154 : int):
          %2317 : int = aten::add(%i.39, %10) # torch/nn/functional.py:1993:27
          %2318 : int = aten::__getitem__(%2310, %2317) # torch/nn/functional.py:1993:22
          %size_prods.155 : int = aten::mul(%size_prods.154, %2318) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.155)
      %2320 : bool = aten::eq(%size_prods.153, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2320) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.113 : Tensor = aten::batch_norm(%out.112, %2308, %2309, %2306, %2307, %2305, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.114 : Tensor = aten::relu_(%out.113) # torch/nn/functional.py:1117:17
  %2323 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1171)
  %2324 : Tensor = prim::GetAttr[name="weight"](%2323)
  %2325 : Tensor? = prim::GetAttr[name="bias"](%2323)
  %2326 : int[] = prim::ListConstruct(%12, %12)
  %2327 : int[] = prim::ListConstruct(%8, %8)
  %2328 : int[] = prim::ListConstruct(%12, %12)
  %out.115 : Tensor = aten::conv2d(%out.114, %2324, %2325, %2326, %2327, %2328, %12) # torch/nn/modules/conv.py:415:15
  %2330 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1171)
  %2331 : int = aten::dim(%out.115) # torch/nn/modules/batchnorm.py:276:11
  %2332 : bool = aten::ne(%2331, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2332) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2333 : bool = prim::GetAttr[name="training"](%2330)
   = prim::If(%2333) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2334 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2330)
      %2335 : Tensor = aten::add(%2334, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2330, %2335)
      -> ()
    block1():
      -> ()
  %2336 : bool = prim::GetAttr[name="training"](%2330)
  %2337 : Tensor = prim::GetAttr[name="running_mean"](%2330)
  %2338 : Tensor = prim::GetAttr[name="running_var"](%2330)
  %2339 : Tensor = prim::GetAttr[name="weight"](%2330)
  %2340 : Tensor = prim::GetAttr[name="bias"](%2330)
   = prim::If(%2336) # torch/nn/functional.py:2011:4
    block0():
      %2341 : int[] = aten::size(%out.115) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%2341, %8) # torch/nn/functional.py:1991:17
      %2343 : int = aten::len(%2341) # torch/nn/functional.py:1992:19
      %2344 : int = aten::sub(%2343, %10) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%2344, %9, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %2348 : int = aten::add(%i.40, %10) # torch/nn/functional.py:1993:27
          %2349 : int = aten::__getitem__(%2341, %2348) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %2349) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.159)
      %2351 : bool = aten::eq(%size_prods.157, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2351) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.116 : Tensor = aten::batch_norm(%out.115, %2339, %2340, %2337, %2338, %2336, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.117 : Tensor = aten::add_(%out.116, %input.23, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.25 : Tensor = aten::relu_(%out.117) # torch/nn/functional.py:1117:17
  %2355 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1172)
  %2356 : Tensor = prim::GetAttr[name="weight"](%2355)
  %2357 : Tensor? = prim::GetAttr[name="bias"](%2355)
  %2358 : int[] = prim::ListConstruct(%12, %12)
  %2359 : int[] = prim::ListConstruct(%8, %8)
  %2360 : int[] = prim::ListConstruct(%12, %12)
  %out.127 : Tensor = aten::conv2d(%input.25, %2356, %2357, %2358, %2359, %2360, %12) # torch/nn/modules/conv.py:415:15
  %2362 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1172)
  %2363 : int = aten::dim(%out.127) # torch/nn/modules/batchnorm.py:276:11
  %2364 : bool = aten::ne(%2363, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2364) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2365 : bool = prim::GetAttr[name="training"](%2362)
   = prim::If(%2365) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2366 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2362)
      %2367 : Tensor = aten::add(%2366, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2362, %2367)
      -> ()
    block1():
      -> ()
  %2368 : bool = prim::GetAttr[name="training"](%2362)
  %2369 : Tensor = prim::GetAttr[name="running_mean"](%2362)
  %2370 : Tensor = prim::GetAttr[name="running_var"](%2362)
  %2371 : Tensor = prim::GetAttr[name="weight"](%2362)
  %2372 : Tensor = prim::GetAttr[name="bias"](%2362)
   = prim::If(%2368) # torch/nn/functional.py:2011:4
    block0():
      %2373 : int[] = aten::size(%out.127) # torch/nn/functional.py:2012:27
      %size_prods.160 : int = aten::__getitem__(%2373, %8) # torch/nn/functional.py:1991:17
      %2375 : int = aten::len(%2373) # torch/nn/functional.py:1992:19
      %2376 : int = aten::sub(%2375, %10) # torch/nn/functional.py:1992:19
      %size_prods.161 : int = prim::Loop(%2376, %9, %size_prods.160) # torch/nn/functional.py:1992:4
        block0(%i.41 : int, %size_prods.162 : int):
          %2380 : int = aten::add(%i.41, %10) # torch/nn/functional.py:1993:27
          %2381 : int = aten::__getitem__(%2373, %2380) # torch/nn/functional.py:1993:22
          %size_prods.163 : int = aten::mul(%size_prods.162, %2381) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.163)
      %2383 : bool = aten::eq(%size_prods.161, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2383) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.119 : Tensor = aten::batch_norm(%out.127, %2371, %2372, %2369, %2370, %2368, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.120 : Tensor = aten::relu_(%out.119) # torch/nn/functional.py:1117:17
  %2386 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1172)
  %2387 : Tensor = prim::GetAttr[name="weight"](%2386)
  %2388 : Tensor? = prim::GetAttr[name="bias"](%2386)
  %2389 : int[] = prim::ListConstruct(%12, %12)
  %2390 : int[] = prim::ListConstruct(%12, %12)
  %2391 : int[] = prim::ListConstruct(%12, %12)
  %out.121 : Tensor = aten::conv2d(%out.120, %2387, %2388, %2389, %2390, %2391, %12) # torch/nn/modules/conv.py:415:15
  %2393 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1172)
  %2394 : int = aten::dim(%out.121) # torch/nn/modules/batchnorm.py:276:11
  %2395 : bool = aten::ne(%2394, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2395) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2396 : bool = prim::GetAttr[name="training"](%2393)
   = prim::If(%2396) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2397 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2393)
      %2398 : Tensor = aten::add(%2397, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2393, %2398)
      -> ()
    block1():
      -> ()
  %2399 : bool = prim::GetAttr[name="training"](%2393)
  %2400 : Tensor = prim::GetAttr[name="running_mean"](%2393)
  %2401 : Tensor = prim::GetAttr[name="running_var"](%2393)
  %2402 : Tensor = prim::GetAttr[name="weight"](%2393)
  %2403 : Tensor = prim::GetAttr[name="bias"](%2393)
   = prim::If(%2399) # torch/nn/functional.py:2011:4
    block0():
      %2404 : int[] = aten::size(%out.121) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%2404, %8) # torch/nn/functional.py:1991:17
      %2406 : int = aten::len(%2404) # torch/nn/functional.py:1992:19
      %2407 : int = aten::sub(%2406, %10) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%2407, %9, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %2411 : int = aten::add(%i.42, %10) # torch/nn/functional.py:1993:27
          %2412 : int = aten::__getitem__(%2404, %2411) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %2412) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.167)
      %2414 : bool = aten::eq(%size_prods.165, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2414) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.122 : Tensor = aten::batch_norm(%out.121, %2402, %2403, %2400, %2401, %2399, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.123 : Tensor = aten::relu_(%out.122) # torch/nn/functional.py:1117:17
  %2417 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1172)
  %2418 : Tensor = prim::GetAttr[name="weight"](%2417)
  %2419 : Tensor? = prim::GetAttr[name="bias"](%2417)
  %2420 : int[] = prim::ListConstruct(%12, %12)
  %2421 : int[] = prim::ListConstruct(%8, %8)
  %2422 : int[] = prim::ListConstruct(%12, %12)
  %out.124 : Tensor = aten::conv2d(%out.123, %2418, %2419, %2420, %2421, %2422, %12) # torch/nn/modules/conv.py:415:15
  %2424 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1172)
  %2425 : int = aten::dim(%out.124) # torch/nn/modules/batchnorm.py:276:11
  %2426 : bool = aten::ne(%2425, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2426) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2427 : bool = prim::GetAttr[name="training"](%2424)
   = prim::If(%2427) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2428 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2424)
      %2429 : Tensor = aten::add(%2428, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2424, %2429)
      -> ()
    block1():
      -> ()
  %2430 : bool = prim::GetAttr[name="training"](%2424)
  %2431 : Tensor = prim::GetAttr[name="running_mean"](%2424)
  %2432 : Tensor = prim::GetAttr[name="running_var"](%2424)
  %2433 : Tensor = prim::GetAttr[name="weight"](%2424)
  %2434 : Tensor = prim::GetAttr[name="bias"](%2424)
   = prim::If(%2430) # torch/nn/functional.py:2011:4
    block0():
      %2435 : int[] = aten::size(%out.124) # torch/nn/functional.py:2012:27
      %size_prods.168 : int = aten::__getitem__(%2435, %8) # torch/nn/functional.py:1991:17
      %2437 : int = aten::len(%2435) # torch/nn/functional.py:1992:19
      %2438 : int = aten::sub(%2437, %10) # torch/nn/functional.py:1992:19
      %size_prods.169 : int = prim::Loop(%2438, %9, %size_prods.168) # torch/nn/functional.py:1992:4
        block0(%i.43 : int, %size_prods.170 : int):
          %2442 : int = aten::add(%i.43, %10) # torch/nn/functional.py:1993:27
          %2443 : int = aten::__getitem__(%2435, %2442) # torch/nn/functional.py:1993:22
          %size_prods.171 : int = aten::mul(%size_prods.170, %2443) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.171)
      %2445 : bool = aten::eq(%size_prods.169, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2445) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.125 : Tensor = aten::batch_norm(%out.124, %2433, %2434, %2431, %2432, %2430, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.126 : Tensor = aten::add_(%out.125, %input.25, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.27 : Tensor = aten::relu_(%out.126) # torch/nn/functional.py:1117:17
  %2449 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1173)
  %2450 : Tensor = prim::GetAttr[name="weight"](%2449)
  %2451 : Tensor? = prim::GetAttr[name="bias"](%2449)
  %2452 : int[] = prim::ListConstruct(%12, %12)
  %2453 : int[] = prim::ListConstruct(%8, %8)
  %2454 : int[] = prim::ListConstruct(%12, %12)
  %out.136 : Tensor = aten::conv2d(%input.27, %2450, %2451, %2452, %2453, %2454, %12) # torch/nn/modules/conv.py:415:15
  %2456 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1173)
  %2457 : int = aten::dim(%out.136) # torch/nn/modules/batchnorm.py:276:11
  %2458 : bool = aten::ne(%2457, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2458) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2459 : bool = prim::GetAttr[name="training"](%2456)
   = prim::If(%2459) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2460 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2456)
      %2461 : Tensor = aten::add(%2460, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2456, %2461)
      -> ()
    block1():
      -> ()
  %2462 : bool = prim::GetAttr[name="training"](%2456)
  %2463 : Tensor = prim::GetAttr[name="running_mean"](%2456)
  %2464 : Tensor = prim::GetAttr[name="running_var"](%2456)
  %2465 : Tensor = prim::GetAttr[name="weight"](%2456)
  %2466 : Tensor = prim::GetAttr[name="bias"](%2456)
   = prim::If(%2462) # torch/nn/functional.py:2011:4
    block0():
      %2467 : int[] = aten::size(%out.136) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%2467, %8) # torch/nn/functional.py:1991:17
      %2469 : int = aten::len(%2467) # torch/nn/functional.py:1992:19
      %2470 : int = aten::sub(%2469, %10) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%2470, %9, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %2474 : int = aten::add(%i.44, %10) # torch/nn/functional.py:1993:27
          %2475 : int = aten::__getitem__(%2467, %2474) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %2475) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.175)
      %2477 : bool = aten::eq(%size_prods.173, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2477) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.128 : Tensor = aten::batch_norm(%out.136, %2465, %2466, %2463, %2464, %2462, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.129 : Tensor = aten::relu_(%out.128) # torch/nn/functional.py:1117:17
  %2480 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1173)
  %2481 : Tensor = prim::GetAttr[name="weight"](%2480)
  %2482 : Tensor? = prim::GetAttr[name="bias"](%2480)
  %2483 : int[] = prim::ListConstruct(%12, %12)
  %2484 : int[] = prim::ListConstruct(%12, %12)
  %2485 : int[] = prim::ListConstruct(%12, %12)
  %out.130 : Tensor = aten::conv2d(%out.129, %2481, %2482, %2483, %2484, %2485, %12) # torch/nn/modules/conv.py:415:15
  %2487 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1173)
  %2488 : int = aten::dim(%out.130) # torch/nn/modules/batchnorm.py:276:11
  %2489 : bool = aten::ne(%2488, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2489) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2490 : bool = prim::GetAttr[name="training"](%2487)
   = prim::If(%2490) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2491 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2487)
      %2492 : Tensor = aten::add(%2491, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2487, %2492)
      -> ()
    block1():
      -> ()
  %2493 : bool = prim::GetAttr[name="training"](%2487)
  %2494 : Tensor = prim::GetAttr[name="running_mean"](%2487)
  %2495 : Tensor = prim::GetAttr[name="running_var"](%2487)
  %2496 : Tensor = prim::GetAttr[name="weight"](%2487)
  %2497 : Tensor = prim::GetAttr[name="bias"](%2487)
   = prim::If(%2493) # torch/nn/functional.py:2011:4
    block0():
      %2498 : int[] = aten::size(%out.130) # torch/nn/functional.py:2012:27
      %size_prods.176 : int = aten::__getitem__(%2498, %8) # torch/nn/functional.py:1991:17
      %2500 : int = aten::len(%2498) # torch/nn/functional.py:1992:19
      %2501 : int = aten::sub(%2500, %10) # torch/nn/functional.py:1992:19
      %size_prods.177 : int = prim::Loop(%2501, %9, %size_prods.176) # torch/nn/functional.py:1992:4
        block0(%i.45 : int, %size_prods.178 : int):
          %2505 : int = aten::add(%i.45, %10) # torch/nn/functional.py:1993:27
          %2506 : int = aten::__getitem__(%2498, %2505) # torch/nn/functional.py:1993:22
          %size_prods.179 : int = aten::mul(%size_prods.178, %2506) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.179)
      %2508 : bool = aten::eq(%size_prods.177, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2508) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.131 : Tensor = aten::batch_norm(%out.130, %2496, %2497, %2494, %2495, %2493, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.132 : Tensor = aten::relu_(%out.131) # torch/nn/functional.py:1117:17
  %2511 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1173)
  %2512 : Tensor = prim::GetAttr[name="weight"](%2511)
  %2513 : Tensor? = prim::GetAttr[name="bias"](%2511)
  %2514 : int[] = prim::ListConstruct(%12, %12)
  %2515 : int[] = prim::ListConstruct(%8, %8)
  %2516 : int[] = prim::ListConstruct(%12, %12)
  %out.133 : Tensor = aten::conv2d(%out.132, %2512, %2513, %2514, %2515, %2516, %12) # torch/nn/modules/conv.py:415:15
  %2518 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1173)
  %2519 : int = aten::dim(%out.133) # torch/nn/modules/batchnorm.py:276:11
  %2520 : bool = aten::ne(%2519, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2520) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2521 : bool = prim::GetAttr[name="training"](%2518)
   = prim::If(%2521) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2522 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2518)
      %2523 : Tensor = aten::add(%2522, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2518, %2523)
      -> ()
    block1():
      -> ()
  %2524 : bool = prim::GetAttr[name="training"](%2518)
  %2525 : Tensor = prim::GetAttr[name="running_mean"](%2518)
  %2526 : Tensor = prim::GetAttr[name="running_var"](%2518)
  %2527 : Tensor = prim::GetAttr[name="weight"](%2518)
  %2528 : Tensor = prim::GetAttr[name="bias"](%2518)
   = prim::If(%2524) # torch/nn/functional.py:2011:4
    block0():
      %2529 : int[] = aten::size(%out.133) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%2529, %8) # torch/nn/functional.py:1991:17
      %2531 : int = aten::len(%2529) # torch/nn/functional.py:1992:19
      %2532 : int = aten::sub(%2531, %10) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%2532, %9, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %2536 : int = aten::add(%i.46, %10) # torch/nn/functional.py:1993:27
          %2537 : int = aten::__getitem__(%2529, %2536) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %2537) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.183)
      %2539 : bool = aten::eq(%size_prods.181, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2539) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.134 : Tensor = aten::batch_norm(%out.133, %2527, %2528, %2525, %2526, %2524, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.135 : Tensor = aten::add_(%out.134, %input.27, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.29 : Tensor = aten::relu_(%out.135) # torch/nn/functional.py:1117:17
  %2543 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1174)
  %2544 : Tensor = prim::GetAttr[name="weight"](%2543)
  %2545 : Tensor? = prim::GetAttr[name="bias"](%2543)
  %2546 : int[] = prim::ListConstruct(%12, %12)
  %2547 : int[] = prim::ListConstruct(%8, %8)
  %2548 : int[] = prim::ListConstruct(%12, %12)
  %out.145 : Tensor = aten::conv2d(%input.29, %2544, %2545, %2546, %2547, %2548, %12) # torch/nn/modules/conv.py:415:15
  %2550 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1174)
  %2551 : int = aten::dim(%out.145) # torch/nn/modules/batchnorm.py:276:11
  %2552 : bool = aten::ne(%2551, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2552) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2553 : bool = prim::GetAttr[name="training"](%2550)
   = prim::If(%2553) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2554 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2550)
      %2555 : Tensor = aten::add(%2554, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2550, %2555)
      -> ()
    block1():
      -> ()
  %2556 : bool = prim::GetAttr[name="training"](%2550)
  %2557 : Tensor = prim::GetAttr[name="running_mean"](%2550)
  %2558 : Tensor = prim::GetAttr[name="running_var"](%2550)
  %2559 : Tensor = prim::GetAttr[name="weight"](%2550)
  %2560 : Tensor = prim::GetAttr[name="bias"](%2550)
   = prim::If(%2556) # torch/nn/functional.py:2011:4
    block0():
      %2561 : int[] = aten::size(%out.145) # torch/nn/functional.py:2012:27
      %size_prods.184 : int = aten::__getitem__(%2561, %8) # torch/nn/functional.py:1991:17
      %2563 : int = aten::len(%2561) # torch/nn/functional.py:1992:19
      %2564 : int = aten::sub(%2563, %10) # torch/nn/functional.py:1992:19
      %size_prods.185 : int = prim::Loop(%2564, %9, %size_prods.184) # torch/nn/functional.py:1992:4
        block0(%i.47 : int, %size_prods.186 : int):
          %2568 : int = aten::add(%i.47, %10) # torch/nn/functional.py:1993:27
          %2569 : int = aten::__getitem__(%2561, %2568) # torch/nn/functional.py:1993:22
          %size_prods.187 : int = aten::mul(%size_prods.186, %2569) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.187)
      %2571 : bool = aten::eq(%size_prods.185, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2571) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.137 : Tensor = aten::batch_norm(%out.145, %2559, %2560, %2557, %2558, %2556, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.138 : Tensor = aten::relu_(%out.137) # torch/nn/functional.py:1117:17
  %2574 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1174)
  %2575 : Tensor = prim::GetAttr[name="weight"](%2574)
  %2576 : Tensor? = prim::GetAttr[name="bias"](%2574)
  %2577 : int[] = prim::ListConstruct(%12, %12)
  %2578 : int[] = prim::ListConstruct(%12, %12)
  %2579 : int[] = prim::ListConstruct(%12, %12)
  %out.139 : Tensor = aten::conv2d(%out.138, %2575, %2576, %2577, %2578, %2579, %12) # torch/nn/modules/conv.py:415:15
  %2581 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1174)
  %2582 : int = aten::dim(%out.139) # torch/nn/modules/batchnorm.py:276:11
  %2583 : bool = aten::ne(%2582, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2583) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2584 : bool = prim::GetAttr[name="training"](%2581)
   = prim::If(%2584) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2585 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2581)
      %2586 : Tensor = aten::add(%2585, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2581, %2586)
      -> ()
    block1():
      -> ()
  %2587 : bool = prim::GetAttr[name="training"](%2581)
  %2588 : Tensor = prim::GetAttr[name="running_mean"](%2581)
  %2589 : Tensor = prim::GetAttr[name="running_var"](%2581)
  %2590 : Tensor = prim::GetAttr[name="weight"](%2581)
  %2591 : Tensor = prim::GetAttr[name="bias"](%2581)
   = prim::If(%2587) # torch/nn/functional.py:2011:4
    block0():
      %2592 : int[] = aten::size(%out.139) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%2592, %8) # torch/nn/functional.py:1991:17
      %2594 : int = aten::len(%2592) # torch/nn/functional.py:1992:19
      %2595 : int = aten::sub(%2594, %10) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%2595, %9, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %2599 : int = aten::add(%i.48, %10) # torch/nn/functional.py:1993:27
          %2600 : int = aten::__getitem__(%2592, %2599) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %2600) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.191)
      %2602 : bool = aten::eq(%size_prods.189, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2602) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.140 : Tensor = aten::batch_norm(%out.139, %2590, %2591, %2588, %2589, %2587, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.141 : Tensor = aten::relu_(%out.140) # torch/nn/functional.py:1117:17
  %2605 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1174)
  %2606 : Tensor = prim::GetAttr[name="weight"](%2605)
  %2607 : Tensor? = prim::GetAttr[name="bias"](%2605)
  %2608 : int[] = prim::ListConstruct(%12, %12)
  %2609 : int[] = prim::ListConstruct(%8, %8)
  %2610 : int[] = prim::ListConstruct(%12, %12)
  %out.142 : Tensor = aten::conv2d(%out.141, %2606, %2607, %2608, %2609, %2610, %12) # torch/nn/modules/conv.py:415:15
  %2612 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1174)
  %2613 : int = aten::dim(%out.142) # torch/nn/modules/batchnorm.py:276:11
  %2614 : bool = aten::ne(%2613, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2614) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2615 : bool = prim::GetAttr[name="training"](%2612)
   = prim::If(%2615) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2616 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2612)
      %2617 : Tensor = aten::add(%2616, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2612, %2617)
      -> ()
    block1():
      -> ()
  %2618 : bool = prim::GetAttr[name="training"](%2612)
  %2619 : Tensor = prim::GetAttr[name="running_mean"](%2612)
  %2620 : Tensor = prim::GetAttr[name="running_var"](%2612)
  %2621 : Tensor = prim::GetAttr[name="weight"](%2612)
  %2622 : Tensor = prim::GetAttr[name="bias"](%2612)
   = prim::If(%2618) # torch/nn/functional.py:2011:4
    block0():
      %2623 : int[] = aten::size(%out.142) # torch/nn/functional.py:2012:27
      %size_prods.192 : int = aten::__getitem__(%2623, %8) # torch/nn/functional.py:1991:17
      %2625 : int = aten::len(%2623) # torch/nn/functional.py:1992:19
      %2626 : int = aten::sub(%2625, %10) # torch/nn/functional.py:1992:19
      %size_prods.193 : int = prim::Loop(%2626, %9, %size_prods.192) # torch/nn/functional.py:1992:4
        block0(%i.49 : int, %size_prods.194 : int):
          %2630 : int = aten::add(%i.49, %10) # torch/nn/functional.py:1993:27
          %2631 : int = aten::__getitem__(%2623, %2630) # torch/nn/functional.py:1993:22
          %size_prods.195 : int = aten::mul(%size_prods.194, %2631) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.195)
      %2633 : bool = aten::eq(%size_prods.193, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2633) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.143 : Tensor = aten::batch_norm(%out.142, %2621, %2622, %2619, %2620, %2618, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.144 : Tensor = aten::add_(%out.143, %input.29, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.31 : Tensor = aten::relu_(%out.144) # torch/nn/functional.py:1117:17
  %2637 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1175)
  %2638 : Tensor = prim::GetAttr[name="weight"](%2637)
  %2639 : Tensor? = prim::GetAttr[name="bias"](%2637)
  %2640 : int[] = prim::ListConstruct(%12, %12)
  %2641 : int[] = prim::ListConstruct(%8, %8)
  %2642 : int[] = prim::ListConstruct(%12, %12)
  %out.154 : Tensor = aten::conv2d(%input.31, %2638, %2639, %2640, %2641, %2642, %12) # torch/nn/modules/conv.py:415:15
  %2644 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1175)
  %2645 : int = aten::dim(%out.154) # torch/nn/modules/batchnorm.py:276:11
  %2646 : bool = aten::ne(%2645, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2646) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2647 : bool = prim::GetAttr[name="training"](%2644)
   = prim::If(%2647) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2648 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2644)
      %2649 : Tensor = aten::add(%2648, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2644, %2649)
      -> ()
    block1():
      -> ()
  %2650 : bool = prim::GetAttr[name="training"](%2644)
  %2651 : Tensor = prim::GetAttr[name="running_mean"](%2644)
  %2652 : Tensor = prim::GetAttr[name="running_var"](%2644)
  %2653 : Tensor = prim::GetAttr[name="weight"](%2644)
  %2654 : Tensor = prim::GetAttr[name="bias"](%2644)
   = prim::If(%2650) # torch/nn/functional.py:2011:4
    block0():
      %2655 : int[] = aten::size(%out.154) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%2655, %8) # torch/nn/functional.py:1991:17
      %2657 : int = aten::len(%2655) # torch/nn/functional.py:1992:19
      %2658 : int = aten::sub(%2657, %10) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%2658, %9, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %2662 : int = aten::add(%i.50, %10) # torch/nn/functional.py:1993:27
          %2663 : int = aten::__getitem__(%2655, %2662) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %2663) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.199)
      %2665 : bool = aten::eq(%size_prods.197, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2665) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.146 : Tensor = aten::batch_norm(%out.154, %2653, %2654, %2651, %2652, %2650, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.147 : Tensor = aten::relu_(%out.146) # torch/nn/functional.py:1117:17
  %2668 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1175)
  %2669 : Tensor = prim::GetAttr[name="weight"](%2668)
  %2670 : Tensor? = prim::GetAttr[name="bias"](%2668)
  %2671 : int[] = prim::ListConstruct(%12, %12)
  %2672 : int[] = prim::ListConstruct(%12, %12)
  %2673 : int[] = prim::ListConstruct(%12, %12)
  %out.148 : Tensor = aten::conv2d(%out.147, %2669, %2670, %2671, %2672, %2673, %12) # torch/nn/modules/conv.py:415:15
  %2675 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1175)
  %2676 : int = aten::dim(%out.148) # torch/nn/modules/batchnorm.py:276:11
  %2677 : bool = aten::ne(%2676, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2677) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2678 : bool = prim::GetAttr[name="training"](%2675)
   = prim::If(%2678) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2679 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2675)
      %2680 : Tensor = aten::add(%2679, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2675, %2680)
      -> ()
    block1():
      -> ()
  %2681 : bool = prim::GetAttr[name="training"](%2675)
  %2682 : Tensor = prim::GetAttr[name="running_mean"](%2675)
  %2683 : Tensor = prim::GetAttr[name="running_var"](%2675)
  %2684 : Tensor = prim::GetAttr[name="weight"](%2675)
  %2685 : Tensor = prim::GetAttr[name="bias"](%2675)
   = prim::If(%2681) # torch/nn/functional.py:2011:4
    block0():
      %2686 : int[] = aten::size(%out.148) # torch/nn/functional.py:2012:27
      %size_prods.200 : int = aten::__getitem__(%2686, %8) # torch/nn/functional.py:1991:17
      %2688 : int = aten::len(%2686) # torch/nn/functional.py:1992:19
      %2689 : int = aten::sub(%2688, %10) # torch/nn/functional.py:1992:19
      %size_prods.201 : int = prim::Loop(%2689, %9, %size_prods.200) # torch/nn/functional.py:1992:4
        block0(%i.51 : int, %size_prods.202 : int):
          %2693 : int = aten::add(%i.51, %10) # torch/nn/functional.py:1993:27
          %2694 : int = aten::__getitem__(%2686, %2693) # torch/nn/functional.py:1993:22
          %size_prods.203 : int = aten::mul(%size_prods.202, %2694) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.203)
      %2696 : bool = aten::eq(%size_prods.201, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2696) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.149 : Tensor = aten::batch_norm(%out.148, %2684, %2685, %2682, %2683, %2681, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.150 : Tensor = aten::relu_(%out.149) # torch/nn/functional.py:1117:17
  %2699 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1175)
  %2700 : Tensor = prim::GetAttr[name="weight"](%2699)
  %2701 : Tensor? = prim::GetAttr[name="bias"](%2699)
  %2702 : int[] = prim::ListConstruct(%12, %12)
  %2703 : int[] = prim::ListConstruct(%8, %8)
  %2704 : int[] = prim::ListConstruct(%12, %12)
  %out.151 : Tensor = aten::conv2d(%out.150, %2700, %2701, %2702, %2703, %2704, %12) # torch/nn/modules/conv.py:415:15
  %2706 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1175)
  %2707 : int = aten::dim(%out.151) # torch/nn/modules/batchnorm.py:276:11
  %2708 : bool = aten::ne(%2707, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2708) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2709 : bool = prim::GetAttr[name="training"](%2706)
   = prim::If(%2709) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2710 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2706)
      %2711 : Tensor = aten::add(%2710, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2706, %2711)
      -> ()
    block1():
      -> ()
  %2712 : bool = prim::GetAttr[name="training"](%2706)
  %2713 : Tensor = prim::GetAttr[name="running_mean"](%2706)
  %2714 : Tensor = prim::GetAttr[name="running_var"](%2706)
  %2715 : Tensor = prim::GetAttr[name="weight"](%2706)
  %2716 : Tensor = prim::GetAttr[name="bias"](%2706)
   = prim::If(%2712) # torch/nn/functional.py:2011:4
    block0():
      %2717 : int[] = aten::size(%out.151) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%2717, %8) # torch/nn/functional.py:1991:17
      %2719 : int = aten::len(%2717) # torch/nn/functional.py:1992:19
      %2720 : int = aten::sub(%2719, %10) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%2720, %9, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %2724 : int = aten::add(%i.52, %10) # torch/nn/functional.py:1993:27
          %2725 : int = aten::__getitem__(%2717, %2724) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %2725) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.207)
      %2727 : bool = aten::eq(%size_prods.205, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2727) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.152 : Tensor = aten::batch_norm(%out.151, %2715, %2716, %2713, %2714, %2712, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.153 : Tensor = aten::add_(%out.152, %input.31, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.33 : Tensor = aten::relu_(%out.153) # torch/nn/functional.py:1117:17
  %2731 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1176)
  %2732 : Tensor = prim::GetAttr[name="weight"](%2731)
  %2733 : Tensor? = prim::GetAttr[name="bias"](%2731)
  %2734 : int[] = prim::ListConstruct(%12, %12)
  %2735 : int[] = prim::ListConstruct(%8, %8)
  %2736 : int[] = prim::ListConstruct(%12, %12)
  %out.163 : Tensor = aten::conv2d(%input.33, %2732, %2733, %2734, %2735, %2736, %12) # torch/nn/modules/conv.py:415:15
  %2738 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1176)
  %2739 : int = aten::dim(%out.163) # torch/nn/modules/batchnorm.py:276:11
  %2740 : bool = aten::ne(%2739, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2740) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2741 : bool = prim::GetAttr[name="training"](%2738)
   = prim::If(%2741) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2742 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2738)
      %2743 : Tensor = aten::add(%2742, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2738, %2743)
      -> ()
    block1():
      -> ()
  %2744 : bool = prim::GetAttr[name="training"](%2738)
  %2745 : Tensor = prim::GetAttr[name="running_mean"](%2738)
  %2746 : Tensor = prim::GetAttr[name="running_var"](%2738)
  %2747 : Tensor = prim::GetAttr[name="weight"](%2738)
  %2748 : Tensor = prim::GetAttr[name="bias"](%2738)
   = prim::If(%2744) # torch/nn/functional.py:2011:4
    block0():
      %2749 : int[] = aten::size(%out.163) # torch/nn/functional.py:2012:27
      %size_prods.208 : int = aten::__getitem__(%2749, %8) # torch/nn/functional.py:1991:17
      %2751 : int = aten::len(%2749) # torch/nn/functional.py:1992:19
      %2752 : int = aten::sub(%2751, %10) # torch/nn/functional.py:1992:19
      %size_prods.209 : int = prim::Loop(%2752, %9, %size_prods.208) # torch/nn/functional.py:1992:4
        block0(%i.53 : int, %size_prods.210 : int):
          %2756 : int = aten::add(%i.53, %10) # torch/nn/functional.py:1993:27
          %2757 : int = aten::__getitem__(%2749, %2756) # torch/nn/functional.py:1993:22
          %size_prods.211 : int = aten::mul(%size_prods.210, %2757) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.211)
      %2759 : bool = aten::eq(%size_prods.209, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2759) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.155 : Tensor = aten::batch_norm(%out.163, %2747, %2748, %2745, %2746, %2744, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.156 : Tensor = aten::relu_(%out.155) # torch/nn/functional.py:1117:17
  %2762 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1176)
  %2763 : Tensor = prim::GetAttr[name="weight"](%2762)
  %2764 : Tensor? = prim::GetAttr[name="bias"](%2762)
  %2765 : int[] = prim::ListConstruct(%12, %12)
  %2766 : int[] = prim::ListConstruct(%12, %12)
  %2767 : int[] = prim::ListConstruct(%12, %12)
  %out.157 : Tensor = aten::conv2d(%out.156, %2763, %2764, %2765, %2766, %2767, %12) # torch/nn/modules/conv.py:415:15
  %2769 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1176)
  %2770 : int = aten::dim(%out.157) # torch/nn/modules/batchnorm.py:276:11
  %2771 : bool = aten::ne(%2770, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2771) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2772 : bool = prim::GetAttr[name="training"](%2769)
   = prim::If(%2772) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2773 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2769)
      %2774 : Tensor = aten::add(%2773, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2769, %2774)
      -> ()
    block1():
      -> ()
  %2775 : bool = prim::GetAttr[name="training"](%2769)
  %2776 : Tensor = prim::GetAttr[name="running_mean"](%2769)
  %2777 : Tensor = prim::GetAttr[name="running_var"](%2769)
  %2778 : Tensor = prim::GetAttr[name="weight"](%2769)
  %2779 : Tensor = prim::GetAttr[name="bias"](%2769)
   = prim::If(%2775) # torch/nn/functional.py:2011:4
    block0():
      %2780 : int[] = aten::size(%out.157) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%2780, %8) # torch/nn/functional.py:1991:17
      %2782 : int = aten::len(%2780) # torch/nn/functional.py:1992:19
      %2783 : int = aten::sub(%2782, %10) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%2783, %9, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %2787 : int = aten::add(%i.54, %10) # torch/nn/functional.py:1993:27
          %2788 : int = aten::__getitem__(%2780, %2787) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %2788) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.215)
      %2790 : bool = aten::eq(%size_prods.213, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2790) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.158 : Tensor = aten::batch_norm(%out.157, %2778, %2779, %2776, %2777, %2775, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.159 : Tensor = aten::relu_(%out.158) # torch/nn/functional.py:1117:17
  %2793 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1176)
  %2794 : Tensor = prim::GetAttr[name="weight"](%2793)
  %2795 : Tensor? = prim::GetAttr[name="bias"](%2793)
  %2796 : int[] = prim::ListConstruct(%12, %12)
  %2797 : int[] = prim::ListConstruct(%8, %8)
  %2798 : int[] = prim::ListConstruct(%12, %12)
  %out.160 : Tensor = aten::conv2d(%out.159, %2794, %2795, %2796, %2797, %2798, %12) # torch/nn/modules/conv.py:415:15
  %2800 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1176)
  %2801 : int = aten::dim(%out.160) # torch/nn/modules/batchnorm.py:276:11
  %2802 : bool = aten::ne(%2801, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2802) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2803 : bool = prim::GetAttr[name="training"](%2800)
   = prim::If(%2803) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2804 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2800)
      %2805 : Tensor = aten::add(%2804, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2800, %2805)
      -> ()
    block1():
      -> ()
  %2806 : bool = prim::GetAttr[name="training"](%2800)
  %2807 : Tensor = prim::GetAttr[name="running_mean"](%2800)
  %2808 : Tensor = prim::GetAttr[name="running_var"](%2800)
  %2809 : Tensor = prim::GetAttr[name="weight"](%2800)
  %2810 : Tensor = prim::GetAttr[name="bias"](%2800)
   = prim::If(%2806) # torch/nn/functional.py:2011:4
    block0():
      %2811 : int[] = aten::size(%out.160) # torch/nn/functional.py:2012:27
      %size_prods.216 : int = aten::__getitem__(%2811, %8) # torch/nn/functional.py:1991:17
      %2813 : int = aten::len(%2811) # torch/nn/functional.py:1992:19
      %2814 : int = aten::sub(%2813, %10) # torch/nn/functional.py:1992:19
      %size_prods.217 : int = prim::Loop(%2814, %9, %size_prods.216) # torch/nn/functional.py:1992:4
        block0(%i.55 : int, %size_prods.218 : int):
          %2818 : int = aten::add(%i.55, %10) # torch/nn/functional.py:1993:27
          %2819 : int = aten::__getitem__(%2811, %2818) # torch/nn/functional.py:1993:22
          %size_prods.219 : int = aten::mul(%size_prods.218, %2819) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.219)
      %2821 : bool = aten::eq(%size_prods.217, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2821) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.161 : Tensor = aten::batch_norm(%out.160, %2809, %2810, %2807, %2808, %2806, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.162 : Tensor = aten::add_(%out.161, %input.33, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.35 : Tensor = aten::relu_(%out.162) # torch/nn/functional.py:1117:17
  %2825 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1177)
  %2826 : Tensor = prim::GetAttr[name="weight"](%2825)
  %2827 : Tensor? = prim::GetAttr[name="bias"](%2825)
  %2828 : int[] = prim::ListConstruct(%12, %12)
  %2829 : int[] = prim::ListConstruct(%8, %8)
  %2830 : int[] = prim::ListConstruct(%12, %12)
  %out.172 : Tensor = aten::conv2d(%input.35, %2826, %2827, %2828, %2829, %2830, %12) # torch/nn/modules/conv.py:415:15
  %2832 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1177)
  %2833 : int = aten::dim(%out.172) # torch/nn/modules/batchnorm.py:276:11
  %2834 : bool = aten::ne(%2833, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2834) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2835 : bool = prim::GetAttr[name="training"](%2832)
   = prim::If(%2835) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2836 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2832)
      %2837 : Tensor = aten::add(%2836, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2832, %2837)
      -> ()
    block1():
      -> ()
  %2838 : bool = prim::GetAttr[name="training"](%2832)
  %2839 : Tensor = prim::GetAttr[name="running_mean"](%2832)
  %2840 : Tensor = prim::GetAttr[name="running_var"](%2832)
  %2841 : Tensor = prim::GetAttr[name="weight"](%2832)
  %2842 : Tensor = prim::GetAttr[name="bias"](%2832)
   = prim::If(%2838) # torch/nn/functional.py:2011:4
    block0():
      %2843 : int[] = aten::size(%out.172) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%2843, %8) # torch/nn/functional.py:1991:17
      %2845 : int = aten::len(%2843) # torch/nn/functional.py:1992:19
      %2846 : int = aten::sub(%2845, %10) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%2846, %9, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %2850 : int = aten::add(%i.56, %10) # torch/nn/functional.py:1993:27
          %2851 : int = aten::__getitem__(%2843, %2850) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %2851) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.223)
      %2853 : bool = aten::eq(%size_prods.221, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2853) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.164 : Tensor = aten::batch_norm(%out.172, %2841, %2842, %2839, %2840, %2838, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.165 : Tensor = aten::relu_(%out.164) # torch/nn/functional.py:1117:17
  %2856 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1177)
  %2857 : Tensor = prim::GetAttr[name="weight"](%2856)
  %2858 : Tensor? = prim::GetAttr[name="bias"](%2856)
  %2859 : int[] = prim::ListConstruct(%12, %12)
  %2860 : int[] = prim::ListConstruct(%12, %12)
  %2861 : int[] = prim::ListConstruct(%12, %12)
  %out.166 : Tensor = aten::conv2d(%out.165, %2857, %2858, %2859, %2860, %2861, %12) # torch/nn/modules/conv.py:415:15
  %2863 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1177)
  %2864 : int = aten::dim(%out.166) # torch/nn/modules/batchnorm.py:276:11
  %2865 : bool = aten::ne(%2864, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2865) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2866 : bool = prim::GetAttr[name="training"](%2863)
   = prim::If(%2866) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2867 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2863)
      %2868 : Tensor = aten::add(%2867, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2863, %2868)
      -> ()
    block1():
      -> ()
  %2869 : bool = prim::GetAttr[name="training"](%2863)
  %2870 : Tensor = prim::GetAttr[name="running_mean"](%2863)
  %2871 : Tensor = prim::GetAttr[name="running_var"](%2863)
  %2872 : Tensor = prim::GetAttr[name="weight"](%2863)
  %2873 : Tensor = prim::GetAttr[name="bias"](%2863)
   = prim::If(%2869) # torch/nn/functional.py:2011:4
    block0():
      %2874 : int[] = aten::size(%out.166) # torch/nn/functional.py:2012:27
      %size_prods.224 : int = aten::__getitem__(%2874, %8) # torch/nn/functional.py:1991:17
      %2876 : int = aten::len(%2874) # torch/nn/functional.py:1992:19
      %2877 : int = aten::sub(%2876, %10) # torch/nn/functional.py:1992:19
      %size_prods.225 : int = prim::Loop(%2877, %9, %size_prods.224) # torch/nn/functional.py:1992:4
        block0(%i.57 : int, %size_prods.226 : int):
          %2881 : int = aten::add(%i.57, %10) # torch/nn/functional.py:1993:27
          %2882 : int = aten::__getitem__(%2874, %2881) # torch/nn/functional.py:1993:22
          %size_prods.227 : int = aten::mul(%size_prods.226, %2882) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.227)
      %2884 : bool = aten::eq(%size_prods.225, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2884) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.167 : Tensor = aten::batch_norm(%out.166, %2872, %2873, %2870, %2871, %2869, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.168 : Tensor = aten::relu_(%out.167) # torch/nn/functional.py:1117:17
  %2887 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1177)
  %2888 : Tensor = prim::GetAttr[name="weight"](%2887)
  %2889 : Tensor? = prim::GetAttr[name="bias"](%2887)
  %2890 : int[] = prim::ListConstruct(%12, %12)
  %2891 : int[] = prim::ListConstruct(%8, %8)
  %2892 : int[] = prim::ListConstruct(%12, %12)
  %out.169 : Tensor = aten::conv2d(%out.168, %2888, %2889, %2890, %2891, %2892, %12) # torch/nn/modules/conv.py:415:15
  %2894 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1177)
  %2895 : int = aten::dim(%out.169) # torch/nn/modules/batchnorm.py:276:11
  %2896 : bool = aten::ne(%2895, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2896) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2897 : bool = prim::GetAttr[name="training"](%2894)
   = prim::If(%2897) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2898 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2894)
      %2899 : Tensor = aten::add(%2898, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2894, %2899)
      -> ()
    block1():
      -> ()
  %2900 : bool = prim::GetAttr[name="training"](%2894)
  %2901 : Tensor = prim::GetAttr[name="running_mean"](%2894)
  %2902 : Tensor = prim::GetAttr[name="running_var"](%2894)
  %2903 : Tensor = prim::GetAttr[name="weight"](%2894)
  %2904 : Tensor = prim::GetAttr[name="bias"](%2894)
   = prim::If(%2900) # torch/nn/functional.py:2011:4
    block0():
      %2905 : int[] = aten::size(%out.169) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%2905, %8) # torch/nn/functional.py:1991:17
      %2907 : int = aten::len(%2905) # torch/nn/functional.py:1992:19
      %2908 : int = aten::sub(%2907, %10) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%2908, %9, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %2912 : int = aten::add(%i.58, %10) # torch/nn/functional.py:1993:27
          %2913 : int = aten::__getitem__(%2905, %2912) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %2913) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.231)
      %2915 : bool = aten::eq(%size_prods.229, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2915) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.170 : Tensor = aten::batch_norm(%out.169, %2903, %2904, %2901, %2902, %2900, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.171 : Tensor = aten::add_(%out.170, %input.35, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.37 : Tensor = aten::relu_(%out.171) # torch/nn/functional.py:1117:17
  %2919 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1178)
  %2920 : Tensor = prim::GetAttr[name="weight"](%2919)
  %2921 : Tensor? = prim::GetAttr[name="bias"](%2919)
  %2922 : int[] = prim::ListConstruct(%12, %12)
  %2923 : int[] = prim::ListConstruct(%8, %8)
  %2924 : int[] = prim::ListConstruct(%12, %12)
  %out.181 : Tensor = aten::conv2d(%input.37, %2920, %2921, %2922, %2923, %2924, %12) # torch/nn/modules/conv.py:415:15
  %2926 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1178)
  %2927 : int = aten::dim(%out.181) # torch/nn/modules/batchnorm.py:276:11
  %2928 : bool = aten::ne(%2927, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2928) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2929 : bool = prim::GetAttr[name="training"](%2926)
   = prim::If(%2929) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2930 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2926)
      %2931 : Tensor = aten::add(%2930, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2926, %2931)
      -> ()
    block1():
      -> ()
  %2932 : bool = prim::GetAttr[name="training"](%2926)
  %2933 : Tensor = prim::GetAttr[name="running_mean"](%2926)
  %2934 : Tensor = prim::GetAttr[name="running_var"](%2926)
  %2935 : Tensor = prim::GetAttr[name="weight"](%2926)
  %2936 : Tensor = prim::GetAttr[name="bias"](%2926)
   = prim::If(%2932) # torch/nn/functional.py:2011:4
    block0():
      %2937 : int[] = aten::size(%out.181) # torch/nn/functional.py:2012:27
      %size_prods.232 : int = aten::__getitem__(%2937, %8) # torch/nn/functional.py:1991:17
      %2939 : int = aten::len(%2937) # torch/nn/functional.py:1992:19
      %2940 : int = aten::sub(%2939, %10) # torch/nn/functional.py:1992:19
      %size_prods.233 : int = prim::Loop(%2940, %9, %size_prods.232) # torch/nn/functional.py:1992:4
        block0(%i.59 : int, %size_prods.234 : int):
          %2944 : int = aten::add(%i.59, %10) # torch/nn/functional.py:1993:27
          %2945 : int = aten::__getitem__(%2937, %2944) # torch/nn/functional.py:1993:22
          %size_prods.235 : int = aten::mul(%size_prods.234, %2945) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.235)
      %2947 : bool = aten::eq(%size_prods.233, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2947) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.173 : Tensor = aten::batch_norm(%out.181, %2935, %2936, %2933, %2934, %2932, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.174 : Tensor = aten::relu_(%out.173) # torch/nn/functional.py:1117:17
  %2950 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1178)
  %2951 : Tensor = prim::GetAttr[name="weight"](%2950)
  %2952 : Tensor? = prim::GetAttr[name="bias"](%2950)
  %2953 : int[] = prim::ListConstruct(%12, %12)
  %2954 : int[] = prim::ListConstruct(%12, %12)
  %2955 : int[] = prim::ListConstruct(%12, %12)
  %out.175 : Tensor = aten::conv2d(%out.174, %2951, %2952, %2953, %2954, %2955, %12) # torch/nn/modules/conv.py:415:15
  %2957 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1178)
  %2958 : int = aten::dim(%out.175) # torch/nn/modules/batchnorm.py:276:11
  %2959 : bool = aten::ne(%2958, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2959) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2960 : bool = prim::GetAttr[name="training"](%2957)
   = prim::If(%2960) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2961 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2957)
      %2962 : Tensor = aten::add(%2961, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2957, %2962)
      -> ()
    block1():
      -> ()
  %2963 : bool = prim::GetAttr[name="training"](%2957)
  %2964 : Tensor = prim::GetAttr[name="running_mean"](%2957)
  %2965 : Tensor = prim::GetAttr[name="running_var"](%2957)
  %2966 : Tensor = prim::GetAttr[name="weight"](%2957)
  %2967 : Tensor = prim::GetAttr[name="bias"](%2957)
   = prim::If(%2963) # torch/nn/functional.py:2011:4
    block0():
      %2968 : int[] = aten::size(%out.175) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%2968, %8) # torch/nn/functional.py:1991:17
      %2970 : int = aten::len(%2968) # torch/nn/functional.py:1992:19
      %2971 : int = aten::sub(%2970, %10) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%2971, %9, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %2975 : int = aten::add(%i.60, %10) # torch/nn/functional.py:1993:27
          %2976 : int = aten::__getitem__(%2968, %2975) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %2976) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.239)
      %2978 : bool = aten::eq(%size_prods.237, %12) # torch/nn/functional.py:1994:7
       = prim::If(%2978) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.176 : Tensor = aten::batch_norm(%out.175, %2966, %2967, %2964, %2965, %2963, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.177 : Tensor = aten::relu_(%out.176) # torch/nn/functional.py:1117:17
  %2981 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1178)
  %2982 : Tensor = prim::GetAttr[name="weight"](%2981)
  %2983 : Tensor? = prim::GetAttr[name="bias"](%2981)
  %2984 : int[] = prim::ListConstruct(%12, %12)
  %2985 : int[] = prim::ListConstruct(%8, %8)
  %2986 : int[] = prim::ListConstruct(%12, %12)
  %out.178 : Tensor = aten::conv2d(%out.177, %2982, %2983, %2984, %2985, %2986, %12) # torch/nn/modules/conv.py:415:15
  %2988 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1178)
  %2989 : int = aten::dim(%out.178) # torch/nn/modules/batchnorm.py:276:11
  %2990 : bool = aten::ne(%2989, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2990) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2991 : bool = prim::GetAttr[name="training"](%2988)
   = prim::If(%2991) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2992 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2988)
      %2993 : Tensor = aten::add(%2992, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2988, %2993)
      -> ()
    block1():
      -> ()
  %2994 : bool = prim::GetAttr[name="training"](%2988)
  %2995 : Tensor = prim::GetAttr[name="running_mean"](%2988)
  %2996 : Tensor = prim::GetAttr[name="running_var"](%2988)
  %2997 : Tensor = prim::GetAttr[name="weight"](%2988)
  %2998 : Tensor = prim::GetAttr[name="bias"](%2988)
   = prim::If(%2994) # torch/nn/functional.py:2011:4
    block0():
      %2999 : int[] = aten::size(%out.178) # torch/nn/functional.py:2012:27
      %size_prods.240 : int = aten::__getitem__(%2999, %8) # torch/nn/functional.py:1991:17
      %3001 : int = aten::len(%2999) # torch/nn/functional.py:1992:19
      %3002 : int = aten::sub(%3001, %10) # torch/nn/functional.py:1992:19
      %size_prods.241 : int = prim::Loop(%3002, %9, %size_prods.240) # torch/nn/functional.py:1992:4
        block0(%i.61 : int, %size_prods.242 : int):
          %3006 : int = aten::add(%i.61, %10) # torch/nn/functional.py:1993:27
          %3007 : int = aten::__getitem__(%2999, %3006) # torch/nn/functional.py:1993:22
          %size_prods.243 : int = aten::mul(%size_prods.242, %3007) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.243)
      %3009 : bool = aten::eq(%size_prods.241, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3009) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.179 : Tensor = aten::batch_norm(%out.178, %2997, %2998, %2995, %2996, %2994, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.180 : Tensor = aten::add_(%out.179, %input.37, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.39 : Tensor = aten::relu_(%out.180) # torch/nn/functional.py:1117:17
  %3013 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1179)
  %3014 : Tensor = prim::GetAttr[name="weight"](%3013)
  %3015 : Tensor? = prim::GetAttr[name="bias"](%3013)
  %3016 : int[] = prim::ListConstruct(%12, %12)
  %3017 : int[] = prim::ListConstruct(%8, %8)
  %3018 : int[] = prim::ListConstruct(%12, %12)
  %out.190 : Tensor = aten::conv2d(%input.39, %3014, %3015, %3016, %3017, %3018, %12) # torch/nn/modules/conv.py:415:15
  %3020 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1179)
  %3021 : int = aten::dim(%out.190) # torch/nn/modules/batchnorm.py:276:11
  %3022 : bool = aten::ne(%3021, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3022) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3023 : bool = prim::GetAttr[name="training"](%3020)
   = prim::If(%3023) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3024 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3020)
      %3025 : Tensor = aten::add(%3024, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3020, %3025)
      -> ()
    block1():
      -> ()
  %3026 : bool = prim::GetAttr[name="training"](%3020)
  %3027 : Tensor = prim::GetAttr[name="running_mean"](%3020)
  %3028 : Tensor = prim::GetAttr[name="running_var"](%3020)
  %3029 : Tensor = prim::GetAttr[name="weight"](%3020)
  %3030 : Tensor = prim::GetAttr[name="bias"](%3020)
   = prim::If(%3026) # torch/nn/functional.py:2011:4
    block0():
      %3031 : int[] = aten::size(%out.190) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%3031, %8) # torch/nn/functional.py:1991:17
      %3033 : int = aten::len(%3031) # torch/nn/functional.py:1992:19
      %3034 : int = aten::sub(%3033, %10) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%3034, %9, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %3038 : int = aten::add(%i.62, %10) # torch/nn/functional.py:1993:27
          %3039 : int = aten::__getitem__(%3031, %3038) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %3039) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.247)
      %3041 : bool = aten::eq(%size_prods.245, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3041) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.182 : Tensor = aten::batch_norm(%out.190, %3029, %3030, %3027, %3028, %3026, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.183 : Tensor = aten::relu_(%out.182) # torch/nn/functional.py:1117:17
  %3044 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1179)
  %3045 : Tensor = prim::GetAttr[name="weight"](%3044)
  %3046 : Tensor? = prim::GetAttr[name="bias"](%3044)
  %3047 : int[] = prim::ListConstruct(%12, %12)
  %3048 : int[] = prim::ListConstruct(%12, %12)
  %3049 : int[] = prim::ListConstruct(%12, %12)
  %out.184 : Tensor = aten::conv2d(%out.183, %3045, %3046, %3047, %3048, %3049, %12) # torch/nn/modules/conv.py:415:15
  %3051 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1179)
  %3052 : int = aten::dim(%out.184) # torch/nn/modules/batchnorm.py:276:11
  %3053 : bool = aten::ne(%3052, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3053) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3054 : bool = prim::GetAttr[name="training"](%3051)
   = prim::If(%3054) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3055 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3051)
      %3056 : Tensor = aten::add(%3055, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3051, %3056)
      -> ()
    block1():
      -> ()
  %3057 : bool = prim::GetAttr[name="training"](%3051)
  %3058 : Tensor = prim::GetAttr[name="running_mean"](%3051)
  %3059 : Tensor = prim::GetAttr[name="running_var"](%3051)
  %3060 : Tensor = prim::GetAttr[name="weight"](%3051)
  %3061 : Tensor = prim::GetAttr[name="bias"](%3051)
   = prim::If(%3057) # torch/nn/functional.py:2011:4
    block0():
      %3062 : int[] = aten::size(%out.184) # torch/nn/functional.py:2012:27
      %size_prods.248 : int = aten::__getitem__(%3062, %8) # torch/nn/functional.py:1991:17
      %3064 : int = aten::len(%3062) # torch/nn/functional.py:1992:19
      %3065 : int = aten::sub(%3064, %10) # torch/nn/functional.py:1992:19
      %size_prods.249 : int = prim::Loop(%3065, %9, %size_prods.248) # torch/nn/functional.py:1992:4
        block0(%i.63 : int, %size_prods.250 : int):
          %3069 : int = aten::add(%i.63, %10) # torch/nn/functional.py:1993:27
          %3070 : int = aten::__getitem__(%3062, %3069) # torch/nn/functional.py:1993:22
          %size_prods.251 : int = aten::mul(%size_prods.250, %3070) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.251)
      %3072 : bool = aten::eq(%size_prods.249, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3072) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.185 : Tensor = aten::batch_norm(%out.184, %3060, %3061, %3058, %3059, %3057, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.186 : Tensor = aten::relu_(%out.185) # torch/nn/functional.py:1117:17
  %3075 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1179)
  %3076 : Tensor = prim::GetAttr[name="weight"](%3075)
  %3077 : Tensor? = prim::GetAttr[name="bias"](%3075)
  %3078 : int[] = prim::ListConstruct(%12, %12)
  %3079 : int[] = prim::ListConstruct(%8, %8)
  %3080 : int[] = prim::ListConstruct(%12, %12)
  %out.187 : Tensor = aten::conv2d(%out.186, %3076, %3077, %3078, %3079, %3080, %12) # torch/nn/modules/conv.py:415:15
  %3082 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1179)
  %3083 : int = aten::dim(%out.187) # torch/nn/modules/batchnorm.py:276:11
  %3084 : bool = aten::ne(%3083, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3084) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3085 : bool = prim::GetAttr[name="training"](%3082)
   = prim::If(%3085) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3086 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3082)
      %3087 : Tensor = aten::add(%3086, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3082, %3087)
      -> ()
    block1():
      -> ()
  %3088 : bool = prim::GetAttr[name="training"](%3082)
  %3089 : Tensor = prim::GetAttr[name="running_mean"](%3082)
  %3090 : Tensor = prim::GetAttr[name="running_var"](%3082)
  %3091 : Tensor = prim::GetAttr[name="weight"](%3082)
  %3092 : Tensor = prim::GetAttr[name="bias"](%3082)
   = prim::If(%3088) # torch/nn/functional.py:2011:4
    block0():
      %3093 : int[] = aten::size(%out.187) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%3093, %8) # torch/nn/functional.py:1991:17
      %3095 : int = aten::len(%3093) # torch/nn/functional.py:1992:19
      %3096 : int = aten::sub(%3095, %10) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%3096, %9, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %3100 : int = aten::add(%i.64, %10) # torch/nn/functional.py:1993:27
          %3101 : int = aten::__getitem__(%3093, %3100) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %3101) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.255)
      %3103 : bool = aten::eq(%size_prods.253, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3103) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.188 : Tensor = aten::batch_norm(%out.187, %3091, %3092, %3089, %3090, %3088, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.189 : Tensor = aten::add_(%out.188, %input.39, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.41 : Tensor = aten::relu_(%out.189) # torch/nn/functional.py:1117:17
  %3107 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1180)
  %3108 : Tensor = prim::GetAttr[name="weight"](%3107)
  %3109 : Tensor? = prim::GetAttr[name="bias"](%3107)
  %3110 : int[] = prim::ListConstruct(%12, %12)
  %3111 : int[] = prim::ListConstruct(%8, %8)
  %3112 : int[] = prim::ListConstruct(%12, %12)
  %out.199 : Tensor = aten::conv2d(%input.41, %3108, %3109, %3110, %3111, %3112, %12) # torch/nn/modules/conv.py:415:15
  %3114 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1180)
  %3115 : int = aten::dim(%out.199) # torch/nn/modules/batchnorm.py:276:11
  %3116 : bool = aten::ne(%3115, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3116) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3117 : bool = prim::GetAttr[name="training"](%3114)
   = prim::If(%3117) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3118 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3114)
      %3119 : Tensor = aten::add(%3118, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3114, %3119)
      -> ()
    block1():
      -> ()
  %3120 : bool = prim::GetAttr[name="training"](%3114)
  %3121 : Tensor = prim::GetAttr[name="running_mean"](%3114)
  %3122 : Tensor = prim::GetAttr[name="running_var"](%3114)
  %3123 : Tensor = prim::GetAttr[name="weight"](%3114)
  %3124 : Tensor = prim::GetAttr[name="bias"](%3114)
   = prim::If(%3120) # torch/nn/functional.py:2011:4
    block0():
      %3125 : int[] = aten::size(%out.199) # torch/nn/functional.py:2012:27
      %size_prods.256 : int = aten::__getitem__(%3125, %8) # torch/nn/functional.py:1991:17
      %3127 : int = aten::len(%3125) # torch/nn/functional.py:1992:19
      %3128 : int = aten::sub(%3127, %10) # torch/nn/functional.py:1992:19
      %size_prods.257 : int = prim::Loop(%3128, %9, %size_prods.256) # torch/nn/functional.py:1992:4
        block0(%i.65 : int, %size_prods.258 : int):
          %3132 : int = aten::add(%i.65, %10) # torch/nn/functional.py:1993:27
          %3133 : int = aten::__getitem__(%3125, %3132) # torch/nn/functional.py:1993:22
          %size_prods.259 : int = aten::mul(%size_prods.258, %3133) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.259)
      %3135 : bool = aten::eq(%size_prods.257, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3135) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.191 : Tensor = aten::batch_norm(%out.199, %3123, %3124, %3121, %3122, %3120, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.192 : Tensor = aten::relu_(%out.191) # torch/nn/functional.py:1117:17
  %3138 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1180)
  %3139 : Tensor = prim::GetAttr[name="weight"](%3138)
  %3140 : Tensor? = prim::GetAttr[name="bias"](%3138)
  %3141 : int[] = prim::ListConstruct(%12, %12)
  %3142 : int[] = prim::ListConstruct(%12, %12)
  %3143 : int[] = prim::ListConstruct(%12, %12)
  %out.193 : Tensor = aten::conv2d(%out.192, %3139, %3140, %3141, %3142, %3143, %12) # torch/nn/modules/conv.py:415:15
  %3145 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1180)
  %3146 : int = aten::dim(%out.193) # torch/nn/modules/batchnorm.py:276:11
  %3147 : bool = aten::ne(%3146, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3147) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3148 : bool = prim::GetAttr[name="training"](%3145)
   = prim::If(%3148) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3149 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3145)
      %3150 : Tensor = aten::add(%3149, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3145, %3150)
      -> ()
    block1():
      -> ()
  %3151 : bool = prim::GetAttr[name="training"](%3145)
  %3152 : Tensor = prim::GetAttr[name="running_mean"](%3145)
  %3153 : Tensor = prim::GetAttr[name="running_var"](%3145)
  %3154 : Tensor = prim::GetAttr[name="weight"](%3145)
  %3155 : Tensor = prim::GetAttr[name="bias"](%3145)
   = prim::If(%3151) # torch/nn/functional.py:2011:4
    block0():
      %3156 : int[] = aten::size(%out.193) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%3156, %8) # torch/nn/functional.py:1991:17
      %3158 : int = aten::len(%3156) # torch/nn/functional.py:1992:19
      %3159 : int = aten::sub(%3158, %10) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%3159, %9, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %3163 : int = aten::add(%i.66, %10) # torch/nn/functional.py:1993:27
          %3164 : int = aten::__getitem__(%3156, %3163) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %3164) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.263)
      %3166 : bool = aten::eq(%size_prods.261, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3166) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.194 : Tensor = aten::batch_norm(%out.193, %3154, %3155, %3152, %3153, %3151, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.195 : Tensor = aten::relu_(%out.194) # torch/nn/functional.py:1117:17
  %3169 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1180)
  %3170 : Tensor = prim::GetAttr[name="weight"](%3169)
  %3171 : Tensor? = prim::GetAttr[name="bias"](%3169)
  %3172 : int[] = prim::ListConstruct(%12, %12)
  %3173 : int[] = prim::ListConstruct(%8, %8)
  %3174 : int[] = prim::ListConstruct(%12, %12)
  %out.196 : Tensor = aten::conv2d(%out.195, %3170, %3171, %3172, %3173, %3174, %12) # torch/nn/modules/conv.py:415:15
  %3176 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1180)
  %3177 : int = aten::dim(%out.196) # torch/nn/modules/batchnorm.py:276:11
  %3178 : bool = aten::ne(%3177, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3178) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3179 : bool = prim::GetAttr[name="training"](%3176)
   = prim::If(%3179) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3180 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3176)
      %3181 : Tensor = aten::add(%3180, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3176, %3181)
      -> ()
    block1():
      -> ()
  %3182 : bool = prim::GetAttr[name="training"](%3176)
  %3183 : Tensor = prim::GetAttr[name="running_mean"](%3176)
  %3184 : Tensor = prim::GetAttr[name="running_var"](%3176)
  %3185 : Tensor = prim::GetAttr[name="weight"](%3176)
  %3186 : Tensor = prim::GetAttr[name="bias"](%3176)
   = prim::If(%3182) # torch/nn/functional.py:2011:4
    block0():
      %3187 : int[] = aten::size(%out.196) # torch/nn/functional.py:2012:27
      %size_prods.264 : int = aten::__getitem__(%3187, %8) # torch/nn/functional.py:1991:17
      %3189 : int = aten::len(%3187) # torch/nn/functional.py:1992:19
      %3190 : int = aten::sub(%3189, %10) # torch/nn/functional.py:1992:19
      %size_prods.265 : int = prim::Loop(%3190, %9, %size_prods.264) # torch/nn/functional.py:1992:4
        block0(%i.67 : int, %size_prods.266 : int):
          %3194 : int = aten::add(%i.67, %10) # torch/nn/functional.py:1993:27
          %3195 : int = aten::__getitem__(%3187, %3194) # torch/nn/functional.py:1993:22
          %size_prods.267 : int = aten::mul(%size_prods.266, %3195) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.267)
      %3197 : bool = aten::eq(%size_prods.265, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3197) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.197 : Tensor = aten::batch_norm(%out.196, %3185, %3186, %3183, %3184, %3182, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.198 : Tensor = aten::add_(%out.197, %input.41, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.43 : Tensor = aten::relu_(%out.198) # torch/nn/functional.py:1117:17
  %3201 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1181)
  %3202 : Tensor = prim::GetAttr[name="weight"](%3201)
  %3203 : Tensor? = prim::GetAttr[name="bias"](%3201)
  %3204 : int[] = prim::ListConstruct(%12, %12)
  %3205 : int[] = prim::ListConstruct(%8, %8)
  %3206 : int[] = prim::ListConstruct(%12, %12)
  %out.208 : Tensor = aten::conv2d(%input.43, %3202, %3203, %3204, %3205, %3206, %12) # torch/nn/modules/conv.py:415:15
  %3208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1181)
  %3209 : int = aten::dim(%out.208) # torch/nn/modules/batchnorm.py:276:11
  %3210 : bool = aten::ne(%3209, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3210) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3211 : bool = prim::GetAttr[name="training"](%3208)
   = prim::If(%3211) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3212 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3208)
      %3213 : Tensor = aten::add(%3212, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3208, %3213)
      -> ()
    block1():
      -> ()
  %3214 : bool = prim::GetAttr[name="training"](%3208)
  %3215 : Tensor = prim::GetAttr[name="running_mean"](%3208)
  %3216 : Tensor = prim::GetAttr[name="running_var"](%3208)
  %3217 : Tensor = prim::GetAttr[name="weight"](%3208)
  %3218 : Tensor = prim::GetAttr[name="bias"](%3208)
   = prim::If(%3214) # torch/nn/functional.py:2011:4
    block0():
      %3219 : int[] = aten::size(%out.208) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%3219, %8) # torch/nn/functional.py:1991:17
      %3221 : int = aten::len(%3219) # torch/nn/functional.py:1992:19
      %3222 : int = aten::sub(%3221, %10) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%3222, %9, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %3226 : int = aten::add(%i.68, %10) # torch/nn/functional.py:1993:27
          %3227 : int = aten::__getitem__(%3219, %3226) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %3227) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.271)
      %3229 : bool = aten::eq(%size_prods.269, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3229) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.200 : Tensor = aten::batch_norm(%out.208, %3217, %3218, %3215, %3216, %3214, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.201 : Tensor = aten::relu_(%out.200) # torch/nn/functional.py:1117:17
  %3232 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1181)
  %3233 : Tensor = prim::GetAttr[name="weight"](%3232)
  %3234 : Tensor? = prim::GetAttr[name="bias"](%3232)
  %3235 : int[] = prim::ListConstruct(%12, %12)
  %3236 : int[] = prim::ListConstruct(%12, %12)
  %3237 : int[] = prim::ListConstruct(%12, %12)
  %out.202 : Tensor = aten::conv2d(%out.201, %3233, %3234, %3235, %3236, %3237, %12) # torch/nn/modules/conv.py:415:15
  %3239 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1181)
  %3240 : int = aten::dim(%out.202) # torch/nn/modules/batchnorm.py:276:11
  %3241 : bool = aten::ne(%3240, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3241) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3242 : bool = prim::GetAttr[name="training"](%3239)
   = prim::If(%3242) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3243 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3239)
      %3244 : Tensor = aten::add(%3243, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3239, %3244)
      -> ()
    block1():
      -> ()
  %3245 : bool = prim::GetAttr[name="training"](%3239)
  %3246 : Tensor = prim::GetAttr[name="running_mean"](%3239)
  %3247 : Tensor = prim::GetAttr[name="running_var"](%3239)
  %3248 : Tensor = prim::GetAttr[name="weight"](%3239)
  %3249 : Tensor = prim::GetAttr[name="bias"](%3239)
   = prim::If(%3245) # torch/nn/functional.py:2011:4
    block0():
      %3250 : int[] = aten::size(%out.202) # torch/nn/functional.py:2012:27
      %size_prods.272 : int = aten::__getitem__(%3250, %8) # torch/nn/functional.py:1991:17
      %3252 : int = aten::len(%3250) # torch/nn/functional.py:1992:19
      %3253 : int = aten::sub(%3252, %10) # torch/nn/functional.py:1992:19
      %size_prods.273 : int = prim::Loop(%3253, %9, %size_prods.272) # torch/nn/functional.py:1992:4
        block0(%i.69 : int, %size_prods.274 : int):
          %3257 : int = aten::add(%i.69, %10) # torch/nn/functional.py:1993:27
          %3258 : int = aten::__getitem__(%3250, %3257) # torch/nn/functional.py:1993:22
          %size_prods.275 : int = aten::mul(%size_prods.274, %3258) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.275)
      %3260 : bool = aten::eq(%size_prods.273, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3260) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.203 : Tensor = aten::batch_norm(%out.202, %3248, %3249, %3246, %3247, %3245, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.204 : Tensor = aten::relu_(%out.203) # torch/nn/functional.py:1117:17
  %3263 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1181)
  %3264 : Tensor = prim::GetAttr[name="weight"](%3263)
  %3265 : Tensor? = prim::GetAttr[name="bias"](%3263)
  %3266 : int[] = prim::ListConstruct(%12, %12)
  %3267 : int[] = prim::ListConstruct(%8, %8)
  %3268 : int[] = prim::ListConstruct(%12, %12)
  %out.205 : Tensor = aten::conv2d(%out.204, %3264, %3265, %3266, %3267, %3268, %12) # torch/nn/modules/conv.py:415:15
  %3270 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1181)
  %3271 : int = aten::dim(%out.205) # torch/nn/modules/batchnorm.py:276:11
  %3272 : bool = aten::ne(%3271, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3272) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3273 : bool = prim::GetAttr[name="training"](%3270)
   = prim::If(%3273) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3274 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3270)
      %3275 : Tensor = aten::add(%3274, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3270, %3275)
      -> ()
    block1():
      -> ()
  %3276 : bool = prim::GetAttr[name="training"](%3270)
  %3277 : Tensor = prim::GetAttr[name="running_mean"](%3270)
  %3278 : Tensor = prim::GetAttr[name="running_var"](%3270)
  %3279 : Tensor = prim::GetAttr[name="weight"](%3270)
  %3280 : Tensor = prim::GetAttr[name="bias"](%3270)
   = prim::If(%3276) # torch/nn/functional.py:2011:4
    block0():
      %3281 : int[] = aten::size(%out.205) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%3281, %8) # torch/nn/functional.py:1991:17
      %3283 : int = aten::len(%3281) # torch/nn/functional.py:1992:19
      %3284 : int = aten::sub(%3283, %10) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%3284, %9, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %3288 : int = aten::add(%i.70, %10) # torch/nn/functional.py:1993:27
          %3289 : int = aten::__getitem__(%3281, %3288) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %3289) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.279)
      %3291 : bool = aten::eq(%size_prods.277, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3291) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.206 : Tensor = aten::batch_norm(%out.205, %3279, %3280, %3277, %3278, %3276, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.207 : Tensor = aten::add_(%out.206, %input.43, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.45 : Tensor = aten::relu_(%out.207) # torch/nn/functional.py:1117:17
  %3295 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1182)
  %3296 : Tensor = prim::GetAttr[name="weight"](%3295)
  %3297 : Tensor? = prim::GetAttr[name="bias"](%3295)
  %3298 : int[] = prim::ListConstruct(%12, %12)
  %3299 : int[] = prim::ListConstruct(%8, %8)
  %3300 : int[] = prim::ListConstruct(%12, %12)
  %out.217 : Tensor = aten::conv2d(%input.45, %3296, %3297, %3298, %3299, %3300, %12) # torch/nn/modules/conv.py:415:15
  %3302 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1182)
  %3303 : int = aten::dim(%out.217) # torch/nn/modules/batchnorm.py:276:11
  %3304 : bool = aten::ne(%3303, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3304) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3305 : bool = prim::GetAttr[name="training"](%3302)
   = prim::If(%3305) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3306 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3302)
      %3307 : Tensor = aten::add(%3306, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3302, %3307)
      -> ()
    block1():
      -> ()
  %3308 : bool = prim::GetAttr[name="training"](%3302)
  %3309 : Tensor = prim::GetAttr[name="running_mean"](%3302)
  %3310 : Tensor = prim::GetAttr[name="running_var"](%3302)
  %3311 : Tensor = prim::GetAttr[name="weight"](%3302)
  %3312 : Tensor = prim::GetAttr[name="bias"](%3302)
   = prim::If(%3308) # torch/nn/functional.py:2011:4
    block0():
      %3313 : int[] = aten::size(%out.217) # torch/nn/functional.py:2012:27
      %size_prods.280 : int = aten::__getitem__(%3313, %8) # torch/nn/functional.py:1991:17
      %3315 : int = aten::len(%3313) # torch/nn/functional.py:1992:19
      %3316 : int = aten::sub(%3315, %10) # torch/nn/functional.py:1992:19
      %size_prods.281 : int = prim::Loop(%3316, %9, %size_prods.280) # torch/nn/functional.py:1992:4
        block0(%i.71 : int, %size_prods.282 : int):
          %3320 : int = aten::add(%i.71, %10) # torch/nn/functional.py:1993:27
          %3321 : int = aten::__getitem__(%3313, %3320) # torch/nn/functional.py:1993:22
          %size_prods.283 : int = aten::mul(%size_prods.282, %3321) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.283)
      %3323 : bool = aten::eq(%size_prods.281, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3323) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.209 : Tensor = aten::batch_norm(%out.217, %3311, %3312, %3309, %3310, %3308, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.210 : Tensor = aten::relu_(%out.209) # torch/nn/functional.py:1117:17
  %3326 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1182)
  %3327 : Tensor = prim::GetAttr[name="weight"](%3326)
  %3328 : Tensor? = prim::GetAttr[name="bias"](%3326)
  %3329 : int[] = prim::ListConstruct(%12, %12)
  %3330 : int[] = prim::ListConstruct(%12, %12)
  %3331 : int[] = prim::ListConstruct(%12, %12)
  %out.211 : Tensor = aten::conv2d(%out.210, %3327, %3328, %3329, %3330, %3331, %12) # torch/nn/modules/conv.py:415:15
  %3333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1182)
  %3334 : int = aten::dim(%out.211) # torch/nn/modules/batchnorm.py:276:11
  %3335 : bool = aten::ne(%3334, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3335) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3336 : bool = prim::GetAttr[name="training"](%3333)
   = prim::If(%3336) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3337 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3333)
      %3338 : Tensor = aten::add(%3337, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3333, %3338)
      -> ()
    block1():
      -> ()
  %3339 : bool = prim::GetAttr[name="training"](%3333)
  %3340 : Tensor = prim::GetAttr[name="running_mean"](%3333)
  %3341 : Tensor = prim::GetAttr[name="running_var"](%3333)
  %3342 : Tensor = prim::GetAttr[name="weight"](%3333)
  %3343 : Tensor = prim::GetAttr[name="bias"](%3333)
   = prim::If(%3339) # torch/nn/functional.py:2011:4
    block0():
      %3344 : int[] = aten::size(%out.211) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%3344, %8) # torch/nn/functional.py:1991:17
      %3346 : int = aten::len(%3344) # torch/nn/functional.py:1992:19
      %3347 : int = aten::sub(%3346, %10) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%3347, %9, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %3351 : int = aten::add(%i.72, %10) # torch/nn/functional.py:1993:27
          %3352 : int = aten::__getitem__(%3344, %3351) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %3352) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.287)
      %3354 : bool = aten::eq(%size_prods.285, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3354) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.212 : Tensor = aten::batch_norm(%out.211, %3342, %3343, %3340, %3341, %3339, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.213 : Tensor = aten::relu_(%out.212) # torch/nn/functional.py:1117:17
  %3357 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1182)
  %3358 : Tensor = prim::GetAttr[name="weight"](%3357)
  %3359 : Tensor? = prim::GetAttr[name="bias"](%3357)
  %3360 : int[] = prim::ListConstruct(%12, %12)
  %3361 : int[] = prim::ListConstruct(%8, %8)
  %3362 : int[] = prim::ListConstruct(%12, %12)
  %out.214 : Tensor = aten::conv2d(%out.213, %3358, %3359, %3360, %3361, %3362, %12) # torch/nn/modules/conv.py:415:15
  %3364 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1182)
  %3365 : int = aten::dim(%out.214) # torch/nn/modules/batchnorm.py:276:11
  %3366 : bool = aten::ne(%3365, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3366) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3367 : bool = prim::GetAttr[name="training"](%3364)
   = prim::If(%3367) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3368 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3364)
      %3369 : Tensor = aten::add(%3368, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3364, %3369)
      -> ()
    block1():
      -> ()
  %3370 : bool = prim::GetAttr[name="training"](%3364)
  %3371 : Tensor = prim::GetAttr[name="running_mean"](%3364)
  %3372 : Tensor = prim::GetAttr[name="running_var"](%3364)
  %3373 : Tensor = prim::GetAttr[name="weight"](%3364)
  %3374 : Tensor = prim::GetAttr[name="bias"](%3364)
   = prim::If(%3370) # torch/nn/functional.py:2011:4
    block0():
      %3375 : int[] = aten::size(%out.214) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%3375, %8) # torch/nn/functional.py:1991:17
      %3377 : int = aten::len(%3375) # torch/nn/functional.py:1992:19
      %3378 : int = aten::sub(%3377, %10) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%3378, %9, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %3382 : int = aten::add(%i.73, %10) # torch/nn/functional.py:1993:27
          %3383 : int = aten::__getitem__(%3375, %3382) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %3383) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.291)
      %3385 : bool = aten::eq(%size_prods.289, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3385) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.215 : Tensor = aten::batch_norm(%out.214, %3373, %3374, %3371, %3372, %3370, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.216 : Tensor = aten::add_(%out.215, %input.45, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.47 : Tensor = aten::relu_(%out.216) # torch/nn/functional.py:1117:17
  %3389 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1183)
  %3390 : Tensor = prim::GetAttr[name="weight"](%3389)
  %3391 : Tensor? = prim::GetAttr[name="bias"](%3389)
  %3392 : int[] = prim::ListConstruct(%12, %12)
  %3393 : int[] = prim::ListConstruct(%8, %8)
  %3394 : int[] = prim::ListConstruct(%12, %12)
  %out.226 : Tensor = aten::conv2d(%input.47, %3390, %3391, %3392, %3393, %3394, %12) # torch/nn/modules/conv.py:415:15
  %3396 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1183)
  %3397 : int = aten::dim(%out.226) # torch/nn/modules/batchnorm.py:276:11
  %3398 : bool = aten::ne(%3397, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3398) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3399 : bool = prim::GetAttr[name="training"](%3396)
   = prim::If(%3399) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3400 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3396)
      %3401 : Tensor = aten::add(%3400, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3396, %3401)
      -> ()
    block1():
      -> ()
  %3402 : bool = prim::GetAttr[name="training"](%3396)
  %3403 : Tensor = prim::GetAttr[name="running_mean"](%3396)
  %3404 : Tensor = prim::GetAttr[name="running_var"](%3396)
  %3405 : Tensor = prim::GetAttr[name="weight"](%3396)
  %3406 : Tensor = prim::GetAttr[name="bias"](%3396)
   = prim::If(%3402) # torch/nn/functional.py:2011:4
    block0():
      %3407 : int[] = aten::size(%out.226) # torch/nn/functional.py:2012:27
      %size_prods.292 : int = aten::__getitem__(%3407, %8) # torch/nn/functional.py:1991:17
      %3409 : int = aten::len(%3407) # torch/nn/functional.py:1992:19
      %3410 : int = aten::sub(%3409, %10) # torch/nn/functional.py:1992:19
      %size_prods.293 : int = prim::Loop(%3410, %9, %size_prods.292) # torch/nn/functional.py:1992:4
        block0(%i.74 : int, %size_prods.294 : int):
          %3414 : int = aten::add(%i.74, %10) # torch/nn/functional.py:1993:27
          %3415 : int = aten::__getitem__(%3407, %3414) # torch/nn/functional.py:1993:22
          %size_prods.295 : int = aten::mul(%size_prods.294, %3415) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.295)
      %3417 : bool = aten::eq(%size_prods.293, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3417) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.218 : Tensor = aten::batch_norm(%out.226, %3405, %3406, %3403, %3404, %3402, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.219 : Tensor = aten::relu_(%out.218) # torch/nn/functional.py:1117:17
  %3420 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1183)
  %3421 : Tensor = prim::GetAttr[name="weight"](%3420)
  %3422 : Tensor? = prim::GetAttr[name="bias"](%3420)
  %3423 : int[] = prim::ListConstruct(%12, %12)
  %3424 : int[] = prim::ListConstruct(%12, %12)
  %3425 : int[] = prim::ListConstruct(%12, %12)
  %out.220 : Tensor = aten::conv2d(%out.219, %3421, %3422, %3423, %3424, %3425, %12) # torch/nn/modules/conv.py:415:15
  %3427 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1183)
  %3428 : int = aten::dim(%out.220) # torch/nn/modules/batchnorm.py:276:11
  %3429 : bool = aten::ne(%3428, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3429) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3430 : bool = prim::GetAttr[name="training"](%3427)
   = prim::If(%3430) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3431 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3427)
      %3432 : Tensor = aten::add(%3431, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3427, %3432)
      -> ()
    block1():
      -> ()
  %3433 : bool = prim::GetAttr[name="training"](%3427)
  %3434 : Tensor = prim::GetAttr[name="running_mean"](%3427)
  %3435 : Tensor = prim::GetAttr[name="running_var"](%3427)
  %3436 : Tensor = prim::GetAttr[name="weight"](%3427)
  %3437 : Tensor = prim::GetAttr[name="bias"](%3427)
   = prim::If(%3433) # torch/nn/functional.py:2011:4
    block0():
      %3438 : int[] = aten::size(%out.220) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%3438, %8) # torch/nn/functional.py:1991:17
      %3440 : int = aten::len(%3438) # torch/nn/functional.py:1992:19
      %3441 : int = aten::sub(%3440, %10) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%3441, %9, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %3445 : int = aten::add(%i.75, %10) # torch/nn/functional.py:1993:27
          %3446 : int = aten::__getitem__(%3438, %3445) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %3446) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.299)
      %3448 : bool = aten::eq(%size_prods.297, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3448) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.221 : Tensor = aten::batch_norm(%out.220, %3436, %3437, %3434, %3435, %3433, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.222 : Tensor = aten::relu_(%out.221) # torch/nn/functional.py:1117:17
  %3451 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1183)
  %3452 : Tensor = prim::GetAttr[name="weight"](%3451)
  %3453 : Tensor? = prim::GetAttr[name="bias"](%3451)
  %3454 : int[] = prim::ListConstruct(%12, %12)
  %3455 : int[] = prim::ListConstruct(%8, %8)
  %3456 : int[] = prim::ListConstruct(%12, %12)
  %out.223 : Tensor = aten::conv2d(%out.222, %3452, %3453, %3454, %3455, %3456, %12) # torch/nn/modules/conv.py:415:15
  %3458 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1183)
  %3459 : int = aten::dim(%out.223) # torch/nn/modules/batchnorm.py:276:11
  %3460 : bool = aten::ne(%3459, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3460) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3461 : bool = prim::GetAttr[name="training"](%3458)
   = prim::If(%3461) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3462 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3458)
      %3463 : Tensor = aten::add(%3462, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3458, %3463)
      -> ()
    block1():
      -> ()
  %3464 : bool = prim::GetAttr[name="training"](%3458)
  %3465 : Tensor = prim::GetAttr[name="running_mean"](%3458)
  %3466 : Tensor = prim::GetAttr[name="running_var"](%3458)
  %3467 : Tensor = prim::GetAttr[name="weight"](%3458)
  %3468 : Tensor = prim::GetAttr[name="bias"](%3458)
   = prim::If(%3464) # torch/nn/functional.py:2011:4
    block0():
      %3469 : int[] = aten::size(%out.223) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%3469, %8) # torch/nn/functional.py:1991:17
      %3471 : int = aten::len(%3469) # torch/nn/functional.py:1992:19
      %3472 : int = aten::sub(%3471, %10) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%3472, %9, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %3476 : int = aten::add(%i.76, %10) # torch/nn/functional.py:1993:27
          %3477 : int = aten::__getitem__(%3469, %3476) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %3477) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.303)
      %3479 : bool = aten::eq(%size_prods.301, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3479) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.224 : Tensor = aten::batch_norm(%out.223, %3467, %3468, %3465, %3466, %3464, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.225 : Tensor = aten::add_(%out.224, %input.47, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.49 : Tensor = aten::relu_(%out.225) # torch/nn/functional.py:1117:17
  %3483 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1184)
  %3484 : Tensor = prim::GetAttr[name="weight"](%3483)
  %3485 : Tensor? = prim::GetAttr[name="bias"](%3483)
  %3486 : int[] = prim::ListConstruct(%12, %12)
  %3487 : int[] = prim::ListConstruct(%8, %8)
  %3488 : int[] = prim::ListConstruct(%12, %12)
  %out.235 : Tensor = aten::conv2d(%input.49, %3484, %3485, %3486, %3487, %3488, %12) # torch/nn/modules/conv.py:415:15
  %3490 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1184)
  %3491 : int = aten::dim(%out.235) # torch/nn/modules/batchnorm.py:276:11
  %3492 : bool = aten::ne(%3491, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3492) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3493 : bool = prim::GetAttr[name="training"](%3490)
   = prim::If(%3493) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3494 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3490)
      %3495 : Tensor = aten::add(%3494, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3490, %3495)
      -> ()
    block1():
      -> ()
  %3496 : bool = prim::GetAttr[name="training"](%3490)
  %3497 : Tensor = prim::GetAttr[name="running_mean"](%3490)
  %3498 : Tensor = prim::GetAttr[name="running_var"](%3490)
  %3499 : Tensor = prim::GetAttr[name="weight"](%3490)
  %3500 : Tensor = prim::GetAttr[name="bias"](%3490)
   = prim::If(%3496) # torch/nn/functional.py:2011:4
    block0():
      %3501 : int[] = aten::size(%out.235) # torch/nn/functional.py:2012:27
      %size_prods.304 : int = aten::__getitem__(%3501, %8) # torch/nn/functional.py:1991:17
      %3503 : int = aten::len(%3501) # torch/nn/functional.py:1992:19
      %3504 : int = aten::sub(%3503, %10) # torch/nn/functional.py:1992:19
      %size_prods.305 : int = prim::Loop(%3504, %9, %size_prods.304) # torch/nn/functional.py:1992:4
        block0(%i.77 : int, %size_prods.306 : int):
          %3508 : int = aten::add(%i.77, %10) # torch/nn/functional.py:1993:27
          %3509 : int = aten::__getitem__(%3501, %3508) # torch/nn/functional.py:1993:22
          %size_prods.307 : int = aten::mul(%size_prods.306, %3509) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.307)
      %3511 : bool = aten::eq(%size_prods.305, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3511) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.227 : Tensor = aten::batch_norm(%out.235, %3499, %3500, %3497, %3498, %3496, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.228 : Tensor = aten::relu_(%out.227) # torch/nn/functional.py:1117:17
  %3514 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1184)
  %3515 : Tensor = prim::GetAttr[name="weight"](%3514)
  %3516 : Tensor? = prim::GetAttr[name="bias"](%3514)
  %3517 : int[] = prim::ListConstruct(%12, %12)
  %3518 : int[] = prim::ListConstruct(%12, %12)
  %3519 : int[] = prim::ListConstruct(%12, %12)
  %out.229 : Tensor = aten::conv2d(%out.228, %3515, %3516, %3517, %3518, %3519, %12) # torch/nn/modules/conv.py:415:15
  %3521 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1184)
  %3522 : int = aten::dim(%out.229) # torch/nn/modules/batchnorm.py:276:11
  %3523 : bool = aten::ne(%3522, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3523) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3524 : bool = prim::GetAttr[name="training"](%3521)
   = prim::If(%3524) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3525 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3521)
      %3526 : Tensor = aten::add(%3525, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3521, %3526)
      -> ()
    block1():
      -> ()
  %3527 : bool = prim::GetAttr[name="training"](%3521)
  %3528 : Tensor = prim::GetAttr[name="running_mean"](%3521)
  %3529 : Tensor = prim::GetAttr[name="running_var"](%3521)
  %3530 : Tensor = prim::GetAttr[name="weight"](%3521)
  %3531 : Tensor = prim::GetAttr[name="bias"](%3521)
   = prim::If(%3527) # torch/nn/functional.py:2011:4
    block0():
      %3532 : int[] = aten::size(%out.229) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%3532, %8) # torch/nn/functional.py:1991:17
      %3534 : int = aten::len(%3532) # torch/nn/functional.py:1992:19
      %3535 : int = aten::sub(%3534, %10) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%3535, %9, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %3539 : int = aten::add(%i.78, %10) # torch/nn/functional.py:1993:27
          %3540 : int = aten::__getitem__(%3532, %3539) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %3540) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.311)
      %3542 : bool = aten::eq(%size_prods.309, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3542) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.230 : Tensor = aten::batch_norm(%out.229, %3530, %3531, %3528, %3529, %3527, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.231 : Tensor = aten::relu_(%out.230) # torch/nn/functional.py:1117:17
  %3545 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1184)
  %3546 : Tensor = prim::GetAttr[name="weight"](%3545)
  %3547 : Tensor? = prim::GetAttr[name="bias"](%3545)
  %3548 : int[] = prim::ListConstruct(%12, %12)
  %3549 : int[] = prim::ListConstruct(%8, %8)
  %3550 : int[] = prim::ListConstruct(%12, %12)
  %out.232 : Tensor = aten::conv2d(%out.231, %3546, %3547, %3548, %3549, %3550, %12) # torch/nn/modules/conv.py:415:15
  %3552 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1184)
  %3553 : int = aten::dim(%out.232) # torch/nn/modules/batchnorm.py:276:11
  %3554 : bool = aten::ne(%3553, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3554) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3555 : bool = prim::GetAttr[name="training"](%3552)
   = prim::If(%3555) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3556 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3552)
      %3557 : Tensor = aten::add(%3556, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3552, %3557)
      -> ()
    block1():
      -> ()
  %3558 : bool = prim::GetAttr[name="training"](%3552)
  %3559 : Tensor = prim::GetAttr[name="running_mean"](%3552)
  %3560 : Tensor = prim::GetAttr[name="running_var"](%3552)
  %3561 : Tensor = prim::GetAttr[name="weight"](%3552)
  %3562 : Tensor = prim::GetAttr[name="bias"](%3552)
   = prim::If(%3558) # torch/nn/functional.py:2011:4
    block0():
      %3563 : int[] = aten::size(%out.232) # torch/nn/functional.py:2012:27
      %size_prods.312 : int = aten::__getitem__(%3563, %8) # torch/nn/functional.py:1991:17
      %3565 : int = aten::len(%3563) # torch/nn/functional.py:1992:19
      %3566 : int = aten::sub(%3565, %10) # torch/nn/functional.py:1992:19
      %size_prods.313 : int = prim::Loop(%3566, %9, %size_prods.312) # torch/nn/functional.py:1992:4
        block0(%i.79 : int, %size_prods.314 : int):
          %3570 : int = aten::add(%i.79, %10) # torch/nn/functional.py:1993:27
          %3571 : int = aten::__getitem__(%3563, %3570) # torch/nn/functional.py:1993:22
          %size_prods.315 : int = aten::mul(%size_prods.314, %3571) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.315)
      %3573 : bool = aten::eq(%size_prods.313, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3573) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.233 : Tensor = aten::batch_norm(%out.232, %3561, %3562, %3559, %3560, %3558, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.234 : Tensor = aten::add_(%out.233, %input.49, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.51 : Tensor = aten::relu_(%out.234) # torch/nn/functional.py:1117:17
  %3577 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1185)
  %3578 : Tensor = prim::GetAttr[name="weight"](%3577)
  %3579 : Tensor? = prim::GetAttr[name="bias"](%3577)
  %3580 : int[] = prim::ListConstruct(%12, %12)
  %3581 : int[] = prim::ListConstruct(%8, %8)
  %3582 : int[] = prim::ListConstruct(%12, %12)
  %out.244 : Tensor = aten::conv2d(%input.51, %3578, %3579, %3580, %3581, %3582, %12) # torch/nn/modules/conv.py:415:15
  %3584 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1185)
  %3585 : int = aten::dim(%out.244) # torch/nn/modules/batchnorm.py:276:11
  %3586 : bool = aten::ne(%3585, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3586) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3587 : bool = prim::GetAttr[name="training"](%3584)
   = prim::If(%3587) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3588 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3584)
      %3589 : Tensor = aten::add(%3588, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3584, %3589)
      -> ()
    block1():
      -> ()
  %3590 : bool = prim::GetAttr[name="training"](%3584)
  %3591 : Tensor = prim::GetAttr[name="running_mean"](%3584)
  %3592 : Tensor = prim::GetAttr[name="running_var"](%3584)
  %3593 : Tensor = prim::GetAttr[name="weight"](%3584)
  %3594 : Tensor = prim::GetAttr[name="bias"](%3584)
   = prim::If(%3590) # torch/nn/functional.py:2011:4
    block0():
      %3595 : int[] = aten::size(%out.244) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%3595, %8) # torch/nn/functional.py:1991:17
      %3597 : int = aten::len(%3595) # torch/nn/functional.py:1992:19
      %3598 : int = aten::sub(%3597, %10) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%3598, %9, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %3602 : int = aten::add(%i.80, %10) # torch/nn/functional.py:1993:27
          %3603 : int = aten::__getitem__(%3595, %3602) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %3603) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.319)
      %3605 : bool = aten::eq(%size_prods.317, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3605) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.236 : Tensor = aten::batch_norm(%out.244, %3593, %3594, %3591, %3592, %3590, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.237 : Tensor = aten::relu_(%out.236) # torch/nn/functional.py:1117:17
  %3608 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1185)
  %3609 : Tensor = prim::GetAttr[name="weight"](%3608)
  %3610 : Tensor? = prim::GetAttr[name="bias"](%3608)
  %3611 : int[] = prim::ListConstruct(%12, %12)
  %3612 : int[] = prim::ListConstruct(%12, %12)
  %3613 : int[] = prim::ListConstruct(%12, %12)
  %out.238 : Tensor = aten::conv2d(%out.237, %3609, %3610, %3611, %3612, %3613, %12) # torch/nn/modules/conv.py:415:15
  %3615 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1185)
  %3616 : int = aten::dim(%out.238) # torch/nn/modules/batchnorm.py:276:11
  %3617 : bool = aten::ne(%3616, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3617) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3618 : bool = prim::GetAttr[name="training"](%3615)
   = prim::If(%3618) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3619 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3615)
      %3620 : Tensor = aten::add(%3619, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3615, %3620)
      -> ()
    block1():
      -> ()
  %3621 : bool = prim::GetAttr[name="training"](%3615)
  %3622 : Tensor = prim::GetAttr[name="running_mean"](%3615)
  %3623 : Tensor = prim::GetAttr[name="running_var"](%3615)
  %3624 : Tensor = prim::GetAttr[name="weight"](%3615)
  %3625 : Tensor = prim::GetAttr[name="bias"](%3615)
   = prim::If(%3621) # torch/nn/functional.py:2011:4
    block0():
      %3626 : int[] = aten::size(%out.238) # torch/nn/functional.py:2012:27
      %size_prods.320 : int = aten::__getitem__(%3626, %8) # torch/nn/functional.py:1991:17
      %3628 : int = aten::len(%3626) # torch/nn/functional.py:1992:19
      %3629 : int = aten::sub(%3628, %10) # torch/nn/functional.py:1992:19
      %size_prods.321 : int = prim::Loop(%3629, %9, %size_prods.320) # torch/nn/functional.py:1992:4
        block0(%i.81 : int, %size_prods.322 : int):
          %3633 : int = aten::add(%i.81, %10) # torch/nn/functional.py:1993:27
          %3634 : int = aten::__getitem__(%3626, %3633) # torch/nn/functional.py:1993:22
          %size_prods.323 : int = aten::mul(%size_prods.322, %3634) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.323)
      %3636 : bool = aten::eq(%size_prods.321, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3636) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.239 : Tensor = aten::batch_norm(%out.238, %3624, %3625, %3622, %3623, %3621, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.240 : Tensor = aten::relu_(%out.239) # torch/nn/functional.py:1117:17
  %3639 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1185)
  %3640 : Tensor = prim::GetAttr[name="weight"](%3639)
  %3641 : Tensor? = prim::GetAttr[name="bias"](%3639)
  %3642 : int[] = prim::ListConstruct(%12, %12)
  %3643 : int[] = prim::ListConstruct(%8, %8)
  %3644 : int[] = prim::ListConstruct(%12, %12)
  %out.241 : Tensor = aten::conv2d(%out.240, %3640, %3641, %3642, %3643, %3644, %12) # torch/nn/modules/conv.py:415:15
  %3646 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1185)
  %3647 : int = aten::dim(%out.241) # torch/nn/modules/batchnorm.py:276:11
  %3648 : bool = aten::ne(%3647, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3648) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3649 : bool = prim::GetAttr[name="training"](%3646)
   = prim::If(%3649) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3650 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3646)
      %3651 : Tensor = aten::add(%3650, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3646, %3651)
      -> ()
    block1():
      -> ()
  %3652 : bool = prim::GetAttr[name="training"](%3646)
  %3653 : Tensor = prim::GetAttr[name="running_mean"](%3646)
  %3654 : Tensor = prim::GetAttr[name="running_var"](%3646)
  %3655 : Tensor = prim::GetAttr[name="weight"](%3646)
  %3656 : Tensor = prim::GetAttr[name="bias"](%3646)
   = prim::If(%3652) # torch/nn/functional.py:2011:4
    block0():
      %3657 : int[] = aten::size(%out.241) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%3657, %8) # torch/nn/functional.py:1991:17
      %3659 : int = aten::len(%3657) # torch/nn/functional.py:1992:19
      %3660 : int = aten::sub(%3659, %10) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%3660, %9, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %3664 : int = aten::add(%i.82, %10) # torch/nn/functional.py:1993:27
          %3665 : int = aten::__getitem__(%3657, %3664) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %3665) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.327)
      %3667 : bool = aten::eq(%size_prods.325, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3667) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.242 : Tensor = aten::batch_norm(%out.241, %3655, %3656, %3653, %3654, %3652, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.243 : Tensor = aten::add_(%out.242, %input.51, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.53 : Tensor = aten::relu_(%out.243) # torch/nn/functional.py:1117:17
  %3671 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1186)
  %3672 : Tensor = prim::GetAttr[name="weight"](%3671)
  %3673 : Tensor? = prim::GetAttr[name="bias"](%3671)
  %3674 : int[] = prim::ListConstruct(%12, %12)
  %3675 : int[] = prim::ListConstruct(%8, %8)
  %3676 : int[] = prim::ListConstruct(%12, %12)
  %out.253 : Tensor = aten::conv2d(%input.53, %3672, %3673, %3674, %3675, %3676, %12) # torch/nn/modules/conv.py:415:15
  %3678 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1186)
  %3679 : int = aten::dim(%out.253) # torch/nn/modules/batchnorm.py:276:11
  %3680 : bool = aten::ne(%3679, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3680) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3681 : bool = prim::GetAttr[name="training"](%3678)
   = prim::If(%3681) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3682 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3678)
      %3683 : Tensor = aten::add(%3682, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3678, %3683)
      -> ()
    block1():
      -> ()
  %3684 : bool = prim::GetAttr[name="training"](%3678)
  %3685 : Tensor = prim::GetAttr[name="running_mean"](%3678)
  %3686 : Tensor = prim::GetAttr[name="running_var"](%3678)
  %3687 : Tensor = prim::GetAttr[name="weight"](%3678)
  %3688 : Tensor = prim::GetAttr[name="bias"](%3678)
   = prim::If(%3684) # torch/nn/functional.py:2011:4
    block0():
      %3689 : int[] = aten::size(%out.253) # torch/nn/functional.py:2012:27
      %size_prods.328 : int = aten::__getitem__(%3689, %8) # torch/nn/functional.py:1991:17
      %3691 : int = aten::len(%3689) # torch/nn/functional.py:1992:19
      %3692 : int = aten::sub(%3691, %10) # torch/nn/functional.py:1992:19
      %size_prods.329 : int = prim::Loop(%3692, %9, %size_prods.328) # torch/nn/functional.py:1992:4
        block0(%i.83 : int, %size_prods.330 : int):
          %3696 : int = aten::add(%i.83, %10) # torch/nn/functional.py:1993:27
          %3697 : int = aten::__getitem__(%3689, %3696) # torch/nn/functional.py:1993:22
          %size_prods.331 : int = aten::mul(%size_prods.330, %3697) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.331)
      %3699 : bool = aten::eq(%size_prods.329, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3699) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.245 : Tensor = aten::batch_norm(%out.253, %3687, %3688, %3685, %3686, %3684, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.246 : Tensor = aten::relu_(%out.245) # torch/nn/functional.py:1117:17
  %3702 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1186)
  %3703 : Tensor = prim::GetAttr[name="weight"](%3702)
  %3704 : Tensor? = prim::GetAttr[name="bias"](%3702)
  %3705 : int[] = prim::ListConstruct(%12, %12)
  %3706 : int[] = prim::ListConstruct(%12, %12)
  %3707 : int[] = prim::ListConstruct(%12, %12)
  %out.247 : Tensor = aten::conv2d(%out.246, %3703, %3704, %3705, %3706, %3707, %12) # torch/nn/modules/conv.py:415:15
  %3709 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1186)
  %3710 : int = aten::dim(%out.247) # torch/nn/modules/batchnorm.py:276:11
  %3711 : bool = aten::ne(%3710, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3711) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3712 : bool = prim::GetAttr[name="training"](%3709)
   = prim::If(%3712) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3713 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3709)
      %3714 : Tensor = aten::add(%3713, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3709, %3714)
      -> ()
    block1():
      -> ()
  %3715 : bool = prim::GetAttr[name="training"](%3709)
  %3716 : Tensor = prim::GetAttr[name="running_mean"](%3709)
  %3717 : Tensor = prim::GetAttr[name="running_var"](%3709)
  %3718 : Tensor = prim::GetAttr[name="weight"](%3709)
  %3719 : Tensor = prim::GetAttr[name="bias"](%3709)
   = prim::If(%3715) # torch/nn/functional.py:2011:4
    block0():
      %3720 : int[] = aten::size(%out.247) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%3720, %8) # torch/nn/functional.py:1991:17
      %3722 : int = aten::len(%3720) # torch/nn/functional.py:1992:19
      %3723 : int = aten::sub(%3722, %10) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%3723, %9, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %3727 : int = aten::add(%i.84, %10) # torch/nn/functional.py:1993:27
          %3728 : int = aten::__getitem__(%3720, %3727) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %3728) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.335)
      %3730 : bool = aten::eq(%size_prods.333, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3730) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.248 : Tensor = aten::batch_norm(%out.247, %3718, %3719, %3716, %3717, %3715, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.249 : Tensor = aten::relu_(%out.248) # torch/nn/functional.py:1117:17
  %3733 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1186)
  %3734 : Tensor = prim::GetAttr[name="weight"](%3733)
  %3735 : Tensor? = prim::GetAttr[name="bias"](%3733)
  %3736 : int[] = prim::ListConstruct(%12, %12)
  %3737 : int[] = prim::ListConstruct(%8, %8)
  %3738 : int[] = prim::ListConstruct(%12, %12)
  %out.250 : Tensor = aten::conv2d(%out.249, %3734, %3735, %3736, %3737, %3738, %12) # torch/nn/modules/conv.py:415:15
  %3740 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1186)
  %3741 : int = aten::dim(%out.250) # torch/nn/modules/batchnorm.py:276:11
  %3742 : bool = aten::ne(%3741, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3742) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3743 : bool = prim::GetAttr[name="training"](%3740)
   = prim::If(%3743) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3744 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3740)
      %3745 : Tensor = aten::add(%3744, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3740, %3745)
      -> ()
    block1():
      -> ()
  %3746 : bool = prim::GetAttr[name="training"](%3740)
  %3747 : Tensor = prim::GetAttr[name="running_mean"](%3740)
  %3748 : Tensor = prim::GetAttr[name="running_var"](%3740)
  %3749 : Tensor = prim::GetAttr[name="weight"](%3740)
  %3750 : Tensor = prim::GetAttr[name="bias"](%3740)
   = prim::If(%3746) # torch/nn/functional.py:2011:4
    block0():
      %3751 : int[] = aten::size(%out.250) # torch/nn/functional.py:2012:27
      %size_prods.336 : int = aten::__getitem__(%3751, %8) # torch/nn/functional.py:1991:17
      %3753 : int = aten::len(%3751) # torch/nn/functional.py:1992:19
      %3754 : int = aten::sub(%3753, %10) # torch/nn/functional.py:1992:19
      %size_prods.337 : int = prim::Loop(%3754, %9, %size_prods.336) # torch/nn/functional.py:1992:4
        block0(%i.85 : int, %size_prods.338 : int):
          %3758 : int = aten::add(%i.85, %10) # torch/nn/functional.py:1993:27
          %3759 : int = aten::__getitem__(%3751, %3758) # torch/nn/functional.py:1993:22
          %size_prods.339 : int = aten::mul(%size_prods.338, %3759) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.339)
      %3761 : bool = aten::eq(%size_prods.337, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3761) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.251 : Tensor = aten::batch_norm(%out.250, %3749, %3750, %3747, %3748, %3746, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.252 : Tensor = aten::add_(%out.251, %input.53, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.55 : Tensor = aten::relu_(%out.252) # torch/nn/functional.py:1117:17
  %3765 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1187)
  %3766 : Tensor = prim::GetAttr[name="weight"](%3765)
  %3767 : Tensor? = prim::GetAttr[name="bias"](%3765)
  %3768 : int[] = prim::ListConstruct(%12, %12)
  %3769 : int[] = prim::ListConstruct(%8, %8)
  %3770 : int[] = prim::ListConstruct(%12, %12)
  %out.262 : Tensor = aten::conv2d(%input.55, %3766, %3767, %3768, %3769, %3770, %12) # torch/nn/modules/conv.py:415:15
  %3772 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1187)
  %3773 : int = aten::dim(%out.262) # torch/nn/modules/batchnorm.py:276:11
  %3774 : bool = aten::ne(%3773, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3774) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3775 : bool = prim::GetAttr[name="training"](%3772)
   = prim::If(%3775) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3776 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3772)
      %3777 : Tensor = aten::add(%3776, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3772, %3777)
      -> ()
    block1():
      -> ()
  %3778 : bool = prim::GetAttr[name="training"](%3772)
  %3779 : Tensor = prim::GetAttr[name="running_mean"](%3772)
  %3780 : Tensor = prim::GetAttr[name="running_var"](%3772)
  %3781 : Tensor = prim::GetAttr[name="weight"](%3772)
  %3782 : Tensor = prim::GetAttr[name="bias"](%3772)
   = prim::If(%3778) # torch/nn/functional.py:2011:4
    block0():
      %3783 : int[] = aten::size(%out.262) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%3783, %8) # torch/nn/functional.py:1991:17
      %3785 : int = aten::len(%3783) # torch/nn/functional.py:1992:19
      %3786 : int = aten::sub(%3785, %10) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%3786, %9, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %3790 : int = aten::add(%i.86, %10) # torch/nn/functional.py:1993:27
          %3791 : int = aten::__getitem__(%3783, %3790) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %3791) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.343)
      %3793 : bool = aten::eq(%size_prods.341, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3793) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.254 : Tensor = aten::batch_norm(%out.262, %3781, %3782, %3779, %3780, %3778, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.255 : Tensor = aten::relu_(%out.254) # torch/nn/functional.py:1117:17
  %3796 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1187)
  %3797 : Tensor = prim::GetAttr[name="weight"](%3796)
  %3798 : Tensor? = prim::GetAttr[name="bias"](%3796)
  %3799 : int[] = prim::ListConstruct(%12, %12)
  %3800 : int[] = prim::ListConstruct(%12, %12)
  %3801 : int[] = prim::ListConstruct(%12, %12)
  %out.256 : Tensor = aten::conv2d(%out.255, %3797, %3798, %3799, %3800, %3801, %12) # torch/nn/modules/conv.py:415:15
  %3803 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1187)
  %3804 : int = aten::dim(%out.256) # torch/nn/modules/batchnorm.py:276:11
  %3805 : bool = aten::ne(%3804, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3805) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3806 : bool = prim::GetAttr[name="training"](%3803)
   = prim::If(%3806) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3807 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3803)
      %3808 : Tensor = aten::add(%3807, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3803, %3808)
      -> ()
    block1():
      -> ()
  %3809 : bool = prim::GetAttr[name="training"](%3803)
  %3810 : Tensor = prim::GetAttr[name="running_mean"](%3803)
  %3811 : Tensor = prim::GetAttr[name="running_var"](%3803)
  %3812 : Tensor = prim::GetAttr[name="weight"](%3803)
  %3813 : Tensor = prim::GetAttr[name="bias"](%3803)
   = prim::If(%3809) # torch/nn/functional.py:2011:4
    block0():
      %3814 : int[] = aten::size(%out.256) # torch/nn/functional.py:2012:27
      %size_prods.344 : int = aten::__getitem__(%3814, %8) # torch/nn/functional.py:1991:17
      %3816 : int = aten::len(%3814) # torch/nn/functional.py:1992:19
      %3817 : int = aten::sub(%3816, %10) # torch/nn/functional.py:1992:19
      %size_prods.345 : int = prim::Loop(%3817, %9, %size_prods.344) # torch/nn/functional.py:1992:4
        block0(%i.87 : int, %size_prods.346 : int):
          %3821 : int = aten::add(%i.87, %10) # torch/nn/functional.py:1993:27
          %3822 : int = aten::__getitem__(%3814, %3821) # torch/nn/functional.py:1993:22
          %size_prods.347 : int = aten::mul(%size_prods.346, %3822) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.347)
      %3824 : bool = aten::eq(%size_prods.345, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3824) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.257 : Tensor = aten::batch_norm(%out.256, %3812, %3813, %3810, %3811, %3809, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.258 : Tensor = aten::relu_(%out.257) # torch/nn/functional.py:1117:17
  %3827 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1187)
  %3828 : Tensor = prim::GetAttr[name="weight"](%3827)
  %3829 : Tensor? = prim::GetAttr[name="bias"](%3827)
  %3830 : int[] = prim::ListConstruct(%12, %12)
  %3831 : int[] = prim::ListConstruct(%8, %8)
  %3832 : int[] = prim::ListConstruct(%12, %12)
  %out.259 : Tensor = aten::conv2d(%out.258, %3828, %3829, %3830, %3831, %3832, %12) # torch/nn/modules/conv.py:415:15
  %3834 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1187)
  %3835 : int = aten::dim(%out.259) # torch/nn/modules/batchnorm.py:276:11
  %3836 : bool = aten::ne(%3835, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3836) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3837 : bool = prim::GetAttr[name="training"](%3834)
   = prim::If(%3837) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3838 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3834)
      %3839 : Tensor = aten::add(%3838, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3834, %3839)
      -> ()
    block1():
      -> ()
  %3840 : bool = prim::GetAttr[name="training"](%3834)
  %3841 : Tensor = prim::GetAttr[name="running_mean"](%3834)
  %3842 : Tensor = prim::GetAttr[name="running_var"](%3834)
  %3843 : Tensor = prim::GetAttr[name="weight"](%3834)
  %3844 : Tensor = prim::GetAttr[name="bias"](%3834)
   = prim::If(%3840) # torch/nn/functional.py:2011:4
    block0():
      %3845 : int[] = aten::size(%out.259) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%3845, %8) # torch/nn/functional.py:1991:17
      %3847 : int = aten::len(%3845) # torch/nn/functional.py:1992:19
      %3848 : int = aten::sub(%3847, %10) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%3848, %9, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %3852 : int = aten::add(%i.88, %10) # torch/nn/functional.py:1993:27
          %3853 : int = aten::__getitem__(%3845, %3852) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %3853) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.351)
      %3855 : bool = aten::eq(%size_prods.349, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3855) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.260 : Tensor = aten::batch_norm(%out.259, %3843, %3844, %3841, %3842, %3840, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.261 : Tensor = aten::add_(%out.260, %input.55, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.57 : Tensor = aten::relu_(%out.261) # torch/nn/functional.py:1117:17
  %3859 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1188)
  %3860 : Tensor = prim::GetAttr[name="weight"](%3859)
  %3861 : Tensor? = prim::GetAttr[name="bias"](%3859)
  %3862 : int[] = prim::ListConstruct(%12, %12)
  %3863 : int[] = prim::ListConstruct(%8, %8)
  %3864 : int[] = prim::ListConstruct(%12, %12)
  %out.271 : Tensor = aten::conv2d(%input.57, %3860, %3861, %3862, %3863, %3864, %12) # torch/nn/modules/conv.py:415:15
  %3866 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1188)
  %3867 : int = aten::dim(%out.271) # torch/nn/modules/batchnorm.py:276:11
  %3868 : bool = aten::ne(%3867, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3868) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3869 : bool = prim::GetAttr[name="training"](%3866)
   = prim::If(%3869) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3870 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3866)
      %3871 : Tensor = aten::add(%3870, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3866, %3871)
      -> ()
    block1():
      -> ()
  %3872 : bool = prim::GetAttr[name="training"](%3866)
  %3873 : Tensor = prim::GetAttr[name="running_mean"](%3866)
  %3874 : Tensor = prim::GetAttr[name="running_var"](%3866)
  %3875 : Tensor = prim::GetAttr[name="weight"](%3866)
  %3876 : Tensor = prim::GetAttr[name="bias"](%3866)
   = prim::If(%3872) # torch/nn/functional.py:2011:4
    block0():
      %3877 : int[] = aten::size(%out.271) # torch/nn/functional.py:2012:27
      %size_prods.352 : int = aten::__getitem__(%3877, %8) # torch/nn/functional.py:1991:17
      %3879 : int = aten::len(%3877) # torch/nn/functional.py:1992:19
      %3880 : int = aten::sub(%3879, %10) # torch/nn/functional.py:1992:19
      %size_prods.353 : int = prim::Loop(%3880, %9, %size_prods.352) # torch/nn/functional.py:1992:4
        block0(%i.89 : int, %size_prods.354 : int):
          %3884 : int = aten::add(%i.89, %10) # torch/nn/functional.py:1993:27
          %3885 : int = aten::__getitem__(%3877, %3884) # torch/nn/functional.py:1993:22
          %size_prods.355 : int = aten::mul(%size_prods.354, %3885) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.355)
      %3887 : bool = aten::eq(%size_prods.353, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3887) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.263 : Tensor = aten::batch_norm(%out.271, %3875, %3876, %3873, %3874, %3872, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.264 : Tensor = aten::relu_(%out.263) # torch/nn/functional.py:1117:17
  %3890 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1188)
  %3891 : Tensor = prim::GetAttr[name="weight"](%3890)
  %3892 : Tensor? = prim::GetAttr[name="bias"](%3890)
  %3893 : int[] = prim::ListConstruct(%12, %12)
  %3894 : int[] = prim::ListConstruct(%12, %12)
  %3895 : int[] = prim::ListConstruct(%12, %12)
  %out.265 : Tensor = aten::conv2d(%out.264, %3891, %3892, %3893, %3894, %3895, %12) # torch/nn/modules/conv.py:415:15
  %3897 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1188)
  %3898 : int = aten::dim(%out.265) # torch/nn/modules/batchnorm.py:276:11
  %3899 : bool = aten::ne(%3898, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3899) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3900 : bool = prim::GetAttr[name="training"](%3897)
   = prim::If(%3900) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3901 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3897)
      %3902 : Tensor = aten::add(%3901, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3897, %3902)
      -> ()
    block1():
      -> ()
  %3903 : bool = prim::GetAttr[name="training"](%3897)
  %3904 : Tensor = prim::GetAttr[name="running_mean"](%3897)
  %3905 : Tensor = prim::GetAttr[name="running_var"](%3897)
  %3906 : Tensor = prim::GetAttr[name="weight"](%3897)
  %3907 : Tensor = prim::GetAttr[name="bias"](%3897)
   = prim::If(%3903) # torch/nn/functional.py:2011:4
    block0():
      %3908 : int[] = aten::size(%out.265) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%3908, %8) # torch/nn/functional.py:1991:17
      %3910 : int = aten::len(%3908) # torch/nn/functional.py:1992:19
      %3911 : int = aten::sub(%3910, %10) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%3911, %9, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %3915 : int = aten::add(%i.90, %10) # torch/nn/functional.py:1993:27
          %3916 : int = aten::__getitem__(%3908, %3915) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %3916) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.359)
      %3918 : bool = aten::eq(%size_prods.357, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3918) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.266 : Tensor = aten::batch_norm(%out.265, %3906, %3907, %3904, %3905, %3903, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.267 : Tensor = aten::relu_(%out.266) # torch/nn/functional.py:1117:17
  %3921 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1188)
  %3922 : Tensor = prim::GetAttr[name="weight"](%3921)
  %3923 : Tensor? = prim::GetAttr[name="bias"](%3921)
  %3924 : int[] = prim::ListConstruct(%12, %12)
  %3925 : int[] = prim::ListConstruct(%8, %8)
  %3926 : int[] = prim::ListConstruct(%12, %12)
  %out.268 : Tensor = aten::conv2d(%out.267, %3922, %3923, %3924, %3925, %3926, %12) # torch/nn/modules/conv.py:415:15
  %3928 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1188)
  %3929 : int = aten::dim(%out.268) # torch/nn/modules/batchnorm.py:276:11
  %3930 : bool = aten::ne(%3929, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3930) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3931 : bool = prim::GetAttr[name="training"](%3928)
   = prim::If(%3931) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3932 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3928)
      %3933 : Tensor = aten::add(%3932, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3928, %3933)
      -> ()
    block1():
      -> ()
  %3934 : bool = prim::GetAttr[name="training"](%3928)
  %3935 : Tensor = prim::GetAttr[name="running_mean"](%3928)
  %3936 : Tensor = prim::GetAttr[name="running_var"](%3928)
  %3937 : Tensor = prim::GetAttr[name="weight"](%3928)
  %3938 : Tensor = prim::GetAttr[name="bias"](%3928)
   = prim::If(%3934) # torch/nn/functional.py:2011:4
    block0():
      %3939 : int[] = aten::size(%out.268) # torch/nn/functional.py:2012:27
      %size_prods.360 : int = aten::__getitem__(%3939, %8) # torch/nn/functional.py:1991:17
      %3941 : int = aten::len(%3939) # torch/nn/functional.py:1992:19
      %3942 : int = aten::sub(%3941, %10) # torch/nn/functional.py:1992:19
      %size_prods.361 : int = prim::Loop(%3942, %9, %size_prods.360) # torch/nn/functional.py:1992:4
        block0(%i.91 : int, %size_prods.362 : int):
          %3946 : int = aten::add(%i.91, %10) # torch/nn/functional.py:1993:27
          %3947 : int = aten::__getitem__(%3939, %3946) # torch/nn/functional.py:1993:22
          %size_prods.363 : int = aten::mul(%size_prods.362, %3947) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.363)
      %3949 : bool = aten::eq(%size_prods.361, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3949) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.269 : Tensor = aten::batch_norm(%out.268, %3937, %3938, %3935, %3936, %3934, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.270 : Tensor = aten::add_(%out.269, %input.57, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.59 : Tensor = aten::relu_(%out.270) # torch/nn/functional.py:1117:17
  %3953 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1189)
  %3954 : Tensor = prim::GetAttr[name="weight"](%3953)
  %3955 : Tensor? = prim::GetAttr[name="bias"](%3953)
  %3956 : int[] = prim::ListConstruct(%12, %12)
  %3957 : int[] = prim::ListConstruct(%8, %8)
  %3958 : int[] = prim::ListConstruct(%12, %12)
  %out.280 : Tensor = aten::conv2d(%input.59, %3954, %3955, %3956, %3957, %3958, %12) # torch/nn/modules/conv.py:415:15
  %3960 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1189)
  %3961 : int = aten::dim(%out.280) # torch/nn/modules/batchnorm.py:276:11
  %3962 : bool = aten::ne(%3961, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3962) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3963 : bool = prim::GetAttr[name="training"](%3960)
   = prim::If(%3963) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3964 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3960)
      %3965 : Tensor = aten::add(%3964, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3960, %3965)
      -> ()
    block1():
      -> ()
  %3966 : bool = prim::GetAttr[name="training"](%3960)
  %3967 : Tensor = prim::GetAttr[name="running_mean"](%3960)
  %3968 : Tensor = prim::GetAttr[name="running_var"](%3960)
  %3969 : Tensor = prim::GetAttr[name="weight"](%3960)
  %3970 : Tensor = prim::GetAttr[name="bias"](%3960)
   = prim::If(%3966) # torch/nn/functional.py:2011:4
    block0():
      %3971 : int[] = aten::size(%out.280) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%3971, %8) # torch/nn/functional.py:1991:17
      %3973 : int = aten::len(%3971) # torch/nn/functional.py:1992:19
      %3974 : int = aten::sub(%3973, %10) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%3974, %9, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %3978 : int = aten::add(%i.92, %10) # torch/nn/functional.py:1993:27
          %3979 : int = aten::__getitem__(%3971, %3978) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %3979) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.367)
      %3981 : bool = aten::eq(%size_prods.365, %12) # torch/nn/functional.py:1994:7
       = prim::If(%3981) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.272 : Tensor = aten::batch_norm(%out.280, %3969, %3970, %3967, %3968, %3966, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.273 : Tensor = aten::relu_(%out.272) # torch/nn/functional.py:1117:17
  %3984 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1189)
  %3985 : Tensor = prim::GetAttr[name="weight"](%3984)
  %3986 : Tensor? = prim::GetAttr[name="bias"](%3984)
  %3987 : int[] = prim::ListConstruct(%12, %12)
  %3988 : int[] = prim::ListConstruct(%12, %12)
  %3989 : int[] = prim::ListConstruct(%12, %12)
  %out.274 : Tensor = aten::conv2d(%out.273, %3985, %3986, %3987, %3988, %3989, %12) # torch/nn/modules/conv.py:415:15
  %3991 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1189)
  %3992 : int = aten::dim(%out.274) # torch/nn/modules/batchnorm.py:276:11
  %3993 : bool = aten::ne(%3992, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3993) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3994 : bool = prim::GetAttr[name="training"](%3991)
   = prim::If(%3994) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3995 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3991)
      %3996 : Tensor = aten::add(%3995, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3991, %3996)
      -> ()
    block1():
      -> ()
  %3997 : bool = prim::GetAttr[name="training"](%3991)
  %3998 : Tensor = prim::GetAttr[name="running_mean"](%3991)
  %3999 : Tensor = prim::GetAttr[name="running_var"](%3991)
  %4000 : Tensor = prim::GetAttr[name="weight"](%3991)
  %4001 : Tensor = prim::GetAttr[name="bias"](%3991)
   = prim::If(%3997) # torch/nn/functional.py:2011:4
    block0():
      %4002 : int[] = aten::size(%out.274) # torch/nn/functional.py:2012:27
      %size_prods.368 : int = aten::__getitem__(%4002, %8) # torch/nn/functional.py:1991:17
      %4004 : int = aten::len(%4002) # torch/nn/functional.py:1992:19
      %4005 : int = aten::sub(%4004, %10) # torch/nn/functional.py:1992:19
      %size_prods.369 : int = prim::Loop(%4005, %9, %size_prods.368) # torch/nn/functional.py:1992:4
        block0(%i.93 : int, %size_prods.370 : int):
          %4009 : int = aten::add(%i.93, %10) # torch/nn/functional.py:1993:27
          %4010 : int = aten::__getitem__(%4002, %4009) # torch/nn/functional.py:1993:22
          %size_prods.371 : int = aten::mul(%size_prods.370, %4010) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.371)
      %4012 : bool = aten::eq(%size_prods.369, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4012) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.275 : Tensor = aten::batch_norm(%out.274, %4000, %4001, %3998, %3999, %3997, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.276 : Tensor = aten::relu_(%out.275) # torch/nn/functional.py:1117:17
  %4015 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1189)
  %4016 : Tensor = prim::GetAttr[name="weight"](%4015)
  %4017 : Tensor? = prim::GetAttr[name="bias"](%4015)
  %4018 : int[] = prim::ListConstruct(%12, %12)
  %4019 : int[] = prim::ListConstruct(%8, %8)
  %4020 : int[] = prim::ListConstruct(%12, %12)
  %out.277 : Tensor = aten::conv2d(%out.276, %4016, %4017, %4018, %4019, %4020, %12) # torch/nn/modules/conv.py:415:15
  %4022 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1189)
  %4023 : int = aten::dim(%out.277) # torch/nn/modules/batchnorm.py:276:11
  %4024 : bool = aten::ne(%4023, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4024) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4025 : bool = prim::GetAttr[name="training"](%4022)
   = prim::If(%4025) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4026 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4022)
      %4027 : Tensor = aten::add(%4026, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4022, %4027)
      -> ()
    block1():
      -> ()
  %4028 : bool = prim::GetAttr[name="training"](%4022)
  %4029 : Tensor = prim::GetAttr[name="running_mean"](%4022)
  %4030 : Tensor = prim::GetAttr[name="running_var"](%4022)
  %4031 : Tensor = prim::GetAttr[name="weight"](%4022)
  %4032 : Tensor = prim::GetAttr[name="bias"](%4022)
   = prim::If(%4028) # torch/nn/functional.py:2011:4
    block0():
      %4033 : int[] = aten::size(%out.277) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%4033, %8) # torch/nn/functional.py:1991:17
      %4035 : int = aten::len(%4033) # torch/nn/functional.py:1992:19
      %4036 : int = aten::sub(%4035, %10) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%4036, %9, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %4040 : int = aten::add(%i.94, %10) # torch/nn/functional.py:1993:27
          %4041 : int = aten::__getitem__(%4033, %4040) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %4041) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.375)
      %4043 : bool = aten::eq(%size_prods.373, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4043) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.278 : Tensor = aten::batch_norm(%out.277, %4031, %4032, %4029, %4030, %4028, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.279 : Tensor = aten::add_(%out.278, %input.59, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.61 : Tensor = aten::relu_(%out.279) # torch/nn/functional.py:1117:17
  %4047 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1190)
  %4048 : Tensor = prim::GetAttr[name="weight"](%4047)
  %4049 : Tensor? = prim::GetAttr[name="bias"](%4047)
  %4050 : int[] = prim::ListConstruct(%12, %12)
  %4051 : int[] = prim::ListConstruct(%8, %8)
  %4052 : int[] = prim::ListConstruct(%12, %12)
  %out.289 : Tensor = aten::conv2d(%input.61, %4048, %4049, %4050, %4051, %4052, %12) # torch/nn/modules/conv.py:415:15
  %4054 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1190)
  %4055 : int = aten::dim(%out.289) # torch/nn/modules/batchnorm.py:276:11
  %4056 : bool = aten::ne(%4055, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4056) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4057 : bool = prim::GetAttr[name="training"](%4054)
   = prim::If(%4057) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4058 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4054)
      %4059 : Tensor = aten::add(%4058, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4054, %4059)
      -> ()
    block1():
      -> ()
  %4060 : bool = prim::GetAttr[name="training"](%4054)
  %4061 : Tensor = prim::GetAttr[name="running_mean"](%4054)
  %4062 : Tensor = prim::GetAttr[name="running_var"](%4054)
  %4063 : Tensor = prim::GetAttr[name="weight"](%4054)
  %4064 : Tensor = prim::GetAttr[name="bias"](%4054)
   = prim::If(%4060) # torch/nn/functional.py:2011:4
    block0():
      %4065 : int[] = aten::size(%out.289) # torch/nn/functional.py:2012:27
      %size_prods.376 : int = aten::__getitem__(%4065, %8) # torch/nn/functional.py:1991:17
      %4067 : int = aten::len(%4065) # torch/nn/functional.py:1992:19
      %4068 : int = aten::sub(%4067, %10) # torch/nn/functional.py:1992:19
      %size_prods.377 : int = prim::Loop(%4068, %9, %size_prods.376) # torch/nn/functional.py:1992:4
        block0(%i.95 : int, %size_prods.378 : int):
          %4072 : int = aten::add(%i.95, %10) # torch/nn/functional.py:1993:27
          %4073 : int = aten::__getitem__(%4065, %4072) # torch/nn/functional.py:1993:22
          %size_prods.379 : int = aten::mul(%size_prods.378, %4073) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.379)
      %4075 : bool = aten::eq(%size_prods.377, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4075) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.281 : Tensor = aten::batch_norm(%out.289, %4063, %4064, %4061, %4062, %4060, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.282 : Tensor = aten::relu_(%out.281) # torch/nn/functional.py:1117:17
  %4078 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1190)
  %4079 : Tensor = prim::GetAttr[name="weight"](%4078)
  %4080 : Tensor? = prim::GetAttr[name="bias"](%4078)
  %4081 : int[] = prim::ListConstruct(%12, %12)
  %4082 : int[] = prim::ListConstruct(%12, %12)
  %4083 : int[] = prim::ListConstruct(%12, %12)
  %out.283 : Tensor = aten::conv2d(%out.282, %4079, %4080, %4081, %4082, %4083, %12) # torch/nn/modules/conv.py:415:15
  %4085 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1190)
  %4086 : int = aten::dim(%out.283) # torch/nn/modules/batchnorm.py:276:11
  %4087 : bool = aten::ne(%4086, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4087) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4088 : bool = prim::GetAttr[name="training"](%4085)
   = prim::If(%4088) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4089 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4085)
      %4090 : Tensor = aten::add(%4089, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4085, %4090)
      -> ()
    block1():
      -> ()
  %4091 : bool = prim::GetAttr[name="training"](%4085)
  %4092 : Tensor = prim::GetAttr[name="running_mean"](%4085)
  %4093 : Tensor = prim::GetAttr[name="running_var"](%4085)
  %4094 : Tensor = prim::GetAttr[name="weight"](%4085)
  %4095 : Tensor = prim::GetAttr[name="bias"](%4085)
   = prim::If(%4091) # torch/nn/functional.py:2011:4
    block0():
      %4096 : int[] = aten::size(%out.283) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%4096, %8) # torch/nn/functional.py:1991:17
      %4098 : int = aten::len(%4096) # torch/nn/functional.py:1992:19
      %4099 : int = aten::sub(%4098, %10) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%4099, %9, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %4103 : int = aten::add(%i.96, %10) # torch/nn/functional.py:1993:27
          %4104 : int = aten::__getitem__(%4096, %4103) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %4104) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.383)
      %4106 : bool = aten::eq(%size_prods.381, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4106) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.284 : Tensor = aten::batch_norm(%out.283, %4094, %4095, %4092, %4093, %4091, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.285 : Tensor = aten::relu_(%out.284) # torch/nn/functional.py:1117:17
  %4109 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1190)
  %4110 : Tensor = prim::GetAttr[name="weight"](%4109)
  %4111 : Tensor? = prim::GetAttr[name="bias"](%4109)
  %4112 : int[] = prim::ListConstruct(%12, %12)
  %4113 : int[] = prim::ListConstruct(%8, %8)
  %4114 : int[] = prim::ListConstruct(%12, %12)
  %out.286 : Tensor = aten::conv2d(%out.285, %4110, %4111, %4112, %4113, %4114, %12) # torch/nn/modules/conv.py:415:15
  %4116 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1190)
  %4117 : int = aten::dim(%out.286) # torch/nn/modules/batchnorm.py:276:11
  %4118 : bool = aten::ne(%4117, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4118) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4119 : bool = prim::GetAttr[name="training"](%4116)
   = prim::If(%4119) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4120 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4116)
      %4121 : Tensor = aten::add(%4120, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4116, %4121)
      -> ()
    block1():
      -> ()
  %4122 : bool = prim::GetAttr[name="training"](%4116)
  %4123 : Tensor = prim::GetAttr[name="running_mean"](%4116)
  %4124 : Tensor = prim::GetAttr[name="running_var"](%4116)
  %4125 : Tensor = prim::GetAttr[name="weight"](%4116)
  %4126 : Tensor = prim::GetAttr[name="bias"](%4116)
   = prim::If(%4122) # torch/nn/functional.py:2011:4
    block0():
      %4127 : int[] = aten::size(%out.286) # torch/nn/functional.py:2012:27
      %size_prods.384 : int = aten::__getitem__(%4127, %8) # torch/nn/functional.py:1991:17
      %4129 : int = aten::len(%4127) # torch/nn/functional.py:1992:19
      %4130 : int = aten::sub(%4129, %10) # torch/nn/functional.py:1992:19
      %size_prods.385 : int = prim::Loop(%4130, %9, %size_prods.384) # torch/nn/functional.py:1992:4
        block0(%i.97 : int, %size_prods.386 : int):
          %4134 : int = aten::add(%i.97, %10) # torch/nn/functional.py:1993:27
          %4135 : int = aten::__getitem__(%4127, %4134) # torch/nn/functional.py:1993:22
          %size_prods.387 : int = aten::mul(%size_prods.386, %4135) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.387)
      %4137 : bool = aten::eq(%size_prods.385, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4137) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.287 : Tensor = aten::batch_norm(%out.286, %4125, %4126, %4123, %4124, %4122, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.288 : Tensor = aten::add_(%out.287, %input.61, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.63 : Tensor = aten::relu_(%out.288) # torch/nn/functional.py:1117:17
  %4141 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1191)
  %4142 : Tensor = prim::GetAttr[name="weight"](%4141)
  %4143 : Tensor? = prim::GetAttr[name="bias"](%4141)
  %4144 : int[] = prim::ListConstruct(%12, %12)
  %4145 : int[] = prim::ListConstruct(%8, %8)
  %4146 : int[] = prim::ListConstruct(%12, %12)
  %out.298 : Tensor = aten::conv2d(%input.63, %4142, %4143, %4144, %4145, %4146, %12) # torch/nn/modules/conv.py:415:15
  %4148 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1191)
  %4149 : int = aten::dim(%out.298) # torch/nn/modules/batchnorm.py:276:11
  %4150 : bool = aten::ne(%4149, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4150) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4151 : bool = prim::GetAttr[name="training"](%4148)
   = prim::If(%4151) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4152 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4148)
      %4153 : Tensor = aten::add(%4152, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4148, %4153)
      -> ()
    block1():
      -> ()
  %4154 : bool = prim::GetAttr[name="training"](%4148)
  %4155 : Tensor = prim::GetAttr[name="running_mean"](%4148)
  %4156 : Tensor = prim::GetAttr[name="running_var"](%4148)
  %4157 : Tensor = prim::GetAttr[name="weight"](%4148)
  %4158 : Tensor = prim::GetAttr[name="bias"](%4148)
   = prim::If(%4154) # torch/nn/functional.py:2011:4
    block0():
      %4159 : int[] = aten::size(%out.298) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%4159, %8) # torch/nn/functional.py:1991:17
      %4161 : int = aten::len(%4159) # torch/nn/functional.py:1992:19
      %4162 : int = aten::sub(%4161, %10) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%4162, %9, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %4166 : int = aten::add(%i.98, %10) # torch/nn/functional.py:1993:27
          %4167 : int = aten::__getitem__(%4159, %4166) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %4167) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.391)
      %4169 : bool = aten::eq(%size_prods.389, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4169) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.290 : Tensor = aten::batch_norm(%out.298, %4157, %4158, %4155, %4156, %4154, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.291 : Tensor = aten::relu_(%out.290) # torch/nn/functional.py:1117:17
  %4172 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1191)
  %4173 : Tensor = prim::GetAttr[name="weight"](%4172)
  %4174 : Tensor? = prim::GetAttr[name="bias"](%4172)
  %4175 : int[] = prim::ListConstruct(%12, %12)
  %4176 : int[] = prim::ListConstruct(%12, %12)
  %4177 : int[] = prim::ListConstruct(%12, %12)
  %out.292 : Tensor = aten::conv2d(%out.291, %4173, %4174, %4175, %4176, %4177, %12) # torch/nn/modules/conv.py:415:15
  %4179 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1191)
  %4180 : int = aten::dim(%out.292) # torch/nn/modules/batchnorm.py:276:11
  %4181 : bool = aten::ne(%4180, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4181) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4182 : bool = prim::GetAttr[name="training"](%4179)
   = prim::If(%4182) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4183 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4179)
      %4184 : Tensor = aten::add(%4183, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4179, %4184)
      -> ()
    block1():
      -> ()
  %4185 : bool = prim::GetAttr[name="training"](%4179)
  %4186 : Tensor = prim::GetAttr[name="running_mean"](%4179)
  %4187 : Tensor = prim::GetAttr[name="running_var"](%4179)
  %4188 : Tensor = prim::GetAttr[name="weight"](%4179)
  %4189 : Tensor = prim::GetAttr[name="bias"](%4179)
   = prim::If(%4185) # torch/nn/functional.py:2011:4
    block0():
      %4190 : int[] = aten::size(%out.292) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%4190, %8) # torch/nn/functional.py:1991:17
      %4192 : int = aten::len(%4190) # torch/nn/functional.py:1992:19
      %4193 : int = aten::sub(%4192, %10) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%4193, %9, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %4197 : int = aten::add(%i.99, %10) # torch/nn/functional.py:1993:27
          %4198 : int = aten::__getitem__(%4190, %4197) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %4198) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.395)
      %4200 : bool = aten::eq(%size_prods.393, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4200) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.293 : Tensor = aten::batch_norm(%out.292, %4188, %4189, %4186, %4187, %4185, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.294 : Tensor = aten::relu_(%out.293) # torch/nn/functional.py:1117:17
  %4203 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1191)
  %4204 : Tensor = prim::GetAttr[name="weight"](%4203)
  %4205 : Tensor? = prim::GetAttr[name="bias"](%4203)
  %4206 : int[] = prim::ListConstruct(%12, %12)
  %4207 : int[] = prim::ListConstruct(%8, %8)
  %4208 : int[] = prim::ListConstruct(%12, %12)
  %out.295 : Tensor = aten::conv2d(%out.294, %4204, %4205, %4206, %4207, %4208, %12) # torch/nn/modules/conv.py:415:15
  %4210 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1191)
  %4211 : int = aten::dim(%out.295) # torch/nn/modules/batchnorm.py:276:11
  %4212 : bool = aten::ne(%4211, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4212) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4213 : bool = prim::GetAttr[name="training"](%4210)
   = prim::If(%4213) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4214 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4210)
      %4215 : Tensor = aten::add(%4214, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4210, %4215)
      -> ()
    block1():
      -> ()
  %4216 : bool = prim::GetAttr[name="training"](%4210)
  %4217 : Tensor = prim::GetAttr[name="running_mean"](%4210)
  %4218 : Tensor = prim::GetAttr[name="running_var"](%4210)
  %4219 : Tensor = prim::GetAttr[name="weight"](%4210)
  %4220 : Tensor = prim::GetAttr[name="bias"](%4210)
   = prim::If(%4216) # torch/nn/functional.py:2011:4
    block0():
      %4221 : int[] = aten::size(%out.295) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%4221, %8) # torch/nn/functional.py:1991:17
      %4223 : int = aten::len(%4221) # torch/nn/functional.py:1992:19
      %4224 : int = aten::sub(%4223, %10) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%4224, %9, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %4228 : int = aten::add(%i.100, %10) # torch/nn/functional.py:1993:27
          %4229 : int = aten::__getitem__(%4221, %4228) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %4229) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.399)
      %4231 : bool = aten::eq(%size_prods.397, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4231) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.296 : Tensor = aten::batch_norm(%out.295, %4219, %4220, %4217, %4218, %4216, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.297 : Tensor = aten::add_(%out.296, %input.63, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.65 : Tensor = aten::relu_(%out.297) # torch/nn/functional.py:1117:17
  %4235 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1192)
  %4236 : Tensor = prim::GetAttr[name="weight"](%4235)
  %4237 : Tensor? = prim::GetAttr[name="bias"](%4235)
  %4238 : int[] = prim::ListConstruct(%12, %12)
  %4239 : int[] = prim::ListConstruct(%8, %8)
  %4240 : int[] = prim::ListConstruct(%12, %12)
  %out.307 : Tensor = aten::conv2d(%input.65, %4236, %4237, %4238, %4239, %4240, %12) # torch/nn/modules/conv.py:415:15
  %4242 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1192)
  %4243 : int = aten::dim(%out.307) # torch/nn/modules/batchnorm.py:276:11
  %4244 : bool = aten::ne(%4243, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4244) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4245 : bool = prim::GetAttr[name="training"](%4242)
   = prim::If(%4245) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4246 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4242)
      %4247 : Tensor = aten::add(%4246, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4242, %4247)
      -> ()
    block1():
      -> ()
  %4248 : bool = prim::GetAttr[name="training"](%4242)
  %4249 : Tensor = prim::GetAttr[name="running_mean"](%4242)
  %4250 : Tensor = prim::GetAttr[name="running_var"](%4242)
  %4251 : Tensor = prim::GetAttr[name="weight"](%4242)
  %4252 : Tensor = prim::GetAttr[name="bias"](%4242)
   = prim::If(%4248) # torch/nn/functional.py:2011:4
    block0():
      %4253 : int[] = aten::size(%out.307) # torch/nn/functional.py:2012:27
      %size_prods.400 : int = aten::__getitem__(%4253, %8) # torch/nn/functional.py:1991:17
      %4255 : int = aten::len(%4253) # torch/nn/functional.py:1992:19
      %4256 : int = aten::sub(%4255, %10) # torch/nn/functional.py:1992:19
      %size_prods.401 : int = prim::Loop(%4256, %9, %size_prods.400) # torch/nn/functional.py:1992:4
        block0(%i.101 : int, %size_prods.402 : int):
          %4260 : int = aten::add(%i.101, %10) # torch/nn/functional.py:1993:27
          %4261 : int = aten::__getitem__(%4253, %4260) # torch/nn/functional.py:1993:22
          %size_prods.403 : int = aten::mul(%size_prods.402, %4261) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.403)
      %4263 : bool = aten::eq(%size_prods.401, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4263) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.299 : Tensor = aten::batch_norm(%out.307, %4251, %4252, %4249, %4250, %4248, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.300 : Tensor = aten::relu_(%out.299) # torch/nn/functional.py:1117:17
  %4266 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1192)
  %4267 : Tensor = prim::GetAttr[name="weight"](%4266)
  %4268 : Tensor? = prim::GetAttr[name="bias"](%4266)
  %4269 : int[] = prim::ListConstruct(%12, %12)
  %4270 : int[] = prim::ListConstruct(%12, %12)
  %4271 : int[] = prim::ListConstruct(%12, %12)
  %out.301 : Tensor = aten::conv2d(%out.300, %4267, %4268, %4269, %4270, %4271, %12) # torch/nn/modules/conv.py:415:15
  %4273 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1192)
  %4274 : int = aten::dim(%out.301) # torch/nn/modules/batchnorm.py:276:11
  %4275 : bool = aten::ne(%4274, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4275) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4276 : bool = prim::GetAttr[name="training"](%4273)
   = prim::If(%4276) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4277 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4273)
      %4278 : Tensor = aten::add(%4277, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4273, %4278)
      -> ()
    block1():
      -> ()
  %4279 : bool = prim::GetAttr[name="training"](%4273)
  %4280 : Tensor = prim::GetAttr[name="running_mean"](%4273)
  %4281 : Tensor = prim::GetAttr[name="running_var"](%4273)
  %4282 : Tensor = prim::GetAttr[name="weight"](%4273)
  %4283 : Tensor = prim::GetAttr[name="bias"](%4273)
   = prim::If(%4279) # torch/nn/functional.py:2011:4
    block0():
      %4284 : int[] = aten::size(%out.301) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%4284, %8) # torch/nn/functional.py:1991:17
      %4286 : int = aten::len(%4284) # torch/nn/functional.py:1992:19
      %4287 : int = aten::sub(%4286, %10) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%4287, %9, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %4291 : int = aten::add(%i.102, %10) # torch/nn/functional.py:1993:27
          %4292 : int = aten::__getitem__(%4284, %4291) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %4292) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.407)
      %4294 : bool = aten::eq(%size_prods.405, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4294) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.302 : Tensor = aten::batch_norm(%out.301, %4282, %4283, %4280, %4281, %4279, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.303 : Tensor = aten::relu_(%out.302) # torch/nn/functional.py:1117:17
  %4297 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1192)
  %4298 : Tensor = prim::GetAttr[name="weight"](%4297)
  %4299 : Tensor? = prim::GetAttr[name="bias"](%4297)
  %4300 : int[] = prim::ListConstruct(%12, %12)
  %4301 : int[] = prim::ListConstruct(%8, %8)
  %4302 : int[] = prim::ListConstruct(%12, %12)
  %out.304 : Tensor = aten::conv2d(%out.303, %4298, %4299, %4300, %4301, %4302, %12) # torch/nn/modules/conv.py:415:15
  %4304 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1192)
  %4305 : int = aten::dim(%out.304) # torch/nn/modules/batchnorm.py:276:11
  %4306 : bool = aten::ne(%4305, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4306) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4307 : bool = prim::GetAttr[name="training"](%4304)
   = prim::If(%4307) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4308 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4304)
      %4309 : Tensor = aten::add(%4308, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4304, %4309)
      -> ()
    block1():
      -> ()
  %4310 : bool = prim::GetAttr[name="training"](%4304)
  %4311 : Tensor = prim::GetAttr[name="running_mean"](%4304)
  %4312 : Tensor = prim::GetAttr[name="running_var"](%4304)
  %4313 : Tensor = prim::GetAttr[name="weight"](%4304)
  %4314 : Tensor = prim::GetAttr[name="bias"](%4304)
   = prim::If(%4310) # torch/nn/functional.py:2011:4
    block0():
      %4315 : int[] = aten::size(%out.304) # torch/nn/functional.py:2012:27
      %size_prods.408 : int = aten::__getitem__(%4315, %8) # torch/nn/functional.py:1991:17
      %4317 : int = aten::len(%4315) # torch/nn/functional.py:1992:19
      %4318 : int = aten::sub(%4317, %10) # torch/nn/functional.py:1992:19
      %size_prods.409 : int = prim::Loop(%4318, %9, %size_prods.408) # torch/nn/functional.py:1992:4
        block0(%i.103 : int, %size_prods.410 : int):
          %4322 : int = aten::add(%i.103, %10) # torch/nn/functional.py:1993:27
          %4323 : int = aten::__getitem__(%4315, %4322) # torch/nn/functional.py:1993:22
          %size_prods.411 : int = aten::mul(%size_prods.410, %4323) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.411)
      %4325 : bool = aten::eq(%size_prods.409, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4325) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.305 : Tensor = aten::batch_norm(%out.304, %4313, %4314, %4311, %4312, %4310, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.306 : Tensor = aten::add_(%out.305, %input.65, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.67 : Tensor = aten::relu_(%out.306) # torch/nn/functional.py:1117:17
  %4329 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1193)
  %4330 : Tensor = prim::GetAttr[name="weight"](%4329)
  %4331 : Tensor? = prim::GetAttr[name="bias"](%4329)
  %4332 : int[] = prim::ListConstruct(%12, %12)
  %4333 : int[] = prim::ListConstruct(%8, %8)
  %4334 : int[] = prim::ListConstruct(%12, %12)
  %out.316 : Tensor = aten::conv2d(%input.67, %4330, %4331, %4332, %4333, %4334, %12) # torch/nn/modules/conv.py:415:15
  %4336 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1193)
  %4337 : int = aten::dim(%out.316) # torch/nn/modules/batchnorm.py:276:11
  %4338 : bool = aten::ne(%4337, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4338) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4339 : bool = prim::GetAttr[name="training"](%4336)
   = prim::If(%4339) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4340 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4336)
      %4341 : Tensor = aten::add(%4340, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4336, %4341)
      -> ()
    block1():
      -> ()
  %4342 : bool = prim::GetAttr[name="training"](%4336)
  %4343 : Tensor = prim::GetAttr[name="running_mean"](%4336)
  %4344 : Tensor = prim::GetAttr[name="running_var"](%4336)
  %4345 : Tensor = prim::GetAttr[name="weight"](%4336)
  %4346 : Tensor = prim::GetAttr[name="bias"](%4336)
   = prim::If(%4342) # torch/nn/functional.py:2011:4
    block0():
      %4347 : int[] = aten::size(%out.316) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%4347, %8) # torch/nn/functional.py:1991:17
      %4349 : int = aten::len(%4347) # torch/nn/functional.py:1992:19
      %4350 : int = aten::sub(%4349, %10) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%4350, %9, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %4354 : int = aten::add(%i.104, %10) # torch/nn/functional.py:1993:27
          %4355 : int = aten::__getitem__(%4347, %4354) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %4355) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.415)
      %4357 : bool = aten::eq(%size_prods.413, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4357) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.308 : Tensor = aten::batch_norm(%out.316, %4345, %4346, %4343, %4344, %4342, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.309 : Tensor = aten::relu_(%out.308) # torch/nn/functional.py:1117:17
  %4360 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1193)
  %4361 : Tensor = prim::GetAttr[name="weight"](%4360)
  %4362 : Tensor? = prim::GetAttr[name="bias"](%4360)
  %4363 : int[] = prim::ListConstruct(%12, %12)
  %4364 : int[] = prim::ListConstruct(%12, %12)
  %4365 : int[] = prim::ListConstruct(%12, %12)
  %out.310 : Tensor = aten::conv2d(%out.309, %4361, %4362, %4363, %4364, %4365, %12) # torch/nn/modules/conv.py:415:15
  %4367 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1193)
  %4368 : int = aten::dim(%out.310) # torch/nn/modules/batchnorm.py:276:11
  %4369 : bool = aten::ne(%4368, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4369) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4370 : bool = prim::GetAttr[name="training"](%4367)
   = prim::If(%4370) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4371 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4367)
      %4372 : Tensor = aten::add(%4371, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4367, %4372)
      -> ()
    block1():
      -> ()
  %4373 : bool = prim::GetAttr[name="training"](%4367)
  %4374 : Tensor = prim::GetAttr[name="running_mean"](%4367)
  %4375 : Tensor = prim::GetAttr[name="running_var"](%4367)
  %4376 : Tensor = prim::GetAttr[name="weight"](%4367)
  %4377 : Tensor = prim::GetAttr[name="bias"](%4367)
   = prim::If(%4373) # torch/nn/functional.py:2011:4
    block0():
      %4378 : int[] = aten::size(%out.310) # torch/nn/functional.py:2012:27
      %size_prods.416 : int = aten::__getitem__(%4378, %8) # torch/nn/functional.py:1991:17
      %4380 : int = aten::len(%4378) # torch/nn/functional.py:1992:19
      %4381 : int = aten::sub(%4380, %10) # torch/nn/functional.py:1992:19
      %size_prods.417 : int = prim::Loop(%4381, %9, %size_prods.416) # torch/nn/functional.py:1992:4
        block0(%i.105 : int, %size_prods.418 : int):
          %4385 : int = aten::add(%i.105, %10) # torch/nn/functional.py:1993:27
          %4386 : int = aten::__getitem__(%4378, %4385) # torch/nn/functional.py:1993:22
          %size_prods.419 : int = aten::mul(%size_prods.418, %4386) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.419)
      %4388 : bool = aten::eq(%size_prods.417, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4388) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.311 : Tensor = aten::batch_norm(%out.310, %4376, %4377, %4374, %4375, %4373, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.312 : Tensor = aten::relu_(%out.311) # torch/nn/functional.py:1117:17
  %4391 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1193)
  %4392 : Tensor = prim::GetAttr[name="weight"](%4391)
  %4393 : Tensor? = prim::GetAttr[name="bias"](%4391)
  %4394 : int[] = prim::ListConstruct(%12, %12)
  %4395 : int[] = prim::ListConstruct(%8, %8)
  %4396 : int[] = prim::ListConstruct(%12, %12)
  %out.313 : Tensor = aten::conv2d(%out.312, %4392, %4393, %4394, %4395, %4396, %12) # torch/nn/modules/conv.py:415:15
  %4398 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1193)
  %4399 : int = aten::dim(%out.313) # torch/nn/modules/batchnorm.py:276:11
  %4400 : bool = aten::ne(%4399, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4400) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4401 : bool = prim::GetAttr[name="training"](%4398)
   = prim::If(%4401) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4402 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4398)
      %4403 : Tensor = aten::add(%4402, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4398, %4403)
      -> ()
    block1():
      -> ()
  %4404 : bool = prim::GetAttr[name="training"](%4398)
  %4405 : Tensor = prim::GetAttr[name="running_mean"](%4398)
  %4406 : Tensor = prim::GetAttr[name="running_var"](%4398)
  %4407 : Tensor = prim::GetAttr[name="weight"](%4398)
  %4408 : Tensor = prim::GetAttr[name="bias"](%4398)
   = prim::If(%4404) # torch/nn/functional.py:2011:4
    block0():
      %4409 : int[] = aten::size(%out.313) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%4409, %8) # torch/nn/functional.py:1991:17
      %4411 : int = aten::len(%4409) # torch/nn/functional.py:1992:19
      %4412 : int = aten::sub(%4411, %10) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%4412, %9, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %4416 : int = aten::add(%i.106, %10) # torch/nn/functional.py:1993:27
          %4417 : int = aten::__getitem__(%4409, %4416) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %4417) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.423)
      %4419 : bool = aten::eq(%size_prods.421, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4419) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.314 : Tensor = aten::batch_norm(%out.313, %4407, %4408, %4405, %4406, %4404, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.315 : Tensor = aten::add_(%out.314, %input.67, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.69 : Tensor = aten::relu_(%out.315) # torch/nn/functional.py:1117:17
  %4423 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1194)
  %4424 : Tensor = prim::GetAttr[name="weight"](%4423)
  %4425 : Tensor? = prim::GetAttr[name="bias"](%4423)
  %4426 : int[] = prim::ListConstruct(%12, %12)
  %4427 : int[] = prim::ListConstruct(%8, %8)
  %4428 : int[] = prim::ListConstruct(%12, %12)
  %out.325 : Tensor = aten::conv2d(%input.69, %4424, %4425, %4426, %4427, %4428, %12) # torch/nn/modules/conv.py:415:15
  %4430 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1194)
  %4431 : int = aten::dim(%out.325) # torch/nn/modules/batchnorm.py:276:11
  %4432 : bool = aten::ne(%4431, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4432) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4433 : bool = prim::GetAttr[name="training"](%4430)
   = prim::If(%4433) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4434 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4430)
      %4435 : Tensor = aten::add(%4434, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4430, %4435)
      -> ()
    block1():
      -> ()
  %4436 : bool = prim::GetAttr[name="training"](%4430)
  %4437 : Tensor = prim::GetAttr[name="running_mean"](%4430)
  %4438 : Tensor = prim::GetAttr[name="running_var"](%4430)
  %4439 : Tensor = prim::GetAttr[name="weight"](%4430)
  %4440 : Tensor = prim::GetAttr[name="bias"](%4430)
   = prim::If(%4436) # torch/nn/functional.py:2011:4
    block0():
      %4441 : int[] = aten::size(%out.325) # torch/nn/functional.py:2012:27
      %size_prods.424 : int = aten::__getitem__(%4441, %8) # torch/nn/functional.py:1991:17
      %4443 : int = aten::len(%4441) # torch/nn/functional.py:1992:19
      %4444 : int = aten::sub(%4443, %10) # torch/nn/functional.py:1992:19
      %size_prods.425 : int = prim::Loop(%4444, %9, %size_prods.424) # torch/nn/functional.py:1992:4
        block0(%i.107 : int, %size_prods.426 : int):
          %4448 : int = aten::add(%i.107, %10) # torch/nn/functional.py:1993:27
          %4449 : int = aten::__getitem__(%4441, %4448) # torch/nn/functional.py:1993:22
          %size_prods.427 : int = aten::mul(%size_prods.426, %4449) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.427)
      %4451 : bool = aten::eq(%size_prods.425, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4451) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.317 : Tensor = aten::batch_norm(%out.325, %4439, %4440, %4437, %4438, %4436, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.318 : Tensor = aten::relu_(%out.317) # torch/nn/functional.py:1117:17
  %4454 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1194)
  %4455 : Tensor = prim::GetAttr[name="weight"](%4454)
  %4456 : Tensor? = prim::GetAttr[name="bias"](%4454)
  %4457 : int[] = prim::ListConstruct(%12, %12)
  %4458 : int[] = prim::ListConstruct(%12, %12)
  %4459 : int[] = prim::ListConstruct(%12, %12)
  %out.319 : Tensor = aten::conv2d(%out.318, %4455, %4456, %4457, %4458, %4459, %12) # torch/nn/modules/conv.py:415:15
  %4461 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1194)
  %4462 : int = aten::dim(%out.319) # torch/nn/modules/batchnorm.py:276:11
  %4463 : bool = aten::ne(%4462, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4463) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4464 : bool = prim::GetAttr[name="training"](%4461)
   = prim::If(%4464) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4465 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4461)
      %4466 : Tensor = aten::add(%4465, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4461, %4466)
      -> ()
    block1():
      -> ()
  %4467 : bool = prim::GetAttr[name="training"](%4461)
  %4468 : Tensor = prim::GetAttr[name="running_mean"](%4461)
  %4469 : Tensor = prim::GetAttr[name="running_var"](%4461)
  %4470 : Tensor = prim::GetAttr[name="weight"](%4461)
  %4471 : Tensor = prim::GetAttr[name="bias"](%4461)
   = prim::If(%4467) # torch/nn/functional.py:2011:4
    block0():
      %4472 : int[] = aten::size(%out.319) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%4472, %8) # torch/nn/functional.py:1991:17
      %4474 : int = aten::len(%4472) # torch/nn/functional.py:1992:19
      %4475 : int = aten::sub(%4474, %10) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%4475, %9, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.430 : int):
          %4479 : int = aten::add(%i.108, %10) # torch/nn/functional.py:1993:27
          %4480 : int = aten::__getitem__(%4472, %4479) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %4480) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.431)
      %4482 : bool = aten::eq(%size_prods.429, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4482) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.320 : Tensor = aten::batch_norm(%out.319, %4470, %4471, %4468, %4469, %4467, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.321 : Tensor = aten::relu_(%out.320) # torch/nn/functional.py:1117:17
  %4485 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1194)
  %4486 : Tensor = prim::GetAttr[name="weight"](%4485)
  %4487 : Tensor? = prim::GetAttr[name="bias"](%4485)
  %4488 : int[] = prim::ListConstruct(%12, %12)
  %4489 : int[] = prim::ListConstruct(%8, %8)
  %4490 : int[] = prim::ListConstruct(%12, %12)
  %out.322 : Tensor = aten::conv2d(%out.321, %4486, %4487, %4488, %4489, %4490, %12) # torch/nn/modules/conv.py:415:15
  %4492 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1194)
  %4493 : int = aten::dim(%out.322) # torch/nn/modules/batchnorm.py:276:11
  %4494 : bool = aten::ne(%4493, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4494) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4495 : bool = prim::GetAttr[name="training"](%4492)
   = prim::If(%4495) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4496 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4492)
      %4497 : Tensor = aten::add(%4496, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4492, %4497)
      -> ()
    block1():
      -> ()
  %4498 : bool = prim::GetAttr[name="training"](%4492)
  %4499 : Tensor = prim::GetAttr[name="running_mean"](%4492)
  %4500 : Tensor = prim::GetAttr[name="running_var"](%4492)
  %4501 : Tensor = prim::GetAttr[name="weight"](%4492)
  %4502 : Tensor = prim::GetAttr[name="bias"](%4492)
   = prim::If(%4498) # torch/nn/functional.py:2011:4
    block0():
      %4503 : int[] = aten::size(%out.322) # torch/nn/functional.py:2012:27
      %size_prods.432 : int = aten::__getitem__(%4503, %8) # torch/nn/functional.py:1991:17
      %4505 : int = aten::len(%4503) # torch/nn/functional.py:1992:19
      %4506 : int = aten::sub(%4505, %10) # torch/nn/functional.py:1992:19
      %size_prods.433 : int = prim::Loop(%4506, %9, %size_prods.432) # torch/nn/functional.py:1992:4
        block0(%i.109 : int, %size_prods.434 : int):
          %4510 : int = aten::add(%i.109, %10) # torch/nn/functional.py:1993:27
          %4511 : int = aten::__getitem__(%4503, %4510) # torch/nn/functional.py:1993:22
          %size_prods.435 : int = aten::mul(%size_prods.434, %4511) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.435)
      %4513 : bool = aten::eq(%size_prods.433, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4513) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.323 : Tensor = aten::batch_norm(%out.322, %4501, %4502, %4499, %4500, %4498, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.324 : Tensor = aten::add_(%out.323, %input.69, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.71 : Tensor = aten::relu_(%out.324) # torch/nn/functional.py:1117:17
  %4517 : __torch__.torch.nn.modules.conv.___torch_mangle_35.Conv2d = prim::GetAttr[name="conv1"](%1195)
  %4518 : Tensor = prim::GetAttr[name="weight"](%4517)
  %4519 : Tensor? = prim::GetAttr[name="bias"](%4517)
  %4520 : int[] = prim::ListConstruct(%12, %12)
  %4521 : int[] = prim::ListConstruct(%8, %8)
  %4522 : int[] = prim::ListConstruct(%12, %12)
  %out.442 : Tensor = aten::conv2d(%input.71, %4518, %4519, %4520, %4521, %4522, %12) # torch/nn/modules/conv.py:415:15
  %4524 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn1"](%1195)
  %4525 : int = aten::dim(%out.442) # torch/nn/modules/batchnorm.py:276:11
  %4526 : bool = aten::ne(%4525, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4526) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4527 : bool = prim::GetAttr[name="training"](%4524)
   = prim::If(%4527) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4528 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4524)
      %4529 : Tensor = aten::add(%4528, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4524, %4529)
      -> ()
    block1():
      -> ()
  %4530 : bool = prim::GetAttr[name="training"](%4524)
  %4531 : Tensor = prim::GetAttr[name="running_mean"](%4524)
  %4532 : Tensor = prim::GetAttr[name="running_var"](%4524)
  %4533 : Tensor = prim::GetAttr[name="weight"](%4524)
  %4534 : Tensor = prim::GetAttr[name="bias"](%4524)
   = prim::If(%4530) # torch/nn/functional.py:2011:4
    block0():
      %4535 : int[] = aten::size(%out.442) # torch/nn/functional.py:2012:27
      %size_prods.608 : int = aten::__getitem__(%4535, %8) # torch/nn/functional.py:1991:17
      %4537 : int = aten::len(%4535) # torch/nn/functional.py:1992:19
      %4538 : int = aten::sub(%4537, %10) # torch/nn/functional.py:1992:19
      %size_prods.609 : int = prim::Loop(%4538, %9, %size_prods.608) # torch/nn/functional.py:1992:4
        block0(%i.153 : int, %size_prods.610 : int):
          %4542 : int = aten::add(%i.153, %10) # torch/nn/functional.py:1993:27
          %4543 : int = aten::__getitem__(%4535, %4542) # torch/nn/functional.py:1993:22
          %size_prods.611 : int = aten::mul(%size_prods.610, %4543) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.611)
      %4545 : bool = aten::eq(%size_prods.609, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4545) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.443 : Tensor = aten::batch_norm(%out.442, %4533, %4534, %4531, %4532, %4530, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.444 : Tensor = aten::relu_(%out.443) # torch/nn/functional.py:1117:17
  %4548 : __torch__.torch.nn.modules.conv.___torch_mangle_3.Conv2d = prim::GetAttr[name="conv2"](%1195)
  %4549 : Tensor = prim::GetAttr[name="weight"](%4548)
  %4550 : Tensor? = prim::GetAttr[name="bias"](%4548)
  %4551 : int[] = prim::ListConstruct(%12, %12)
  %4552 : int[] = prim::ListConstruct(%12, %12)
  %4553 : int[] = prim::ListConstruct(%12, %12)
  %out.445 : Tensor = aten::conv2d(%out.444, %4549, %4550, %4551, %4552, %4553, %12) # torch/nn/modules/conv.py:415:15
  %4555 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_12.BatchNorm2d = prim::GetAttr[name="bn2"](%1195)
  %4556 : int = aten::dim(%out.445) # torch/nn/modules/batchnorm.py:276:11
  %4557 : bool = aten::ne(%4556, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4557) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4558 : bool = prim::GetAttr[name="training"](%4555)
   = prim::If(%4558) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4559 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4555)
      %4560 : Tensor = aten::add(%4559, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4555, %4560)
      -> ()
    block1():
      -> ()
  %4561 : bool = prim::GetAttr[name="training"](%4555)
  %4562 : Tensor = prim::GetAttr[name="running_mean"](%4555)
  %4563 : Tensor = prim::GetAttr[name="running_var"](%4555)
  %4564 : Tensor = prim::GetAttr[name="weight"](%4555)
  %4565 : Tensor = prim::GetAttr[name="bias"](%4555)
   = prim::If(%4561) # torch/nn/functional.py:2011:4
    block0():
      %4566 : int[] = aten::size(%out.445) # torch/nn/functional.py:2012:27
      %size_prods.612 : int = aten::__getitem__(%4566, %8) # torch/nn/functional.py:1991:17
      %4568 : int = aten::len(%4566) # torch/nn/functional.py:1992:19
      %4569 : int = aten::sub(%4568, %10) # torch/nn/functional.py:1992:19
      %size_prods.613 : int = prim::Loop(%4569, %9, %size_prods.612) # torch/nn/functional.py:1992:4
        block0(%i.154 : int, %size_prods.614 : int):
          %4573 : int = aten::add(%i.154, %10) # torch/nn/functional.py:1993:27
          %4574 : int = aten::__getitem__(%4566, %4573) # torch/nn/functional.py:1993:22
          %size_prods.615 : int = aten::mul(%size_prods.614, %4574) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.615)
      %4576 : bool = aten::eq(%size_prods.613, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4576) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.446 : Tensor = aten::batch_norm(%out.445, %4564, %4565, %4562, %4563, %4561, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.447 : Tensor = aten::relu_(%out.446) # torch/nn/functional.py:1117:17
  %4579 : __torch__.torch.nn.modules.conv.___torch_mangle_30.Conv2d = prim::GetAttr[name="conv3"](%1195)
  %4580 : Tensor = prim::GetAttr[name="weight"](%4579)
  %4581 : Tensor? = prim::GetAttr[name="bias"](%4579)
  %4582 : int[] = prim::ListConstruct(%12, %12)
  %4583 : int[] = prim::ListConstruct(%8, %8)
  %4584 : int[] = prim::ListConstruct(%12, %12)
  %out.448 : Tensor = aten::conv2d(%out.447, %4580, %4581, %4582, %4583, %4584, %12) # torch/nn/modules/conv.py:415:15
  %4586 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_31.BatchNorm2d = prim::GetAttr[name="bn3"](%1195)
  %4587 : int = aten::dim(%out.448) # torch/nn/modules/batchnorm.py:276:11
  %4588 : bool = aten::ne(%4587, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4588) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4589 : bool = prim::GetAttr[name="training"](%4586)
   = prim::If(%4589) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4590 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4586)
      %4591 : Tensor = aten::add(%4590, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4586, %4591)
      -> ()
    block1():
      -> ()
  %4592 : bool = prim::GetAttr[name="training"](%4586)
  %4593 : Tensor = prim::GetAttr[name="running_mean"](%4586)
  %4594 : Tensor = prim::GetAttr[name="running_var"](%4586)
  %4595 : Tensor = prim::GetAttr[name="weight"](%4586)
  %4596 : Tensor = prim::GetAttr[name="bias"](%4586)
   = prim::If(%4592) # torch/nn/functional.py:2011:4
    block0():
      %4597 : int[] = aten::size(%out.448) # torch/nn/functional.py:2012:27
      %size_prods.616 : int = aten::__getitem__(%4597, %8) # torch/nn/functional.py:1991:17
      %4599 : int = aten::len(%4597) # torch/nn/functional.py:1992:19
      %4600 : int = aten::sub(%4599, %10) # torch/nn/functional.py:1992:19
      %size_prods.617 : int = prim::Loop(%4600, %9, %size_prods.616) # torch/nn/functional.py:1992:4
        block0(%i.155 : int, %size_prods.618 : int):
          %4604 : int = aten::add(%i.155, %10) # torch/nn/functional.py:1993:27
          %4605 : int = aten::__getitem__(%4597, %4604) # torch/nn/functional.py:1993:22
          %size_prods.619 : int = aten::mul(%size_prods.618, %4605) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.619)
      %4607 : bool = aten::eq(%size_prods.617, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4607) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.449 : Tensor = aten::batch_norm(%out.448, %4595, %4596, %4593, %4594, %4592, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.450 : Tensor = aten::add_(%out.449, %input.71, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.15 : Tensor = aten::relu_(%out.450) # torch/nn/functional.py:1117:17
  %4611 : __torch__.torch.nn.modules.container.___torch_mangle_950.Sequential = prim::GetAttr[name="layer4"](%self)
  %4612 : __torch__.torchvision.models.resnet.___torch_mangle_947.Bottleneck = prim::GetAttr[name="0"](%4611)
  %4613 : __torch__.torchvision.models.resnet.___torch_mangle_949.Bottleneck = prim::GetAttr[name="1"](%4611)
  %4614 : __torch__.torchvision.models.resnet.___torch_mangle_949.Bottleneck = prim::GetAttr[name="2"](%4611)
  %4615 : __torch__.torch.nn.modules.conv.___torch_mangle_39.Conv2d = prim::GetAttr[name="conv1"](%4612)
  %4616 : Tensor = prim::GetAttr[name="weight"](%4615)
  %4617 : Tensor? = prim::GetAttr[name="bias"](%4615)
  %4618 : int[] = prim::ListConstruct(%12, %12)
  %4619 : int[] = prim::ListConstruct(%8, %8)
  %4620 : int[] = prim::ListConstruct(%12, %12)
  %out.2 : Tensor = aten::conv2d(%x.15, %4616, %4617, %4618, %4619, %4620, %12) # torch/nn/modules/conv.py:415:15
  %4622 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%4612)
  %4623 : int = aten::dim(%out.2) # torch/nn/modules/batchnorm.py:276:11
  %4624 : bool = aten::ne(%4623, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4624) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4625 : bool = prim::GetAttr[name="training"](%4622)
   = prim::If(%4625) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4626 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4622)
      %4627 : Tensor = aten::add(%4626, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4622, %4627)
      -> ()
    block1():
      -> ()
  %4628 : bool = prim::GetAttr[name="training"](%4622)
  %4629 : Tensor = prim::GetAttr[name="running_mean"](%4622)
  %4630 : Tensor = prim::GetAttr[name="running_var"](%4622)
  %4631 : Tensor = prim::GetAttr[name="weight"](%4622)
  %4632 : Tensor = prim::GetAttr[name="bias"](%4622)
   = prim::If(%4628) # torch/nn/functional.py:2011:4
    block0():
      %4633 : int[] = aten::size(%out.2) # torch/nn/functional.py:2012:27
      %size_prods.16 : int = aten::__getitem__(%4633, %8) # torch/nn/functional.py:1991:17
      %4635 : int = aten::len(%4633) # torch/nn/functional.py:1992:19
      %4636 : int = aten::sub(%4635, %10) # torch/nn/functional.py:1992:19
      %size_prods.17 : int = prim::Loop(%4636, %9, %size_prods.16) # torch/nn/functional.py:1992:4
        block0(%i.5 : int, %size_prods.18 : int):
          %4640 : int = aten::add(%i.5, %10) # torch/nn/functional.py:1993:27
          %4641 : int = aten::__getitem__(%4633, %4640) # torch/nn/functional.py:1993:22
          %size_prods.19 : int = aten::mul(%size_prods.18, %4641) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.19)
      %4643 : bool = aten::eq(%size_prods.17, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4643) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.4 : Tensor = aten::batch_norm(%out.2, %4631, %4632, %4629, %4630, %4628, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.6 : Tensor = aten::relu_(%out.4) # torch/nn/functional.py:1117:17
  %4646 : __torch__.torch.nn.modules.conv.___torch_mangle_944.Conv2d = prim::GetAttr[name="conv2"](%4612)
  %4647 : Tensor = prim::GetAttr[name="weight"](%4646)
  %4648 : Tensor? = prim::GetAttr[name="bias"](%4646)
  %4649 : int[] = prim::ListConstruct(%10, %10)
  %4650 : int[] = prim::ListConstruct(%12, %12)
  %4651 : int[] = prim::ListConstruct(%12, %12)
  %out.8 : Tensor = aten::conv2d(%out.6, %4647, %4648, %4649, %4650, %4651, %12) # torch/nn/modules/conv.py:415:15
  %4653 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%4612)
  %4654 : int = aten::dim(%out.8) # torch/nn/modules/batchnorm.py:276:11
  %4655 : bool = aten::ne(%4654, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4655) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4656 : bool = prim::GetAttr[name="training"](%4653)
   = prim::If(%4656) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4657 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4653)
      %4658 : Tensor = aten::add(%4657, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4653, %4658)
      -> ()
    block1():
      -> ()
  %4659 : bool = prim::GetAttr[name="training"](%4653)
  %4660 : Tensor = prim::GetAttr[name="running_mean"](%4653)
  %4661 : Tensor = prim::GetAttr[name="running_var"](%4653)
  %4662 : Tensor = prim::GetAttr[name="weight"](%4653)
  %4663 : Tensor = prim::GetAttr[name="bias"](%4653)
   = prim::If(%4659) # torch/nn/functional.py:2011:4
    block0():
      %4664 : int[] = aten::size(%out.8) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%4664, %8) # torch/nn/functional.py:1991:17
      %4666 : int = aten::len(%4664) # torch/nn/functional.py:1992:19
      %4667 : int = aten::sub(%4666, %10) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%4667, %9, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %4671 : int = aten::add(%i.6, %10) # torch/nn/functional.py:1993:27
          %4672 : int = aten::__getitem__(%4664, %4671) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %4672) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.23)
      %4674 : bool = aten::eq(%size_prods.21, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4674) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.10 : Tensor = aten::batch_norm(%out.8, %4662, %4663, %4660, %4661, %4659, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.12 : Tensor = aten::relu_(%out.10) # torch/nn/functional.py:1117:17
  %4677 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%4612)
  %4678 : Tensor = prim::GetAttr[name="weight"](%4677)
  %4679 : Tensor? = prim::GetAttr[name="bias"](%4677)
  %4680 : int[] = prim::ListConstruct(%12, %12)
  %4681 : int[] = prim::ListConstruct(%8, %8)
  %4682 : int[] = prim::ListConstruct(%12, %12)
  %out.14 : Tensor = aten::conv2d(%out.12, %4678, %4679, %4680, %4681, %4682, %12) # torch/nn/modules/conv.py:415:15
  %4684 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%4612)
  %4685 : int = aten::dim(%out.14) # torch/nn/modules/batchnorm.py:276:11
  %4686 : bool = aten::ne(%4685, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4686) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4687 : bool = prim::GetAttr[name="training"](%4684)
   = prim::If(%4687) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4688 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4684)
      %4689 : Tensor = aten::add(%4688, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4684, %4689)
      -> ()
    block1():
      -> ()
  %4690 : bool = prim::GetAttr[name="training"](%4684)
  %4691 : Tensor = prim::GetAttr[name="running_mean"](%4684)
  %4692 : Tensor = prim::GetAttr[name="running_var"](%4684)
  %4693 : Tensor = prim::GetAttr[name="weight"](%4684)
  %4694 : Tensor = prim::GetAttr[name="bias"](%4684)
   = prim::If(%4690) # torch/nn/functional.py:2011:4
    block0():
      %4695 : int[] = aten::size(%out.14) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%4695, %8) # torch/nn/functional.py:1991:17
      %4697 : int = aten::len(%4695) # torch/nn/functional.py:1992:19
      %4698 : int = aten::sub(%4697, %10) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%4698, %9, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %4702 : int = aten::add(%i.4, %10) # torch/nn/functional.py:1993:27
          %4703 : int = aten::__getitem__(%4695, %4702) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %4703) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.15)
      %4705 : bool = aten::eq(%size_prods.13, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4705) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.16 : Tensor = aten::batch_norm(%out.14, %4693, %4694, %4691, %4692, %4690, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %4707 : __torch__.torch.nn.modules.container.___torch_mangle_946.Sequential = prim::GetAttr[name="downsample"](%4612)
  %4708 : __torch__.torch.nn.modules.conv.___torch_mangle_945.Conv2d = prim::GetAttr[name="0"](%4707)
  %4709 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="1"](%4707)
  %4710 : Tensor = prim::GetAttr[name="weight"](%4708)
  %4711 : Tensor? = prim::GetAttr[name="bias"](%4708)
  %4712 : int[] = prim::ListConstruct(%10, %10)
  %4713 : int[] = prim::ListConstruct(%8, %8)
  %4714 : int[] = prim::ListConstruct(%12, %12)
  %input.3 : Tensor = aten::conv2d(%x.15, %4710, %4711, %4712, %4713, %4714, %12) # torch/nn/modules/conv.py:415:15
  %4716 : int = aten::dim(%input.3) # torch/nn/modules/batchnorm.py:276:11
  %4717 : bool = aten::ne(%4716, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4717) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4718 : bool = prim::GetAttr[name="training"](%4709)
   = prim::If(%4718) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4719 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4709)
      %4720 : Tensor = aten::add(%4719, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4709, %4720)
      -> ()
    block1():
      -> ()
  %4721 : bool = prim::GetAttr[name="training"](%4709)
  %4722 : Tensor = prim::GetAttr[name="running_mean"](%4709)
  %4723 : Tensor = prim::GetAttr[name="running_var"](%4709)
  %4724 : Tensor = prim::GetAttr[name="weight"](%4709)
  %4725 : Tensor = prim::GetAttr[name="bias"](%4709)
   = prim::If(%4721) # torch/nn/functional.py:2011:4
    block0():
      %4726 : int[] = aten::size(%input.3) # torch/nn/functional.py:2012:27
      %size_prods.24 : int = aten::__getitem__(%4726, %8) # torch/nn/functional.py:1991:17
      %4728 : int = aten::len(%4726) # torch/nn/functional.py:1992:19
      %4729 : int = aten::sub(%4728, %10) # torch/nn/functional.py:1992:19
      %size_prods.25 : int = prim::Loop(%4729, %9, %size_prods.24) # torch/nn/functional.py:1992:4
        block0(%i.7 : int, %size_prods.26 : int):
          %4733 : int = aten::add(%i.7, %10) # torch/nn/functional.py:1993:27
          %4734 : int = aten::__getitem__(%4726, %4733) # torch/nn/functional.py:1993:22
          %size_prods.27 : int = aten::mul(%size_prods.26, %4734) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.27)
      %4736 : bool = aten::eq(%size_prods.25, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4736) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %identity.1 : Tensor = aten::batch_norm(%input.3, %4724, %4725, %4722, %4723, %4721, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.18 : Tensor = aten::add_(%out.16, %identity.1, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.4 : Tensor = aten::relu_(%out.18) # torch/nn/functional.py:1117:17
  %4740 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%4613)
  %4741 : Tensor = prim::GetAttr[name="weight"](%4740)
  %4742 : Tensor? = prim::GetAttr[name="bias"](%4740)
  %4743 : int[] = prim::ListConstruct(%12, %12)
  %4744 : int[] = prim::ListConstruct(%8, %8)
  %4745 : int[] = prim::ListConstruct(%12, %12)
  %out.28 : Tensor = aten::conv2d(%input.4, %4741, %4742, %4743, %4744, %4745, %12) # torch/nn/modules/conv.py:415:15
  %4747 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%4613)
  %4748 : int = aten::dim(%out.28) # torch/nn/modules/batchnorm.py:276:11
  %4749 : bool = aten::ne(%4748, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4749) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4750 : bool = prim::GetAttr[name="training"](%4747)
   = prim::If(%4750) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4751 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4747)
      %4752 : Tensor = aten::add(%4751, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4747, %4752)
      -> ()
    block1():
      -> ()
  %4753 : bool = prim::GetAttr[name="training"](%4747)
  %4754 : Tensor = prim::GetAttr[name="running_mean"](%4747)
  %4755 : Tensor = prim::GetAttr[name="running_var"](%4747)
  %4756 : Tensor = prim::GetAttr[name="weight"](%4747)
  %4757 : Tensor = prim::GetAttr[name="bias"](%4747)
   = prim::If(%4753) # torch/nn/functional.py:2011:4
    block0():
      %4758 : int[] = aten::size(%out.28) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%4758, %8) # torch/nn/functional.py:1991:17
      %4760 : int = aten::len(%4758) # torch/nn/functional.py:1992:19
      %4761 : int = aten::sub(%4760, %10) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%4761, %9, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %4765 : int = aten::add(%i.8, %10) # torch/nn/functional.py:1993:27
          %4766 : int = aten::__getitem__(%4758, %4765) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %4766) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.31)
      %4768 : bool = aten::eq(%size_prods.29, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4768) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.20 : Tensor = aten::batch_norm(%out.28, %4756, %4757, %4754, %4755, %4753, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.21 : Tensor = aten::relu_(%out.20) # torch/nn/functional.py:1117:17
  %4771 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%4613)
  %4772 : Tensor = prim::GetAttr[name="weight"](%4771)
  %4773 : Tensor? = prim::GetAttr[name="bias"](%4771)
  %4774 : int[] = prim::ListConstruct(%12, %12)
  %4775 : int[] = prim::ListConstruct(%12, %12)
  %4776 : int[] = prim::ListConstruct(%12, %12)
  %out.22 : Tensor = aten::conv2d(%out.21, %4772, %4773, %4774, %4775, %4776, %12) # torch/nn/modules/conv.py:415:15
  %4778 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%4613)
  %4779 : int = aten::dim(%out.22) # torch/nn/modules/batchnorm.py:276:11
  %4780 : bool = aten::ne(%4779, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4780) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4781 : bool = prim::GetAttr[name="training"](%4778)
   = prim::If(%4781) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4782 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4778)
      %4783 : Tensor = aten::add(%4782, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4778, %4783)
      -> ()
    block1():
      -> ()
  %4784 : bool = prim::GetAttr[name="training"](%4778)
  %4785 : Tensor = prim::GetAttr[name="running_mean"](%4778)
  %4786 : Tensor = prim::GetAttr[name="running_var"](%4778)
  %4787 : Tensor = prim::GetAttr[name="weight"](%4778)
  %4788 : Tensor = prim::GetAttr[name="bias"](%4778)
   = prim::If(%4784) # torch/nn/functional.py:2011:4
    block0():
      %4789 : int[] = aten::size(%out.22) # torch/nn/functional.py:2012:27
      %size_prods.32 : int = aten::__getitem__(%4789, %8) # torch/nn/functional.py:1991:17
      %4791 : int = aten::len(%4789) # torch/nn/functional.py:1992:19
      %4792 : int = aten::sub(%4791, %10) # torch/nn/functional.py:1992:19
      %size_prods.33 : int = prim::Loop(%4792, %9, %size_prods.32) # torch/nn/functional.py:1992:4
        block0(%i.9 : int, %size_prods.34 : int):
          %4796 : int = aten::add(%i.9, %10) # torch/nn/functional.py:1993:27
          %4797 : int = aten::__getitem__(%4789, %4796) # torch/nn/functional.py:1993:22
          %size_prods.35 : int = aten::mul(%size_prods.34, %4797) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.35)
      %4799 : bool = aten::eq(%size_prods.33, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4799) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.23 : Tensor = aten::batch_norm(%out.22, %4787, %4788, %4785, %4786, %4784, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.24 : Tensor = aten::relu_(%out.23) # torch/nn/functional.py:1117:17
  %4802 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%4613)
  %4803 : Tensor = prim::GetAttr[name="weight"](%4802)
  %4804 : Tensor? = prim::GetAttr[name="bias"](%4802)
  %4805 : int[] = prim::ListConstruct(%12, %12)
  %4806 : int[] = prim::ListConstruct(%8, %8)
  %4807 : int[] = prim::ListConstruct(%12, %12)
  %out.25 : Tensor = aten::conv2d(%out.24, %4803, %4804, %4805, %4806, %4807, %12) # torch/nn/modules/conv.py:415:15
  %4809 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%4613)
  %4810 : int = aten::dim(%out.25) # torch/nn/modules/batchnorm.py:276:11
  %4811 : bool = aten::ne(%4810, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4811) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4812 : bool = prim::GetAttr[name="training"](%4809)
   = prim::If(%4812) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4813 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4809)
      %4814 : Tensor = aten::add(%4813, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4809, %4814)
      -> ()
    block1():
      -> ()
  %4815 : bool = prim::GetAttr[name="training"](%4809)
  %4816 : Tensor = prim::GetAttr[name="running_mean"](%4809)
  %4817 : Tensor = prim::GetAttr[name="running_var"](%4809)
  %4818 : Tensor = prim::GetAttr[name="weight"](%4809)
  %4819 : Tensor = prim::GetAttr[name="bias"](%4809)
   = prim::If(%4815) # torch/nn/functional.py:2011:4
    block0():
      %4820 : int[] = aten::size(%out.25) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%4820, %8) # torch/nn/functional.py:1991:17
      %4822 : int = aten::len(%4820) # torch/nn/functional.py:1992:19
      %4823 : int = aten::sub(%4822, %10) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%4823, %9, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %4827 : int = aten::add(%i.10, %10) # torch/nn/functional.py:1993:27
          %4828 : int = aten::__getitem__(%4820, %4827) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %4828) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.39)
      %4830 : bool = aten::eq(%size_prods.37, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4830) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.26 : Tensor = aten::batch_norm(%out.25, %4818, %4819, %4816, %4817, %4815, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.27 : Tensor = aten::add_(%out.26, %input.4, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %input.5 : Tensor = aten::relu_(%out.27) # torch/nn/functional.py:1117:17
  %4834 : __torch__.torch.nn.modules.conv.___torch_mangle_46.Conv2d = prim::GetAttr[name="conv1"](%4614)
  %4835 : Tensor = prim::GetAttr[name="weight"](%4834)
  %4836 : Tensor? = prim::GetAttr[name="bias"](%4834)
  %4837 : int[] = prim::ListConstruct(%12, %12)
  %4838 : int[] = prim::ListConstruct(%8, %8)
  %4839 : int[] = prim::ListConstruct(%12, %12)
  %out.1 : Tensor = aten::conv2d(%input.5, %4835, %4836, %4837, %4838, %4839, %12) # torch/nn/modules/conv.py:415:15
  %4841 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn1"](%4614)
  %4842 : int = aten::dim(%out.1) # torch/nn/modules/batchnorm.py:276:11
  %4843 : bool = aten::ne(%4842, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4844 : bool = prim::GetAttr[name="training"](%4841)
   = prim::If(%4844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4841)
      %4846 : Tensor = aten::add(%4845, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4841, %4846)
      -> ()
    block1():
      -> ()
  %4847 : bool = prim::GetAttr[name="training"](%4841)
  %4848 : Tensor = prim::GetAttr[name="running_mean"](%4841)
  %4849 : Tensor = prim::GetAttr[name="running_var"](%4841)
  %4850 : Tensor = prim::GetAttr[name="weight"](%4841)
  %4851 : Tensor = prim::GetAttr[name="bias"](%4841)
   = prim::If(%4847) # torch/nn/functional.py:2011:4
    block0():
      %4852 : int[] = aten::size(%out.1) # torch/nn/functional.py:2012:27
      %size_prods.2 : int = aten::__getitem__(%4852, %8) # torch/nn/functional.py:1991:17
      %4854 : int = aten::len(%4852) # torch/nn/functional.py:1992:19
      %4855 : int = aten::sub(%4854, %10) # torch/nn/functional.py:1992:19
      %size_prods.4 : int = prim::Loop(%4855, %9, %size_prods.2) # torch/nn/functional.py:1992:4
        block0(%i.2 : int, %size_prods.7 : int):
          %4859 : int = aten::add(%i.2, %10) # torch/nn/functional.py:1993:27
          %4860 : int = aten::__getitem__(%4852, %4859) # torch/nn/functional.py:1993:22
          %size_prods.5 : int = aten::mul(%size_prods.7, %4860) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.5)
      %4862 : bool = aten::eq(%size_prods.4, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.3 : Tensor = aten::batch_norm(%out.1, %4850, %4851, %4848, %4849, %4847, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.5 : Tensor = aten::relu_(%out.3) # torch/nn/functional.py:1117:17
  %4865 : __torch__.torch.nn.modules.conv.___torch_mangle_948.Conv2d = prim::GetAttr[name="conv2"](%4614)
  %4866 : Tensor = prim::GetAttr[name="weight"](%4865)
  %4867 : Tensor? = prim::GetAttr[name="bias"](%4865)
  %4868 : int[] = prim::ListConstruct(%12, %12)
  %4869 : int[] = prim::ListConstruct(%12, %12)
  %4870 : int[] = prim::ListConstruct(%12, %12)
  %out.7 : Tensor = aten::conv2d(%out.5, %4866, %4867, %4868, %4869, %4870, %12) # torch/nn/modules/conv.py:415:15
  %4872 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_21.BatchNorm2d = prim::GetAttr[name="bn2"](%4614)
  %4873 : int = aten::dim(%out.7) # torch/nn/modules/batchnorm.py:276:11
  %4874 : bool = aten::ne(%4873, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4874) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4875 : bool = prim::GetAttr[name="training"](%4872)
   = prim::If(%4875) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4876 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4872)
      %4877 : Tensor = aten::add(%4876, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4872, %4877)
      -> ()
    block1():
      -> ()
  %4878 : bool = prim::GetAttr[name="training"](%4872)
  %4879 : Tensor = prim::GetAttr[name="running_mean"](%4872)
  %4880 : Tensor = prim::GetAttr[name="running_var"](%4872)
  %4881 : Tensor = prim::GetAttr[name="weight"](%4872)
  %4882 : Tensor = prim::GetAttr[name="bias"](%4872)
   = prim::If(%4878) # torch/nn/functional.py:2011:4
    block0():
      %4883 : int[] = aten::size(%out.7) # torch/nn/functional.py:2012:27
      %size_prods.8 : int = aten::__getitem__(%4883, %8) # torch/nn/functional.py:1991:17
      %4885 : int = aten::len(%4883) # torch/nn/functional.py:1992:19
      %4886 : int = aten::sub(%4885, %10) # torch/nn/functional.py:1992:19
      %size_prods.9 : int = prim::Loop(%4886, %9, %size_prods.8) # torch/nn/functional.py:1992:4
        block0(%i.3 : int, %size_prods.10 : int):
          %4890 : int = aten::add(%i.3, %10) # torch/nn/functional.py:1993:27
          %4891 : int = aten::__getitem__(%4883, %4890) # torch/nn/functional.py:1993:22
          %size_prods.11 : int = aten::mul(%size_prods.10, %4891) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.11)
      %4893 : bool = aten::eq(%size_prods.9, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4893) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.9 : Tensor = aten::batch_norm(%out.7, %4881, %4882, %4879, %4880, %4878, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.11 : Tensor = aten::relu_(%out.9) # torch/nn/functional.py:1117:17
  %4896 : __torch__.torch.nn.modules.conv.___torch_mangle_41.Conv2d = prim::GetAttr[name="conv3"](%4614)
  %4897 : Tensor = prim::GetAttr[name="weight"](%4896)
  %4898 : Tensor? = prim::GetAttr[name="bias"](%4896)
  %4899 : int[] = prim::ListConstruct(%12, %12)
  %4900 : int[] = prim::ListConstruct(%8, %8)
  %4901 : int[] = prim::ListConstruct(%12, %12)
  %out.13 : Tensor = aten::conv2d(%out.11, %4897, %4898, %4899, %4900, %4901, %12) # torch/nn/modules/conv.py:415:15
  %4903 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_42.BatchNorm2d = prim::GetAttr[name="bn3"](%4614)
  %4904 : int = aten::dim(%out.13) # torch/nn/modules/batchnorm.py:276:11
  %4905 : bool = aten::ne(%4904, %6) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4905) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%7) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4906 : bool = prim::GetAttr[name="training"](%4903)
   = prim::If(%4906) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4907 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4903)
      %4908 : Tensor = aten::add(%4907, %12, %12) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4903, %4908)
      -> ()
    block1():
      -> ()
  %4909 : bool = prim::GetAttr[name="training"](%4903)
  %4910 : Tensor = prim::GetAttr[name="running_mean"](%4903)
  %4911 : Tensor = prim::GetAttr[name="running_var"](%4903)
  %4912 : Tensor = prim::GetAttr[name="weight"](%4903)
  %4913 : Tensor = prim::GetAttr[name="bias"](%4903)
   = prim::If(%4909) # torch/nn/functional.py:2011:4
    block0():
      %4914 : int[] = aten::size(%out.13) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%4914, %8) # torch/nn/functional.py:1991:17
      %4916 : int = aten::len(%4914) # torch/nn/functional.py:1992:19
      %4917 : int = aten::sub(%4916, %10) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%4917, %9, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %4921 : int = aten::add(%i.1, %10) # torch/nn/functional.py:1993:27
          %4922 : int = aten::__getitem__(%4914, %4921) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %4922) # torch/nn/functional.py:1993:8
          -> (%9, %size_prods.3)
      %4924 : bool = aten::eq(%size_prods, %12) # torch/nn/functional.py:1994:7
       = prim::If(%4924) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%7) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %out.15 : Tensor = aten::batch_norm(%out.13, %4912, %4913, %4910, %4911, %4909, %exponential_average_factor.1, %4, %9) # torch/nn/functional.py:2014:11
  %out.17 : Tensor = aten::add_(%out.15, %input.5, %12) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:118:8
  %x.17 : Tensor = aten::relu_(%out.17) # torch/nn/functional.py:1117:17
  %4928 : int[] = prim::ListConstruct(%12, %12)
  %4929 : int[] = aten::size(%x.17) # torch/nn/functional.py:925:51
  %4930 : int = aten::len(%4929) # <string>:5:9
  %4931 : bool = aten::gt(%4930, %10) # <string>:5:9
   = prim::If(%4931) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%7) # <string>:5:2
      -> ()
  %x.19 : Tensor = aten::adaptive_avg_pool2d(%x.17, %4928) # torch/nn/functional.py:926:11
  %x.21 : Tensor = aten::flatten(%x.19, %12, %13) # torch/hub/pytorch_vision_master/torchvision/models/resnet.py:214:12
  %4934 : __torch__.torch.nn.modules.linear.___torch_mangle_621.Linear = prim::GetAttr[name="fc"](%self)
  %4935 : Tensor = prim::GetAttr[name="weight"](%4934)
  %4936 : Tensor = prim::GetAttr[name="bias"](%4934)
  %4937 : int = aten::dim(%x.21) # torch/nn/functional.py:1672:7
  %4938 : bool = aten::eq(%4937, %10) # torch/nn/functional.py:1672:7
  %x.23 : Tensor = prim::If(%4938) # torch/nn/functional.py:1672:4
    block0():
      %4940 : Tensor = aten::t(%4935) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%4936, %x.21, %4940, %12, %12) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %4942 : Tensor = aten::t(%4935) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%x.21, %4942) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %4936, %12) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%x.23)
