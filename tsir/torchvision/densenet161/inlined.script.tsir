graph(%self : __torch__.torchvision.models.densenet.___torch_mangle_298.DenseNet,
      %x.1 : Tensor):
  %2 : int = prim::Constant[value=-1]()
  %3 : Function = prim::Constant[name="adaptive_avg_pool2d"]()
  %4 : Function = prim::Constant[name="relu"]()
  %5 : bool = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:193:39
  %6 : int = prim::Constant[value=1]() # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:194:42
  %7 : __torch__.torch.nn.modules.container.___torch_mangle_296.Sequential = prim::GetAttr[name="features"](%self)
  %15 : None = prim::Constant() # torch/nn/modules/pooling.py:599:82
  %16 : float = prim::Constant[value=0.]() # torch/nn/functional.py:968:11
  %17 : float = prim::Constant[value=1.]() # torch/nn/functional.py:968:21
  %18 : int = prim::Constant[value=9223372036854775807]()
  %19 : bool = prim::Constant[value=0]() # torch/nn/modules/pooling.py:158:57
  %20 : float = prim::Constant[value=1.0000000000000001e-05]() # torch/nn/modules/batchnorm.py:136:77
  %exponential_average_factor.2 : float = prim::Constant[value=0.10000000000000001]() # torch/nn/modules/batchnorm.py:108:41
  %22 : int = prim::Constant[value=4]() # torch/nn/modules/batchnorm.py:276:26
  %23 : str = prim::Constant[value="Exception"]() # torch/nn/modules/batchnorm.py:277:12
  %24 : int = prim::Constant[value=0]() # torch/nn/functional.py:1991:22
  %25 : bool = prim::Constant[value=1]() # torch/nn/functional.py:2016:33
  %26 : int = prim::Constant[value=2]() # torch/nn/modules/conv.py:413:47
  %27 : int = prim::Constant[value=1]() # torch/nn/modules/conv.py:414:38
  %28 : int = prim::Constant[value=3]() # torch/nn/modules/conv.py:416:24
  %29 : __torch__.torch.nn.modules.conv.___torch_mangle_162.Conv2d = prim::GetAttr[name="conv0"](%7)
  %30 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="norm0"](%7)
  %31 : __torch__.torchvision.models.densenet.___torch_mangle_179._DenseBlock = prim::GetAttr[name="denseblock1"](%7)
  %32 : __torch__.torchvision.models.densenet.___torch_mangle_181._Transition = prim::GetAttr[name="transition1"](%7)
  %33 : __torch__.torchvision.models.densenet.___torch_mangle_201._DenseBlock = prim::GetAttr[name="denseblock2"](%7)
  %34 : __torch__.torchvision.models.densenet.___torch_mangle_203._Transition = prim::GetAttr[name="transition2"](%7)
  %35 : __torch__.torchvision.models.densenet.___torch_mangle_285._DenseBlock = prim::GetAttr[name="denseblock3"](%7)
  %36 : __torch__.torchvision.models.densenet.___torch_mangle_288._Transition = prim::GetAttr[name="transition3"](%7)
  %37 : __torch__.torchvision.models.densenet.___torch_mangle_294._DenseBlock = prim::GetAttr[name="denseblock4"](%7)
  %38 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_295.BatchNorm2d = prim::GetAttr[name="norm5"](%7)
  %39 : Tensor = prim::GetAttr[name="weight"](%29)
  %40 : Tensor? = prim::GetAttr[name="bias"](%29)
  %41 : int[] = prim::ListConstruct(%26, %26)
  %42 : int[] = prim::ListConstruct(%28, %28)
  %43 : int[] = prim::ListConstruct(%27, %27)
  %input.4 : Tensor = aten::conv2d(%x.1, %39, %40, %41, %42, %43, %27) # torch/nn/modules/conv.py:415:15
  %45 : int = aten::dim(%input.4) # torch/nn/modules/batchnorm.py:276:11
  %46 : bool = aten::ne(%45, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%46) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %47 : bool = prim::GetAttr[name="training"](%30)
   = prim::If(%47) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %48 : Tensor = prim::GetAttr[name="num_batches_tracked"](%30)
      %49 : Tensor = aten::add(%48, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%30, %49)
      -> ()
    block1():
      -> ()
  %50 : bool = prim::GetAttr[name="training"](%30)
  %51 : Tensor = prim::GetAttr[name="running_mean"](%30)
  %52 : Tensor = prim::GetAttr[name="running_var"](%30)
  %53 : Tensor = prim::GetAttr[name="weight"](%30)
  %54 : Tensor = prim::GetAttr[name="bias"](%30)
   = prim::If(%50) # torch/nn/functional.py:2011:4
    block0():
      %55 : int[] = aten::size(%input.4) # torch/nn/functional.py:2012:27
      %size_prods.392 : int = aten::__getitem__(%55, %24) # torch/nn/functional.py:1991:17
      %57 : int = aten::len(%55) # torch/nn/functional.py:1992:19
      %58 : int = aten::sub(%57, %26) # torch/nn/functional.py:1992:19
      %size_prods.393 : int = prim::Loop(%58, %25, %size_prods.392) # torch/nn/functional.py:1992:4
        block0(%i.99 : int, %size_prods.394 : int):
          %62 : int = aten::add(%i.99, %26) # torch/nn/functional.py:1993:27
          %63 : int = aten::__getitem__(%55, %62) # torch/nn/functional.py:1993:22
          %size_prods.395 : int = aten::mul(%size_prods.394, %63) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.395)
      %65 : bool = aten::eq(%size_prods.393, %27) # torch/nn/functional.py:1994:7
       = prim::If(%65) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.6 : Tensor = aten::batch_norm(%input.4, %53, %54, %51, %52, %50, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.8 : Tensor = aten::relu_(%input.6) # torch/nn/functional.py:1117:17
  %68 : int[] = prim::ListConstruct(%28, %28)
  %69 : int[] = prim::ListConstruct(%26, %26)
  %70 : int[] = prim::ListConstruct(%27, %27)
  %71 : int[] = prim::ListConstruct(%27, %27)
  %input.10 : Tensor = aten::max_pool2d(%input.8, %68, %69, %70, %71, %19) # torch/nn/functional.py:575:11
  %features.2 : Tensor[] = prim::ListConstruct(%input.10)
  %74 : __torch__.torchvision.models.densenet.___torch_mangle_165._DenseLayer = prim::GetAttr[name="denselayer1"](%31)
  %75 : __torch__.torchvision.models.densenet.___torch_mangle_168._DenseLayer = prim::GetAttr[name="denselayer2"](%31)
  %76 : __torch__.torchvision.models.densenet.___torch_mangle_170._DenseLayer = prim::GetAttr[name="denselayer3"](%31)
  %77 : __torch__.torchvision.models.densenet.___torch_mangle_173._DenseLayer = prim::GetAttr[name="denselayer4"](%31)
  %78 : __torch__.torchvision.models.densenet.___torch_mangle_175._DenseLayer = prim::GetAttr[name="denselayer5"](%31)
  %79 : __torch__.torchvision.models.densenet.___torch_mangle_178._DenseLayer = prim::GetAttr[name="denselayer6"](%31)
  %80 : Tensor = prim::Uninitialized()
  %81 : bool = prim::GetAttr[name="memory_efficient"](%74)
  %82 : bool = prim::If(%81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %83 : bool = prim::Uninitialized()
      %84 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %85 : bool = aten::gt(%84, %24)
      %86 : bool, %87 : bool, %88 : int = prim::Loop(%18, %85, %19, %83, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%89 : int, %90 : bool, %91 : bool, %92 : int):
          %tensor.51 : Tensor = aten::__getitem__(%features.2, %92) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %94 : bool = prim::requires_grad(%tensor.51)
          %95 : bool, %96 : bool = prim::If(%94) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %83)
          %97 : int = aten::add(%92, %27)
          %98 : bool = aten::lt(%97, %84)
          %99 : bool = aten::__and__(%98, %95)
          -> (%99, %94, %96, %97)
      %100 : bool = prim::If(%86)
        block0():
          -> (%87)
        block1():
          -> (%19)
      -> (%100)
    block1():
      -> (%19)
  %bottleneck_output.100 : Tensor = prim::If(%82) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%80)
    block1():
      %concated_features.51 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %103 : __torch__.torch.nn.modules.conv.___torch_mangle_163.Conv2d = prim::GetAttr[name="conv1"](%74)
      %104 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_73.BatchNorm2d = prim::GetAttr[name="norm1"](%74)
      %105 : int = aten::dim(%concated_features.51) # torch/nn/modules/batchnorm.py:276:11
      %106 : bool = aten::ne(%105, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%106) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %107 : bool = prim::GetAttr[name="training"](%104)
       = prim::If(%107) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %108 : Tensor = prim::GetAttr[name="num_batches_tracked"](%104)
          %109 : Tensor = aten::add(%108, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%104, %109)
          -> ()
        block1():
          -> ()
      %110 : bool = prim::GetAttr[name="training"](%104)
      %111 : Tensor = prim::GetAttr[name="running_mean"](%104)
      %112 : Tensor = prim::GetAttr[name="running_var"](%104)
      %113 : Tensor = prim::GetAttr[name="weight"](%104)
      %114 : Tensor = prim::GetAttr[name="bias"](%104)
       = prim::If(%110) # torch/nn/functional.py:2011:4
        block0():
          %115 : int[] = aten::size(%concated_features.51) # torch/nn/functional.py:2012:27
          %size_prods.400 : int = aten::__getitem__(%115, %24) # torch/nn/functional.py:1991:17
          %117 : int = aten::len(%115) # torch/nn/functional.py:1992:19
          %118 : int = aten::sub(%117, %26) # torch/nn/functional.py:1992:19
          %size_prods.401 : int = prim::Loop(%118, %25, %size_prods.400) # torch/nn/functional.py:1992:4
            block0(%i.101 : int, %size_prods.402 : int):
              %122 : int = aten::add(%i.101, %26) # torch/nn/functional.py:1993:27
              %123 : int = aten::__getitem__(%115, %122) # torch/nn/functional.py:1993:22
              %size_prods.403 : int = aten::mul(%size_prods.402, %123) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.403)
          %125 : bool = aten::eq(%size_prods.401, %27) # torch/nn/functional.py:1994:7
           = prim::If(%125) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %126 : Tensor = aten::batch_norm(%concated_features.51, %113, %114, %111, %112, %110, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.101 : Tensor = aten::relu_(%126) # torch/nn/functional.py:1117:17
      %128 : Tensor = prim::GetAttr[name="weight"](%103)
      %129 : Tensor? = prim::GetAttr[name="bias"](%103)
      %130 : int[] = prim::ListConstruct(%27, %27)
      %131 : int[] = prim::ListConstruct(%24, %24)
      %132 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.101 : Tensor = aten::conv2d(%result.101, %128, %129, %130, %131, %132, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.101)
  %134 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%74)
  %135 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%74)
  %136 : int = aten::dim(%bottleneck_output.100) # torch/nn/modules/batchnorm.py:276:11
  %137 : bool = aten::ne(%136, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%137) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %138 : bool = prim::GetAttr[name="training"](%135)
   = prim::If(%138) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %139 : Tensor = prim::GetAttr[name="num_batches_tracked"](%135)
      %140 : Tensor = aten::add(%139, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%135, %140)
      -> ()
    block1():
      -> ()
  %141 : bool = prim::GetAttr[name="training"](%135)
  %142 : Tensor = prim::GetAttr[name="running_mean"](%135)
  %143 : Tensor = prim::GetAttr[name="running_var"](%135)
  %144 : Tensor = prim::GetAttr[name="weight"](%135)
  %145 : Tensor = prim::GetAttr[name="bias"](%135)
   = prim::If(%141) # torch/nn/functional.py:2011:4
    block0():
      %146 : int[] = aten::size(%bottleneck_output.100) # torch/nn/functional.py:2012:27
      %size_prods.404 : int = aten::__getitem__(%146, %24) # torch/nn/functional.py:1991:17
      %148 : int = aten::len(%146) # torch/nn/functional.py:1992:19
      %149 : int = aten::sub(%148, %26) # torch/nn/functional.py:1992:19
      %size_prods.405 : int = prim::Loop(%149, %25, %size_prods.404) # torch/nn/functional.py:1992:4
        block0(%i.102 : int, %size_prods.406 : int):
          %153 : int = aten::add(%i.102, %26) # torch/nn/functional.py:1993:27
          %154 : int = aten::__getitem__(%146, %153) # torch/nn/functional.py:1993:22
          %size_prods.407 : int = aten::mul(%size_prods.406, %154) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.407)
      %156 : bool = aten::eq(%size_prods.405, %27) # torch/nn/functional.py:1994:7
       = prim::If(%156) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %157 : Tensor = aten::batch_norm(%bottleneck_output.100, %144, %145, %142, %143, %141, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.102 : Tensor = aten::relu_(%157) # torch/nn/functional.py:1117:17
  %159 : Tensor = prim::GetAttr[name="weight"](%134)
  %160 : Tensor? = prim::GetAttr[name="bias"](%134)
  %161 : int[] = prim::ListConstruct(%27, %27)
  %162 : int[] = prim::ListConstruct(%27, %27)
  %163 : int[] = prim::ListConstruct(%27, %27)
  %new_features.96 : Tensor = aten::conv2d(%result.102, %159, %160, %161, %162, %163, %27) # torch/nn/modules/conv.py:415:15
  %165 : float = prim::GetAttr[name="drop_rate"](%74)
  %166 : bool = aten::gt(%165, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.106 : Tensor = prim::If(%166) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %168 : float = prim::GetAttr[name="drop_rate"](%74)
      %169 : bool = prim::GetAttr[name="training"](%74)
      %170 : bool = aten::lt(%168, %16) # torch/nn/functional.py:968:7
      %171 : bool = prim::If(%170) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %172 : bool = aten::gt(%168, %17) # torch/nn/functional.py:968:17
          -> (%172)
       = prim::If(%171) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %173 : Tensor = aten::dropout(%new_features.96, %168, %169) # torch/nn/functional.py:973:17
      -> (%173)
    block1():
      -> (%new_features.96)
  %174 : Tensor[] = aten::append(%features.2, %new_features.106) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %175 : Tensor = prim::Uninitialized()
  %176 : bool = prim::GetAttr[name="memory_efficient"](%75)
  %177 : bool = prim::If(%176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %178 : bool = prim::Uninitialized()
      %179 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %180 : bool = aten::gt(%179, %24)
      %181 : bool, %182 : bool, %183 : int = prim::Loop(%18, %180, %19, %178, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%184 : int, %185 : bool, %186 : bool, %187 : int):
          %tensor.47 : Tensor = aten::__getitem__(%features.2, %187) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %189 : bool = prim::requires_grad(%tensor.47)
          %190 : bool, %191 : bool = prim::If(%189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %178)
          %192 : int = aten::add(%187, %27)
          %193 : bool = aten::lt(%192, %179)
          %194 : bool = aten::__and__(%193, %190)
          -> (%194, %189, %191, %192)
      %195 : bool = prim::If(%181)
        block0():
          -> (%182)
        block1():
          -> (%19)
      -> (%195)
    block1():
      -> (%19)
  %bottleneck_output.92 : Tensor = prim::If(%177) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%175)
    block1():
      %concated_features.47 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %198 : __torch__.torch.nn.modules.conv.___torch_mangle_167.Conv2d = prim::GetAttr[name="conv1"](%75)
      %199 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_166.BatchNorm2d = prim::GetAttr[name="norm1"](%75)
      %200 : int = aten::dim(%concated_features.47) # torch/nn/modules/batchnorm.py:276:11
      %201 : bool = aten::ne(%200, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%201) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %202 : bool = prim::GetAttr[name="training"](%199)
       = prim::If(%202) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %203 : Tensor = prim::GetAttr[name="num_batches_tracked"](%199)
          %204 : Tensor = aten::add(%203, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%199, %204)
          -> ()
        block1():
          -> ()
      %205 : bool = prim::GetAttr[name="training"](%199)
      %206 : Tensor = prim::GetAttr[name="running_mean"](%199)
      %207 : Tensor = prim::GetAttr[name="running_var"](%199)
      %208 : Tensor = prim::GetAttr[name="weight"](%199)
      %209 : Tensor = prim::GetAttr[name="bias"](%199)
       = prim::If(%205) # torch/nn/functional.py:2011:4
        block0():
          %210 : int[] = aten::size(%concated_features.47) # torch/nn/functional.py:2012:27
          %size_prods.408 : int = aten::__getitem__(%210, %24) # torch/nn/functional.py:1991:17
          %212 : int = aten::len(%210) # torch/nn/functional.py:1992:19
          %213 : int = aten::sub(%212, %26) # torch/nn/functional.py:1992:19
          %size_prods.409 : int = prim::Loop(%213, %25, %size_prods.408) # torch/nn/functional.py:1992:4
            block0(%i.103 : int, %size_prods.410 : int):
              %217 : int = aten::add(%i.103, %26) # torch/nn/functional.py:1993:27
              %218 : int = aten::__getitem__(%210, %217) # torch/nn/functional.py:1993:22
              %size_prods.411 : int = aten::mul(%size_prods.410, %218) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.411)
          %220 : bool = aten::eq(%size_prods.409, %27) # torch/nn/functional.py:1994:7
           = prim::If(%220) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %221 : Tensor = aten::batch_norm(%concated_features.47, %208, %209, %206, %207, %205, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.93 : Tensor = aten::relu_(%221) # torch/nn/functional.py:1117:17
      %223 : Tensor = prim::GetAttr[name="weight"](%198)
      %224 : Tensor? = prim::GetAttr[name="bias"](%198)
      %225 : int[] = prim::ListConstruct(%27, %27)
      %226 : int[] = prim::ListConstruct(%24, %24)
      %227 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.93 : Tensor = aten::conv2d(%result.93, %223, %224, %225, %226, %227, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.93)
  %229 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%75)
  %230 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%75)
  %231 : int = aten::dim(%bottleneck_output.92) # torch/nn/modules/batchnorm.py:276:11
  %232 : bool = aten::ne(%231, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%232) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %233 : bool = prim::GetAttr[name="training"](%230)
   = prim::If(%233) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %234 : Tensor = prim::GetAttr[name="num_batches_tracked"](%230)
      %235 : Tensor = aten::add(%234, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%230, %235)
      -> ()
    block1():
      -> ()
  %236 : bool = prim::GetAttr[name="training"](%230)
  %237 : Tensor = prim::GetAttr[name="running_mean"](%230)
  %238 : Tensor = prim::GetAttr[name="running_var"](%230)
  %239 : Tensor = prim::GetAttr[name="weight"](%230)
  %240 : Tensor = prim::GetAttr[name="bias"](%230)
   = prim::If(%236) # torch/nn/functional.py:2011:4
    block0():
      %241 : int[] = aten::size(%bottleneck_output.92) # torch/nn/functional.py:2012:27
      %size_prods.412 : int = aten::__getitem__(%241, %24) # torch/nn/functional.py:1991:17
      %243 : int = aten::len(%241) # torch/nn/functional.py:1992:19
      %244 : int = aten::sub(%243, %26) # torch/nn/functional.py:1992:19
      %size_prods.413 : int = prim::Loop(%244, %25, %size_prods.412) # torch/nn/functional.py:1992:4
        block0(%i.104 : int, %size_prods.414 : int):
          %248 : int = aten::add(%i.104, %26) # torch/nn/functional.py:1993:27
          %249 : int = aten::__getitem__(%241, %248) # torch/nn/functional.py:1993:22
          %size_prods.415 : int = aten::mul(%size_prods.414, %249) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.415)
      %251 : bool = aten::eq(%size_prods.413, %27) # torch/nn/functional.py:1994:7
       = prim::If(%251) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %252 : Tensor = aten::batch_norm(%bottleneck_output.92, %239, %240, %237, %238, %236, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.94 : Tensor = aten::relu_(%252) # torch/nn/functional.py:1117:17
  %254 : Tensor = prim::GetAttr[name="weight"](%229)
  %255 : Tensor? = prim::GetAttr[name="bias"](%229)
  %256 : int[] = prim::ListConstruct(%27, %27)
  %257 : int[] = prim::ListConstruct(%27, %27)
  %258 : int[] = prim::ListConstruct(%27, %27)
  %new_features.98 : Tensor = aten::conv2d(%result.94, %254, %255, %256, %257, %258, %27) # torch/nn/modules/conv.py:415:15
  %260 : float = prim::GetAttr[name="drop_rate"](%75)
  %261 : bool = aten::gt(%260, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.95 : Tensor = prim::If(%261) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %263 : float = prim::GetAttr[name="drop_rate"](%75)
      %264 : bool = prim::GetAttr[name="training"](%75)
      %265 : bool = aten::lt(%263, %16) # torch/nn/functional.py:968:7
      %266 : bool = prim::If(%265) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %267 : bool = aten::gt(%263, %17) # torch/nn/functional.py:968:17
          -> (%267)
       = prim::If(%266) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %268 : Tensor = aten::dropout(%new_features.98, %263, %264) # torch/nn/functional.py:973:17
      -> (%268)
    block1():
      -> (%new_features.98)
  %269 : Tensor[] = aten::append(%features.2, %new_features.95) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %270 : Tensor = prim::Uninitialized()
  %271 : bool = prim::GetAttr[name="memory_efficient"](%76)
  %272 : bool = prim::If(%271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %273 : bool = prim::Uninitialized()
      %274 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %275 : bool = aten::gt(%274, %24)
      %276 : bool, %277 : bool, %278 : int = prim::Loop(%18, %275, %19, %273, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%279 : int, %280 : bool, %281 : bool, %282 : int):
          %tensor.48 : Tensor = aten::__getitem__(%features.2, %282) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %284 : bool = prim::requires_grad(%tensor.48)
          %285 : bool, %286 : bool = prim::If(%284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %273)
          %287 : int = aten::add(%282, %27)
          %288 : bool = aten::lt(%287, %274)
          %289 : bool = aten::__and__(%288, %285)
          -> (%289, %284, %286, %287)
      %290 : bool = prim::If(%276)
        block0():
          -> (%277)
        block1():
          -> (%19)
      -> (%290)
    block1():
      -> (%19)
  %bottleneck_output.94 : Tensor = prim::If(%272) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%270)
    block1():
      %concated_features.48 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %293 : __torch__.torch.nn.modules.conv.___torch_mangle_169.Conv2d = prim::GetAttr[name="conv1"](%76)
      %294 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%76)
      %295 : int = aten::dim(%concated_features.48) # torch/nn/modules/batchnorm.py:276:11
      %296 : bool = aten::ne(%295, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%296) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %297 : bool = prim::GetAttr[name="training"](%294)
       = prim::If(%297) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %298 : Tensor = prim::GetAttr[name="num_batches_tracked"](%294)
          %299 : Tensor = aten::add(%298, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%294, %299)
          -> ()
        block1():
          -> ()
      %300 : bool = prim::GetAttr[name="training"](%294)
      %301 : Tensor = prim::GetAttr[name="running_mean"](%294)
      %302 : Tensor = prim::GetAttr[name="running_var"](%294)
      %303 : Tensor = prim::GetAttr[name="weight"](%294)
      %304 : Tensor = prim::GetAttr[name="bias"](%294)
       = prim::If(%300) # torch/nn/functional.py:2011:4
        block0():
          %305 : int[] = aten::size(%concated_features.48) # torch/nn/functional.py:2012:27
          %size_prods.416 : int = aten::__getitem__(%305, %24) # torch/nn/functional.py:1991:17
          %307 : int = aten::len(%305) # torch/nn/functional.py:1992:19
          %308 : int = aten::sub(%307, %26) # torch/nn/functional.py:1992:19
          %size_prods.417 : int = prim::Loop(%308, %25, %size_prods.416) # torch/nn/functional.py:1992:4
            block0(%i.105 : int, %size_prods.418 : int):
              %312 : int = aten::add(%i.105, %26) # torch/nn/functional.py:1993:27
              %313 : int = aten::__getitem__(%305, %312) # torch/nn/functional.py:1993:22
              %size_prods.419 : int = aten::mul(%size_prods.418, %313) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.419)
          %315 : bool = aten::eq(%size_prods.417, %27) # torch/nn/functional.py:1994:7
           = prim::If(%315) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %316 : Tensor = aten::batch_norm(%concated_features.48, %303, %304, %301, %302, %300, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.95 : Tensor = aten::relu_(%316) # torch/nn/functional.py:1117:17
      %318 : Tensor = prim::GetAttr[name="weight"](%293)
      %319 : Tensor? = prim::GetAttr[name="bias"](%293)
      %320 : int[] = prim::ListConstruct(%27, %27)
      %321 : int[] = prim::ListConstruct(%24, %24)
      %322 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.95 : Tensor = aten::conv2d(%result.95, %318, %319, %320, %321, %322, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.95)
  %324 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%76)
  %325 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%76)
  %326 : int = aten::dim(%bottleneck_output.94) # torch/nn/modules/batchnorm.py:276:11
  %327 : bool = aten::ne(%326, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%327) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %328 : bool = prim::GetAttr[name="training"](%325)
   = prim::If(%328) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %329 : Tensor = prim::GetAttr[name="num_batches_tracked"](%325)
      %330 : Tensor = aten::add(%329, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%325, %330)
      -> ()
    block1():
      -> ()
  %331 : bool = prim::GetAttr[name="training"](%325)
  %332 : Tensor = prim::GetAttr[name="running_mean"](%325)
  %333 : Tensor = prim::GetAttr[name="running_var"](%325)
  %334 : Tensor = prim::GetAttr[name="weight"](%325)
  %335 : Tensor = prim::GetAttr[name="bias"](%325)
   = prim::If(%331) # torch/nn/functional.py:2011:4
    block0():
      %336 : int[] = aten::size(%bottleneck_output.94) # torch/nn/functional.py:2012:27
      %size_prods.420 : int = aten::__getitem__(%336, %24) # torch/nn/functional.py:1991:17
      %338 : int = aten::len(%336) # torch/nn/functional.py:1992:19
      %339 : int = aten::sub(%338, %26) # torch/nn/functional.py:1992:19
      %size_prods.421 : int = prim::Loop(%339, %25, %size_prods.420) # torch/nn/functional.py:1992:4
        block0(%i.106 : int, %size_prods.422 : int):
          %343 : int = aten::add(%i.106, %26) # torch/nn/functional.py:1993:27
          %344 : int = aten::__getitem__(%336, %343) # torch/nn/functional.py:1993:22
          %size_prods.423 : int = aten::mul(%size_prods.422, %344) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.423)
      %346 : bool = aten::eq(%size_prods.421, %27) # torch/nn/functional.py:1994:7
       = prim::If(%346) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %347 : Tensor = aten::batch_norm(%bottleneck_output.94, %334, %335, %332, %333, %331, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.96 : Tensor = aten::relu_(%347) # torch/nn/functional.py:1117:17
  %349 : Tensor = prim::GetAttr[name="weight"](%324)
  %350 : Tensor? = prim::GetAttr[name="bias"](%324)
  %351 : int[] = prim::ListConstruct(%27, %27)
  %352 : int[] = prim::ListConstruct(%27, %27)
  %353 : int[] = prim::ListConstruct(%27, %27)
  %new_features.100 : Tensor = aten::conv2d(%result.96, %349, %350, %351, %352, %353, %27) # torch/nn/modules/conv.py:415:15
  %355 : float = prim::GetAttr[name="drop_rate"](%76)
  %356 : bool = aten::gt(%355, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.97 : Tensor = prim::If(%356) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %358 : float = prim::GetAttr[name="drop_rate"](%76)
      %359 : bool = prim::GetAttr[name="training"](%76)
      %360 : bool = aten::lt(%358, %16) # torch/nn/functional.py:968:7
      %361 : bool = prim::If(%360) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %362 : bool = aten::gt(%358, %17) # torch/nn/functional.py:968:17
          -> (%362)
       = prim::If(%361) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %363 : Tensor = aten::dropout(%new_features.100, %358, %359) # torch/nn/functional.py:973:17
      -> (%363)
    block1():
      -> (%new_features.100)
  %364 : Tensor[] = aten::append(%features.2, %new_features.97) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %365 : Tensor = prim::Uninitialized()
  %366 : bool = prim::GetAttr[name="memory_efficient"](%77)
  %367 : bool = prim::If(%366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %368 : bool = prim::Uninitialized()
      %369 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %370 : bool = aten::gt(%369, %24)
      %371 : bool, %372 : bool, %373 : int = prim::Loop(%18, %370, %19, %368, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%374 : int, %375 : bool, %376 : bool, %377 : int):
          %tensor.49 : Tensor = aten::__getitem__(%features.2, %377) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %379 : bool = prim::requires_grad(%tensor.49)
          %380 : bool, %381 : bool = prim::If(%379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %368)
          %382 : int = aten::add(%377, %27)
          %383 : bool = aten::lt(%382, %369)
          %384 : bool = aten::__and__(%383, %380)
          -> (%384, %379, %381, %382)
      %385 : bool = prim::If(%371)
        block0():
          -> (%372)
        block1():
          -> (%19)
      -> (%385)
    block1():
      -> (%19)
  %bottleneck_output.96 : Tensor = prim::If(%367) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%365)
    block1():
      %concated_features.49 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %388 : __torch__.torch.nn.modules.conv.___torch_mangle_172.Conv2d = prim::GetAttr[name="conv1"](%77)
      %389 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_171.BatchNorm2d = prim::GetAttr[name="norm1"](%77)
      %390 : int = aten::dim(%concated_features.49) # torch/nn/modules/batchnorm.py:276:11
      %391 : bool = aten::ne(%390, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%391) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %392 : bool = prim::GetAttr[name="training"](%389)
       = prim::If(%392) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %393 : Tensor = prim::GetAttr[name="num_batches_tracked"](%389)
          %394 : Tensor = aten::add(%393, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%389, %394)
          -> ()
        block1():
          -> ()
      %395 : bool = prim::GetAttr[name="training"](%389)
      %396 : Tensor = prim::GetAttr[name="running_mean"](%389)
      %397 : Tensor = prim::GetAttr[name="running_var"](%389)
      %398 : Tensor = prim::GetAttr[name="weight"](%389)
      %399 : Tensor = prim::GetAttr[name="bias"](%389)
       = prim::If(%395) # torch/nn/functional.py:2011:4
        block0():
          %400 : int[] = aten::size(%concated_features.49) # torch/nn/functional.py:2012:27
          %size_prods.424 : int = aten::__getitem__(%400, %24) # torch/nn/functional.py:1991:17
          %402 : int = aten::len(%400) # torch/nn/functional.py:1992:19
          %403 : int = aten::sub(%402, %26) # torch/nn/functional.py:1992:19
          %size_prods.425 : int = prim::Loop(%403, %25, %size_prods.424) # torch/nn/functional.py:1992:4
            block0(%i.107 : int, %size_prods.426 : int):
              %407 : int = aten::add(%i.107, %26) # torch/nn/functional.py:1993:27
              %408 : int = aten::__getitem__(%400, %407) # torch/nn/functional.py:1993:22
              %size_prods.427 : int = aten::mul(%size_prods.426, %408) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.427)
          %410 : bool = aten::eq(%size_prods.425, %27) # torch/nn/functional.py:1994:7
           = prim::If(%410) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %411 : Tensor = aten::batch_norm(%concated_features.49, %398, %399, %396, %397, %395, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.97 : Tensor = aten::relu_(%411) # torch/nn/functional.py:1117:17
      %413 : Tensor = prim::GetAttr[name="weight"](%388)
      %414 : Tensor? = prim::GetAttr[name="bias"](%388)
      %415 : int[] = prim::ListConstruct(%27, %27)
      %416 : int[] = prim::ListConstruct(%24, %24)
      %417 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.97 : Tensor = aten::conv2d(%result.97, %413, %414, %415, %416, %417, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.97)
  %419 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%77)
  %420 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%77)
  %421 : int = aten::dim(%bottleneck_output.96) # torch/nn/modules/batchnorm.py:276:11
  %422 : bool = aten::ne(%421, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%422) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %423 : bool = prim::GetAttr[name="training"](%420)
   = prim::If(%423) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %424 : Tensor = prim::GetAttr[name="num_batches_tracked"](%420)
      %425 : Tensor = aten::add(%424, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%420, %425)
      -> ()
    block1():
      -> ()
  %426 : bool = prim::GetAttr[name="training"](%420)
  %427 : Tensor = prim::GetAttr[name="running_mean"](%420)
  %428 : Tensor = prim::GetAttr[name="running_var"](%420)
  %429 : Tensor = prim::GetAttr[name="weight"](%420)
  %430 : Tensor = prim::GetAttr[name="bias"](%420)
   = prim::If(%426) # torch/nn/functional.py:2011:4
    block0():
      %431 : int[] = aten::size(%bottleneck_output.96) # torch/nn/functional.py:2012:27
      %size_prods.428 : int = aten::__getitem__(%431, %24) # torch/nn/functional.py:1991:17
      %433 : int = aten::len(%431) # torch/nn/functional.py:1992:19
      %434 : int = aten::sub(%433, %26) # torch/nn/functional.py:1992:19
      %size_prods.429 : int = prim::Loop(%434, %25, %size_prods.428) # torch/nn/functional.py:1992:4
        block0(%i.108 : int, %size_prods.430 : int):
          %438 : int = aten::add(%i.108, %26) # torch/nn/functional.py:1993:27
          %439 : int = aten::__getitem__(%431, %438) # torch/nn/functional.py:1993:22
          %size_prods.431 : int = aten::mul(%size_prods.430, %439) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.431)
      %441 : bool = aten::eq(%size_prods.429, %27) # torch/nn/functional.py:1994:7
       = prim::If(%441) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %442 : Tensor = aten::batch_norm(%bottleneck_output.96, %429, %430, %427, %428, %426, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.98 : Tensor = aten::relu_(%442) # torch/nn/functional.py:1117:17
  %444 : Tensor = prim::GetAttr[name="weight"](%419)
  %445 : Tensor? = prim::GetAttr[name="bias"](%419)
  %446 : int[] = prim::ListConstruct(%27, %27)
  %447 : int[] = prim::ListConstruct(%27, %27)
  %448 : int[] = prim::ListConstruct(%27, %27)
  %new_features.102 : Tensor = aten::conv2d(%result.98, %444, %445, %446, %447, %448, %27) # torch/nn/modules/conv.py:415:15
  %450 : float = prim::GetAttr[name="drop_rate"](%77)
  %451 : bool = aten::gt(%450, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.99 : Tensor = prim::If(%451) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %453 : float = prim::GetAttr[name="drop_rate"](%77)
      %454 : bool = prim::GetAttr[name="training"](%77)
      %455 : bool = aten::lt(%453, %16) # torch/nn/functional.py:968:7
      %456 : bool = prim::If(%455) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %457 : bool = aten::gt(%453, %17) # torch/nn/functional.py:968:17
          -> (%457)
       = prim::If(%456) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %458 : Tensor = aten::dropout(%new_features.102, %453, %454) # torch/nn/functional.py:973:17
      -> (%458)
    block1():
      -> (%new_features.102)
  %459 : Tensor[] = aten::append(%features.2, %new_features.99) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %460 : Tensor = prim::Uninitialized()
  %461 : bool = prim::GetAttr[name="memory_efficient"](%78)
  %462 : bool = prim::If(%461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %463 : bool = prim::Uninitialized()
      %464 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %465 : bool = aten::gt(%464, %24)
      %466 : bool, %467 : bool, %468 : int = prim::Loop(%18, %465, %19, %463, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%469 : int, %470 : bool, %471 : bool, %472 : int):
          %tensor.50 : Tensor = aten::__getitem__(%features.2, %472) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %474 : bool = prim::requires_grad(%tensor.50)
          %475 : bool, %476 : bool = prim::If(%474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %463)
          %477 : int = aten::add(%472, %27)
          %478 : bool = aten::lt(%477, %464)
          %479 : bool = aten::__and__(%478, %475)
          -> (%479, %474, %476, %477)
      %480 : bool = prim::If(%466)
        block0():
          -> (%467)
        block1():
          -> (%19)
      -> (%480)
    block1():
      -> (%19)
  %bottleneck_output.98 : Tensor = prim::If(%462) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%460)
    block1():
      %concated_features.50 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %483 : __torch__.torch.nn.modules.conv.___torch_mangle_174.Conv2d = prim::GetAttr[name="conv1"](%78)
      %484 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%78)
      %485 : int = aten::dim(%concated_features.50) # torch/nn/modules/batchnorm.py:276:11
      %486 : bool = aten::ne(%485, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%486) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %487 : bool = prim::GetAttr[name="training"](%484)
       = prim::If(%487) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %488 : Tensor = prim::GetAttr[name="num_batches_tracked"](%484)
          %489 : Tensor = aten::add(%488, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%484, %489)
          -> ()
        block1():
          -> ()
      %490 : bool = prim::GetAttr[name="training"](%484)
      %491 : Tensor = prim::GetAttr[name="running_mean"](%484)
      %492 : Tensor = prim::GetAttr[name="running_var"](%484)
      %493 : Tensor = prim::GetAttr[name="weight"](%484)
      %494 : Tensor = prim::GetAttr[name="bias"](%484)
       = prim::If(%490) # torch/nn/functional.py:2011:4
        block0():
          %495 : int[] = aten::size(%concated_features.50) # torch/nn/functional.py:2012:27
          %size_prods.432 : int = aten::__getitem__(%495, %24) # torch/nn/functional.py:1991:17
          %497 : int = aten::len(%495) # torch/nn/functional.py:1992:19
          %498 : int = aten::sub(%497, %26) # torch/nn/functional.py:1992:19
          %size_prods.433 : int = prim::Loop(%498, %25, %size_prods.432) # torch/nn/functional.py:1992:4
            block0(%i.109 : int, %size_prods.434 : int):
              %502 : int = aten::add(%i.109, %26) # torch/nn/functional.py:1993:27
              %503 : int = aten::__getitem__(%495, %502) # torch/nn/functional.py:1993:22
              %size_prods.435 : int = aten::mul(%size_prods.434, %503) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.435)
          %505 : bool = aten::eq(%size_prods.433, %27) # torch/nn/functional.py:1994:7
           = prim::If(%505) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %506 : Tensor = aten::batch_norm(%concated_features.50, %493, %494, %491, %492, %490, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.99 : Tensor = aten::relu_(%506) # torch/nn/functional.py:1117:17
      %508 : Tensor = prim::GetAttr[name="weight"](%483)
      %509 : Tensor? = prim::GetAttr[name="bias"](%483)
      %510 : int[] = prim::ListConstruct(%27, %27)
      %511 : int[] = prim::ListConstruct(%24, %24)
      %512 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.99 : Tensor = aten::conv2d(%result.99, %508, %509, %510, %511, %512, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.99)
  %514 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%78)
  %515 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%78)
  %516 : int = aten::dim(%bottleneck_output.98) # torch/nn/modules/batchnorm.py:276:11
  %517 : bool = aten::ne(%516, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%517) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %518 : bool = prim::GetAttr[name="training"](%515)
   = prim::If(%518) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %519 : Tensor = prim::GetAttr[name="num_batches_tracked"](%515)
      %520 : Tensor = aten::add(%519, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%515, %520)
      -> ()
    block1():
      -> ()
  %521 : bool = prim::GetAttr[name="training"](%515)
  %522 : Tensor = prim::GetAttr[name="running_mean"](%515)
  %523 : Tensor = prim::GetAttr[name="running_var"](%515)
  %524 : Tensor = prim::GetAttr[name="weight"](%515)
  %525 : Tensor = prim::GetAttr[name="bias"](%515)
   = prim::If(%521) # torch/nn/functional.py:2011:4
    block0():
      %526 : int[] = aten::size(%bottleneck_output.98) # torch/nn/functional.py:2012:27
      %size_prods.288 : int = aten::__getitem__(%526, %24) # torch/nn/functional.py:1991:17
      %528 : int = aten::len(%526) # torch/nn/functional.py:1992:19
      %529 : int = aten::sub(%528, %26) # torch/nn/functional.py:1992:19
      %size_prods.289 : int = prim::Loop(%529, %25, %size_prods.288) # torch/nn/functional.py:1992:4
        block0(%i.73 : int, %size_prods.290 : int):
          %533 : int = aten::add(%i.73, %26) # torch/nn/functional.py:1993:27
          %534 : int = aten::__getitem__(%526, %533) # torch/nn/functional.py:1993:22
          %size_prods.291 : int = aten::mul(%size_prods.290, %534) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.291)
      %536 : bool = aten::eq(%size_prods.289, %27) # torch/nn/functional.py:1994:7
       = prim::If(%536) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %537 : Tensor = aten::batch_norm(%bottleneck_output.98, %524, %525, %522, %523, %521, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.100 : Tensor = aten::relu_(%537) # torch/nn/functional.py:1117:17
  %539 : Tensor = prim::GetAttr[name="weight"](%514)
  %540 : Tensor? = prim::GetAttr[name="bias"](%514)
  %541 : int[] = prim::ListConstruct(%27, %27)
  %542 : int[] = prim::ListConstruct(%27, %27)
  %543 : int[] = prim::ListConstruct(%27, %27)
  %new_features.104 : Tensor = aten::conv2d(%result.100, %539, %540, %541, %542, %543, %27) # torch/nn/modules/conv.py:415:15
  %545 : float = prim::GetAttr[name="drop_rate"](%78)
  %546 : bool = aten::gt(%545, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.101 : Tensor = prim::If(%546) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %548 : float = prim::GetAttr[name="drop_rate"](%78)
      %549 : bool = prim::GetAttr[name="training"](%78)
      %550 : bool = aten::lt(%548, %16) # torch/nn/functional.py:968:7
      %551 : bool = prim::If(%550) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %552 : bool = aten::gt(%548, %17) # torch/nn/functional.py:968:17
          -> (%552)
       = prim::If(%551) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %553 : Tensor = aten::dropout(%new_features.104, %548, %549) # torch/nn/functional.py:973:17
      -> (%553)
    block1():
      -> (%new_features.104)
  %554 : Tensor[] = aten::append(%features.2, %new_features.101) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %555 : Tensor = prim::Uninitialized()
  %556 : bool = prim::GetAttr[name="memory_efficient"](%79)
  %557 : bool = prim::If(%556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %558 : bool = prim::Uninitialized()
      %559 : int = aten::len(%features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %560 : bool = aten::gt(%559, %24)
      %561 : bool, %562 : bool, %563 : int = prim::Loop(%18, %560, %19, %558, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%564 : int, %565 : bool, %566 : bool, %567 : int):
          %tensor.52 : Tensor = aten::__getitem__(%features.2, %567) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %569 : bool = prim::requires_grad(%tensor.52)
          %570 : bool, %571 : bool = prim::If(%569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %558)
          %572 : int = aten::add(%567, %27)
          %573 : bool = aten::lt(%572, %559)
          %574 : bool = aten::__and__(%573, %570)
          -> (%574, %569, %571, %572)
      %575 : bool = prim::If(%561)
        block0():
          -> (%562)
        block1():
          -> (%19)
      -> (%575)
    block1():
      -> (%19)
  %bottleneck_output.102 : Tensor = prim::If(%557) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%555)
    block1():
      %concated_features.52 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %578 : __torch__.torch.nn.modules.conv.___torch_mangle_177.Conv2d = prim::GetAttr[name="conv1"](%79)
      %579 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_176.BatchNorm2d = prim::GetAttr[name="norm1"](%79)
      %580 : int = aten::dim(%concated_features.52) # torch/nn/modules/batchnorm.py:276:11
      %581 : bool = aten::ne(%580, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%581) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %582 : bool = prim::GetAttr[name="training"](%579)
       = prim::If(%582) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %583 : Tensor = prim::GetAttr[name="num_batches_tracked"](%579)
          %584 : Tensor = aten::add(%583, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%579, %584)
          -> ()
        block1():
          -> ()
      %585 : bool = prim::GetAttr[name="training"](%579)
      %586 : Tensor = prim::GetAttr[name="running_mean"](%579)
      %587 : Tensor = prim::GetAttr[name="running_var"](%579)
      %588 : Tensor = prim::GetAttr[name="weight"](%579)
      %589 : Tensor = prim::GetAttr[name="bias"](%579)
       = prim::If(%585) # torch/nn/functional.py:2011:4
        block0():
          %590 : int[] = aten::size(%concated_features.52) # torch/nn/functional.py:2012:27
          %size_prods.292 : int = aten::__getitem__(%590, %24) # torch/nn/functional.py:1991:17
          %592 : int = aten::len(%590) # torch/nn/functional.py:1992:19
          %593 : int = aten::sub(%592, %26) # torch/nn/functional.py:1992:19
          %size_prods.293 : int = prim::Loop(%593, %25, %size_prods.292) # torch/nn/functional.py:1992:4
            block0(%i.74 : int, %size_prods.294 : int):
              %597 : int = aten::add(%i.74, %26) # torch/nn/functional.py:1993:27
              %598 : int = aten::__getitem__(%590, %597) # torch/nn/functional.py:1993:22
              %size_prods.295 : int = aten::mul(%size_prods.294, %598) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.295)
          %600 : bool = aten::eq(%size_prods.293, %27) # torch/nn/functional.py:1994:7
           = prim::If(%600) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %601 : Tensor = aten::batch_norm(%concated_features.52, %588, %589, %586, %587, %585, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.103 : Tensor = aten::relu_(%601) # torch/nn/functional.py:1117:17
      %603 : Tensor = prim::GetAttr[name="weight"](%578)
      %604 : Tensor? = prim::GetAttr[name="bias"](%578)
      %605 : int[] = prim::ListConstruct(%27, %27)
      %606 : int[] = prim::ListConstruct(%24, %24)
      %607 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.103 : Tensor = aten::conv2d(%result.103, %603, %604, %605, %606, %607, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.103)
  %609 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%79)
  %610 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%79)
  %611 : int = aten::dim(%bottleneck_output.102) # torch/nn/modules/batchnorm.py:276:11
  %612 : bool = aten::ne(%611, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%612) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %613 : bool = prim::GetAttr[name="training"](%610)
   = prim::If(%613) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %614 : Tensor = prim::GetAttr[name="num_batches_tracked"](%610)
      %615 : Tensor = aten::add(%614, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%610, %615)
      -> ()
    block1():
      -> ()
  %616 : bool = prim::GetAttr[name="training"](%610)
  %617 : Tensor = prim::GetAttr[name="running_mean"](%610)
  %618 : Tensor = prim::GetAttr[name="running_var"](%610)
  %619 : Tensor = prim::GetAttr[name="weight"](%610)
  %620 : Tensor = prim::GetAttr[name="bias"](%610)
   = prim::If(%616) # torch/nn/functional.py:2011:4
    block0():
      %621 : int[] = aten::size(%bottleneck_output.102) # torch/nn/functional.py:2012:27
      %size_prods.396 : int = aten::__getitem__(%621, %24) # torch/nn/functional.py:1991:17
      %623 : int = aten::len(%621) # torch/nn/functional.py:1992:19
      %624 : int = aten::sub(%623, %26) # torch/nn/functional.py:1992:19
      %size_prods.397 : int = prim::Loop(%624, %25, %size_prods.396) # torch/nn/functional.py:1992:4
        block0(%i.100 : int, %size_prods.398 : int):
          %628 : int = aten::add(%i.100, %26) # torch/nn/functional.py:1993:27
          %629 : int = aten::__getitem__(%621, %628) # torch/nn/functional.py:1993:22
          %size_prods.399 : int = aten::mul(%size_prods.398, %629) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.399)
      %631 : bool = aten::eq(%size_prods.397, %27) # torch/nn/functional.py:1994:7
       = prim::If(%631) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %632 : Tensor = aten::batch_norm(%bottleneck_output.102, %619, %620, %617, %618, %616, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.104 : Tensor = aten::relu_(%632) # torch/nn/functional.py:1117:17
  %634 : Tensor = prim::GetAttr[name="weight"](%609)
  %635 : Tensor? = prim::GetAttr[name="bias"](%609)
  %636 : int[] = prim::ListConstruct(%27, %27)
  %637 : int[] = prim::ListConstruct(%27, %27)
  %638 : int[] = prim::ListConstruct(%27, %27)
  %new_features.105 : Tensor = aten::conv2d(%result.104, %634, %635, %636, %637, %638, %27) # torch/nn/modules/conv.py:415:15
  %640 : float = prim::GetAttr[name="drop_rate"](%79)
  %641 : bool = aten::gt(%640, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.103 : Tensor = prim::If(%641) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %643 : float = prim::GetAttr[name="drop_rate"](%79)
      %644 : bool = prim::GetAttr[name="training"](%79)
      %645 : bool = aten::lt(%643, %16) # torch/nn/functional.py:968:7
      %646 : bool = prim::If(%645) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %647 : bool = aten::gt(%643, %17) # torch/nn/functional.py:968:17
          -> (%647)
       = prim::If(%646) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %648 : Tensor = aten::dropout(%new_features.105, %643, %644) # torch/nn/functional.py:973:17
      -> (%648)
    block1():
      -> (%new_features.105)
  %649 : Tensor[] = aten::append(%features.2, %new_features.103) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.11 : Tensor = aten::cat(%features.2, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %651 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm"](%32)
  %652 : __torch__.torch.nn.modules.conv.___torch_mangle_180.Conv2d = prim::GetAttr[name="conv"](%32)
  %653 : int = aten::dim(%input.11) # torch/nn/modules/batchnorm.py:276:11
  %654 : bool = aten::ne(%653, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%654) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %655 : bool = prim::GetAttr[name="training"](%651)
   = prim::If(%655) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %656 : Tensor = prim::GetAttr[name="num_batches_tracked"](%651)
      %657 : Tensor = aten::add(%656, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%651, %657)
      -> ()
    block1():
      -> ()
  %658 : bool = prim::GetAttr[name="training"](%651)
  %659 : Tensor = prim::GetAttr[name="running_mean"](%651)
  %660 : Tensor = prim::GetAttr[name="running_var"](%651)
  %661 : Tensor = prim::GetAttr[name="weight"](%651)
  %662 : Tensor = prim::GetAttr[name="bias"](%651)
   = prim::If(%658) # torch/nn/functional.py:2011:4
    block0():
      %663 : int[] = aten::size(%input.11) # torch/nn/functional.py:2012:27
      %size_prods.296 : int = aten::__getitem__(%663, %24) # torch/nn/functional.py:1991:17
      %665 : int = aten::len(%663) # torch/nn/functional.py:1992:19
      %666 : int = aten::sub(%665, %26) # torch/nn/functional.py:1992:19
      %size_prods.297 : int = prim::Loop(%666, %25, %size_prods.296) # torch/nn/functional.py:1992:4
        block0(%i.75 : int, %size_prods.298 : int):
          %670 : int = aten::add(%i.75, %26) # torch/nn/functional.py:1993:27
          %671 : int = aten::__getitem__(%663, %670) # torch/nn/functional.py:1993:22
          %size_prods.299 : int = aten::mul(%size_prods.298, %671) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.299)
      %673 : bool = aten::eq(%size_prods.297, %27) # torch/nn/functional.py:1994:7
       = prim::If(%673) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.16 : Tensor = aten::batch_norm(%input.11, %661, %662, %659, %660, %658, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.12 : Tensor = aten::relu_(%input.16) # torch/nn/functional.py:1117:17
  %676 : Tensor = prim::GetAttr[name="weight"](%652)
  %677 : Tensor? = prim::GetAttr[name="bias"](%652)
  %678 : int[] = prim::ListConstruct(%27, %27)
  %679 : int[] = prim::ListConstruct(%24, %24)
  %680 : int[] = prim::ListConstruct(%27, %27)
  %input.14 : Tensor = aten::conv2d(%input.12, %676, %677, %678, %679, %680, %27) # torch/nn/modules/conv.py:415:15
  %682 : int[] = prim::ListConstruct(%26, %26)
  %683 : int[] = prim::ListConstruct(%26, %26)
  %684 : int[] = prim::ListConstruct(%24, %24)
  %input.13 : Tensor = aten::avg_pool2d(%input.14, %682, %683, %684, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.3 : Tensor[] = prim::ListConstruct(%input.13)
  %687 : __torch__.torchvision.models.densenet.___torch_mangle_170._DenseLayer = prim::GetAttr[name="denselayer1"](%33)
  %688 : __torch__.torchvision.models.densenet.___torch_mangle_173._DenseLayer = prim::GetAttr[name="denselayer2"](%33)
  %689 : __torch__.torchvision.models.densenet.___torch_mangle_175._DenseLayer = prim::GetAttr[name="denselayer3"](%33)
  %690 : __torch__.torchvision.models.densenet.___torch_mangle_178._DenseLayer = prim::GetAttr[name="denselayer4"](%33)
  %691 : __torch__.torchvision.models.densenet.___torch_mangle_182._DenseLayer = prim::GetAttr[name="denselayer5"](%33)
  %692 : __torch__.torchvision.models.densenet.___torch_mangle_185._DenseLayer = prim::GetAttr[name="denselayer6"](%33)
  %693 : __torch__.torchvision.models.densenet.___torch_mangle_187._DenseLayer = prim::GetAttr[name="denselayer7"](%33)
  %694 : __torch__.torchvision.models.densenet.___torch_mangle_190._DenseLayer = prim::GetAttr[name="denselayer8"](%33)
  %695 : __torch__.torchvision.models.densenet.___torch_mangle_192._DenseLayer = prim::GetAttr[name="denselayer9"](%33)
  %696 : __torch__.torchvision.models.densenet.___torch_mangle_195._DenseLayer = prim::GetAttr[name="denselayer10"](%33)
  %697 : __torch__.torchvision.models.densenet.___torch_mangle_197._DenseLayer = prim::GetAttr[name="denselayer11"](%33)
  %698 : __torch__.torchvision.models.densenet.___torch_mangle_200._DenseLayer = prim::GetAttr[name="denselayer12"](%33)
  %699 : Tensor = prim::Uninitialized()
  %700 : bool = prim::GetAttr[name="memory_efficient"](%687)
  %701 : bool = prim::If(%700) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %702 : bool = prim::Uninitialized()
      %703 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %704 : bool = aten::gt(%703, %24)
      %705 : bool, %706 : bool, %707 : int = prim::Loop(%18, %704, %19, %702, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%708 : int, %709 : bool, %710 : bool, %711 : int):
          %tensor.53 : Tensor = aten::__getitem__(%features.3, %711) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %713 : bool = prim::requires_grad(%tensor.53)
          %714 : bool, %715 : bool = prim::If(%713) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %702)
          %716 : int = aten::add(%711, %27)
          %717 : bool = aten::lt(%716, %703)
          %718 : bool = aten::__and__(%717, %714)
          -> (%718, %713, %715, %716)
      %719 : bool = prim::If(%705)
        block0():
          -> (%706)
        block1():
          -> (%19)
      -> (%719)
    block1():
      -> (%19)
  %bottleneck_output.104 : Tensor = prim::If(%701) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%699)
    block1():
      %concated_features.53 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %722 : __torch__.torch.nn.modules.conv.___torch_mangle_169.Conv2d = prim::GetAttr[name="conv1"](%687)
      %723 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm1"](%687)
      %724 : int = aten::dim(%concated_features.53) # torch/nn/modules/batchnorm.py:276:11
      %725 : bool = aten::ne(%724, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%725) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %726 : bool = prim::GetAttr[name="training"](%723)
       = prim::If(%726) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %727 : Tensor = prim::GetAttr[name="num_batches_tracked"](%723)
          %728 : Tensor = aten::add(%727, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%723, %728)
          -> ()
        block1():
          -> ()
      %729 : bool = prim::GetAttr[name="training"](%723)
      %730 : Tensor = prim::GetAttr[name="running_mean"](%723)
      %731 : Tensor = prim::GetAttr[name="running_var"](%723)
      %732 : Tensor = prim::GetAttr[name="weight"](%723)
      %733 : Tensor = prim::GetAttr[name="bias"](%723)
       = prim::If(%729) # torch/nn/functional.py:2011:4
        block0():
          %734 : int[] = aten::size(%concated_features.53) # torch/nn/functional.py:2012:27
          %size_prods.304 : int = aten::__getitem__(%734, %24) # torch/nn/functional.py:1991:17
          %736 : int = aten::len(%734) # torch/nn/functional.py:1992:19
          %737 : int = aten::sub(%736, %26) # torch/nn/functional.py:1992:19
          %size_prods.305 : int = prim::Loop(%737, %25, %size_prods.304) # torch/nn/functional.py:1992:4
            block0(%i.77 : int, %size_prods.306 : int):
              %741 : int = aten::add(%i.77, %26) # torch/nn/functional.py:1993:27
              %742 : int = aten::__getitem__(%734, %741) # torch/nn/functional.py:1993:22
              %size_prods.307 : int = aten::mul(%size_prods.306, %742) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.307)
          %744 : bool = aten::eq(%size_prods.305, %27) # torch/nn/functional.py:1994:7
           = prim::If(%744) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %745 : Tensor = aten::batch_norm(%concated_features.53, %732, %733, %730, %731, %729, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.105 : Tensor = aten::relu_(%745) # torch/nn/functional.py:1117:17
      %747 : Tensor = prim::GetAttr[name="weight"](%722)
      %748 : Tensor? = prim::GetAttr[name="bias"](%722)
      %749 : int[] = prim::ListConstruct(%27, %27)
      %750 : int[] = prim::ListConstruct(%24, %24)
      %751 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.105 : Tensor = aten::conv2d(%result.105, %747, %748, %749, %750, %751, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.105)
  %753 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%687)
  %754 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%687)
  %755 : int = aten::dim(%bottleneck_output.104) # torch/nn/modules/batchnorm.py:276:11
  %756 : bool = aten::ne(%755, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%756) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %757 : bool = prim::GetAttr[name="training"](%754)
   = prim::If(%757) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %758 : Tensor = prim::GetAttr[name="num_batches_tracked"](%754)
      %759 : Tensor = aten::add(%758, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%754, %759)
      -> ()
    block1():
      -> ()
  %760 : bool = prim::GetAttr[name="training"](%754)
  %761 : Tensor = prim::GetAttr[name="running_mean"](%754)
  %762 : Tensor = prim::GetAttr[name="running_var"](%754)
  %763 : Tensor = prim::GetAttr[name="weight"](%754)
  %764 : Tensor = prim::GetAttr[name="bias"](%754)
   = prim::If(%760) # torch/nn/functional.py:2011:4
    block0():
      %765 : int[] = aten::size(%bottleneck_output.104) # torch/nn/functional.py:2012:27
      %size_prods.308 : int = aten::__getitem__(%765, %24) # torch/nn/functional.py:1991:17
      %767 : int = aten::len(%765) # torch/nn/functional.py:1992:19
      %768 : int = aten::sub(%767, %26) # torch/nn/functional.py:1992:19
      %size_prods.309 : int = prim::Loop(%768, %25, %size_prods.308) # torch/nn/functional.py:1992:4
        block0(%i.78 : int, %size_prods.310 : int):
          %772 : int = aten::add(%i.78, %26) # torch/nn/functional.py:1993:27
          %773 : int = aten::__getitem__(%765, %772) # torch/nn/functional.py:1993:22
          %size_prods.311 : int = aten::mul(%size_prods.310, %773) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.311)
      %775 : bool = aten::eq(%size_prods.309, %27) # torch/nn/functional.py:1994:7
       = prim::If(%775) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %776 : Tensor = aten::batch_norm(%bottleneck_output.104, %763, %764, %761, %762, %760, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.106 : Tensor = aten::relu_(%776) # torch/nn/functional.py:1117:17
  %778 : Tensor = prim::GetAttr[name="weight"](%753)
  %779 : Tensor? = prim::GetAttr[name="bias"](%753)
  %780 : int[] = prim::ListConstruct(%27, %27)
  %781 : int[] = prim::ListConstruct(%27, %27)
  %782 : int[] = prim::ListConstruct(%27, %27)
  %new_features.74 : Tensor = aten::conv2d(%result.106, %778, %779, %780, %781, %782, %27) # torch/nn/modules/conv.py:415:15
  %784 : float = prim::GetAttr[name="drop_rate"](%687)
  %785 : bool = aten::gt(%784, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.108 : Tensor = prim::If(%785) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %787 : float = prim::GetAttr[name="drop_rate"](%687)
      %788 : bool = prim::GetAttr[name="training"](%687)
      %789 : bool = aten::lt(%787, %16) # torch/nn/functional.py:968:7
      %790 : bool = prim::If(%789) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %791 : bool = aten::gt(%787, %17) # torch/nn/functional.py:968:17
          -> (%791)
       = prim::If(%790) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %792 : Tensor = aten::dropout(%new_features.74, %787, %788) # torch/nn/functional.py:973:17
      -> (%792)
    block1():
      -> (%new_features.74)
  %793 : Tensor[] = aten::append(%features.3, %new_features.108) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %794 : Tensor = prim::Uninitialized()
  %795 : bool = prim::GetAttr[name="memory_efficient"](%688)
  %796 : bool = prim::If(%795) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %797 : bool = prim::Uninitialized()
      %798 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %799 : bool = aten::gt(%798, %24)
      %800 : bool, %801 : bool, %802 : int = prim::Loop(%18, %799, %19, %797, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%803 : int, %804 : bool, %805 : bool, %806 : int):
          %tensor.37 : Tensor = aten::__getitem__(%features.3, %806) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %808 : bool = prim::requires_grad(%tensor.37)
          %809 : bool, %810 : bool = prim::If(%808) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %797)
          %811 : int = aten::add(%806, %27)
          %812 : bool = aten::lt(%811, %798)
          %813 : bool = aten::__and__(%812, %809)
          -> (%813, %808, %810, %811)
      %814 : bool = prim::If(%800)
        block0():
          -> (%801)
        block1():
          -> (%19)
      -> (%814)
    block1():
      -> (%19)
  %bottleneck_output.72 : Tensor = prim::If(%796) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%794)
    block1():
      %concated_features.37 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %817 : __torch__.torch.nn.modules.conv.___torch_mangle_172.Conv2d = prim::GetAttr[name="conv1"](%688)
      %818 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_171.BatchNorm2d = prim::GetAttr[name="norm1"](%688)
      %819 : int = aten::dim(%concated_features.37) # torch/nn/modules/batchnorm.py:276:11
      %820 : bool = aten::ne(%819, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%820) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %821 : bool = prim::GetAttr[name="training"](%818)
       = prim::If(%821) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %822 : Tensor = prim::GetAttr[name="num_batches_tracked"](%818)
          %823 : Tensor = aten::add(%822, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%818, %823)
          -> ()
        block1():
          -> ()
      %824 : bool = prim::GetAttr[name="training"](%818)
      %825 : Tensor = prim::GetAttr[name="running_mean"](%818)
      %826 : Tensor = prim::GetAttr[name="running_var"](%818)
      %827 : Tensor = prim::GetAttr[name="weight"](%818)
      %828 : Tensor = prim::GetAttr[name="bias"](%818)
       = prim::If(%824) # torch/nn/functional.py:2011:4
        block0():
          %829 : int[] = aten::size(%concated_features.37) # torch/nn/functional.py:2012:27
          %size_prods.312 : int = aten::__getitem__(%829, %24) # torch/nn/functional.py:1991:17
          %831 : int = aten::len(%829) # torch/nn/functional.py:1992:19
          %832 : int = aten::sub(%831, %26) # torch/nn/functional.py:1992:19
          %size_prods.313 : int = prim::Loop(%832, %25, %size_prods.312) # torch/nn/functional.py:1992:4
            block0(%i.79 : int, %size_prods.314 : int):
              %836 : int = aten::add(%i.79, %26) # torch/nn/functional.py:1993:27
              %837 : int = aten::__getitem__(%829, %836) # torch/nn/functional.py:1993:22
              %size_prods.315 : int = aten::mul(%size_prods.314, %837) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.315)
          %839 : bool = aten::eq(%size_prods.313, %27) # torch/nn/functional.py:1994:7
           = prim::If(%839) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %840 : Tensor = aten::batch_norm(%concated_features.37, %827, %828, %825, %826, %824, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.73 : Tensor = aten::relu_(%840) # torch/nn/functional.py:1117:17
      %842 : Tensor = prim::GetAttr[name="weight"](%817)
      %843 : Tensor? = prim::GetAttr[name="bias"](%817)
      %844 : int[] = prim::ListConstruct(%27, %27)
      %845 : int[] = prim::ListConstruct(%24, %24)
      %846 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.73 : Tensor = aten::conv2d(%result.73, %842, %843, %844, %845, %846, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.73)
  %848 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%688)
  %849 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%688)
  %850 : int = aten::dim(%bottleneck_output.72) # torch/nn/modules/batchnorm.py:276:11
  %851 : bool = aten::ne(%850, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%851) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %852 : bool = prim::GetAttr[name="training"](%849)
   = prim::If(%852) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %853 : Tensor = prim::GetAttr[name="num_batches_tracked"](%849)
      %854 : Tensor = aten::add(%853, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%849, %854)
      -> ()
    block1():
      -> ()
  %855 : bool = prim::GetAttr[name="training"](%849)
  %856 : Tensor = prim::GetAttr[name="running_mean"](%849)
  %857 : Tensor = prim::GetAttr[name="running_var"](%849)
  %858 : Tensor = prim::GetAttr[name="weight"](%849)
  %859 : Tensor = prim::GetAttr[name="bias"](%849)
   = prim::If(%855) # torch/nn/functional.py:2011:4
    block0():
      %860 : int[] = aten::size(%bottleneck_output.72) # torch/nn/functional.py:2012:27
      %size_prods.316 : int = aten::__getitem__(%860, %24) # torch/nn/functional.py:1991:17
      %862 : int = aten::len(%860) # torch/nn/functional.py:1992:19
      %863 : int = aten::sub(%862, %26) # torch/nn/functional.py:1992:19
      %size_prods.317 : int = prim::Loop(%863, %25, %size_prods.316) # torch/nn/functional.py:1992:4
        block0(%i.80 : int, %size_prods.318 : int):
          %867 : int = aten::add(%i.80, %26) # torch/nn/functional.py:1993:27
          %868 : int = aten::__getitem__(%860, %867) # torch/nn/functional.py:1993:22
          %size_prods.319 : int = aten::mul(%size_prods.318, %868) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.319)
      %870 : bool = aten::eq(%size_prods.317, %27) # torch/nn/functional.py:1994:7
       = prim::If(%870) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %871 : Tensor = aten::batch_norm(%bottleneck_output.72, %858, %859, %856, %857, %855, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.74 : Tensor = aten::relu_(%871) # torch/nn/functional.py:1117:17
  %873 : Tensor = prim::GetAttr[name="weight"](%848)
  %874 : Tensor? = prim::GetAttr[name="bias"](%848)
  %875 : int[] = prim::ListConstruct(%27, %27)
  %876 : int[] = prim::ListConstruct(%27, %27)
  %877 : int[] = prim::ListConstruct(%27, %27)
  %new_features.76 : Tensor = aten::conv2d(%result.74, %873, %874, %875, %876, %877, %27) # torch/nn/modules/conv.py:415:15
  %879 : float = prim::GetAttr[name="drop_rate"](%688)
  %880 : bool = aten::gt(%879, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.73 : Tensor = prim::If(%880) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %882 : float = prim::GetAttr[name="drop_rate"](%688)
      %883 : bool = prim::GetAttr[name="training"](%688)
      %884 : bool = aten::lt(%882, %16) # torch/nn/functional.py:968:7
      %885 : bool = prim::If(%884) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %886 : bool = aten::gt(%882, %17) # torch/nn/functional.py:968:17
          -> (%886)
       = prim::If(%885) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %887 : Tensor = aten::dropout(%new_features.76, %882, %883) # torch/nn/functional.py:973:17
      -> (%887)
    block1():
      -> (%new_features.76)
  %888 : Tensor[] = aten::append(%features.3, %new_features.73) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %889 : Tensor = prim::Uninitialized()
  %890 : bool = prim::GetAttr[name="memory_efficient"](%689)
  %891 : bool = prim::If(%890) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %892 : bool = prim::Uninitialized()
      %893 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %894 : bool = aten::gt(%893, %24)
      %895 : bool, %896 : bool, %897 : int = prim::Loop(%18, %894, %19, %892, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%898 : int, %899 : bool, %900 : bool, %901 : int):
          %tensor.38 : Tensor = aten::__getitem__(%features.3, %901) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %903 : bool = prim::requires_grad(%tensor.38)
          %904 : bool, %905 : bool = prim::If(%903) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %892)
          %906 : int = aten::add(%901, %27)
          %907 : bool = aten::lt(%906, %893)
          %908 : bool = aten::__and__(%907, %904)
          -> (%908, %903, %905, %906)
      %909 : bool = prim::If(%895)
        block0():
          -> (%896)
        block1():
          -> (%19)
      -> (%909)
    block1():
      -> (%19)
  %bottleneck_output.74 : Tensor = prim::If(%891) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%889)
    block1():
      %concated_features.38 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %912 : __torch__.torch.nn.modules.conv.___torch_mangle_174.Conv2d = prim::GetAttr[name="conv1"](%689)
      %913 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_88.BatchNorm2d = prim::GetAttr[name="norm1"](%689)
      %914 : int = aten::dim(%concated_features.38) # torch/nn/modules/batchnorm.py:276:11
      %915 : bool = aten::ne(%914, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%915) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %916 : bool = prim::GetAttr[name="training"](%913)
       = prim::If(%916) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %917 : Tensor = prim::GetAttr[name="num_batches_tracked"](%913)
          %918 : Tensor = aten::add(%917, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%913, %918)
          -> ()
        block1():
          -> ()
      %919 : bool = prim::GetAttr[name="training"](%913)
      %920 : Tensor = prim::GetAttr[name="running_mean"](%913)
      %921 : Tensor = prim::GetAttr[name="running_var"](%913)
      %922 : Tensor = prim::GetAttr[name="weight"](%913)
      %923 : Tensor = prim::GetAttr[name="bias"](%913)
       = prim::If(%919) # torch/nn/functional.py:2011:4
        block0():
          %924 : int[] = aten::size(%concated_features.38) # torch/nn/functional.py:2012:27
          %size_prods.320 : int = aten::__getitem__(%924, %24) # torch/nn/functional.py:1991:17
          %926 : int = aten::len(%924) # torch/nn/functional.py:1992:19
          %927 : int = aten::sub(%926, %26) # torch/nn/functional.py:1992:19
          %size_prods.321 : int = prim::Loop(%927, %25, %size_prods.320) # torch/nn/functional.py:1992:4
            block0(%i.81 : int, %size_prods.322 : int):
              %931 : int = aten::add(%i.81, %26) # torch/nn/functional.py:1993:27
              %932 : int = aten::__getitem__(%924, %931) # torch/nn/functional.py:1993:22
              %size_prods.323 : int = aten::mul(%size_prods.322, %932) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.323)
          %934 : bool = aten::eq(%size_prods.321, %27) # torch/nn/functional.py:1994:7
           = prim::If(%934) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %935 : Tensor = aten::batch_norm(%concated_features.38, %922, %923, %920, %921, %919, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.75 : Tensor = aten::relu_(%935) # torch/nn/functional.py:1117:17
      %937 : Tensor = prim::GetAttr[name="weight"](%912)
      %938 : Tensor? = prim::GetAttr[name="bias"](%912)
      %939 : int[] = prim::ListConstruct(%27, %27)
      %940 : int[] = prim::ListConstruct(%24, %24)
      %941 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.75 : Tensor = aten::conv2d(%result.75, %937, %938, %939, %940, %941, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.75)
  %943 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%689)
  %944 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%689)
  %945 : int = aten::dim(%bottleneck_output.74) # torch/nn/modules/batchnorm.py:276:11
  %946 : bool = aten::ne(%945, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%946) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %947 : bool = prim::GetAttr[name="training"](%944)
   = prim::If(%947) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %948 : Tensor = prim::GetAttr[name="num_batches_tracked"](%944)
      %949 : Tensor = aten::add(%948, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%944, %949)
      -> ()
    block1():
      -> ()
  %950 : bool = prim::GetAttr[name="training"](%944)
  %951 : Tensor = prim::GetAttr[name="running_mean"](%944)
  %952 : Tensor = prim::GetAttr[name="running_var"](%944)
  %953 : Tensor = prim::GetAttr[name="weight"](%944)
  %954 : Tensor = prim::GetAttr[name="bias"](%944)
   = prim::If(%950) # torch/nn/functional.py:2011:4
    block0():
      %955 : int[] = aten::size(%bottleneck_output.74) # torch/nn/functional.py:2012:27
      %size_prods.324 : int = aten::__getitem__(%955, %24) # torch/nn/functional.py:1991:17
      %957 : int = aten::len(%955) # torch/nn/functional.py:1992:19
      %958 : int = aten::sub(%957, %26) # torch/nn/functional.py:1992:19
      %size_prods.325 : int = prim::Loop(%958, %25, %size_prods.324) # torch/nn/functional.py:1992:4
        block0(%i.82 : int, %size_prods.326 : int):
          %962 : int = aten::add(%i.82, %26) # torch/nn/functional.py:1993:27
          %963 : int = aten::__getitem__(%955, %962) # torch/nn/functional.py:1993:22
          %size_prods.327 : int = aten::mul(%size_prods.326, %963) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.327)
      %965 : bool = aten::eq(%size_prods.325, %27) # torch/nn/functional.py:1994:7
       = prim::If(%965) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %966 : Tensor = aten::batch_norm(%bottleneck_output.74, %953, %954, %951, %952, %950, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.76 : Tensor = aten::relu_(%966) # torch/nn/functional.py:1117:17
  %968 : Tensor = prim::GetAttr[name="weight"](%943)
  %969 : Tensor? = prim::GetAttr[name="bias"](%943)
  %970 : int[] = prim::ListConstruct(%27, %27)
  %971 : int[] = prim::ListConstruct(%27, %27)
  %972 : int[] = prim::ListConstruct(%27, %27)
  %new_features.78 : Tensor = aten::conv2d(%result.76, %968, %969, %970, %971, %972, %27) # torch/nn/modules/conv.py:415:15
  %974 : float = prim::GetAttr[name="drop_rate"](%689)
  %975 : bool = aten::gt(%974, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.75 : Tensor = prim::If(%975) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %977 : float = prim::GetAttr[name="drop_rate"](%689)
      %978 : bool = prim::GetAttr[name="training"](%689)
      %979 : bool = aten::lt(%977, %16) # torch/nn/functional.py:968:7
      %980 : bool = prim::If(%979) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %981 : bool = aten::gt(%977, %17) # torch/nn/functional.py:968:17
          -> (%981)
       = prim::If(%980) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %982 : Tensor = aten::dropout(%new_features.78, %977, %978) # torch/nn/functional.py:973:17
      -> (%982)
    block1():
      -> (%new_features.78)
  %983 : Tensor[] = aten::append(%features.3, %new_features.75) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %984 : Tensor = prim::Uninitialized()
  %985 : bool = prim::GetAttr[name="memory_efficient"](%690)
  %986 : bool = prim::If(%985) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %987 : bool = prim::Uninitialized()
      %988 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %989 : bool = aten::gt(%988, %24)
      %990 : bool, %991 : bool, %992 : int = prim::Loop(%18, %989, %19, %987, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%993 : int, %994 : bool, %995 : bool, %996 : int):
          %tensor.39 : Tensor = aten::__getitem__(%features.3, %996) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %998 : bool = prim::requires_grad(%tensor.39)
          %999 : bool, %1000 : bool = prim::If(%998) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %987)
          %1001 : int = aten::add(%996, %27)
          %1002 : bool = aten::lt(%1001, %988)
          %1003 : bool = aten::__and__(%1002, %999)
          -> (%1003, %998, %1000, %1001)
      %1004 : bool = prim::If(%990)
        block0():
          -> (%991)
        block1():
          -> (%19)
      -> (%1004)
    block1():
      -> (%19)
  %bottleneck_output.76 : Tensor = prim::If(%986) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%984)
    block1():
      %concated_features.39 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1007 : __torch__.torch.nn.modules.conv.___torch_mangle_177.Conv2d = prim::GetAttr[name="conv1"](%690)
      %1008 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_176.BatchNorm2d = prim::GetAttr[name="norm1"](%690)
      %1009 : int = aten::dim(%concated_features.39) # torch/nn/modules/batchnorm.py:276:11
      %1010 : bool = aten::ne(%1009, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1010) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1011 : bool = prim::GetAttr[name="training"](%1008)
       = prim::If(%1011) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1012 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1008)
          %1013 : Tensor = aten::add(%1012, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1008, %1013)
          -> ()
        block1():
          -> ()
      %1014 : bool = prim::GetAttr[name="training"](%1008)
      %1015 : Tensor = prim::GetAttr[name="running_mean"](%1008)
      %1016 : Tensor = prim::GetAttr[name="running_var"](%1008)
      %1017 : Tensor = prim::GetAttr[name="weight"](%1008)
      %1018 : Tensor = prim::GetAttr[name="bias"](%1008)
       = prim::If(%1014) # torch/nn/functional.py:2011:4
        block0():
          %1019 : int[] = aten::size(%concated_features.39) # torch/nn/functional.py:2012:27
          %size_prods.328 : int = aten::__getitem__(%1019, %24) # torch/nn/functional.py:1991:17
          %1021 : int = aten::len(%1019) # torch/nn/functional.py:1992:19
          %1022 : int = aten::sub(%1021, %26) # torch/nn/functional.py:1992:19
          %size_prods.329 : int = prim::Loop(%1022, %25, %size_prods.328) # torch/nn/functional.py:1992:4
            block0(%i.83 : int, %size_prods.330 : int):
              %1026 : int = aten::add(%i.83, %26) # torch/nn/functional.py:1993:27
              %1027 : int = aten::__getitem__(%1019, %1026) # torch/nn/functional.py:1993:22
              %size_prods.331 : int = aten::mul(%size_prods.330, %1027) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.331)
          %1029 : bool = aten::eq(%size_prods.329, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1029) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1030 : Tensor = aten::batch_norm(%concated_features.39, %1017, %1018, %1015, %1016, %1014, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.77 : Tensor = aten::relu_(%1030) # torch/nn/functional.py:1117:17
      %1032 : Tensor = prim::GetAttr[name="weight"](%1007)
      %1033 : Tensor? = prim::GetAttr[name="bias"](%1007)
      %1034 : int[] = prim::ListConstruct(%27, %27)
      %1035 : int[] = prim::ListConstruct(%24, %24)
      %1036 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.77 : Tensor = aten::conv2d(%result.77, %1032, %1033, %1034, %1035, %1036, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.77)
  %1038 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%690)
  %1039 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%690)
  %1040 : int = aten::dim(%bottleneck_output.76) # torch/nn/modules/batchnorm.py:276:11
  %1041 : bool = aten::ne(%1040, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1041) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1042 : bool = prim::GetAttr[name="training"](%1039)
   = prim::If(%1042) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1043 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1039)
      %1044 : Tensor = aten::add(%1043, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1039, %1044)
      -> ()
    block1():
      -> ()
  %1045 : bool = prim::GetAttr[name="training"](%1039)
  %1046 : Tensor = prim::GetAttr[name="running_mean"](%1039)
  %1047 : Tensor = prim::GetAttr[name="running_var"](%1039)
  %1048 : Tensor = prim::GetAttr[name="weight"](%1039)
  %1049 : Tensor = prim::GetAttr[name="bias"](%1039)
   = prim::If(%1045) # torch/nn/functional.py:2011:4
    block0():
      %1050 : int[] = aten::size(%bottleneck_output.76) # torch/nn/functional.py:2012:27
      %size_prods.332 : int = aten::__getitem__(%1050, %24) # torch/nn/functional.py:1991:17
      %1052 : int = aten::len(%1050) # torch/nn/functional.py:1992:19
      %1053 : int = aten::sub(%1052, %26) # torch/nn/functional.py:1992:19
      %size_prods.333 : int = prim::Loop(%1053, %25, %size_prods.332) # torch/nn/functional.py:1992:4
        block0(%i.84 : int, %size_prods.334 : int):
          %1057 : int = aten::add(%i.84, %26) # torch/nn/functional.py:1993:27
          %1058 : int = aten::__getitem__(%1050, %1057) # torch/nn/functional.py:1993:22
          %size_prods.335 : int = aten::mul(%size_prods.334, %1058) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.335)
      %1060 : bool = aten::eq(%size_prods.333, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1060) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1061 : Tensor = aten::batch_norm(%bottleneck_output.76, %1048, %1049, %1046, %1047, %1045, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.78 : Tensor = aten::relu_(%1061) # torch/nn/functional.py:1117:17
  %1063 : Tensor = prim::GetAttr[name="weight"](%1038)
  %1064 : Tensor? = prim::GetAttr[name="bias"](%1038)
  %1065 : int[] = prim::ListConstruct(%27, %27)
  %1066 : int[] = prim::ListConstruct(%27, %27)
  %1067 : int[] = prim::ListConstruct(%27, %27)
  %new_features.80 : Tensor = aten::conv2d(%result.78, %1063, %1064, %1065, %1066, %1067, %27) # torch/nn/modules/conv.py:415:15
  %1069 : float = prim::GetAttr[name="drop_rate"](%690)
  %1070 : bool = aten::gt(%1069, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.77 : Tensor = prim::If(%1070) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1072 : float = prim::GetAttr[name="drop_rate"](%690)
      %1073 : bool = prim::GetAttr[name="training"](%690)
      %1074 : bool = aten::lt(%1072, %16) # torch/nn/functional.py:968:7
      %1075 : bool = prim::If(%1074) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1076 : bool = aten::gt(%1072, %17) # torch/nn/functional.py:968:17
          -> (%1076)
       = prim::If(%1075) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1077 : Tensor = aten::dropout(%new_features.80, %1072, %1073) # torch/nn/functional.py:973:17
      -> (%1077)
    block1():
      -> (%new_features.80)
  %1078 : Tensor[] = aten::append(%features.3, %new_features.77) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1079 : Tensor = prim::Uninitialized()
  %1080 : bool = prim::GetAttr[name="memory_efficient"](%691)
  %1081 : bool = prim::If(%1080) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1082 : bool = prim::Uninitialized()
      %1083 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1084 : bool = aten::gt(%1083, %24)
      %1085 : bool, %1086 : bool, %1087 : int = prim::Loop(%18, %1084, %19, %1082, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1088 : int, %1089 : bool, %1090 : bool, %1091 : int):
          %tensor.40 : Tensor = aten::__getitem__(%features.3, %1091) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1093 : bool = prim::requires_grad(%tensor.40)
          %1094 : bool, %1095 : bool = prim::If(%1093) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1082)
          %1096 : int = aten::add(%1091, %27)
          %1097 : bool = aten::lt(%1096, %1083)
          %1098 : bool = aten::__and__(%1097, %1094)
          -> (%1098, %1093, %1095, %1096)
      %1099 : bool = prim::If(%1085)
        block0():
          -> (%1086)
        block1():
          -> (%19)
      -> (%1099)
    block1():
      -> (%19)
  %bottleneck_output.78 : Tensor = prim::If(%1081) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1079)
    block1():
      %concated_features.40 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1102 : __torch__.torch.nn.modules.conv.___torch_mangle_180.Conv2d = prim::GetAttr[name="conv1"](%691)
      %1103 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%691)
      %1104 : int = aten::dim(%concated_features.40) # torch/nn/modules/batchnorm.py:276:11
      %1105 : bool = aten::ne(%1104, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1105) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1106 : bool = prim::GetAttr[name="training"](%1103)
       = prim::If(%1106) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1107 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1103)
          %1108 : Tensor = aten::add(%1107, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1103, %1108)
          -> ()
        block1():
          -> ()
      %1109 : bool = prim::GetAttr[name="training"](%1103)
      %1110 : Tensor = prim::GetAttr[name="running_mean"](%1103)
      %1111 : Tensor = prim::GetAttr[name="running_var"](%1103)
      %1112 : Tensor = prim::GetAttr[name="weight"](%1103)
      %1113 : Tensor = prim::GetAttr[name="bias"](%1103)
       = prim::If(%1109) # torch/nn/functional.py:2011:4
        block0():
          %1114 : int[] = aten::size(%concated_features.40) # torch/nn/functional.py:2012:27
          %size_prods.336 : int = aten::__getitem__(%1114, %24) # torch/nn/functional.py:1991:17
          %1116 : int = aten::len(%1114) # torch/nn/functional.py:1992:19
          %1117 : int = aten::sub(%1116, %26) # torch/nn/functional.py:1992:19
          %size_prods.337 : int = prim::Loop(%1117, %25, %size_prods.336) # torch/nn/functional.py:1992:4
            block0(%i.85 : int, %size_prods.338 : int):
              %1121 : int = aten::add(%i.85, %26) # torch/nn/functional.py:1993:27
              %1122 : int = aten::__getitem__(%1114, %1121) # torch/nn/functional.py:1993:22
              %size_prods.339 : int = aten::mul(%size_prods.338, %1122) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.339)
          %1124 : bool = aten::eq(%size_prods.337, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1124) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1125 : Tensor = aten::batch_norm(%concated_features.40, %1112, %1113, %1110, %1111, %1109, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.79 : Tensor = aten::relu_(%1125) # torch/nn/functional.py:1117:17
      %1127 : Tensor = prim::GetAttr[name="weight"](%1102)
      %1128 : Tensor? = prim::GetAttr[name="bias"](%1102)
      %1129 : int[] = prim::ListConstruct(%27, %27)
      %1130 : int[] = prim::ListConstruct(%24, %24)
      %1131 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.79 : Tensor = aten::conv2d(%result.79, %1127, %1128, %1129, %1130, %1131, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.79)
  %1133 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%691)
  %1134 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%691)
  %1135 : int = aten::dim(%bottleneck_output.78) # torch/nn/modules/batchnorm.py:276:11
  %1136 : bool = aten::ne(%1135, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1136) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1137 : bool = prim::GetAttr[name="training"](%1134)
   = prim::If(%1137) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1138 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1134)
      %1139 : Tensor = aten::add(%1138, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1134, %1139)
      -> ()
    block1():
      -> ()
  %1140 : bool = prim::GetAttr[name="training"](%1134)
  %1141 : Tensor = prim::GetAttr[name="running_mean"](%1134)
  %1142 : Tensor = prim::GetAttr[name="running_var"](%1134)
  %1143 : Tensor = prim::GetAttr[name="weight"](%1134)
  %1144 : Tensor = prim::GetAttr[name="bias"](%1134)
   = prim::If(%1140) # torch/nn/functional.py:2011:4
    block0():
      %1145 : int[] = aten::size(%bottleneck_output.78) # torch/nn/functional.py:2012:27
      %size_prods.340 : int = aten::__getitem__(%1145, %24) # torch/nn/functional.py:1991:17
      %1147 : int = aten::len(%1145) # torch/nn/functional.py:1992:19
      %1148 : int = aten::sub(%1147, %26) # torch/nn/functional.py:1992:19
      %size_prods.341 : int = prim::Loop(%1148, %25, %size_prods.340) # torch/nn/functional.py:1992:4
        block0(%i.86 : int, %size_prods.342 : int):
          %1152 : int = aten::add(%i.86, %26) # torch/nn/functional.py:1993:27
          %1153 : int = aten::__getitem__(%1145, %1152) # torch/nn/functional.py:1993:22
          %size_prods.343 : int = aten::mul(%size_prods.342, %1153) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.343)
      %1155 : bool = aten::eq(%size_prods.341, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1155) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1156 : Tensor = aten::batch_norm(%bottleneck_output.78, %1143, %1144, %1141, %1142, %1140, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.80 : Tensor = aten::relu_(%1156) # torch/nn/functional.py:1117:17
  %1158 : Tensor = prim::GetAttr[name="weight"](%1133)
  %1159 : Tensor? = prim::GetAttr[name="bias"](%1133)
  %1160 : int[] = prim::ListConstruct(%27, %27)
  %1161 : int[] = prim::ListConstruct(%27, %27)
  %1162 : int[] = prim::ListConstruct(%27, %27)
  %new_features.82 : Tensor = aten::conv2d(%result.80, %1158, %1159, %1160, %1161, %1162, %27) # torch/nn/modules/conv.py:415:15
  %1164 : float = prim::GetAttr[name="drop_rate"](%691)
  %1165 : bool = aten::gt(%1164, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.79 : Tensor = prim::If(%1165) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1167 : float = prim::GetAttr[name="drop_rate"](%691)
      %1168 : bool = prim::GetAttr[name="training"](%691)
      %1169 : bool = aten::lt(%1167, %16) # torch/nn/functional.py:968:7
      %1170 : bool = prim::If(%1169) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1171 : bool = aten::gt(%1167, %17) # torch/nn/functional.py:968:17
          -> (%1171)
       = prim::If(%1170) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1172 : Tensor = aten::dropout(%new_features.82, %1167, %1168) # torch/nn/functional.py:973:17
      -> (%1172)
    block1():
      -> (%new_features.82)
  %1173 : Tensor[] = aten::append(%features.3, %new_features.79) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1174 : Tensor = prim::Uninitialized()
  %1175 : bool = prim::GetAttr[name="memory_efficient"](%692)
  %1176 : bool = prim::If(%1175) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1177 : bool = prim::Uninitialized()
      %1178 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1179 : bool = aten::gt(%1178, %24)
      %1180 : bool, %1181 : bool, %1182 : int = prim::Loop(%18, %1179, %19, %1177, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1183 : int, %1184 : bool, %1185 : bool, %1186 : int):
          %tensor.41 : Tensor = aten::__getitem__(%features.3, %1186) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1188 : bool = prim::requires_grad(%tensor.41)
          %1189 : bool, %1190 : bool = prim::If(%1188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1177)
          %1191 : int = aten::add(%1186, %27)
          %1192 : bool = aten::lt(%1191, %1178)
          %1193 : bool = aten::__and__(%1192, %1189)
          -> (%1193, %1188, %1190, %1191)
      %1194 : bool = prim::If(%1180)
        block0():
          -> (%1181)
        block1():
          -> (%19)
      -> (%1194)
    block1():
      -> (%19)
  %bottleneck_output.80 : Tensor = prim::If(%1176) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1174)
    block1():
      %concated_features.41 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1197 : __torch__.torch.nn.modules.conv.___torch_mangle_184.Conv2d = prim::GetAttr[name="conv1"](%692)
      %1198 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_183.BatchNorm2d = prim::GetAttr[name="norm1"](%692)
      %1199 : int = aten::dim(%concated_features.41) # torch/nn/modules/batchnorm.py:276:11
      %1200 : bool = aten::ne(%1199, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1200) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1201 : bool = prim::GetAttr[name="training"](%1198)
       = prim::If(%1201) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1202 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1198)
          %1203 : Tensor = aten::add(%1202, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1198, %1203)
          -> ()
        block1():
          -> ()
      %1204 : bool = prim::GetAttr[name="training"](%1198)
      %1205 : Tensor = prim::GetAttr[name="running_mean"](%1198)
      %1206 : Tensor = prim::GetAttr[name="running_var"](%1198)
      %1207 : Tensor = prim::GetAttr[name="weight"](%1198)
      %1208 : Tensor = prim::GetAttr[name="bias"](%1198)
       = prim::If(%1204) # torch/nn/functional.py:2011:4
        block0():
          %1209 : int[] = aten::size(%concated_features.41) # torch/nn/functional.py:2012:27
          %size_prods.344 : int = aten::__getitem__(%1209, %24) # torch/nn/functional.py:1991:17
          %1211 : int = aten::len(%1209) # torch/nn/functional.py:1992:19
          %1212 : int = aten::sub(%1211, %26) # torch/nn/functional.py:1992:19
          %size_prods.345 : int = prim::Loop(%1212, %25, %size_prods.344) # torch/nn/functional.py:1992:4
            block0(%i.87 : int, %size_prods.346 : int):
              %1216 : int = aten::add(%i.87, %26) # torch/nn/functional.py:1993:27
              %1217 : int = aten::__getitem__(%1209, %1216) # torch/nn/functional.py:1993:22
              %size_prods.347 : int = aten::mul(%size_prods.346, %1217) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.347)
          %1219 : bool = aten::eq(%size_prods.345, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1219) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1220 : Tensor = aten::batch_norm(%concated_features.41, %1207, %1208, %1205, %1206, %1204, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.81 : Tensor = aten::relu_(%1220) # torch/nn/functional.py:1117:17
      %1222 : Tensor = prim::GetAttr[name="weight"](%1197)
      %1223 : Tensor? = prim::GetAttr[name="bias"](%1197)
      %1224 : int[] = prim::ListConstruct(%27, %27)
      %1225 : int[] = prim::ListConstruct(%24, %24)
      %1226 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.81 : Tensor = aten::conv2d(%result.81, %1222, %1223, %1224, %1225, %1226, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.81)
  %1228 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%692)
  %1229 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%692)
  %1230 : int = aten::dim(%bottleneck_output.80) # torch/nn/modules/batchnorm.py:276:11
  %1231 : bool = aten::ne(%1230, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1231) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1232 : bool = prim::GetAttr[name="training"](%1229)
   = prim::If(%1232) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1233 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1229)
      %1234 : Tensor = aten::add(%1233, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1229, %1234)
      -> ()
    block1():
      -> ()
  %1235 : bool = prim::GetAttr[name="training"](%1229)
  %1236 : Tensor = prim::GetAttr[name="running_mean"](%1229)
  %1237 : Tensor = prim::GetAttr[name="running_var"](%1229)
  %1238 : Tensor = prim::GetAttr[name="weight"](%1229)
  %1239 : Tensor = prim::GetAttr[name="bias"](%1229)
   = prim::If(%1235) # torch/nn/functional.py:2011:4
    block0():
      %1240 : int[] = aten::size(%bottleneck_output.80) # torch/nn/functional.py:2012:27
      %size_prods.348 : int = aten::__getitem__(%1240, %24) # torch/nn/functional.py:1991:17
      %1242 : int = aten::len(%1240) # torch/nn/functional.py:1992:19
      %1243 : int = aten::sub(%1242, %26) # torch/nn/functional.py:1992:19
      %size_prods.349 : int = prim::Loop(%1243, %25, %size_prods.348) # torch/nn/functional.py:1992:4
        block0(%i.88 : int, %size_prods.350 : int):
          %1247 : int = aten::add(%i.88, %26) # torch/nn/functional.py:1993:27
          %1248 : int = aten::__getitem__(%1240, %1247) # torch/nn/functional.py:1993:22
          %size_prods.351 : int = aten::mul(%size_prods.350, %1248) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.351)
      %1250 : bool = aten::eq(%size_prods.349, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1250) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1251 : Tensor = aten::batch_norm(%bottleneck_output.80, %1238, %1239, %1236, %1237, %1235, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.82 : Tensor = aten::relu_(%1251) # torch/nn/functional.py:1117:17
  %1253 : Tensor = prim::GetAttr[name="weight"](%1228)
  %1254 : Tensor? = prim::GetAttr[name="bias"](%1228)
  %1255 : int[] = prim::ListConstruct(%27, %27)
  %1256 : int[] = prim::ListConstruct(%27, %27)
  %1257 : int[] = prim::ListConstruct(%27, %27)
  %new_features.84 : Tensor = aten::conv2d(%result.82, %1253, %1254, %1255, %1256, %1257, %27) # torch/nn/modules/conv.py:415:15
  %1259 : float = prim::GetAttr[name="drop_rate"](%692)
  %1260 : bool = aten::gt(%1259, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.81 : Tensor = prim::If(%1260) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1262 : float = prim::GetAttr[name="drop_rate"](%692)
      %1263 : bool = prim::GetAttr[name="training"](%692)
      %1264 : bool = aten::lt(%1262, %16) # torch/nn/functional.py:968:7
      %1265 : bool = prim::If(%1264) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1266 : bool = aten::gt(%1262, %17) # torch/nn/functional.py:968:17
          -> (%1266)
       = prim::If(%1265) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1267 : Tensor = aten::dropout(%new_features.84, %1262, %1263) # torch/nn/functional.py:973:17
      -> (%1267)
    block1():
      -> (%new_features.84)
  %1268 : Tensor[] = aten::append(%features.3, %new_features.81) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1269 : Tensor = prim::Uninitialized()
  %1270 : bool = prim::GetAttr[name="memory_efficient"](%693)
  %1271 : bool = prim::If(%1270) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1272 : bool = prim::Uninitialized()
      %1273 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1274 : bool = aten::gt(%1273, %24)
      %1275 : bool, %1276 : bool, %1277 : int = prim::Loop(%18, %1274, %19, %1272, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1278 : int, %1279 : bool, %1280 : bool, %1281 : int):
          %tensor.42 : Tensor = aten::__getitem__(%features.3, %1281) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1283 : bool = prim::requires_grad(%tensor.42)
          %1284 : bool, %1285 : bool = prim::If(%1283) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1272)
          %1286 : int = aten::add(%1281, %27)
          %1287 : bool = aten::lt(%1286, %1273)
          %1288 : bool = aten::__and__(%1287, %1284)
          -> (%1288, %1283, %1285, %1286)
      %1289 : bool = prim::If(%1275)
        block0():
          -> (%1276)
        block1():
          -> (%19)
      -> (%1289)
    block1():
      -> (%19)
  %bottleneck_output.82 : Tensor = prim::If(%1271) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1269)
    block1():
      %concated_features.42 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1292 : __torch__.torch.nn.modules.conv.___torch_mangle_186.Conv2d = prim::GetAttr[name="conv1"](%693)
      %1293 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%693)
      %1294 : int = aten::dim(%concated_features.42) # torch/nn/modules/batchnorm.py:276:11
      %1295 : bool = aten::ne(%1294, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1295) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1296 : bool = prim::GetAttr[name="training"](%1293)
       = prim::If(%1296) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1297 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1293)
          %1298 : Tensor = aten::add(%1297, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1293, %1298)
          -> ()
        block1():
          -> ()
      %1299 : bool = prim::GetAttr[name="training"](%1293)
      %1300 : Tensor = prim::GetAttr[name="running_mean"](%1293)
      %1301 : Tensor = prim::GetAttr[name="running_var"](%1293)
      %1302 : Tensor = prim::GetAttr[name="weight"](%1293)
      %1303 : Tensor = prim::GetAttr[name="bias"](%1293)
       = prim::If(%1299) # torch/nn/functional.py:2011:4
        block0():
          %1304 : int[] = aten::size(%concated_features.42) # torch/nn/functional.py:2012:27
          %size_prods.352 : int = aten::__getitem__(%1304, %24) # torch/nn/functional.py:1991:17
          %1306 : int = aten::len(%1304) # torch/nn/functional.py:1992:19
          %1307 : int = aten::sub(%1306, %26) # torch/nn/functional.py:1992:19
          %size_prods.353 : int = prim::Loop(%1307, %25, %size_prods.352) # torch/nn/functional.py:1992:4
            block0(%i.89 : int, %size_prods.354 : int):
              %1311 : int = aten::add(%i.89, %26) # torch/nn/functional.py:1993:27
              %1312 : int = aten::__getitem__(%1304, %1311) # torch/nn/functional.py:1993:22
              %size_prods.355 : int = aten::mul(%size_prods.354, %1312) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.355)
          %1314 : bool = aten::eq(%size_prods.353, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1314) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1315 : Tensor = aten::batch_norm(%concated_features.42, %1302, %1303, %1300, %1301, %1299, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.83 : Tensor = aten::relu_(%1315) # torch/nn/functional.py:1117:17
      %1317 : Tensor = prim::GetAttr[name="weight"](%1292)
      %1318 : Tensor? = prim::GetAttr[name="bias"](%1292)
      %1319 : int[] = prim::ListConstruct(%27, %27)
      %1320 : int[] = prim::ListConstruct(%24, %24)
      %1321 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.83 : Tensor = aten::conv2d(%result.83, %1317, %1318, %1319, %1320, %1321, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.83)
  %1323 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%693)
  %1324 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%693)
  %1325 : int = aten::dim(%bottleneck_output.82) # torch/nn/modules/batchnorm.py:276:11
  %1326 : bool = aten::ne(%1325, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1326) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1327 : bool = prim::GetAttr[name="training"](%1324)
   = prim::If(%1327) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1328 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1324)
      %1329 : Tensor = aten::add(%1328, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1324, %1329)
      -> ()
    block1():
      -> ()
  %1330 : bool = prim::GetAttr[name="training"](%1324)
  %1331 : Tensor = prim::GetAttr[name="running_mean"](%1324)
  %1332 : Tensor = prim::GetAttr[name="running_var"](%1324)
  %1333 : Tensor = prim::GetAttr[name="weight"](%1324)
  %1334 : Tensor = prim::GetAttr[name="bias"](%1324)
   = prim::If(%1330) # torch/nn/functional.py:2011:4
    block0():
      %1335 : int[] = aten::size(%bottleneck_output.82) # torch/nn/functional.py:2012:27
      %size_prods.356 : int = aten::__getitem__(%1335, %24) # torch/nn/functional.py:1991:17
      %1337 : int = aten::len(%1335) # torch/nn/functional.py:1992:19
      %1338 : int = aten::sub(%1337, %26) # torch/nn/functional.py:1992:19
      %size_prods.357 : int = prim::Loop(%1338, %25, %size_prods.356) # torch/nn/functional.py:1992:4
        block0(%i.90 : int, %size_prods.358 : int):
          %1342 : int = aten::add(%i.90, %26) # torch/nn/functional.py:1993:27
          %1343 : int = aten::__getitem__(%1335, %1342) # torch/nn/functional.py:1993:22
          %size_prods.359 : int = aten::mul(%size_prods.358, %1343) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.359)
      %1345 : bool = aten::eq(%size_prods.357, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1345) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1346 : Tensor = aten::batch_norm(%bottleneck_output.82, %1333, %1334, %1331, %1332, %1330, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.84 : Tensor = aten::relu_(%1346) # torch/nn/functional.py:1117:17
  %1348 : Tensor = prim::GetAttr[name="weight"](%1323)
  %1349 : Tensor? = prim::GetAttr[name="bias"](%1323)
  %1350 : int[] = prim::ListConstruct(%27, %27)
  %1351 : int[] = prim::ListConstruct(%27, %27)
  %1352 : int[] = prim::ListConstruct(%27, %27)
  %new_features.86 : Tensor = aten::conv2d(%result.84, %1348, %1349, %1350, %1351, %1352, %27) # torch/nn/modules/conv.py:415:15
  %1354 : float = prim::GetAttr[name="drop_rate"](%693)
  %1355 : bool = aten::gt(%1354, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.83 : Tensor = prim::If(%1355) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1357 : float = prim::GetAttr[name="drop_rate"](%693)
      %1358 : bool = prim::GetAttr[name="training"](%693)
      %1359 : bool = aten::lt(%1357, %16) # torch/nn/functional.py:968:7
      %1360 : bool = prim::If(%1359) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1361 : bool = aten::gt(%1357, %17) # torch/nn/functional.py:968:17
          -> (%1361)
       = prim::If(%1360) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1362 : Tensor = aten::dropout(%new_features.86, %1357, %1358) # torch/nn/functional.py:973:17
      -> (%1362)
    block1():
      -> (%new_features.86)
  %1363 : Tensor[] = aten::append(%features.3, %new_features.83) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1364 : Tensor = prim::Uninitialized()
  %1365 : bool = prim::GetAttr[name="memory_efficient"](%694)
  %1366 : bool = prim::If(%1365) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1367 : bool = prim::Uninitialized()
      %1368 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1369 : bool = aten::gt(%1368, %24)
      %1370 : bool, %1371 : bool, %1372 : int = prim::Loop(%18, %1369, %19, %1367, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1373 : int, %1374 : bool, %1375 : bool, %1376 : int):
          %tensor.43 : Tensor = aten::__getitem__(%features.3, %1376) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1378 : bool = prim::requires_grad(%tensor.43)
          %1379 : bool, %1380 : bool = prim::If(%1378) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1367)
          %1381 : int = aten::add(%1376, %27)
          %1382 : bool = aten::lt(%1381, %1368)
          %1383 : bool = aten::__and__(%1382, %1379)
          -> (%1383, %1378, %1380, %1381)
      %1384 : bool = prim::If(%1370)
        block0():
          -> (%1371)
        block1():
          -> (%19)
      -> (%1384)
    block1():
      -> (%19)
  %bottleneck_output.84 : Tensor = prim::If(%1366) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1364)
    block1():
      %concated_features.43 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1387 : __torch__.torch.nn.modules.conv.___torch_mangle_189.Conv2d = prim::GetAttr[name="conv1"](%694)
      %1388 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_188.BatchNorm2d = prim::GetAttr[name="norm1"](%694)
      %1389 : int = aten::dim(%concated_features.43) # torch/nn/modules/batchnorm.py:276:11
      %1390 : bool = aten::ne(%1389, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1390) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1391 : bool = prim::GetAttr[name="training"](%1388)
       = prim::If(%1391) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1392 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1388)
          %1393 : Tensor = aten::add(%1392, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1388, %1393)
          -> ()
        block1():
          -> ()
      %1394 : bool = prim::GetAttr[name="training"](%1388)
      %1395 : Tensor = prim::GetAttr[name="running_mean"](%1388)
      %1396 : Tensor = prim::GetAttr[name="running_var"](%1388)
      %1397 : Tensor = prim::GetAttr[name="weight"](%1388)
      %1398 : Tensor = prim::GetAttr[name="bias"](%1388)
       = prim::If(%1394) # torch/nn/functional.py:2011:4
        block0():
          %1399 : int[] = aten::size(%concated_features.43) # torch/nn/functional.py:2012:27
          %size_prods.360 : int = aten::__getitem__(%1399, %24) # torch/nn/functional.py:1991:17
          %1401 : int = aten::len(%1399) # torch/nn/functional.py:1992:19
          %1402 : int = aten::sub(%1401, %26) # torch/nn/functional.py:1992:19
          %size_prods.361 : int = prim::Loop(%1402, %25, %size_prods.360) # torch/nn/functional.py:1992:4
            block0(%i.91 : int, %size_prods.362 : int):
              %1406 : int = aten::add(%i.91, %26) # torch/nn/functional.py:1993:27
              %1407 : int = aten::__getitem__(%1399, %1406) # torch/nn/functional.py:1993:22
              %size_prods.363 : int = aten::mul(%size_prods.362, %1407) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.363)
          %1409 : bool = aten::eq(%size_prods.361, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1409) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1410 : Tensor = aten::batch_norm(%concated_features.43, %1397, %1398, %1395, %1396, %1394, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.85 : Tensor = aten::relu_(%1410) # torch/nn/functional.py:1117:17
      %1412 : Tensor = prim::GetAttr[name="weight"](%1387)
      %1413 : Tensor? = prim::GetAttr[name="bias"](%1387)
      %1414 : int[] = prim::ListConstruct(%27, %27)
      %1415 : int[] = prim::ListConstruct(%24, %24)
      %1416 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.85 : Tensor = aten::conv2d(%result.85, %1412, %1413, %1414, %1415, %1416, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.85)
  %1418 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%694)
  %1419 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%694)
  %1420 : int = aten::dim(%bottleneck_output.84) # torch/nn/modules/batchnorm.py:276:11
  %1421 : bool = aten::ne(%1420, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1421) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1422 : bool = prim::GetAttr[name="training"](%1419)
   = prim::If(%1422) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1423 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1419)
      %1424 : Tensor = aten::add(%1423, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1419, %1424)
      -> ()
    block1():
      -> ()
  %1425 : bool = prim::GetAttr[name="training"](%1419)
  %1426 : Tensor = prim::GetAttr[name="running_mean"](%1419)
  %1427 : Tensor = prim::GetAttr[name="running_var"](%1419)
  %1428 : Tensor = prim::GetAttr[name="weight"](%1419)
  %1429 : Tensor = prim::GetAttr[name="bias"](%1419)
   = prim::If(%1425) # torch/nn/functional.py:2011:4
    block0():
      %1430 : int[] = aten::size(%bottleneck_output.84) # torch/nn/functional.py:2012:27
      %size_prods.364 : int = aten::__getitem__(%1430, %24) # torch/nn/functional.py:1991:17
      %1432 : int = aten::len(%1430) # torch/nn/functional.py:1992:19
      %1433 : int = aten::sub(%1432, %26) # torch/nn/functional.py:1992:19
      %size_prods.365 : int = prim::Loop(%1433, %25, %size_prods.364) # torch/nn/functional.py:1992:4
        block0(%i.92 : int, %size_prods.366 : int):
          %1437 : int = aten::add(%i.92, %26) # torch/nn/functional.py:1993:27
          %1438 : int = aten::__getitem__(%1430, %1437) # torch/nn/functional.py:1993:22
          %size_prods.367 : int = aten::mul(%size_prods.366, %1438) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.367)
      %1440 : bool = aten::eq(%size_prods.365, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1440) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1441 : Tensor = aten::batch_norm(%bottleneck_output.84, %1428, %1429, %1426, %1427, %1425, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.86 : Tensor = aten::relu_(%1441) # torch/nn/functional.py:1117:17
  %1443 : Tensor = prim::GetAttr[name="weight"](%1418)
  %1444 : Tensor? = prim::GetAttr[name="bias"](%1418)
  %1445 : int[] = prim::ListConstruct(%27, %27)
  %1446 : int[] = prim::ListConstruct(%27, %27)
  %1447 : int[] = prim::ListConstruct(%27, %27)
  %new_features.88 : Tensor = aten::conv2d(%result.86, %1443, %1444, %1445, %1446, %1447, %27) # torch/nn/modules/conv.py:415:15
  %1449 : float = prim::GetAttr[name="drop_rate"](%694)
  %1450 : bool = aten::gt(%1449, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.85 : Tensor = prim::If(%1450) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1452 : float = prim::GetAttr[name="drop_rate"](%694)
      %1453 : bool = prim::GetAttr[name="training"](%694)
      %1454 : bool = aten::lt(%1452, %16) # torch/nn/functional.py:968:7
      %1455 : bool = prim::If(%1454) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1456 : bool = aten::gt(%1452, %17) # torch/nn/functional.py:968:17
          -> (%1456)
       = prim::If(%1455) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1457 : Tensor = aten::dropout(%new_features.88, %1452, %1453) # torch/nn/functional.py:973:17
      -> (%1457)
    block1():
      -> (%new_features.88)
  %1458 : Tensor[] = aten::append(%features.3, %new_features.85) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1459 : Tensor = prim::Uninitialized()
  %1460 : bool = prim::GetAttr[name="memory_efficient"](%695)
  %1461 : bool = prim::If(%1460) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1462 : bool = prim::Uninitialized()
      %1463 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1464 : bool = aten::gt(%1463, %24)
      %1465 : bool, %1466 : bool, %1467 : int = prim::Loop(%18, %1464, %19, %1462, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1468 : int, %1469 : bool, %1470 : bool, %1471 : int):
          %tensor.44 : Tensor = aten::__getitem__(%features.3, %1471) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1473 : bool = prim::requires_grad(%tensor.44)
          %1474 : bool, %1475 : bool = prim::If(%1473) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1462)
          %1476 : int = aten::add(%1471, %27)
          %1477 : bool = aten::lt(%1476, %1463)
          %1478 : bool = aten::__and__(%1477, %1474)
          -> (%1478, %1473, %1475, %1476)
      %1479 : bool = prim::If(%1465)
        block0():
          -> (%1466)
        block1():
          -> (%19)
      -> (%1479)
    block1():
      -> (%19)
  %bottleneck_output.86 : Tensor = prim::If(%1461) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1459)
    block1():
      %concated_features.44 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1482 : __torch__.torch.nn.modules.conv.___torch_mangle_191.Conv2d = prim::GetAttr[name="conv1"](%695)
      %1483 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%695)
      %1484 : int = aten::dim(%concated_features.44) # torch/nn/modules/batchnorm.py:276:11
      %1485 : bool = aten::ne(%1484, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1485) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1486 : bool = prim::GetAttr[name="training"](%1483)
       = prim::If(%1486) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1487 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1483)
          %1488 : Tensor = aten::add(%1487, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1483, %1488)
          -> ()
        block1():
          -> ()
      %1489 : bool = prim::GetAttr[name="training"](%1483)
      %1490 : Tensor = prim::GetAttr[name="running_mean"](%1483)
      %1491 : Tensor = prim::GetAttr[name="running_var"](%1483)
      %1492 : Tensor = prim::GetAttr[name="weight"](%1483)
      %1493 : Tensor = prim::GetAttr[name="bias"](%1483)
       = prim::If(%1489) # torch/nn/functional.py:2011:4
        block0():
          %1494 : int[] = aten::size(%concated_features.44) # torch/nn/functional.py:2012:27
          %size_prods.368 : int = aten::__getitem__(%1494, %24) # torch/nn/functional.py:1991:17
          %1496 : int = aten::len(%1494) # torch/nn/functional.py:1992:19
          %1497 : int = aten::sub(%1496, %26) # torch/nn/functional.py:1992:19
          %size_prods.369 : int = prim::Loop(%1497, %25, %size_prods.368) # torch/nn/functional.py:1992:4
            block0(%i.93 : int, %size_prods.370 : int):
              %1501 : int = aten::add(%i.93, %26) # torch/nn/functional.py:1993:27
              %1502 : int = aten::__getitem__(%1494, %1501) # torch/nn/functional.py:1993:22
              %size_prods.371 : int = aten::mul(%size_prods.370, %1502) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.371)
          %1504 : bool = aten::eq(%size_prods.369, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1504) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1505 : Tensor = aten::batch_norm(%concated_features.44, %1492, %1493, %1490, %1491, %1489, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.87 : Tensor = aten::relu_(%1505) # torch/nn/functional.py:1117:17
      %1507 : Tensor = prim::GetAttr[name="weight"](%1482)
      %1508 : Tensor? = prim::GetAttr[name="bias"](%1482)
      %1509 : int[] = prim::ListConstruct(%27, %27)
      %1510 : int[] = prim::ListConstruct(%24, %24)
      %1511 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.87 : Tensor = aten::conv2d(%result.87, %1507, %1508, %1509, %1510, %1511, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.87)
  %1513 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%695)
  %1514 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%695)
  %1515 : int = aten::dim(%bottleneck_output.86) # torch/nn/modules/batchnorm.py:276:11
  %1516 : bool = aten::ne(%1515, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1516) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1517 : bool = prim::GetAttr[name="training"](%1514)
   = prim::If(%1517) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1518 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1514)
      %1519 : Tensor = aten::add(%1518, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1514, %1519)
      -> ()
    block1():
      -> ()
  %1520 : bool = prim::GetAttr[name="training"](%1514)
  %1521 : Tensor = prim::GetAttr[name="running_mean"](%1514)
  %1522 : Tensor = prim::GetAttr[name="running_var"](%1514)
  %1523 : Tensor = prim::GetAttr[name="weight"](%1514)
  %1524 : Tensor = prim::GetAttr[name="bias"](%1514)
   = prim::If(%1520) # torch/nn/functional.py:2011:4
    block0():
      %1525 : int[] = aten::size(%bottleneck_output.86) # torch/nn/functional.py:2012:27
      %size_prods.372 : int = aten::__getitem__(%1525, %24) # torch/nn/functional.py:1991:17
      %1527 : int = aten::len(%1525) # torch/nn/functional.py:1992:19
      %1528 : int = aten::sub(%1527, %26) # torch/nn/functional.py:1992:19
      %size_prods.373 : int = prim::Loop(%1528, %25, %size_prods.372) # torch/nn/functional.py:1992:4
        block0(%i.94 : int, %size_prods.374 : int):
          %1532 : int = aten::add(%i.94, %26) # torch/nn/functional.py:1993:27
          %1533 : int = aten::__getitem__(%1525, %1532) # torch/nn/functional.py:1993:22
          %size_prods.375 : int = aten::mul(%size_prods.374, %1533) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.375)
      %1535 : bool = aten::eq(%size_prods.373, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1535) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1536 : Tensor = aten::batch_norm(%bottleneck_output.86, %1523, %1524, %1521, %1522, %1520, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.88 : Tensor = aten::relu_(%1536) # torch/nn/functional.py:1117:17
  %1538 : Tensor = prim::GetAttr[name="weight"](%1513)
  %1539 : Tensor? = prim::GetAttr[name="bias"](%1513)
  %1540 : int[] = prim::ListConstruct(%27, %27)
  %1541 : int[] = prim::ListConstruct(%27, %27)
  %1542 : int[] = prim::ListConstruct(%27, %27)
  %new_features.90 : Tensor = aten::conv2d(%result.88, %1538, %1539, %1540, %1541, %1542, %27) # torch/nn/modules/conv.py:415:15
  %1544 : float = prim::GetAttr[name="drop_rate"](%695)
  %1545 : bool = aten::gt(%1544, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.87 : Tensor = prim::If(%1545) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1547 : float = prim::GetAttr[name="drop_rate"](%695)
      %1548 : bool = prim::GetAttr[name="training"](%695)
      %1549 : bool = aten::lt(%1547, %16) # torch/nn/functional.py:968:7
      %1550 : bool = prim::If(%1549) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1551 : bool = aten::gt(%1547, %17) # torch/nn/functional.py:968:17
          -> (%1551)
       = prim::If(%1550) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1552 : Tensor = aten::dropout(%new_features.90, %1547, %1548) # torch/nn/functional.py:973:17
      -> (%1552)
    block1():
      -> (%new_features.90)
  %1553 : Tensor[] = aten::append(%features.3, %new_features.87) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1554 : Tensor = prim::Uninitialized()
  %1555 : bool = prim::GetAttr[name="memory_efficient"](%696)
  %1556 : bool = prim::If(%1555) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1557 : bool = prim::Uninitialized()
      %1558 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1559 : bool = aten::gt(%1558, %24)
      %1560 : bool, %1561 : bool, %1562 : int = prim::Loop(%18, %1559, %19, %1557, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1563 : int, %1564 : bool, %1565 : bool, %1566 : int):
          %tensor.45 : Tensor = aten::__getitem__(%features.3, %1566) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1568 : bool = prim::requires_grad(%tensor.45)
          %1569 : bool, %1570 : bool = prim::If(%1568) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1557)
          %1571 : int = aten::add(%1566, %27)
          %1572 : bool = aten::lt(%1571, %1558)
          %1573 : bool = aten::__and__(%1572, %1569)
          -> (%1573, %1568, %1570, %1571)
      %1574 : bool = prim::If(%1560)
        block0():
          -> (%1561)
        block1():
          -> (%19)
      -> (%1574)
    block1():
      -> (%19)
  %bottleneck_output.88 : Tensor = prim::If(%1556) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1554)
    block1():
      %concated_features.45 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1577 : __torch__.torch.nn.modules.conv.___torch_mangle_194.Conv2d = prim::GetAttr[name="conv1"](%696)
      %1578 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_193.BatchNorm2d = prim::GetAttr[name="norm1"](%696)
      %1579 : int = aten::dim(%concated_features.45) # torch/nn/modules/batchnorm.py:276:11
      %1580 : bool = aten::ne(%1579, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1580) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1581 : bool = prim::GetAttr[name="training"](%1578)
       = prim::If(%1581) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1582 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1578)
          %1583 : Tensor = aten::add(%1582, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1578, %1583)
          -> ()
        block1():
          -> ()
      %1584 : bool = prim::GetAttr[name="training"](%1578)
      %1585 : Tensor = prim::GetAttr[name="running_mean"](%1578)
      %1586 : Tensor = prim::GetAttr[name="running_var"](%1578)
      %1587 : Tensor = prim::GetAttr[name="weight"](%1578)
      %1588 : Tensor = prim::GetAttr[name="bias"](%1578)
       = prim::If(%1584) # torch/nn/functional.py:2011:4
        block0():
          %1589 : int[] = aten::size(%concated_features.45) # torch/nn/functional.py:2012:27
          %size_prods.376 : int = aten::__getitem__(%1589, %24) # torch/nn/functional.py:1991:17
          %1591 : int = aten::len(%1589) # torch/nn/functional.py:1992:19
          %1592 : int = aten::sub(%1591, %26) # torch/nn/functional.py:1992:19
          %size_prods.377 : int = prim::Loop(%1592, %25, %size_prods.376) # torch/nn/functional.py:1992:4
            block0(%i.95 : int, %size_prods.378 : int):
              %1596 : int = aten::add(%i.95, %26) # torch/nn/functional.py:1993:27
              %1597 : int = aten::__getitem__(%1589, %1596) # torch/nn/functional.py:1993:22
              %size_prods.379 : int = aten::mul(%size_prods.378, %1597) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.379)
          %1599 : bool = aten::eq(%size_prods.377, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1599) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1600 : Tensor = aten::batch_norm(%concated_features.45, %1587, %1588, %1585, %1586, %1584, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.89 : Tensor = aten::relu_(%1600) # torch/nn/functional.py:1117:17
      %1602 : Tensor = prim::GetAttr[name="weight"](%1577)
      %1603 : Tensor? = prim::GetAttr[name="bias"](%1577)
      %1604 : int[] = prim::ListConstruct(%27, %27)
      %1605 : int[] = prim::ListConstruct(%24, %24)
      %1606 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.89 : Tensor = aten::conv2d(%result.89, %1602, %1603, %1604, %1605, %1606, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.89)
  %1608 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%696)
  %1609 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%696)
  %1610 : int = aten::dim(%bottleneck_output.88) # torch/nn/modules/batchnorm.py:276:11
  %1611 : bool = aten::ne(%1610, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1611) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1612 : bool = prim::GetAttr[name="training"](%1609)
   = prim::If(%1612) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1613 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1609)
      %1614 : Tensor = aten::add(%1613, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1609, %1614)
      -> ()
    block1():
      -> ()
  %1615 : bool = prim::GetAttr[name="training"](%1609)
  %1616 : Tensor = prim::GetAttr[name="running_mean"](%1609)
  %1617 : Tensor = prim::GetAttr[name="running_var"](%1609)
  %1618 : Tensor = prim::GetAttr[name="weight"](%1609)
  %1619 : Tensor = prim::GetAttr[name="bias"](%1609)
   = prim::If(%1615) # torch/nn/functional.py:2011:4
    block0():
      %1620 : int[] = aten::size(%bottleneck_output.88) # torch/nn/functional.py:2012:27
      %size_prods.380 : int = aten::__getitem__(%1620, %24) # torch/nn/functional.py:1991:17
      %1622 : int = aten::len(%1620) # torch/nn/functional.py:1992:19
      %1623 : int = aten::sub(%1622, %26) # torch/nn/functional.py:1992:19
      %size_prods.381 : int = prim::Loop(%1623, %25, %size_prods.380) # torch/nn/functional.py:1992:4
        block0(%i.96 : int, %size_prods.382 : int):
          %1627 : int = aten::add(%i.96, %26) # torch/nn/functional.py:1993:27
          %1628 : int = aten::__getitem__(%1620, %1627) # torch/nn/functional.py:1993:22
          %size_prods.383 : int = aten::mul(%size_prods.382, %1628) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.383)
      %1630 : bool = aten::eq(%size_prods.381, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1630) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1631 : Tensor = aten::batch_norm(%bottleneck_output.88, %1618, %1619, %1616, %1617, %1615, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.90 : Tensor = aten::relu_(%1631) # torch/nn/functional.py:1117:17
  %1633 : Tensor = prim::GetAttr[name="weight"](%1608)
  %1634 : Tensor? = prim::GetAttr[name="bias"](%1608)
  %1635 : int[] = prim::ListConstruct(%27, %27)
  %1636 : int[] = prim::ListConstruct(%27, %27)
  %1637 : int[] = prim::ListConstruct(%27, %27)
  %new_features.92 : Tensor = aten::conv2d(%result.90, %1633, %1634, %1635, %1636, %1637, %27) # torch/nn/modules/conv.py:415:15
  %1639 : float = prim::GetAttr[name="drop_rate"](%696)
  %1640 : bool = aten::gt(%1639, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.89 : Tensor = prim::If(%1640) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1642 : float = prim::GetAttr[name="drop_rate"](%696)
      %1643 : bool = prim::GetAttr[name="training"](%696)
      %1644 : bool = aten::lt(%1642, %16) # torch/nn/functional.py:968:7
      %1645 : bool = prim::If(%1644) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1646 : bool = aten::gt(%1642, %17) # torch/nn/functional.py:968:17
          -> (%1646)
       = prim::If(%1645) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1647 : Tensor = aten::dropout(%new_features.92, %1642, %1643) # torch/nn/functional.py:973:17
      -> (%1647)
    block1():
      -> (%new_features.92)
  %1648 : Tensor[] = aten::append(%features.3, %new_features.89) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1649 : Tensor = prim::Uninitialized()
  %1650 : bool = prim::GetAttr[name="memory_efficient"](%697)
  %1651 : bool = prim::If(%1650) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1652 : bool = prim::Uninitialized()
      %1653 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1654 : bool = aten::gt(%1653, %24)
      %1655 : bool, %1656 : bool, %1657 : int = prim::Loop(%18, %1654, %19, %1652, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1658 : int, %1659 : bool, %1660 : bool, %1661 : int):
          %tensor.46 : Tensor = aten::__getitem__(%features.3, %1661) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1663 : bool = prim::requires_grad(%tensor.46)
          %1664 : bool, %1665 : bool = prim::If(%1663) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1652)
          %1666 : int = aten::add(%1661, %27)
          %1667 : bool = aten::lt(%1666, %1653)
          %1668 : bool = aten::__and__(%1667, %1664)
          -> (%1668, %1663, %1665, %1666)
      %1669 : bool = prim::If(%1655)
        block0():
          -> (%1656)
        block1():
          -> (%19)
      -> (%1669)
    block1():
      -> (%19)
  %bottleneck_output.90 : Tensor = prim::If(%1651) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1649)
    block1():
      %concated_features.46 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1672 : __torch__.torch.nn.modules.conv.___torch_mangle_196.Conv2d = prim::GetAttr[name="conv1"](%697)
      %1673 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%697)
      %1674 : int = aten::dim(%concated_features.46) # torch/nn/modules/batchnorm.py:276:11
      %1675 : bool = aten::ne(%1674, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1675) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1676 : bool = prim::GetAttr[name="training"](%1673)
       = prim::If(%1676) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1673)
          %1678 : Tensor = aten::add(%1677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1673, %1678)
          -> ()
        block1():
          -> ()
      %1679 : bool = prim::GetAttr[name="training"](%1673)
      %1680 : Tensor = prim::GetAttr[name="running_mean"](%1673)
      %1681 : Tensor = prim::GetAttr[name="running_var"](%1673)
      %1682 : Tensor = prim::GetAttr[name="weight"](%1673)
      %1683 : Tensor = prim::GetAttr[name="bias"](%1673)
       = prim::If(%1679) # torch/nn/functional.py:2011:4
        block0():
          %1684 : int[] = aten::size(%concated_features.46) # torch/nn/functional.py:2012:27
          %size_prods.384 : int = aten::__getitem__(%1684, %24) # torch/nn/functional.py:1991:17
          %1686 : int = aten::len(%1684) # torch/nn/functional.py:1992:19
          %1687 : int = aten::sub(%1686, %26) # torch/nn/functional.py:1992:19
          %size_prods.385 : int = prim::Loop(%1687, %25, %size_prods.384) # torch/nn/functional.py:1992:4
            block0(%i.97 : int, %size_prods.386 : int):
              %1691 : int = aten::add(%i.97, %26) # torch/nn/functional.py:1993:27
              %1692 : int = aten::__getitem__(%1684, %1691) # torch/nn/functional.py:1993:22
              %size_prods.387 : int = aten::mul(%size_prods.386, %1692) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.387)
          %1694 : bool = aten::eq(%size_prods.385, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1694) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1695 : Tensor = aten::batch_norm(%concated_features.46, %1682, %1683, %1680, %1681, %1679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.91 : Tensor = aten::relu_(%1695) # torch/nn/functional.py:1117:17
      %1697 : Tensor = prim::GetAttr[name="weight"](%1672)
      %1698 : Tensor? = prim::GetAttr[name="bias"](%1672)
      %1699 : int[] = prim::ListConstruct(%27, %27)
      %1700 : int[] = prim::ListConstruct(%24, %24)
      %1701 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.91 : Tensor = aten::conv2d(%result.91, %1697, %1698, %1699, %1700, %1701, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.91)
  %1703 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%697)
  %1704 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%697)
  %1705 : int = aten::dim(%bottleneck_output.90) # torch/nn/modules/batchnorm.py:276:11
  %1706 : bool = aten::ne(%1705, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1706) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1707 : bool = prim::GetAttr[name="training"](%1704)
   = prim::If(%1707) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1708 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1704)
      %1709 : Tensor = aten::add(%1708, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1704, %1709)
      -> ()
    block1():
      -> ()
  %1710 : bool = prim::GetAttr[name="training"](%1704)
  %1711 : Tensor = prim::GetAttr[name="running_mean"](%1704)
  %1712 : Tensor = prim::GetAttr[name="running_var"](%1704)
  %1713 : Tensor = prim::GetAttr[name="weight"](%1704)
  %1714 : Tensor = prim::GetAttr[name="bias"](%1704)
   = prim::If(%1710) # torch/nn/functional.py:2011:4
    block0():
      %1715 : int[] = aten::size(%bottleneck_output.90) # torch/nn/functional.py:2012:27
      %size_prods.388 : int = aten::__getitem__(%1715, %24) # torch/nn/functional.py:1991:17
      %1717 : int = aten::len(%1715) # torch/nn/functional.py:1992:19
      %1718 : int = aten::sub(%1717, %26) # torch/nn/functional.py:1992:19
      %size_prods.389 : int = prim::Loop(%1718, %25, %size_prods.388) # torch/nn/functional.py:1992:4
        block0(%i.98 : int, %size_prods.390 : int):
          %1722 : int = aten::add(%i.98, %26) # torch/nn/functional.py:1993:27
          %1723 : int = aten::__getitem__(%1715, %1722) # torch/nn/functional.py:1993:22
          %size_prods.391 : int = aten::mul(%size_prods.390, %1723) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.391)
      %1725 : bool = aten::eq(%size_prods.389, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1725) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1726 : Tensor = aten::batch_norm(%bottleneck_output.90, %1713, %1714, %1711, %1712, %1710, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.92 : Tensor = aten::relu_(%1726) # torch/nn/functional.py:1117:17
  %1728 : Tensor = prim::GetAttr[name="weight"](%1703)
  %1729 : Tensor? = prim::GetAttr[name="bias"](%1703)
  %1730 : int[] = prim::ListConstruct(%27, %27)
  %1731 : int[] = prim::ListConstruct(%27, %27)
  %1732 : int[] = prim::ListConstruct(%27, %27)
  %new_features.94 : Tensor = aten::conv2d(%result.92, %1728, %1729, %1730, %1731, %1732, %27) # torch/nn/modules/conv.py:415:15
  %1734 : float = prim::GetAttr[name="drop_rate"](%697)
  %1735 : bool = aten::gt(%1734, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.91 : Tensor = prim::If(%1735) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1737 : float = prim::GetAttr[name="drop_rate"](%697)
      %1738 : bool = prim::GetAttr[name="training"](%697)
      %1739 : bool = aten::lt(%1737, %16) # torch/nn/functional.py:968:7
      %1740 : bool = prim::If(%1739) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1741 : bool = aten::gt(%1737, %17) # torch/nn/functional.py:968:17
          -> (%1741)
       = prim::If(%1740) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1742 : Tensor = aten::dropout(%new_features.94, %1737, %1738) # torch/nn/functional.py:973:17
      -> (%1742)
    block1():
      -> (%new_features.94)
  %1743 : Tensor[] = aten::append(%features.3, %new_features.91) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %1744 : Tensor = prim::Uninitialized()
  %1745 : bool = prim::GetAttr[name="memory_efficient"](%698)
  %1746 : bool = prim::If(%1745) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1747 : bool = prim::Uninitialized()
      %1748 : int = aten::len(%features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1749 : bool = aten::gt(%1748, %24)
      %1750 : bool, %1751 : bool, %1752 : int = prim::Loop(%18, %1749, %19, %1747, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1753 : int, %1754 : bool, %1755 : bool, %1756 : int):
          %tensor.54 : Tensor = aten::__getitem__(%features.3, %1756) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1758 : bool = prim::requires_grad(%tensor.54)
          %1759 : bool, %1760 : bool = prim::If(%1758) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1747)
          %1761 : int = aten::add(%1756, %27)
          %1762 : bool = aten::lt(%1761, %1748)
          %1763 : bool = aten::__and__(%1762, %1759)
          -> (%1763, %1758, %1760, %1761)
      %1764 : bool = prim::If(%1750)
        block0():
          -> (%1751)
        block1():
          -> (%19)
      -> (%1764)
    block1():
      -> (%19)
  %bottleneck_output.106 : Tensor = prim::If(%1746) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1744)
    block1():
      %concated_features.54 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1767 : __torch__.torch.nn.modules.conv.___torch_mangle_199.Conv2d = prim::GetAttr[name="conv1"](%698)
      %1768 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_198.BatchNorm2d = prim::GetAttr[name="norm1"](%698)
      %1769 : int = aten::dim(%concated_features.54) # torch/nn/modules/batchnorm.py:276:11
      %1770 : bool = aten::ne(%1769, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1770) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1771 : bool = prim::GetAttr[name="training"](%1768)
       = prim::If(%1771) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1772 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1768)
          %1773 : Tensor = aten::add(%1772, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1768, %1773)
          -> ()
        block1():
          -> ()
      %1774 : bool = prim::GetAttr[name="training"](%1768)
      %1775 : Tensor = prim::GetAttr[name="running_mean"](%1768)
      %1776 : Tensor = prim::GetAttr[name="running_var"](%1768)
      %1777 : Tensor = prim::GetAttr[name="weight"](%1768)
      %1778 : Tensor = prim::GetAttr[name="bias"](%1768)
       = prim::If(%1774) # torch/nn/functional.py:2011:4
        block0():
          %1779 : int[] = aten::size(%concated_features.54) # torch/nn/functional.py:2012:27
          %size_prods.436 : int = aten::__getitem__(%1779, %24) # torch/nn/functional.py:1991:17
          %1781 : int = aten::len(%1779) # torch/nn/functional.py:1992:19
          %1782 : int = aten::sub(%1781, %26) # torch/nn/functional.py:1992:19
          %size_prods.437 : int = prim::Loop(%1782, %25, %size_prods.436) # torch/nn/functional.py:1992:4
            block0(%i.110 : int, %size_prods.438 : int):
              %1786 : int = aten::add(%i.110, %26) # torch/nn/functional.py:1993:27
              %1787 : int = aten::__getitem__(%1779, %1786) # torch/nn/functional.py:1993:22
              %size_prods.439 : int = aten::mul(%size_prods.438, %1787) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.439)
          %1789 : bool = aten::eq(%size_prods.437, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1789) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1790 : Tensor = aten::batch_norm(%concated_features.54, %1777, %1778, %1775, %1776, %1774, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.107 : Tensor = aten::relu_(%1790) # torch/nn/functional.py:1117:17
      %1792 : Tensor = prim::GetAttr[name="weight"](%1767)
      %1793 : Tensor? = prim::GetAttr[name="bias"](%1767)
      %1794 : int[] = prim::ListConstruct(%27, %27)
      %1795 : int[] = prim::ListConstruct(%24, %24)
      %1796 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.107 : Tensor = aten::conv2d(%result.107, %1792, %1793, %1794, %1795, %1796, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.107)
  %1798 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%698)
  %1799 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%698)
  %1800 : int = aten::dim(%bottleneck_output.106) # torch/nn/modules/batchnorm.py:276:11
  %1801 : bool = aten::ne(%1800, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1801) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1802 : bool = prim::GetAttr[name="training"](%1799)
   = prim::If(%1802) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1803 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1799)
      %1804 : Tensor = aten::add(%1803, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1799, %1804)
      -> ()
    block1():
      -> ()
  %1805 : bool = prim::GetAttr[name="training"](%1799)
  %1806 : Tensor = prim::GetAttr[name="running_mean"](%1799)
  %1807 : Tensor = prim::GetAttr[name="running_var"](%1799)
  %1808 : Tensor = prim::GetAttr[name="weight"](%1799)
  %1809 : Tensor = prim::GetAttr[name="bias"](%1799)
   = prim::If(%1805) # torch/nn/functional.py:2011:4
    block0():
      %1810 : int[] = aten::size(%bottleneck_output.106) # torch/nn/functional.py:2012:27
      %size_prods.300 : int = aten::__getitem__(%1810, %24) # torch/nn/functional.py:1991:17
      %1812 : int = aten::len(%1810) # torch/nn/functional.py:1992:19
      %1813 : int = aten::sub(%1812, %26) # torch/nn/functional.py:1992:19
      %size_prods.301 : int = prim::Loop(%1813, %25, %size_prods.300) # torch/nn/functional.py:1992:4
        block0(%i.76 : int, %size_prods.302 : int):
          %1817 : int = aten::add(%i.76, %26) # torch/nn/functional.py:1993:27
          %1818 : int = aten::__getitem__(%1810, %1817) # torch/nn/functional.py:1993:22
          %size_prods.303 : int = aten::mul(%size_prods.302, %1818) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.303)
      %1820 : bool = aten::eq(%size_prods.301, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1820) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1821 : Tensor = aten::batch_norm(%bottleneck_output.106, %1808, %1809, %1806, %1807, %1805, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.108 : Tensor = aten::relu_(%1821) # torch/nn/functional.py:1117:17
  %1823 : Tensor = prim::GetAttr[name="weight"](%1798)
  %1824 : Tensor? = prim::GetAttr[name="bias"](%1798)
  %1825 : int[] = prim::ListConstruct(%27, %27)
  %1826 : int[] = prim::ListConstruct(%27, %27)
  %1827 : int[] = prim::ListConstruct(%27, %27)
  %new_features.107 : Tensor = aten::conv2d(%result.108, %1823, %1824, %1825, %1826, %1827, %27) # torch/nn/modules/conv.py:415:15
  %1829 : float = prim::GetAttr[name="drop_rate"](%698)
  %1830 : bool = aten::gt(%1829, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.93 : Tensor = prim::If(%1830) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %1832 : float = prim::GetAttr[name="drop_rate"](%698)
      %1833 : bool = prim::GetAttr[name="training"](%698)
      %1834 : bool = aten::lt(%1832, %16) # torch/nn/functional.py:968:7
      %1835 : bool = prim::If(%1834) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %1836 : bool = aten::gt(%1832, %17) # torch/nn/functional.py:968:17
          -> (%1836)
       = prim::If(%1835) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %1837 : Tensor = aten::dropout(%new_features.107, %1832, %1833) # torch/nn/functional.py:973:17
      -> (%1837)
    block1():
      -> (%new_features.107)
  %1838 : Tensor[] = aten::append(%features.3, %new_features.93) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.15 : Tensor = aten::cat(%features.3, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %1840 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm"](%34)
  %1841 : __torch__.torch.nn.modules.conv.___torch_mangle_202.Conv2d = prim::GetAttr[name="conv"](%34)
  %1842 : int = aten::dim(%input.15) # torch/nn/modules/batchnorm.py:276:11
  %1843 : bool = aten::ne(%1842, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1843) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1844 : bool = prim::GetAttr[name="training"](%1840)
   = prim::If(%1844) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1845 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1840)
      %1846 : Tensor = aten::add(%1845, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1840, %1846)
      -> ()
    block1():
      -> ()
  %1847 : bool = prim::GetAttr[name="training"](%1840)
  %1848 : Tensor = prim::GetAttr[name="running_mean"](%1840)
  %1849 : Tensor = prim::GetAttr[name="running_var"](%1840)
  %1850 : Tensor = prim::GetAttr[name="weight"](%1840)
  %1851 : Tensor = prim::GetAttr[name="bias"](%1840)
   = prim::If(%1847) # torch/nn/functional.py:2011:4
    block0():
      %1852 : int[] = aten::size(%input.15) # torch/nn/functional.py:2012:27
      %size_prods.440 : int = aten::__getitem__(%1852, %24) # torch/nn/functional.py:1991:17
      %1854 : int = aten::len(%1852) # torch/nn/functional.py:1992:19
      %1855 : int = aten::sub(%1854, %26) # torch/nn/functional.py:1992:19
      %size_prods.441 : int = prim::Loop(%1855, %25, %size_prods.440) # torch/nn/functional.py:1992:4
        block0(%i.111 : int, %size_prods.442 : int):
          %1859 : int = aten::add(%i.111, %26) # torch/nn/functional.py:1993:27
          %1860 : int = aten::__getitem__(%1852, %1859) # torch/nn/functional.py:1993:22
          %size_prods.443 : int = aten::mul(%size_prods.442, %1860) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.443)
      %1862 : bool = aten::eq(%size_prods.441, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1862) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.22 : Tensor = aten::batch_norm(%input.15, %1850, %1851, %1848, %1849, %1847, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.18 : Tensor = aten::relu_(%input.22) # torch/nn/functional.py:1117:17
  %1865 : Tensor = prim::GetAttr[name="weight"](%1841)
  %1866 : Tensor? = prim::GetAttr[name="bias"](%1841)
  %1867 : int[] = prim::ListConstruct(%27, %27)
  %1868 : int[] = prim::ListConstruct(%24, %24)
  %1869 : int[] = prim::ListConstruct(%27, %27)
  %input.20 : Tensor = aten::conv2d(%input.18, %1865, %1866, %1867, %1868, %1869, %27) # torch/nn/modules/conv.py:415:15
  %1871 : int[] = prim::ListConstruct(%26, %26)
  %1872 : int[] = prim::ListConstruct(%26, %26)
  %1873 : int[] = prim::ListConstruct(%24, %24)
  %input.17 : Tensor = aten::avg_pool2d(%input.20, %1871, %1872, %1873, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.4 : Tensor[] = prim::ListConstruct(%input.17)
  %1876 : __torch__.torchvision.models.densenet.___torch_mangle_182._DenseLayer = prim::GetAttr[name="denselayer1"](%35)
  %1877 : __torch__.torchvision.models.densenet.___torch_mangle_185._DenseLayer = prim::GetAttr[name="denselayer2"](%35)
  %1878 : __torch__.torchvision.models.densenet.___torch_mangle_187._DenseLayer = prim::GetAttr[name="denselayer3"](%35)
  %1879 : __torch__.torchvision.models.densenet.___torch_mangle_190._DenseLayer = prim::GetAttr[name="denselayer4"](%35)
  %1880 : __torch__.torchvision.models.densenet.___torch_mangle_192._DenseLayer = prim::GetAttr[name="denselayer5"](%35)
  %1881 : __torch__.torchvision.models.densenet.___torch_mangle_195._DenseLayer = prim::GetAttr[name="denselayer6"](%35)
  %1882 : __torch__.torchvision.models.densenet.___torch_mangle_197._DenseLayer = prim::GetAttr[name="denselayer7"](%35)
  %1883 : __torch__.torchvision.models.densenet.___torch_mangle_200._DenseLayer = prim::GetAttr[name="denselayer8"](%35)
  %1884 : __torch__.torchvision.models.densenet.___torch_mangle_205._DenseLayer = prim::GetAttr[name="denselayer9"](%35)
  %1885 : __torch__.torchvision.models.densenet.___torch_mangle_208._DenseLayer = prim::GetAttr[name="denselayer10"](%35)
  %1886 : __torch__.torchvision.models.densenet.___torch_mangle_210._DenseLayer = prim::GetAttr[name="denselayer11"](%35)
  %1887 : __torch__.torchvision.models.densenet.___torch_mangle_213._DenseLayer = prim::GetAttr[name="denselayer12"](%35)
  %1888 : __torch__.torchvision.models.densenet.___torch_mangle_215._DenseLayer = prim::GetAttr[name="denselayer13"](%35)
  %1889 : __torch__.torchvision.models.densenet.___torch_mangle_218._DenseLayer = prim::GetAttr[name="denselayer14"](%35)
  %1890 : __torch__.torchvision.models.densenet.___torch_mangle_221._DenseLayer = prim::GetAttr[name="denselayer15"](%35)
  %1891 : __torch__.torchvision.models.densenet.___torch_mangle_224._DenseLayer = prim::GetAttr[name="denselayer16"](%35)
  %1892 : __torch__.torchvision.models.densenet.___torch_mangle_227._DenseLayer = prim::GetAttr[name="denselayer17"](%35)
  %1893 : __torch__.torchvision.models.densenet.___torch_mangle_230._DenseLayer = prim::GetAttr[name="denselayer18"](%35)
  %1894 : __torch__.torchvision.models.densenet.___torch_mangle_233._DenseLayer = prim::GetAttr[name="denselayer19"](%35)
  %1895 : __torch__.torchvision.models.densenet.___torch_mangle_236._DenseLayer = prim::GetAttr[name="denselayer20"](%35)
  %1896 : __torch__.torchvision.models.densenet.___torch_mangle_239._DenseLayer = prim::GetAttr[name="denselayer21"](%35)
  %1897 : __torch__.torchvision.models.densenet.___torch_mangle_242._DenseLayer = prim::GetAttr[name="denselayer22"](%35)
  %1898 : __torch__.torchvision.models.densenet.___torch_mangle_245._DenseLayer = prim::GetAttr[name="denselayer23"](%35)
  %1899 : __torch__.torchvision.models.densenet.___torch_mangle_248._DenseLayer = prim::GetAttr[name="denselayer24"](%35)
  %1900 : __torch__.torchvision.models.densenet.___torch_mangle_251._DenseLayer = prim::GetAttr[name="denselayer25"](%35)
  %1901 : __torch__.torchvision.models.densenet.___torch_mangle_254._DenseLayer = prim::GetAttr[name="denselayer26"](%35)
  %1902 : __torch__.torchvision.models.densenet.___torch_mangle_257._DenseLayer = prim::GetAttr[name="denselayer27"](%35)
  %1903 : __torch__.torchvision.models.densenet.___torch_mangle_260._DenseLayer = prim::GetAttr[name="denselayer28"](%35)
  %1904 : __torch__.torchvision.models.densenet.___torch_mangle_263._DenseLayer = prim::GetAttr[name="denselayer29"](%35)
  %1905 : __torch__.torchvision.models.densenet.___torch_mangle_266._DenseLayer = prim::GetAttr[name="denselayer30"](%35)
  %1906 : __torch__.torchvision.models.densenet.___torch_mangle_269._DenseLayer = prim::GetAttr[name="denselayer31"](%35)
  %1907 : __torch__.torchvision.models.densenet.___torch_mangle_272._DenseLayer = prim::GetAttr[name="denselayer32"](%35)
  %1908 : __torch__.torchvision.models.densenet.___torch_mangle_275._DenseLayer = prim::GetAttr[name="denselayer33"](%35)
  %1909 : __torch__.torchvision.models.densenet.___torch_mangle_278._DenseLayer = prim::GetAttr[name="denselayer34"](%35)
  %1910 : __torch__.torchvision.models.densenet.___torch_mangle_281._DenseLayer = prim::GetAttr[name="denselayer35"](%35)
  %1911 : __torch__.torchvision.models.densenet.___torch_mangle_284._DenseLayer = prim::GetAttr[name="denselayer36"](%35)
  %1912 : Tensor = prim::Uninitialized()
  %1913 : bool = prim::GetAttr[name="memory_efficient"](%1876)
  %1914 : bool = prim::If(%1913) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %1915 : bool = prim::Uninitialized()
      %1916 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %1917 : bool = aten::gt(%1916, %24)
      %1918 : bool, %1919 : bool, %1920 : int = prim::Loop(%18, %1917, %19, %1915, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%1921 : int, %1922 : bool, %1923 : bool, %1924 : int):
          %tensor.55 : Tensor = aten::__getitem__(%features.4, %1924) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %1926 : bool = prim::requires_grad(%tensor.55)
          %1927 : bool, %1928 : bool = prim::If(%1926) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %1915)
          %1929 : int = aten::add(%1924, %27)
          %1930 : bool = aten::lt(%1929, %1916)
          %1931 : bool = aten::__and__(%1930, %1927)
          -> (%1931, %1926, %1928, %1929)
      %1932 : bool = prim::If(%1918)
        block0():
          -> (%1919)
        block1():
          -> (%19)
      -> (%1932)
    block1():
      -> (%19)
  %bottleneck_output.108 : Tensor = prim::If(%1914) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%1912)
    block1():
      %concated_features.55 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %1935 : __torch__.torch.nn.modules.conv.___torch_mangle_180.Conv2d = prim::GetAttr[name="conv1"](%1876)
      %1936 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_97.BatchNorm2d = prim::GetAttr[name="norm1"](%1876)
      %1937 : int = aten::dim(%concated_features.55) # torch/nn/modules/batchnorm.py:276:11
      %1938 : bool = aten::ne(%1937, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%1938) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %1939 : bool = prim::GetAttr[name="training"](%1936)
       = prim::If(%1939) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %1940 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1936)
          %1941 : Tensor = aten::add(%1940, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%1936, %1941)
          -> ()
        block1():
          -> ()
      %1942 : bool = prim::GetAttr[name="training"](%1936)
      %1943 : Tensor = prim::GetAttr[name="running_mean"](%1936)
      %1944 : Tensor = prim::GetAttr[name="running_var"](%1936)
      %1945 : Tensor = prim::GetAttr[name="weight"](%1936)
      %1946 : Tensor = prim::GetAttr[name="bias"](%1936)
       = prim::If(%1942) # torch/nn/functional.py:2011:4
        block0():
          %1947 : int[] = aten::size(%concated_features.55) # torch/nn/functional.py:2012:27
          %size_prods.448 : int = aten::__getitem__(%1947, %24) # torch/nn/functional.py:1991:17
          %1949 : int = aten::len(%1947) # torch/nn/functional.py:1992:19
          %1950 : int = aten::sub(%1949, %26) # torch/nn/functional.py:1992:19
          %size_prods.449 : int = prim::Loop(%1950, %25, %size_prods.448) # torch/nn/functional.py:1992:4
            block0(%i.113 : int, %size_prods.450 : int):
              %1954 : int = aten::add(%i.113, %26) # torch/nn/functional.py:1993:27
              %1955 : int = aten::__getitem__(%1947, %1954) # torch/nn/functional.py:1993:22
              %size_prods.451 : int = aten::mul(%size_prods.450, %1955) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.451)
          %1957 : bool = aten::eq(%size_prods.449, %27) # torch/nn/functional.py:1994:7
           = prim::If(%1957) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %1958 : Tensor = aten::batch_norm(%concated_features.55, %1945, %1946, %1943, %1944, %1942, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.109 : Tensor = aten::relu_(%1958) # torch/nn/functional.py:1117:17
      %1960 : Tensor = prim::GetAttr[name="weight"](%1935)
      %1961 : Tensor? = prim::GetAttr[name="bias"](%1935)
      %1962 : int[] = prim::ListConstruct(%27, %27)
      %1963 : int[] = prim::ListConstruct(%24, %24)
      %1964 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.109 : Tensor = aten::conv2d(%result.109, %1960, %1961, %1962, %1963, %1964, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.109)
  %1966 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1876)
  %1967 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1876)
  %1968 : int = aten::dim(%bottleneck_output.108) # torch/nn/modules/batchnorm.py:276:11
  %1969 : bool = aten::ne(%1968, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%1969) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %1970 : bool = prim::GetAttr[name="training"](%1967)
   = prim::If(%1970) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %1971 : Tensor = prim::GetAttr[name="num_batches_tracked"](%1967)
      %1972 : Tensor = aten::add(%1971, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%1967, %1972)
      -> ()
    block1():
      -> ()
  %1973 : bool = prim::GetAttr[name="training"](%1967)
  %1974 : Tensor = prim::GetAttr[name="running_mean"](%1967)
  %1975 : Tensor = prim::GetAttr[name="running_var"](%1967)
  %1976 : Tensor = prim::GetAttr[name="weight"](%1967)
  %1977 : Tensor = prim::GetAttr[name="bias"](%1967)
   = prim::If(%1973) # torch/nn/functional.py:2011:4
    block0():
      %1978 : int[] = aten::size(%bottleneck_output.108) # torch/nn/functional.py:2012:27
      %size_prods.452 : int = aten::__getitem__(%1978, %24) # torch/nn/functional.py:1991:17
      %1980 : int = aten::len(%1978) # torch/nn/functional.py:1992:19
      %1981 : int = aten::sub(%1980, %26) # torch/nn/functional.py:1992:19
      %size_prods.453 : int = prim::Loop(%1981, %25, %size_prods.452) # torch/nn/functional.py:1992:4
        block0(%i.114 : int, %size_prods.454 : int):
          %1985 : int = aten::add(%i.114, %26) # torch/nn/functional.py:1993:27
          %1986 : int = aten::__getitem__(%1978, %1985) # torch/nn/functional.py:1993:22
          %size_prods.455 : int = aten::mul(%size_prods.454, %1986) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.455)
      %1988 : bool = aten::eq(%size_prods.453, %27) # torch/nn/functional.py:1994:7
       = prim::If(%1988) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %1989 : Tensor = aten::batch_norm(%bottleneck_output.108, %1976, %1977, %1974, %1975, %1973, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.110 : Tensor = aten::relu_(%1989) # torch/nn/functional.py:1117:17
  %1991 : Tensor = prim::GetAttr[name="weight"](%1966)
  %1992 : Tensor? = prim::GetAttr[name="bias"](%1966)
  %1993 : int[] = prim::ListConstruct(%27, %27)
  %1994 : int[] = prim::ListConstruct(%27, %27)
  %1995 : int[] = prim::ListConstruct(%27, %27)
  %new_features.109 : Tensor = aten::conv2d(%result.110, %1991, %1992, %1993, %1994, %1995, %27) # torch/nn/modules/conv.py:415:15
  %1997 : float = prim::GetAttr[name="drop_rate"](%1876)
  %1998 : bool = aten::gt(%1997, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.110 : Tensor = prim::If(%1998) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2000 : float = prim::GetAttr[name="drop_rate"](%1876)
      %2001 : bool = prim::GetAttr[name="training"](%1876)
      %2002 : bool = aten::lt(%2000, %16) # torch/nn/functional.py:968:7
      %2003 : bool = prim::If(%2002) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2004 : bool = aten::gt(%2000, %17) # torch/nn/functional.py:968:17
          -> (%2004)
       = prim::If(%2003) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2005 : Tensor = aten::dropout(%new_features.109, %2000, %2001) # torch/nn/functional.py:973:17
      -> (%2005)
    block1():
      -> (%new_features.109)
  %2006 : Tensor[] = aten::append(%features.4, %new_features.110) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2007 : Tensor = prim::Uninitialized()
  %2008 : bool = prim::GetAttr[name="memory_efficient"](%1877)
  %2009 : bool = prim::If(%2008) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2010 : bool = prim::Uninitialized()
      %2011 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2012 : bool = aten::gt(%2011, %24)
      %2013 : bool, %2014 : bool, %2015 : int = prim::Loop(%18, %2012, %19, %2010, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2016 : int, %2017 : bool, %2018 : bool, %2019 : int):
          %tensor.56 : Tensor = aten::__getitem__(%features.4, %2019) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2021 : bool = prim::requires_grad(%tensor.56)
          %2022 : bool, %2023 : bool = prim::If(%2021) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2010)
          %2024 : int = aten::add(%2019, %27)
          %2025 : bool = aten::lt(%2024, %2011)
          %2026 : bool = aten::__and__(%2025, %2022)
          -> (%2026, %2021, %2023, %2024)
      %2027 : bool = prim::If(%2013)
        block0():
          -> (%2014)
        block1():
          -> (%19)
      -> (%2027)
    block1():
      -> (%19)
  %bottleneck_output.110 : Tensor = prim::If(%2009) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2007)
    block1():
      %concated_features.56 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2030 : __torch__.torch.nn.modules.conv.___torch_mangle_184.Conv2d = prim::GetAttr[name="conv1"](%1877)
      %2031 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_183.BatchNorm2d = prim::GetAttr[name="norm1"](%1877)
      %2032 : int = aten::dim(%concated_features.56) # torch/nn/modules/batchnorm.py:276:11
      %2033 : bool = aten::ne(%2032, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2033) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2034 : bool = prim::GetAttr[name="training"](%2031)
       = prim::If(%2034) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2035 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2031)
          %2036 : Tensor = aten::add(%2035, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2031, %2036)
          -> ()
        block1():
          -> ()
      %2037 : bool = prim::GetAttr[name="training"](%2031)
      %2038 : Tensor = prim::GetAttr[name="running_mean"](%2031)
      %2039 : Tensor = prim::GetAttr[name="running_var"](%2031)
      %2040 : Tensor = prim::GetAttr[name="weight"](%2031)
      %2041 : Tensor = prim::GetAttr[name="bias"](%2031)
       = prim::If(%2037) # torch/nn/functional.py:2011:4
        block0():
          %2042 : int[] = aten::size(%concated_features.56) # torch/nn/functional.py:2012:27
          %size_prods.456 : int = aten::__getitem__(%2042, %24) # torch/nn/functional.py:1991:17
          %2044 : int = aten::len(%2042) # torch/nn/functional.py:1992:19
          %2045 : int = aten::sub(%2044, %26) # torch/nn/functional.py:1992:19
          %size_prods.457 : int = prim::Loop(%2045, %25, %size_prods.456) # torch/nn/functional.py:1992:4
            block0(%i.115 : int, %size_prods.458 : int):
              %2049 : int = aten::add(%i.115, %26) # torch/nn/functional.py:1993:27
              %2050 : int = aten::__getitem__(%2042, %2049) # torch/nn/functional.py:1993:22
              %size_prods.459 : int = aten::mul(%size_prods.458, %2050) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.459)
          %2052 : bool = aten::eq(%size_prods.457, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2052) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2053 : Tensor = aten::batch_norm(%concated_features.56, %2040, %2041, %2038, %2039, %2037, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.111 : Tensor = aten::relu_(%2053) # torch/nn/functional.py:1117:17
      %2055 : Tensor = prim::GetAttr[name="weight"](%2030)
      %2056 : Tensor? = prim::GetAttr[name="bias"](%2030)
      %2057 : int[] = prim::ListConstruct(%27, %27)
      %2058 : int[] = prim::ListConstruct(%24, %24)
      %2059 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.111 : Tensor = aten::conv2d(%result.111, %2055, %2056, %2057, %2058, %2059, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.111)
  %2061 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1877)
  %2062 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1877)
  %2063 : int = aten::dim(%bottleneck_output.110) # torch/nn/modules/batchnorm.py:276:11
  %2064 : bool = aten::ne(%2063, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2064) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2065 : bool = prim::GetAttr[name="training"](%2062)
   = prim::If(%2065) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2066 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2062)
      %2067 : Tensor = aten::add(%2066, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2062, %2067)
      -> ()
    block1():
      -> ()
  %2068 : bool = prim::GetAttr[name="training"](%2062)
  %2069 : Tensor = prim::GetAttr[name="running_mean"](%2062)
  %2070 : Tensor = prim::GetAttr[name="running_var"](%2062)
  %2071 : Tensor = prim::GetAttr[name="weight"](%2062)
  %2072 : Tensor = prim::GetAttr[name="bias"](%2062)
   = prim::If(%2068) # torch/nn/functional.py:2011:4
    block0():
      %2073 : int[] = aten::size(%bottleneck_output.110) # torch/nn/functional.py:2012:27
      %size_prods.460 : int = aten::__getitem__(%2073, %24) # torch/nn/functional.py:1991:17
      %2075 : int = aten::len(%2073) # torch/nn/functional.py:1992:19
      %2076 : int = aten::sub(%2075, %26) # torch/nn/functional.py:1992:19
      %size_prods.461 : int = prim::Loop(%2076, %25, %size_prods.460) # torch/nn/functional.py:1992:4
        block0(%i.116 : int, %size_prods.462 : int):
          %2080 : int = aten::add(%i.116, %26) # torch/nn/functional.py:1993:27
          %2081 : int = aten::__getitem__(%2073, %2080) # torch/nn/functional.py:1993:22
          %size_prods.463 : int = aten::mul(%size_prods.462, %2081) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.463)
      %2083 : bool = aten::eq(%size_prods.461, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2083) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2084 : Tensor = aten::batch_norm(%bottleneck_output.110, %2071, %2072, %2069, %2070, %2068, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.112 : Tensor = aten::relu_(%2084) # torch/nn/functional.py:1117:17
  %2086 : Tensor = prim::GetAttr[name="weight"](%2061)
  %2087 : Tensor? = prim::GetAttr[name="bias"](%2061)
  %2088 : int[] = prim::ListConstruct(%27, %27)
  %2089 : int[] = prim::ListConstruct(%27, %27)
  %2090 : int[] = prim::ListConstruct(%27, %27)
  %new_features.111 : Tensor = aten::conv2d(%result.112, %2086, %2087, %2088, %2089, %2090, %27) # torch/nn/modules/conv.py:415:15
  %2092 : float = prim::GetAttr[name="drop_rate"](%1877)
  %2093 : bool = aten::gt(%2092, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.112 : Tensor = prim::If(%2093) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2095 : float = prim::GetAttr[name="drop_rate"](%1877)
      %2096 : bool = prim::GetAttr[name="training"](%1877)
      %2097 : bool = aten::lt(%2095, %16) # torch/nn/functional.py:968:7
      %2098 : bool = prim::If(%2097) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2099 : bool = aten::gt(%2095, %17) # torch/nn/functional.py:968:17
          -> (%2099)
       = prim::If(%2098) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2100 : Tensor = aten::dropout(%new_features.111, %2095, %2096) # torch/nn/functional.py:973:17
      -> (%2100)
    block1():
      -> (%new_features.111)
  %2101 : Tensor[] = aten::append(%features.4, %new_features.112) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2102 : Tensor = prim::Uninitialized()
  %2103 : bool = prim::GetAttr[name="memory_efficient"](%1878)
  %2104 : bool = prim::If(%2103) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2105 : bool = prim::Uninitialized()
      %2106 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2107 : bool = aten::gt(%2106, %24)
      %2108 : bool, %2109 : bool, %2110 : int = prim::Loop(%18, %2107, %19, %2105, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2111 : int, %2112 : bool, %2113 : bool, %2114 : int):
          %tensor.57 : Tensor = aten::__getitem__(%features.4, %2114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2116 : bool = prim::requires_grad(%tensor.57)
          %2117 : bool, %2118 : bool = prim::If(%2116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2105)
          %2119 : int = aten::add(%2114, %27)
          %2120 : bool = aten::lt(%2119, %2106)
          %2121 : bool = aten::__and__(%2120, %2117)
          -> (%2121, %2116, %2118, %2119)
      %2122 : bool = prim::If(%2108)
        block0():
          -> (%2109)
        block1():
          -> (%19)
      -> (%2122)
    block1():
      -> (%19)
  %bottleneck_output.112 : Tensor = prim::If(%2104) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2102)
    block1():
      %concated_features.57 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2125 : __torch__.torch.nn.modules.conv.___torch_mangle_186.Conv2d = prim::GetAttr[name="conv1"](%1878)
      %2126 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_106.BatchNorm2d = prim::GetAttr[name="norm1"](%1878)
      %2127 : int = aten::dim(%concated_features.57) # torch/nn/modules/batchnorm.py:276:11
      %2128 : bool = aten::ne(%2127, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2128) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2129 : bool = prim::GetAttr[name="training"](%2126)
       = prim::If(%2129) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2130 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2126)
          %2131 : Tensor = aten::add(%2130, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2126, %2131)
          -> ()
        block1():
          -> ()
      %2132 : bool = prim::GetAttr[name="training"](%2126)
      %2133 : Tensor = prim::GetAttr[name="running_mean"](%2126)
      %2134 : Tensor = prim::GetAttr[name="running_var"](%2126)
      %2135 : Tensor = prim::GetAttr[name="weight"](%2126)
      %2136 : Tensor = prim::GetAttr[name="bias"](%2126)
       = prim::If(%2132) # torch/nn/functional.py:2011:4
        block0():
          %2137 : int[] = aten::size(%concated_features.57) # torch/nn/functional.py:2012:27
          %size_prods.464 : int = aten::__getitem__(%2137, %24) # torch/nn/functional.py:1991:17
          %2139 : int = aten::len(%2137) # torch/nn/functional.py:1992:19
          %2140 : int = aten::sub(%2139, %26) # torch/nn/functional.py:1992:19
          %size_prods.465 : int = prim::Loop(%2140, %25, %size_prods.464) # torch/nn/functional.py:1992:4
            block0(%i.117 : int, %size_prods.466 : int):
              %2144 : int = aten::add(%i.117, %26) # torch/nn/functional.py:1993:27
              %2145 : int = aten::__getitem__(%2137, %2144) # torch/nn/functional.py:1993:22
              %size_prods.467 : int = aten::mul(%size_prods.466, %2145) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.467)
          %2147 : bool = aten::eq(%size_prods.465, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2147) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2148 : Tensor = aten::batch_norm(%concated_features.57, %2135, %2136, %2133, %2134, %2132, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.113 : Tensor = aten::relu_(%2148) # torch/nn/functional.py:1117:17
      %2150 : Tensor = prim::GetAttr[name="weight"](%2125)
      %2151 : Tensor? = prim::GetAttr[name="bias"](%2125)
      %2152 : int[] = prim::ListConstruct(%27, %27)
      %2153 : int[] = prim::ListConstruct(%24, %24)
      %2154 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.113 : Tensor = aten::conv2d(%result.113, %2150, %2151, %2152, %2153, %2154, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.113)
  %2156 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1878)
  %2157 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1878)
  %2158 : int = aten::dim(%bottleneck_output.112) # torch/nn/modules/batchnorm.py:276:11
  %2159 : bool = aten::ne(%2158, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2159) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2160 : bool = prim::GetAttr[name="training"](%2157)
   = prim::If(%2160) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2161 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2157)
      %2162 : Tensor = aten::add(%2161, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2157, %2162)
      -> ()
    block1():
      -> ()
  %2163 : bool = prim::GetAttr[name="training"](%2157)
  %2164 : Tensor = prim::GetAttr[name="running_mean"](%2157)
  %2165 : Tensor = prim::GetAttr[name="running_var"](%2157)
  %2166 : Tensor = prim::GetAttr[name="weight"](%2157)
  %2167 : Tensor = prim::GetAttr[name="bias"](%2157)
   = prim::If(%2163) # torch/nn/functional.py:2011:4
    block0():
      %2168 : int[] = aten::size(%bottleneck_output.112) # torch/nn/functional.py:2012:27
      %size_prods.468 : int = aten::__getitem__(%2168, %24) # torch/nn/functional.py:1991:17
      %2170 : int = aten::len(%2168) # torch/nn/functional.py:1992:19
      %2171 : int = aten::sub(%2170, %26) # torch/nn/functional.py:1992:19
      %size_prods.469 : int = prim::Loop(%2171, %25, %size_prods.468) # torch/nn/functional.py:1992:4
        block0(%i.118 : int, %size_prods.470 : int):
          %2175 : int = aten::add(%i.118, %26) # torch/nn/functional.py:1993:27
          %2176 : int = aten::__getitem__(%2168, %2175) # torch/nn/functional.py:1993:22
          %size_prods.471 : int = aten::mul(%size_prods.470, %2176) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.471)
      %2178 : bool = aten::eq(%size_prods.469, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2178) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2179 : Tensor = aten::batch_norm(%bottleneck_output.112, %2166, %2167, %2164, %2165, %2163, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.114 : Tensor = aten::relu_(%2179) # torch/nn/functional.py:1117:17
  %2181 : Tensor = prim::GetAttr[name="weight"](%2156)
  %2182 : Tensor? = prim::GetAttr[name="bias"](%2156)
  %2183 : int[] = prim::ListConstruct(%27, %27)
  %2184 : int[] = prim::ListConstruct(%27, %27)
  %2185 : int[] = prim::ListConstruct(%27, %27)
  %new_features.113 : Tensor = aten::conv2d(%result.114, %2181, %2182, %2183, %2184, %2185, %27) # torch/nn/modules/conv.py:415:15
  %2187 : float = prim::GetAttr[name="drop_rate"](%1878)
  %2188 : bool = aten::gt(%2187, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.114 : Tensor = prim::If(%2188) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2190 : float = prim::GetAttr[name="drop_rate"](%1878)
      %2191 : bool = prim::GetAttr[name="training"](%1878)
      %2192 : bool = aten::lt(%2190, %16) # torch/nn/functional.py:968:7
      %2193 : bool = prim::If(%2192) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2194 : bool = aten::gt(%2190, %17) # torch/nn/functional.py:968:17
          -> (%2194)
       = prim::If(%2193) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2195 : Tensor = aten::dropout(%new_features.113, %2190, %2191) # torch/nn/functional.py:973:17
      -> (%2195)
    block1():
      -> (%new_features.113)
  %2196 : Tensor[] = aten::append(%features.4, %new_features.114) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2197 : Tensor = prim::Uninitialized()
  %2198 : bool = prim::GetAttr[name="memory_efficient"](%1879)
  %2199 : bool = prim::If(%2198) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2200 : bool = prim::Uninitialized()
      %2201 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2202 : bool = aten::gt(%2201, %24)
      %2203 : bool, %2204 : bool, %2205 : int = prim::Loop(%18, %2202, %19, %2200, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2206 : int, %2207 : bool, %2208 : bool, %2209 : int):
          %tensor.58 : Tensor = aten::__getitem__(%features.4, %2209) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2211 : bool = prim::requires_grad(%tensor.58)
          %2212 : bool, %2213 : bool = prim::If(%2211) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2200)
          %2214 : int = aten::add(%2209, %27)
          %2215 : bool = aten::lt(%2214, %2201)
          %2216 : bool = aten::__and__(%2215, %2212)
          -> (%2216, %2211, %2213, %2214)
      %2217 : bool = prim::If(%2203)
        block0():
          -> (%2204)
        block1():
          -> (%19)
      -> (%2217)
    block1():
      -> (%19)
  %bottleneck_output.114 : Tensor = prim::If(%2199) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2197)
    block1():
      %concated_features.58 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2220 : __torch__.torch.nn.modules.conv.___torch_mangle_189.Conv2d = prim::GetAttr[name="conv1"](%1879)
      %2221 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_188.BatchNorm2d = prim::GetAttr[name="norm1"](%1879)
      %2222 : int = aten::dim(%concated_features.58) # torch/nn/modules/batchnorm.py:276:11
      %2223 : bool = aten::ne(%2222, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2223) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2224 : bool = prim::GetAttr[name="training"](%2221)
       = prim::If(%2224) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2225 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2221)
          %2226 : Tensor = aten::add(%2225, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2221, %2226)
          -> ()
        block1():
          -> ()
      %2227 : bool = prim::GetAttr[name="training"](%2221)
      %2228 : Tensor = prim::GetAttr[name="running_mean"](%2221)
      %2229 : Tensor = prim::GetAttr[name="running_var"](%2221)
      %2230 : Tensor = prim::GetAttr[name="weight"](%2221)
      %2231 : Tensor = prim::GetAttr[name="bias"](%2221)
       = prim::If(%2227) # torch/nn/functional.py:2011:4
        block0():
          %2232 : int[] = aten::size(%concated_features.58) # torch/nn/functional.py:2012:27
          %size_prods.472 : int = aten::__getitem__(%2232, %24) # torch/nn/functional.py:1991:17
          %2234 : int = aten::len(%2232) # torch/nn/functional.py:1992:19
          %2235 : int = aten::sub(%2234, %26) # torch/nn/functional.py:1992:19
          %size_prods.473 : int = prim::Loop(%2235, %25, %size_prods.472) # torch/nn/functional.py:1992:4
            block0(%i.119 : int, %size_prods.474 : int):
              %2239 : int = aten::add(%i.119, %26) # torch/nn/functional.py:1993:27
              %2240 : int = aten::__getitem__(%2232, %2239) # torch/nn/functional.py:1993:22
              %size_prods.475 : int = aten::mul(%size_prods.474, %2240) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.475)
          %2242 : bool = aten::eq(%size_prods.473, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2242) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2243 : Tensor = aten::batch_norm(%concated_features.58, %2230, %2231, %2228, %2229, %2227, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.115 : Tensor = aten::relu_(%2243) # torch/nn/functional.py:1117:17
      %2245 : Tensor = prim::GetAttr[name="weight"](%2220)
      %2246 : Tensor? = prim::GetAttr[name="bias"](%2220)
      %2247 : int[] = prim::ListConstruct(%27, %27)
      %2248 : int[] = prim::ListConstruct(%24, %24)
      %2249 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.115 : Tensor = aten::conv2d(%result.115, %2245, %2246, %2247, %2248, %2249, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.115)
  %2251 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1879)
  %2252 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1879)
  %2253 : int = aten::dim(%bottleneck_output.114) # torch/nn/modules/batchnorm.py:276:11
  %2254 : bool = aten::ne(%2253, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2254) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2255 : bool = prim::GetAttr[name="training"](%2252)
   = prim::If(%2255) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2256 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2252)
      %2257 : Tensor = aten::add(%2256, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2252, %2257)
      -> ()
    block1():
      -> ()
  %2258 : bool = prim::GetAttr[name="training"](%2252)
  %2259 : Tensor = prim::GetAttr[name="running_mean"](%2252)
  %2260 : Tensor = prim::GetAttr[name="running_var"](%2252)
  %2261 : Tensor = prim::GetAttr[name="weight"](%2252)
  %2262 : Tensor = prim::GetAttr[name="bias"](%2252)
   = prim::If(%2258) # torch/nn/functional.py:2011:4
    block0():
      %2263 : int[] = aten::size(%bottleneck_output.114) # torch/nn/functional.py:2012:27
      %size_prods.476 : int = aten::__getitem__(%2263, %24) # torch/nn/functional.py:1991:17
      %2265 : int = aten::len(%2263) # torch/nn/functional.py:1992:19
      %2266 : int = aten::sub(%2265, %26) # torch/nn/functional.py:1992:19
      %size_prods.477 : int = prim::Loop(%2266, %25, %size_prods.476) # torch/nn/functional.py:1992:4
        block0(%i.120 : int, %size_prods.478 : int):
          %2270 : int = aten::add(%i.120, %26) # torch/nn/functional.py:1993:27
          %2271 : int = aten::__getitem__(%2263, %2270) # torch/nn/functional.py:1993:22
          %size_prods.479 : int = aten::mul(%size_prods.478, %2271) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.479)
      %2273 : bool = aten::eq(%size_prods.477, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2273) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2274 : Tensor = aten::batch_norm(%bottleneck_output.114, %2261, %2262, %2259, %2260, %2258, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.116 : Tensor = aten::relu_(%2274) # torch/nn/functional.py:1117:17
  %2276 : Tensor = prim::GetAttr[name="weight"](%2251)
  %2277 : Tensor? = prim::GetAttr[name="bias"](%2251)
  %2278 : int[] = prim::ListConstruct(%27, %27)
  %2279 : int[] = prim::ListConstruct(%27, %27)
  %2280 : int[] = prim::ListConstruct(%27, %27)
  %new_features.115 : Tensor = aten::conv2d(%result.116, %2276, %2277, %2278, %2279, %2280, %27) # torch/nn/modules/conv.py:415:15
  %2282 : float = prim::GetAttr[name="drop_rate"](%1879)
  %2283 : bool = aten::gt(%2282, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.116 : Tensor = prim::If(%2283) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2285 : float = prim::GetAttr[name="drop_rate"](%1879)
      %2286 : bool = prim::GetAttr[name="training"](%1879)
      %2287 : bool = aten::lt(%2285, %16) # torch/nn/functional.py:968:7
      %2288 : bool = prim::If(%2287) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2289 : bool = aten::gt(%2285, %17) # torch/nn/functional.py:968:17
          -> (%2289)
       = prim::If(%2288) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2290 : Tensor = aten::dropout(%new_features.115, %2285, %2286) # torch/nn/functional.py:973:17
      -> (%2290)
    block1():
      -> (%new_features.115)
  %2291 : Tensor[] = aten::append(%features.4, %new_features.116) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2292 : Tensor = prim::Uninitialized()
  %2293 : bool = prim::GetAttr[name="memory_efficient"](%1880)
  %2294 : bool = prim::If(%2293) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2295 : bool = prim::Uninitialized()
      %2296 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2297 : bool = aten::gt(%2296, %24)
      %2298 : bool, %2299 : bool, %2300 : int = prim::Loop(%18, %2297, %19, %2295, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2301 : int, %2302 : bool, %2303 : bool, %2304 : int):
          %tensor.59 : Tensor = aten::__getitem__(%features.4, %2304) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2306 : bool = prim::requires_grad(%tensor.59)
          %2307 : bool, %2308 : bool = prim::If(%2306) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2295)
          %2309 : int = aten::add(%2304, %27)
          %2310 : bool = aten::lt(%2309, %2296)
          %2311 : bool = aten::__and__(%2310, %2307)
          -> (%2311, %2306, %2308, %2309)
      %2312 : bool = prim::If(%2298)
        block0():
          -> (%2299)
        block1():
          -> (%19)
      -> (%2312)
    block1():
      -> (%19)
  %bottleneck_output.116 : Tensor = prim::If(%2294) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2292)
    block1():
      %concated_features.59 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2315 : __torch__.torch.nn.modules.conv.___torch_mangle_191.Conv2d = prim::GetAttr[name="conv1"](%1880)
      %2316 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_115.BatchNorm2d = prim::GetAttr[name="norm1"](%1880)
      %2317 : int = aten::dim(%concated_features.59) # torch/nn/modules/batchnorm.py:276:11
      %2318 : bool = aten::ne(%2317, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2318) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2319 : bool = prim::GetAttr[name="training"](%2316)
       = prim::If(%2319) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2320 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2316)
          %2321 : Tensor = aten::add(%2320, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2316, %2321)
          -> ()
        block1():
          -> ()
      %2322 : bool = prim::GetAttr[name="training"](%2316)
      %2323 : Tensor = prim::GetAttr[name="running_mean"](%2316)
      %2324 : Tensor = prim::GetAttr[name="running_var"](%2316)
      %2325 : Tensor = prim::GetAttr[name="weight"](%2316)
      %2326 : Tensor = prim::GetAttr[name="bias"](%2316)
       = prim::If(%2322) # torch/nn/functional.py:2011:4
        block0():
          %2327 : int[] = aten::size(%concated_features.59) # torch/nn/functional.py:2012:27
          %size_prods.480 : int = aten::__getitem__(%2327, %24) # torch/nn/functional.py:1991:17
          %2329 : int = aten::len(%2327) # torch/nn/functional.py:1992:19
          %2330 : int = aten::sub(%2329, %26) # torch/nn/functional.py:1992:19
          %size_prods.481 : int = prim::Loop(%2330, %25, %size_prods.480) # torch/nn/functional.py:1992:4
            block0(%i.121 : int, %size_prods.482 : int):
              %2334 : int = aten::add(%i.121, %26) # torch/nn/functional.py:1993:27
              %2335 : int = aten::__getitem__(%2327, %2334) # torch/nn/functional.py:1993:22
              %size_prods.483 : int = aten::mul(%size_prods.482, %2335) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.483)
          %2337 : bool = aten::eq(%size_prods.481, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2337) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2338 : Tensor = aten::batch_norm(%concated_features.59, %2325, %2326, %2323, %2324, %2322, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.117 : Tensor = aten::relu_(%2338) # torch/nn/functional.py:1117:17
      %2340 : Tensor = prim::GetAttr[name="weight"](%2315)
      %2341 : Tensor? = prim::GetAttr[name="bias"](%2315)
      %2342 : int[] = prim::ListConstruct(%27, %27)
      %2343 : int[] = prim::ListConstruct(%24, %24)
      %2344 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.117 : Tensor = aten::conv2d(%result.117, %2340, %2341, %2342, %2343, %2344, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.117)
  %2346 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1880)
  %2347 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1880)
  %2348 : int = aten::dim(%bottleneck_output.116) # torch/nn/modules/batchnorm.py:276:11
  %2349 : bool = aten::ne(%2348, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2349) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2350 : bool = prim::GetAttr[name="training"](%2347)
   = prim::If(%2350) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2351 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2347)
      %2352 : Tensor = aten::add(%2351, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2347, %2352)
      -> ()
    block1():
      -> ()
  %2353 : bool = prim::GetAttr[name="training"](%2347)
  %2354 : Tensor = prim::GetAttr[name="running_mean"](%2347)
  %2355 : Tensor = prim::GetAttr[name="running_var"](%2347)
  %2356 : Tensor = prim::GetAttr[name="weight"](%2347)
  %2357 : Tensor = prim::GetAttr[name="bias"](%2347)
   = prim::If(%2353) # torch/nn/functional.py:2011:4
    block0():
      %2358 : int[] = aten::size(%bottleneck_output.116) # torch/nn/functional.py:2012:27
      %size_prods.484 : int = aten::__getitem__(%2358, %24) # torch/nn/functional.py:1991:17
      %2360 : int = aten::len(%2358) # torch/nn/functional.py:1992:19
      %2361 : int = aten::sub(%2360, %26) # torch/nn/functional.py:1992:19
      %size_prods.485 : int = prim::Loop(%2361, %25, %size_prods.484) # torch/nn/functional.py:1992:4
        block0(%i.122 : int, %size_prods.486 : int):
          %2365 : int = aten::add(%i.122, %26) # torch/nn/functional.py:1993:27
          %2366 : int = aten::__getitem__(%2358, %2365) # torch/nn/functional.py:1993:22
          %size_prods.487 : int = aten::mul(%size_prods.486, %2366) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.487)
      %2368 : bool = aten::eq(%size_prods.485, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2368) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2369 : Tensor = aten::batch_norm(%bottleneck_output.116, %2356, %2357, %2354, %2355, %2353, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.118 : Tensor = aten::relu_(%2369) # torch/nn/functional.py:1117:17
  %2371 : Tensor = prim::GetAttr[name="weight"](%2346)
  %2372 : Tensor? = prim::GetAttr[name="bias"](%2346)
  %2373 : int[] = prim::ListConstruct(%27, %27)
  %2374 : int[] = prim::ListConstruct(%27, %27)
  %2375 : int[] = prim::ListConstruct(%27, %27)
  %new_features.117 : Tensor = aten::conv2d(%result.118, %2371, %2372, %2373, %2374, %2375, %27) # torch/nn/modules/conv.py:415:15
  %2377 : float = prim::GetAttr[name="drop_rate"](%1880)
  %2378 : bool = aten::gt(%2377, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.118 : Tensor = prim::If(%2378) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2380 : float = prim::GetAttr[name="drop_rate"](%1880)
      %2381 : bool = prim::GetAttr[name="training"](%1880)
      %2382 : bool = aten::lt(%2380, %16) # torch/nn/functional.py:968:7
      %2383 : bool = prim::If(%2382) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2384 : bool = aten::gt(%2380, %17) # torch/nn/functional.py:968:17
          -> (%2384)
       = prim::If(%2383) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2385 : Tensor = aten::dropout(%new_features.117, %2380, %2381) # torch/nn/functional.py:973:17
      -> (%2385)
    block1():
      -> (%new_features.117)
  %2386 : Tensor[] = aten::append(%features.4, %new_features.118) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2387 : Tensor = prim::Uninitialized()
  %2388 : bool = prim::GetAttr[name="memory_efficient"](%1881)
  %2389 : bool = prim::If(%2388) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2390 : bool = prim::Uninitialized()
      %2391 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2392 : bool = aten::gt(%2391, %24)
      %2393 : bool, %2394 : bool, %2395 : int = prim::Loop(%18, %2392, %19, %2390, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2396 : int, %2397 : bool, %2398 : bool, %2399 : int):
          %tensor.60 : Tensor = aten::__getitem__(%features.4, %2399) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2401 : bool = prim::requires_grad(%tensor.60)
          %2402 : bool, %2403 : bool = prim::If(%2401) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2390)
          %2404 : int = aten::add(%2399, %27)
          %2405 : bool = aten::lt(%2404, %2391)
          %2406 : bool = aten::__and__(%2405, %2402)
          -> (%2406, %2401, %2403, %2404)
      %2407 : bool = prim::If(%2393)
        block0():
          -> (%2394)
        block1():
          -> (%19)
      -> (%2407)
    block1():
      -> (%19)
  %bottleneck_output.118 : Tensor = prim::If(%2389) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2387)
    block1():
      %concated_features.60 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2410 : __torch__.torch.nn.modules.conv.___torch_mangle_194.Conv2d = prim::GetAttr[name="conv1"](%1881)
      %2411 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_193.BatchNorm2d = prim::GetAttr[name="norm1"](%1881)
      %2412 : int = aten::dim(%concated_features.60) # torch/nn/modules/batchnorm.py:276:11
      %2413 : bool = aten::ne(%2412, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2413) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2414 : bool = prim::GetAttr[name="training"](%2411)
       = prim::If(%2414) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2415 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2411)
          %2416 : Tensor = aten::add(%2415, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2411, %2416)
          -> ()
        block1():
          -> ()
      %2417 : bool = prim::GetAttr[name="training"](%2411)
      %2418 : Tensor = prim::GetAttr[name="running_mean"](%2411)
      %2419 : Tensor = prim::GetAttr[name="running_var"](%2411)
      %2420 : Tensor = prim::GetAttr[name="weight"](%2411)
      %2421 : Tensor = prim::GetAttr[name="bias"](%2411)
       = prim::If(%2417) # torch/nn/functional.py:2011:4
        block0():
          %2422 : int[] = aten::size(%concated_features.60) # torch/nn/functional.py:2012:27
          %size_prods.488 : int = aten::__getitem__(%2422, %24) # torch/nn/functional.py:1991:17
          %2424 : int = aten::len(%2422) # torch/nn/functional.py:1992:19
          %2425 : int = aten::sub(%2424, %26) # torch/nn/functional.py:1992:19
          %size_prods.489 : int = prim::Loop(%2425, %25, %size_prods.488) # torch/nn/functional.py:1992:4
            block0(%i.123 : int, %size_prods.490 : int):
              %2429 : int = aten::add(%i.123, %26) # torch/nn/functional.py:1993:27
              %2430 : int = aten::__getitem__(%2422, %2429) # torch/nn/functional.py:1993:22
              %size_prods.491 : int = aten::mul(%size_prods.490, %2430) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.491)
          %2432 : bool = aten::eq(%size_prods.489, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2432) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2433 : Tensor = aten::batch_norm(%concated_features.60, %2420, %2421, %2418, %2419, %2417, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.119 : Tensor = aten::relu_(%2433) # torch/nn/functional.py:1117:17
      %2435 : Tensor = prim::GetAttr[name="weight"](%2410)
      %2436 : Tensor? = prim::GetAttr[name="bias"](%2410)
      %2437 : int[] = prim::ListConstruct(%27, %27)
      %2438 : int[] = prim::ListConstruct(%24, %24)
      %2439 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.119 : Tensor = aten::conv2d(%result.119, %2435, %2436, %2437, %2438, %2439, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.119)
  %2441 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1881)
  %2442 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1881)
  %2443 : int = aten::dim(%bottleneck_output.118) # torch/nn/modules/batchnorm.py:276:11
  %2444 : bool = aten::ne(%2443, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2444) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2445 : bool = prim::GetAttr[name="training"](%2442)
   = prim::If(%2445) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2446 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2442)
      %2447 : Tensor = aten::add(%2446, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2442, %2447)
      -> ()
    block1():
      -> ()
  %2448 : bool = prim::GetAttr[name="training"](%2442)
  %2449 : Tensor = prim::GetAttr[name="running_mean"](%2442)
  %2450 : Tensor = prim::GetAttr[name="running_var"](%2442)
  %2451 : Tensor = prim::GetAttr[name="weight"](%2442)
  %2452 : Tensor = prim::GetAttr[name="bias"](%2442)
   = prim::If(%2448) # torch/nn/functional.py:2011:4
    block0():
      %2453 : int[] = aten::size(%bottleneck_output.118) # torch/nn/functional.py:2012:27
      %size_prods.492 : int = aten::__getitem__(%2453, %24) # torch/nn/functional.py:1991:17
      %2455 : int = aten::len(%2453) # torch/nn/functional.py:1992:19
      %2456 : int = aten::sub(%2455, %26) # torch/nn/functional.py:1992:19
      %size_prods.493 : int = prim::Loop(%2456, %25, %size_prods.492) # torch/nn/functional.py:1992:4
        block0(%i.124 : int, %size_prods.494 : int):
          %2460 : int = aten::add(%i.124, %26) # torch/nn/functional.py:1993:27
          %2461 : int = aten::__getitem__(%2453, %2460) # torch/nn/functional.py:1993:22
          %size_prods.495 : int = aten::mul(%size_prods.494, %2461) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.495)
      %2463 : bool = aten::eq(%size_prods.493, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2463) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2464 : Tensor = aten::batch_norm(%bottleneck_output.118, %2451, %2452, %2449, %2450, %2448, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.120 : Tensor = aten::relu_(%2464) # torch/nn/functional.py:1117:17
  %2466 : Tensor = prim::GetAttr[name="weight"](%2441)
  %2467 : Tensor? = prim::GetAttr[name="bias"](%2441)
  %2468 : int[] = prim::ListConstruct(%27, %27)
  %2469 : int[] = prim::ListConstruct(%27, %27)
  %2470 : int[] = prim::ListConstruct(%27, %27)
  %new_features.119 : Tensor = aten::conv2d(%result.120, %2466, %2467, %2468, %2469, %2470, %27) # torch/nn/modules/conv.py:415:15
  %2472 : float = prim::GetAttr[name="drop_rate"](%1881)
  %2473 : bool = aten::gt(%2472, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.120 : Tensor = prim::If(%2473) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2475 : float = prim::GetAttr[name="drop_rate"](%1881)
      %2476 : bool = prim::GetAttr[name="training"](%1881)
      %2477 : bool = aten::lt(%2475, %16) # torch/nn/functional.py:968:7
      %2478 : bool = prim::If(%2477) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2479 : bool = aten::gt(%2475, %17) # torch/nn/functional.py:968:17
          -> (%2479)
       = prim::If(%2478) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2480 : Tensor = aten::dropout(%new_features.119, %2475, %2476) # torch/nn/functional.py:973:17
      -> (%2480)
    block1():
      -> (%new_features.119)
  %2481 : Tensor[] = aten::append(%features.4, %new_features.120) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2482 : Tensor = prim::Uninitialized()
  %2483 : bool = prim::GetAttr[name="memory_efficient"](%1882)
  %2484 : bool = prim::If(%2483) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2485 : bool = prim::Uninitialized()
      %2486 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2487 : bool = aten::gt(%2486, %24)
      %2488 : bool, %2489 : bool, %2490 : int = prim::Loop(%18, %2487, %19, %2485, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2491 : int, %2492 : bool, %2493 : bool, %2494 : int):
          %tensor.61 : Tensor = aten::__getitem__(%features.4, %2494) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2496 : bool = prim::requires_grad(%tensor.61)
          %2497 : bool, %2498 : bool = prim::If(%2496) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2485)
          %2499 : int = aten::add(%2494, %27)
          %2500 : bool = aten::lt(%2499, %2486)
          %2501 : bool = aten::__and__(%2500, %2497)
          -> (%2501, %2496, %2498, %2499)
      %2502 : bool = prim::If(%2488)
        block0():
          -> (%2489)
        block1():
          -> (%19)
      -> (%2502)
    block1():
      -> (%19)
  %bottleneck_output.120 : Tensor = prim::If(%2484) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2482)
    block1():
      %concated_features.61 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2505 : __torch__.torch.nn.modules.conv.___torch_mangle_196.Conv2d = prim::GetAttr[name="conv1"](%1882)
      %2506 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_124.BatchNorm2d = prim::GetAttr[name="norm1"](%1882)
      %2507 : int = aten::dim(%concated_features.61) # torch/nn/modules/batchnorm.py:276:11
      %2508 : bool = aten::ne(%2507, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2508) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2509 : bool = prim::GetAttr[name="training"](%2506)
       = prim::If(%2509) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2510 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2506)
          %2511 : Tensor = aten::add(%2510, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2506, %2511)
          -> ()
        block1():
          -> ()
      %2512 : bool = prim::GetAttr[name="training"](%2506)
      %2513 : Tensor = prim::GetAttr[name="running_mean"](%2506)
      %2514 : Tensor = prim::GetAttr[name="running_var"](%2506)
      %2515 : Tensor = prim::GetAttr[name="weight"](%2506)
      %2516 : Tensor = prim::GetAttr[name="bias"](%2506)
       = prim::If(%2512) # torch/nn/functional.py:2011:4
        block0():
          %2517 : int[] = aten::size(%concated_features.61) # torch/nn/functional.py:2012:27
          %size_prods.496 : int = aten::__getitem__(%2517, %24) # torch/nn/functional.py:1991:17
          %2519 : int = aten::len(%2517) # torch/nn/functional.py:1992:19
          %2520 : int = aten::sub(%2519, %26) # torch/nn/functional.py:1992:19
          %size_prods.497 : int = prim::Loop(%2520, %25, %size_prods.496) # torch/nn/functional.py:1992:4
            block0(%i.125 : int, %size_prods.498 : int):
              %2524 : int = aten::add(%i.125, %26) # torch/nn/functional.py:1993:27
              %2525 : int = aten::__getitem__(%2517, %2524) # torch/nn/functional.py:1993:22
              %size_prods.499 : int = aten::mul(%size_prods.498, %2525) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.499)
          %2527 : bool = aten::eq(%size_prods.497, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2527) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2528 : Tensor = aten::batch_norm(%concated_features.61, %2515, %2516, %2513, %2514, %2512, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.121 : Tensor = aten::relu_(%2528) # torch/nn/functional.py:1117:17
      %2530 : Tensor = prim::GetAttr[name="weight"](%2505)
      %2531 : Tensor? = prim::GetAttr[name="bias"](%2505)
      %2532 : int[] = prim::ListConstruct(%27, %27)
      %2533 : int[] = prim::ListConstruct(%24, %24)
      %2534 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.121 : Tensor = aten::conv2d(%result.121, %2530, %2531, %2532, %2533, %2534, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.121)
  %2536 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1882)
  %2537 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1882)
  %2538 : int = aten::dim(%bottleneck_output.120) # torch/nn/modules/batchnorm.py:276:11
  %2539 : bool = aten::ne(%2538, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2539) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2540 : bool = prim::GetAttr[name="training"](%2537)
   = prim::If(%2540) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2541 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2537)
      %2542 : Tensor = aten::add(%2541, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2537, %2542)
      -> ()
    block1():
      -> ()
  %2543 : bool = prim::GetAttr[name="training"](%2537)
  %2544 : Tensor = prim::GetAttr[name="running_mean"](%2537)
  %2545 : Tensor = prim::GetAttr[name="running_var"](%2537)
  %2546 : Tensor = prim::GetAttr[name="weight"](%2537)
  %2547 : Tensor = prim::GetAttr[name="bias"](%2537)
   = prim::If(%2543) # torch/nn/functional.py:2011:4
    block0():
      %2548 : int[] = aten::size(%bottleneck_output.120) # torch/nn/functional.py:2012:27
      %size_prods.500 : int = aten::__getitem__(%2548, %24) # torch/nn/functional.py:1991:17
      %2550 : int = aten::len(%2548) # torch/nn/functional.py:1992:19
      %2551 : int = aten::sub(%2550, %26) # torch/nn/functional.py:1992:19
      %size_prods.501 : int = prim::Loop(%2551, %25, %size_prods.500) # torch/nn/functional.py:1992:4
        block0(%i.126 : int, %size_prods.502 : int):
          %2555 : int = aten::add(%i.126, %26) # torch/nn/functional.py:1993:27
          %2556 : int = aten::__getitem__(%2548, %2555) # torch/nn/functional.py:1993:22
          %size_prods.503 : int = aten::mul(%size_prods.502, %2556) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.503)
      %2558 : bool = aten::eq(%size_prods.501, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2558) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2559 : Tensor = aten::batch_norm(%bottleneck_output.120, %2546, %2547, %2544, %2545, %2543, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.122 : Tensor = aten::relu_(%2559) # torch/nn/functional.py:1117:17
  %2561 : Tensor = prim::GetAttr[name="weight"](%2536)
  %2562 : Tensor? = prim::GetAttr[name="bias"](%2536)
  %2563 : int[] = prim::ListConstruct(%27, %27)
  %2564 : int[] = prim::ListConstruct(%27, %27)
  %2565 : int[] = prim::ListConstruct(%27, %27)
  %new_features.121 : Tensor = aten::conv2d(%result.122, %2561, %2562, %2563, %2564, %2565, %27) # torch/nn/modules/conv.py:415:15
  %2567 : float = prim::GetAttr[name="drop_rate"](%1882)
  %2568 : bool = aten::gt(%2567, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.122 : Tensor = prim::If(%2568) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2570 : float = prim::GetAttr[name="drop_rate"](%1882)
      %2571 : bool = prim::GetAttr[name="training"](%1882)
      %2572 : bool = aten::lt(%2570, %16) # torch/nn/functional.py:968:7
      %2573 : bool = prim::If(%2572) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2574 : bool = aten::gt(%2570, %17) # torch/nn/functional.py:968:17
          -> (%2574)
       = prim::If(%2573) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2575 : Tensor = aten::dropout(%new_features.121, %2570, %2571) # torch/nn/functional.py:973:17
      -> (%2575)
    block1():
      -> (%new_features.121)
  %2576 : Tensor[] = aten::append(%features.4, %new_features.122) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2577 : Tensor = prim::Uninitialized()
  %2578 : bool = prim::GetAttr[name="memory_efficient"](%1883)
  %2579 : bool = prim::If(%2578) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2580 : bool = prim::Uninitialized()
      %2581 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2582 : bool = aten::gt(%2581, %24)
      %2583 : bool, %2584 : bool, %2585 : int = prim::Loop(%18, %2582, %19, %2580, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2586 : int, %2587 : bool, %2588 : bool, %2589 : int):
          %tensor.62 : Tensor = aten::__getitem__(%features.4, %2589) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2591 : bool = prim::requires_grad(%tensor.62)
          %2592 : bool, %2593 : bool = prim::If(%2591) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2580)
          %2594 : int = aten::add(%2589, %27)
          %2595 : bool = aten::lt(%2594, %2581)
          %2596 : bool = aten::__and__(%2595, %2592)
          -> (%2596, %2591, %2593, %2594)
      %2597 : bool = prim::If(%2583)
        block0():
          -> (%2584)
        block1():
          -> (%19)
      -> (%2597)
    block1():
      -> (%19)
  %bottleneck_output.122 : Tensor = prim::If(%2579) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2577)
    block1():
      %concated_features.62 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2600 : __torch__.torch.nn.modules.conv.___torch_mangle_199.Conv2d = prim::GetAttr[name="conv1"](%1883)
      %2601 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_198.BatchNorm2d = prim::GetAttr[name="norm1"](%1883)
      %2602 : int = aten::dim(%concated_features.62) # torch/nn/modules/batchnorm.py:276:11
      %2603 : bool = aten::ne(%2602, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2603) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2604 : bool = prim::GetAttr[name="training"](%2601)
       = prim::If(%2604) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2605 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2601)
          %2606 : Tensor = aten::add(%2605, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2601, %2606)
          -> ()
        block1():
          -> ()
      %2607 : bool = prim::GetAttr[name="training"](%2601)
      %2608 : Tensor = prim::GetAttr[name="running_mean"](%2601)
      %2609 : Tensor = prim::GetAttr[name="running_var"](%2601)
      %2610 : Tensor = prim::GetAttr[name="weight"](%2601)
      %2611 : Tensor = prim::GetAttr[name="bias"](%2601)
       = prim::If(%2607) # torch/nn/functional.py:2011:4
        block0():
          %2612 : int[] = aten::size(%concated_features.62) # torch/nn/functional.py:2012:27
          %size_prods.504 : int = aten::__getitem__(%2612, %24) # torch/nn/functional.py:1991:17
          %2614 : int = aten::len(%2612) # torch/nn/functional.py:1992:19
          %2615 : int = aten::sub(%2614, %26) # torch/nn/functional.py:1992:19
          %size_prods.505 : int = prim::Loop(%2615, %25, %size_prods.504) # torch/nn/functional.py:1992:4
            block0(%i.127 : int, %size_prods.506 : int):
              %2619 : int = aten::add(%i.127, %26) # torch/nn/functional.py:1993:27
              %2620 : int = aten::__getitem__(%2612, %2619) # torch/nn/functional.py:1993:22
              %size_prods.507 : int = aten::mul(%size_prods.506, %2620) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.507)
          %2622 : bool = aten::eq(%size_prods.505, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2622) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2623 : Tensor = aten::batch_norm(%concated_features.62, %2610, %2611, %2608, %2609, %2607, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.123 : Tensor = aten::relu_(%2623) # torch/nn/functional.py:1117:17
      %2625 : Tensor = prim::GetAttr[name="weight"](%2600)
      %2626 : Tensor? = prim::GetAttr[name="bias"](%2600)
      %2627 : int[] = prim::ListConstruct(%27, %27)
      %2628 : int[] = prim::ListConstruct(%24, %24)
      %2629 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.123 : Tensor = aten::conv2d(%result.123, %2625, %2626, %2627, %2628, %2629, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.123)
  %2631 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1883)
  %2632 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1883)
  %2633 : int = aten::dim(%bottleneck_output.122) # torch/nn/modules/batchnorm.py:276:11
  %2634 : bool = aten::ne(%2633, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2634) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2635 : bool = prim::GetAttr[name="training"](%2632)
   = prim::If(%2635) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2636 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2632)
      %2637 : Tensor = aten::add(%2636, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2632, %2637)
      -> ()
    block1():
      -> ()
  %2638 : bool = prim::GetAttr[name="training"](%2632)
  %2639 : Tensor = prim::GetAttr[name="running_mean"](%2632)
  %2640 : Tensor = prim::GetAttr[name="running_var"](%2632)
  %2641 : Tensor = prim::GetAttr[name="weight"](%2632)
  %2642 : Tensor = prim::GetAttr[name="bias"](%2632)
   = prim::If(%2638) # torch/nn/functional.py:2011:4
    block0():
      %2643 : int[] = aten::size(%bottleneck_output.122) # torch/nn/functional.py:2012:27
      %size_prods.508 : int = aten::__getitem__(%2643, %24) # torch/nn/functional.py:1991:17
      %2645 : int = aten::len(%2643) # torch/nn/functional.py:1992:19
      %2646 : int = aten::sub(%2645, %26) # torch/nn/functional.py:1992:19
      %size_prods.509 : int = prim::Loop(%2646, %25, %size_prods.508) # torch/nn/functional.py:1992:4
        block0(%i.128 : int, %size_prods.510 : int):
          %2650 : int = aten::add(%i.128, %26) # torch/nn/functional.py:1993:27
          %2651 : int = aten::__getitem__(%2643, %2650) # torch/nn/functional.py:1993:22
          %size_prods.511 : int = aten::mul(%size_prods.510, %2651) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.511)
      %2653 : bool = aten::eq(%size_prods.509, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2653) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2654 : Tensor = aten::batch_norm(%bottleneck_output.122, %2641, %2642, %2639, %2640, %2638, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.124 : Tensor = aten::relu_(%2654) # torch/nn/functional.py:1117:17
  %2656 : Tensor = prim::GetAttr[name="weight"](%2631)
  %2657 : Tensor? = prim::GetAttr[name="bias"](%2631)
  %2658 : int[] = prim::ListConstruct(%27, %27)
  %2659 : int[] = prim::ListConstruct(%27, %27)
  %2660 : int[] = prim::ListConstruct(%27, %27)
  %new_features.123 : Tensor = aten::conv2d(%result.124, %2656, %2657, %2658, %2659, %2660, %27) # torch/nn/modules/conv.py:415:15
  %2662 : float = prim::GetAttr[name="drop_rate"](%1883)
  %2663 : bool = aten::gt(%2662, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.124 : Tensor = prim::If(%2663) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2665 : float = prim::GetAttr[name="drop_rate"](%1883)
      %2666 : bool = prim::GetAttr[name="training"](%1883)
      %2667 : bool = aten::lt(%2665, %16) # torch/nn/functional.py:968:7
      %2668 : bool = prim::If(%2667) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2669 : bool = aten::gt(%2665, %17) # torch/nn/functional.py:968:17
          -> (%2669)
       = prim::If(%2668) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2670 : Tensor = aten::dropout(%new_features.123, %2665, %2666) # torch/nn/functional.py:973:17
      -> (%2670)
    block1():
      -> (%new_features.123)
  %2671 : Tensor[] = aten::append(%features.4, %new_features.124) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2672 : Tensor = prim::Uninitialized()
  %2673 : bool = prim::GetAttr[name="memory_efficient"](%1884)
  %2674 : bool = prim::If(%2673) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2675 : bool = prim::Uninitialized()
      %2676 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2677 : bool = aten::gt(%2676, %24)
      %2678 : bool, %2679 : bool, %2680 : int = prim::Loop(%18, %2677, %19, %2675, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2681 : int, %2682 : bool, %2683 : bool, %2684 : int):
          %tensor.63 : Tensor = aten::__getitem__(%features.4, %2684) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2686 : bool = prim::requires_grad(%tensor.63)
          %2687 : bool, %2688 : bool = prim::If(%2686) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2675)
          %2689 : int = aten::add(%2684, %27)
          %2690 : bool = aten::lt(%2689, %2676)
          %2691 : bool = aten::__and__(%2690, %2687)
          -> (%2691, %2686, %2688, %2689)
      %2692 : bool = prim::If(%2678)
        block0():
          -> (%2679)
        block1():
          -> (%19)
      -> (%2692)
    block1():
      -> (%19)
  %bottleneck_output.124 : Tensor = prim::If(%2674) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2672)
    block1():
      %concated_features.63 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2695 : __torch__.torch.nn.modules.conv.___torch_mangle_204.Conv2d = prim::GetAttr[name="conv1"](%1884)
      %2696 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_133.BatchNorm2d = prim::GetAttr[name="norm1"](%1884)
      %2697 : int = aten::dim(%concated_features.63) # torch/nn/modules/batchnorm.py:276:11
      %2698 : bool = aten::ne(%2697, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2698) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2699 : bool = prim::GetAttr[name="training"](%2696)
       = prim::If(%2699) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2700 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2696)
          %2701 : Tensor = aten::add(%2700, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2696, %2701)
          -> ()
        block1():
          -> ()
      %2702 : bool = prim::GetAttr[name="training"](%2696)
      %2703 : Tensor = prim::GetAttr[name="running_mean"](%2696)
      %2704 : Tensor = prim::GetAttr[name="running_var"](%2696)
      %2705 : Tensor = prim::GetAttr[name="weight"](%2696)
      %2706 : Tensor = prim::GetAttr[name="bias"](%2696)
       = prim::If(%2702) # torch/nn/functional.py:2011:4
        block0():
          %2707 : int[] = aten::size(%concated_features.63) # torch/nn/functional.py:2012:27
          %size_prods.512 : int = aten::__getitem__(%2707, %24) # torch/nn/functional.py:1991:17
          %2709 : int = aten::len(%2707) # torch/nn/functional.py:1992:19
          %2710 : int = aten::sub(%2709, %26) # torch/nn/functional.py:1992:19
          %size_prods.513 : int = prim::Loop(%2710, %25, %size_prods.512) # torch/nn/functional.py:1992:4
            block0(%i.129 : int, %size_prods.514 : int):
              %2714 : int = aten::add(%i.129, %26) # torch/nn/functional.py:1993:27
              %2715 : int = aten::__getitem__(%2707, %2714) # torch/nn/functional.py:1993:22
              %size_prods.515 : int = aten::mul(%size_prods.514, %2715) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.515)
          %2717 : bool = aten::eq(%size_prods.513, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2717) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2718 : Tensor = aten::batch_norm(%concated_features.63, %2705, %2706, %2703, %2704, %2702, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.125 : Tensor = aten::relu_(%2718) # torch/nn/functional.py:1117:17
      %2720 : Tensor = prim::GetAttr[name="weight"](%2695)
      %2721 : Tensor? = prim::GetAttr[name="bias"](%2695)
      %2722 : int[] = prim::ListConstruct(%27, %27)
      %2723 : int[] = prim::ListConstruct(%24, %24)
      %2724 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.125 : Tensor = aten::conv2d(%result.125, %2720, %2721, %2722, %2723, %2724, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.125)
  %2726 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1884)
  %2727 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1884)
  %2728 : int = aten::dim(%bottleneck_output.124) # torch/nn/modules/batchnorm.py:276:11
  %2729 : bool = aten::ne(%2728, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2729) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2730 : bool = prim::GetAttr[name="training"](%2727)
   = prim::If(%2730) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2731 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2727)
      %2732 : Tensor = aten::add(%2731, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2727, %2732)
      -> ()
    block1():
      -> ()
  %2733 : bool = prim::GetAttr[name="training"](%2727)
  %2734 : Tensor = prim::GetAttr[name="running_mean"](%2727)
  %2735 : Tensor = prim::GetAttr[name="running_var"](%2727)
  %2736 : Tensor = prim::GetAttr[name="weight"](%2727)
  %2737 : Tensor = prim::GetAttr[name="bias"](%2727)
   = prim::If(%2733) # torch/nn/functional.py:2011:4
    block0():
      %2738 : int[] = aten::size(%bottleneck_output.124) # torch/nn/functional.py:2012:27
      %size_prods.516 : int = aten::__getitem__(%2738, %24) # torch/nn/functional.py:1991:17
      %2740 : int = aten::len(%2738) # torch/nn/functional.py:1992:19
      %2741 : int = aten::sub(%2740, %26) # torch/nn/functional.py:1992:19
      %size_prods.517 : int = prim::Loop(%2741, %25, %size_prods.516) # torch/nn/functional.py:1992:4
        block0(%i.130 : int, %size_prods.518 : int):
          %2745 : int = aten::add(%i.130, %26) # torch/nn/functional.py:1993:27
          %2746 : int = aten::__getitem__(%2738, %2745) # torch/nn/functional.py:1993:22
          %size_prods.519 : int = aten::mul(%size_prods.518, %2746) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.519)
      %2748 : bool = aten::eq(%size_prods.517, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2748) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2749 : Tensor = aten::batch_norm(%bottleneck_output.124, %2736, %2737, %2734, %2735, %2733, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.126 : Tensor = aten::relu_(%2749) # torch/nn/functional.py:1117:17
  %2751 : Tensor = prim::GetAttr[name="weight"](%2726)
  %2752 : Tensor? = prim::GetAttr[name="bias"](%2726)
  %2753 : int[] = prim::ListConstruct(%27, %27)
  %2754 : int[] = prim::ListConstruct(%27, %27)
  %2755 : int[] = prim::ListConstruct(%27, %27)
  %new_features.125 : Tensor = aten::conv2d(%result.126, %2751, %2752, %2753, %2754, %2755, %27) # torch/nn/modules/conv.py:415:15
  %2757 : float = prim::GetAttr[name="drop_rate"](%1884)
  %2758 : bool = aten::gt(%2757, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.126 : Tensor = prim::If(%2758) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2760 : float = prim::GetAttr[name="drop_rate"](%1884)
      %2761 : bool = prim::GetAttr[name="training"](%1884)
      %2762 : bool = aten::lt(%2760, %16) # torch/nn/functional.py:968:7
      %2763 : bool = prim::If(%2762) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2764 : bool = aten::gt(%2760, %17) # torch/nn/functional.py:968:17
          -> (%2764)
       = prim::If(%2763) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2765 : Tensor = aten::dropout(%new_features.125, %2760, %2761) # torch/nn/functional.py:973:17
      -> (%2765)
    block1():
      -> (%new_features.125)
  %2766 : Tensor[] = aten::append(%features.4, %new_features.126) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2767 : Tensor = prim::Uninitialized()
  %2768 : bool = prim::GetAttr[name="memory_efficient"](%1885)
  %2769 : bool = prim::If(%2768) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2770 : bool = prim::Uninitialized()
      %2771 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2772 : bool = aten::gt(%2771, %24)
      %2773 : bool, %2774 : bool, %2775 : int = prim::Loop(%18, %2772, %19, %2770, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2776 : int, %2777 : bool, %2778 : bool, %2779 : int):
          %tensor.64 : Tensor = aten::__getitem__(%features.4, %2779) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2781 : bool = prim::requires_grad(%tensor.64)
          %2782 : bool, %2783 : bool = prim::If(%2781) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2770)
          %2784 : int = aten::add(%2779, %27)
          %2785 : bool = aten::lt(%2784, %2771)
          %2786 : bool = aten::__and__(%2785, %2782)
          -> (%2786, %2781, %2783, %2784)
      %2787 : bool = prim::If(%2773)
        block0():
          -> (%2774)
        block1():
          -> (%19)
      -> (%2787)
    block1():
      -> (%19)
  %bottleneck_output.126 : Tensor = prim::If(%2769) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2767)
    block1():
      %concated_features.64 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2790 : __torch__.torch.nn.modules.conv.___torch_mangle_207.Conv2d = prim::GetAttr[name="conv1"](%1885)
      %2791 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_206.BatchNorm2d = prim::GetAttr[name="norm1"](%1885)
      %2792 : int = aten::dim(%concated_features.64) # torch/nn/modules/batchnorm.py:276:11
      %2793 : bool = aten::ne(%2792, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2793) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2794 : bool = prim::GetAttr[name="training"](%2791)
       = prim::If(%2794) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2795 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2791)
          %2796 : Tensor = aten::add(%2795, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2791, %2796)
          -> ()
        block1():
          -> ()
      %2797 : bool = prim::GetAttr[name="training"](%2791)
      %2798 : Tensor = prim::GetAttr[name="running_mean"](%2791)
      %2799 : Tensor = prim::GetAttr[name="running_var"](%2791)
      %2800 : Tensor = prim::GetAttr[name="weight"](%2791)
      %2801 : Tensor = prim::GetAttr[name="bias"](%2791)
       = prim::If(%2797) # torch/nn/functional.py:2011:4
        block0():
          %2802 : int[] = aten::size(%concated_features.64) # torch/nn/functional.py:2012:27
          %size_prods.520 : int = aten::__getitem__(%2802, %24) # torch/nn/functional.py:1991:17
          %2804 : int = aten::len(%2802) # torch/nn/functional.py:1992:19
          %2805 : int = aten::sub(%2804, %26) # torch/nn/functional.py:1992:19
          %size_prods.521 : int = prim::Loop(%2805, %25, %size_prods.520) # torch/nn/functional.py:1992:4
            block0(%i.131 : int, %size_prods.522 : int):
              %2809 : int = aten::add(%i.131, %26) # torch/nn/functional.py:1993:27
              %2810 : int = aten::__getitem__(%2802, %2809) # torch/nn/functional.py:1993:22
              %size_prods.523 : int = aten::mul(%size_prods.522, %2810) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.523)
          %2812 : bool = aten::eq(%size_prods.521, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2812) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2813 : Tensor = aten::batch_norm(%concated_features.64, %2800, %2801, %2798, %2799, %2797, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.127 : Tensor = aten::relu_(%2813) # torch/nn/functional.py:1117:17
      %2815 : Tensor = prim::GetAttr[name="weight"](%2790)
      %2816 : Tensor? = prim::GetAttr[name="bias"](%2790)
      %2817 : int[] = prim::ListConstruct(%27, %27)
      %2818 : int[] = prim::ListConstruct(%24, %24)
      %2819 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.127 : Tensor = aten::conv2d(%result.127, %2815, %2816, %2817, %2818, %2819, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.127)
  %2821 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1885)
  %2822 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1885)
  %2823 : int = aten::dim(%bottleneck_output.126) # torch/nn/modules/batchnorm.py:276:11
  %2824 : bool = aten::ne(%2823, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2824) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2825 : bool = prim::GetAttr[name="training"](%2822)
   = prim::If(%2825) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2826 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2822)
      %2827 : Tensor = aten::add(%2826, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2822, %2827)
      -> ()
    block1():
      -> ()
  %2828 : bool = prim::GetAttr[name="training"](%2822)
  %2829 : Tensor = prim::GetAttr[name="running_mean"](%2822)
  %2830 : Tensor = prim::GetAttr[name="running_var"](%2822)
  %2831 : Tensor = prim::GetAttr[name="weight"](%2822)
  %2832 : Tensor = prim::GetAttr[name="bias"](%2822)
   = prim::If(%2828) # torch/nn/functional.py:2011:4
    block0():
      %2833 : int[] = aten::size(%bottleneck_output.126) # torch/nn/functional.py:2012:27
      %size_prods.524 : int = aten::__getitem__(%2833, %24) # torch/nn/functional.py:1991:17
      %2835 : int = aten::len(%2833) # torch/nn/functional.py:1992:19
      %2836 : int = aten::sub(%2835, %26) # torch/nn/functional.py:1992:19
      %size_prods.525 : int = prim::Loop(%2836, %25, %size_prods.524) # torch/nn/functional.py:1992:4
        block0(%i.132 : int, %size_prods.526 : int):
          %2840 : int = aten::add(%i.132, %26) # torch/nn/functional.py:1993:27
          %2841 : int = aten::__getitem__(%2833, %2840) # torch/nn/functional.py:1993:22
          %size_prods.527 : int = aten::mul(%size_prods.526, %2841) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.527)
      %2843 : bool = aten::eq(%size_prods.525, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2843) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2844 : Tensor = aten::batch_norm(%bottleneck_output.126, %2831, %2832, %2829, %2830, %2828, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.128 : Tensor = aten::relu_(%2844) # torch/nn/functional.py:1117:17
  %2846 : Tensor = prim::GetAttr[name="weight"](%2821)
  %2847 : Tensor? = prim::GetAttr[name="bias"](%2821)
  %2848 : int[] = prim::ListConstruct(%27, %27)
  %2849 : int[] = prim::ListConstruct(%27, %27)
  %2850 : int[] = prim::ListConstruct(%27, %27)
  %new_features.127 : Tensor = aten::conv2d(%result.128, %2846, %2847, %2848, %2849, %2850, %27) # torch/nn/modules/conv.py:415:15
  %2852 : float = prim::GetAttr[name="drop_rate"](%1885)
  %2853 : bool = aten::gt(%2852, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.128 : Tensor = prim::If(%2853) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2855 : float = prim::GetAttr[name="drop_rate"](%1885)
      %2856 : bool = prim::GetAttr[name="training"](%1885)
      %2857 : bool = aten::lt(%2855, %16) # torch/nn/functional.py:968:7
      %2858 : bool = prim::If(%2857) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2859 : bool = aten::gt(%2855, %17) # torch/nn/functional.py:968:17
          -> (%2859)
       = prim::If(%2858) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2860 : Tensor = aten::dropout(%new_features.127, %2855, %2856) # torch/nn/functional.py:973:17
      -> (%2860)
    block1():
      -> (%new_features.127)
  %2861 : Tensor[] = aten::append(%features.4, %new_features.128) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2862 : Tensor = prim::Uninitialized()
  %2863 : bool = prim::GetAttr[name="memory_efficient"](%1886)
  %2864 : bool = prim::If(%2863) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2865 : bool = prim::Uninitialized()
      %2866 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2867 : bool = aten::gt(%2866, %24)
      %2868 : bool, %2869 : bool, %2870 : int = prim::Loop(%18, %2867, %19, %2865, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2871 : int, %2872 : bool, %2873 : bool, %2874 : int):
          %tensor.65 : Tensor = aten::__getitem__(%features.4, %2874) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2876 : bool = prim::requires_grad(%tensor.65)
          %2877 : bool, %2878 : bool = prim::If(%2876) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2865)
          %2879 : int = aten::add(%2874, %27)
          %2880 : bool = aten::lt(%2879, %2866)
          %2881 : bool = aten::__and__(%2880, %2877)
          -> (%2881, %2876, %2878, %2879)
      %2882 : bool = prim::If(%2868)
        block0():
          -> (%2869)
        block1():
          -> (%19)
      -> (%2882)
    block1():
      -> (%19)
  %bottleneck_output.128 : Tensor = prim::If(%2864) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2862)
    block1():
      %concated_features.65 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2885 : __torch__.torch.nn.modules.conv.___torch_mangle_209.Conv2d = prim::GetAttr[name="conv1"](%1886)
      %2886 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_142.BatchNorm2d = prim::GetAttr[name="norm1"](%1886)
      %2887 : int = aten::dim(%concated_features.65) # torch/nn/modules/batchnorm.py:276:11
      %2888 : bool = aten::ne(%2887, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2888) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2889 : bool = prim::GetAttr[name="training"](%2886)
       = prim::If(%2889) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2890 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2886)
          %2891 : Tensor = aten::add(%2890, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2886, %2891)
          -> ()
        block1():
          -> ()
      %2892 : bool = prim::GetAttr[name="training"](%2886)
      %2893 : Tensor = prim::GetAttr[name="running_mean"](%2886)
      %2894 : Tensor = prim::GetAttr[name="running_var"](%2886)
      %2895 : Tensor = prim::GetAttr[name="weight"](%2886)
      %2896 : Tensor = prim::GetAttr[name="bias"](%2886)
       = prim::If(%2892) # torch/nn/functional.py:2011:4
        block0():
          %2897 : int[] = aten::size(%concated_features.65) # torch/nn/functional.py:2012:27
          %size_prods.528 : int = aten::__getitem__(%2897, %24) # torch/nn/functional.py:1991:17
          %2899 : int = aten::len(%2897) # torch/nn/functional.py:1992:19
          %2900 : int = aten::sub(%2899, %26) # torch/nn/functional.py:1992:19
          %size_prods.529 : int = prim::Loop(%2900, %25, %size_prods.528) # torch/nn/functional.py:1992:4
            block0(%i.133 : int, %size_prods.530 : int):
              %2904 : int = aten::add(%i.133, %26) # torch/nn/functional.py:1993:27
              %2905 : int = aten::__getitem__(%2897, %2904) # torch/nn/functional.py:1993:22
              %size_prods.531 : int = aten::mul(%size_prods.530, %2905) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.531)
          %2907 : bool = aten::eq(%size_prods.529, %27) # torch/nn/functional.py:1994:7
           = prim::If(%2907) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %2908 : Tensor = aten::batch_norm(%concated_features.65, %2895, %2896, %2893, %2894, %2892, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.129 : Tensor = aten::relu_(%2908) # torch/nn/functional.py:1117:17
      %2910 : Tensor = prim::GetAttr[name="weight"](%2885)
      %2911 : Tensor? = prim::GetAttr[name="bias"](%2885)
      %2912 : int[] = prim::ListConstruct(%27, %27)
      %2913 : int[] = prim::ListConstruct(%24, %24)
      %2914 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.129 : Tensor = aten::conv2d(%result.129, %2910, %2911, %2912, %2913, %2914, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.129)
  %2916 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1886)
  %2917 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1886)
  %2918 : int = aten::dim(%bottleneck_output.128) # torch/nn/modules/batchnorm.py:276:11
  %2919 : bool = aten::ne(%2918, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%2919) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %2920 : bool = prim::GetAttr[name="training"](%2917)
   = prim::If(%2920) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %2921 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2917)
      %2922 : Tensor = aten::add(%2921, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%2917, %2922)
      -> ()
    block1():
      -> ()
  %2923 : bool = prim::GetAttr[name="training"](%2917)
  %2924 : Tensor = prim::GetAttr[name="running_mean"](%2917)
  %2925 : Tensor = prim::GetAttr[name="running_var"](%2917)
  %2926 : Tensor = prim::GetAttr[name="weight"](%2917)
  %2927 : Tensor = prim::GetAttr[name="bias"](%2917)
   = prim::If(%2923) # torch/nn/functional.py:2011:4
    block0():
      %2928 : int[] = aten::size(%bottleneck_output.128) # torch/nn/functional.py:2012:27
      %size_prods.532 : int = aten::__getitem__(%2928, %24) # torch/nn/functional.py:1991:17
      %2930 : int = aten::len(%2928) # torch/nn/functional.py:1992:19
      %2931 : int = aten::sub(%2930, %26) # torch/nn/functional.py:1992:19
      %size_prods.533 : int = prim::Loop(%2931, %25, %size_prods.532) # torch/nn/functional.py:1992:4
        block0(%i.134 : int, %size_prods.534 : int):
          %2935 : int = aten::add(%i.134, %26) # torch/nn/functional.py:1993:27
          %2936 : int = aten::__getitem__(%2928, %2935) # torch/nn/functional.py:1993:22
          %size_prods.535 : int = aten::mul(%size_prods.534, %2936) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.535)
      %2938 : bool = aten::eq(%size_prods.533, %27) # torch/nn/functional.py:1994:7
       = prim::If(%2938) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %2939 : Tensor = aten::batch_norm(%bottleneck_output.128, %2926, %2927, %2924, %2925, %2923, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.130 : Tensor = aten::relu_(%2939) # torch/nn/functional.py:1117:17
  %2941 : Tensor = prim::GetAttr[name="weight"](%2916)
  %2942 : Tensor? = prim::GetAttr[name="bias"](%2916)
  %2943 : int[] = prim::ListConstruct(%27, %27)
  %2944 : int[] = prim::ListConstruct(%27, %27)
  %2945 : int[] = prim::ListConstruct(%27, %27)
  %new_features.129 : Tensor = aten::conv2d(%result.130, %2941, %2942, %2943, %2944, %2945, %27) # torch/nn/modules/conv.py:415:15
  %2947 : float = prim::GetAttr[name="drop_rate"](%1886)
  %2948 : bool = aten::gt(%2947, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.130 : Tensor = prim::If(%2948) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %2950 : float = prim::GetAttr[name="drop_rate"](%1886)
      %2951 : bool = prim::GetAttr[name="training"](%1886)
      %2952 : bool = aten::lt(%2950, %16) # torch/nn/functional.py:968:7
      %2953 : bool = prim::If(%2952) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %2954 : bool = aten::gt(%2950, %17) # torch/nn/functional.py:968:17
          -> (%2954)
       = prim::If(%2953) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %2955 : Tensor = aten::dropout(%new_features.129, %2950, %2951) # torch/nn/functional.py:973:17
      -> (%2955)
    block1():
      -> (%new_features.129)
  %2956 : Tensor[] = aten::append(%features.4, %new_features.130) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %2957 : Tensor = prim::Uninitialized()
  %2958 : bool = prim::GetAttr[name="memory_efficient"](%1887)
  %2959 : bool = prim::If(%2958) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %2960 : bool = prim::Uninitialized()
      %2961 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %2962 : bool = aten::gt(%2961, %24)
      %2963 : bool, %2964 : bool, %2965 : int = prim::Loop(%18, %2962, %19, %2960, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%2966 : int, %2967 : bool, %2968 : bool, %2969 : int):
          %tensor.66 : Tensor = aten::__getitem__(%features.4, %2969) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %2971 : bool = prim::requires_grad(%tensor.66)
          %2972 : bool, %2973 : bool = prim::If(%2971) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %2960)
          %2974 : int = aten::add(%2969, %27)
          %2975 : bool = aten::lt(%2974, %2961)
          %2976 : bool = aten::__and__(%2975, %2972)
          -> (%2976, %2971, %2973, %2974)
      %2977 : bool = prim::If(%2963)
        block0():
          -> (%2964)
        block1():
          -> (%19)
      -> (%2977)
    block1():
      -> (%19)
  %bottleneck_output.130 : Tensor = prim::If(%2959) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%2957)
    block1():
      %concated_features.66 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %2980 : __torch__.torch.nn.modules.conv.___torch_mangle_212.Conv2d = prim::GetAttr[name="conv1"](%1887)
      %2981 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_211.BatchNorm2d = prim::GetAttr[name="norm1"](%1887)
      %2982 : int = aten::dim(%concated_features.66) # torch/nn/modules/batchnorm.py:276:11
      %2983 : bool = aten::ne(%2982, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%2983) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %2984 : bool = prim::GetAttr[name="training"](%2981)
       = prim::If(%2984) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %2985 : Tensor = prim::GetAttr[name="num_batches_tracked"](%2981)
          %2986 : Tensor = aten::add(%2985, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%2981, %2986)
          -> ()
        block1():
          -> ()
      %2987 : bool = prim::GetAttr[name="training"](%2981)
      %2988 : Tensor = prim::GetAttr[name="running_mean"](%2981)
      %2989 : Tensor = prim::GetAttr[name="running_var"](%2981)
      %2990 : Tensor = prim::GetAttr[name="weight"](%2981)
      %2991 : Tensor = prim::GetAttr[name="bias"](%2981)
       = prim::If(%2987) # torch/nn/functional.py:2011:4
        block0():
          %2992 : int[] = aten::size(%concated_features.66) # torch/nn/functional.py:2012:27
          %size_prods.536 : int = aten::__getitem__(%2992, %24) # torch/nn/functional.py:1991:17
          %2994 : int = aten::len(%2992) # torch/nn/functional.py:1992:19
          %2995 : int = aten::sub(%2994, %26) # torch/nn/functional.py:1992:19
          %size_prods.537 : int = prim::Loop(%2995, %25, %size_prods.536) # torch/nn/functional.py:1992:4
            block0(%i.135 : int, %size_prods.538 : int):
              %2999 : int = aten::add(%i.135, %26) # torch/nn/functional.py:1993:27
              %3000 : int = aten::__getitem__(%2992, %2999) # torch/nn/functional.py:1993:22
              %size_prods.539 : int = aten::mul(%size_prods.538, %3000) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.539)
          %3002 : bool = aten::eq(%size_prods.537, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3002) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3003 : Tensor = aten::batch_norm(%concated_features.66, %2990, %2991, %2988, %2989, %2987, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.131 : Tensor = aten::relu_(%3003) # torch/nn/functional.py:1117:17
      %3005 : Tensor = prim::GetAttr[name="weight"](%2980)
      %3006 : Tensor? = prim::GetAttr[name="bias"](%2980)
      %3007 : int[] = prim::ListConstruct(%27, %27)
      %3008 : int[] = prim::ListConstruct(%24, %24)
      %3009 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.131 : Tensor = aten::conv2d(%result.131, %3005, %3006, %3007, %3008, %3009, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.131)
  %3011 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1887)
  %3012 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1887)
  %3013 : int = aten::dim(%bottleneck_output.130) # torch/nn/modules/batchnorm.py:276:11
  %3014 : bool = aten::ne(%3013, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3014) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3015 : bool = prim::GetAttr[name="training"](%3012)
   = prim::If(%3015) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3016 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3012)
      %3017 : Tensor = aten::add(%3016, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3012, %3017)
      -> ()
    block1():
      -> ()
  %3018 : bool = prim::GetAttr[name="training"](%3012)
  %3019 : Tensor = prim::GetAttr[name="running_mean"](%3012)
  %3020 : Tensor = prim::GetAttr[name="running_var"](%3012)
  %3021 : Tensor = prim::GetAttr[name="weight"](%3012)
  %3022 : Tensor = prim::GetAttr[name="bias"](%3012)
   = prim::If(%3018) # torch/nn/functional.py:2011:4
    block0():
      %3023 : int[] = aten::size(%bottleneck_output.130) # torch/nn/functional.py:2012:27
      %size_prods.540 : int = aten::__getitem__(%3023, %24) # torch/nn/functional.py:1991:17
      %3025 : int = aten::len(%3023) # torch/nn/functional.py:1992:19
      %3026 : int = aten::sub(%3025, %26) # torch/nn/functional.py:1992:19
      %size_prods.541 : int = prim::Loop(%3026, %25, %size_prods.540) # torch/nn/functional.py:1992:4
        block0(%i.136 : int, %size_prods.542 : int):
          %3030 : int = aten::add(%i.136, %26) # torch/nn/functional.py:1993:27
          %3031 : int = aten::__getitem__(%3023, %3030) # torch/nn/functional.py:1993:22
          %size_prods.543 : int = aten::mul(%size_prods.542, %3031) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.543)
      %3033 : bool = aten::eq(%size_prods.541, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3033) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3034 : Tensor = aten::batch_norm(%bottleneck_output.130, %3021, %3022, %3019, %3020, %3018, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.132 : Tensor = aten::relu_(%3034) # torch/nn/functional.py:1117:17
  %3036 : Tensor = prim::GetAttr[name="weight"](%3011)
  %3037 : Tensor? = prim::GetAttr[name="bias"](%3011)
  %3038 : int[] = prim::ListConstruct(%27, %27)
  %3039 : int[] = prim::ListConstruct(%27, %27)
  %3040 : int[] = prim::ListConstruct(%27, %27)
  %new_features.131 : Tensor = aten::conv2d(%result.132, %3036, %3037, %3038, %3039, %3040, %27) # torch/nn/modules/conv.py:415:15
  %3042 : float = prim::GetAttr[name="drop_rate"](%1887)
  %3043 : bool = aten::gt(%3042, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.132 : Tensor = prim::If(%3043) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3045 : float = prim::GetAttr[name="drop_rate"](%1887)
      %3046 : bool = prim::GetAttr[name="training"](%1887)
      %3047 : bool = aten::lt(%3045, %16) # torch/nn/functional.py:968:7
      %3048 : bool = prim::If(%3047) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3049 : bool = aten::gt(%3045, %17) # torch/nn/functional.py:968:17
          -> (%3049)
       = prim::If(%3048) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3050 : Tensor = aten::dropout(%new_features.131, %3045, %3046) # torch/nn/functional.py:973:17
      -> (%3050)
    block1():
      -> (%new_features.131)
  %3051 : Tensor[] = aten::append(%features.4, %new_features.132) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3052 : Tensor = prim::Uninitialized()
  %3053 : bool = prim::GetAttr[name="memory_efficient"](%1888)
  %3054 : bool = prim::If(%3053) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3055 : bool = prim::Uninitialized()
      %3056 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3057 : bool = aten::gt(%3056, %24)
      %3058 : bool, %3059 : bool, %3060 : int = prim::Loop(%18, %3057, %19, %3055, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3061 : int, %3062 : bool, %3063 : bool, %3064 : int):
          %tensor.67 : Tensor = aten::__getitem__(%features.4, %3064) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3066 : bool = prim::requires_grad(%tensor.67)
          %3067 : bool, %3068 : bool = prim::If(%3066) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3055)
          %3069 : int = aten::add(%3064, %27)
          %3070 : bool = aten::lt(%3069, %3056)
          %3071 : bool = aten::__and__(%3070, %3067)
          -> (%3071, %3066, %3068, %3069)
      %3072 : bool = prim::If(%3058)
        block0():
          -> (%3059)
        block1():
          -> (%19)
      -> (%3072)
    block1():
      -> (%19)
  %bottleneck_output.132 : Tensor = prim::If(%3054) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3052)
    block1():
      %concated_features.67 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3075 : __torch__.torch.nn.modules.conv.___torch_mangle_214.Conv2d = prim::GetAttr[name="conv1"](%1888)
      %3076 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_151.BatchNorm2d = prim::GetAttr[name="norm1"](%1888)
      %3077 : int = aten::dim(%concated_features.67) # torch/nn/modules/batchnorm.py:276:11
      %3078 : bool = aten::ne(%3077, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3078) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3079 : bool = prim::GetAttr[name="training"](%3076)
       = prim::If(%3079) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3080 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3076)
          %3081 : Tensor = aten::add(%3080, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3076, %3081)
          -> ()
        block1():
          -> ()
      %3082 : bool = prim::GetAttr[name="training"](%3076)
      %3083 : Tensor = prim::GetAttr[name="running_mean"](%3076)
      %3084 : Tensor = prim::GetAttr[name="running_var"](%3076)
      %3085 : Tensor = prim::GetAttr[name="weight"](%3076)
      %3086 : Tensor = prim::GetAttr[name="bias"](%3076)
       = prim::If(%3082) # torch/nn/functional.py:2011:4
        block0():
          %3087 : int[] = aten::size(%concated_features.67) # torch/nn/functional.py:2012:27
          %size_prods.544 : int = aten::__getitem__(%3087, %24) # torch/nn/functional.py:1991:17
          %3089 : int = aten::len(%3087) # torch/nn/functional.py:1992:19
          %3090 : int = aten::sub(%3089, %26) # torch/nn/functional.py:1992:19
          %size_prods.545 : int = prim::Loop(%3090, %25, %size_prods.544) # torch/nn/functional.py:1992:4
            block0(%i.137 : int, %size_prods.546 : int):
              %3094 : int = aten::add(%i.137, %26) # torch/nn/functional.py:1993:27
              %3095 : int = aten::__getitem__(%3087, %3094) # torch/nn/functional.py:1993:22
              %size_prods.547 : int = aten::mul(%size_prods.546, %3095) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.547)
          %3097 : bool = aten::eq(%size_prods.545, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3097) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3098 : Tensor = aten::batch_norm(%concated_features.67, %3085, %3086, %3083, %3084, %3082, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.133 : Tensor = aten::relu_(%3098) # torch/nn/functional.py:1117:17
      %3100 : Tensor = prim::GetAttr[name="weight"](%3075)
      %3101 : Tensor? = prim::GetAttr[name="bias"](%3075)
      %3102 : int[] = prim::ListConstruct(%27, %27)
      %3103 : int[] = prim::ListConstruct(%24, %24)
      %3104 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.133 : Tensor = aten::conv2d(%result.133, %3100, %3101, %3102, %3103, %3104, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.133)
  %3106 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1888)
  %3107 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1888)
  %3108 : int = aten::dim(%bottleneck_output.132) # torch/nn/modules/batchnorm.py:276:11
  %3109 : bool = aten::ne(%3108, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3109) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3110 : bool = prim::GetAttr[name="training"](%3107)
   = prim::If(%3110) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3111 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3107)
      %3112 : Tensor = aten::add(%3111, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3107, %3112)
      -> ()
    block1():
      -> ()
  %3113 : bool = prim::GetAttr[name="training"](%3107)
  %3114 : Tensor = prim::GetAttr[name="running_mean"](%3107)
  %3115 : Tensor = prim::GetAttr[name="running_var"](%3107)
  %3116 : Tensor = prim::GetAttr[name="weight"](%3107)
  %3117 : Tensor = prim::GetAttr[name="bias"](%3107)
   = prim::If(%3113) # torch/nn/functional.py:2011:4
    block0():
      %3118 : int[] = aten::size(%bottleneck_output.132) # torch/nn/functional.py:2012:27
      %size_prods.548 : int = aten::__getitem__(%3118, %24) # torch/nn/functional.py:1991:17
      %3120 : int = aten::len(%3118) # torch/nn/functional.py:1992:19
      %3121 : int = aten::sub(%3120, %26) # torch/nn/functional.py:1992:19
      %size_prods.549 : int = prim::Loop(%3121, %25, %size_prods.548) # torch/nn/functional.py:1992:4
        block0(%i.138 : int, %size_prods.550 : int):
          %3125 : int = aten::add(%i.138, %26) # torch/nn/functional.py:1993:27
          %3126 : int = aten::__getitem__(%3118, %3125) # torch/nn/functional.py:1993:22
          %size_prods.551 : int = aten::mul(%size_prods.550, %3126) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.551)
      %3128 : bool = aten::eq(%size_prods.549, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3128) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3129 : Tensor = aten::batch_norm(%bottleneck_output.132, %3116, %3117, %3114, %3115, %3113, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.134 : Tensor = aten::relu_(%3129) # torch/nn/functional.py:1117:17
  %3131 : Tensor = prim::GetAttr[name="weight"](%3106)
  %3132 : Tensor? = prim::GetAttr[name="bias"](%3106)
  %3133 : int[] = prim::ListConstruct(%27, %27)
  %3134 : int[] = prim::ListConstruct(%27, %27)
  %3135 : int[] = prim::ListConstruct(%27, %27)
  %new_features.133 : Tensor = aten::conv2d(%result.134, %3131, %3132, %3133, %3134, %3135, %27) # torch/nn/modules/conv.py:415:15
  %3137 : float = prim::GetAttr[name="drop_rate"](%1888)
  %3138 : bool = aten::gt(%3137, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.134 : Tensor = prim::If(%3138) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3140 : float = prim::GetAttr[name="drop_rate"](%1888)
      %3141 : bool = prim::GetAttr[name="training"](%1888)
      %3142 : bool = aten::lt(%3140, %16) # torch/nn/functional.py:968:7
      %3143 : bool = prim::If(%3142) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3144 : bool = aten::gt(%3140, %17) # torch/nn/functional.py:968:17
          -> (%3144)
       = prim::If(%3143) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3145 : Tensor = aten::dropout(%new_features.133, %3140, %3141) # torch/nn/functional.py:973:17
      -> (%3145)
    block1():
      -> (%new_features.133)
  %3146 : Tensor[] = aten::append(%features.4, %new_features.134) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3147 : Tensor = prim::Uninitialized()
  %3148 : bool = prim::GetAttr[name="memory_efficient"](%1889)
  %3149 : bool = prim::If(%3148) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3150 : bool = prim::Uninitialized()
      %3151 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3152 : bool = aten::gt(%3151, %24)
      %3153 : bool, %3154 : bool, %3155 : int = prim::Loop(%18, %3152, %19, %3150, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3156 : int, %3157 : bool, %3158 : bool, %3159 : int):
          %tensor.68 : Tensor = aten::__getitem__(%features.4, %3159) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3161 : bool = prim::requires_grad(%tensor.68)
          %3162 : bool, %3163 : bool = prim::If(%3161) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3150)
          %3164 : int = aten::add(%3159, %27)
          %3165 : bool = aten::lt(%3164, %3151)
          %3166 : bool = aten::__and__(%3165, %3162)
          -> (%3166, %3161, %3163, %3164)
      %3167 : bool = prim::If(%3153)
        block0():
          -> (%3154)
        block1():
          -> (%19)
      -> (%3167)
    block1():
      -> (%19)
  %bottleneck_output.134 : Tensor = prim::If(%3149) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3147)
    block1():
      %concated_features.68 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3170 : __torch__.torch.nn.modules.conv.___torch_mangle_217.Conv2d = prim::GetAttr[name="conv1"](%1889)
      %3171 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_216.BatchNorm2d = prim::GetAttr[name="norm1"](%1889)
      %3172 : int = aten::dim(%concated_features.68) # torch/nn/modules/batchnorm.py:276:11
      %3173 : bool = aten::ne(%3172, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3173) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3174 : bool = prim::GetAttr[name="training"](%3171)
       = prim::If(%3174) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3175 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3171)
          %3176 : Tensor = aten::add(%3175, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3171, %3176)
          -> ()
        block1():
          -> ()
      %3177 : bool = prim::GetAttr[name="training"](%3171)
      %3178 : Tensor = prim::GetAttr[name="running_mean"](%3171)
      %3179 : Tensor = prim::GetAttr[name="running_var"](%3171)
      %3180 : Tensor = prim::GetAttr[name="weight"](%3171)
      %3181 : Tensor = prim::GetAttr[name="bias"](%3171)
       = prim::If(%3177) # torch/nn/functional.py:2011:4
        block0():
          %3182 : int[] = aten::size(%concated_features.68) # torch/nn/functional.py:2012:27
          %size_prods.552 : int = aten::__getitem__(%3182, %24) # torch/nn/functional.py:1991:17
          %3184 : int = aten::len(%3182) # torch/nn/functional.py:1992:19
          %3185 : int = aten::sub(%3184, %26) # torch/nn/functional.py:1992:19
          %size_prods.553 : int = prim::Loop(%3185, %25, %size_prods.552) # torch/nn/functional.py:1992:4
            block0(%i.139 : int, %size_prods.554 : int):
              %3189 : int = aten::add(%i.139, %26) # torch/nn/functional.py:1993:27
              %3190 : int = aten::__getitem__(%3182, %3189) # torch/nn/functional.py:1993:22
              %size_prods.555 : int = aten::mul(%size_prods.554, %3190) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.555)
          %3192 : bool = aten::eq(%size_prods.553, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3192) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3193 : Tensor = aten::batch_norm(%concated_features.68, %3180, %3181, %3178, %3179, %3177, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.135 : Tensor = aten::relu_(%3193) # torch/nn/functional.py:1117:17
      %3195 : Tensor = prim::GetAttr[name="weight"](%3170)
      %3196 : Tensor? = prim::GetAttr[name="bias"](%3170)
      %3197 : int[] = prim::ListConstruct(%27, %27)
      %3198 : int[] = prim::ListConstruct(%24, %24)
      %3199 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.135 : Tensor = aten::conv2d(%result.135, %3195, %3196, %3197, %3198, %3199, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.135)
  %3201 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1889)
  %3202 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1889)
  %3203 : int = aten::dim(%bottleneck_output.134) # torch/nn/modules/batchnorm.py:276:11
  %3204 : bool = aten::ne(%3203, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3204) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3205 : bool = prim::GetAttr[name="training"](%3202)
   = prim::If(%3205) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3206 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3202)
      %3207 : Tensor = aten::add(%3206, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3202, %3207)
      -> ()
    block1():
      -> ()
  %3208 : bool = prim::GetAttr[name="training"](%3202)
  %3209 : Tensor = prim::GetAttr[name="running_mean"](%3202)
  %3210 : Tensor = prim::GetAttr[name="running_var"](%3202)
  %3211 : Tensor = prim::GetAttr[name="weight"](%3202)
  %3212 : Tensor = prim::GetAttr[name="bias"](%3202)
   = prim::If(%3208) # torch/nn/functional.py:2011:4
    block0():
      %3213 : int[] = aten::size(%bottleneck_output.134) # torch/nn/functional.py:2012:27
      %size_prods.556 : int = aten::__getitem__(%3213, %24) # torch/nn/functional.py:1991:17
      %3215 : int = aten::len(%3213) # torch/nn/functional.py:1992:19
      %3216 : int = aten::sub(%3215, %26) # torch/nn/functional.py:1992:19
      %size_prods.557 : int = prim::Loop(%3216, %25, %size_prods.556) # torch/nn/functional.py:1992:4
        block0(%i.140 : int, %size_prods.558 : int):
          %3220 : int = aten::add(%i.140, %26) # torch/nn/functional.py:1993:27
          %3221 : int = aten::__getitem__(%3213, %3220) # torch/nn/functional.py:1993:22
          %size_prods.559 : int = aten::mul(%size_prods.558, %3221) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.559)
      %3223 : bool = aten::eq(%size_prods.557, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3223) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3224 : Tensor = aten::batch_norm(%bottleneck_output.134, %3211, %3212, %3209, %3210, %3208, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.136 : Tensor = aten::relu_(%3224) # torch/nn/functional.py:1117:17
  %3226 : Tensor = prim::GetAttr[name="weight"](%3201)
  %3227 : Tensor? = prim::GetAttr[name="bias"](%3201)
  %3228 : int[] = prim::ListConstruct(%27, %27)
  %3229 : int[] = prim::ListConstruct(%27, %27)
  %3230 : int[] = prim::ListConstruct(%27, %27)
  %new_features.135 : Tensor = aten::conv2d(%result.136, %3226, %3227, %3228, %3229, %3230, %27) # torch/nn/modules/conv.py:415:15
  %3232 : float = prim::GetAttr[name="drop_rate"](%1889)
  %3233 : bool = aten::gt(%3232, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.136 : Tensor = prim::If(%3233) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3235 : float = prim::GetAttr[name="drop_rate"](%1889)
      %3236 : bool = prim::GetAttr[name="training"](%1889)
      %3237 : bool = aten::lt(%3235, %16) # torch/nn/functional.py:968:7
      %3238 : bool = prim::If(%3237) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3239 : bool = aten::gt(%3235, %17) # torch/nn/functional.py:968:17
          -> (%3239)
       = prim::If(%3238) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3240 : Tensor = aten::dropout(%new_features.135, %3235, %3236) # torch/nn/functional.py:973:17
      -> (%3240)
    block1():
      -> (%new_features.135)
  %3241 : Tensor[] = aten::append(%features.4, %new_features.136) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3242 : Tensor = prim::Uninitialized()
  %3243 : bool = prim::GetAttr[name="memory_efficient"](%1890)
  %3244 : bool = prim::If(%3243) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3245 : bool = prim::Uninitialized()
      %3246 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3247 : bool = aten::gt(%3246, %24)
      %3248 : bool, %3249 : bool, %3250 : int = prim::Loop(%18, %3247, %19, %3245, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3251 : int, %3252 : bool, %3253 : bool, %3254 : int):
          %tensor.69 : Tensor = aten::__getitem__(%features.4, %3254) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3256 : bool = prim::requires_grad(%tensor.69)
          %3257 : bool, %3258 : bool = prim::If(%3256) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3245)
          %3259 : int = aten::add(%3254, %27)
          %3260 : bool = aten::lt(%3259, %3246)
          %3261 : bool = aten::__and__(%3260, %3257)
          -> (%3261, %3256, %3258, %3259)
      %3262 : bool = prim::If(%3248)
        block0():
          -> (%3249)
        block1():
          -> (%19)
      -> (%3262)
    block1():
      -> (%19)
  %bottleneck_output.136 : Tensor = prim::If(%3244) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3242)
    block1():
      %concated_features.69 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3265 : __torch__.torch.nn.modules.conv.___torch_mangle_220.Conv2d = prim::GetAttr[name="conv1"](%1890)
      %3266 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%1890)
      %3267 : int = aten::dim(%concated_features.69) # torch/nn/modules/batchnorm.py:276:11
      %3268 : bool = aten::ne(%3267, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3268) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3269 : bool = prim::GetAttr[name="training"](%3266)
       = prim::If(%3269) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3270 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3266)
          %3271 : Tensor = aten::add(%3270, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3266, %3271)
          -> ()
        block1():
          -> ()
      %3272 : bool = prim::GetAttr[name="training"](%3266)
      %3273 : Tensor = prim::GetAttr[name="running_mean"](%3266)
      %3274 : Tensor = prim::GetAttr[name="running_var"](%3266)
      %3275 : Tensor = prim::GetAttr[name="weight"](%3266)
      %3276 : Tensor = prim::GetAttr[name="bias"](%3266)
       = prim::If(%3272) # torch/nn/functional.py:2011:4
        block0():
          %3277 : int[] = aten::size(%concated_features.69) # torch/nn/functional.py:2012:27
          %size_prods.560 : int = aten::__getitem__(%3277, %24) # torch/nn/functional.py:1991:17
          %3279 : int = aten::len(%3277) # torch/nn/functional.py:1992:19
          %3280 : int = aten::sub(%3279, %26) # torch/nn/functional.py:1992:19
          %size_prods.561 : int = prim::Loop(%3280, %25, %size_prods.560) # torch/nn/functional.py:1992:4
            block0(%i.141 : int, %size_prods.562 : int):
              %3284 : int = aten::add(%i.141, %26) # torch/nn/functional.py:1993:27
              %3285 : int = aten::__getitem__(%3277, %3284) # torch/nn/functional.py:1993:22
              %size_prods.563 : int = aten::mul(%size_prods.562, %3285) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.563)
          %3287 : bool = aten::eq(%size_prods.561, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3287) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3288 : Tensor = aten::batch_norm(%concated_features.69, %3275, %3276, %3273, %3274, %3272, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.137 : Tensor = aten::relu_(%3288) # torch/nn/functional.py:1117:17
      %3290 : Tensor = prim::GetAttr[name="weight"](%3265)
      %3291 : Tensor? = prim::GetAttr[name="bias"](%3265)
      %3292 : int[] = prim::ListConstruct(%27, %27)
      %3293 : int[] = prim::ListConstruct(%24, %24)
      %3294 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.137 : Tensor = aten::conv2d(%result.137, %3290, %3291, %3292, %3293, %3294, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.137)
  %3296 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1890)
  %3297 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1890)
  %3298 : int = aten::dim(%bottleneck_output.136) # torch/nn/modules/batchnorm.py:276:11
  %3299 : bool = aten::ne(%3298, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3299) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3300 : bool = prim::GetAttr[name="training"](%3297)
   = prim::If(%3300) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3301 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3297)
      %3302 : Tensor = aten::add(%3301, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3297, %3302)
      -> ()
    block1():
      -> ()
  %3303 : bool = prim::GetAttr[name="training"](%3297)
  %3304 : Tensor = prim::GetAttr[name="running_mean"](%3297)
  %3305 : Tensor = prim::GetAttr[name="running_var"](%3297)
  %3306 : Tensor = prim::GetAttr[name="weight"](%3297)
  %3307 : Tensor = prim::GetAttr[name="bias"](%3297)
   = prim::If(%3303) # torch/nn/functional.py:2011:4
    block0():
      %3308 : int[] = aten::size(%bottleneck_output.136) # torch/nn/functional.py:2012:27
      %size_prods.564 : int = aten::__getitem__(%3308, %24) # torch/nn/functional.py:1991:17
      %3310 : int = aten::len(%3308) # torch/nn/functional.py:1992:19
      %3311 : int = aten::sub(%3310, %26) # torch/nn/functional.py:1992:19
      %size_prods.565 : int = prim::Loop(%3311, %25, %size_prods.564) # torch/nn/functional.py:1992:4
        block0(%i.142 : int, %size_prods.566 : int):
          %3315 : int = aten::add(%i.142, %26) # torch/nn/functional.py:1993:27
          %3316 : int = aten::__getitem__(%3308, %3315) # torch/nn/functional.py:1993:22
          %size_prods.567 : int = aten::mul(%size_prods.566, %3316) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.567)
      %3318 : bool = aten::eq(%size_prods.565, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3318) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3319 : Tensor = aten::batch_norm(%bottleneck_output.136, %3306, %3307, %3304, %3305, %3303, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.138 : Tensor = aten::relu_(%3319) # torch/nn/functional.py:1117:17
  %3321 : Tensor = prim::GetAttr[name="weight"](%3296)
  %3322 : Tensor? = prim::GetAttr[name="bias"](%3296)
  %3323 : int[] = prim::ListConstruct(%27, %27)
  %3324 : int[] = prim::ListConstruct(%27, %27)
  %3325 : int[] = prim::ListConstruct(%27, %27)
  %new_features.137 : Tensor = aten::conv2d(%result.138, %3321, %3322, %3323, %3324, %3325, %27) # torch/nn/modules/conv.py:415:15
  %3327 : float = prim::GetAttr[name="drop_rate"](%1890)
  %3328 : bool = aten::gt(%3327, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.138 : Tensor = prim::If(%3328) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3330 : float = prim::GetAttr[name="drop_rate"](%1890)
      %3331 : bool = prim::GetAttr[name="training"](%1890)
      %3332 : bool = aten::lt(%3330, %16) # torch/nn/functional.py:968:7
      %3333 : bool = prim::If(%3332) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3334 : bool = aten::gt(%3330, %17) # torch/nn/functional.py:968:17
          -> (%3334)
       = prim::If(%3333) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3335 : Tensor = aten::dropout(%new_features.137, %3330, %3331) # torch/nn/functional.py:973:17
      -> (%3335)
    block1():
      -> (%new_features.137)
  %3336 : Tensor[] = aten::append(%features.4, %new_features.138) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3337 : Tensor = prim::Uninitialized()
  %3338 : bool = prim::GetAttr[name="memory_efficient"](%1891)
  %3339 : bool = prim::If(%3338) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3340 : bool = prim::Uninitialized()
      %3341 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3342 : bool = aten::gt(%3341, %24)
      %3343 : bool, %3344 : bool, %3345 : int = prim::Loop(%18, %3342, %19, %3340, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3346 : int, %3347 : bool, %3348 : bool, %3349 : int):
          %tensor.70 : Tensor = aten::__getitem__(%features.4, %3349) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3351 : bool = prim::requires_grad(%tensor.70)
          %3352 : bool, %3353 : bool = prim::If(%3351) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3340)
          %3354 : int = aten::add(%3349, %27)
          %3355 : bool = aten::lt(%3354, %3341)
          %3356 : bool = aten::__and__(%3355, %3352)
          -> (%3356, %3351, %3353, %3354)
      %3357 : bool = prim::If(%3343)
        block0():
          -> (%3344)
        block1():
          -> (%19)
      -> (%3357)
    block1():
      -> (%19)
  %bottleneck_output.138 : Tensor = prim::If(%3339) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3337)
    block1():
      %concated_features.70 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3360 : __torch__.torch.nn.modules.conv.___torch_mangle_223.Conv2d = prim::GetAttr[name="conv1"](%1891)
      %3361 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_222.BatchNorm2d = prim::GetAttr[name="norm1"](%1891)
      %3362 : int = aten::dim(%concated_features.70) # torch/nn/modules/batchnorm.py:276:11
      %3363 : bool = aten::ne(%3362, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3363) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3364 : bool = prim::GetAttr[name="training"](%3361)
       = prim::If(%3364) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3365 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3361)
          %3366 : Tensor = aten::add(%3365, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3361, %3366)
          -> ()
        block1():
          -> ()
      %3367 : bool = prim::GetAttr[name="training"](%3361)
      %3368 : Tensor = prim::GetAttr[name="running_mean"](%3361)
      %3369 : Tensor = prim::GetAttr[name="running_var"](%3361)
      %3370 : Tensor = prim::GetAttr[name="weight"](%3361)
      %3371 : Tensor = prim::GetAttr[name="bias"](%3361)
       = prim::If(%3367) # torch/nn/functional.py:2011:4
        block0():
          %3372 : int[] = aten::size(%concated_features.70) # torch/nn/functional.py:2012:27
          %size_prods.568 : int = aten::__getitem__(%3372, %24) # torch/nn/functional.py:1991:17
          %3374 : int = aten::len(%3372) # torch/nn/functional.py:1992:19
          %3375 : int = aten::sub(%3374, %26) # torch/nn/functional.py:1992:19
          %size_prods.569 : int = prim::Loop(%3375, %25, %size_prods.568) # torch/nn/functional.py:1992:4
            block0(%i.143 : int, %size_prods.570 : int):
              %3379 : int = aten::add(%i.143, %26) # torch/nn/functional.py:1993:27
              %3380 : int = aten::__getitem__(%3372, %3379) # torch/nn/functional.py:1993:22
              %size_prods.571 : int = aten::mul(%size_prods.570, %3380) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.571)
          %3382 : bool = aten::eq(%size_prods.569, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3382) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3383 : Tensor = aten::batch_norm(%concated_features.70, %3370, %3371, %3368, %3369, %3367, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.139 : Tensor = aten::relu_(%3383) # torch/nn/functional.py:1117:17
      %3385 : Tensor = prim::GetAttr[name="weight"](%3360)
      %3386 : Tensor? = prim::GetAttr[name="bias"](%3360)
      %3387 : int[] = prim::ListConstruct(%27, %27)
      %3388 : int[] = prim::ListConstruct(%24, %24)
      %3389 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.139 : Tensor = aten::conv2d(%result.139, %3385, %3386, %3387, %3388, %3389, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.139)
  %3391 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1891)
  %3392 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1891)
  %3393 : int = aten::dim(%bottleneck_output.138) # torch/nn/modules/batchnorm.py:276:11
  %3394 : bool = aten::ne(%3393, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3394) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3395 : bool = prim::GetAttr[name="training"](%3392)
   = prim::If(%3395) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3396 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3392)
      %3397 : Tensor = aten::add(%3396, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3392, %3397)
      -> ()
    block1():
      -> ()
  %3398 : bool = prim::GetAttr[name="training"](%3392)
  %3399 : Tensor = prim::GetAttr[name="running_mean"](%3392)
  %3400 : Tensor = prim::GetAttr[name="running_var"](%3392)
  %3401 : Tensor = prim::GetAttr[name="weight"](%3392)
  %3402 : Tensor = prim::GetAttr[name="bias"](%3392)
   = prim::If(%3398) # torch/nn/functional.py:2011:4
    block0():
      %3403 : int[] = aten::size(%bottleneck_output.138) # torch/nn/functional.py:2012:27
      %size_prods.572 : int = aten::__getitem__(%3403, %24) # torch/nn/functional.py:1991:17
      %3405 : int = aten::len(%3403) # torch/nn/functional.py:1992:19
      %3406 : int = aten::sub(%3405, %26) # torch/nn/functional.py:1992:19
      %size_prods.573 : int = prim::Loop(%3406, %25, %size_prods.572) # torch/nn/functional.py:1992:4
        block0(%i.144 : int, %size_prods.574 : int):
          %3410 : int = aten::add(%i.144, %26) # torch/nn/functional.py:1993:27
          %3411 : int = aten::__getitem__(%3403, %3410) # torch/nn/functional.py:1993:22
          %size_prods.575 : int = aten::mul(%size_prods.574, %3411) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.575)
      %3413 : bool = aten::eq(%size_prods.573, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3413) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3414 : Tensor = aten::batch_norm(%bottleneck_output.138, %3401, %3402, %3399, %3400, %3398, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.140 : Tensor = aten::relu_(%3414) # torch/nn/functional.py:1117:17
  %3416 : Tensor = prim::GetAttr[name="weight"](%3391)
  %3417 : Tensor? = prim::GetAttr[name="bias"](%3391)
  %3418 : int[] = prim::ListConstruct(%27, %27)
  %3419 : int[] = prim::ListConstruct(%27, %27)
  %3420 : int[] = prim::ListConstruct(%27, %27)
  %new_features.139 : Tensor = aten::conv2d(%result.140, %3416, %3417, %3418, %3419, %3420, %27) # torch/nn/modules/conv.py:415:15
  %3422 : float = prim::GetAttr[name="drop_rate"](%1891)
  %3423 : bool = aten::gt(%3422, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.140 : Tensor = prim::If(%3423) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3425 : float = prim::GetAttr[name="drop_rate"](%1891)
      %3426 : bool = prim::GetAttr[name="training"](%1891)
      %3427 : bool = aten::lt(%3425, %16) # torch/nn/functional.py:968:7
      %3428 : bool = prim::If(%3427) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3429 : bool = aten::gt(%3425, %17) # torch/nn/functional.py:968:17
          -> (%3429)
       = prim::If(%3428) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3430 : Tensor = aten::dropout(%new_features.139, %3425, %3426) # torch/nn/functional.py:973:17
      -> (%3430)
    block1():
      -> (%new_features.139)
  %3431 : Tensor[] = aten::append(%features.4, %new_features.140) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3432 : Tensor = prim::Uninitialized()
  %3433 : bool = prim::GetAttr[name="memory_efficient"](%1892)
  %3434 : bool = prim::If(%3433) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3435 : bool = prim::Uninitialized()
      %3436 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3437 : bool = aten::gt(%3436, %24)
      %3438 : bool, %3439 : bool, %3440 : int = prim::Loop(%18, %3437, %19, %3435, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3441 : int, %3442 : bool, %3443 : bool, %3444 : int):
          %tensor.71 : Tensor = aten::__getitem__(%features.4, %3444) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3446 : bool = prim::requires_grad(%tensor.71)
          %3447 : bool, %3448 : bool = prim::If(%3446) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3435)
          %3449 : int = aten::add(%3444, %27)
          %3450 : bool = aten::lt(%3449, %3436)
          %3451 : bool = aten::__and__(%3450, %3447)
          -> (%3451, %3446, %3448, %3449)
      %3452 : bool = prim::If(%3438)
        block0():
          -> (%3439)
        block1():
          -> (%19)
      -> (%3452)
    block1():
      -> (%19)
  %bottleneck_output.140 : Tensor = prim::If(%3434) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3432)
    block1():
      %concated_features.71 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3455 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="conv1"](%1892)
      %3456 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%1892)
      %3457 : int = aten::dim(%concated_features.71) # torch/nn/modules/batchnorm.py:276:11
      %3458 : bool = aten::ne(%3457, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3458) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3459 : bool = prim::GetAttr[name="training"](%3456)
       = prim::If(%3459) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3460 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3456)
          %3461 : Tensor = aten::add(%3460, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3456, %3461)
          -> ()
        block1():
          -> ()
      %3462 : bool = prim::GetAttr[name="training"](%3456)
      %3463 : Tensor = prim::GetAttr[name="running_mean"](%3456)
      %3464 : Tensor = prim::GetAttr[name="running_var"](%3456)
      %3465 : Tensor = prim::GetAttr[name="weight"](%3456)
      %3466 : Tensor = prim::GetAttr[name="bias"](%3456)
       = prim::If(%3462) # torch/nn/functional.py:2011:4
        block0():
          %3467 : int[] = aten::size(%concated_features.71) # torch/nn/functional.py:2012:27
          %size_prods.576 : int = aten::__getitem__(%3467, %24) # torch/nn/functional.py:1991:17
          %3469 : int = aten::len(%3467) # torch/nn/functional.py:1992:19
          %3470 : int = aten::sub(%3469, %26) # torch/nn/functional.py:1992:19
          %size_prods.577 : int = prim::Loop(%3470, %25, %size_prods.576) # torch/nn/functional.py:1992:4
            block0(%i.145 : int, %size_prods.578 : int):
              %3474 : int = aten::add(%i.145, %26) # torch/nn/functional.py:1993:27
              %3475 : int = aten::__getitem__(%3467, %3474) # torch/nn/functional.py:1993:22
              %size_prods.579 : int = aten::mul(%size_prods.578, %3475) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.579)
          %3477 : bool = aten::eq(%size_prods.577, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3477) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3478 : Tensor = aten::batch_norm(%concated_features.71, %3465, %3466, %3463, %3464, %3462, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.141 : Tensor = aten::relu_(%3478) # torch/nn/functional.py:1117:17
      %3480 : Tensor = prim::GetAttr[name="weight"](%3455)
      %3481 : Tensor? = prim::GetAttr[name="bias"](%3455)
      %3482 : int[] = prim::ListConstruct(%27, %27)
      %3483 : int[] = prim::ListConstruct(%24, %24)
      %3484 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.141 : Tensor = aten::conv2d(%result.141, %3480, %3481, %3482, %3483, %3484, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.141)
  %3486 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1892)
  %3487 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1892)
  %3488 : int = aten::dim(%bottleneck_output.140) # torch/nn/modules/batchnorm.py:276:11
  %3489 : bool = aten::ne(%3488, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3489) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3490 : bool = prim::GetAttr[name="training"](%3487)
   = prim::If(%3490) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3491 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3487)
      %3492 : Tensor = aten::add(%3491, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3487, %3492)
      -> ()
    block1():
      -> ()
  %3493 : bool = prim::GetAttr[name="training"](%3487)
  %3494 : Tensor = prim::GetAttr[name="running_mean"](%3487)
  %3495 : Tensor = prim::GetAttr[name="running_var"](%3487)
  %3496 : Tensor = prim::GetAttr[name="weight"](%3487)
  %3497 : Tensor = prim::GetAttr[name="bias"](%3487)
   = prim::If(%3493) # torch/nn/functional.py:2011:4
    block0():
      %3498 : int[] = aten::size(%bottleneck_output.140) # torch/nn/functional.py:2012:27
      %size_prods.580 : int = aten::__getitem__(%3498, %24) # torch/nn/functional.py:1991:17
      %3500 : int = aten::len(%3498) # torch/nn/functional.py:1992:19
      %3501 : int = aten::sub(%3500, %26) # torch/nn/functional.py:1992:19
      %size_prods.581 : int = prim::Loop(%3501, %25, %size_prods.580) # torch/nn/functional.py:1992:4
        block0(%i.146 : int, %size_prods.582 : int):
          %3505 : int = aten::add(%i.146, %26) # torch/nn/functional.py:1993:27
          %3506 : int = aten::__getitem__(%3498, %3505) # torch/nn/functional.py:1993:22
          %size_prods.583 : int = aten::mul(%size_prods.582, %3506) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.583)
      %3508 : bool = aten::eq(%size_prods.581, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3508) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3509 : Tensor = aten::batch_norm(%bottleneck_output.140, %3496, %3497, %3494, %3495, %3493, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.142 : Tensor = aten::relu_(%3509) # torch/nn/functional.py:1117:17
  %3511 : Tensor = prim::GetAttr[name="weight"](%3486)
  %3512 : Tensor? = prim::GetAttr[name="bias"](%3486)
  %3513 : int[] = prim::ListConstruct(%27, %27)
  %3514 : int[] = prim::ListConstruct(%27, %27)
  %3515 : int[] = prim::ListConstruct(%27, %27)
  %new_features.141 : Tensor = aten::conv2d(%result.142, %3511, %3512, %3513, %3514, %3515, %27) # torch/nn/modules/conv.py:415:15
  %3517 : float = prim::GetAttr[name="drop_rate"](%1892)
  %3518 : bool = aten::gt(%3517, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.142 : Tensor = prim::If(%3518) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3520 : float = prim::GetAttr[name="drop_rate"](%1892)
      %3521 : bool = prim::GetAttr[name="training"](%1892)
      %3522 : bool = aten::lt(%3520, %16) # torch/nn/functional.py:968:7
      %3523 : bool = prim::If(%3522) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3524 : bool = aten::gt(%3520, %17) # torch/nn/functional.py:968:17
          -> (%3524)
       = prim::If(%3523) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3525 : Tensor = aten::dropout(%new_features.141, %3520, %3521) # torch/nn/functional.py:973:17
      -> (%3525)
    block1():
      -> (%new_features.141)
  %3526 : Tensor[] = aten::append(%features.4, %new_features.142) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3527 : Tensor = prim::Uninitialized()
  %3528 : bool = prim::GetAttr[name="memory_efficient"](%1893)
  %3529 : bool = prim::If(%3528) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3530 : bool = prim::Uninitialized()
      %3531 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3532 : bool = aten::gt(%3531, %24)
      %3533 : bool, %3534 : bool, %3535 : int = prim::Loop(%18, %3532, %19, %3530, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3536 : int, %3537 : bool, %3538 : bool, %3539 : int):
          %tensor.72 : Tensor = aten::__getitem__(%features.4, %3539) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3541 : bool = prim::requires_grad(%tensor.72)
          %3542 : bool, %3543 : bool = prim::If(%3541) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3530)
          %3544 : int = aten::add(%3539, %27)
          %3545 : bool = aten::lt(%3544, %3531)
          %3546 : bool = aten::__and__(%3545, %3542)
          -> (%3546, %3541, %3543, %3544)
      %3547 : bool = prim::If(%3533)
        block0():
          -> (%3534)
        block1():
          -> (%19)
      -> (%3547)
    block1():
      -> (%19)
  %bottleneck_output.142 : Tensor = prim::If(%3529) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3527)
    block1():
      %concated_features.72 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3550 : __torch__.torch.nn.modules.conv.___torch_mangle_229.Conv2d = prim::GetAttr[name="conv1"](%1893)
      %3551 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_228.BatchNorm2d = prim::GetAttr[name="norm1"](%1893)
      %3552 : int = aten::dim(%concated_features.72) # torch/nn/modules/batchnorm.py:276:11
      %3553 : bool = aten::ne(%3552, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3553) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3554 : bool = prim::GetAttr[name="training"](%3551)
       = prim::If(%3554) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3555 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3551)
          %3556 : Tensor = aten::add(%3555, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3551, %3556)
          -> ()
        block1():
          -> ()
      %3557 : bool = prim::GetAttr[name="training"](%3551)
      %3558 : Tensor = prim::GetAttr[name="running_mean"](%3551)
      %3559 : Tensor = prim::GetAttr[name="running_var"](%3551)
      %3560 : Tensor = prim::GetAttr[name="weight"](%3551)
      %3561 : Tensor = prim::GetAttr[name="bias"](%3551)
       = prim::If(%3557) # torch/nn/functional.py:2011:4
        block0():
          %3562 : int[] = aten::size(%concated_features.72) # torch/nn/functional.py:2012:27
          %size_prods.584 : int = aten::__getitem__(%3562, %24) # torch/nn/functional.py:1991:17
          %3564 : int = aten::len(%3562) # torch/nn/functional.py:1992:19
          %3565 : int = aten::sub(%3564, %26) # torch/nn/functional.py:1992:19
          %size_prods.585 : int = prim::Loop(%3565, %25, %size_prods.584) # torch/nn/functional.py:1992:4
            block0(%i.147 : int, %size_prods.586 : int):
              %3569 : int = aten::add(%i.147, %26) # torch/nn/functional.py:1993:27
              %3570 : int = aten::__getitem__(%3562, %3569) # torch/nn/functional.py:1993:22
              %size_prods.587 : int = aten::mul(%size_prods.586, %3570) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.587)
          %3572 : bool = aten::eq(%size_prods.585, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3572) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3573 : Tensor = aten::batch_norm(%concated_features.72, %3560, %3561, %3558, %3559, %3557, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.143 : Tensor = aten::relu_(%3573) # torch/nn/functional.py:1117:17
      %3575 : Tensor = prim::GetAttr[name="weight"](%3550)
      %3576 : Tensor? = prim::GetAttr[name="bias"](%3550)
      %3577 : int[] = prim::ListConstruct(%27, %27)
      %3578 : int[] = prim::ListConstruct(%24, %24)
      %3579 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.143 : Tensor = aten::conv2d(%result.143, %3575, %3576, %3577, %3578, %3579, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.143)
  %3581 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1893)
  %3582 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1893)
  %3583 : int = aten::dim(%bottleneck_output.142) # torch/nn/modules/batchnorm.py:276:11
  %3584 : bool = aten::ne(%3583, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3584) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3585 : bool = prim::GetAttr[name="training"](%3582)
   = prim::If(%3585) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3586 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3582)
      %3587 : Tensor = aten::add(%3586, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3582, %3587)
      -> ()
    block1():
      -> ()
  %3588 : bool = prim::GetAttr[name="training"](%3582)
  %3589 : Tensor = prim::GetAttr[name="running_mean"](%3582)
  %3590 : Tensor = prim::GetAttr[name="running_var"](%3582)
  %3591 : Tensor = prim::GetAttr[name="weight"](%3582)
  %3592 : Tensor = prim::GetAttr[name="bias"](%3582)
   = prim::If(%3588) # torch/nn/functional.py:2011:4
    block0():
      %3593 : int[] = aten::size(%bottleneck_output.142) # torch/nn/functional.py:2012:27
      %size_prods.588 : int = aten::__getitem__(%3593, %24) # torch/nn/functional.py:1991:17
      %3595 : int = aten::len(%3593) # torch/nn/functional.py:1992:19
      %3596 : int = aten::sub(%3595, %26) # torch/nn/functional.py:1992:19
      %size_prods.589 : int = prim::Loop(%3596, %25, %size_prods.588) # torch/nn/functional.py:1992:4
        block0(%i.148 : int, %size_prods.590 : int):
          %3600 : int = aten::add(%i.148, %26) # torch/nn/functional.py:1993:27
          %3601 : int = aten::__getitem__(%3593, %3600) # torch/nn/functional.py:1993:22
          %size_prods.591 : int = aten::mul(%size_prods.590, %3601) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.591)
      %3603 : bool = aten::eq(%size_prods.589, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3603) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3604 : Tensor = aten::batch_norm(%bottleneck_output.142, %3591, %3592, %3589, %3590, %3588, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.144 : Tensor = aten::relu_(%3604) # torch/nn/functional.py:1117:17
  %3606 : Tensor = prim::GetAttr[name="weight"](%3581)
  %3607 : Tensor? = prim::GetAttr[name="bias"](%3581)
  %3608 : int[] = prim::ListConstruct(%27, %27)
  %3609 : int[] = prim::ListConstruct(%27, %27)
  %3610 : int[] = prim::ListConstruct(%27, %27)
  %new_features.143 : Tensor = aten::conv2d(%result.144, %3606, %3607, %3608, %3609, %3610, %27) # torch/nn/modules/conv.py:415:15
  %3612 : float = prim::GetAttr[name="drop_rate"](%1893)
  %3613 : bool = aten::gt(%3612, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.144 : Tensor = prim::If(%3613) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3615 : float = prim::GetAttr[name="drop_rate"](%1893)
      %3616 : bool = prim::GetAttr[name="training"](%1893)
      %3617 : bool = aten::lt(%3615, %16) # torch/nn/functional.py:968:7
      %3618 : bool = prim::If(%3617) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3619 : bool = aten::gt(%3615, %17) # torch/nn/functional.py:968:17
          -> (%3619)
       = prim::If(%3618) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3620 : Tensor = aten::dropout(%new_features.143, %3615, %3616) # torch/nn/functional.py:973:17
      -> (%3620)
    block1():
      -> (%new_features.143)
  %3621 : Tensor[] = aten::append(%features.4, %new_features.144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3622 : Tensor = prim::Uninitialized()
  %3623 : bool = prim::GetAttr[name="memory_efficient"](%1894)
  %3624 : bool = prim::If(%3623) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3625 : bool = prim::Uninitialized()
      %3626 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3627 : bool = aten::gt(%3626, %24)
      %3628 : bool, %3629 : bool, %3630 : int = prim::Loop(%18, %3627, %19, %3625, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3631 : int, %3632 : bool, %3633 : bool, %3634 : int):
          %tensor.73 : Tensor = aten::__getitem__(%features.4, %3634) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3636 : bool = prim::requires_grad(%tensor.73)
          %3637 : bool, %3638 : bool = prim::If(%3636) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3625)
          %3639 : int = aten::add(%3634, %27)
          %3640 : bool = aten::lt(%3639, %3626)
          %3641 : bool = aten::__and__(%3640, %3637)
          -> (%3641, %3636, %3638, %3639)
      %3642 : bool = prim::If(%3628)
        block0():
          -> (%3629)
        block1():
          -> (%19)
      -> (%3642)
    block1():
      -> (%19)
  %bottleneck_output.144 : Tensor = prim::If(%3624) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3622)
    block1():
      %concated_features.73 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3645 : __torch__.torch.nn.modules.conv.___torch_mangle_232.Conv2d = prim::GetAttr[name="conv1"](%1894)
      %3646 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%1894)
      %3647 : int = aten::dim(%concated_features.73) # torch/nn/modules/batchnorm.py:276:11
      %3648 : bool = aten::ne(%3647, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3648) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3649 : bool = prim::GetAttr[name="training"](%3646)
       = prim::If(%3649) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3650 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3646)
          %3651 : Tensor = aten::add(%3650, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3646, %3651)
          -> ()
        block1():
          -> ()
      %3652 : bool = prim::GetAttr[name="training"](%3646)
      %3653 : Tensor = prim::GetAttr[name="running_mean"](%3646)
      %3654 : Tensor = prim::GetAttr[name="running_var"](%3646)
      %3655 : Tensor = prim::GetAttr[name="weight"](%3646)
      %3656 : Tensor = prim::GetAttr[name="bias"](%3646)
       = prim::If(%3652) # torch/nn/functional.py:2011:4
        block0():
          %3657 : int[] = aten::size(%concated_features.73) # torch/nn/functional.py:2012:27
          %size_prods.592 : int = aten::__getitem__(%3657, %24) # torch/nn/functional.py:1991:17
          %3659 : int = aten::len(%3657) # torch/nn/functional.py:1992:19
          %3660 : int = aten::sub(%3659, %26) # torch/nn/functional.py:1992:19
          %size_prods.593 : int = prim::Loop(%3660, %25, %size_prods.592) # torch/nn/functional.py:1992:4
            block0(%i.149 : int, %size_prods.594 : int):
              %3664 : int = aten::add(%i.149, %26) # torch/nn/functional.py:1993:27
              %3665 : int = aten::__getitem__(%3657, %3664) # torch/nn/functional.py:1993:22
              %size_prods.595 : int = aten::mul(%size_prods.594, %3665) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.595)
          %3667 : bool = aten::eq(%size_prods.593, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3667) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3668 : Tensor = aten::batch_norm(%concated_features.73, %3655, %3656, %3653, %3654, %3652, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.145 : Tensor = aten::relu_(%3668) # torch/nn/functional.py:1117:17
      %3670 : Tensor = prim::GetAttr[name="weight"](%3645)
      %3671 : Tensor? = prim::GetAttr[name="bias"](%3645)
      %3672 : int[] = prim::ListConstruct(%27, %27)
      %3673 : int[] = prim::ListConstruct(%24, %24)
      %3674 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.145 : Tensor = aten::conv2d(%result.145, %3670, %3671, %3672, %3673, %3674, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.145)
  %3676 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1894)
  %3677 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1894)
  %3678 : int = aten::dim(%bottleneck_output.144) # torch/nn/modules/batchnorm.py:276:11
  %3679 : bool = aten::ne(%3678, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3679) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3680 : bool = prim::GetAttr[name="training"](%3677)
   = prim::If(%3680) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3681 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3677)
      %3682 : Tensor = aten::add(%3681, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3677, %3682)
      -> ()
    block1():
      -> ()
  %3683 : bool = prim::GetAttr[name="training"](%3677)
  %3684 : Tensor = prim::GetAttr[name="running_mean"](%3677)
  %3685 : Tensor = prim::GetAttr[name="running_var"](%3677)
  %3686 : Tensor = prim::GetAttr[name="weight"](%3677)
  %3687 : Tensor = prim::GetAttr[name="bias"](%3677)
   = prim::If(%3683) # torch/nn/functional.py:2011:4
    block0():
      %3688 : int[] = aten::size(%bottleneck_output.144) # torch/nn/functional.py:2012:27
      %size_prods.596 : int = aten::__getitem__(%3688, %24) # torch/nn/functional.py:1991:17
      %3690 : int = aten::len(%3688) # torch/nn/functional.py:1992:19
      %3691 : int = aten::sub(%3690, %26) # torch/nn/functional.py:1992:19
      %size_prods.597 : int = prim::Loop(%3691, %25, %size_prods.596) # torch/nn/functional.py:1992:4
        block0(%i.150 : int, %size_prods.598 : int):
          %3695 : int = aten::add(%i.150, %26) # torch/nn/functional.py:1993:27
          %3696 : int = aten::__getitem__(%3688, %3695) # torch/nn/functional.py:1993:22
          %size_prods.599 : int = aten::mul(%size_prods.598, %3696) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.599)
      %3698 : bool = aten::eq(%size_prods.597, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3698) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3699 : Tensor = aten::batch_norm(%bottleneck_output.144, %3686, %3687, %3684, %3685, %3683, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.146 : Tensor = aten::relu_(%3699) # torch/nn/functional.py:1117:17
  %3701 : Tensor = prim::GetAttr[name="weight"](%3676)
  %3702 : Tensor? = prim::GetAttr[name="bias"](%3676)
  %3703 : int[] = prim::ListConstruct(%27, %27)
  %3704 : int[] = prim::ListConstruct(%27, %27)
  %3705 : int[] = prim::ListConstruct(%27, %27)
  %new_features.145 : Tensor = aten::conv2d(%result.146, %3701, %3702, %3703, %3704, %3705, %27) # torch/nn/modules/conv.py:415:15
  %3707 : float = prim::GetAttr[name="drop_rate"](%1894)
  %3708 : bool = aten::gt(%3707, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.146 : Tensor = prim::If(%3708) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3710 : float = prim::GetAttr[name="drop_rate"](%1894)
      %3711 : bool = prim::GetAttr[name="training"](%1894)
      %3712 : bool = aten::lt(%3710, %16) # torch/nn/functional.py:968:7
      %3713 : bool = prim::If(%3712) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3714 : bool = aten::gt(%3710, %17) # torch/nn/functional.py:968:17
          -> (%3714)
       = prim::If(%3713) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3715 : Tensor = aten::dropout(%new_features.145, %3710, %3711) # torch/nn/functional.py:973:17
      -> (%3715)
    block1():
      -> (%new_features.145)
  %3716 : Tensor[] = aten::append(%features.4, %new_features.146) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3717 : Tensor = prim::Uninitialized()
  %3718 : bool = prim::GetAttr[name="memory_efficient"](%1895)
  %3719 : bool = prim::If(%3718) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3720 : bool = prim::Uninitialized()
      %3721 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3722 : bool = aten::gt(%3721, %24)
      %3723 : bool, %3724 : bool, %3725 : int = prim::Loop(%18, %3722, %19, %3720, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3726 : int, %3727 : bool, %3728 : bool, %3729 : int):
          %tensor.74 : Tensor = aten::__getitem__(%features.4, %3729) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3731 : bool = prim::requires_grad(%tensor.74)
          %3732 : bool, %3733 : bool = prim::If(%3731) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3720)
          %3734 : int = aten::add(%3729, %27)
          %3735 : bool = aten::lt(%3734, %3721)
          %3736 : bool = aten::__and__(%3735, %3732)
          -> (%3736, %3731, %3733, %3734)
      %3737 : bool = prim::If(%3723)
        block0():
          -> (%3724)
        block1():
          -> (%19)
      -> (%3737)
    block1():
      -> (%19)
  %bottleneck_output.146 : Tensor = prim::If(%3719) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3717)
    block1():
      %concated_features.74 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3740 : __torch__.torch.nn.modules.conv.___torch_mangle_235.Conv2d = prim::GetAttr[name="conv1"](%1895)
      %3741 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_234.BatchNorm2d = prim::GetAttr[name="norm1"](%1895)
      %3742 : int = aten::dim(%concated_features.74) # torch/nn/modules/batchnorm.py:276:11
      %3743 : bool = aten::ne(%3742, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3743) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3744 : bool = prim::GetAttr[name="training"](%3741)
       = prim::If(%3744) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3745 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3741)
          %3746 : Tensor = aten::add(%3745, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3741, %3746)
          -> ()
        block1():
          -> ()
      %3747 : bool = prim::GetAttr[name="training"](%3741)
      %3748 : Tensor = prim::GetAttr[name="running_mean"](%3741)
      %3749 : Tensor = prim::GetAttr[name="running_var"](%3741)
      %3750 : Tensor = prim::GetAttr[name="weight"](%3741)
      %3751 : Tensor = prim::GetAttr[name="bias"](%3741)
       = prim::If(%3747) # torch/nn/functional.py:2011:4
        block0():
          %3752 : int[] = aten::size(%concated_features.74) # torch/nn/functional.py:2012:27
          %size_prods.600 : int = aten::__getitem__(%3752, %24) # torch/nn/functional.py:1991:17
          %3754 : int = aten::len(%3752) # torch/nn/functional.py:1992:19
          %3755 : int = aten::sub(%3754, %26) # torch/nn/functional.py:1992:19
          %size_prods.601 : int = prim::Loop(%3755, %25, %size_prods.600) # torch/nn/functional.py:1992:4
            block0(%i.151 : int, %size_prods.602 : int):
              %3759 : int = aten::add(%i.151, %26) # torch/nn/functional.py:1993:27
              %3760 : int = aten::__getitem__(%3752, %3759) # torch/nn/functional.py:1993:22
              %size_prods.603 : int = aten::mul(%size_prods.602, %3760) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.603)
          %3762 : bool = aten::eq(%size_prods.601, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3762) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3763 : Tensor = aten::batch_norm(%concated_features.74, %3750, %3751, %3748, %3749, %3747, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.147 : Tensor = aten::relu_(%3763) # torch/nn/functional.py:1117:17
      %3765 : Tensor = prim::GetAttr[name="weight"](%3740)
      %3766 : Tensor? = prim::GetAttr[name="bias"](%3740)
      %3767 : int[] = prim::ListConstruct(%27, %27)
      %3768 : int[] = prim::ListConstruct(%24, %24)
      %3769 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.147 : Tensor = aten::conv2d(%result.147, %3765, %3766, %3767, %3768, %3769, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.147)
  %3771 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1895)
  %3772 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1895)
  %3773 : int = aten::dim(%bottleneck_output.146) # torch/nn/modules/batchnorm.py:276:11
  %3774 : bool = aten::ne(%3773, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3774) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3775 : bool = prim::GetAttr[name="training"](%3772)
   = prim::If(%3775) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3776 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3772)
      %3777 : Tensor = aten::add(%3776, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3772, %3777)
      -> ()
    block1():
      -> ()
  %3778 : bool = prim::GetAttr[name="training"](%3772)
  %3779 : Tensor = prim::GetAttr[name="running_mean"](%3772)
  %3780 : Tensor = prim::GetAttr[name="running_var"](%3772)
  %3781 : Tensor = prim::GetAttr[name="weight"](%3772)
  %3782 : Tensor = prim::GetAttr[name="bias"](%3772)
   = prim::If(%3778) # torch/nn/functional.py:2011:4
    block0():
      %3783 : int[] = aten::size(%bottleneck_output.146) # torch/nn/functional.py:2012:27
      %size_prods.604 : int = aten::__getitem__(%3783, %24) # torch/nn/functional.py:1991:17
      %3785 : int = aten::len(%3783) # torch/nn/functional.py:1992:19
      %3786 : int = aten::sub(%3785, %26) # torch/nn/functional.py:1992:19
      %size_prods.605 : int = prim::Loop(%3786, %25, %size_prods.604) # torch/nn/functional.py:1992:4
        block0(%i.152 : int, %size_prods.606 : int):
          %3790 : int = aten::add(%i.152, %26) # torch/nn/functional.py:1993:27
          %3791 : int = aten::__getitem__(%3783, %3790) # torch/nn/functional.py:1993:22
          %size_prods.607 : int = aten::mul(%size_prods.606, %3791) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.607)
      %3793 : bool = aten::eq(%size_prods.605, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3793) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3794 : Tensor = aten::batch_norm(%bottleneck_output.146, %3781, %3782, %3779, %3780, %3778, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.148 : Tensor = aten::relu_(%3794) # torch/nn/functional.py:1117:17
  %3796 : Tensor = prim::GetAttr[name="weight"](%3771)
  %3797 : Tensor? = prim::GetAttr[name="bias"](%3771)
  %3798 : int[] = prim::ListConstruct(%27, %27)
  %3799 : int[] = prim::ListConstruct(%27, %27)
  %3800 : int[] = prim::ListConstruct(%27, %27)
  %new_features.147 : Tensor = aten::conv2d(%result.148, %3796, %3797, %3798, %3799, %3800, %27) # torch/nn/modules/conv.py:415:15
  %3802 : float = prim::GetAttr[name="drop_rate"](%1895)
  %3803 : bool = aten::gt(%3802, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.148 : Tensor = prim::If(%3803) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3805 : float = prim::GetAttr[name="drop_rate"](%1895)
      %3806 : bool = prim::GetAttr[name="training"](%1895)
      %3807 : bool = aten::lt(%3805, %16) # torch/nn/functional.py:968:7
      %3808 : bool = prim::If(%3807) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3809 : bool = aten::gt(%3805, %17) # torch/nn/functional.py:968:17
          -> (%3809)
       = prim::If(%3808) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3810 : Tensor = aten::dropout(%new_features.147, %3805, %3806) # torch/nn/functional.py:973:17
      -> (%3810)
    block1():
      -> (%new_features.147)
  %3811 : Tensor[] = aten::append(%features.4, %new_features.148) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3812 : Tensor = prim::Uninitialized()
  %3813 : bool = prim::GetAttr[name="memory_efficient"](%1896)
  %3814 : bool = prim::If(%3813) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3815 : bool = prim::Uninitialized()
      %3816 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3817 : bool = aten::gt(%3816, %24)
      %3818 : bool, %3819 : bool, %3820 : int = prim::Loop(%18, %3817, %19, %3815, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3821 : int, %3822 : bool, %3823 : bool, %3824 : int):
          %tensor.75 : Tensor = aten::__getitem__(%features.4, %3824) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3826 : bool = prim::requires_grad(%tensor.75)
          %3827 : bool, %3828 : bool = prim::If(%3826) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3815)
          %3829 : int = aten::add(%3824, %27)
          %3830 : bool = aten::lt(%3829, %3816)
          %3831 : bool = aten::__and__(%3830, %3827)
          -> (%3831, %3826, %3828, %3829)
      %3832 : bool = prim::If(%3818)
        block0():
          -> (%3819)
        block1():
          -> (%19)
      -> (%3832)
    block1():
      -> (%19)
  %bottleneck_output.148 : Tensor = prim::If(%3814) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3812)
    block1():
      %concated_features.75 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3835 : __torch__.torch.nn.modules.conv.___torch_mangle_238.Conv2d = prim::GetAttr[name="conv1"](%1896)
      %3836 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_237.BatchNorm2d = prim::GetAttr[name="norm1"](%1896)
      %3837 : int = aten::dim(%concated_features.75) # torch/nn/modules/batchnorm.py:276:11
      %3838 : bool = aten::ne(%3837, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3838) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3839 : bool = prim::GetAttr[name="training"](%3836)
       = prim::If(%3839) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3840 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3836)
          %3841 : Tensor = aten::add(%3840, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3836, %3841)
          -> ()
        block1():
          -> ()
      %3842 : bool = prim::GetAttr[name="training"](%3836)
      %3843 : Tensor = prim::GetAttr[name="running_mean"](%3836)
      %3844 : Tensor = prim::GetAttr[name="running_var"](%3836)
      %3845 : Tensor = prim::GetAttr[name="weight"](%3836)
      %3846 : Tensor = prim::GetAttr[name="bias"](%3836)
       = prim::If(%3842) # torch/nn/functional.py:2011:4
        block0():
          %3847 : int[] = aten::size(%concated_features.75) # torch/nn/functional.py:2012:27
          %size_prods.608 : int = aten::__getitem__(%3847, %24) # torch/nn/functional.py:1991:17
          %3849 : int = aten::len(%3847) # torch/nn/functional.py:1992:19
          %3850 : int = aten::sub(%3849, %26) # torch/nn/functional.py:1992:19
          %size_prods.609 : int = prim::Loop(%3850, %25, %size_prods.608) # torch/nn/functional.py:1992:4
            block0(%i.153 : int, %size_prods.610 : int):
              %3854 : int = aten::add(%i.153, %26) # torch/nn/functional.py:1993:27
              %3855 : int = aten::__getitem__(%3847, %3854) # torch/nn/functional.py:1993:22
              %size_prods.611 : int = aten::mul(%size_prods.610, %3855) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.611)
          %3857 : bool = aten::eq(%size_prods.609, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3857) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3858 : Tensor = aten::batch_norm(%concated_features.75, %3845, %3846, %3843, %3844, %3842, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.149 : Tensor = aten::relu_(%3858) # torch/nn/functional.py:1117:17
      %3860 : Tensor = prim::GetAttr[name="weight"](%3835)
      %3861 : Tensor? = prim::GetAttr[name="bias"](%3835)
      %3862 : int[] = prim::ListConstruct(%27, %27)
      %3863 : int[] = prim::ListConstruct(%24, %24)
      %3864 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.149 : Tensor = aten::conv2d(%result.149, %3860, %3861, %3862, %3863, %3864, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.149)
  %3866 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1896)
  %3867 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1896)
  %3868 : int = aten::dim(%bottleneck_output.148) # torch/nn/modules/batchnorm.py:276:11
  %3869 : bool = aten::ne(%3868, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3869) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3870 : bool = prim::GetAttr[name="training"](%3867)
   = prim::If(%3870) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3871 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3867)
      %3872 : Tensor = aten::add(%3871, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3867, %3872)
      -> ()
    block1():
      -> ()
  %3873 : bool = prim::GetAttr[name="training"](%3867)
  %3874 : Tensor = prim::GetAttr[name="running_mean"](%3867)
  %3875 : Tensor = prim::GetAttr[name="running_var"](%3867)
  %3876 : Tensor = prim::GetAttr[name="weight"](%3867)
  %3877 : Tensor = prim::GetAttr[name="bias"](%3867)
   = prim::If(%3873) # torch/nn/functional.py:2011:4
    block0():
      %3878 : int[] = aten::size(%bottleneck_output.148) # torch/nn/functional.py:2012:27
      %size_prods.612 : int = aten::__getitem__(%3878, %24) # torch/nn/functional.py:1991:17
      %3880 : int = aten::len(%3878) # torch/nn/functional.py:1992:19
      %3881 : int = aten::sub(%3880, %26) # torch/nn/functional.py:1992:19
      %size_prods.613 : int = prim::Loop(%3881, %25, %size_prods.612) # torch/nn/functional.py:1992:4
        block0(%i.154 : int, %size_prods.614 : int):
          %3885 : int = aten::add(%i.154, %26) # torch/nn/functional.py:1993:27
          %3886 : int = aten::__getitem__(%3878, %3885) # torch/nn/functional.py:1993:22
          %size_prods.615 : int = aten::mul(%size_prods.614, %3886) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.615)
      %3888 : bool = aten::eq(%size_prods.613, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3888) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3889 : Tensor = aten::batch_norm(%bottleneck_output.148, %3876, %3877, %3874, %3875, %3873, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.150 : Tensor = aten::relu_(%3889) # torch/nn/functional.py:1117:17
  %3891 : Tensor = prim::GetAttr[name="weight"](%3866)
  %3892 : Tensor? = prim::GetAttr[name="bias"](%3866)
  %3893 : int[] = prim::ListConstruct(%27, %27)
  %3894 : int[] = prim::ListConstruct(%27, %27)
  %3895 : int[] = prim::ListConstruct(%27, %27)
  %new_features.149 : Tensor = aten::conv2d(%result.150, %3891, %3892, %3893, %3894, %3895, %27) # torch/nn/modules/conv.py:415:15
  %3897 : float = prim::GetAttr[name="drop_rate"](%1896)
  %3898 : bool = aten::gt(%3897, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.150 : Tensor = prim::If(%3898) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3900 : float = prim::GetAttr[name="drop_rate"](%1896)
      %3901 : bool = prim::GetAttr[name="training"](%1896)
      %3902 : bool = aten::lt(%3900, %16) # torch/nn/functional.py:968:7
      %3903 : bool = prim::If(%3902) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3904 : bool = aten::gt(%3900, %17) # torch/nn/functional.py:968:17
          -> (%3904)
       = prim::If(%3903) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %3905 : Tensor = aten::dropout(%new_features.149, %3900, %3901) # torch/nn/functional.py:973:17
      -> (%3905)
    block1():
      -> (%new_features.149)
  %3906 : Tensor[] = aten::append(%features.4, %new_features.150) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %3907 : Tensor = prim::Uninitialized()
  %3908 : bool = prim::GetAttr[name="memory_efficient"](%1897)
  %3909 : bool = prim::If(%3908) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %3910 : bool = prim::Uninitialized()
      %3911 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %3912 : bool = aten::gt(%3911, %24)
      %3913 : bool, %3914 : bool, %3915 : int = prim::Loop(%18, %3912, %19, %3910, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%3916 : int, %3917 : bool, %3918 : bool, %3919 : int):
          %tensor.76 : Tensor = aten::__getitem__(%features.4, %3919) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %3921 : bool = prim::requires_grad(%tensor.76)
          %3922 : bool, %3923 : bool = prim::If(%3921) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %3910)
          %3924 : int = aten::add(%3919, %27)
          %3925 : bool = aten::lt(%3924, %3911)
          %3926 : bool = aten::__and__(%3925, %3922)
          -> (%3926, %3921, %3923, %3924)
      %3927 : bool = prim::If(%3913)
        block0():
          -> (%3914)
        block1():
          -> (%19)
      -> (%3927)
    block1():
      -> (%19)
  %bottleneck_output.150 : Tensor = prim::If(%3909) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%3907)
    block1():
      %concated_features.76 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %3930 : __torch__.torch.nn.modules.conv.___torch_mangle_241.Conv2d = prim::GetAttr[name="conv1"](%1897)
      %3931 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_240.BatchNorm2d = prim::GetAttr[name="norm1"](%1897)
      %3932 : int = aten::dim(%concated_features.76) # torch/nn/modules/batchnorm.py:276:11
      %3933 : bool = aten::ne(%3932, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%3933) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %3934 : bool = prim::GetAttr[name="training"](%3931)
       = prim::If(%3934) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %3935 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3931)
          %3936 : Tensor = aten::add(%3935, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%3931, %3936)
          -> ()
        block1():
          -> ()
      %3937 : bool = prim::GetAttr[name="training"](%3931)
      %3938 : Tensor = prim::GetAttr[name="running_mean"](%3931)
      %3939 : Tensor = prim::GetAttr[name="running_var"](%3931)
      %3940 : Tensor = prim::GetAttr[name="weight"](%3931)
      %3941 : Tensor = prim::GetAttr[name="bias"](%3931)
       = prim::If(%3937) # torch/nn/functional.py:2011:4
        block0():
          %3942 : int[] = aten::size(%concated_features.76) # torch/nn/functional.py:2012:27
          %size_prods.616 : int = aten::__getitem__(%3942, %24) # torch/nn/functional.py:1991:17
          %3944 : int = aten::len(%3942) # torch/nn/functional.py:1992:19
          %3945 : int = aten::sub(%3944, %26) # torch/nn/functional.py:1992:19
          %size_prods.617 : int = prim::Loop(%3945, %25, %size_prods.616) # torch/nn/functional.py:1992:4
            block0(%i.155 : int, %size_prods.618 : int):
              %3949 : int = aten::add(%i.155, %26) # torch/nn/functional.py:1993:27
              %3950 : int = aten::__getitem__(%3942, %3949) # torch/nn/functional.py:1993:22
              %size_prods.619 : int = aten::mul(%size_prods.618, %3950) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.619)
          %3952 : bool = aten::eq(%size_prods.617, %27) # torch/nn/functional.py:1994:7
           = prim::If(%3952) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %3953 : Tensor = aten::batch_norm(%concated_features.76, %3940, %3941, %3938, %3939, %3937, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.151 : Tensor = aten::relu_(%3953) # torch/nn/functional.py:1117:17
      %3955 : Tensor = prim::GetAttr[name="weight"](%3930)
      %3956 : Tensor? = prim::GetAttr[name="bias"](%3930)
      %3957 : int[] = prim::ListConstruct(%27, %27)
      %3958 : int[] = prim::ListConstruct(%24, %24)
      %3959 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.151 : Tensor = aten::conv2d(%result.151, %3955, %3956, %3957, %3958, %3959, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.151)
  %3961 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1897)
  %3962 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1897)
  %3963 : int = aten::dim(%bottleneck_output.150) # torch/nn/modules/batchnorm.py:276:11
  %3964 : bool = aten::ne(%3963, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%3964) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %3965 : bool = prim::GetAttr[name="training"](%3962)
   = prim::If(%3965) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %3966 : Tensor = prim::GetAttr[name="num_batches_tracked"](%3962)
      %3967 : Tensor = aten::add(%3966, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%3962, %3967)
      -> ()
    block1():
      -> ()
  %3968 : bool = prim::GetAttr[name="training"](%3962)
  %3969 : Tensor = prim::GetAttr[name="running_mean"](%3962)
  %3970 : Tensor = prim::GetAttr[name="running_var"](%3962)
  %3971 : Tensor = prim::GetAttr[name="weight"](%3962)
  %3972 : Tensor = prim::GetAttr[name="bias"](%3962)
   = prim::If(%3968) # torch/nn/functional.py:2011:4
    block0():
      %3973 : int[] = aten::size(%bottleneck_output.150) # torch/nn/functional.py:2012:27
      %size_prods.620 : int = aten::__getitem__(%3973, %24) # torch/nn/functional.py:1991:17
      %3975 : int = aten::len(%3973) # torch/nn/functional.py:1992:19
      %3976 : int = aten::sub(%3975, %26) # torch/nn/functional.py:1992:19
      %size_prods.621 : int = prim::Loop(%3976, %25, %size_prods.620) # torch/nn/functional.py:1992:4
        block0(%i.156 : int, %size_prods.622 : int):
          %3980 : int = aten::add(%i.156, %26) # torch/nn/functional.py:1993:27
          %3981 : int = aten::__getitem__(%3973, %3980) # torch/nn/functional.py:1993:22
          %size_prods.623 : int = aten::mul(%size_prods.622, %3981) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.623)
      %3983 : bool = aten::eq(%size_prods.621, %27) # torch/nn/functional.py:1994:7
       = prim::If(%3983) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %3984 : Tensor = aten::batch_norm(%bottleneck_output.150, %3971, %3972, %3969, %3970, %3968, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.152 : Tensor = aten::relu_(%3984) # torch/nn/functional.py:1117:17
  %3986 : Tensor = prim::GetAttr[name="weight"](%3961)
  %3987 : Tensor? = prim::GetAttr[name="bias"](%3961)
  %3988 : int[] = prim::ListConstruct(%27, %27)
  %3989 : int[] = prim::ListConstruct(%27, %27)
  %3990 : int[] = prim::ListConstruct(%27, %27)
  %new_features.151 : Tensor = aten::conv2d(%result.152, %3986, %3987, %3988, %3989, %3990, %27) # torch/nn/modules/conv.py:415:15
  %3992 : float = prim::GetAttr[name="drop_rate"](%1897)
  %3993 : bool = aten::gt(%3992, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.152 : Tensor = prim::If(%3993) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %3995 : float = prim::GetAttr[name="drop_rate"](%1897)
      %3996 : bool = prim::GetAttr[name="training"](%1897)
      %3997 : bool = aten::lt(%3995, %16) # torch/nn/functional.py:968:7
      %3998 : bool = prim::If(%3997) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %3999 : bool = aten::gt(%3995, %17) # torch/nn/functional.py:968:17
          -> (%3999)
       = prim::If(%3998) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4000 : Tensor = aten::dropout(%new_features.151, %3995, %3996) # torch/nn/functional.py:973:17
      -> (%4000)
    block1():
      -> (%new_features.151)
  %4001 : Tensor[] = aten::append(%features.4, %new_features.152) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4002 : Tensor = prim::Uninitialized()
  %4003 : bool = prim::GetAttr[name="memory_efficient"](%1898)
  %4004 : bool = prim::If(%4003) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4005 : bool = prim::Uninitialized()
      %4006 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4007 : bool = aten::gt(%4006, %24)
      %4008 : bool, %4009 : bool, %4010 : int = prim::Loop(%18, %4007, %19, %4005, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4011 : int, %4012 : bool, %4013 : bool, %4014 : int):
          %tensor.77 : Tensor = aten::__getitem__(%features.4, %4014) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4016 : bool = prim::requires_grad(%tensor.77)
          %4017 : bool, %4018 : bool = prim::If(%4016) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4005)
          %4019 : int = aten::add(%4014, %27)
          %4020 : bool = aten::lt(%4019, %4006)
          %4021 : bool = aten::__and__(%4020, %4017)
          -> (%4021, %4016, %4018, %4019)
      %4022 : bool = prim::If(%4008)
        block0():
          -> (%4009)
        block1():
          -> (%19)
      -> (%4022)
    block1():
      -> (%19)
  %bottleneck_output.152 : Tensor = prim::If(%4004) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4002)
    block1():
      %concated_features.77 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4025 : __torch__.torch.nn.modules.conv.___torch_mangle_244.Conv2d = prim::GetAttr[name="conv1"](%1898)
      %4026 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_243.BatchNorm2d = prim::GetAttr[name="norm1"](%1898)
      %4027 : int = aten::dim(%concated_features.77) # torch/nn/modules/batchnorm.py:276:11
      %4028 : bool = aten::ne(%4027, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4028) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4029 : bool = prim::GetAttr[name="training"](%4026)
       = prim::If(%4029) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4030 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4026)
          %4031 : Tensor = aten::add(%4030, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4026, %4031)
          -> ()
        block1():
          -> ()
      %4032 : bool = prim::GetAttr[name="training"](%4026)
      %4033 : Tensor = prim::GetAttr[name="running_mean"](%4026)
      %4034 : Tensor = prim::GetAttr[name="running_var"](%4026)
      %4035 : Tensor = prim::GetAttr[name="weight"](%4026)
      %4036 : Tensor = prim::GetAttr[name="bias"](%4026)
       = prim::If(%4032) # torch/nn/functional.py:2011:4
        block0():
          %4037 : int[] = aten::size(%concated_features.77) # torch/nn/functional.py:2012:27
          %size_prods.624 : int = aten::__getitem__(%4037, %24) # torch/nn/functional.py:1991:17
          %4039 : int = aten::len(%4037) # torch/nn/functional.py:1992:19
          %4040 : int = aten::sub(%4039, %26) # torch/nn/functional.py:1992:19
          %size_prods.625 : int = prim::Loop(%4040, %25, %size_prods.624) # torch/nn/functional.py:1992:4
            block0(%i.157 : int, %size_prods.626 : int):
              %4044 : int = aten::add(%i.157, %26) # torch/nn/functional.py:1993:27
              %4045 : int = aten::__getitem__(%4037, %4044) # torch/nn/functional.py:1993:22
              %size_prods.627 : int = aten::mul(%size_prods.626, %4045) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.627)
          %4047 : bool = aten::eq(%size_prods.625, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4047) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4048 : Tensor = aten::batch_norm(%concated_features.77, %4035, %4036, %4033, %4034, %4032, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.153 : Tensor = aten::relu_(%4048) # torch/nn/functional.py:1117:17
      %4050 : Tensor = prim::GetAttr[name="weight"](%4025)
      %4051 : Tensor? = prim::GetAttr[name="bias"](%4025)
      %4052 : int[] = prim::ListConstruct(%27, %27)
      %4053 : int[] = prim::ListConstruct(%24, %24)
      %4054 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.153 : Tensor = aten::conv2d(%result.153, %4050, %4051, %4052, %4053, %4054, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.153)
  %4056 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1898)
  %4057 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1898)
  %4058 : int = aten::dim(%bottleneck_output.152) # torch/nn/modules/batchnorm.py:276:11
  %4059 : bool = aten::ne(%4058, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4059) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4060 : bool = prim::GetAttr[name="training"](%4057)
   = prim::If(%4060) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4061 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4057)
      %4062 : Tensor = aten::add(%4061, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4057, %4062)
      -> ()
    block1():
      -> ()
  %4063 : bool = prim::GetAttr[name="training"](%4057)
  %4064 : Tensor = prim::GetAttr[name="running_mean"](%4057)
  %4065 : Tensor = prim::GetAttr[name="running_var"](%4057)
  %4066 : Tensor = prim::GetAttr[name="weight"](%4057)
  %4067 : Tensor = prim::GetAttr[name="bias"](%4057)
   = prim::If(%4063) # torch/nn/functional.py:2011:4
    block0():
      %4068 : int[] = aten::size(%bottleneck_output.152) # torch/nn/functional.py:2012:27
      %size_prods.628 : int = aten::__getitem__(%4068, %24) # torch/nn/functional.py:1991:17
      %4070 : int = aten::len(%4068) # torch/nn/functional.py:1992:19
      %4071 : int = aten::sub(%4070, %26) # torch/nn/functional.py:1992:19
      %size_prods.629 : int = prim::Loop(%4071, %25, %size_prods.628) # torch/nn/functional.py:1992:4
        block0(%i.158 : int, %size_prods.630 : int):
          %4075 : int = aten::add(%i.158, %26) # torch/nn/functional.py:1993:27
          %4076 : int = aten::__getitem__(%4068, %4075) # torch/nn/functional.py:1993:22
          %size_prods.631 : int = aten::mul(%size_prods.630, %4076) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.631)
      %4078 : bool = aten::eq(%size_prods.629, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4078) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4079 : Tensor = aten::batch_norm(%bottleneck_output.152, %4066, %4067, %4064, %4065, %4063, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.154 : Tensor = aten::relu_(%4079) # torch/nn/functional.py:1117:17
  %4081 : Tensor = prim::GetAttr[name="weight"](%4056)
  %4082 : Tensor? = prim::GetAttr[name="bias"](%4056)
  %4083 : int[] = prim::ListConstruct(%27, %27)
  %4084 : int[] = prim::ListConstruct(%27, %27)
  %4085 : int[] = prim::ListConstruct(%27, %27)
  %new_features.153 : Tensor = aten::conv2d(%result.154, %4081, %4082, %4083, %4084, %4085, %27) # torch/nn/modules/conv.py:415:15
  %4087 : float = prim::GetAttr[name="drop_rate"](%1898)
  %4088 : bool = aten::gt(%4087, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.154 : Tensor = prim::If(%4088) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4090 : float = prim::GetAttr[name="drop_rate"](%1898)
      %4091 : bool = prim::GetAttr[name="training"](%1898)
      %4092 : bool = aten::lt(%4090, %16) # torch/nn/functional.py:968:7
      %4093 : bool = prim::If(%4092) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4094 : bool = aten::gt(%4090, %17) # torch/nn/functional.py:968:17
          -> (%4094)
       = prim::If(%4093) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4095 : Tensor = aten::dropout(%new_features.153, %4090, %4091) # torch/nn/functional.py:973:17
      -> (%4095)
    block1():
      -> (%new_features.153)
  %4096 : Tensor[] = aten::append(%features.4, %new_features.154) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4097 : Tensor = prim::Uninitialized()
  %4098 : bool = prim::GetAttr[name="memory_efficient"](%1899)
  %4099 : bool = prim::If(%4098) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4100 : bool = prim::Uninitialized()
      %4101 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4102 : bool = aten::gt(%4101, %24)
      %4103 : bool, %4104 : bool, %4105 : int = prim::Loop(%18, %4102, %19, %4100, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4106 : int, %4107 : bool, %4108 : bool, %4109 : int):
          %tensor.25 : Tensor = aten::__getitem__(%features.4, %4109) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4111 : bool = prim::requires_grad(%tensor.25)
          %4112 : bool, %4113 : bool = prim::If(%4111) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4100)
          %4114 : int = aten::add(%4109, %27)
          %4115 : bool = aten::lt(%4114, %4101)
          %4116 : bool = aten::__and__(%4115, %4112)
          -> (%4116, %4111, %4113, %4114)
      %4117 : bool = prim::If(%4103)
        block0():
          -> (%4104)
        block1():
          -> (%19)
      -> (%4117)
    block1():
      -> (%19)
  %bottleneck_output.48 : Tensor = prim::If(%4099) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4097)
    block1():
      %concated_features.25 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4120 : __torch__.torch.nn.modules.conv.___torch_mangle_247.Conv2d = prim::GetAttr[name="conv1"](%1899)
      %4121 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_246.BatchNorm2d = prim::GetAttr[name="norm1"](%1899)
      %4122 : int = aten::dim(%concated_features.25) # torch/nn/modules/batchnorm.py:276:11
      %4123 : bool = aten::ne(%4122, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4123) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4124 : bool = prim::GetAttr[name="training"](%4121)
       = prim::If(%4124) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4125 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4121)
          %4126 : Tensor = aten::add(%4125, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4121, %4126)
          -> ()
        block1():
          -> ()
      %4127 : bool = prim::GetAttr[name="training"](%4121)
      %4128 : Tensor = prim::GetAttr[name="running_mean"](%4121)
      %4129 : Tensor = prim::GetAttr[name="running_var"](%4121)
      %4130 : Tensor = prim::GetAttr[name="weight"](%4121)
      %4131 : Tensor = prim::GetAttr[name="bias"](%4121)
       = prim::If(%4127) # torch/nn/functional.py:2011:4
        block0():
          %4132 : int[] = aten::size(%concated_features.25) # torch/nn/functional.py:2012:27
          %size_prods.192 : int = aten::__getitem__(%4132, %24) # torch/nn/functional.py:1991:17
          %4134 : int = aten::len(%4132) # torch/nn/functional.py:1992:19
          %4135 : int = aten::sub(%4134, %26) # torch/nn/functional.py:1992:19
          %size_prods.193 : int = prim::Loop(%4135, %25, %size_prods.192) # torch/nn/functional.py:1992:4
            block0(%i.49 : int, %size_prods.194 : int):
              %4139 : int = aten::add(%i.49, %26) # torch/nn/functional.py:1993:27
              %4140 : int = aten::__getitem__(%4132, %4139) # torch/nn/functional.py:1993:22
              %size_prods.195 : int = aten::mul(%size_prods.194, %4140) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.195)
          %4142 : bool = aten::eq(%size_prods.193, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4142) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4143 : Tensor = aten::batch_norm(%concated_features.25, %4130, %4131, %4128, %4129, %4127, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.49 : Tensor = aten::relu_(%4143) # torch/nn/functional.py:1117:17
      %4145 : Tensor = prim::GetAttr[name="weight"](%4120)
      %4146 : Tensor? = prim::GetAttr[name="bias"](%4120)
      %4147 : int[] = prim::ListConstruct(%27, %27)
      %4148 : int[] = prim::ListConstruct(%24, %24)
      %4149 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.49 : Tensor = aten::conv2d(%result.49, %4145, %4146, %4147, %4148, %4149, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.49)
  %4151 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1899)
  %4152 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1899)
  %4153 : int = aten::dim(%bottleneck_output.48) # torch/nn/modules/batchnorm.py:276:11
  %4154 : bool = aten::ne(%4153, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4154) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4155 : bool = prim::GetAttr[name="training"](%4152)
   = prim::If(%4155) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4156 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4152)
      %4157 : Tensor = aten::add(%4156, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4152, %4157)
      -> ()
    block1():
      -> ()
  %4158 : bool = prim::GetAttr[name="training"](%4152)
  %4159 : Tensor = prim::GetAttr[name="running_mean"](%4152)
  %4160 : Tensor = prim::GetAttr[name="running_var"](%4152)
  %4161 : Tensor = prim::GetAttr[name="weight"](%4152)
  %4162 : Tensor = prim::GetAttr[name="bias"](%4152)
   = prim::If(%4158) # torch/nn/functional.py:2011:4
    block0():
      %4163 : int[] = aten::size(%bottleneck_output.48) # torch/nn/functional.py:2012:27
      %size_prods.196 : int = aten::__getitem__(%4163, %24) # torch/nn/functional.py:1991:17
      %4165 : int = aten::len(%4163) # torch/nn/functional.py:1992:19
      %4166 : int = aten::sub(%4165, %26) # torch/nn/functional.py:1992:19
      %size_prods.197 : int = prim::Loop(%4166, %25, %size_prods.196) # torch/nn/functional.py:1992:4
        block0(%i.50 : int, %size_prods.198 : int):
          %4170 : int = aten::add(%i.50, %26) # torch/nn/functional.py:1993:27
          %4171 : int = aten::__getitem__(%4163, %4170) # torch/nn/functional.py:1993:22
          %size_prods.199 : int = aten::mul(%size_prods.198, %4171) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.199)
      %4173 : bool = aten::eq(%size_prods.197, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4173) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4174 : Tensor = aten::batch_norm(%bottleneck_output.48, %4161, %4162, %4159, %4160, %4158, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.50 : Tensor = aten::relu_(%4174) # torch/nn/functional.py:1117:17
  %4176 : Tensor = prim::GetAttr[name="weight"](%4151)
  %4177 : Tensor? = prim::GetAttr[name="bias"](%4151)
  %4178 : int[] = prim::ListConstruct(%27, %27)
  %4179 : int[] = prim::ListConstruct(%27, %27)
  %4180 : int[] = prim::ListConstruct(%27, %27)
  %new_features.50 : Tensor = aten::conv2d(%result.50, %4176, %4177, %4178, %4179, %4180, %27) # torch/nn/modules/conv.py:415:15
  %4182 : float = prim::GetAttr[name="drop_rate"](%1899)
  %4183 : bool = aten::gt(%4182, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.156 : Tensor = prim::If(%4183) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4185 : float = prim::GetAttr[name="drop_rate"](%1899)
      %4186 : bool = prim::GetAttr[name="training"](%1899)
      %4187 : bool = aten::lt(%4185, %16) # torch/nn/functional.py:968:7
      %4188 : bool = prim::If(%4187) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4189 : bool = aten::gt(%4185, %17) # torch/nn/functional.py:968:17
          -> (%4189)
       = prim::If(%4188) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4190 : Tensor = aten::dropout(%new_features.50, %4185, %4186) # torch/nn/functional.py:973:17
      -> (%4190)
    block1():
      -> (%new_features.50)
  %4191 : Tensor[] = aten::append(%features.4, %new_features.156) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4192 : Tensor = prim::Uninitialized()
  %4193 : bool = prim::GetAttr[name="memory_efficient"](%1900)
  %4194 : bool = prim::If(%4193) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4195 : bool = prim::Uninitialized()
      %4196 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4197 : bool = aten::gt(%4196, %24)
      %4198 : bool, %4199 : bool, %4200 : int = prim::Loop(%18, %4197, %19, %4195, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4201 : int, %4202 : bool, %4203 : bool, %4204 : int):
          %tensor.26 : Tensor = aten::__getitem__(%features.4, %4204) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4206 : bool = prim::requires_grad(%tensor.26)
          %4207 : bool, %4208 : bool = prim::If(%4206) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4195)
          %4209 : int = aten::add(%4204, %27)
          %4210 : bool = aten::lt(%4209, %4196)
          %4211 : bool = aten::__and__(%4210, %4207)
          -> (%4211, %4206, %4208, %4209)
      %4212 : bool = prim::If(%4198)
        block0():
          -> (%4199)
        block1():
          -> (%19)
      -> (%4212)
    block1():
      -> (%19)
  %bottleneck_output.50 : Tensor = prim::If(%4194) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4192)
    block1():
      %concated_features.26 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4215 : __torch__.torch.nn.modules.conv.___torch_mangle_250.Conv2d = prim::GetAttr[name="conv1"](%1900)
      %4216 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_249.BatchNorm2d = prim::GetAttr[name="norm1"](%1900)
      %4217 : int = aten::dim(%concated_features.26) # torch/nn/modules/batchnorm.py:276:11
      %4218 : bool = aten::ne(%4217, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4218) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4219 : bool = prim::GetAttr[name="training"](%4216)
       = prim::If(%4219) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4220 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4216)
          %4221 : Tensor = aten::add(%4220, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4216, %4221)
          -> ()
        block1():
          -> ()
      %4222 : bool = prim::GetAttr[name="training"](%4216)
      %4223 : Tensor = prim::GetAttr[name="running_mean"](%4216)
      %4224 : Tensor = prim::GetAttr[name="running_var"](%4216)
      %4225 : Tensor = prim::GetAttr[name="weight"](%4216)
      %4226 : Tensor = prim::GetAttr[name="bias"](%4216)
       = prim::If(%4222) # torch/nn/functional.py:2011:4
        block0():
          %4227 : int[] = aten::size(%concated_features.26) # torch/nn/functional.py:2012:27
          %size_prods.200 : int = aten::__getitem__(%4227, %24) # torch/nn/functional.py:1991:17
          %4229 : int = aten::len(%4227) # torch/nn/functional.py:1992:19
          %4230 : int = aten::sub(%4229, %26) # torch/nn/functional.py:1992:19
          %size_prods.201 : int = prim::Loop(%4230, %25, %size_prods.200) # torch/nn/functional.py:1992:4
            block0(%i.51 : int, %size_prods.202 : int):
              %4234 : int = aten::add(%i.51, %26) # torch/nn/functional.py:1993:27
              %4235 : int = aten::__getitem__(%4227, %4234) # torch/nn/functional.py:1993:22
              %size_prods.203 : int = aten::mul(%size_prods.202, %4235) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.203)
          %4237 : bool = aten::eq(%size_prods.201, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4237) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4238 : Tensor = aten::batch_norm(%concated_features.26, %4225, %4226, %4223, %4224, %4222, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.51 : Tensor = aten::relu_(%4238) # torch/nn/functional.py:1117:17
      %4240 : Tensor = prim::GetAttr[name="weight"](%4215)
      %4241 : Tensor? = prim::GetAttr[name="bias"](%4215)
      %4242 : int[] = prim::ListConstruct(%27, %27)
      %4243 : int[] = prim::ListConstruct(%24, %24)
      %4244 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.51 : Tensor = aten::conv2d(%result.51, %4240, %4241, %4242, %4243, %4244, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.51)
  %4246 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1900)
  %4247 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1900)
  %4248 : int = aten::dim(%bottleneck_output.50) # torch/nn/modules/batchnorm.py:276:11
  %4249 : bool = aten::ne(%4248, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4249) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4250 : bool = prim::GetAttr[name="training"](%4247)
   = prim::If(%4250) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4251 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4247)
      %4252 : Tensor = aten::add(%4251, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4247, %4252)
      -> ()
    block1():
      -> ()
  %4253 : bool = prim::GetAttr[name="training"](%4247)
  %4254 : Tensor = prim::GetAttr[name="running_mean"](%4247)
  %4255 : Tensor = prim::GetAttr[name="running_var"](%4247)
  %4256 : Tensor = prim::GetAttr[name="weight"](%4247)
  %4257 : Tensor = prim::GetAttr[name="bias"](%4247)
   = prim::If(%4253) # torch/nn/functional.py:2011:4
    block0():
      %4258 : int[] = aten::size(%bottleneck_output.50) # torch/nn/functional.py:2012:27
      %size_prods.204 : int = aten::__getitem__(%4258, %24) # torch/nn/functional.py:1991:17
      %4260 : int = aten::len(%4258) # torch/nn/functional.py:1992:19
      %4261 : int = aten::sub(%4260, %26) # torch/nn/functional.py:1992:19
      %size_prods.205 : int = prim::Loop(%4261, %25, %size_prods.204) # torch/nn/functional.py:1992:4
        block0(%i.52 : int, %size_prods.206 : int):
          %4265 : int = aten::add(%i.52, %26) # torch/nn/functional.py:1993:27
          %4266 : int = aten::__getitem__(%4258, %4265) # torch/nn/functional.py:1993:22
          %size_prods.207 : int = aten::mul(%size_prods.206, %4266) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.207)
      %4268 : bool = aten::eq(%size_prods.205, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4268) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4269 : Tensor = aten::batch_norm(%bottleneck_output.50, %4256, %4257, %4254, %4255, %4253, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.52 : Tensor = aten::relu_(%4269) # torch/nn/functional.py:1117:17
  %4271 : Tensor = prim::GetAttr[name="weight"](%4246)
  %4272 : Tensor? = prim::GetAttr[name="bias"](%4246)
  %4273 : int[] = prim::ListConstruct(%27, %27)
  %4274 : int[] = prim::ListConstruct(%27, %27)
  %4275 : int[] = prim::ListConstruct(%27, %27)
  %new_features.52 : Tensor = aten::conv2d(%result.52, %4271, %4272, %4273, %4274, %4275, %27) # torch/nn/modules/conv.py:415:15
  %4277 : float = prim::GetAttr[name="drop_rate"](%1900)
  %4278 : bool = aten::gt(%4277, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.49 : Tensor = prim::If(%4278) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4280 : float = prim::GetAttr[name="drop_rate"](%1900)
      %4281 : bool = prim::GetAttr[name="training"](%1900)
      %4282 : bool = aten::lt(%4280, %16) # torch/nn/functional.py:968:7
      %4283 : bool = prim::If(%4282) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4284 : bool = aten::gt(%4280, %17) # torch/nn/functional.py:968:17
          -> (%4284)
       = prim::If(%4283) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4285 : Tensor = aten::dropout(%new_features.52, %4280, %4281) # torch/nn/functional.py:973:17
      -> (%4285)
    block1():
      -> (%new_features.52)
  %4286 : Tensor[] = aten::append(%features.4, %new_features.49) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4287 : Tensor = prim::Uninitialized()
  %4288 : bool = prim::GetAttr[name="memory_efficient"](%1901)
  %4289 : bool = prim::If(%4288) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4290 : bool = prim::Uninitialized()
      %4291 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4292 : bool = aten::gt(%4291, %24)
      %4293 : bool, %4294 : bool, %4295 : int = prim::Loop(%18, %4292, %19, %4290, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4296 : int, %4297 : bool, %4298 : bool, %4299 : int):
          %tensor.27 : Tensor = aten::__getitem__(%features.4, %4299) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4301 : bool = prim::requires_grad(%tensor.27)
          %4302 : bool, %4303 : bool = prim::If(%4301) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4290)
          %4304 : int = aten::add(%4299, %27)
          %4305 : bool = aten::lt(%4304, %4291)
          %4306 : bool = aten::__and__(%4305, %4302)
          -> (%4306, %4301, %4303, %4304)
      %4307 : bool = prim::If(%4293)
        block0():
          -> (%4294)
        block1():
          -> (%19)
      -> (%4307)
    block1():
      -> (%19)
  %bottleneck_output.52 : Tensor = prim::If(%4289) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4287)
    block1():
      %concated_features.27 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4310 : __torch__.torch.nn.modules.conv.___torch_mangle_253.Conv2d = prim::GetAttr[name="conv1"](%1901)
      %4311 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_252.BatchNorm2d = prim::GetAttr[name="norm1"](%1901)
      %4312 : int = aten::dim(%concated_features.27) # torch/nn/modules/batchnorm.py:276:11
      %4313 : bool = aten::ne(%4312, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4313) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4314 : bool = prim::GetAttr[name="training"](%4311)
       = prim::If(%4314) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4315 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4311)
          %4316 : Tensor = aten::add(%4315, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4311, %4316)
          -> ()
        block1():
          -> ()
      %4317 : bool = prim::GetAttr[name="training"](%4311)
      %4318 : Tensor = prim::GetAttr[name="running_mean"](%4311)
      %4319 : Tensor = prim::GetAttr[name="running_var"](%4311)
      %4320 : Tensor = prim::GetAttr[name="weight"](%4311)
      %4321 : Tensor = prim::GetAttr[name="bias"](%4311)
       = prim::If(%4317) # torch/nn/functional.py:2011:4
        block0():
          %4322 : int[] = aten::size(%concated_features.27) # torch/nn/functional.py:2012:27
          %size_prods.208 : int = aten::__getitem__(%4322, %24) # torch/nn/functional.py:1991:17
          %4324 : int = aten::len(%4322) # torch/nn/functional.py:1992:19
          %4325 : int = aten::sub(%4324, %26) # torch/nn/functional.py:1992:19
          %size_prods.209 : int = prim::Loop(%4325, %25, %size_prods.208) # torch/nn/functional.py:1992:4
            block0(%i.53 : int, %size_prods.210 : int):
              %4329 : int = aten::add(%i.53, %26) # torch/nn/functional.py:1993:27
              %4330 : int = aten::__getitem__(%4322, %4329) # torch/nn/functional.py:1993:22
              %size_prods.211 : int = aten::mul(%size_prods.210, %4330) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.211)
          %4332 : bool = aten::eq(%size_prods.209, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4332) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4333 : Tensor = aten::batch_norm(%concated_features.27, %4320, %4321, %4318, %4319, %4317, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.53 : Tensor = aten::relu_(%4333) # torch/nn/functional.py:1117:17
      %4335 : Tensor = prim::GetAttr[name="weight"](%4310)
      %4336 : Tensor? = prim::GetAttr[name="bias"](%4310)
      %4337 : int[] = prim::ListConstruct(%27, %27)
      %4338 : int[] = prim::ListConstruct(%24, %24)
      %4339 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.53 : Tensor = aten::conv2d(%result.53, %4335, %4336, %4337, %4338, %4339, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.53)
  %4341 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1901)
  %4342 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1901)
  %4343 : int = aten::dim(%bottleneck_output.52) # torch/nn/modules/batchnorm.py:276:11
  %4344 : bool = aten::ne(%4343, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4344) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4345 : bool = prim::GetAttr[name="training"](%4342)
   = prim::If(%4345) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4346 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4342)
      %4347 : Tensor = aten::add(%4346, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4342, %4347)
      -> ()
    block1():
      -> ()
  %4348 : bool = prim::GetAttr[name="training"](%4342)
  %4349 : Tensor = prim::GetAttr[name="running_mean"](%4342)
  %4350 : Tensor = prim::GetAttr[name="running_var"](%4342)
  %4351 : Tensor = prim::GetAttr[name="weight"](%4342)
  %4352 : Tensor = prim::GetAttr[name="bias"](%4342)
   = prim::If(%4348) # torch/nn/functional.py:2011:4
    block0():
      %4353 : int[] = aten::size(%bottleneck_output.52) # torch/nn/functional.py:2012:27
      %size_prods.212 : int = aten::__getitem__(%4353, %24) # torch/nn/functional.py:1991:17
      %4355 : int = aten::len(%4353) # torch/nn/functional.py:1992:19
      %4356 : int = aten::sub(%4355, %26) # torch/nn/functional.py:1992:19
      %size_prods.213 : int = prim::Loop(%4356, %25, %size_prods.212) # torch/nn/functional.py:1992:4
        block0(%i.54 : int, %size_prods.214 : int):
          %4360 : int = aten::add(%i.54, %26) # torch/nn/functional.py:1993:27
          %4361 : int = aten::__getitem__(%4353, %4360) # torch/nn/functional.py:1993:22
          %size_prods.215 : int = aten::mul(%size_prods.214, %4361) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.215)
      %4363 : bool = aten::eq(%size_prods.213, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4363) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4364 : Tensor = aten::batch_norm(%bottleneck_output.52, %4351, %4352, %4349, %4350, %4348, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.54 : Tensor = aten::relu_(%4364) # torch/nn/functional.py:1117:17
  %4366 : Tensor = prim::GetAttr[name="weight"](%4341)
  %4367 : Tensor? = prim::GetAttr[name="bias"](%4341)
  %4368 : int[] = prim::ListConstruct(%27, %27)
  %4369 : int[] = prim::ListConstruct(%27, %27)
  %4370 : int[] = prim::ListConstruct(%27, %27)
  %new_features.54 : Tensor = aten::conv2d(%result.54, %4366, %4367, %4368, %4369, %4370, %27) # torch/nn/modules/conv.py:415:15
  %4372 : float = prim::GetAttr[name="drop_rate"](%1901)
  %4373 : bool = aten::gt(%4372, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.51 : Tensor = prim::If(%4373) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4375 : float = prim::GetAttr[name="drop_rate"](%1901)
      %4376 : bool = prim::GetAttr[name="training"](%1901)
      %4377 : bool = aten::lt(%4375, %16) # torch/nn/functional.py:968:7
      %4378 : bool = prim::If(%4377) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4379 : bool = aten::gt(%4375, %17) # torch/nn/functional.py:968:17
          -> (%4379)
       = prim::If(%4378) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4380 : Tensor = aten::dropout(%new_features.54, %4375, %4376) # torch/nn/functional.py:973:17
      -> (%4380)
    block1():
      -> (%new_features.54)
  %4381 : Tensor[] = aten::append(%features.4, %new_features.51) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4382 : Tensor = prim::Uninitialized()
  %4383 : bool = prim::GetAttr[name="memory_efficient"](%1902)
  %4384 : bool = prim::If(%4383) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4385 : bool = prim::Uninitialized()
      %4386 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4387 : bool = aten::gt(%4386, %24)
      %4388 : bool, %4389 : bool, %4390 : int = prim::Loop(%18, %4387, %19, %4385, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4391 : int, %4392 : bool, %4393 : bool, %4394 : int):
          %tensor.28 : Tensor = aten::__getitem__(%features.4, %4394) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4396 : bool = prim::requires_grad(%tensor.28)
          %4397 : bool, %4398 : bool = prim::If(%4396) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4385)
          %4399 : int = aten::add(%4394, %27)
          %4400 : bool = aten::lt(%4399, %4386)
          %4401 : bool = aten::__and__(%4400, %4397)
          -> (%4401, %4396, %4398, %4399)
      %4402 : bool = prim::If(%4388)
        block0():
          -> (%4389)
        block1():
          -> (%19)
      -> (%4402)
    block1():
      -> (%19)
  %bottleneck_output.54 : Tensor = prim::If(%4384) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4382)
    block1():
      %concated_features.28 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4405 : __torch__.torch.nn.modules.conv.___torch_mangle_256.Conv2d = prim::GetAttr[name="conv1"](%1902)
      %4406 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_255.BatchNorm2d = prim::GetAttr[name="norm1"](%1902)
      %4407 : int = aten::dim(%concated_features.28) # torch/nn/modules/batchnorm.py:276:11
      %4408 : bool = aten::ne(%4407, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4408) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4409 : bool = prim::GetAttr[name="training"](%4406)
       = prim::If(%4409) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4410 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4406)
          %4411 : Tensor = aten::add(%4410, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4406, %4411)
          -> ()
        block1():
          -> ()
      %4412 : bool = prim::GetAttr[name="training"](%4406)
      %4413 : Tensor = prim::GetAttr[name="running_mean"](%4406)
      %4414 : Tensor = prim::GetAttr[name="running_var"](%4406)
      %4415 : Tensor = prim::GetAttr[name="weight"](%4406)
      %4416 : Tensor = prim::GetAttr[name="bias"](%4406)
       = prim::If(%4412) # torch/nn/functional.py:2011:4
        block0():
          %4417 : int[] = aten::size(%concated_features.28) # torch/nn/functional.py:2012:27
          %size_prods.216 : int = aten::__getitem__(%4417, %24) # torch/nn/functional.py:1991:17
          %4419 : int = aten::len(%4417) # torch/nn/functional.py:1992:19
          %4420 : int = aten::sub(%4419, %26) # torch/nn/functional.py:1992:19
          %size_prods.217 : int = prim::Loop(%4420, %25, %size_prods.216) # torch/nn/functional.py:1992:4
            block0(%i.55 : int, %size_prods.218 : int):
              %4424 : int = aten::add(%i.55, %26) # torch/nn/functional.py:1993:27
              %4425 : int = aten::__getitem__(%4417, %4424) # torch/nn/functional.py:1993:22
              %size_prods.219 : int = aten::mul(%size_prods.218, %4425) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.219)
          %4427 : bool = aten::eq(%size_prods.217, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4427) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4428 : Tensor = aten::batch_norm(%concated_features.28, %4415, %4416, %4413, %4414, %4412, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.55 : Tensor = aten::relu_(%4428) # torch/nn/functional.py:1117:17
      %4430 : Tensor = prim::GetAttr[name="weight"](%4405)
      %4431 : Tensor? = prim::GetAttr[name="bias"](%4405)
      %4432 : int[] = prim::ListConstruct(%27, %27)
      %4433 : int[] = prim::ListConstruct(%24, %24)
      %4434 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.55 : Tensor = aten::conv2d(%result.55, %4430, %4431, %4432, %4433, %4434, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.55)
  %4436 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1902)
  %4437 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1902)
  %4438 : int = aten::dim(%bottleneck_output.54) # torch/nn/modules/batchnorm.py:276:11
  %4439 : bool = aten::ne(%4438, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4439) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4440 : bool = prim::GetAttr[name="training"](%4437)
   = prim::If(%4440) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4441 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4437)
      %4442 : Tensor = aten::add(%4441, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4437, %4442)
      -> ()
    block1():
      -> ()
  %4443 : bool = prim::GetAttr[name="training"](%4437)
  %4444 : Tensor = prim::GetAttr[name="running_mean"](%4437)
  %4445 : Tensor = prim::GetAttr[name="running_var"](%4437)
  %4446 : Tensor = prim::GetAttr[name="weight"](%4437)
  %4447 : Tensor = prim::GetAttr[name="bias"](%4437)
   = prim::If(%4443) # torch/nn/functional.py:2011:4
    block0():
      %4448 : int[] = aten::size(%bottleneck_output.54) # torch/nn/functional.py:2012:27
      %size_prods.220 : int = aten::__getitem__(%4448, %24) # torch/nn/functional.py:1991:17
      %4450 : int = aten::len(%4448) # torch/nn/functional.py:1992:19
      %4451 : int = aten::sub(%4450, %26) # torch/nn/functional.py:1992:19
      %size_prods.221 : int = prim::Loop(%4451, %25, %size_prods.220) # torch/nn/functional.py:1992:4
        block0(%i.56 : int, %size_prods.222 : int):
          %4455 : int = aten::add(%i.56, %26) # torch/nn/functional.py:1993:27
          %4456 : int = aten::__getitem__(%4448, %4455) # torch/nn/functional.py:1993:22
          %size_prods.223 : int = aten::mul(%size_prods.222, %4456) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.223)
      %4458 : bool = aten::eq(%size_prods.221, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4458) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4459 : Tensor = aten::batch_norm(%bottleneck_output.54, %4446, %4447, %4444, %4445, %4443, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.56 : Tensor = aten::relu_(%4459) # torch/nn/functional.py:1117:17
  %4461 : Tensor = prim::GetAttr[name="weight"](%4436)
  %4462 : Tensor? = prim::GetAttr[name="bias"](%4436)
  %4463 : int[] = prim::ListConstruct(%27, %27)
  %4464 : int[] = prim::ListConstruct(%27, %27)
  %4465 : int[] = prim::ListConstruct(%27, %27)
  %new_features.56 : Tensor = aten::conv2d(%result.56, %4461, %4462, %4463, %4464, %4465, %27) # torch/nn/modules/conv.py:415:15
  %4467 : float = prim::GetAttr[name="drop_rate"](%1902)
  %4468 : bool = aten::gt(%4467, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.53 : Tensor = prim::If(%4468) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4470 : float = prim::GetAttr[name="drop_rate"](%1902)
      %4471 : bool = prim::GetAttr[name="training"](%1902)
      %4472 : bool = aten::lt(%4470, %16) # torch/nn/functional.py:968:7
      %4473 : bool = prim::If(%4472) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4474 : bool = aten::gt(%4470, %17) # torch/nn/functional.py:968:17
          -> (%4474)
       = prim::If(%4473) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4475 : Tensor = aten::dropout(%new_features.56, %4470, %4471) # torch/nn/functional.py:973:17
      -> (%4475)
    block1():
      -> (%new_features.56)
  %4476 : Tensor[] = aten::append(%features.4, %new_features.53) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4477 : Tensor = prim::Uninitialized()
  %4478 : bool = prim::GetAttr[name="memory_efficient"](%1903)
  %4479 : bool = prim::If(%4478) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4480 : bool = prim::Uninitialized()
      %4481 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4482 : bool = aten::gt(%4481, %24)
      %4483 : bool, %4484 : bool, %4485 : int = prim::Loop(%18, %4482, %19, %4480, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4486 : int, %4487 : bool, %4488 : bool, %4489 : int):
          %tensor.29 : Tensor = aten::__getitem__(%features.4, %4489) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4491 : bool = prim::requires_grad(%tensor.29)
          %4492 : bool, %4493 : bool = prim::If(%4491) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4480)
          %4494 : int = aten::add(%4489, %27)
          %4495 : bool = aten::lt(%4494, %4481)
          %4496 : bool = aten::__and__(%4495, %4492)
          -> (%4496, %4491, %4493, %4494)
      %4497 : bool = prim::If(%4483)
        block0():
          -> (%4484)
        block1():
          -> (%19)
      -> (%4497)
    block1():
      -> (%19)
  %bottleneck_output.56 : Tensor = prim::If(%4479) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4477)
    block1():
      %concated_features.29 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4500 : __torch__.torch.nn.modules.conv.___torch_mangle_259.Conv2d = prim::GetAttr[name="conv1"](%1903)
      %4501 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_258.BatchNorm2d = prim::GetAttr[name="norm1"](%1903)
      %4502 : int = aten::dim(%concated_features.29) # torch/nn/modules/batchnorm.py:276:11
      %4503 : bool = aten::ne(%4502, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4503) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4504 : bool = prim::GetAttr[name="training"](%4501)
       = prim::If(%4504) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4505 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4501)
          %4506 : Tensor = aten::add(%4505, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4501, %4506)
          -> ()
        block1():
          -> ()
      %4507 : bool = prim::GetAttr[name="training"](%4501)
      %4508 : Tensor = prim::GetAttr[name="running_mean"](%4501)
      %4509 : Tensor = prim::GetAttr[name="running_var"](%4501)
      %4510 : Tensor = prim::GetAttr[name="weight"](%4501)
      %4511 : Tensor = prim::GetAttr[name="bias"](%4501)
       = prim::If(%4507) # torch/nn/functional.py:2011:4
        block0():
          %4512 : int[] = aten::size(%concated_features.29) # torch/nn/functional.py:2012:27
          %size_prods.224 : int = aten::__getitem__(%4512, %24) # torch/nn/functional.py:1991:17
          %4514 : int = aten::len(%4512) # torch/nn/functional.py:1992:19
          %4515 : int = aten::sub(%4514, %26) # torch/nn/functional.py:1992:19
          %size_prods.225 : int = prim::Loop(%4515, %25, %size_prods.224) # torch/nn/functional.py:1992:4
            block0(%i.57 : int, %size_prods.226 : int):
              %4519 : int = aten::add(%i.57, %26) # torch/nn/functional.py:1993:27
              %4520 : int = aten::__getitem__(%4512, %4519) # torch/nn/functional.py:1993:22
              %size_prods.227 : int = aten::mul(%size_prods.226, %4520) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.227)
          %4522 : bool = aten::eq(%size_prods.225, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4522) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4523 : Tensor = aten::batch_norm(%concated_features.29, %4510, %4511, %4508, %4509, %4507, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.57 : Tensor = aten::relu_(%4523) # torch/nn/functional.py:1117:17
      %4525 : Tensor = prim::GetAttr[name="weight"](%4500)
      %4526 : Tensor? = prim::GetAttr[name="bias"](%4500)
      %4527 : int[] = prim::ListConstruct(%27, %27)
      %4528 : int[] = prim::ListConstruct(%24, %24)
      %4529 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.57 : Tensor = aten::conv2d(%result.57, %4525, %4526, %4527, %4528, %4529, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.57)
  %4531 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1903)
  %4532 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1903)
  %4533 : int = aten::dim(%bottleneck_output.56) # torch/nn/modules/batchnorm.py:276:11
  %4534 : bool = aten::ne(%4533, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4534) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4535 : bool = prim::GetAttr[name="training"](%4532)
   = prim::If(%4535) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4536 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4532)
      %4537 : Tensor = aten::add(%4536, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4532, %4537)
      -> ()
    block1():
      -> ()
  %4538 : bool = prim::GetAttr[name="training"](%4532)
  %4539 : Tensor = prim::GetAttr[name="running_mean"](%4532)
  %4540 : Tensor = prim::GetAttr[name="running_var"](%4532)
  %4541 : Tensor = prim::GetAttr[name="weight"](%4532)
  %4542 : Tensor = prim::GetAttr[name="bias"](%4532)
   = prim::If(%4538) # torch/nn/functional.py:2011:4
    block0():
      %4543 : int[] = aten::size(%bottleneck_output.56) # torch/nn/functional.py:2012:27
      %size_prods.228 : int = aten::__getitem__(%4543, %24) # torch/nn/functional.py:1991:17
      %4545 : int = aten::len(%4543) # torch/nn/functional.py:1992:19
      %4546 : int = aten::sub(%4545, %26) # torch/nn/functional.py:1992:19
      %size_prods.229 : int = prim::Loop(%4546, %25, %size_prods.228) # torch/nn/functional.py:1992:4
        block0(%i.58 : int, %size_prods.230 : int):
          %4550 : int = aten::add(%i.58, %26) # torch/nn/functional.py:1993:27
          %4551 : int = aten::__getitem__(%4543, %4550) # torch/nn/functional.py:1993:22
          %size_prods.231 : int = aten::mul(%size_prods.230, %4551) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.231)
      %4553 : bool = aten::eq(%size_prods.229, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4553) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4554 : Tensor = aten::batch_norm(%bottleneck_output.56, %4541, %4542, %4539, %4540, %4538, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.58 : Tensor = aten::relu_(%4554) # torch/nn/functional.py:1117:17
  %4556 : Tensor = prim::GetAttr[name="weight"](%4531)
  %4557 : Tensor? = prim::GetAttr[name="bias"](%4531)
  %4558 : int[] = prim::ListConstruct(%27, %27)
  %4559 : int[] = prim::ListConstruct(%27, %27)
  %4560 : int[] = prim::ListConstruct(%27, %27)
  %new_features.58 : Tensor = aten::conv2d(%result.58, %4556, %4557, %4558, %4559, %4560, %27) # torch/nn/modules/conv.py:415:15
  %4562 : float = prim::GetAttr[name="drop_rate"](%1903)
  %4563 : bool = aten::gt(%4562, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.55 : Tensor = prim::If(%4563) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4565 : float = prim::GetAttr[name="drop_rate"](%1903)
      %4566 : bool = prim::GetAttr[name="training"](%1903)
      %4567 : bool = aten::lt(%4565, %16) # torch/nn/functional.py:968:7
      %4568 : bool = prim::If(%4567) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4569 : bool = aten::gt(%4565, %17) # torch/nn/functional.py:968:17
          -> (%4569)
       = prim::If(%4568) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4570 : Tensor = aten::dropout(%new_features.58, %4565, %4566) # torch/nn/functional.py:973:17
      -> (%4570)
    block1():
      -> (%new_features.58)
  %4571 : Tensor[] = aten::append(%features.4, %new_features.55) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4572 : Tensor = prim::Uninitialized()
  %4573 : bool = prim::GetAttr[name="memory_efficient"](%1904)
  %4574 : bool = prim::If(%4573) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4575 : bool = prim::Uninitialized()
      %4576 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4577 : bool = aten::gt(%4576, %24)
      %4578 : bool, %4579 : bool, %4580 : int = prim::Loop(%18, %4577, %19, %4575, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4581 : int, %4582 : bool, %4583 : bool, %4584 : int):
          %tensor.30 : Tensor = aten::__getitem__(%features.4, %4584) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4586 : bool = prim::requires_grad(%tensor.30)
          %4587 : bool, %4588 : bool = prim::If(%4586) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4575)
          %4589 : int = aten::add(%4584, %27)
          %4590 : bool = aten::lt(%4589, %4576)
          %4591 : bool = aten::__and__(%4590, %4587)
          -> (%4591, %4586, %4588, %4589)
      %4592 : bool = prim::If(%4578)
        block0():
          -> (%4579)
        block1():
          -> (%19)
      -> (%4592)
    block1():
      -> (%19)
  %bottleneck_output.58 : Tensor = prim::If(%4574) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4572)
    block1():
      %concated_features.30 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4595 : __torch__.torch.nn.modules.conv.___torch_mangle_262.Conv2d = prim::GetAttr[name="conv1"](%1904)
      %4596 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_261.BatchNorm2d = prim::GetAttr[name="norm1"](%1904)
      %4597 : int = aten::dim(%concated_features.30) # torch/nn/modules/batchnorm.py:276:11
      %4598 : bool = aten::ne(%4597, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4598) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4599 : bool = prim::GetAttr[name="training"](%4596)
       = prim::If(%4599) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4600 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4596)
          %4601 : Tensor = aten::add(%4600, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4596, %4601)
          -> ()
        block1():
          -> ()
      %4602 : bool = prim::GetAttr[name="training"](%4596)
      %4603 : Tensor = prim::GetAttr[name="running_mean"](%4596)
      %4604 : Tensor = prim::GetAttr[name="running_var"](%4596)
      %4605 : Tensor = prim::GetAttr[name="weight"](%4596)
      %4606 : Tensor = prim::GetAttr[name="bias"](%4596)
       = prim::If(%4602) # torch/nn/functional.py:2011:4
        block0():
          %4607 : int[] = aten::size(%concated_features.30) # torch/nn/functional.py:2012:27
          %size_prods.232 : int = aten::__getitem__(%4607, %24) # torch/nn/functional.py:1991:17
          %4609 : int = aten::len(%4607) # torch/nn/functional.py:1992:19
          %4610 : int = aten::sub(%4609, %26) # torch/nn/functional.py:1992:19
          %size_prods.233 : int = prim::Loop(%4610, %25, %size_prods.232) # torch/nn/functional.py:1992:4
            block0(%i.59 : int, %size_prods.234 : int):
              %4614 : int = aten::add(%i.59, %26) # torch/nn/functional.py:1993:27
              %4615 : int = aten::__getitem__(%4607, %4614) # torch/nn/functional.py:1993:22
              %size_prods.235 : int = aten::mul(%size_prods.234, %4615) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.235)
          %4617 : bool = aten::eq(%size_prods.233, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4617) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4618 : Tensor = aten::batch_norm(%concated_features.30, %4605, %4606, %4603, %4604, %4602, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.59 : Tensor = aten::relu_(%4618) # torch/nn/functional.py:1117:17
      %4620 : Tensor = prim::GetAttr[name="weight"](%4595)
      %4621 : Tensor? = prim::GetAttr[name="bias"](%4595)
      %4622 : int[] = prim::ListConstruct(%27, %27)
      %4623 : int[] = prim::ListConstruct(%24, %24)
      %4624 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.59 : Tensor = aten::conv2d(%result.59, %4620, %4621, %4622, %4623, %4624, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.59)
  %4626 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1904)
  %4627 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1904)
  %4628 : int = aten::dim(%bottleneck_output.58) # torch/nn/modules/batchnorm.py:276:11
  %4629 : bool = aten::ne(%4628, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4629) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4630 : bool = prim::GetAttr[name="training"](%4627)
   = prim::If(%4630) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4631 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4627)
      %4632 : Tensor = aten::add(%4631, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4627, %4632)
      -> ()
    block1():
      -> ()
  %4633 : bool = prim::GetAttr[name="training"](%4627)
  %4634 : Tensor = prim::GetAttr[name="running_mean"](%4627)
  %4635 : Tensor = prim::GetAttr[name="running_var"](%4627)
  %4636 : Tensor = prim::GetAttr[name="weight"](%4627)
  %4637 : Tensor = prim::GetAttr[name="bias"](%4627)
   = prim::If(%4633) # torch/nn/functional.py:2011:4
    block0():
      %4638 : int[] = aten::size(%bottleneck_output.58) # torch/nn/functional.py:2012:27
      %size_prods.236 : int = aten::__getitem__(%4638, %24) # torch/nn/functional.py:1991:17
      %4640 : int = aten::len(%4638) # torch/nn/functional.py:1992:19
      %4641 : int = aten::sub(%4640, %26) # torch/nn/functional.py:1992:19
      %size_prods.237 : int = prim::Loop(%4641, %25, %size_prods.236) # torch/nn/functional.py:1992:4
        block0(%i.60 : int, %size_prods.238 : int):
          %4645 : int = aten::add(%i.60, %26) # torch/nn/functional.py:1993:27
          %4646 : int = aten::__getitem__(%4638, %4645) # torch/nn/functional.py:1993:22
          %size_prods.239 : int = aten::mul(%size_prods.238, %4646) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.239)
      %4648 : bool = aten::eq(%size_prods.237, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4648) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4649 : Tensor = aten::batch_norm(%bottleneck_output.58, %4636, %4637, %4634, %4635, %4633, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.60 : Tensor = aten::relu_(%4649) # torch/nn/functional.py:1117:17
  %4651 : Tensor = prim::GetAttr[name="weight"](%4626)
  %4652 : Tensor? = prim::GetAttr[name="bias"](%4626)
  %4653 : int[] = prim::ListConstruct(%27, %27)
  %4654 : int[] = prim::ListConstruct(%27, %27)
  %4655 : int[] = prim::ListConstruct(%27, %27)
  %new_features.60 : Tensor = aten::conv2d(%result.60, %4651, %4652, %4653, %4654, %4655, %27) # torch/nn/modules/conv.py:415:15
  %4657 : float = prim::GetAttr[name="drop_rate"](%1904)
  %4658 : bool = aten::gt(%4657, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.57 : Tensor = prim::If(%4658) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4660 : float = prim::GetAttr[name="drop_rate"](%1904)
      %4661 : bool = prim::GetAttr[name="training"](%1904)
      %4662 : bool = aten::lt(%4660, %16) # torch/nn/functional.py:968:7
      %4663 : bool = prim::If(%4662) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4664 : bool = aten::gt(%4660, %17) # torch/nn/functional.py:968:17
          -> (%4664)
       = prim::If(%4663) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4665 : Tensor = aten::dropout(%new_features.60, %4660, %4661) # torch/nn/functional.py:973:17
      -> (%4665)
    block1():
      -> (%new_features.60)
  %4666 : Tensor[] = aten::append(%features.4, %new_features.57) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4667 : Tensor = prim::Uninitialized()
  %4668 : bool = prim::GetAttr[name="memory_efficient"](%1905)
  %4669 : bool = prim::If(%4668) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4670 : bool = prim::Uninitialized()
      %4671 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4672 : bool = aten::gt(%4671, %24)
      %4673 : bool, %4674 : bool, %4675 : int = prim::Loop(%18, %4672, %19, %4670, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4676 : int, %4677 : bool, %4678 : bool, %4679 : int):
          %tensor.31 : Tensor = aten::__getitem__(%features.4, %4679) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4681 : bool = prim::requires_grad(%tensor.31)
          %4682 : bool, %4683 : bool = prim::If(%4681) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4670)
          %4684 : int = aten::add(%4679, %27)
          %4685 : bool = aten::lt(%4684, %4671)
          %4686 : bool = aten::__and__(%4685, %4682)
          -> (%4686, %4681, %4683, %4684)
      %4687 : bool = prim::If(%4673)
        block0():
          -> (%4674)
        block1():
          -> (%19)
      -> (%4687)
    block1():
      -> (%19)
  %bottleneck_output.60 : Tensor = prim::If(%4669) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4667)
    block1():
      %concated_features.31 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4690 : __torch__.torch.nn.modules.conv.___torch_mangle_265.Conv2d = prim::GetAttr[name="conv1"](%1905)
      %4691 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_264.BatchNorm2d = prim::GetAttr[name="norm1"](%1905)
      %4692 : int = aten::dim(%concated_features.31) # torch/nn/modules/batchnorm.py:276:11
      %4693 : bool = aten::ne(%4692, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4693) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4694 : bool = prim::GetAttr[name="training"](%4691)
       = prim::If(%4694) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4695 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4691)
          %4696 : Tensor = aten::add(%4695, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4691, %4696)
          -> ()
        block1():
          -> ()
      %4697 : bool = prim::GetAttr[name="training"](%4691)
      %4698 : Tensor = prim::GetAttr[name="running_mean"](%4691)
      %4699 : Tensor = prim::GetAttr[name="running_var"](%4691)
      %4700 : Tensor = prim::GetAttr[name="weight"](%4691)
      %4701 : Tensor = prim::GetAttr[name="bias"](%4691)
       = prim::If(%4697) # torch/nn/functional.py:2011:4
        block0():
          %4702 : int[] = aten::size(%concated_features.31) # torch/nn/functional.py:2012:27
          %size_prods.240 : int = aten::__getitem__(%4702, %24) # torch/nn/functional.py:1991:17
          %4704 : int = aten::len(%4702) # torch/nn/functional.py:1992:19
          %4705 : int = aten::sub(%4704, %26) # torch/nn/functional.py:1992:19
          %size_prods.241 : int = prim::Loop(%4705, %25, %size_prods.240) # torch/nn/functional.py:1992:4
            block0(%i.61 : int, %size_prods.242 : int):
              %4709 : int = aten::add(%i.61, %26) # torch/nn/functional.py:1993:27
              %4710 : int = aten::__getitem__(%4702, %4709) # torch/nn/functional.py:1993:22
              %size_prods.243 : int = aten::mul(%size_prods.242, %4710) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.243)
          %4712 : bool = aten::eq(%size_prods.241, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4712) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4713 : Tensor = aten::batch_norm(%concated_features.31, %4700, %4701, %4698, %4699, %4697, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.61 : Tensor = aten::relu_(%4713) # torch/nn/functional.py:1117:17
      %4715 : Tensor = prim::GetAttr[name="weight"](%4690)
      %4716 : Tensor? = prim::GetAttr[name="bias"](%4690)
      %4717 : int[] = prim::ListConstruct(%27, %27)
      %4718 : int[] = prim::ListConstruct(%24, %24)
      %4719 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.61 : Tensor = aten::conv2d(%result.61, %4715, %4716, %4717, %4718, %4719, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.61)
  %4721 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1905)
  %4722 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1905)
  %4723 : int = aten::dim(%bottleneck_output.60) # torch/nn/modules/batchnorm.py:276:11
  %4724 : bool = aten::ne(%4723, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4724) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4725 : bool = prim::GetAttr[name="training"](%4722)
   = prim::If(%4725) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4726 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4722)
      %4727 : Tensor = aten::add(%4726, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4722, %4727)
      -> ()
    block1():
      -> ()
  %4728 : bool = prim::GetAttr[name="training"](%4722)
  %4729 : Tensor = prim::GetAttr[name="running_mean"](%4722)
  %4730 : Tensor = prim::GetAttr[name="running_var"](%4722)
  %4731 : Tensor = prim::GetAttr[name="weight"](%4722)
  %4732 : Tensor = prim::GetAttr[name="bias"](%4722)
   = prim::If(%4728) # torch/nn/functional.py:2011:4
    block0():
      %4733 : int[] = aten::size(%bottleneck_output.60) # torch/nn/functional.py:2012:27
      %size_prods.244 : int = aten::__getitem__(%4733, %24) # torch/nn/functional.py:1991:17
      %4735 : int = aten::len(%4733) # torch/nn/functional.py:1992:19
      %4736 : int = aten::sub(%4735, %26) # torch/nn/functional.py:1992:19
      %size_prods.245 : int = prim::Loop(%4736, %25, %size_prods.244) # torch/nn/functional.py:1992:4
        block0(%i.62 : int, %size_prods.246 : int):
          %4740 : int = aten::add(%i.62, %26) # torch/nn/functional.py:1993:27
          %4741 : int = aten::__getitem__(%4733, %4740) # torch/nn/functional.py:1993:22
          %size_prods.247 : int = aten::mul(%size_prods.246, %4741) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.247)
      %4743 : bool = aten::eq(%size_prods.245, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4743) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4744 : Tensor = aten::batch_norm(%bottleneck_output.60, %4731, %4732, %4729, %4730, %4728, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.62 : Tensor = aten::relu_(%4744) # torch/nn/functional.py:1117:17
  %4746 : Tensor = prim::GetAttr[name="weight"](%4721)
  %4747 : Tensor? = prim::GetAttr[name="bias"](%4721)
  %4748 : int[] = prim::ListConstruct(%27, %27)
  %4749 : int[] = prim::ListConstruct(%27, %27)
  %4750 : int[] = prim::ListConstruct(%27, %27)
  %new_features.62 : Tensor = aten::conv2d(%result.62, %4746, %4747, %4748, %4749, %4750, %27) # torch/nn/modules/conv.py:415:15
  %4752 : float = prim::GetAttr[name="drop_rate"](%1905)
  %4753 : bool = aten::gt(%4752, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.59 : Tensor = prim::If(%4753) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4755 : float = prim::GetAttr[name="drop_rate"](%1905)
      %4756 : bool = prim::GetAttr[name="training"](%1905)
      %4757 : bool = aten::lt(%4755, %16) # torch/nn/functional.py:968:7
      %4758 : bool = prim::If(%4757) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4759 : bool = aten::gt(%4755, %17) # torch/nn/functional.py:968:17
          -> (%4759)
       = prim::If(%4758) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4760 : Tensor = aten::dropout(%new_features.62, %4755, %4756) # torch/nn/functional.py:973:17
      -> (%4760)
    block1():
      -> (%new_features.62)
  %4761 : Tensor[] = aten::append(%features.4, %new_features.59) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4762 : Tensor = prim::Uninitialized()
  %4763 : bool = prim::GetAttr[name="memory_efficient"](%1906)
  %4764 : bool = prim::If(%4763) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4765 : bool = prim::Uninitialized()
      %4766 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4767 : bool = aten::gt(%4766, %24)
      %4768 : bool, %4769 : bool, %4770 : int = prim::Loop(%18, %4767, %19, %4765, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4771 : int, %4772 : bool, %4773 : bool, %4774 : int):
          %tensor.32 : Tensor = aten::__getitem__(%features.4, %4774) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4776 : bool = prim::requires_grad(%tensor.32)
          %4777 : bool, %4778 : bool = prim::If(%4776) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4765)
          %4779 : int = aten::add(%4774, %27)
          %4780 : bool = aten::lt(%4779, %4766)
          %4781 : bool = aten::__and__(%4780, %4777)
          -> (%4781, %4776, %4778, %4779)
      %4782 : bool = prim::If(%4768)
        block0():
          -> (%4769)
        block1():
          -> (%19)
      -> (%4782)
    block1():
      -> (%19)
  %bottleneck_output.62 : Tensor = prim::If(%4764) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4762)
    block1():
      %concated_features.32 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4785 : __torch__.torch.nn.modules.conv.___torch_mangle_268.Conv2d = prim::GetAttr[name="conv1"](%1906)
      %4786 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_267.BatchNorm2d = prim::GetAttr[name="norm1"](%1906)
      %4787 : int = aten::dim(%concated_features.32) # torch/nn/modules/batchnorm.py:276:11
      %4788 : bool = aten::ne(%4787, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4788) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4789 : bool = prim::GetAttr[name="training"](%4786)
       = prim::If(%4789) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4790 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4786)
          %4791 : Tensor = aten::add(%4790, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4786, %4791)
          -> ()
        block1():
          -> ()
      %4792 : bool = prim::GetAttr[name="training"](%4786)
      %4793 : Tensor = prim::GetAttr[name="running_mean"](%4786)
      %4794 : Tensor = prim::GetAttr[name="running_var"](%4786)
      %4795 : Tensor = prim::GetAttr[name="weight"](%4786)
      %4796 : Tensor = prim::GetAttr[name="bias"](%4786)
       = prim::If(%4792) # torch/nn/functional.py:2011:4
        block0():
          %4797 : int[] = aten::size(%concated_features.32) # torch/nn/functional.py:2012:27
          %size_prods.248 : int = aten::__getitem__(%4797, %24) # torch/nn/functional.py:1991:17
          %4799 : int = aten::len(%4797) # torch/nn/functional.py:1992:19
          %4800 : int = aten::sub(%4799, %26) # torch/nn/functional.py:1992:19
          %size_prods.249 : int = prim::Loop(%4800, %25, %size_prods.248) # torch/nn/functional.py:1992:4
            block0(%i.63 : int, %size_prods.250 : int):
              %4804 : int = aten::add(%i.63, %26) # torch/nn/functional.py:1993:27
              %4805 : int = aten::__getitem__(%4797, %4804) # torch/nn/functional.py:1993:22
              %size_prods.251 : int = aten::mul(%size_prods.250, %4805) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.251)
          %4807 : bool = aten::eq(%size_prods.249, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4807) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4808 : Tensor = aten::batch_norm(%concated_features.32, %4795, %4796, %4793, %4794, %4792, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.63 : Tensor = aten::relu_(%4808) # torch/nn/functional.py:1117:17
      %4810 : Tensor = prim::GetAttr[name="weight"](%4785)
      %4811 : Tensor? = prim::GetAttr[name="bias"](%4785)
      %4812 : int[] = prim::ListConstruct(%27, %27)
      %4813 : int[] = prim::ListConstruct(%24, %24)
      %4814 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.63 : Tensor = aten::conv2d(%result.63, %4810, %4811, %4812, %4813, %4814, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.63)
  %4816 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1906)
  %4817 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1906)
  %4818 : int = aten::dim(%bottleneck_output.62) # torch/nn/modules/batchnorm.py:276:11
  %4819 : bool = aten::ne(%4818, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4819) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4820 : bool = prim::GetAttr[name="training"](%4817)
   = prim::If(%4820) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4821 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4817)
      %4822 : Tensor = aten::add(%4821, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4817, %4822)
      -> ()
    block1():
      -> ()
  %4823 : bool = prim::GetAttr[name="training"](%4817)
  %4824 : Tensor = prim::GetAttr[name="running_mean"](%4817)
  %4825 : Tensor = prim::GetAttr[name="running_var"](%4817)
  %4826 : Tensor = prim::GetAttr[name="weight"](%4817)
  %4827 : Tensor = prim::GetAttr[name="bias"](%4817)
   = prim::If(%4823) # torch/nn/functional.py:2011:4
    block0():
      %4828 : int[] = aten::size(%bottleneck_output.62) # torch/nn/functional.py:2012:27
      %size_prods.252 : int = aten::__getitem__(%4828, %24) # torch/nn/functional.py:1991:17
      %4830 : int = aten::len(%4828) # torch/nn/functional.py:1992:19
      %4831 : int = aten::sub(%4830, %26) # torch/nn/functional.py:1992:19
      %size_prods.253 : int = prim::Loop(%4831, %25, %size_prods.252) # torch/nn/functional.py:1992:4
        block0(%i.64 : int, %size_prods.254 : int):
          %4835 : int = aten::add(%i.64, %26) # torch/nn/functional.py:1993:27
          %4836 : int = aten::__getitem__(%4828, %4835) # torch/nn/functional.py:1993:22
          %size_prods.255 : int = aten::mul(%size_prods.254, %4836) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.255)
      %4838 : bool = aten::eq(%size_prods.253, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4838) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4839 : Tensor = aten::batch_norm(%bottleneck_output.62, %4826, %4827, %4824, %4825, %4823, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.64 : Tensor = aten::relu_(%4839) # torch/nn/functional.py:1117:17
  %4841 : Tensor = prim::GetAttr[name="weight"](%4816)
  %4842 : Tensor? = prim::GetAttr[name="bias"](%4816)
  %4843 : int[] = prim::ListConstruct(%27, %27)
  %4844 : int[] = prim::ListConstruct(%27, %27)
  %4845 : int[] = prim::ListConstruct(%27, %27)
  %new_features.64 : Tensor = aten::conv2d(%result.64, %4841, %4842, %4843, %4844, %4845, %27) # torch/nn/modules/conv.py:415:15
  %4847 : float = prim::GetAttr[name="drop_rate"](%1906)
  %4848 : bool = aten::gt(%4847, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.61 : Tensor = prim::If(%4848) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4850 : float = prim::GetAttr[name="drop_rate"](%1906)
      %4851 : bool = prim::GetAttr[name="training"](%1906)
      %4852 : bool = aten::lt(%4850, %16) # torch/nn/functional.py:968:7
      %4853 : bool = prim::If(%4852) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4854 : bool = aten::gt(%4850, %17) # torch/nn/functional.py:968:17
          -> (%4854)
       = prim::If(%4853) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4855 : Tensor = aten::dropout(%new_features.64, %4850, %4851) # torch/nn/functional.py:973:17
      -> (%4855)
    block1():
      -> (%new_features.64)
  %4856 : Tensor[] = aten::append(%features.4, %new_features.61) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4857 : Tensor = prim::Uninitialized()
  %4858 : bool = prim::GetAttr[name="memory_efficient"](%1907)
  %4859 : bool = prim::If(%4858) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4860 : bool = prim::Uninitialized()
      %4861 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4862 : bool = aten::gt(%4861, %24)
      %4863 : bool, %4864 : bool, %4865 : int = prim::Loop(%18, %4862, %19, %4860, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4866 : int, %4867 : bool, %4868 : bool, %4869 : int):
          %tensor.33 : Tensor = aten::__getitem__(%features.4, %4869) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4871 : bool = prim::requires_grad(%tensor.33)
          %4872 : bool, %4873 : bool = prim::If(%4871) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4860)
          %4874 : int = aten::add(%4869, %27)
          %4875 : bool = aten::lt(%4874, %4861)
          %4876 : bool = aten::__and__(%4875, %4872)
          -> (%4876, %4871, %4873, %4874)
      %4877 : bool = prim::If(%4863)
        block0():
          -> (%4864)
        block1():
          -> (%19)
      -> (%4877)
    block1():
      -> (%19)
  %bottleneck_output.64 : Tensor = prim::If(%4859) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4857)
    block1():
      %concated_features.33 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4880 : __torch__.torch.nn.modules.conv.___torch_mangle_271.Conv2d = prim::GetAttr[name="conv1"](%1907)
      %4881 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_270.BatchNorm2d = prim::GetAttr[name="norm1"](%1907)
      %4882 : int = aten::dim(%concated_features.33) # torch/nn/modules/batchnorm.py:276:11
      %4883 : bool = aten::ne(%4882, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4883) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4884 : bool = prim::GetAttr[name="training"](%4881)
       = prim::If(%4884) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4885 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4881)
          %4886 : Tensor = aten::add(%4885, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4881, %4886)
          -> ()
        block1():
          -> ()
      %4887 : bool = prim::GetAttr[name="training"](%4881)
      %4888 : Tensor = prim::GetAttr[name="running_mean"](%4881)
      %4889 : Tensor = prim::GetAttr[name="running_var"](%4881)
      %4890 : Tensor = prim::GetAttr[name="weight"](%4881)
      %4891 : Tensor = prim::GetAttr[name="bias"](%4881)
       = prim::If(%4887) # torch/nn/functional.py:2011:4
        block0():
          %4892 : int[] = aten::size(%concated_features.33) # torch/nn/functional.py:2012:27
          %size_prods.256 : int = aten::__getitem__(%4892, %24) # torch/nn/functional.py:1991:17
          %4894 : int = aten::len(%4892) # torch/nn/functional.py:1992:19
          %4895 : int = aten::sub(%4894, %26) # torch/nn/functional.py:1992:19
          %size_prods.257 : int = prim::Loop(%4895, %25, %size_prods.256) # torch/nn/functional.py:1992:4
            block0(%i.65 : int, %size_prods.258 : int):
              %4899 : int = aten::add(%i.65, %26) # torch/nn/functional.py:1993:27
              %4900 : int = aten::__getitem__(%4892, %4899) # torch/nn/functional.py:1993:22
              %size_prods.259 : int = aten::mul(%size_prods.258, %4900) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.259)
          %4902 : bool = aten::eq(%size_prods.257, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4902) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4903 : Tensor = aten::batch_norm(%concated_features.33, %4890, %4891, %4888, %4889, %4887, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.65 : Tensor = aten::relu_(%4903) # torch/nn/functional.py:1117:17
      %4905 : Tensor = prim::GetAttr[name="weight"](%4880)
      %4906 : Tensor? = prim::GetAttr[name="bias"](%4880)
      %4907 : int[] = prim::ListConstruct(%27, %27)
      %4908 : int[] = prim::ListConstruct(%24, %24)
      %4909 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.65 : Tensor = aten::conv2d(%result.65, %4905, %4906, %4907, %4908, %4909, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.65)
  %4911 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1907)
  %4912 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1907)
  %4913 : int = aten::dim(%bottleneck_output.64) # torch/nn/modules/batchnorm.py:276:11
  %4914 : bool = aten::ne(%4913, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%4914) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %4915 : bool = prim::GetAttr[name="training"](%4912)
   = prim::If(%4915) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %4916 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4912)
      %4917 : Tensor = aten::add(%4916, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%4912, %4917)
      -> ()
    block1():
      -> ()
  %4918 : bool = prim::GetAttr[name="training"](%4912)
  %4919 : Tensor = prim::GetAttr[name="running_mean"](%4912)
  %4920 : Tensor = prim::GetAttr[name="running_var"](%4912)
  %4921 : Tensor = prim::GetAttr[name="weight"](%4912)
  %4922 : Tensor = prim::GetAttr[name="bias"](%4912)
   = prim::If(%4918) # torch/nn/functional.py:2011:4
    block0():
      %4923 : int[] = aten::size(%bottleneck_output.64) # torch/nn/functional.py:2012:27
      %size_prods.260 : int = aten::__getitem__(%4923, %24) # torch/nn/functional.py:1991:17
      %4925 : int = aten::len(%4923) # torch/nn/functional.py:1992:19
      %4926 : int = aten::sub(%4925, %26) # torch/nn/functional.py:1992:19
      %size_prods.261 : int = prim::Loop(%4926, %25, %size_prods.260) # torch/nn/functional.py:1992:4
        block0(%i.66 : int, %size_prods.262 : int):
          %4930 : int = aten::add(%i.66, %26) # torch/nn/functional.py:1993:27
          %4931 : int = aten::__getitem__(%4923, %4930) # torch/nn/functional.py:1993:22
          %size_prods.263 : int = aten::mul(%size_prods.262, %4931) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.263)
      %4933 : bool = aten::eq(%size_prods.261, %27) # torch/nn/functional.py:1994:7
       = prim::If(%4933) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %4934 : Tensor = aten::batch_norm(%bottleneck_output.64, %4921, %4922, %4919, %4920, %4918, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.66 : Tensor = aten::relu_(%4934) # torch/nn/functional.py:1117:17
  %4936 : Tensor = prim::GetAttr[name="weight"](%4911)
  %4937 : Tensor? = prim::GetAttr[name="bias"](%4911)
  %4938 : int[] = prim::ListConstruct(%27, %27)
  %4939 : int[] = prim::ListConstruct(%27, %27)
  %4940 : int[] = prim::ListConstruct(%27, %27)
  %new_features.66 : Tensor = aten::conv2d(%result.66, %4936, %4937, %4938, %4939, %4940, %27) # torch/nn/modules/conv.py:415:15
  %4942 : float = prim::GetAttr[name="drop_rate"](%1907)
  %4943 : bool = aten::gt(%4942, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.63 : Tensor = prim::If(%4943) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %4945 : float = prim::GetAttr[name="drop_rate"](%1907)
      %4946 : bool = prim::GetAttr[name="training"](%1907)
      %4947 : bool = aten::lt(%4945, %16) # torch/nn/functional.py:968:7
      %4948 : bool = prim::If(%4947) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %4949 : bool = aten::gt(%4945, %17) # torch/nn/functional.py:968:17
          -> (%4949)
       = prim::If(%4948) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %4950 : Tensor = aten::dropout(%new_features.66, %4945, %4946) # torch/nn/functional.py:973:17
      -> (%4950)
    block1():
      -> (%new_features.66)
  %4951 : Tensor[] = aten::append(%features.4, %new_features.63) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %4952 : Tensor = prim::Uninitialized()
  %4953 : bool = prim::GetAttr[name="memory_efficient"](%1908)
  %4954 : bool = prim::If(%4953) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %4955 : bool = prim::Uninitialized()
      %4956 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %4957 : bool = aten::gt(%4956, %24)
      %4958 : bool, %4959 : bool, %4960 : int = prim::Loop(%18, %4957, %19, %4955, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%4961 : int, %4962 : bool, %4963 : bool, %4964 : int):
          %tensor.34 : Tensor = aten::__getitem__(%features.4, %4964) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %4966 : bool = prim::requires_grad(%tensor.34)
          %4967 : bool, %4968 : bool = prim::If(%4966) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %4955)
          %4969 : int = aten::add(%4964, %27)
          %4970 : bool = aten::lt(%4969, %4956)
          %4971 : bool = aten::__and__(%4970, %4967)
          -> (%4971, %4966, %4968, %4969)
      %4972 : bool = prim::If(%4958)
        block0():
          -> (%4959)
        block1():
          -> (%19)
      -> (%4972)
    block1():
      -> (%19)
  %bottleneck_output.66 : Tensor = prim::If(%4954) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%4952)
    block1():
      %concated_features.34 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %4975 : __torch__.torch.nn.modules.conv.___torch_mangle_274.Conv2d = prim::GetAttr[name="conv1"](%1908)
      %4976 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_273.BatchNorm2d = prim::GetAttr[name="norm1"](%1908)
      %4977 : int = aten::dim(%concated_features.34) # torch/nn/modules/batchnorm.py:276:11
      %4978 : bool = aten::ne(%4977, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%4978) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %4979 : bool = prim::GetAttr[name="training"](%4976)
       = prim::If(%4979) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %4980 : Tensor = prim::GetAttr[name="num_batches_tracked"](%4976)
          %4981 : Tensor = aten::add(%4980, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%4976, %4981)
          -> ()
        block1():
          -> ()
      %4982 : bool = prim::GetAttr[name="training"](%4976)
      %4983 : Tensor = prim::GetAttr[name="running_mean"](%4976)
      %4984 : Tensor = prim::GetAttr[name="running_var"](%4976)
      %4985 : Tensor = prim::GetAttr[name="weight"](%4976)
      %4986 : Tensor = prim::GetAttr[name="bias"](%4976)
       = prim::If(%4982) # torch/nn/functional.py:2011:4
        block0():
          %4987 : int[] = aten::size(%concated_features.34) # torch/nn/functional.py:2012:27
          %size_prods.264 : int = aten::__getitem__(%4987, %24) # torch/nn/functional.py:1991:17
          %4989 : int = aten::len(%4987) # torch/nn/functional.py:1992:19
          %4990 : int = aten::sub(%4989, %26) # torch/nn/functional.py:1992:19
          %size_prods.265 : int = prim::Loop(%4990, %25, %size_prods.264) # torch/nn/functional.py:1992:4
            block0(%i.67 : int, %size_prods.266 : int):
              %4994 : int = aten::add(%i.67, %26) # torch/nn/functional.py:1993:27
              %4995 : int = aten::__getitem__(%4987, %4994) # torch/nn/functional.py:1993:22
              %size_prods.267 : int = aten::mul(%size_prods.266, %4995) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.267)
          %4997 : bool = aten::eq(%size_prods.265, %27) # torch/nn/functional.py:1994:7
           = prim::If(%4997) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %4998 : Tensor = aten::batch_norm(%concated_features.34, %4985, %4986, %4983, %4984, %4982, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.67 : Tensor = aten::relu_(%4998) # torch/nn/functional.py:1117:17
      %5000 : Tensor = prim::GetAttr[name="weight"](%4975)
      %5001 : Tensor? = prim::GetAttr[name="bias"](%4975)
      %5002 : int[] = prim::ListConstruct(%27, %27)
      %5003 : int[] = prim::ListConstruct(%24, %24)
      %5004 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.67 : Tensor = aten::conv2d(%result.67, %5000, %5001, %5002, %5003, %5004, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.67)
  %5006 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1908)
  %5007 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1908)
  %5008 : int = aten::dim(%bottleneck_output.66) # torch/nn/modules/batchnorm.py:276:11
  %5009 : bool = aten::ne(%5008, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5009) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5010 : bool = prim::GetAttr[name="training"](%5007)
   = prim::If(%5010) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5011 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5007)
      %5012 : Tensor = aten::add(%5011, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5007, %5012)
      -> ()
    block1():
      -> ()
  %5013 : bool = prim::GetAttr[name="training"](%5007)
  %5014 : Tensor = prim::GetAttr[name="running_mean"](%5007)
  %5015 : Tensor = prim::GetAttr[name="running_var"](%5007)
  %5016 : Tensor = prim::GetAttr[name="weight"](%5007)
  %5017 : Tensor = prim::GetAttr[name="bias"](%5007)
   = prim::If(%5013) # torch/nn/functional.py:2011:4
    block0():
      %5018 : int[] = aten::size(%bottleneck_output.66) # torch/nn/functional.py:2012:27
      %size_prods.268 : int = aten::__getitem__(%5018, %24) # torch/nn/functional.py:1991:17
      %5020 : int = aten::len(%5018) # torch/nn/functional.py:1992:19
      %5021 : int = aten::sub(%5020, %26) # torch/nn/functional.py:1992:19
      %size_prods.269 : int = prim::Loop(%5021, %25, %size_prods.268) # torch/nn/functional.py:1992:4
        block0(%i.68 : int, %size_prods.270 : int):
          %5025 : int = aten::add(%i.68, %26) # torch/nn/functional.py:1993:27
          %5026 : int = aten::__getitem__(%5018, %5025) # torch/nn/functional.py:1993:22
          %size_prods.271 : int = aten::mul(%size_prods.270, %5026) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.271)
      %5028 : bool = aten::eq(%size_prods.269, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5028) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5029 : Tensor = aten::batch_norm(%bottleneck_output.66, %5016, %5017, %5014, %5015, %5013, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.68 : Tensor = aten::relu_(%5029) # torch/nn/functional.py:1117:17
  %5031 : Tensor = prim::GetAttr[name="weight"](%5006)
  %5032 : Tensor? = prim::GetAttr[name="bias"](%5006)
  %5033 : int[] = prim::ListConstruct(%27, %27)
  %5034 : int[] = prim::ListConstruct(%27, %27)
  %5035 : int[] = prim::ListConstruct(%27, %27)
  %new_features.68 : Tensor = aten::conv2d(%result.68, %5031, %5032, %5033, %5034, %5035, %27) # torch/nn/modules/conv.py:415:15
  %5037 : float = prim::GetAttr[name="drop_rate"](%1908)
  %5038 : bool = aten::gt(%5037, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.65 : Tensor = prim::If(%5038) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5040 : float = prim::GetAttr[name="drop_rate"](%1908)
      %5041 : bool = prim::GetAttr[name="training"](%1908)
      %5042 : bool = aten::lt(%5040, %16) # torch/nn/functional.py:968:7
      %5043 : bool = prim::If(%5042) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5044 : bool = aten::gt(%5040, %17) # torch/nn/functional.py:968:17
          -> (%5044)
       = prim::If(%5043) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5045 : Tensor = aten::dropout(%new_features.68, %5040, %5041) # torch/nn/functional.py:973:17
      -> (%5045)
    block1():
      -> (%new_features.68)
  %5046 : Tensor[] = aten::append(%features.4, %new_features.65) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5047 : Tensor = prim::Uninitialized()
  %5048 : bool = prim::GetAttr[name="memory_efficient"](%1909)
  %5049 : bool = prim::If(%5048) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5050 : bool = prim::Uninitialized()
      %5051 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5052 : bool = aten::gt(%5051, %24)
      %5053 : bool, %5054 : bool, %5055 : int = prim::Loop(%18, %5052, %19, %5050, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5056 : int, %5057 : bool, %5058 : bool, %5059 : int):
          %tensor.35 : Tensor = aten::__getitem__(%features.4, %5059) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5061 : bool = prim::requires_grad(%tensor.35)
          %5062 : bool, %5063 : bool = prim::If(%5061) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5050)
          %5064 : int = aten::add(%5059, %27)
          %5065 : bool = aten::lt(%5064, %5051)
          %5066 : bool = aten::__and__(%5065, %5062)
          -> (%5066, %5061, %5063, %5064)
      %5067 : bool = prim::If(%5053)
        block0():
          -> (%5054)
        block1():
          -> (%19)
      -> (%5067)
    block1():
      -> (%19)
  %bottleneck_output.68 : Tensor = prim::If(%5049) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5047)
    block1():
      %concated_features.35 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5070 : __torch__.torch.nn.modules.conv.___torch_mangle_277.Conv2d = prim::GetAttr[name="conv1"](%1909)
      %5071 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_276.BatchNorm2d = prim::GetAttr[name="norm1"](%1909)
      %5072 : int = aten::dim(%concated_features.35) # torch/nn/modules/batchnorm.py:276:11
      %5073 : bool = aten::ne(%5072, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5073) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5074 : bool = prim::GetAttr[name="training"](%5071)
       = prim::If(%5074) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5075 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5071)
          %5076 : Tensor = aten::add(%5075, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5071, %5076)
          -> ()
        block1():
          -> ()
      %5077 : bool = prim::GetAttr[name="training"](%5071)
      %5078 : Tensor = prim::GetAttr[name="running_mean"](%5071)
      %5079 : Tensor = prim::GetAttr[name="running_var"](%5071)
      %5080 : Tensor = prim::GetAttr[name="weight"](%5071)
      %5081 : Tensor = prim::GetAttr[name="bias"](%5071)
       = prim::If(%5077) # torch/nn/functional.py:2011:4
        block0():
          %5082 : int[] = aten::size(%concated_features.35) # torch/nn/functional.py:2012:27
          %size_prods.272 : int = aten::__getitem__(%5082, %24) # torch/nn/functional.py:1991:17
          %5084 : int = aten::len(%5082) # torch/nn/functional.py:1992:19
          %5085 : int = aten::sub(%5084, %26) # torch/nn/functional.py:1992:19
          %size_prods.273 : int = prim::Loop(%5085, %25, %size_prods.272) # torch/nn/functional.py:1992:4
            block0(%i.69 : int, %size_prods.274 : int):
              %5089 : int = aten::add(%i.69, %26) # torch/nn/functional.py:1993:27
              %5090 : int = aten::__getitem__(%5082, %5089) # torch/nn/functional.py:1993:22
              %size_prods.275 : int = aten::mul(%size_prods.274, %5090) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.275)
          %5092 : bool = aten::eq(%size_prods.273, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5092) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5093 : Tensor = aten::batch_norm(%concated_features.35, %5080, %5081, %5078, %5079, %5077, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.69 : Tensor = aten::relu_(%5093) # torch/nn/functional.py:1117:17
      %5095 : Tensor = prim::GetAttr[name="weight"](%5070)
      %5096 : Tensor? = prim::GetAttr[name="bias"](%5070)
      %5097 : int[] = prim::ListConstruct(%27, %27)
      %5098 : int[] = prim::ListConstruct(%24, %24)
      %5099 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.69 : Tensor = aten::conv2d(%result.69, %5095, %5096, %5097, %5098, %5099, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.69)
  %5101 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1909)
  %5102 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1909)
  %5103 : int = aten::dim(%bottleneck_output.68) # torch/nn/modules/batchnorm.py:276:11
  %5104 : bool = aten::ne(%5103, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5104) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5105 : bool = prim::GetAttr[name="training"](%5102)
   = prim::If(%5105) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5106 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5102)
      %5107 : Tensor = aten::add(%5106, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5102, %5107)
      -> ()
    block1():
      -> ()
  %5108 : bool = prim::GetAttr[name="training"](%5102)
  %5109 : Tensor = prim::GetAttr[name="running_mean"](%5102)
  %5110 : Tensor = prim::GetAttr[name="running_var"](%5102)
  %5111 : Tensor = prim::GetAttr[name="weight"](%5102)
  %5112 : Tensor = prim::GetAttr[name="bias"](%5102)
   = prim::If(%5108) # torch/nn/functional.py:2011:4
    block0():
      %5113 : int[] = aten::size(%bottleneck_output.68) # torch/nn/functional.py:2012:27
      %size_prods.276 : int = aten::__getitem__(%5113, %24) # torch/nn/functional.py:1991:17
      %5115 : int = aten::len(%5113) # torch/nn/functional.py:1992:19
      %5116 : int = aten::sub(%5115, %26) # torch/nn/functional.py:1992:19
      %size_prods.277 : int = prim::Loop(%5116, %25, %size_prods.276) # torch/nn/functional.py:1992:4
        block0(%i.70 : int, %size_prods.278 : int):
          %5120 : int = aten::add(%i.70, %26) # torch/nn/functional.py:1993:27
          %5121 : int = aten::__getitem__(%5113, %5120) # torch/nn/functional.py:1993:22
          %size_prods.279 : int = aten::mul(%size_prods.278, %5121) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.279)
      %5123 : bool = aten::eq(%size_prods.277, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5123) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5124 : Tensor = aten::batch_norm(%bottleneck_output.68, %5111, %5112, %5109, %5110, %5108, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.70 : Tensor = aten::relu_(%5124) # torch/nn/functional.py:1117:17
  %5126 : Tensor = prim::GetAttr[name="weight"](%5101)
  %5127 : Tensor? = prim::GetAttr[name="bias"](%5101)
  %5128 : int[] = prim::ListConstruct(%27, %27)
  %5129 : int[] = prim::ListConstruct(%27, %27)
  %5130 : int[] = prim::ListConstruct(%27, %27)
  %new_features.70 : Tensor = aten::conv2d(%result.70, %5126, %5127, %5128, %5129, %5130, %27) # torch/nn/modules/conv.py:415:15
  %5132 : float = prim::GetAttr[name="drop_rate"](%1909)
  %5133 : bool = aten::gt(%5132, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.67 : Tensor = prim::If(%5133) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5135 : float = prim::GetAttr[name="drop_rate"](%1909)
      %5136 : bool = prim::GetAttr[name="training"](%1909)
      %5137 : bool = aten::lt(%5135, %16) # torch/nn/functional.py:968:7
      %5138 : bool = prim::If(%5137) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5139 : bool = aten::gt(%5135, %17) # torch/nn/functional.py:968:17
          -> (%5139)
       = prim::If(%5138) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5140 : Tensor = aten::dropout(%new_features.70, %5135, %5136) # torch/nn/functional.py:973:17
      -> (%5140)
    block1():
      -> (%new_features.70)
  %5141 : Tensor[] = aten::append(%features.4, %new_features.67) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5142 : Tensor = prim::Uninitialized()
  %5143 : bool = prim::GetAttr[name="memory_efficient"](%1910)
  %5144 : bool = prim::If(%5143) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5145 : bool = prim::Uninitialized()
      %5146 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5147 : bool = aten::gt(%5146, %24)
      %5148 : bool, %5149 : bool, %5150 : int = prim::Loop(%18, %5147, %19, %5145, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5151 : int, %5152 : bool, %5153 : bool, %5154 : int):
          %tensor.36 : Tensor = aten::__getitem__(%features.4, %5154) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5156 : bool = prim::requires_grad(%tensor.36)
          %5157 : bool, %5158 : bool = prim::If(%5156) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5145)
          %5159 : int = aten::add(%5154, %27)
          %5160 : bool = aten::lt(%5159, %5146)
          %5161 : bool = aten::__and__(%5160, %5157)
          -> (%5161, %5156, %5158, %5159)
      %5162 : bool = prim::If(%5148)
        block0():
          -> (%5149)
        block1():
          -> (%19)
      -> (%5162)
    block1():
      -> (%19)
  %bottleneck_output.70 : Tensor = prim::If(%5144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5142)
    block1():
      %concated_features.36 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5165 : __torch__.torch.nn.modules.conv.___torch_mangle_280.Conv2d = prim::GetAttr[name="conv1"](%1910)
      %5166 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_279.BatchNorm2d = prim::GetAttr[name="norm1"](%1910)
      %5167 : int = aten::dim(%concated_features.36) # torch/nn/modules/batchnorm.py:276:11
      %5168 : bool = aten::ne(%5167, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5168) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5169 : bool = prim::GetAttr[name="training"](%5166)
       = prim::If(%5169) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5170 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5166)
          %5171 : Tensor = aten::add(%5170, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5166, %5171)
          -> ()
        block1():
          -> ()
      %5172 : bool = prim::GetAttr[name="training"](%5166)
      %5173 : Tensor = prim::GetAttr[name="running_mean"](%5166)
      %5174 : Tensor = prim::GetAttr[name="running_var"](%5166)
      %5175 : Tensor = prim::GetAttr[name="weight"](%5166)
      %5176 : Tensor = prim::GetAttr[name="bias"](%5166)
       = prim::If(%5172) # torch/nn/functional.py:2011:4
        block0():
          %5177 : int[] = aten::size(%concated_features.36) # torch/nn/functional.py:2012:27
          %size_prods.280 : int = aten::__getitem__(%5177, %24) # torch/nn/functional.py:1991:17
          %5179 : int = aten::len(%5177) # torch/nn/functional.py:1992:19
          %5180 : int = aten::sub(%5179, %26) # torch/nn/functional.py:1992:19
          %size_prods.281 : int = prim::Loop(%5180, %25, %size_prods.280) # torch/nn/functional.py:1992:4
            block0(%i.71 : int, %size_prods.282 : int):
              %5184 : int = aten::add(%i.71, %26) # torch/nn/functional.py:1993:27
              %5185 : int = aten::__getitem__(%5177, %5184) # torch/nn/functional.py:1993:22
              %size_prods.283 : int = aten::mul(%size_prods.282, %5185) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.283)
          %5187 : bool = aten::eq(%size_prods.281, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5187) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5188 : Tensor = aten::batch_norm(%concated_features.36, %5175, %5176, %5173, %5174, %5172, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.71 : Tensor = aten::relu_(%5188) # torch/nn/functional.py:1117:17
      %5190 : Tensor = prim::GetAttr[name="weight"](%5165)
      %5191 : Tensor? = prim::GetAttr[name="bias"](%5165)
      %5192 : int[] = prim::ListConstruct(%27, %27)
      %5193 : int[] = prim::ListConstruct(%24, %24)
      %5194 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.71 : Tensor = aten::conv2d(%result.71, %5190, %5191, %5192, %5193, %5194, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.71)
  %5196 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1910)
  %5197 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1910)
  %5198 : int = aten::dim(%bottleneck_output.70) # torch/nn/modules/batchnorm.py:276:11
  %5199 : bool = aten::ne(%5198, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5199) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5200 : bool = prim::GetAttr[name="training"](%5197)
   = prim::If(%5200) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5201 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5197)
      %5202 : Tensor = aten::add(%5201, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5197, %5202)
      -> ()
    block1():
      -> ()
  %5203 : bool = prim::GetAttr[name="training"](%5197)
  %5204 : Tensor = prim::GetAttr[name="running_mean"](%5197)
  %5205 : Tensor = prim::GetAttr[name="running_var"](%5197)
  %5206 : Tensor = prim::GetAttr[name="weight"](%5197)
  %5207 : Tensor = prim::GetAttr[name="bias"](%5197)
   = prim::If(%5203) # torch/nn/functional.py:2011:4
    block0():
      %5208 : int[] = aten::size(%bottleneck_output.70) # torch/nn/functional.py:2012:27
      %size_prods.284 : int = aten::__getitem__(%5208, %24) # torch/nn/functional.py:1991:17
      %5210 : int = aten::len(%5208) # torch/nn/functional.py:1992:19
      %5211 : int = aten::sub(%5210, %26) # torch/nn/functional.py:1992:19
      %size_prods.285 : int = prim::Loop(%5211, %25, %size_prods.284) # torch/nn/functional.py:1992:4
        block0(%i.72 : int, %size_prods.286 : int):
          %5215 : int = aten::add(%i.72, %26) # torch/nn/functional.py:1993:27
          %5216 : int = aten::__getitem__(%5208, %5215) # torch/nn/functional.py:1993:22
          %size_prods.287 : int = aten::mul(%size_prods.286, %5216) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.287)
      %5218 : bool = aten::eq(%size_prods.285, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5218) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5219 : Tensor = aten::batch_norm(%bottleneck_output.70, %5206, %5207, %5204, %5205, %5203, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.72 : Tensor = aten::relu_(%5219) # torch/nn/functional.py:1117:17
  %5221 : Tensor = prim::GetAttr[name="weight"](%5196)
  %5222 : Tensor? = prim::GetAttr[name="bias"](%5196)
  %5223 : int[] = prim::ListConstruct(%27, %27)
  %5224 : int[] = prim::ListConstruct(%27, %27)
  %5225 : int[] = prim::ListConstruct(%27, %27)
  %new_features.72 : Tensor = aten::conv2d(%result.72, %5221, %5222, %5223, %5224, %5225, %27) # torch/nn/modules/conv.py:415:15
  %5227 : float = prim::GetAttr[name="drop_rate"](%1910)
  %5228 : bool = aten::gt(%5227, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.69 : Tensor = prim::If(%5228) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5230 : float = prim::GetAttr[name="drop_rate"](%1910)
      %5231 : bool = prim::GetAttr[name="training"](%1910)
      %5232 : bool = aten::lt(%5230, %16) # torch/nn/functional.py:968:7
      %5233 : bool = prim::If(%5232) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5234 : bool = aten::gt(%5230, %17) # torch/nn/functional.py:968:17
          -> (%5234)
       = prim::If(%5233) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5235 : Tensor = aten::dropout(%new_features.72, %5230, %5231) # torch/nn/functional.py:973:17
      -> (%5235)
    block1():
      -> (%new_features.72)
  %5236 : Tensor[] = aten::append(%features.4, %new_features.69) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5237 : Tensor = prim::Uninitialized()
  %5238 : bool = prim::GetAttr[name="memory_efficient"](%1911)
  %5239 : bool = prim::If(%5238) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5240 : bool = prim::Uninitialized()
      %5241 : int = aten::len(%features.4) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5242 : bool = aten::gt(%5241, %24)
      %5243 : bool, %5244 : bool, %5245 : int = prim::Loop(%18, %5242, %19, %5240, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5246 : int, %5247 : bool, %5248 : bool, %5249 : int):
          %tensor.78 : Tensor = aten::__getitem__(%features.4, %5249) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5251 : bool = prim::requires_grad(%tensor.78)
          %5252 : bool, %5253 : bool = prim::If(%5251) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5240)
          %5254 : int = aten::add(%5249, %27)
          %5255 : bool = aten::lt(%5254, %5241)
          %5256 : bool = aten::__and__(%5255, %5252)
          -> (%5256, %5251, %5253, %5254)
      %5257 : bool = prim::If(%5243)
        block0():
          -> (%5244)
        block1():
          -> (%19)
      -> (%5257)
    block1():
      -> (%19)
  %bottleneck_output.154 : Tensor = prim::If(%5239) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5237)
    block1():
      %concated_features.78 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5260 : __torch__.torch.nn.modules.conv.___torch_mangle_283.Conv2d = prim::GetAttr[name="conv1"](%1911)
      %5261 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_282.BatchNorm2d = prim::GetAttr[name="norm1"](%1911)
      %5262 : int = aten::dim(%concated_features.78) # torch/nn/modules/batchnorm.py:276:11
      %5263 : bool = aten::ne(%5262, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5263) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5264 : bool = prim::GetAttr[name="training"](%5261)
       = prim::If(%5264) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5265 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5261)
          %5266 : Tensor = aten::add(%5265, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5261, %5266)
          -> ()
        block1():
          -> ()
      %5267 : bool = prim::GetAttr[name="training"](%5261)
      %5268 : Tensor = prim::GetAttr[name="running_mean"](%5261)
      %5269 : Tensor = prim::GetAttr[name="running_var"](%5261)
      %5270 : Tensor = prim::GetAttr[name="weight"](%5261)
      %5271 : Tensor = prim::GetAttr[name="bias"](%5261)
       = prim::If(%5267) # torch/nn/functional.py:2011:4
        block0():
          %5272 : int[] = aten::size(%concated_features.78) # torch/nn/functional.py:2012:27
          %size_prods.632 : int = aten::__getitem__(%5272, %24) # torch/nn/functional.py:1991:17
          %5274 : int = aten::len(%5272) # torch/nn/functional.py:1992:19
          %5275 : int = aten::sub(%5274, %26) # torch/nn/functional.py:1992:19
          %size_prods.633 : int = prim::Loop(%5275, %25, %size_prods.632) # torch/nn/functional.py:1992:4
            block0(%i.159 : int, %size_prods.634 : int):
              %5279 : int = aten::add(%i.159, %26) # torch/nn/functional.py:1993:27
              %5280 : int = aten::__getitem__(%5272, %5279) # torch/nn/functional.py:1993:22
              %size_prods.635 : int = aten::mul(%size_prods.634, %5280) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.635)
          %5282 : bool = aten::eq(%size_prods.633, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5282) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5283 : Tensor = aten::batch_norm(%concated_features.78, %5270, %5271, %5268, %5269, %5267, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.155 : Tensor = aten::relu_(%5283) # torch/nn/functional.py:1117:17
      %5285 : Tensor = prim::GetAttr[name="weight"](%5260)
      %5286 : Tensor? = prim::GetAttr[name="bias"](%5260)
      %5287 : int[] = prim::ListConstruct(%27, %27)
      %5288 : int[] = prim::ListConstruct(%24, %24)
      %5289 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.155 : Tensor = aten::conv2d(%result.155, %5285, %5286, %5287, %5288, %5289, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.155)
  %5291 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%1911)
  %5292 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%1911)
  %5293 : int = aten::dim(%bottleneck_output.154) # torch/nn/modules/batchnorm.py:276:11
  %5294 : bool = aten::ne(%5293, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5294) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5295 : bool = prim::GetAttr[name="training"](%5292)
   = prim::If(%5295) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5296 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5292)
      %5297 : Tensor = aten::add(%5296, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5292, %5297)
      -> ()
    block1():
      -> ()
  %5298 : bool = prim::GetAttr[name="training"](%5292)
  %5299 : Tensor = prim::GetAttr[name="running_mean"](%5292)
  %5300 : Tensor = prim::GetAttr[name="running_var"](%5292)
  %5301 : Tensor = prim::GetAttr[name="weight"](%5292)
  %5302 : Tensor = prim::GetAttr[name="bias"](%5292)
   = prim::If(%5298) # torch/nn/functional.py:2011:4
    block0():
      %5303 : int[] = aten::size(%bottleneck_output.154) # torch/nn/functional.py:2012:27
      %size_prods.444 : int = aten::__getitem__(%5303, %24) # torch/nn/functional.py:1991:17
      %5305 : int = aten::len(%5303) # torch/nn/functional.py:1992:19
      %5306 : int = aten::sub(%5305, %26) # torch/nn/functional.py:1992:19
      %size_prods.445 : int = prim::Loop(%5306, %25, %size_prods.444) # torch/nn/functional.py:1992:4
        block0(%i.112 : int, %size_prods.446 : int):
          %5310 : int = aten::add(%i.112, %26) # torch/nn/functional.py:1993:27
          %5311 : int = aten::__getitem__(%5303, %5310) # torch/nn/functional.py:1993:22
          %size_prods.447 : int = aten::mul(%size_prods.446, %5311) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.447)
      %5313 : bool = aten::eq(%size_prods.445, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5313) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5314 : Tensor = aten::batch_norm(%bottleneck_output.154, %5301, %5302, %5299, %5300, %5298, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.156 : Tensor = aten::relu_(%5314) # torch/nn/functional.py:1117:17
  %5316 : Tensor = prim::GetAttr[name="weight"](%5291)
  %5317 : Tensor? = prim::GetAttr[name="bias"](%5291)
  %5318 : int[] = prim::ListConstruct(%27, %27)
  %5319 : int[] = prim::ListConstruct(%27, %27)
  %5320 : int[] = prim::ListConstruct(%27, %27)
  %new_features.155 : Tensor = aten::conv2d(%result.156, %5316, %5317, %5318, %5319, %5320, %27) # torch/nn/modules/conv.py:415:15
  %5322 : float = prim::GetAttr[name="drop_rate"](%1911)
  %5323 : bool = aten::gt(%5322, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.71 : Tensor = prim::If(%5323) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5325 : float = prim::GetAttr[name="drop_rate"](%1911)
      %5326 : bool = prim::GetAttr[name="training"](%1911)
      %5327 : bool = aten::lt(%5325, %16) # torch/nn/functional.py:968:7
      %5328 : bool = prim::If(%5327) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5329 : bool = aten::gt(%5325, %17) # torch/nn/functional.py:968:17
          -> (%5329)
       = prim::If(%5328) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5330 : Tensor = aten::dropout(%new_features.155, %5325, %5326) # torch/nn/functional.py:973:17
      -> (%5330)
    block1():
      -> (%new_features.155)
  %5331 : Tensor[] = aten::append(%features.4, %new_features.71) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.19 : Tensor = aten::cat(%features.4, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %5333 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_286.BatchNorm2d = prim::GetAttr[name="norm"](%36)
  %5334 : __torch__.torch.nn.modules.conv.___torch_mangle_287.Conv2d = prim::GetAttr[name="conv"](%36)
  %5335 : int = aten::dim(%input.19) # torch/nn/modules/batchnorm.py:276:11
  %5336 : bool = aten::ne(%5335, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5336) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5337 : bool = prim::GetAttr[name="training"](%5333)
   = prim::If(%5337) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5338 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5333)
      %5339 : Tensor = aten::add(%5338, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5333, %5339)
      -> ()
    block1():
      -> ()
  %5340 : bool = prim::GetAttr[name="training"](%5333)
  %5341 : Tensor = prim::GetAttr[name="running_mean"](%5333)
  %5342 : Tensor = prim::GetAttr[name="running_var"](%5333)
  %5343 : Tensor = prim::GetAttr[name="weight"](%5333)
  %5344 : Tensor = prim::GetAttr[name="bias"](%5333)
   = prim::If(%5340) # torch/nn/functional.py:2011:4
    block0():
      %5345 : int[] = aten::size(%input.19) # torch/nn/functional.py:2012:27
      %size_prods.636 : int = aten::__getitem__(%5345, %24) # torch/nn/functional.py:1991:17
      %5347 : int = aten::len(%5345) # torch/nn/functional.py:1992:19
      %5348 : int = aten::sub(%5347, %26) # torch/nn/functional.py:1992:19
      %size_prods.637 : int = prim::Loop(%5348, %25, %size_prods.636) # torch/nn/functional.py:1992:4
        block0(%i.160 : int, %size_prods.638 : int):
          %5352 : int = aten::add(%i.160, %26) # torch/nn/functional.py:1993:27
          %5353 : int = aten::__getitem__(%5345, %5352) # torch/nn/functional.py:1993:22
          %size_prods.639 : int = aten::mul(%size_prods.638, %5353) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.639)
      %5355 : bool = aten::eq(%size_prods.637, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5355) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %input.3 : Tensor = aten::batch_norm(%input.19, %5343, %5344, %5341, %5342, %5340, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %input.5 : Tensor = aten::relu_(%input.3) # torch/nn/functional.py:1117:17
  %5358 : Tensor = prim::GetAttr[name="weight"](%5334)
  %5359 : Tensor? = prim::GetAttr[name="bias"](%5334)
  %5360 : int[] = prim::ListConstruct(%27, %27)
  %5361 : int[] = prim::ListConstruct(%24, %24)
  %5362 : int[] = prim::ListConstruct(%27, %27)
  %input.7 : Tensor = aten::conv2d(%input.5, %5358, %5359, %5360, %5361, %5362, %27) # torch/nn/modules/conv.py:415:15
  %5364 : int[] = prim::ListConstruct(%26, %26)
  %5365 : int[] = prim::ListConstruct(%26, %26)
  %5366 : int[] = prim::ListConstruct(%24, %24)
  %input.21 : Tensor = aten::avg_pool2d(%input.7, %5364, %5365, %5366, %19, %25, %15) # torch/nn/modules/pooling.py:598:15
  %features.1 : Tensor[] = prim::ListConstruct(%input.21)
  %5369 : __torch__.torchvision.models.densenet.___torch_mangle_221._DenseLayer = prim::GetAttr[name="denselayer1"](%37)
  %5370 : __torch__.torchvision.models.densenet.___torch_mangle_224._DenseLayer = prim::GetAttr[name="denselayer2"](%37)
  %5371 : __torch__.torchvision.models.densenet.___torch_mangle_227._DenseLayer = prim::GetAttr[name="denselayer3"](%37)
  %5372 : __torch__.torchvision.models.densenet.___torch_mangle_230._DenseLayer = prim::GetAttr[name="denselayer4"](%37)
  %5373 : __torch__.torchvision.models.densenet.___torch_mangle_233._DenseLayer = prim::GetAttr[name="denselayer5"](%37)
  %5374 : __torch__.torchvision.models.densenet.___torch_mangle_236._DenseLayer = prim::GetAttr[name="denselayer6"](%37)
  %5375 : __torch__.torchvision.models.densenet.___torch_mangle_239._DenseLayer = prim::GetAttr[name="denselayer7"](%37)
  %5376 : __torch__.torchvision.models.densenet.___torch_mangle_242._DenseLayer = prim::GetAttr[name="denselayer8"](%37)
  %5377 : __torch__.torchvision.models.densenet.___torch_mangle_245._DenseLayer = prim::GetAttr[name="denselayer9"](%37)
  %5378 : __torch__.torchvision.models.densenet.___torch_mangle_248._DenseLayer = prim::GetAttr[name="denselayer10"](%37)
  %5379 : __torch__.torchvision.models.densenet.___torch_mangle_251._DenseLayer = prim::GetAttr[name="denselayer11"](%37)
  %5380 : __torch__.torchvision.models.densenet.___torch_mangle_254._DenseLayer = prim::GetAttr[name="denselayer12"](%37)
  %5381 : __torch__.torchvision.models.densenet.___torch_mangle_257._DenseLayer = prim::GetAttr[name="denselayer13"](%37)
  %5382 : __torch__.torchvision.models.densenet.___torch_mangle_260._DenseLayer = prim::GetAttr[name="denselayer14"](%37)
  %5383 : __torch__.torchvision.models.densenet.___torch_mangle_263._DenseLayer = prim::GetAttr[name="denselayer15"](%37)
  %5384 : __torch__.torchvision.models.densenet.___torch_mangle_266._DenseLayer = prim::GetAttr[name="denselayer16"](%37)
  %5385 : __torch__.torchvision.models.densenet.___torch_mangle_269._DenseLayer = prim::GetAttr[name="denselayer17"](%37)
  %5386 : __torch__.torchvision.models.densenet.___torch_mangle_272._DenseLayer = prim::GetAttr[name="denselayer18"](%37)
  %5387 : __torch__.torchvision.models.densenet.___torch_mangle_275._DenseLayer = prim::GetAttr[name="denselayer19"](%37)
  %5388 : __torch__.torchvision.models.densenet.___torch_mangle_278._DenseLayer = prim::GetAttr[name="denselayer20"](%37)
  %5389 : __torch__.torchvision.models.densenet.___torch_mangle_281._DenseLayer = prim::GetAttr[name="denselayer21"](%37)
  %5390 : __torch__.torchvision.models.densenet.___torch_mangle_284._DenseLayer = prim::GetAttr[name="denselayer22"](%37)
  %5391 : __torch__.torchvision.models.densenet.___torch_mangle_290._DenseLayer = prim::GetAttr[name="denselayer23"](%37)
  %5392 : __torch__.torchvision.models.densenet.___torch_mangle_293._DenseLayer = prim::GetAttr[name="denselayer24"](%37)
  %5393 : Tensor = prim::Uninitialized()
  %5394 : bool = prim::GetAttr[name="memory_efficient"](%5369)
  %5395 : bool = prim::If(%5394) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5396 : bool = prim::Uninitialized()
      %5397 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5398 : bool = aten::gt(%5397, %24)
      %5399 : bool, %5400 : bool, %5401 : int = prim::Loop(%18, %5398, %19, %5396, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5402 : int, %5403 : bool, %5404 : bool, %5405 : int):
          %tensor.2 : Tensor = aten::__getitem__(%features.1, %5405) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5407 : bool = prim::requires_grad(%tensor.2)
          %5408 : bool, %5409 : bool = prim::If(%5407) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5396)
          %5410 : int = aten::add(%5405, %27)
          %5411 : bool = aten::lt(%5410, %5397)
          %5412 : bool = aten::__and__(%5411, %5408)
          -> (%5412, %5407, %5409, %5410)
      %5413 : bool = prim::If(%5399)
        block0():
          -> (%5400)
        block1():
          -> (%19)
      -> (%5413)
    block1():
      -> (%19)
  %bottleneck_output.1 : Tensor = prim::If(%5395) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5393)
    block1():
      %concated_features.2 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5416 : __torch__.torch.nn.modules.conv.___torch_mangle_220.Conv2d = prim::GetAttr[name="conv1"](%5369)
      %5417 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_219.BatchNorm2d = prim::GetAttr[name="norm1"](%5369)
      %5418 : int = aten::dim(%concated_features.2) # torch/nn/modules/batchnorm.py:276:11
      %5419 : bool = aten::ne(%5418, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5419) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5420 : bool = prim::GetAttr[name="training"](%5417)
       = prim::If(%5420) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5421 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5417)
          %5422 : Tensor = aten::add(%5421, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5417, %5422)
          -> ()
        block1():
          -> ()
      %5423 : bool = prim::GetAttr[name="training"](%5417)
      %5424 : Tensor = prim::GetAttr[name="running_mean"](%5417)
      %5425 : Tensor = prim::GetAttr[name="running_var"](%5417)
      %5426 : Tensor = prim::GetAttr[name="weight"](%5417)
      %5427 : Tensor = prim::GetAttr[name="bias"](%5417)
       = prim::If(%5423) # torch/nn/functional.py:2011:4
        block0():
          %5428 : int[] = aten::size(%concated_features.2) # torch/nn/functional.py:2012:27
          %size_prods.8 : int = aten::__getitem__(%5428, %24) # torch/nn/functional.py:1991:17
          %5430 : int = aten::len(%5428) # torch/nn/functional.py:1992:19
          %5431 : int = aten::sub(%5430, %26) # torch/nn/functional.py:1992:19
          %size_prods.9 : int = prim::Loop(%5431, %25, %size_prods.8) # torch/nn/functional.py:1992:4
            block0(%i.3 : int, %size_prods.10 : int):
              %5435 : int = aten::add(%i.3, %26) # torch/nn/functional.py:1993:27
              %5436 : int = aten::__getitem__(%5428, %5435) # torch/nn/functional.py:1993:22
              %size_prods.11 : int = aten::mul(%size_prods.10, %5436) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.11)
          %5438 : bool = aten::eq(%size_prods.9, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5438) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5439 : Tensor = aten::batch_norm(%concated_features.2, %5426, %5427, %5424, %5425, %5423, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.3 : Tensor = aten::relu_(%5439) # torch/nn/functional.py:1117:17
      %5441 : Tensor = prim::GetAttr[name="weight"](%5416)
      %5442 : Tensor? = prim::GetAttr[name="bias"](%5416)
      %5443 : int[] = prim::ListConstruct(%27, %27)
      %5444 : int[] = prim::ListConstruct(%24, %24)
      %5445 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.3 : Tensor = aten::conv2d(%result.3, %5441, %5442, %5443, %5444, %5445, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.3)
  %5447 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5369)
  %5448 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5369)
  %5449 : int = aten::dim(%bottleneck_output.1) # torch/nn/modules/batchnorm.py:276:11
  %5450 : bool = aten::ne(%5449, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5450) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5451 : bool = prim::GetAttr[name="training"](%5448)
   = prim::If(%5451) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5452 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5448)
      %5453 : Tensor = aten::add(%5452, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5448, %5453)
      -> ()
    block1():
      -> ()
  %5454 : bool = prim::GetAttr[name="training"](%5448)
  %5455 : Tensor = prim::GetAttr[name="running_mean"](%5448)
  %5456 : Tensor = prim::GetAttr[name="running_var"](%5448)
  %5457 : Tensor = prim::GetAttr[name="weight"](%5448)
  %5458 : Tensor = prim::GetAttr[name="bias"](%5448)
   = prim::If(%5454) # torch/nn/functional.py:2011:4
    block0():
      %5459 : int[] = aten::size(%bottleneck_output.1) # torch/nn/functional.py:2012:27
      %size_prods.12 : int = aten::__getitem__(%5459, %24) # torch/nn/functional.py:1991:17
      %5461 : int = aten::len(%5459) # torch/nn/functional.py:1992:19
      %5462 : int = aten::sub(%5461, %26) # torch/nn/functional.py:1992:19
      %size_prods.13 : int = prim::Loop(%5462, %25, %size_prods.12) # torch/nn/functional.py:1992:4
        block0(%i.4 : int, %size_prods.14 : int):
          %5466 : int = aten::add(%i.4, %26) # torch/nn/functional.py:1993:27
          %5467 : int = aten::__getitem__(%5459, %5466) # torch/nn/functional.py:1993:22
          %size_prods.15 : int = aten::mul(%size_prods.14, %5467) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.15)
      %5469 : bool = aten::eq(%size_prods.13, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5469) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5470 : Tensor = aten::batch_norm(%bottleneck_output.1, %5457, %5458, %5455, %5456, %5454, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.4 : Tensor = aten::relu_(%5470) # torch/nn/functional.py:1117:17
  %5472 : Tensor = prim::GetAttr[name="weight"](%5447)
  %5473 : Tensor? = prim::GetAttr[name="bias"](%5447)
  %5474 : int[] = prim::ListConstruct(%27, %27)
  %5475 : int[] = prim::ListConstruct(%27, %27)
  %5476 : int[] = prim::ListConstruct(%27, %27)
  %new_features.4 : Tensor = aten::conv2d(%result.4, %5472, %5473, %5474, %5475, %5476, %27) # torch/nn/modules/conv.py:415:15
  %5478 : float = prim::GetAttr[name="drop_rate"](%5369)
  %5479 : bool = aten::gt(%5478, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.2 : Tensor = prim::If(%5479) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5481 : float = prim::GetAttr[name="drop_rate"](%5369)
      %5482 : bool = prim::GetAttr[name="training"](%5369)
      %5483 : bool = aten::lt(%5481, %16) # torch/nn/functional.py:968:7
      %5484 : bool = prim::If(%5483) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5485 : bool = aten::gt(%5481, %17) # torch/nn/functional.py:968:17
          -> (%5485)
       = prim::If(%5484) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5486 : Tensor = aten::dropout(%new_features.4, %5481, %5482) # torch/nn/functional.py:973:17
      -> (%5486)
    block1():
      -> (%new_features.4)
  %5487 : Tensor[] = aten::append(%features.1, %new_features.2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5488 : Tensor = prim::Uninitialized()
  %5489 : bool = prim::GetAttr[name="memory_efficient"](%5370)
  %5490 : bool = prim::If(%5489) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5491 : bool = prim::Uninitialized()
      %5492 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5493 : bool = aten::gt(%5492, %24)
      %5494 : bool, %5495 : bool, %5496 : int = prim::Loop(%18, %5493, %19, %5491, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5497 : int, %5498 : bool, %5499 : bool, %5500 : int):
          %tensor.3 : Tensor = aten::__getitem__(%features.1, %5500) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5502 : bool = prim::requires_grad(%tensor.3)
          %5503 : bool, %5504 : bool = prim::If(%5502) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5491)
          %5505 : int = aten::add(%5500, %27)
          %5506 : bool = aten::lt(%5505, %5492)
          %5507 : bool = aten::__and__(%5506, %5503)
          -> (%5507, %5502, %5504, %5505)
      %5508 : bool = prim::If(%5494)
        block0():
          -> (%5495)
        block1():
          -> (%19)
      -> (%5508)
    block1():
      -> (%19)
  %bottleneck_output.4 : Tensor = prim::If(%5490) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5488)
    block1():
      %concated_features.3 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5511 : __torch__.torch.nn.modules.conv.___torch_mangle_223.Conv2d = prim::GetAttr[name="conv1"](%5370)
      %5512 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_222.BatchNorm2d = prim::GetAttr[name="norm1"](%5370)
      %5513 : int = aten::dim(%concated_features.3) # torch/nn/modules/batchnorm.py:276:11
      %5514 : bool = aten::ne(%5513, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5514) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5515 : bool = prim::GetAttr[name="training"](%5512)
       = prim::If(%5515) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5516 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5512)
          %5517 : Tensor = aten::add(%5516, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5512, %5517)
          -> ()
        block1():
          -> ()
      %5518 : bool = prim::GetAttr[name="training"](%5512)
      %5519 : Tensor = prim::GetAttr[name="running_mean"](%5512)
      %5520 : Tensor = prim::GetAttr[name="running_var"](%5512)
      %5521 : Tensor = prim::GetAttr[name="weight"](%5512)
      %5522 : Tensor = prim::GetAttr[name="bias"](%5512)
       = prim::If(%5518) # torch/nn/functional.py:2011:4
        block0():
          %5523 : int[] = aten::size(%concated_features.3) # torch/nn/functional.py:2012:27
          %size_prods.16 : int = aten::__getitem__(%5523, %24) # torch/nn/functional.py:1991:17
          %5525 : int = aten::len(%5523) # torch/nn/functional.py:1992:19
          %5526 : int = aten::sub(%5525, %26) # torch/nn/functional.py:1992:19
          %size_prods.17 : int = prim::Loop(%5526, %25, %size_prods.16) # torch/nn/functional.py:1992:4
            block0(%i.5 : int, %size_prods.18 : int):
              %5530 : int = aten::add(%i.5, %26) # torch/nn/functional.py:1993:27
              %5531 : int = aten::__getitem__(%5523, %5530) # torch/nn/functional.py:1993:22
              %size_prods.19 : int = aten::mul(%size_prods.18, %5531) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.19)
          %5533 : bool = aten::eq(%size_prods.17, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5533) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5534 : Tensor = aten::batch_norm(%concated_features.3, %5521, %5522, %5519, %5520, %5518, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.5 : Tensor = aten::relu_(%5534) # torch/nn/functional.py:1117:17
      %5536 : Tensor = prim::GetAttr[name="weight"](%5511)
      %5537 : Tensor? = prim::GetAttr[name="bias"](%5511)
      %5538 : int[] = prim::ListConstruct(%27, %27)
      %5539 : int[] = prim::ListConstruct(%24, %24)
      %5540 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.5 : Tensor = aten::conv2d(%result.5, %5536, %5537, %5538, %5539, %5540, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.5)
  %5542 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5370)
  %5543 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5370)
  %5544 : int = aten::dim(%bottleneck_output.4) # torch/nn/modules/batchnorm.py:276:11
  %5545 : bool = aten::ne(%5544, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5545) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5546 : bool = prim::GetAttr[name="training"](%5543)
   = prim::If(%5546) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5547 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5543)
      %5548 : Tensor = aten::add(%5547, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5543, %5548)
      -> ()
    block1():
      -> ()
  %5549 : bool = prim::GetAttr[name="training"](%5543)
  %5550 : Tensor = prim::GetAttr[name="running_mean"](%5543)
  %5551 : Tensor = prim::GetAttr[name="running_var"](%5543)
  %5552 : Tensor = prim::GetAttr[name="weight"](%5543)
  %5553 : Tensor = prim::GetAttr[name="bias"](%5543)
   = prim::If(%5549) # torch/nn/functional.py:2011:4
    block0():
      %5554 : int[] = aten::size(%bottleneck_output.4) # torch/nn/functional.py:2012:27
      %size_prods.20 : int = aten::__getitem__(%5554, %24) # torch/nn/functional.py:1991:17
      %5556 : int = aten::len(%5554) # torch/nn/functional.py:1992:19
      %5557 : int = aten::sub(%5556, %26) # torch/nn/functional.py:1992:19
      %size_prods.21 : int = prim::Loop(%5557, %25, %size_prods.20) # torch/nn/functional.py:1992:4
        block0(%i.6 : int, %size_prods.22 : int):
          %5561 : int = aten::add(%i.6, %26) # torch/nn/functional.py:1993:27
          %5562 : int = aten::__getitem__(%5554, %5561) # torch/nn/functional.py:1993:22
          %size_prods.23 : int = aten::mul(%size_prods.22, %5562) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.23)
      %5564 : bool = aten::eq(%size_prods.21, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5564) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5565 : Tensor = aten::batch_norm(%bottleneck_output.4, %5552, %5553, %5550, %5551, %5549, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.6 : Tensor = aten::relu_(%5565) # torch/nn/functional.py:1117:17
  %5567 : Tensor = prim::GetAttr[name="weight"](%5542)
  %5568 : Tensor? = prim::GetAttr[name="bias"](%5542)
  %5569 : int[] = prim::ListConstruct(%27, %27)
  %5570 : int[] = prim::ListConstruct(%27, %27)
  %5571 : int[] = prim::ListConstruct(%27, %27)
  %new_features.6 : Tensor = aten::conv2d(%result.6, %5567, %5568, %5569, %5570, %5571, %27) # torch/nn/modules/conv.py:415:15
  %5573 : float = prim::GetAttr[name="drop_rate"](%5370)
  %5574 : bool = aten::gt(%5573, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.3 : Tensor = prim::If(%5574) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5576 : float = prim::GetAttr[name="drop_rate"](%5370)
      %5577 : bool = prim::GetAttr[name="training"](%5370)
      %5578 : bool = aten::lt(%5576, %16) # torch/nn/functional.py:968:7
      %5579 : bool = prim::If(%5578) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5580 : bool = aten::gt(%5576, %17) # torch/nn/functional.py:968:17
          -> (%5580)
       = prim::If(%5579) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5581 : Tensor = aten::dropout(%new_features.6, %5576, %5577) # torch/nn/functional.py:973:17
      -> (%5581)
    block1():
      -> (%new_features.6)
  %5582 : Tensor[] = aten::append(%features.1, %new_features.3) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5583 : Tensor = prim::Uninitialized()
  %5584 : bool = prim::GetAttr[name="memory_efficient"](%5371)
  %5585 : bool = prim::If(%5584) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5586 : bool = prim::Uninitialized()
      %5587 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5588 : bool = aten::gt(%5587, %24)
      %5589 : bool, %5590 : bool, %5591 : int = prim::Loop(%18, %5588, %19, %5586, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5592 : int, %5593 : bool, %5594 : bool, %5595 : int):
          %tensor.4 : Tensor = aten::__getitem__(%features.1, %5595) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5597 : bool = prim::requires_grad(%tensor.4)
          %5598 : bool, %5599 : bool = prim::If(%5597) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5586)
          %5600 : int = aten::add(%5595, %27)
          %5601 : bool = aten::lt(%5600, %5587)
          %5602 : bool = aten::__and__(%5601, %5598)
          -> (%5602, %5597, %5599, %5600)
      %5603 : bool = prim::If(%5589)
        block0():
          -> (%5590)
        block1():
          -> (%19)
      -> (%5603)
    block1():
      -> (%19)
  %bottleneck_output.6 : Tensor = prim::If(%5585) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5583)
    block1():
      %concated_features.4 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5606 : __torch__.torch.nn.modules.conv.___torch_mangle_226.Conv2d = prim::GetAttr[name="conv1"](%5371)
      %5607 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_225.BatchNorm2d = prim::GetAttr[name="norm1"](%5371)
      %5608 : int = aten::dim(%concated_features.4) # torch/nn/modules/batchnorm.py:276:11
      %5609 : bool = aten::ne(%5608, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5609) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5610 : bool = prim::GetAttr[name="training"](%5607)
       = prim::If(%5610) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5611 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5607)
          %5612 : Tensor = aten::add(%5611, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5607, %5612)
          -> ()
        block1():
          -> ()
      %5613 : bool = prim::GetAttr[name="training"](%5607)
      %5614 : Tensor = prim::GetAttr[name="running_mean"](%5607)
      %5615 : Tensor = prim::GetAttr[name="running_var"](%5607)
      %5616 : Tensor = prim::GetAttr[name="weight"](%5607)
      %5617 : Tensor = prim::GetAttr[name="bias"](%5607)
       = prim::If(%5613) # torch/nn/functional.py:2011:4
        block0():
          %5618 : int[] = aten::size(%concated_features.4) # torch/nn/functional.py:2012:27
          %size_prods.24 : int = aten::__getitem__(%5618, %24) # torch/nn/functional.py:1991:17
          %5620 : int = aten::len(%5618) # torch/nn/functional.py:1992:19
          %5621 : int = aten::sub(%5620, %26) # torch/nn/functional.py:1992:19
          %size_prods.25 : int = prim::Loop(%5621, %25, %size_prods.24) # torch/nn/functional.py:1992:4
            block0(%i.7 : int, %size_prods.26 : int):
              %5625 : int = aten::add(%i.7, %26) # torch/nn/functional.py:1993:27
              %5626 : int = aten::__getitem__(%5618, %5625) # torch/nn/functional.py:1993:22
              %size_prods.27 : int = aten::mul(%size_prods.26, %5626) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.27)
          %5628 : bool = aten::eq(%size_prods.25, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5628) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5629 : Tensor = aten::batch_norm(%concated_features.4, %5616, %5617, %5614, %5615, %5613, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.7 : Tensor = aten::relu_(%5629) # torch/nn/functional.py:1117:17
      %5631 : Tensor = prim::GetAttr[name="weight"](%5606)
      %5632 : Tensor? = prim::GetAttr[name="bias"](%5606)
      %5633 : int[] = prim::ListConstruct(%27, %27)
      %5634 : int[] = prim::ListConstruct(%24, %24)
      %5635 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.7 : Tensor = aten::conv2d(%result.7, %5631, %5632, %5633, %5634, %5635, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.7)
  %5637 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5371)
  %5638 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5371)
  %5639 : int = aten::dim(%bottleneck_output.6) # torch/nn/modules/batchnorm.py:276:11
  %5640 : bool = aten::ne(%5639, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5640) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5641 : bool = prim::GetAttr[name="training"](%5638)
   = prim::If(%5641) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5642 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5638)
      %5643 : Tensor = aten::add(%5642, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5638, %5643)
      -> ()
    block1():
      -> ()
  %5644 : bool = prim::GetAttr[name="training"](%5638)
  %5645 : Tensor = prim::GetAttr[name="running_mean"](%5638)
  %5646 : Tensor = prim::GetAttr[name="running_var"](%5638)
  %5647 : Tensor = prim::GetAttr[name="weight"](%5638)
  %5648 : Tensor = prim::GetAttr[name="bias"](%5638)
   = prim::If(%5644) # torch/nn/functional.py:2011:4
    block0():
      %5649 : int[] = aten::size(%bottleneck_output.6) # torch/nn/functional.py:2012:27
      %size_prods.28 : int = aten::__getitem__(%5649, %24) # torch/nn/functional.py:1991:17
      %5651 : int = aten::len(%5649) # torch/nn/functional.py:1992:19
      %5652 : int = aten::sub(%5651, %26) # torch/nn/functional.py:1992:19
      %size_prods.29 : int = prim::Loop(%5652, %25, %size_prods.28) # torch/nn/functional.py:1992:4
        block0(%i.8 : int, %size_prods.30 : int):
          %5656 : int = aten::add(%i.8, %26) # torch/nn/functional.py:1993:27
          %5657 : int = aten::__getitem__(%5649, %5656) # torch/nn/functional.py:1993:22
          %size_prods.31 : int = aten::mul(%size_prods.30, %5657) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.31)
      %5659 : bool = aten::eq(%size_prods.29, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5659) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5660 : Tensor = aten::batch_norm(%bottleneck_output.6, %5647, %5648, %5645, %5646, %5644, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.8 : Tensor = aten::relu_(%5660) # torch/nn/functional.py:1117:17
  %5662 : Tensor = prim::GetAttr[name="weight"](%5637)
  %5663 : Tensor? = prim::GetAttr[name="bias"](%5637)
  %5664 : int[] = prim::ListConstruct(%27, %27)
  %5665 : int[] = prim::ListConstruct(%27, %27)
  %5666 : int[] = prim::ListConstruct(%27, %27)
  %new_features.8 : Tensor = aten::conv2d(%result.8, %5662, %5663, %5664, %5665, %5666, %27) # torch/nn/modules/conv.py:415:15
  %5668 : float = prim::GetAttr[name="drop_rate"](%5371)
  %5669 : bool = aten::gt(%5668, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.5 : Tensor = prim::If(%5669) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5671 : float = prim::GetAttr[name="drop_rate"](%5371)
      %5672 : bool = prim::GetAttr[name="training"](%5371)
      %5673 : bool = aten::lt(%5671, %16) # torch/nn/functional.py:968:7
      %5674 : bool = prim::If(%5673) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5675 : bool = aten::gt(%5671, %17) # torch/nn/functional.py:968:17
          -> (%5675)
       = prim::If(%5674) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5676 : Tensor = aten::dropout(%new_features.8, %5671, %5672) # torch/nn/functional.py:973:17
      -> (%5676)
    block1():
      -> (%new_features.8)
  %5677 : Tensor[] = aten::append(%features.1, %new_features.5) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5678 : Tensor = prim::Uninitialized()
  %5679 : bool = prim::GetAttr[name="memory_efficient"](%5372)
  %5680 : bool = prim::If(%5679) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5681 : bool = prim::Uninitialized()
      %5682 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5683 : bool = aten::gt(%5682, %24)
      %5684 : bool, %5685 : bool, %5686 : int = prim::Loop(%18, %5683, %19, %5681, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5687 : int, %5688 : bool, %5689 : bool, %5690 : int):
          %tensor.5 : Tensor = aten::__getitem__(%features.1, %5690) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5692 : bool = prim::requires_grad(%tensor.5)
          %5693 : bool, %5694 : bool = prim::If(%5692) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5681)
          %5695 : int = aten::add(%5690, %27)
          %5696 : bool = aten::lt(%5695, %5682)
          %5697 : bool = aten::__and__(%5696, %5693)
          -> (%5697, %5692, %5694, %5695)
      %5698 : bool = prim::If(%5684)
        block0():
          -> (%5685)
        block1():
          -> (%19)
      -> (%5698)
    block1():
      -> (%19)
  %bottleneck_output.8 : Tensor = prim::If(%5680) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5678)
    block1():
      %concated_features.5 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5701 : __torch__.torch.nn.modules.conv.___torch_mangle_229.Conv2d = prim::GetAttr[name="conv1"](%5372)
      %5702 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_228.BatchNorm2d = prim::GetAttr[name="norm1"](%5372)
      %5703 : int = aten::dim(%concated_features.5) # torch/nn/modules/batchnorm.py:276:11
      %5704 : bool = aten::ne(%5703, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5704) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5705 : bool = prim::GetAttr[name="training"](%5702)
       = prim::If(%5705) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5706 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5702)
          %5707 : Tensor = aten::add(%5706, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5702, %5707)
          -> ()
        block1():
          -> ()
      %5708 : bool = prim::GetAttr[name="training"](%5702)
      %5709 : Tensor = prim::GetAttr[name="running_mean"](%5702)
      %5710 : Tensor = prim::GetAttr[name="running_var"](%5702)
      %5711 : Tensor = prim::GetAttr[name="weight"](%5702)
      %5712 : Tensor = prim::GetAttr[name="bias"](%5702)
       = prim::If(%5708) # torch/nn/functional.py:2011:4
        block0():
          %5713 : int[] = aten::size(%concated_features.5) # torch/nn/functional.py:2012:27
          %size_prods.32 : int = aten::__getitem__(%5713, %24) # torch/nn/functional.py:1991:17
          %5715 : int = aten::len(%5713) # torch/nn/functional.py:1992:19
          %5716 : int = aten::sub(%5715, %26) # torch/nn/functional.py:1992:19
          %size_prods.33 : int = prim::Loop(%5716, %25, %size_prods.32) # torch/nn/functional.py:1992:4
            block0(%i.9 : int, %size_prods.34 : int):
              %5720 : int = aten::add(%i.9, %26) # torch/nn/functional.py:1993:27
              %5721 : int = aten::__getitem__(%5713, %5720) # torch/nn/functional.py:1993:22
              %size_prods.35 : int = aten::mul(%size_prods.34, %5721) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.35)
          %5723 : bool = aten::eq(%size_prods.33, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5723) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5724 : Tensor = aten::batch_norm(%concated_features.5, %5711, %5712, %5709, %5710, %5708, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.9 : Tensor = aten::relu_(%5724) # torch/nn/functional.py:1117:17
      %5726 : Tensor = prim::GetAttr[name="weight"](%5701)
      %5727 : Tensor? = prim::GetAttr[name="bias"](%5701)
      %5728 : int[] = prim::ListConstruct(%27, %27)
      %5729 : int[] = prim::ListConstruct(%24, %24)
      %5730 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.9 : Tensor = aten::conv2d(%result.9, %5726, %5727, %5728, %5729, %5730, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.9)
  %5732 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5372)
  %5733 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5372)
  %5734 : int = aten::dim(%bottleneck_output.8) # torch/nn/modules/batchnorm.py:276:11
  %5735 : bool = aten::ne(%5734, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5735) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5736 : bool = prim::GetAttr[name="training"](%5733)
   = prim::If(%5736) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5737 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5733)
      %5738 : Tensor = aten::add(%5737, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5733, %5738)
      -> ()
    block1():
      -> ()
  %5739 : bool = prim::GetAttr[name="training"](%5733)
  %5740 : Tensor = prim::GetAttr[name="running_mean"](%5733)
  %5741 : Tensor = prim::GetAttr[name="running_var"](%5733)
  %5742 : Tensor = prim::GetAttr[name="weight"](%5733)
  %5743 : Tensor = prim::GetAttr[name="bias"](%5733)
   = prim::If(%5739) # torch/nn/functional.py:2011:4
    block0():
      %5744 : int[] = aten::size(%bottleneck_output.8) # torch/nn/functional.py:2012:27
      %size_prods.36 : int = aten::__getitem__(%5744, %24) # torch/nn/functional.py:1991:17
      %5746 : int = aten::len(%5744) # torch/nn/functional.py:1992:19
      %5747 : int = aten::sub(%5746, %26) # torch/nn/functional.py:1992:19
      %size_prods.37 : int = prim::Loop(%5747, %25, %size_prods.36) # torch/nn/functional.py:1992:4
        block0(%i.10 : int, %size_prods.38 : int):
          %5751 : int = aten::add(%i.10, %26) # torch/nn/functional.py:1993:27
          %5752 : int = aten::__getitem__(%5744, %5751) # torch/nn/functional.py:1993:22
          %size_prods.39 : int = aten::mul(%size_prods.38, %5752) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.39)
      %5754 : bool = aten::eq(%size_prods.37, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5754) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5755 : Tensor = aten::batch_norm(%bottleneck_output.8, %5742, %5743, %5740, %5741, %5739, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.10 : Tensor = aten::relu_(%5755) # torch/nn/functional.py:1117:17
  %5757 : Tensor = prim::GetAttr[name="weight"](%5732)
  %5758 : Tensor? = prim::GetAttr[name="bias"](%5732)
  %5759 : int[] = prim::ListConstruct(%27, %27)
  %5760 : int[] = prim::ListConstruct(%27, %27)
  %5761 : int[] = prim::ListConstruct(%27, %27)
  %new_features.10 : Tensor = aten::conv2d(%result.10, %5757, %5758, %5759, %5760, %5761, %27) # torch/nn/modules/conv.py:415:15
  %5763 : float = prim::GetAttr[name="drop_rate"](%5372)
  %5764 : bool = aten::gt(%5763, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.7 : Tensor = prim::If(%5764) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5766 : float = prim::GetAttr[name="drop_rate"](%5372)
      %5767 : bool = prim::GetAttr[name="training"](%5372)
      %5768 : bool = aten::lt(%5766, %16) # torch/nn/functional.py:968:7
      %5769 : bool = prim::If(%5768) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5770 : bool = aten::gt(%5766, %17) # torch/nn/functional.py:968:17
          -> (%5770)
       = prim::If(%5769) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5771 : Tensor = aten::dropout(%new_features.10, %5766, %5767) # torch/nn/functional.py:973:17
      -> (%5771)
    block1():
      -> (%new_features.10)
  %5772 : Tensor[] = aten::append(%features.1, %new_features.7) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5773 : Tensor = prim::Uninitialized()
  %5774 : bool = prim::GetAttr[name="memory_efficient"](%5373)
  %5775 : bool = prim::If(%5774) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5776 : bool = prim::Uninitialized()
      %5777 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5778 : bool = aten::gt(%5777, %24)
      %5779 : bool, %5780 : bool, %5781 : int = prim::Loop(%18, %5778, %19, %5776, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5782 : int, %5783 : bool, %5784 : bool, %5785 : int):
          %tensor.6 : Tensor = aten::__getitem__(%features.1, %5785) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5787 : bool = prim::requires_grad(%tensor.6)
          %5788 : bool, %5789 : bool = prim::If(%5787) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5776)
          %5790 : int = aten::add(%5785, %27)
          %5791 : bool = aten::lt(%5790, %5777)
          %5792 : bool = aten::__and__(%5791, %5788)
          -> (%5792, %5787, %5789, %5790)
      %5793 : bool = prim::If(%5779)
        block0():
          -> (%5780)
        block1():
          -> (%19)
      -> (%5793)
    block1():
      -> (%19)
  %bottleneck_output.10 : Tensor = prim::If(%5775) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5773)
    block1():
      %concated_features.6 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5796 : __torch__.torch.nn.modules.conv.___torch_mangle_232.Conv2d = prim::GetAttr[name="conv1"](%5373)
      %5797 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_231.BatchNorm2d = prim::GetAttr[name="norm1"](%5373)
      %5798 : int = aten::dim(%concated_features.6) # torch/nn/modules/batchnorm.py:276:11
      %5799 : bool = aten::ne(%5798, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5799) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5800 : bool = prim::GetAttr[name="training"](%5797)
       = prim::If(%5800) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5801 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5797)
          %5802 : Tensor = aten::add(%5801, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5797, %5802)
          -> ()
        block1():
          -> ()
      %5803 : bool = prim::GetAttr[name="training"](%5797)
      %5804 : Tensor = prim::GetAttr[name="running_mean"](%5797)
      %5805 : Tensor = prim::GetAttr[name="running_var"](%5797)
      %5806 : Tensor = prim::GetAttr[name="weight"](%5797)
      %5807 : Tensor = prim::GetAttr[name="bias"](%5797)
       = prim::If(%5803) # torch/nn/functional.py:2011:4
        block0():
          %5808 : int[] = aten::size(%concated_features.6) # torch/nn/functional.py:2012:27
          %size_prods.40 : int = aten::__getitem__(%5808, %24) # torch/nn/functional.py:1991:17
          %5810 : int = aten::len(%5808) # torch/nn/functional.py:1992:19
          %5811 : int = aten::sub(%5810, %26) # torch/nn/functional.py:1992:19
          %size_prods.41 : int = prim::Loop(%5811, %25, %size_prods.40) # torch/nn/functional.py:1992:4
            block0(%i.11 : int, %size_prods.42 : int):
              %5815 : int = aten::add(%i.11, %26) # torch/nn/functional.py:1993:27
              %5816 : int = aten::__getitem__(%5808, %5815) # torch/nn/functional.py:1993:22
              %size_prods.43 : int = aten::mul(%size_prods.42, %5816) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.43)
          %5818 : bool = aten::eq(%size_prods.41, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5818) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5819 : Tensor = aten::batch_norm(%concated_features.6, %5806, %5807, %5804, %5805, %5803, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.11 : Tensor = aten::relu_(%5819) # torch/nn/functional.py:1117:17
      %5821 : Tensor = prim::GetAttr[name="weight"](%5796)
      %5822 : Tensor? = prim::GetAttr[name="bias"](%5796)
      %5823 : int[] = prim::ListConstruct(%27, %27)
      %5824 : int[] = prim::ListConstruct(%24, %24)
      %5825 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.11 : Tensor = aten::conv2d(%result.11, %5821, %5822, %5823, %5824, %5825, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.11)
  %5827 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5373)
  %5828 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5373)
  %5829 : int = aten::dim(%bottleneck_output.10) # torch/nn/modules/batchnorm.py:276:11
  %5830 : bool = aten::ne(%5829, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5830) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5831 : bool = prim::GetAttr[name="training"](%5828)
   = prim::If(%5831) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5832 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5828)
      %5833 : Tensor = aten::add(%5832, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5828, %5833)
      -> ()
    block1():
      -> ()
  %5834 : bool = prim::GetAttr[name="training"](%5828)
  %5835 : Tensor = prim::GetAttr[name="running_mean"](%5828)
  %5836 : Tensor = prim::GetAttr[name="running_var"](%5828)
  %5837 : Tensor = prim::GetAttr[name="weight"](%5828)
  %5838 : Tensor = prim::GetAttr[name="bias"](%5828)
   = prim::If(%5834) # torch/nn/functional.py:2011:4
    block0():
      %5839 : int[] = aten::size(%bottleneck_output.10) # torch/nn/functional.py:2012:27
      %size_prods.44 : int = aten::__getitem__(%5839, %24) # torch/nn/functional.py:1991:17
      %5841 : int = aten::len(%5839) # torch/nn/functional.py:1992:19
      %5842 : int = aten::sub(%5841, %26) # torch/nn/functional.py:1992:19
      %size_prods.45 : int = prim::Loop(%5842, %25, %size_prods.44) # torch/nn/functional.py:1992:4
        block0(%i.12 : int, %size_prods.46 : int):
          %5846 : int = aten::add(%i.12, %26) # torch/nn/functional.py:1993:27
          %5847 : int = aten::__getitem__(%5839, %5846) # torch/nn/functional.py:1993:22
          %size_prods.47 : int = aten::mul(%size_prods.46, %5847) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.47)
      %5849 : bool = aten::eq(%size_prods.45, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5849) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5850 : Tensor = aten::batch_norm(%bottleneck_output.10, %5837, %5838, %5835, %5836, %5834, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.12 : Tensor = aten::relu_(%5850) # torch/nn/functional.py:1117:17
  %5852 : Tensor = prim::GetAttr[name="weight"](%5827)
  %5853 : Tensor? = prim::GetAttr[name="bias"](%5827)
  %5854 : int[] = prim::ListConstruct(%27, %27)
  %5855 : int[] = prim::ListConstruct(%27, %27)
  %5856 : int[] = prim::ListConstruct(%27, %27)
  %new_features.12 : Tensor = aten::conv2d(%result.12, %5852, %5853, %5854, %5855, %5856, %27) # torch/nn/modules/conv.py:415:15
  %5858 : float = prim::GetAttr[name="drop_rate"](%5373)
  %5859 : bool = aten::gt(%5858, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.9 : Tensor = prim::If(%5859) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5861 : float = prim::GetAttr[name="drop_rate"](%5373)
      %5862 : bool = prim::GetAttr[name="training"](%5373)
      %5863 : bool = aten::lt(%5861, %16) # torch/nn/functional.py:968:7
      %5864 : bool = prim::If(%5863) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5865 : bool = aten::gt(%5861, %17) # torch/nn/functional.py:968:17
          -> (%5865)
       = prim::If(%5864) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5866 : Tensor = aten::dropout(%new_features.12, %5861, %5862) # torch/nn/functional.py:973:17
      -> (%5866)
    block1():
      -> (%new_features.12)
  %5867 : Tensor[] = aten::append(%features.1, %new_features.9) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5868 : Tensor = prim::Uninitialized()
  %5869 : bool = prim::GetAttr[name="memory_efficient"](%5374)
  %5870 : bool = prim::If(%5869) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5871 : bool = prim::Uninitialized()
      %5872 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5873 : bool = aten::gt(%5872, %24)
      %5874 : bool, %5875 : bool, %5876 : int = prim::Loop(%18, %5873, %19, %5871, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5877 : int, %5878 : bool, %5879 : bool, %5880 : int):
          %tensor.7 : Tensor = aten::__getitem__(%features.1, %5880) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5882 : bool = prim::requires_grad(%tensor.7)
          %5883 : bool, %5884 : bool = prim::If(%5882) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5871)
          %5885 : int = aten::add(%5880, %27)
          %5886 : bool = aten::lt(%5885, %5872)
          %5887 : bool = aten::__and__(%5886, %5883)
          -> (%5887, %5882, %5884, %5885)
      %5888 : bool = prim::If(%5874)
        block0():
          -> (%5875)
        block1():
          -> (%19)
      -> (%5888)
    block1():
      -> (%19)
  %bottleneck_output.12 : Tensor = prim::If(%5870) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5868)
    block1():
      %concated_features.7 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5891 : __torch__.torch.nn.modules.conv.___torch_mangle_235.Conv2d = prim::GetAttr[name="conv1"](%5374)
      %5892 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_234.BatchNorm2d = prim::GetAttr[name="norm1"](%5374)
      %5893 : int = aten::dim(%concated_features.7) # torch/nn/modules/batchnorm.py:276:11
      %5894 : bool = aten::ne(%5893, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5894) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5895 : bool = prim::GetAttr[name="training"](%5892)
       = prim::If(%5895) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5896 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5892)
          %5897 : Tensor = aten::add(%5896, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5892, %5897)
          -> ()
        block1():
          -> ()
      %5898 : bool = prim::GetAttr[name="training"](%5892)
      %5899 : Tensor = prim::GetAttr[name="running_mean"](%5892)
      %5900 : Tensor = prim::GetAttr[name="running_var"](%5892)
      %5901 : Tensor = prim::GetAttr[name="weight"](%5892)
      %5902 : Tensor = prim::GetAttr[name="bias"](%5892)
       = prim::If(%5898) # torch/nn/functional.py:2011:4
        block0():
          %5903 : int[] = aten::size(%concated_features.7) # torch/nn/functional.py:2012:27
          %size_prods.48 : int = aten::__getitem__(%5903, %24) # torch/nn/functional.py:1991:17
          %5905 : int = aten::len(%5903) # torch/nn/functional.py:1992:19
          %5906 : int = aten::sub(%5905, %26) # torch/nn/functional.py:1992:19
          %size_prods.49 : int = prim::Loop(%5906, %25, %size_prods.48) # torch/nn/functional.py:1992:4
            block0(%i.13 : int, %size_prods.50 : int):
              %5910 : int = aten::add(%i.13, %26) # torch/nn/functional.py:1993:27
              %5911 : int = aten::__getitem__(%5903, %5910) # torch/nn/functional.py:1993:22
              %size_prods.51 : int = aten::mul(%size_prods.50, %5911) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.51)
          %5913 : bool = aten::eq(%size_prods.49, %27) # torch/nn/functional.py:1994:7
           = prim::If(%5913) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %5914 : Tensor = aten::batch_norm(%concated_features.7, %5901, %5902, %5899, %5900, %5898, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.13 : Tensor = aten::relu_(%5914) # torch/nn/functional.py:1117:17
      %5916 : Tensor = prim::GetAttr[name="weight"](%5891)
      %5917 : Tensor? = prim::GetAttr[name="bias"](%5891)
      %5918 : int[] = prim::ListConstruct(%27, %27)
      %5919 : int[] = prim::ListConstruct(%24, %24)
      %5920 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.13 : Tensor = aten::conv2d(%result.13, %5916, %5917, %5918, %5919, %5920, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.13)
  %5922 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5374)
  %5923 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5374)
  %5924 : int = aten::dim(%bottleneck_output.12) # torch/nn/modules/batchnorm.py:276:11
  %5925 : bool = aten::ne(%5924, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%5925) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %5926 : bool = prim::GetAttr[name="training"](%5923)
   = prim::If(%5926) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %5927 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5923)
      %5928 : Tensor = aten::add(%5927, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%5923, %5928)
      -> ()
    block1():
      -> ()
  %5929 : bool = prim::GetAttr[name="training"](%5923)
  %5930 : Tensor = prim::GetAttr[name="running_mean"](%5923)
  %5931 : Tensor = prim::GetAttr[name="running_var"](%5923)
  %5932 : Tensor = prim::GetAttr[name="weight"](%5923)
  %5933 : Tensor = prim::GetAttr[name="bias"](%5923)
   = prim::If(%5929) # torch/nn/functional.py:2011:4
    block0():
      %5934 : int[] = aten::size(%bottleneck_output.12) # torch/nn/functional.py:2012:27
      %size_prods.52 : int = aten::__getitem__(%5934, %24) # torch/nn/functional.py:1991:17
      %5936 : int = aten::len(%5934) # torch/nn/functional.py:1992:19
      %5937 : int = aten::sub(%5936, %26) # torch/nn/functional.py:1992:19
      %size_prods.53 : int = prim::Loop(%5937, %25, %size_prods.52) # torch/nn/functional.py:1992:4
        block0(%i.14 : int, %size_prods.54 : int):
          %5941 : int = aten::add(%i.14, %26) # torch/nn/functional.py:1993:27
          %5942 : int = aten::__getitem__(%5934, %5941) # torch/nn/functional.py:1993:22
          %size_prods.55 : int = aten::mul(%size_prods.54, %5942) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.55)
      %5944 : bool = aten::eq(%size_prods.53, %27) # torch/nn/functional.py:1994:7
       = prim::If(%5944) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %5945 : Tensor = aten::batch_norm(%bottleneck_output.12, %5932, %5933, %5930, %5931, %5929, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.14 : Tensor = aten::relu_(%5945) # torch/nn/functional.py:1117:17
  %5947 : Tensor = prim::GetAttr[name="weight"](%5922)
  %5948 : Tensor? = prim::GetAttr[name="bias"](%5922)
  %5949 : int[] = prim::ListConstruct(%27, %27)
  %5950 : int[] = prim::ListConstruct(%27, %27)
  %5951 : int[] = prim::ListConstruct(%27, %27)
  %new_features.14 : Tensor = aten::conv2d(%result.14, %5947, %5948, %5949, %5950, %5951, %27) # torch/nn/modules/conv.py:415:15
  %5953 : float = prim::GetAttr[name="drop_rate"](%5374)
  %5954 : bool = aten::gt(%5953, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.11 : Tensor = prim::If(%5954) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %5956 : float = prim::GetAttr[name="drop_rate"](%5374)
      %5957 : bool = prim::GetAttr[name="training"](%5374)
      %5958 : bool = aten::lt(%5956, %16) # torch/nn/functional.py:968:7
      %5959 : bool = prim::If(%5958) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %5960 : bool = aten::gt(%5956, %17) # torch/nn/functional.py:968:17
          -> (%5960)
       = prim::If(%5959) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %5961 : Tensor = aten::dropout(%new_features.14, %5956, %5957) # torch/nn/functional.py:973:17
      -> (%5961)
    block1():
      -> (%new_features.14)
  %5962 : Tensor[] = aten::append(%features.1, %new_features.11) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %5963 : Tensor = prim::Uninitialized()
  %5964 : bool = prim::GetAttr[name="memory_efficient"](%5375)
  %5965 : bool = prim::If(%5964) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %5966 : bool = prim::Uninitialized()
      %5967 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %5968 : bool = aten::gt(%5967, %24)
      %5969 : bool, %5970 : bool, %5971 : int = prim::Loop(%18, %5968, %19, %5966, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%5972 : int, %5973 : bool, %5974 : bool, %5975 : int):
          %tensor.8 : Tensor = aten::__getitem__(%features.1, %5975) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %5977 : bool = prim::requires_grad(%tensor.8)
          %5978 : bool, %5979 : bool = prim::If(%5977) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %5966)
          %5980 : int = aten::add(%5975, %27)
          %5981 : bool = aten::lt(%5980, %5967)
          %5982 : bool = aten::__and__(%5981, %5978)
          -> (%5982, %5977, %5979, %5980)
      %5983 : bool = prim::If(%5969)
        block0():
          -> (%5970)
        block1():
          -> (%19)
      -> (%5983)
    block1():
      -> (%19)
  %bottleneck_output.14 : Tensor = prim::If(%5965) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%5963)
    block1():
      %concated_features.8 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %5986 : __torch__.torch.nn.modules.conv.___torch_mangle_238.Conv2d = prim::GetAttr[name="conv1"](%5375)
      %5987 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_237.BatchNorm2d = prim::GetAttr[name="norm1"](%5375)
      %5988 : int = aten::dim(%concated_features.8) # torch/nn/modules/batchnorm.py:276:11
      %5989 : bool = aten::ne(%5988, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%5989) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %5990 : bool = prim::GetAttr[name="training"](%5987)
       = prim::If(%5990) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %5991 : Tensor = prim::GetAttr[name="num_batches_tracked"](%5987)
          %5992 : Tensor = aten::add(%5991, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%5987, %5992)
          -> ()
        block1():
          -> ()
      %5993 : bool = prim::GetAttr[name="training"](%5987)
      %5994 : Tensor = prim::GetAttr[name="running_mean"](%5987)
      %5995 : Tensor = prim::GetAttr[name="running_var"](%5987)
      %5996 : Tensor = prim::GetAttr[name="weight"](%5987)
      %5997 : Tensor = prim::GetAttr[name="bias"](%5987)
       = prim::If(%5993) # torch/nn/functional.py:2011:4
        block0():
          %5998 : int[] = aten::size(%concated_features.8) # torch/nn/functional.py:2012:27
          %size_prods.56 : int = aten::__getitem__(%5998, %24) # torch/nn/functional.py:1991:17
          %6000 : int = aten::len(%5998) # torch/nn/functional.py:1992:19
          %6001 : int = aten::sub(%6000, %26) # torch/nn/functional.py:1992:19
          %size_prods.57 : int = prim::Loop(%6001, %25, %size_prods.56) # torch/nn/functional.py:1992:4
            block0(%i.15 : int, %size_prods.58 : int):
              %6005 : int = aten::add(%i.15, %26) # torch/nn/functional.py:1993:27
              %6006 : int = aten::__getitem__(%5998, %6005) # torch/nn/functional.py:1993:22
              %size_prods.59 : int = aten::mul(%size_prods.58, %6006) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.59)
          %6008 : bool = aten::eq(%size_prods.57, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6008) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6009 : Tensor = aten::batch_norm(%concated_features.8, %5996, %5997, %5994, %5995, %5993, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.15 : Tensor = aten::relu_(%6009) # torch/nn/functional.py:1117:17
      %6011 : Tensor = prim::GetAttr[name="weight"](%5986)
      %6012 : Tensor? = prim::GetAttr[name="bias"](%5986)
      %6013 : int[] = prim::ListConstruct(%27, %27)
      %6014 : int[] = prim::ListConstruct(%24, %24)
      %6015 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.15 : Tensor = aten::conv2d(%result.15, %6011, %6012, %6013, %6014, %6015, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.15)
  %6017 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5375)
  %6018 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5375)
  %6019 : int = aten::dim(%bottleneck_output.14) # torch/nn/modules/batchnorm.py:276:11
  %6020 : bool = aten::ne(%6019, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6020) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6021 : bool = prim::GetAttr[name="training"](%6018)
   = prim::If(%6021) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6022 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6018)
      %6023 : Tensor = aten::add(%6022, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6018, %6023)
      -> ()
    block1():
      -> ()
  %6024 : bool = prim::GetAttr[name="training"](%6018)
  %6025 : Tensor = prim::GetAttr[name="running_mean"](%6018)
  %6026 : Tensor = prim::GetAttr[name="running_var"](%6018)
  %6027 : Tensor = prim::GetAttr[name="weight"](%6018)
  %6028 : Tensor = prim::GetAttr[name="bias"](%6018)
   = prim::If(%6024) # torch/nn/functional.py:2011:4
    block0():
      %6029 : int[] = aten::size(%bottleneck_output.14) # torch/nn/functional.py:2012:27
      %size_prods.60 : int = aten::__getitem__(%6029, %24) # torch/nn/functional.py:1991:17
      %6031 : int = aten::len(%6029) # torch/nn/functional.py:1992:19
      %6032 : int = aten::sub(%6031, %26) # torch/nn/functional.py:1992:19
      %size_prods.61 : int = prim::Loop(%6032, %25, %size_prods.60) # torch/nn/functional.py:1992:4
        block0(%i.16 : int, %size_prods.62 : int):
          %6036 : int = aten::add(%i.16, %26) # torch/nn/functional.py:1993:27
          %6037 : int = aten::__getitem__(%6029, %6036) # torch/nn/functional.py:1993:22
          %size_prods.63 : int = aten::mul(%size_prods.62, %6037) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.63)
      %6039 : bool = aten::eq(%size_prods.61, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6039) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6040 : Tensor = aten::batch_norm(%bottleneck_output.14, %6027, %6028, %6025, %6026, %6024, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.16 : Tensor = aten::relu_(%6040) # torch/nn/functional.py:1117:17
  %6042 : Tensor = prim::GetAttr[name="weight"](%6017)
  %6043 : Tensor? = prim::GetAttr[name="bias"](%6017)
  %6044 : int[] = prim::ListConstruct(%27, %27)
  %6045 : int[] = prim::ListConstruct(%27, %27)
  %6046 : int[] = prim::ListConstruct(%27, %27)
  %new_features.16 : Tensor = aten::conv2d(%result.16, %6042, %6043, %6044, %6045, %6046, %27) # torch/nn/modules/conv.py:415:15
  %6048 : float = prim::GetAttr[name="drop_rate"](%5375)
  %6049 : bool = aten::gt(%6048, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.13 : Tensor = prim::If(%6049) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6051 : float = prim::GetAttr[name="drop_rate"](%5375)
      %6052 : bool = prim::GetAttr[name="training"](%5375)
      %6053 : bool = aten::lt(%6051, %16) # torch/nn/functional.py:968:7
      %6054 : bool = prim::If(%6053) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6055 : bool = aten::gt(%6051, %17) # torch/nn/functional.py:968:17
          -> (%6055)
       = prim::If(%6054) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6056 : Tensor = aten::dropout(%new_features.16, %6051, %6052) # torch/nn/functional.py:973:17
      -> (%6056)
    block1():
      -> (%new_features.16)
  %6057 : Tensor[] = aten::append(%features.1, %new_features.13) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6058 : Tensor = prim::Uninitialized()
  %6059 : bool = prim::GetAttr[name="memory_efficient"](%5376)
  %6060 : bool = prim::If(%6059) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6061 : bool = prim::Uninitialized()
      %6062 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6063 : bool = aten::gt(%6062, %24)
      %6064 : bool, %6065 : bool, %6066 : int = prim::Loop(%18, %6063, %19, %6061, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6067 : int, %6068 : bool, %6069 : bool, %6070 : int):
          %tensor.9 : Tensor = aten::__getitem__(%features.1, %6070) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6072 : bool = prim::requires_grad(%tensor.9)
          %6073 : bool, %6074 : bool = prim::If(%6072) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6061)
          %6075 : int = aten::add(%6070, %27)
          %6076 : bool = aten::lt(%6075, %6062)
          %6077 : bool = aten::__and__(%6076, %6073)
          -> (%6077, %6072, %6074, %6075)
      %6078 : bool = prim::If(%6064)
        block0():
          -> (%6065)
        block1():
          -> (%19)
      -> (%6078)
    block1():
      -> (%19)
  %bottleneck_output.16 : Tensor = prim::If(%6060) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6058)
    block1():
      %concated_features.9 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6081 : __torch__.torch.nn.modules.conv.___torch_mangle_241.Conv2d = prim::GetAttr[name="conv1"](%5376)
      %6082 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_240.BatchNorm2d = prim::GetAttr[name="norm1"](%5376)
      %6083 : int = aten::dim(%concated_features.9) # torch/nn/modules/batchnorm.py:276:11
      %6084 : bool = aten::ne(%6083, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6084) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6085 : bool = prim::GetAttr[name="training"](%6082)
       = prim::If(%6085) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6086 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6082)
          %6087 : Tensor = aten::add(%6086, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6082, %6087)
          -> ()
        block1():
          -> ()
      %6088 : bool = prim::GetAttr[name="training"](%6082)
      %6089 : Tensor = prim::GetAttr[name="running_mean"](%6082)
      %6090 : Tensor = prim::GetAttr[name="running_var"](%6082)
      %6091 : Tensor = prim::GetAttr[name="weight"](%6082)
      %6092 : Tensor = prim::GetAttr[name="bias"](%6082)
       = prim::If(%6088) # torch/nn/functional.py:2011:4
        block0():
          %6093 : int[] = aten::size(%concated_features.9) # torch/nn/functional.py:2012:27
          %size_prods.64 : int = aten::__getitem__(%6093, %24) # torch/nn/functional.py:1991:17
          %6095 : int = aten::len(%6093) # torch/nn/functional.py:1992:19
          %6096 : int = aten::sub(%6095, %26) # torch/nn/functional.py:1992:19
          %size_prods.65 : int = prim::Loop(%6096, %25, %size_prods.64) # torch/nn/functional.py:1992:4
            block0(%i.17 : int, %size_prods.66 : int):
              %6100 : int = aten::add(%i.17, %26) # torch/nn/functional.py:1993:27
              %6101 : int = aten::__getitem__(%6093, %6100) # torch/nn/functional.py:1993:22
              %size_prods.67 : int = aten::mul(%size_prods.66, %6101) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.67)
          %6103 : bool = aten::eq(%size_prods.65, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6103) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6104 : Tensor = aten::batch_norm(%concated_features.9, %6091, %6092, %6089, %6090, %6088, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.17 : Tensor = aten::relu_(%6104) # torch/nn/functional.py:1117:17
      %6106 : Tensor = prim::GetAttr[name="weight"](%6081)
      %6107 : Tensor? = prim::GetAttr[name="bias"](%6081)
      %6108 : int[] = prim::ListConstruct(%27, %27)
      %6109 : int[] = prim::ListConstruct(%24, %24)
      %6110 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.17 : Tensor = aten::conv2d(%result.17, %6106, %6107, %6108, %6109, %6110, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.17)
  %6112 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5376)
  %6113 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5376)
  %6114 : int = aten::dim(%bottleneck_output.16) # torch/nn/modules/batchnorm.py:276:11
  %6115 : bool = aten::ne(%6114, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6115) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6116 : bool = prim::GetAttr[name="training"](%6113)
   = prim::If(%6116) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6117 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6113)
      %6118 : Tensor = aten::add(%6117, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6113, %6118)
      -> ()
    block1():
      -> ()
  %6119 : bool = prim::GetAttr[name="training"](%6113)
  %6120 : Tensor = prim::GetAttr[name="running_mean"](%6113)
  %6121 : Tensor = prim::GetAttr[name="running_var"](%6113)
  %6122 : Tensor = prim::GetAttr[name="weight"](%6113)
  %6123 : Tensor = prim::GetAttr[name="bias"](%6113)
   = prim::If(%6119) # torch/nn/functional.py:2011:4
    block0():
      %6124 : int[] = aten::size(%bottleneck_output.16) # torch/nn/functional.py:2012:27
      %size_prods.68 : int = aten::__getitem__(%6124, %24) # torch/nn/functional.py:1991:17
      %6126 : int = aten::len(%6124) # torch/nn/functional.py:1992:19
      %6127 : int = aten::sub(%6126, %26) # torch/nn/functional.py:1992:19
      %size_prods.69 : int = prim::Loop(%6127, %25, %size_prods.68) # torch/nn/functional.py:1992:4
        block0(%i.18 : int, %size_prods.70 : int):
          %6131 : int = aten::add(%i.18, %26) # torch/nn/functional.py:1993:27
          %6132 : int = aten::__getitem__(%6124, %6131) # torch/nn/functional.py:1993:22
          %size_prods.71 : int = aten::mul(%size_prods.70, %6132) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.71)
      %6134 : bool = aten::eq(%size_prods.69, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6134) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6135 : Tensor = aten::batch_norm(%bottleneck_output.16, %6122, %6123, %6120, %6121, %6119, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.18 : Tensor = aten::relu_(%6135) # torch/nn/functional.py:1117:17
  %6137 : Tensor = prim::GetAttr[name="weight"](%6112)
  %6138 : Tensor? = prim::GetAttr[name="bias"](%6112)
  %6139 : int[] = prim::ListConstruct(%27, %27)
  %6140 : int[] = prim::ListConstruct(%27, %27)
  %6141 : int[] = prim::ListConstruct(%27, %27)
  %new_features.18 : Tensor = aten::conv2d(%result.18, %6137, %6138, %6139, %6140, %6141, %27) # torch/nn/modules/conv.py:415:15
  %6143 : float = prim::GetAttr[name="drop_rate"](%5376)
  %6144 : bool = aten::gt(%6143, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.15 : Tensor = prim::If(%6144) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6146 : float = prim::GetAttr[name="drop_rate"](%5376)
      %6147 : bool = prim::GetAttr[name="training"](%5376)
      %6148 : bool = aten::lt(%6146, %16) # torch/nn/functional.py:968:7
      %6149 : bool = prim::If(%6148) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6150 : bool = aten::gt(%6146, %17) # torch/nn/functional.py:968:17
          -> (%6150)
       = prim::If(%6149) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6151 : Tensor = aten::dropout(%new_features.18, %6146, %6147) # torch/nn/functional.py:973:17
      -> (%6151)
    block1():
      -> (%new_features.18)
  %6152 : Tensor[] = aten::append(%features.1, %new_features.15) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6153 : Tensor = prim::Uninitialized()
  %6154 : bool = prim::GetAttr[name="memory_efficient"](%5377)
  %6155 : bool = prim::If(%6154) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6156 : bool = prim::Uninitialized()
      %6157 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6158 : bool = aten::gt(%6157, %24)
      %6159 : bool, %6160 : bool, %6161 : int = prim::Loop(%18, %6158, %19, %6156, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6162 : int, %6163 : bool, %6164 : bool, %6165 : int):
          %tensor.10 : Tensor = aten::__getitem__(%features.1, %6165) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6167 : bool = prim::requires_grad(%tensor.10)
          %6168 : bool, %6169 : bool = prim::If(%6167) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6156)
          %6170 : int = aten::add(%6165, %27)
          %6171 : bool = aten::lt(%6170, %6157)
          %6172 : bool = aten::__and__(%6171, %6168)
          -> (%6172, %6167, %6169, %6170)
      %6173 : bool = prim::If(%6159)
        block0():
          -> (%6160)
        block1():
          -> (%19)
      -> (%6173)
    block1():
      -> (%19)
  %bottleneck_output.18 : Tensor = prim::If(%6155) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6153)
    block1():
      %concated_features.10 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6176 : __torch__.torch.nn.modules.conv.___torch_mangle_244.Conv2d = prim::GetAttr[name="conv1"](%5377)
      %6177 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_243.BatchNorm2d = prim::GetAttr[name="norm1"](%5377)
      %6178 : int = aten::dim(%concated_features.10) # torch/nn/modules/batchnorm.py:276:11
      %6179 : bool = aten::ne(%6178, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6179) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6180 : bool = prim::GetAttr[name="training"](%6177)
       = prim::If(%6180) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6181 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6177)
          %6182 : Tensor = aten::add(%6181, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6177, %6182)
          -> ()
        block1():
          -> ()
      %6183 : bool = prim::GetAttr[name="training"](%6177)
      %6184 : Tensor = prim::GetAttr[name="running_mean"](%6177)
      %6185 : Tensor = prim::GetAttr[name="running_var"](%6177)
      %6186 : Tensor = prim::GetAttr[name="weight"](%6177)
      %6187 : Tensor = prim::GetAttr[name="bias"](%6177)
       = prim::If(%6183) # torch/nn/functional.py:2011:4
        block0():
          %6188 : int[] = aten::size(%concated_features.10) # torch/nn/functional.py:2012:27
          %size_prods.72 : int = aten::__getitem__(%6188, %24) # torch/nn/functional.py:1991:17
          %6190 : int = aten::len(%6188) # torch/nn/functional.py:1992:19
          %6191 : int = aten::sub(%6190, %26) # torch/nn/functional.py:1992:19
          %size_prods.73 : int = prim::Loop(%6191, %25, %size_prods.72) # torch/nn/functional.py:1992:4
            block0(%i.19 : int, %size_prods.74 : int):
              %6195 : int = aten::add(%i.19, %26) # torch/nn/functional.py:1993:27
              %6196 : int = aten::__getitem__(%6188, %6195) # torch/nn/functional.py:1993:22
              %size_prods.75 : int = aten::mul(%size_prods.74, %6196) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.75)
          %6198 : bool = aten::eq(%size_prods.73, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6198) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6199 : Tensor = aten::batch_norm(%concated_features.10, %6186, %6187, %6184, %6185, %6183, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.19 : Tensor = aten::relu_(%6199) # torch/nn/functional.py:1117:17
      %6201 : Tensor = prim::GetAttr[name="weight"](%6176)
      %6202 : Tensor? = prim::GetAttr[name="bias"](%6176)
      %6203 : int[] = prim::ListConstruct(%27, %27)
      %6204 : int[] = prim::ListConstruct(%24, %24)
      %6205 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.19 : Tensor = aten::conv2d(%result.19, %6201, %6202, %6203, %6204, %6205, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.19)
  %6207 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5377)
  %6208 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5377)
  %6209 : int = aten::dim(%bottleneck_output.18) # torch/nn/modules/batchnorm.py:276:11
  %6210 : bool = aten::ne(%6209, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6210) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6211 : bool = prim::GetAttr[name="training"](%6208)
   = prim::If(%6211) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6212 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6208)
      %6213 : Tensor = aten::add(%6212, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6208, %6213)
      -> ()
    block1():
      -> ()
  %6214 : bool = prim::GetAttr[name="training"](%6208)
  %6215 : Tensor = prim::GetAttr[name="running_mean"](%6208)
  %6216 : Tensor = prim::GetAttr[name="running_var"](%6208)
  %6217 : Tensor = prim::GetAttr[name="weight"](%6208)
  %6218 : Tensor = prim::GetAttr[name="bias"](%6208)
   = prim::If(%6214) # torch/nn/functional.py:2011:4
    block0():
      %6219 : int[] = aten::size(%bottleneck_output.18) # torch/nn/functional.py:2012:27
      %size_prods.76 : int = aten::__getitem__(%6219, %24) # torch/nn/functional.py:1991:17
      %6221 : int = aten::len(%6219) # torch/nn/functional.py:1992:19
      %6222 : int = aten::sub(%6221, %26) # torch/nn/functional.py:1992:19
      %size_prods.77 : int = prim::Loop(%6222, %25, %size_prods.76) # torch/nn/functional.py:1992:4
        block0(%i.20 : int, %size_prods.78 : int):
          %6226 : int = aten::add(%i.20, %26) # torch/nn/functional.py:1993:27
          %6227 : int = aten::__getitem__(%6219, %6226) # torch/nn/functional.py:1993:22
          %size_prods.79 : int = aten::mul(%size_prods.78, %6227) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.79)
      %6229 : bool = aten::eq(%size_prods.77, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6229) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6230 : Tensor = aten::batch_norm(%bottleneck_output.18, %6217, %6218, %6215, %6216, %6214, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.20 : Tensor = aten::relu_(%6230) # torch/nn/functional.py:1117:17
  %6232 : Tensor = prim::GetAttr[name="weight"](%6207)
  %6233 : Tensor? = prim::GetAttr[name="bias"](%6207)
  %6234 : int[] = prim::ListConstruct(%27, %27)
  %6235 : int[] = prim::ListConstruct(%27, %27)
  %6236 : int[] = prim::ListConstruct(%27, %27)
  %new_features.20 : Tensor = aten::conv2d(%result.20, %6232, %6233, %6234, %6235, %6236, %27) # torch/nn/modules/conv.py:415:15
  %6238 : float = prim::GetAttr[name="drop_rate"](%5377)
  %6239 : bool = aten::gt(%6238, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.17 : Tensor = prim::If(%6239) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6241 : float = prim::GetAttr[name="drop_rate"](%5377)
      %6242 : bool = prim::GetAttr[name="training"](%5377)
      %6243 : bool = aten::lt(%6241, %16) # torch/nn/functional.py:968:7
      %6244 : bool = prim::If(%6243) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6245 : bool = aten::gt(%6241, %17) # torch/nn/functional.py:968:17
          -> (%6245)
       = prim::If(%6244) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6246 : Tensor = aten::dropout(%new_features.20, %6241, %6242) # torch/nn/functional.py:973:17
      -> (%6246)
    block1():
      -> (%new_features.20)
  %6247 : Tensor[] = aten::append(%features.1, %new_features.17) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6248 : Tensor = prim::Uninitialized()
  %6249 : bool = prim::GetAttr[name="memory_efficient"](%5378)
  %6250 : bool = prim::If(%6249) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6251 : bool = prim::Uninitialized()
      %6252 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6253 : bool = aten::gt(%6252, %24)
      %6254 : bool, %6255 : bool, %6256 : int = prim::Loop(%18, %6253, %19, %6251, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6257 : int, %6258 : bool, %6259 : bool, %6260 : int):
          %tensor.11 : Tensor = aten::__getitem__(%features.1, %6260) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6262 : bool = prim::requires_grad(%tensor.11)
          %6263 : bool, %6264 : bool = prim::If(%6262) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6251)
          %6265 : int = aten::add(%6260, %27)
          %6266 : bool = aten::lt(%6265, %6252)
          %6267 : bool = aten::__and__(%6266, %6263)
          -> (%6267, %6262, %6264, %6265)
      %6268 : bool = prim::If(%6254)
        block0():
          -> (%6255)
        block1():
          -> (%19)
      -> (%6268)
    block1():
      -> (%19)
  %bottleneck_output.20 : Tensor = prim::If(%6250) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6248)
    block1():
      %concated_features.11 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6271 : __torch__.torch.nn.modules.conv.___torch_mangle_247.Conv2d = prim::GetAttr[name="conv1"](%5378)
      %6272 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_246.BatchNorm2d = prim::GetAttr[name="norm1"](%5378)
      %6273 : int = aten::dim(%concated_features.11) # torch/nn/modules/batchnorm.py:276:11
      %6274 : bool = aten::ne(%6273, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6274) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6275 : bool = prim::GetAttr[name="training"](%6272)
       = prim::If(%6275) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6276 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6272)
          %6277 : Tensor = aten::add(%6276, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6272, %6277)
          -> ()
        block1():
          -> ()
      %6278 : bool = prim::GetAttr[name="training"](%6272)
      %6279 : Tensor = prim::GetAttr[name="running_mean"](%6272)
      %6280 : Tensor = prim::GetAttr[name="running_var"](%6272)
      %6281 : Tensor = prim::GetAttr[name="weight"](%6272)
      %6282 : Tensor = prim::GetAttr[name="bias"](%6272)
       = prim::If(%6278) # torch/nn/functional.py:2011:4
        block0():
          %6283 : int[] = aten::size(%concated_features.11) # torch/nn/functional.py:2012:27
          %size_prods.80 : int = aten::__getitem__(%6283, %24) # torch/nn/functional.py:1991:17
          %6285 : int = aten::len(%6283) # torch/nn/functional.py:1992:19
          %6286 : int = aten::sub(%6285, %26) # torch/nn/functional.py:1992:19
          %size_prods.81 : int = prim::Loop(%6286, %25, %size_prods.80) # torch/nn/functional.py:1992:4
            block0(%i.21 : int, %size_prods.82 : int):
              %6290 : int = aten::add(%i.21, %26) # torch/nn/functional.py:1993:27
              %6291 : int = aten::__getitem__(%6283, %6290) # torch/nn/functional.py:1993:22
              %size_prods.83 : int = aten::mul(%size_prods.82, %6291) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.83)
          %6293 : bool = aten::eq(%size_prods.81, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6293) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6294 : Tensor = aten::batch_norm(%concated_features.11, %6281, %6282, %6279, %6280, %6278, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.21 : Tensor = aten::relu_(%6294) # torch/nn/functional.py:1117:17
      %6296 : Tensor = prim::GetAttr[name="weight"](%6271)
      %6297 : Tensor? = prim::GetAttr[name="bias"](%6271)
      %6298 : int[] = prim::ListConstruct(%27, %27)
      %6299 : int[] = prim::ListConstruct(%24, %24)
      %6300 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.21 : Tensor = aten::conv2d(%result.21, %6296, %6297, %6298, %6299, %6300, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.21)
  %6302 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5378)
  %6303 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5378)
  %6304 : int = aten::dim(%bottleneck_output.20) # torch/nn/modules/batchnorm.py:276:11
  %6305 : bool = aten::ne(%6304, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6305) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6306 : bool = prim::GetAttr[name="training"](%6303)
   = prim::If(%6306) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6307 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6303)
      %6308 : Tensor = aten::add(%6307, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6303, %6308)
      -> ()
    block1():
      -> ()
  %6309 : bool = prim::GetAttr[name="training"](%6303)
  %6310 : Tensor = prim::GetAttr[name="running_mean"](%6303)
  %6311 : Tensor = prim::GetAttr[name="running_var"](%6303)
  %6312 : Tensor = prim::GetAttr[name="weight"](%6303)
  %6313 : Tensor = prim::GetAttr[name="bias"](%6303)
   = prim::If(%6309) # torch/nn/functional.py:2011:4
    block0():
      %6314 : int[] = aten::size(%bottleneck_output.20) # torch/nn/functional.py:2012:27
      %size_prods.84 : int = aten::__getitem__(%6314, %24) # torch/nn/functional.py:1991:17
      %6316 : int = aten::len(%6314) # torch/nn/functional.py:1992:19
      %6317 : int = aten::sub(%6316, %26) # torch/nn/functional.py:1992:19
      %size_prods.85 : int = prim::Loop(%6317, %25, %size_prods.84) # torch/nn/functional.py:1992:4
        block0(%i.22 : int, %size_prods.86 : int):
          %6321 : int = aten::add(%i.22, %26) # torch/nn/functional.py:1993:27
          %6322 : int = aten::__getitem__(%6314, %6321) # torch/nn/functional.py:1993:22
          %size_prods.87 : int = aten::mul(%size_prods.86, %6322) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.87)
      %6324 : bool = aten::eq(%size_prods.85, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6324) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6325 : Tensor = aten::batch_norm(%bottleneck_output.20, %6312, %6313, %6310, %6311, %6309, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.22 : Tensor = aten::relu_(%6325) # torch/nn/functional.py:1117:17
  %6327 : Tensor = prim::GetAttr[name="weight"](%6302)
  %6328 : Tensor? = prim::GetAttr[name="bias"](%6302)
  %6329 : int[] = prim::ListConstruct(%27, %27)
  %6330 : int[] = prim::ListConstruct(%27, %27)
  %6331 : int[] = prim::ListConstruct(%27, %27)
  %new_features.22 : Tensor = aten::conv2d(%result.22, %6327, %6328, %6329, %6330, %6331, %27) # torch/nn/modules/conv.py:415:15
  %6333 : float = prim::GetAttr[name="drop_rate"](%5378)
  %6334 : bool = aten::gt(%6333, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.19 : Tensor = prim::If(%6334) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6336 : float = prim::GetAttr[name="drop_rate"](%5378)
      %6337 : bool = prim::GetAttr[name="training"](%5378)
      %6338 : bool = aten::lt(%6336, %16) # torch/nn/functional.py:968:7
      %6339 : bool = prim::If(%6338) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6340 : bool = aten::gt(%6336, %17) # torch/nn/functional.py:968:17
          -> (%6340)
       = prim::If(%6339) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6341 : Tensor = aten::dropout(%new_features.22, %6336, %6337) # torch/nn/functional.py:973:17
      -> (%6341)
    block1():
      -> (%new_features.22)
  %6342 : Tensor[] = aten::append(%features.1, %new_features.19) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6343 : Tensor = prim::Uninitialized()
  %6344 : bool = prim::GetAttr[name="memory_efficient"](%5379)
  %6345 : bool = prim::If(%6344) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6346 : bool = prim::Uninitialized()
      %6347 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6348 : bool = aten::gt(%6347, %24)
      %6349 : bool, %6350 : bool, %6351 : int = prim::Loop(%18, %6348, %19, %6346, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6352 : int, %6353 : bool, %6354 : bool, %6355 : int):
          %tensor.12 : Tensor = aten::__getitem__(%features.1, %6355) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6357 : bool = prim::requires_grad(%tensor.12)
          %6358 : bool, %6359 : bool = prim::If(%6357) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6346)
          %6360 : int = aten::add(%6355, %27)
          %6361 : bool = aten::lt(%6360, %6347)
          %6362 : bool = aten::__and__(%6361, %6358)
          -> (%6362, %6357, %6359, %6360)
      %6363 : bool = prim::If(%6349)
        block0():
          -> (%6350)
        block1():
          -> (%19)
      -> (%6363)
    block1():
      -> (%19)
  %bottleneck_output.22 : Tensor = prim::If(%6345) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6343)
    block1():
      %concated_features.12 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6366 : __torch__.torch.nn.modules.conv.___torch_mangle_250.Conv2d = prim::GetAttr[name="conv1"](%5379)
      %6367 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_249.BatchNorm2d = prim::GetAttr[name="norm1"](%5379)
      %6368 : int = aten::dim(%concated_features.12) # torch/nn/modules/batchnorm.py:276:11
      %6369 : bool = aten::ne(%6368, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6369) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6370 : bool = prim::GetAttr[name="training"](%6367)
       = prim::If(%6370) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6371 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6367)
          %6372 : Tensor = aten::add(%6371, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6367, %6372)
          -> ()
        block1():
          -> ()
      %6373 : bool = prim::GetAttr[name="training"](%6367)
      %6374 : Tensor = prim::GetAttr[name="running_mean"](%6367)
      %6375 : Tensor = prim::GetAttr[name="running_var"](%6367)
      %6376 : Tensor = prim::GetAttr[name="weight"](%6367)
      %6377 : Tensor = prim::GetAttr[name="bias"](%6367)
       = prim::If(%6373) # torch/nn/functional.py:2011:4
        block0():
          %6378 : int[] = aten::size(%concated_features.12) # torch/nn/functional.py:2012:27
          %size_prods.88 : int = aten::__getitem__(%6378, %24) # torch/nn/functional.py:1991:17
          %6380 : int = aten::len(%6378) # torch/nn/functional.py:1992:19
          %6381 : int = aten::sub(%6380, %26) # torch/nn/functional.py:1992:19
          %size_prods.89 : int = prim::Loop(%6381, %25, %size_prods.88) # torch/nn/functional.py:1992:4
            block0(%i.23 : int, %size_prods.90 : int):
              %6385 : int = aten::add(%i.23, %26) # torch/nn/functional.py:1993:27
              %6386 : int = aten::__getitem__(%6378, %6385) # torch/nn/functional.py:1993:22
              %size_prods.91 : int = aten::mul(%size_prods.90, %6386) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.91)
          %6388 : bool = aten::eq(%size_prods.89, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6388) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6389 : Tensor = aten::batch_norm(%concated_features.12, %6376, %6377, %6374, %6375, %6373, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.23 : Tensor = aten::relu_(%6389) # torch/nn/functional.py:1117:17
      %6391 : Tensor = prim::GetAttr[name="weight"](%6366)
      %6392 : Tensor? = prim::GetAttr[name="bias"](%6366)
      %6393 : int[] = prim::ListConstruct(%27, %27)
      %6394 : int[] = prim::ListConstruct(%24, %24)
      %6395 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.23 : Tensor = aten::conv2d(%result.23, %6391, %6392, %6393, %6394, %6395, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.23)
  %6397 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5379)
  %6398 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5379)
  %6399 : int = aten::dim(%bottleneck_output.22) # torch/nn/modules/batchnorm.py:276:11
  %6400 : bool = aten::ne(%6399, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6400) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6401 : bool = prim::GetAttr[name="training"](%6398)
   = prim::If(%6401) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6402 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6398)
      %6403 : Tensor = aten::add(%6402, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6398, %6403)
      -> ()
    block1():
      -> ()
  %6404 : bool = prim::GetAttr[name="training"](%6398)
  %6405 : Tensor = prim::GetAttr[name="running_mean"](%6398)
  %6406 : Tensor = prim::GetAttr[name="running_var"](%6398)
  %6407 : Tensor = prim::GetAttr[name="weight"](%6398)
  %6408 : Tensor = prim::GetAttr[name="bias"](%6398)
   = prim::If(%6404) # torch/nn/functional.py:2011:4
    block0():
      %6409 : int[] = aten::size(%bottleneck_output.22) # torch/nn/functional.py:2012:27
      %size_prods.92 : int = aten::__getitem__(%6409, %24) # torch/nn/functional.py:1991:17
      %6411 : int = aten::len(%6409) # torch/nn/functional.py:1992:19
      %6412 : int = aten::sub(%6411, %26) # torch/nn/functional.py:1992:19
      %size_prods.93 : int = prim::Loop(%6412, %25, %size_prods.92) # torch/nn/functional.py:1992:4
        block0(%i.24 : int, %size_prods.94 : int):
          %6416 : int = aten::add(%i.24, %26) # torch/nn/functional.py:1993:27
          %6417 : int = aten::__getitem__(%6409, %6416) # torch/nn/functional.py:1993:22
          %size_prods.95 : int = aten::mul(%size_prods.94, %6417) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.95)
      %6419 : bool = aten::eq(%size_prods.93, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6419) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6420 : Tensor = aten::batch_norm(%bottleneck_output.22, %6407, %6408, %6405, %6406, %6404, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.24 : Tensor = aten::relu_(%6420) # torch/nn/functional.py:1117:17
  %6422 : Tensor = prim::GetAttr[name="weight"](%6397)
  %6423 : Tensor? = prim::GetAttr[name="bias"](%6397)
  %6424 : int[] = prim::ListConstruct(%27, %27)
  %6425 : int[] = prim::ListConstruct(%27, %27)
  %6426 : int[] = prim::ListConstruct(%27, %27)
  %new_features.24 : Tensor = aten::conv2d(%result.24, %6422, %6423, %6424, %6425, %6426, %27) # torch/nn/modules/conv.py:415:15
  %6428 : float = prim::GetAttr[name="drop_rate"](%5379)
  %6429 : bool = aten::gt(%6428, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.21 : Tensor = prim::If(%6429) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6431 : float = prim::GetAttr[name="drop_rate"](%5379)
      %6432 : bool = prim::GetAttr[name="training"](%5379)
      %6433 : bool = aten::lt(%6431, %16) # torch/nn/functional.py:968:7
      %6434 : bool = prim::If(%6433) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6435 : bool = aten::gt(%6431, %17) # torch/nn/functional.py:968:17
          -> (%6435)
       = prim::If(%6434) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6436 : Tensor = aten::dropout(%new_features.24, %6431, %6432) # torch/nn/functional.py:973:17
      -> (%6436)
    block1():
      -> (%new_features.24)
  %6437 : Tensor[] = aten::append(%features.1, %new_features.21) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6438 : Tensor = prim::Uninitialized()
  %6439 : bool = prim::GetAttr[name="memory_efficient"](%5380)
  %6440 : bool = prim::If(%6439) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6441 : bool = prim::Uninitialized()
      %6442 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6443 : bool = aten::gt(%6442, %24)
      %6444 : bool, %6445 : bool, %6446 : int = prim::Loop(%18, %6443, %19, %6441, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6447 : int, %6448 : bool, %6449 : bool, %6450 : int):
          %tensor.13 : Tensor = aten::__getitem__(%features.1, %6450) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6452 : bool = prim::requires_grad(%tensor.13)
          %6453 : bool, %6454 : bool = prim::If(%6452) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6441)
          %6455 : int = aten::add(%6450, %27)
          %6456 : bool = aten::lt(%6455, %6442)
          %6457 : bool = aten::__and__(%6456, %6453)
          -> (%6457, %6452, %6454, %6455)
      %6458 : bool = prim::If(%6444)
        block0():
          -> (%6445)
        block1():
          -> (%19)
      -> (%6458)
    block1():
      -> (%19)
  %bottleneck_output.24 : Tensor = prim::If(%6440) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6438)
    block1():
      %concated_features.13 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6461 : __torch__.torch.nn.modules.conv.___torch_mangle_253.Conv2d = prim::GetAttr[name="conv1"](%5380)
      %6462 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_252.BatchNorm2d = prim::GetAttr[name="norm1"](%5380)
      %6463 : int = aten::dim(%concated_features.13) # torch/nn/modules/batchnorm.py:276:11
      %6464 : bool = aten::ne(%6463, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6464) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6465 : bool = prim::GetAttr[name="training"](%6462)
       = prim::If(%6465) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6466 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6462)
          %6467 : Tensor = aten::add(%6466, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6462, %6467)
          -> ()
        block1():
          -> ()
      %6468 : bool = prim::GetAttr[name="training"](%6462)
      %6469 : Tensor = prim::GetAttr[name="running_mean"](%6462)
      %6470 : Tensor = prim::GetAttr[name="running_var"](%6462)
      %6471 : Tensor = prim::GetAttr[name="weight"](%6462)
      %6472 : Tensor = prim::GetAttr[name="bias"](%6462)
       = prim::If(%6468) # torch/nn/functional.py:2011:4
        block0():
          %6473 : int[] = aten::size(%concated_features.13) # torch/nn/functional.py:2012:27
          %size_prods.96 : int = aten::__getitem__(%6473, %24) # torch/nn/functional.py:1991:17
          %6475 : int = aten::len(%6473) # torch/nn/functional.py:1992:19
          %6476 : int = aten::sub(%6475, %26) # torch/nn/functional.py:1992:19
          %size_prods.97 : int = prim::Loop(%6476, %25, %size_prods.96) # torch/nn/functional.py:1992:4
            block0(%i.25 : int, %size_prods.98 : int):
              %6480 : int = aten::add(%i.25, %26) # torch/nn/functional.py:1993:27
              %6481 : int = aten::__getitem__(%6473, %6480) # torch/nn/functional.py:1993:22
              %size_prods.99 : int = aten::mul(%size_prods.98, %6481) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.99)
          %6483 : bool = aten::eq(%size_prods.97, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6483) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6484 : Tensor = aten::batch_norm(%concated_features.13, %6471, %6472, %6469, %6470, %6468, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.25 : Tensor = aten::relu_(%6484) # torch/nn/functional.py:1117:17
      %6486 : Tensor = prim::GetAttr[name="weight"](%6461)
      %6487 : Tensor? = prim::GetAttr[name="bias"](%6461)
      %6488 : int[] = prim::ListConstruct(%27, %27)
      %6489 : int[] = prim::ListConstruct(%24, %24)
      %6490 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.25 : Tensor = aten::conv2d(%result.25, %6486, %6487, %6488, %6489, %6490, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.25)
  %6492 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5380)
  %6493 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5380)
  %6494 : int = aten::dim(%bottleneck_output.24) # torch/nn/modules/batchnorm.py:276:11
  %6495 : bool = aten::ne(%6494, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6495) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6496 : bool = prim::GetAttr[name="training"](%6493)
   = prim::If(%6496) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6497 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6493)
      %6498 : Tensor = aten::add(%6497, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6493, %6498)
      -> ()
    block1():
      -> ()
  %6499 : bool = prim::GetAttr[name="training"](%6493)
  %6500 : Tensor = prim::GetAttr[name="running_mean"](%6493)
  %6501 : Tensor = prim::GetAttr[name="running_var"](%6493)
  %6502 : Tensor = prim::GetAttr[name="weight"](%6493)
  %6503 : Tensor = prim::GetAttr[name="bias"](%6493)
   = prim::If(%6499) # torch/nn/functional.py:2011:4
    block0():
      %6504 : int[] = aten::size(%bottleneck_output.24) # torch/nn/functional.py:2012:27
      %size_prods.100 : int = aten::__getitem__(%6504, %24) # torch/nn/functional.py:1991:17
      %6506 : int = aten::len(%6504) # torch/nn/functional.py:1992:19
      %6507 : int = aten::sub(%6506, %26) # torch/nn/functional.py:1992:19
      %size_prods.101 : int = prim::Loop(%6507, %25, %size_prods.100) # torch/nn/functional.py:1992:4
        block0(%i.26 : int, %size_prods.102 : int):
          %6511 : int = aten::add(%i.26, %26) # torch/nn/functional.py:1993:27
          %6512 : int = aten::__getitem__(%6504, %6511) # torch/nn/functional.py:1993:22
          %size_prods.103 : int = aten::mul(%size_prods.102, %6512) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.103)
      %6514 : bool = aten::eq(%size_prods.101, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6514) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6515 : Tensor = aten::batch_norm(%bottleneck_output.24, %6502, %6503, %6500, %6501, %6499, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.26 : Tensor = aten::relu_(%6515) # torch/nn/functional.py:1117:17
  %6517 : Tensor = prim::GetAttr[name="weight"](%6492)
  %6518 : Tensor? = prim::GetAttr[name="bias"](%6492)
  %6519 : int[] = prim::ListConstruct(%27, %27)
  %6520 : int[] = prim::ListConstruct(%27, %27)
  %6521 : int[] = prim::ListConstruct(%27, %27)
  %new_features.26 : Tensor = aten::conv2d(%result.26, %6517, %6518, %6519, %6520, %6521, %27) # torch/nn/modules/conv.py:415:15
  %6523 : float = prim::GetAttr[name="drop_rate"](%5380)
  %6524 : bool = aten::gt(%6523, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.23 : Tensor = prim::If(%6524) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6526 : float = prim::GetAttr[name="drop_rate"](%5380)
      %6527 : bool = prim::GetAttr[name="training"](%5380)
      %6528 : bool = aten::lt(%6526, %16) # torch/nn/functional.py:968:7
      %6529 : bool = prim::If(%6528) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6530 : bool = aten::gt(%6526, %17) # torch/nn/functional.py:968:17
          -> (%6530)
       = prim::If(%6529) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6531 : Tensor = aten::dropout(%new_features.26, %6526, %6527) # torch/nn/functional.py:973:17
      -> (%6531)
    block1():
      -> (%new_features.26)
  %6532 : Tensor[] = aten::append(%features.1, %new_features.23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6533 : Tensor = prim::Uninitialized()
  %6534 : bool = prim::GetAttr[name="memory_efficient"](%5381)
  %6535 : bool = prim::If(%6534) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6536 : bool = prim::Uninitialized()
      %6537 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6538 : bool = aten::gt(%6537, %24)
      %6539 : bool, %6540 : bool, %6541 : int = prim::Loop(%18, %6538, %19, %6536, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6542 : int, %6543 : bool, %6544 : bool, %6545 : int):
          %tensor.14 : Tensor = aten::__getitem__(%features.1, %6545) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6547 : bool = prim::requires_grad(%tensor.14)
          %6548 : bool, %6549 : bool = prim::If(%6547) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6536)
          %6550 : int = aten::add(%6545, %27)
          %6551 : bool = aten::lt(%6550, %6537)
          %6552 : bool = aten::__and__(%6551, %6548)
          -> (%6552, %6547, %6549, %6550)
      %6553 : bool = prim::If(%6539)
        block0():
          -> (%6540)
        block1():
          -> (%19)
      -> (%6553)
    block1():
      -> (%19)
  %bottleneck_output.26 : Tensor = prim::If(%6535) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6533)
    block1():
      %concated_features.14 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6556 : __torch__.torch.nn.modules.conv.___torch_mangle_256.Conv2d = prim::GetAttr[name="conv1"](%5381)
      %6557 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_255.BatchNorm2d = prim::GetAttr[name="norm1"](%5381)
      %6558 : int = aten::dim(%concated_features.14) # torch/nn/modules/batchnorm.py:276:11
      %6559 : bool = aten::ne(%6558, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6559) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6560 : bool = prim::GetAttr[name="training"](%6557)
       = prim::If(%6560) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6561 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6557)
          %6562 : Tensor = aten::add(%6561, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6557, %6562)
          -> ()
        block1():
          -> ()
      %6563 : bool = prim::GetAttr[name="training"](%6557)
      %6564 : Tensor = prim::GetAttr[name="running_mean"](%6557)
      %6565 : Tensor = prim::GetAttr[name="running_var"](%6557)
      %6566 : Tensor = prim::GetAttr[name="weight"](%6557)
      %6567 : Tensor = prim::GetAttr[name="bias"](%6557)
       = prim::If(%6563) # torch/nn/functional.py:2011:4
        block0():
          %6568 : int[] = aten::size(%concated_features.14) # torch/nn/functional.py:2012:27
          %size_prods.104 : int = aten::__getitem__(%6568, %24) # torch/nn/functional.py:1991:17
          %6570 : int = aten::len(%6568) # torch/nn/functional.py:1992:19
          %6571 : int = aten::sub(%6570, %26) # torch/nn/functional.py:1992:19
          %size_prods.105 : int = prim::Loop(%6571, %25, %size_prods.104) # torch/nn/functional.py:1992:4
            block0(%i.27 : int, %size_prods.106 : int):
              %6575 : int = aten::add(%i.27, %26) # torch/nn/functional.py:1993:27
              %6576 : int = aten::__getitem__(%6568, %6575) # torch/nn/functional.py:1993:22
              %size_prods.107 : int = aten::mul(%size_prods.106, %6576) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.107)
          %6578 : bool = aten::eq(%size_prods.105, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6578) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6579 : Tensor = aten::batch_norm(%concated_features.14, %6566, %6567, %6564, %6565, %6563, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.27 : Tensor = aten::relu_(%6579) # torch/nn/functional.py:1117:17
      %6581 : Tensor = prim::GetAttr[name="weight"](%6556)
      %6582 : Tensor? = prim::GetAttr[name="bias"](%6556)
      %6583 : int[] = prim::ListConstruct(%27, %27)
      %6584 : int[] = prim::ListConstruct(%24, %24)
      %6585 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.27 : Tensor = aten::conv2d(%result.27, %6581, %6582, %6583, %6584, %6585, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.27)
  %6587 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5381)
  %6588 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5381)
  %6589 : int = aten::dim(%bottleneck_output.26) # torch/nn/modules/batchnorm.py:276:11
  %6590 : bool = aten::ne(%6589, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6590) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6591 : bool = prim::GetAttr[name="training"](%6588)
   = prim::If(%6591) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6592 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6588)
      %6593 : Tensor = aten::add(%6592, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6588, %6593)
      -> ()
    block1():
      -> ()
  %6594 : bool = prim::GetAttr[name="training"](%6588)
  %6595 : Tensor = prim::GetAttr[name="running_mean"](%6588)
  %6596 : Tensor = prim::GetAttr[name="running_var"](%6588)
  %6597 : Tensor = prim::GetAttr[name="weight"](%6588)
  %6598 : Tensor = prim::GetAttr[name="bias"](%6588)
   = prim::If(%6594) # torch/nn/functional.py:2011:4
    block0():
      %6599 : int[] = aten::size(%bottleneck_output.26) # torch/nn/functional.py:2012:27
      %size_prods.108 : int = aten::__getitem__(%6599, %24) # torch/nn/functional.py:1991:17
      %6601 : int = aten::len(%6599) # torch/nn/functional.py:1992:19
      %6602 : int = aten::sub(%6601, %26) # torch/nn/functional.py:1992:19
      %size_prods.109 : int = prim::Loop(%6602, %25, %size_prods.108) # torch/nn/functional.py:1992:4
        block0(%i.28 : int, %size_prods.110 : int):
          %6606 : int = aten::add(%i.28, %26) # torch/nn/functional.py:1993:27
          %6607 : int = aten::__getitem__(%6599, %6606) # torch/nn/functional.py:1993:22
          %size_prods.111 : int = aten::mul(%size_prods.110, %6607) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.111)
      %6609 : bool = aten::eq(%size_prods.109, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6609) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6610 : Tensor = aten::batch_norm(%bottleneck_output.26, %6597, %6598, %6595, %6596, %6594, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.28 : Tensor = aten::relu_(%6610) # torch/nn/functional.py:1117:17
  %6612 : Tensor = prim::GetAttr[name="weight"](%6587)
  %6613 : Tensor? = prim::GetAttr[name="bias"](%6587)
  %6614 : int[] = prim::ListConstruct(%27, %27)
  %6615 : int[] = prim::ListConstruct(%27, %27)
  %6616 : int[] = prim::ListConstruct(%27, %27)
  %new_features.28 : Tensor = aten::conv2d(%result.28, %6612, %6613, %6614, %6615, %6616, %27) # torch/nn/modules/conv.py:415:15
  %6618 : float = prim::GetAttr[name="drop_rate"](%5381)
  %6619 : bool = aten::gt(%6618, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.25 : Tensor = prim::If(%6619) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6621 : float = prim::GetAttr[name="drop_rate"](%5381)
      %6622 : bool = prim::GetAttr[name="training"](%5381)
      %6623 : bool = aten::lt(%6621, %16) # torch/nn/functional.py:968:7
      %6624 : bool = prim::If(%6623) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6625 : bool = aten::gt(%6621, %17) # torch/nn/functional.py:968:17
          -> (%6625)
       = prim::If(%6624) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6626 : Tensor = aten::dropout(%new_features.28, %6621, %6622) # torch/nn/functional.py:973:17
      -> (%6626)
    block1():
      -> (%new_features.28)
  %6627 : Tensor[] = aten::append(%features.1, %new_features.25) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6628 : Tensor = prim::Uninitialized()
  %6629 : bool = prim::GetAttr[name="memory_efficient"](%5382)
  %6630 : bool = prim::If(%6629) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6631 : bool = prim::Uninitialized()
      %6632 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6633 : bool = aten::gt(%6632, %24)
      %6634 : bool, %6635 : bool, %6636 : int = prim::Loop(%18, %6633, %19, %6631, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6637 : int, %6638 : bool, %6639 : bool, %6640 : int):
          %tensor.15 : Tensor = aten::__getitem__(%features.1, %6640) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6642 : bool = prim::requires_grad(%tensor.15)
          %6643 : bool, %6644 : bool = prim::If(%6642) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6631)
          %6645 : int = aten::add(%6640, %27)
          %6646 : bool = aten::lt(%6645, %6632)
          %6647 : bool = aten::__and__(%6646, %6643)
          -> (%6647, %6642, %6644, %6645)
      %6648 : bool = prim::If(%6634)
        block0():
          -> (%6635)
        block1():
          -> (%19)
      -> (%6648)
    block1():
      -> (%19)
  %bottleneck_output.28 : Tensor = prim::If(%6630) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6628)
    block1():
      %concated_features.15 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6651 : __torch__.torch.nn.modules.conv.___torch_mangle_259.Conv2d = prim::GetAttr[name="conv1"](%5382)
      %6652 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_258.BatchNorm2d = prim::GetAttr[name="norm1"](%5382)
      %6653 : int = aten::dim(%concated_features.15) # torch/nn/modules/batchnorm.py:276:11
      %6654 : bool = aten::ne(%6653, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6654) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6655 : bool = prim::GetAttr[name="training"](%6652)
       = prim::If(%6655) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6656 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6652)
          %6657 : Tensor = aten::add(%6656, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6652, %6657)
          -> ()
        block1():
          -> ()
      %6658 : bool = prim::GetAttr[name="training"](%6652)
      %6659 : Tensor = prim::GetAttr[name="running_mean"](%6652)
      %6660 : Tensor = prim::GetAttr[name="running_var"](%6652)
      %6661 : Tensor = prim::GetAttr[name="weight"](%6652)
      %6662 : Tensor = prim::GetAttr[name="bias"](%6652)
       = prim::If(%6658) # torch/nn/functional.py:2011:4
        block0():
          %6663 : int[] = aten::size(%concated_features.15) # torch/nn/functional.py:2012:27
          %size_prods.112 : int = aten::__getitem__(%6663, %24) # torch/nn/functional.py:1991:17
          %6665 : int = aten::len(%6663) # torch/nn/functional.py:1992:19
          %6666 : int = aten::sub(%6665, %26) # torch/nn/functional.py:1992:19
          %size_prods.113 : int = prim::Loop(%6666, %25, %size_prods.112) # torch/nn/functional.py:1992:4
            block0(%i.29 : int, %size_prods.114 : int):
              %6670 : int = aten::add(%i.29, %26) # torch/nn/functional.py:1993:27
              %6671 : int = aten::__getitem__(%6663, %6670) # torch/nn/functional.py:1993:22
              %size_prods.115 : int = aten::mul(%size_prods.114, %6671) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.115)
          %6673 : bool = aten::eq(%size_prods.113, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6673) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6674 : Tensor = aten::batch_norm(%concated_features.15, %6661, %6662, %6659, %6660, %6658, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.29 : Tensor = aten::relu_(%6674) # torch/nn/functional.py:1117:17
      %6676 : Tensor = prim::GetAttr[name="weight"](%6651)
      %6677 : Tensor? = prim::GetAttr[name="bias"](%6651)
      %6678 : int[] = prim::ListConstruct(%27, %27)
      %6679 : int[] = prim::ListConstruct(%24, %24)
      %6680 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.29 : Tensor = aten::conv2d(%result.29, %6676, %6677, %6678, %6679, %6680, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.29)
  %6682 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5382)
  %6683 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5382)
  %6684 : int = aten::dim(%bottleneck_output.28) # torch/nn/modules/batchnorm.py:276:11
  %6685 : bool = aten::ne(%6684, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6685) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6686 : bool = prim::GetAttr[name="training"](%6683)
   = prim::If(%6686) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6687 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6683)
      %6688 : Tensor = aten::add(%6687, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6683, %6688)
      -> ()
    block1():
      -> ()
  %6689 : bool = prim::GetAttr[name="training"](%6683)
  %6690 : Tensor = prim::GetAttr[name="running_mean"](%6683)
  %6691 : Tensor = prim::GetAttr[name="running_var"](%6683)
  %6692 : Tensor = prim::GetAttr[name="weight"](%6683)
  %6693 : Tensor = prim::GetAttr[name="bias"](%6683)
   = prim::If(%6689) # torch/nn/functional.py:2011:4
    block0():
      %6694 : int[] = aten::size(%bottleneck_output.28) # torch/nn/functional.py:2012:27
      %size_prods.116 : int = aten::__getitem__(%6694, %24) # torch/nn/functional.py:1991:17
      %6696 : int = aten::len(%6694) # torch/nn/functional.py:1992:19
      %6697 : int = aten::sub(%6696, %26) # torch/nn/functional.py:1992:19
      %size_prods.117 : int = prim::Loop(%6697, %25, %size_prods.116) # torch/nn/functional.py:1992:4
        block0(%i.30 : int, %size_prods.118 : int):
          %6701 : int = aten::add(%i.30, %26) # torch/nn/functional.py:1993:27
          %6702 : int = aten::__getitem__(%6694, %6701) # torch/nn/functional.py:1993:22
          %size_prods.119 : int = aten::mul(%size_prods.118, %6702) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.119)
      %6704 : bool = aten::eq(%size_prods.117, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6704) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6705 : Tensor = aten::batch_norm(%bottleneck_output.28, %6692, %6693, %6690, %6691, %6689, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.30 : Tensor = aten::relu_(%6705) # torch/nn/functional.py:1117:17
  %6707 : Tensor = prim::GetAttr[name="weight"](%6682)
  %6708 : Tensor? = prim::GetAttr[name="bias"](%6682)
  %6709 : int[] = prim::ListConstruct(%27, %27)
  %6710 : int[] = prim::ListConstruct(%27, %27)
  %6711 : int[] = prim::ListConstruct(%27, %27)
  %new_features.30 : Tensor = aten::conv2d(%result.30, %6707, %6708, %6709, %6710, %6711, %27) # torch/nn/modules/conv.py:415:15
  %6713 : float = prim::GetAttr[name="drop_rate"](%5382)
  %6714 : bool = aten::gt(%6713, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.27 : Tensor = prim::If(%6714) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6716 : float = prim::GetAttr[name="drop_rate"](%5382)
      %6717 : bool = prim::GetAttr[name="training"](%5382)
      %6718 : bool = aten::lt(%6716, %16) # torch/nn/functional.py:968:7
      %6719 : bool = prim::If(%6718) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6720 : bool = aten::gt(%6716, %17) # torch/nn/functional.py:968:17
          -> (%6720)
       = prim::If(%6719) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6721 : Tensor = aten::dropout(%new_features.30, %6716, %6717) # torch/nn/functional.py:973:17
      -> (%6721)
    block1():
      -> (%new_features.30)
  %6722 : Tensor[] = aten::append(%features.1, %new_features.27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6723 : Tensor = prim::Uninitialized()
  %6724 : bool = prim::GetAttr[name="memory_efficient"](%5383)
  %6725 : bool = prim::If(%6724) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6726 : bool = prim::Uninitialized()
      %6727 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6728 : bool = aten::gt(%6727, %24)
      %6729 : bool, %6730 : bool, %6731 : int = prim::Loop(%18, %6728, %19, %6726, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6732 : int, %6733 : bool, %6734 : bool, %6735 : int):
          %tensor.16 : Tensor = aten::__getitem__(%features.1, %6735) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6737 : bool = prim::requires_grad(%tensor.16)
          %6738 : bool, %6739 : bool = prim::If(%6737) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6726)
          %6740 : int = aten::add(%6735, %27)
          %6741 : bool = aten::lt(%6740, %6727)
          %6742 : bool = aten::__and__(%6741, %6738)
          -> (%6742, %6737, %6739, %6740)
      %6743 : bool = prim::If(%6729)
        block0():
          -> (%6730)
        block1():
          -> (%19)
      -> (%6743)
    block1():
      -> (%19)
  %bottleneck_output.30 : Tensor = prim::If(%6725) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6723)
    block1():
      %concated_features.16 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6746 : __torch__.torch.nn.modules.conv.___torch_mangle_262.Conv2d = prim::GetAttr[name="conv1"](%5383)
      %6747 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_261.BatchNorm2d = prim::GetAttr[name="norm1"](%5383)
      %6748 : int = aten::dim(%concated_features.16) # torch/nn/modules/batchnorm.py:276:11
      %6749 : bool = aten::ne(%6748, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6749) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6750 : bool = prim::GetAttr[name="training"](%6747)
       = prim::If(%6750) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6751 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6747)
          %6752 : Tensor = aten::add(%6751, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6747, %6752)
          -> ()
        block1():
          -> ()
      %6753 : bool = prim::GetAttr[name="training"](%6747)
      %6754 : Tensor = prim::GetAttr[name="running_mean"](%6747)
      %6755 : Tensor = prim::GetAttr[name="running_var"](%6747)
      %6756 : Tensor = prim::GetAttr[name="weight"](%6747)
      %6757 : Tensor = prim::GetAttr[name="bias"](%6747)
       = prim::If(%6753) # torch/nn/functional.py:2011:4
        block0():
          %6758 : int[] = aten::size(%concated_features.16) # torch/nn/functional.py:2012:27
          %size_prods.120 : int = aten::__getitem__(%6758, %24) # torch/nn/functional.py:1991:17
          %6760 : int = aten::len(%6758) # torch/nn/functional.py:1992:19
          %6761 : int = aten::sub(%6760, %26) # torch/nn/functional.py:1992:19
          %size_prods.121 : int = prim::Loop(%6761, %25, %size_prods.120) # torch/nn/functional.py:1992:4
            block0(%i.31 : int, %size_prods.122 : int):
              %6765 : int = aten::add(%i.31, %26) # torch/nn/functional.py:1993:27
              %6766 : int = aten::__getitem__(%6758, %6765) # torch/nn/functional.py:1993:22
              %size_prods.123 : int = aten::mul(%size_prods.122, %6766) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.123)
          %6768 : bool = aten::eq(%size_prods.121, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6768) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6769 : Tensor = aten::batch_norm(%concated_features.16, %6756, %6757, %6754, %6755, %6753, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.31 : Tensor = aten::relu_(%6769) # torch/nn/functional.py:1117:17
      %6771 : Tensor = prim::GetAttr[name="weight"](%6746)
      %6772 : Tensor? = prim::GetAttr[name="bias"](%6746)
      %6773 : int[] = prim::ListConstruct(%27, %27)
      %6774 : int[] = prim::ListConstruct(%24, %24)
      %6775 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.31 : Tensor = aten::conv2d(%result.31, %6771, %6772, %6773, %6774, %6775, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.31)
  %6777 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5383)
  %6778 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5383)
  %6779 : int = aten::dim(%bottleneck_output.30) # torch/nn/modules/batchnorm.py:276:11
  %6780 : bool = aten::ne(%6779, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6780) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6781 : bool = prim::GetAttr[name="training"](%6778)
   = prim::If(%6781) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6782 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6778)
      %6783 : Tensor = aten::add(%6782, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6778, %6783)
      -> ()
    block1():
      -> ()
  %6784 : bool = prim::GetAttr[name="training"](%6778)
  %6785 : Tensor = prim::GetAttr[name="running_mean"](%6778)
  %6786 : Tensor = prim::GetAttr[name="running_var"](%6778)
  %6787 : Tensor = prim::GetAttr[name="weight"](%6778)
  %6788 : Tensor = prim::GetAttr[name="bias"](%6778)
   = prim::If(%6784) # torch/nn/functional.py:2011:4
    block0():
      %6789 : int[] = aten::size(%bottleneck_output.30) # torch/nn/functional.py:2012:27
      %size_prods.124 : int = aten::__getitem__(%6789, %24) # torch/nn/functional.py:1991:17
      %6791 : int = aten::len(%6789) # torch/nn/functional.py:1992:19
      %6792 : int = aten::sub(%6791, %26) # torch/nn/functional.py:1992:19
      %size_prods.125 : int = prim::Loop(%6792, %25, %size_prods.124) # torch/nn/functional.py:1992:4
        block0(%i.32 : int, %size_prods.126 : int):
          %6796 : int = aten::add(%i.32, %26) # torch/nn/functional.py:1993:27
          %6797 : int = aten::__getitem__(%6789, %6796) # torch/nn/functional.py:1993:22
          %size_prods.127 : int = aten::mul(%size_prods.126, %6797) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.127)
      %6799 : bool = aten::eq(%size_prods.125, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6799) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6800 : Tensor = aten::batch_norm(%bottleneck_output.30, %6787, %6788, %6785, %6786, %6784, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.32 : Tensor = aten::relu_(%6800) # torch/nn/functional.py:1117:17
  %6802 : Tensor = prim::GetAttr[name="weight"](%6777)
  %6803 : Tensor? = prim::GetAttr[name="bias"](%6777)
  %6804 : int[] = prim::ListConstruct(%27, %27)
  %6805 : int[] = prim::ListConstruct(%27, %27)
  %6806 : int[] = prim::ListConstruct(%27, %27)
  %new_features.32 : Tensor = aten::conv2d(%result.32, %6802, %6803, %6804, %6805, %6806, %27) # torch/nn/modules/conv.py:415:15
  %6808 : float = prim::GetAttr[name="drop_rate"](%5383)
  %6809 : bool = aten::gt(%6808, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.29 : Tensor = prim::If(%6809) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6811 : float = prim::GetAttr[name="drop_rate"](%5383)
      %6812 : bool = prim::GetAttr[name="training"](%5383)
      %6813 : bool = aten::lt(%6811, %16) # torch/nn/functional.py:968:7
      %6814 : bool = prim::If(%6813) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6815 : bool = aten::gt(%6811, %17) # torch/nn/functional.py:968:17
          -> (%6815)
       = prim::If(%6814) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6816 : Tensor = aten::dropout(%new_features.32, %6811, %6812) # torch/nn/functional.py:973:17
      -> (%6816)
    block1():
      -> (%new_features.32)
  %6817 : Tensor[] = aten::append(%features.1, %new_features.29) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6818 : Tensor = prim::Uninitialized()
  %6819 : bool = prim::GetAttr[name="memory_efficient"](%5384)
  %6820 : bool = prim::If(%6819) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6821 : bool = prim::Uninitialized()
      %6822 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6823 : bool = aten::gt(%6822, %24)
      %6824 : bool, %6825 : bool, %6826 : int = prim::Loop(%18, %6823, %19, %6821, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6827 : int, %6828 : bool, %6829 : bool, %6830 : int):
          %tensor.17 : Tensor = aten::__getitem__(%features.1, %6830) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6832 : bool = prim::requires_grad(%tensor.17)
          %6833 : bool, %6834 : bool = prim::If(%6832) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6821)
          %6835 : int = aten::add(%6830, %27)
          %6836 : bool = aten::lt(%6835, %6822)
          %6837 : bool = aten::__and__(%6836, %6833)
          -> (%6837, %6832, %6834, %6835)
      %6838 : bool = prim::If(%6824)
        block0():
          -> (%6825)
        block1():
          -> (%19)
      -> (%6838)
    block1():
      -> (%19)
  %bottleneck_output.32 : Tensor = prim::If(%6820) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6818)
    block1():
      %concated_features.17 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6841 : __torch__.torch.nn.modules.conv.___torch_mangle_265.Conv2d = prim::GetAttr[name="conv1"](%5384)
      %6842 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_264.BatchNorm2d = prim::GetAttr[name="norm1"](%5384)
      %6843 : int = aten::dim(%concated_features.17) # torch/nn/modules/batchnorm.py:276:11
      %6844 : bool = aten::ne(%6843, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6844) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6845 : bool = prim::GetAttr[name="training"](%6842)
       = prim::If(%6845) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6846 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6842)
          %6847 : Tensor = aten::add(%6846, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6842, %6847)
          -> ()
        block1():
          -> ()
      %6848 : bool = prim::GetAttr[name="training"](%6842)
      %6849 : Tensor = prim::GetAttr[name="running_mean"](%6842)
      %6850 : Tensor = prim::GetAttr[name="running_var"](%6842)
      %6851 : Tensor = prim::GetAttr[name="weight"](%6842)
      %6852 : Tensor = prim::GetAttr[name="bias"](%6842)
       = prim::If(%6848) # torch/nn/functional.py:2011:4
        block0():
          %6853 : int[] = aten::size(%concated_features.17) # torch/nn/functional.py:2012:27
          %size_prods.128 : int = aten::__getitem__(%6853, %24) # torch/nn/functional.py:1991:17
          %6855 : int = aten::len(%6853) # torch/nn/functional.py:1992:19
          %6856 : int = aten::sub(%6855, %26) # torch/nn/functional.py:1992:19
          %size_prods.129 : int = prim::Loop(%6856, %25, %size_prods.128) # torch/nn/functional.py:1992:4
            block0(%i.33 : int, %size_prods.130 : int):
              %6860 : int = aten::add(%i.33, %26) # torch/nn/functional.py:1993:27
              %6861 : int = aten::__getitem__(%6853, %6860) # torch/nn/functional.py:1993:22
              %size_prods.131 : int = aten::mul(%size_prods.130, %6861) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.131)
          %6863 : bool = aten::eq(%size_prods.129, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6863) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6864 : Tensor = aten::batch_norm(%concated_features.17, %6851, %6852, %6849, %6850, %6848, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.33 : Tensor = aten::relu_(%6864) # torch/nn/functional.py:1117:17
      %6866 : Tensor = prim::GetAttr[name="weight"](%6841)
      %6867 : Tensor? = prim::GetAttr[name="bias"](%6841)
      %6868 : int[] = prim::ListConstruct(%27, %27)
      %6869 : int[] = prim::ListConstruct(%24, %24)
      %6870 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.33 : Tensor = aten::conv2d(%result.33, %6866, %6867, %6868, %6869, %6870, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.33)
  %6872 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5384)
  %6873 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5384)
  %6874 : int = aten::dim(%bottleneck_output.32) # torch/nn/modules/batchnorm.py:276:11
  %6875 : bool = aten::ne(%6874, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6875) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6876 : bool = prim::GetAttr[name="training"](%6873)
   = prim::If(%6876) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6877 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6873)
      %6878 : Tensor = aten::add(%6877, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6873, %6878)
      -> ()
    block1():
      -> ()
  %6879 : bool = prim::GetAttr[name="training"](%6873)
  %6880 : Tensor = prim::GetAttr[name="running_mean"](%6873)
  %6881 : Tensor = prim::GetAttr[name="running_var"](%6873)
  %6882 : Tensor = prim::GetAttr[name="weight"](%6873)
  %6883 : Tensor = prim::GetAttr[name="bias"](%6873)
   = prim::If(%6879) # torch/nn/functional.py:2011:4
    block0():
      %6884 : int[] = aten::size(%bottleneck_output.32) # torch/nn/functional.py:2012:27
      %size_prods.132 : int = aten::__getitem__(%6884, %24) # torch/nn/functional.py:1991:17
      %6886 : int = aten::len(%6884) # torch/nn/functional.py:1992:19
      %6887 : int = aten::sub(%6886, %26) # torch/nn/functional.py:1992:19
      %size_prods.133 : int = prim::Loop(%6887, %25, %size_prods.132) # torch/nn/functional.py:1992:4
        block0(%i.34 : int, %size_prods.134 : int):
          %6891 : int = aten::add(%i.34, %26) # torch/nn/functional.py:1993:27
          %6892 : int = aten::__getitem__(%6884, %6891) # torch/nn/functional.py:1993:22
          %size_prods.135 : int = aten::mul(%size_prods.134, %6892) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.135)
      %6894 : bool = aten::eq(%size_prods.133, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6894) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6895 : Tensor = aten::batch_norm(%bottleneck_output.32, %6882, %6883, %6880, %6881, %6879, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.34 : Tensor = aten::relu_(%6895) # torch/nn/functional.py:1117:17
  %6897 : Tensor = prim::GetAttr[name="weight"](%6872)
  %6898 : Tensor? = prim::GetAttr[name="bias"](%6872)
  %6899 : int[] = prim::ListConstruct(%27, %27)
  %6900 : int[] = prim::ListConstruct(%27, %27)
  %6901 : int[] = prim::ListConstruct(%27, %27)
  %new_features.34 : Tensor = aten::conv2d(%result.34, %6897, %6898, %6899, %6900, %6901, %27) # torch/nn/modules/conv.py:415:15
  %6903 : float = prim::GetAttr[name="drop_rate"](%5384)
  %6904 : bool = aten::gt(%6903, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.31 : Tensor = prim::If(%6904) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %6906 : float = prim::GetAttr[name="drop_rate"](%5384)
      %6907 : bool = prim::GetAttr[name="training"](%5384)
      %6908 : bool = aten::lt(%6906, %16) # torch/nn/functional.py:968:7
      %6909 : bool = prim::If(%6908) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %6910 : bool = aten::gt(%6906, %17) # torch/nn/functional.py:968:17
          -> (%6910)
       = prim::If(%6909) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %6911 : Tensor = aten::dropout(%new_features.34, %6906, %6907) # torch/nn/functional.py:973:17
      -> (%6911)
    block1():
      -> (%new_features.34)
  %6912 : Tensor[] = aten::append(%features.1, %new_features.31) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %6913 : Tensor = prim::Uninitialized()
  %6914 : bool = prim::GetAttr[name="memory_efficient"](%5385)
  %6915 : bool = prim::If(%6914) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %6916 : bool = prim::Uninitialized()
      %6917 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %6918 : bool = aten::gt(%6917, %24)
      %6919 : bool, %6920 : bool, %6921 : int = prim::Loop(%18, %6918, %19, %6916, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%6922 : int, %6923 : bool, %6924 : bool, %6925 : int):
          %tensor.18 : Tensor = aten::__getitem__(%features.1, %6925) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %6927 : bool = prim::requires_grad(%tensor.18)
          %6928 : bool, %6929 : bool = prim::If(%6927) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %6916)
          %6930 : int = aten::add(%6925, %27)
          %6931 : bool = aten::lt(%6930, %6917)
          %6932 : bool = aten::__and__(%6931, %6928)
          -> (%6932, %6927, %6929, %6930)
      %6933 : bool = prim::If(%6919)
        block0():
          -> (%6920)
        block1():
          -> (%19)
      -> (%6933)
    block1():
      -> (%19)
  %bottleneck_output.34 : Tensor = prim::If(%6915) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%6913)
    block1():
      %concated_features.18 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %6936 : __torch__.torch.nn.modules.conv.___torch_mangle_268.Conv2d = prim::GetAttr[name="conv1"](%5385)
      %6937 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_267.BatchNorm2d = prim::GetAttr[name="norm1"](%5385)
      %6938 : int = aten::dim(%concated_features.18) # torch/nn/modules/batchnorm.py:276:11
      %6939 : bool = aten::ne(%6938, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%6939) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %6940 : bool = prim::GetAttr[name="training"](%6937)
       = prim::If(%6940) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %6941 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6937)
          %6942 : Tensor = aten::add(%6941, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%6937, %6942)
          -> ()
        block1():
          -> ()
      %6943 : bool = prim::GetAttr[name="training"](%6937)
      %6944 : Tensor = prim::GetAttr[name="running_mean"](%6937)
      %6945 : Tensor = prim::GetAttr[name="running_var"](%6937)
      %6946 : Tensor = prim::GetAttr[name="weight"](%6937)
      %6947 : Tensor = prim::GetAttr[name="bias"](%6937)
       = prim::If(%6943) # torch/nn/functional.py:2011:4
        block0():
          %6948 : int[] = aten::size(%concated_features.18) # torch/nn/functional.py:2012:27
          %size_prods.136 : int = aten::__getitem__(%6948, %24) # torch/nn/functional.py:1991:17
          %6950 : int = aten::len(%6948) # torch/nn/functional.py:1992:19
          %6951 : int = aten::sub(%6950, %26) # torch/nn/functional.py:1992:19
          %size_prods.137 : int = prim::Loop(%6951, %25, %size_prods.136) # torch/nn/functional.py:1992:4
            block0(%i.35 : int, %size_prods.138 : int):
              %6955 : int = aten::add(%i.35, %26) # torch/nn/functional.py:1993:27
              %6956 : int = aten::__getitem__(%6948, %6955) # torch/nn/functional.py:1993:22
              %size_prods.139 : int = aten::mul(%size_prods.138, %6956) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.139)
          %6958 : bool = aten::eq(%size_prods.137, %27) # torch/nn/functional.py:1994:7
           = prim::If(%6958) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %6959 : Tensor = aten::batch_norm(%concated_features.18, %6946, %6947, %6944, %6945, %6943, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.35 : Tensor = aten::relu_(%6959) # torch/nn/functional.py:1117:17
      %6961 : Tensor = prim::GetAttr[name="weight"](%6936)
      %6962 : Tensor? = prim::GetAttr[name="bias"](%6936)
      %6963 : int[] = prim::ListConstruct(%27, %27)
      %6964 : int[] = prim::ListConstruct(%24, %24)
      %6965 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.35 : Tensor = aten::conv2d(%result.35, %6961, %6962, %6963, %6964, %6965, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.35)
  %6967 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5385)
  %6968 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5385)
  %6969 : int = aten::dim(%bottleneck_output.34) # torch/nn/modules/batchnorm.py:276:11
  %6970 : bool = aten::ne(%6969, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%6970) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %6971 : bool = prim::GetAttr[name="training"](%6968)
   = prim::If(%6971) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %6972 : Tensor = prim::GetAttr[name="num_batches_tracked"](%6968)
      %6973 : Tensor = aten::add(%6972, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%6968, %6973)
      -> ()
    block1():
      -> ()
  %6974 : bool = prim::GetAttr[name="training"](%6968)
  %6975 : Tensor = prim::GetAttr[name="running_mean"](%6968)
  %6976 : Tensor = prim::GetAttr[name="running_var"](%6968)
  %6977 : Tensor = prim::GetAttr[name="weight"](%6968)
  %6978 : Tensor = prim::GetAttr[name="bias"](%6968)
   = prim::If(%6974) # torch/nn/functional.py:2011:4
    block0():
      %6979 : int[] = aten::size(%bottleneck_output.34) # torch/nn/functional.py:2012:27
      %size_prods.140 : int = aten::__getitem__(%6979, %24) # torch/nn/functional.py:1991:17
      %6981 : int = aten::len(%6979) # torch/nn/functional.py:1992:19
      %6982 : int = aten::sub(%6981, %26) # torch/nn/functional.py:1992:19
      %size_prods.141 : int = prim::Loop(%6982, %25, %size_prods.140) # torch/nn/functional.py:1992:4
        block0(%i.36 : int, %size_prods.142 : int):
          %6986 : int = aten::add(%i.36, %26) # torch/nn/functional.py:1993:27
          %6987 : int = aten::__getitem__(%6979, %6986) # torch/nn/functional.py:1993:22
          %size_prods.143 : int = aten::mul(%size_prods.142, %6987) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.143)
      %6989 : bool = aten::eq(%size_prods.141, %27) # torch/nn/functional.py:1994:7
       = prim::If(%6989) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %6990 : Tensor = aten::batch_norm(%bottleneck_output.34, %6977, %6978, %6975, %6976, %6974, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.36 : Tensor = aten::relu_(%6990) # torch/nn/functional.py:1117:17
  %6992 : Tensor = prim::GetAttr[name="weight"](%6967)
  %6993 : Tensor? = prim::GetAttr[name="bias"](%6967)
  %6994 : int[] = prim::ListConstruct(%27, %27)
  %6995 : int[] = prim::ListConstruct(%27, %27)
  %6996 : int[] = prim::ListConstruct(%27, %27)
  %new_features.36 : Tensor = aten::conv2d(%result.36, %6992, %6993, %6994, %6995, %6996, %27) # torch/nn/modules/conv.py:415:15
  %6998 : float = prim::GetAttr[name="drop_rate"](%5385)
  %6999 : bool = aten::gt(%6998, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.33 : Tensor = prim::If(%6999) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7001 : float = prim::GetAttr[name="drop_rate"](%5385)
      %7002 : bool = prim::GetAttr[name="training"](%5385)
      %7003 : bool = aten::lt(%7001, %16) # torch/nn/functional.py:968:7
      %7004 : bool = prim::If(%7003) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7005 : bool = aten::gt(%7001, %17) # torch/nn/functional.py:968:17
          -> (%7005)
       = prim::If(%7004) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7006 : Tensor = aten::dropout(%new_features.36, %7001, %7002) # torch/nn/functional.py:973:17
      -> (%7006)
    block1():
      -> (%new_features.36)
  %7007 : Tensor[] = aten::append(%features.1, %new_features.33) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7008 : Tensor = prim::Uninitialized()
  %7009 : bool = prim::GetAttr[name="memory_efficient"](%5386)
  %7010 : bool = prim::If(%7009) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7011 : bool = prim::Uninitialized()
      %7012 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7013 : bool = aten::gt(%7012, %24)
      %7014 : bool, %7015 : bool, %7016 : int = prim::Loop(%18, %7013, %19, %7011, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7017 : int, %7018 : bool, %7019 : bool, %7020 : int):
          %tensor.19 : Tensor = aten::__getitem__(%features.1, %7020) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7022 : bool = prim::requires_grad(%tensor.19)
          %7023 : bool, %7024 : bool = prim::If(%7022) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7011)
          %7025 : int = aten::add(%7020, %27)
          %7026 : bool = aten::lt(%7025, %7012)
          %7027 : bool = aten::__and__(%7026, %7023)
          -> (%7027, %7022, %7024, %7025)
      %7028 : bool = prim::If(%7014)
        block0():
          -> (%7015)
        block1():
          -> (%19)
      -> (%7028)
    block1():
      -> (%19)
  %bottleneck_output.36 : Tensor = prim::If(%7010) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7008)
    block1():
      %concated_features.19 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7031 : __torch__.torch.nn.modules.conv.___torch_mangle_271.Conv2d = prim::GetAttr[name="conv1"](%5386)
      %7032 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_270.BatchNorm2d = prim::GetAttr[name="norm1"](%5386)
      %7033 : int = aten::dim(%concated_features.19) # torch/nn/modules/batchnorm.py:276:11
      %7034 : bool = aten::ne(%7033, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7034) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7035 : bool = prim::GetAttr[name="training"](%7032)
       = prim::If(%7035) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7036 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7032)
          %7037 : Tensor = aten::add(%7036, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7032, %7037)
          -> ()
        block1():
          -> ()
      %7038 : bool = prim::GetAttr[name="training"](%7032)
      %7039 : Tensor = prim::GetAttr[name="running_mean"](%7032)
      %7040 : Tensor = prim::GetAttr[name="running_var"](%7032)
      %7041 : Tensor = prim::GetAttr[name="weight"](%7032)
      %7042 : Tensor = prim::GetAttr[name="bias"](%7032)
       = prim::If(%7038) # torch/nn/functional.py:2011:4
        block0():
          %7043 : int[] = aten::size(%concated_features.19) # torch/nn/functional.py:2012:27
          %size_prods.144 : int = aten::__getitem__(%7043, %24) # torch/nn/functional.py:1991:17
          %7045 : int = aten::len(%7043) # torch/nn/functional.py:1992:19
          %7046 : int = aten::sub(%7045, %26) # torch/nn/functional.py:1992:19
          %size_prods.145 : int = prim::Loop(%7046, %25, %size_prods.144) # torch/nn/functional.py:1992:4
            block0(%i.37 : int, %size_prods.146 : int):
              %7050 : int = aten::add(%i.37, %26) # torch/nn/functional.py:1993:27
              %7051 : int = aten::__getitem__(%7043, %7050) # torch/nn/functional.py:1993:22
              %size_prods.147 : int = aten::mul(%size_prods.146, %7051) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.147)
          %7053 : bool = aten::eq(%size_prods.145, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7053) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7054 : Tensor = aten::batch_norm(%concated_features.19, %7041, %7042, %7039, %7040, %7038, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.37 : Tensor = aten::relu_(%7054) # torch/nn/functional.py:1117:17
      %7056 : Tensor = prim::GetAttr[name="weight"](%7031)
      %7057 : Tensor? = prim::GetAttr[name="bias"](%7031)
      %7058 : int[] = prim::ListConstruct(%27, %27)
      %7059 : int[] = prim::ListConstruct(%24, %24)
      %7060 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.37 : Tensor = aten::conv2d(%result.37, %7056, %7057, %7058, %7059, %7060, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.37)
  %7062 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5386)
  %7063 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5386)
  %7064 : int = aten::dim(%bottleneck_output.36) # torch/nn/modules/batchnorm.py:276:11
  %7065 : bool = aten::ne(%7064, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7065) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7066 : bool = prim::GetAttr[name="training"](%7063)
   = prim::If(%7066) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7067 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7063)
      %7068 : Tensor = aten::add(%7067, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7063, %7068)
      -> ()
    block1():
      -> ()
  %7069 : bool = prim::GetAttr[name="training"](%7063)
  %7070 : Tensor = prim::GetAttr[name="running_mean"](%7063)
  %7071 : Tensor = prim::GetAttr[name="running_var"](%7063)
  %7072 : Tensor = prim::GetAttr[name="weight"](%7063)
  %7073 : Tensor = prim::GetAttr[name="bias"](%7063)
   = prim::If(%7069) # torch/nn/functional.py:2011:4
    block0():
      %7074 : int[] = aten::size(%bottleneck_output.36) # torch/nn/functional.py:2012:27
      %size_prods.148 : int = aten::__getitem__(%7074, %24) # torch/nn/functional.py:1991:17
      %7076 : int = aten::len(%7074) # torch/nn/functional.py:1992:19
      %7077 : int = aten::sub(%7076, %26) # torch/nn/functional.py:1992:19
      %size_prods.149 : int = prim::Loop(%7077, %25, %size_prods.148) # torch/nn/functional.py:1992:4
        block0(%i.38 : int, %size_prods.150 : int):
          %7081 : int = aten::add(%i.38, %26) # torch/nn/functional.py:1993:27
          %7082 : int = aten::__getitem__(%7074, %7081) # torch/nn/functional.py:1993:22
          %size_prods.151 : int = aten::mul(%size_prods.150, %7082) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.151)
      %7084 : bool = aten::eq(%size_prods.149, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7084) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7085 : Tensor = aten::batch_norm(%bottleneck_output.36, %7072, %7073, %7070, %7071, %7069, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.38 : Tensor = aten::relu_(%7085) # torch/nn/functional.py:1117:17
  %7087 : Tensor = prim::GetAttr[name="weight"](%7062)
  %7088 : Tensor? = prim::GetAttr[name="bias"](%7062)
  %7089 : int[] = prim::ListConstruct(%27, %27)
  %7090 : int[] = prim::ListConstruct(%27, %27)
  %7091 : int[] = prim::ListConstruct(%27, %27)
  %new_features.38 : Tensor = aten::conv2d(%result.38, %7087, %7088, %7089, %7090, %7091, %27) # torch/nn/modules/conv.py:415:15
  %7093 : float = prim::GetAttr[name="drop_rate"](%5386)
  %7094 : bool = aten::gt(%7093, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.35 : Tensor = prim::If(%7094) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7096 : float = prim::GetAttr[name="drop_rate"](%5386)
      %7097 : bool = prim::GetAttr[name="training"](%5386)
      %7098 : bool = aten::lt(%7096, %16) # torch/nn/functional.py:968:7
      %7099 : bool = prim::If(%7098) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7100 : bool = aten::gt(%7096, %17) # torch/nn/functional.py:968:17
          -> (%7100)
       = prim::If(%7099) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7101 : Tensor = aten::dropout(%new_features.38, %7096, %7097) # torch/nn/functional.py:973:17
      -> (%7101)
    block1():
      -> (%new_features.38)
  %7102 : Tensor[] = aten::append(%features.1, %new_features.35) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7103 : Tensor = prim::Uninitialized()
  %7104 : bool = prim::GetAttr[name="memory_efficient"](%5387)
  %7105 : bool = prim::If(%7104) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7106 : bool = prim::Uninitialized()
      %7107 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7108 : bool = aten::gt(%7107, %24)
      %7109 : bool, %7110 : bool, %7111 : int = prim::Loop(%18, %7108, %19, %7106, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7112 : int, %7113 : bool, %7114 : bool, %7115 : int):
          %tensor.20 : Tensor = aten::__getitem__(%features.1, %7115) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7117 : bool = prim::requires_grad(%tensor.20)
          %7118 : bool, %7119 : bool = prim::If(%7117) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7106)
          %7120 : int = aten::add(%7115, %27)
          %7121 : bool = aten::lt(%7120, %7107)
          %7122 : bool = aten::__and__(%7121, %7118)
          -> (%7122, %7117, %7119, %7120)
      %7123 : bool = prim::If(%7109)
        block0():
          -> (%7110)
        block1():
          -> (%19)
      -> (%7123)
    block1():
      -> (%19)
  %bottleneck_output.38 : Tensor = prim::If(%7105) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7103)
    block1():
      %concated_features.20 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7126 : __torch__.torch.nn.modules.conv.___torch_mangle_274.Conv2d = prim::GetAttr[name="conv1"](%5387)
      %7127 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_273.BatchNorm2d = prim::GetAttr[name="norm1"](%5387)
      %7128 : int = aten::dim(%concated_features.20) # torch/nn/modules/batchnorm.py:276:11
      %7129 : bool = aten::ne(%7128, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7129) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7130 : bool = prim::GetAttr[name="training"](%7127)
       = prim::If(%7130) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7131 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7127)
          %7132 : Tensor = aten::add(%7131, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7127, %7132)
          -> ()
        block1():
          -> ()
      %7133 : bool = prim::GetAttr[name="training"](%7127)
      %7134 : Tensor = prim::GetAttr[name="running_mean"](%7127)
      %7135 : Tensor = prim::GetAttr[name="running_var"](%7127)
      %7136 : Tensor = prim::GetAttr[name="weight"](%7127)
      %7137 : Tensor = prim::GetAttr[name="bias"](%7127)
       = prim::If(%7133) # torch/nn/functional.py:2011:4
        block0():
          %7138 : int[] = aten::size(%concated_features.20) # torch/nn/functional.py:2012:27
          %size_prods.152 : int = aten::__getitem__(%7138, %24) # torch/nn/functional.py:1991:17
          %7140 : int = aten::len(%7138) # torch/nn/functional.py:1992:19
          %7141 : int = aten::sub(%7140, %26) # torch/nn/functional.py:1992:19
          %size_prods.153 : int = prim::Loop(%7141, %25, %size_prods.152) # torch/nn/functional.py:1992:4
            block0(%i.39 : int, %size_prods.154 : int):
              %7145 : int = aten::add(%i.39, %26) # torch/nn/functional.py:1993:27
              %7146 : int = aten::__getitem__(%7138, %7145) # torch/nn/functional.py:1993:22
              %size_prods.155 : int = aten::mul(%size_prods.154, %7146) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.155)
          %7148 : bool = aten::eq(%size_prods.153, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7148) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7149 : Tensor = aten::batch_norm(%concated_features.20, %7136, %7137, %7134, %7135, %7133, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.39 : Tensor = aten::relu_(%7149) # torch/nn/functional.py:1117:17
      %7151 : Tensor = prim::GetAttr[name="weight"](%7126)
      %7152 : Tensor? = prim::GetAttr[name="bias"](%7126)
      %7153 : int[] = prim::ListConstruct(%27, %27)
      %7154 : int[] = prim::ListConstruct(%24, %24)
      %7155 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.39 : Tensor = aten::conv2d(%result.39, %7151, %7152, %7153, %7154, %7155, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.39)
  %7157 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5387)
  %7158 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5387)
  %7159 : int = aten::dim(%bottleneck_output.38) # torch/nn/modules/batchnorm.py:276:11
  %7160 : bool = aten::ne(%7159, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7160) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7161 : bool = prim::GetAttr[name="training"](%7158)
   = prim::If(%7161) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7162 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7158)
      %7163 : Tensor = aten::add(%7162, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7158, %7163)
      -> ()
    block1():
      -> ()
  %7164 : bool = prim::GetAttr[name="training"](%7158)
  %7165 : Tensor = prim::GetAttr[name="running_mean"](%7158)
  %7166 : Tensor = prim::GetAttr[name="running_var"](%7158)
  %7167 : Tensor = prim::GetAttr[name="weight"](%7158)
  %7168 : Tensor = prim::GetAttr[name="bias"](%7158)
   = prim::If(%7164) # torch/nn/functional.py:2011:4
    block0():
      %7169 : int[] = aten::size(%bottleneck_output.38) # torch/nn/functional.py:2012:27
      %size_prods.156 : int = aten::__getitem__(%7169, %24) # torch/nn/functional.py:1991:17
      %7171 : int = aten::len(%7169) # torch/nn/functional.py:1992:19
      %7172 : int = aten::sub(%7171, %26) # torch/nn/functional.py:1992:19
      %size_prods.157 : int = prim::Loop(%7172, %25, %size_prods.156) # torch/nn/functional.py:1992:4
        block0(%i.40 : int, %size_prods.158 : int):
          %7176 : int = aten::add(%i.40, %26) # torch/nn/functional.py:1993:27
          %7177 : int = aten::__getitem__(%7169, %7176) # torch/nn/functional.py:1993:22
          %size_prods.159 : int = aten::mul(%size_prods.158, %7177) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.159)
      %7179 : bool = aten::eq(%size_prods.157, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7179) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7180 : Tensor = aten::batch_norm(%bottleneck_output.38, %7167, %7168, %7165, %7166, %7164, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.40 : Tensor = aten::relu_(%7180) # torch/nn/functional.py:1117:17
  %7182 : Tensor = prim::GetAttr[name="weight"](%7157)
  %7183 : Tensor? = prim::GetAttr[name="bias"](%7157)
  %7184 : int[] = prim::ListConstruct(%27, %27)
  %7185 : int[] = prim::ListConstruct(%27, %27)
  %7186 : int[] = prim::ListConstruct(%27, %27)
  %new_features.40 : Tensor = aten::conv2d(%result.40, %7182, %7183, %7184, %7185, %7186, %27) # torch/nn/modules/conv.py:415:15
  %7188 : float = prim::GetAttr[name="drop_rate"](%5387)
  %7189 : bool = aten::gt(%7188, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.37 : Tensor = prim::If(%7189) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7191 : float = prim::GetAttr[name="drop_rate"](%5387)
      %7192 : bool = prim::GetAttr[name="training"](%5387)
      %7193 : bool = aten::lt(%7191, %16) # torch/nn/functional.py:968:7
      %7194 : bool = prim::If(%7193) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7195 : bool = aten::gt(%7191, %17) # torch/nn/functional.py:968:17
          -> (%7195)
       = prim::If(%7194) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7196 : Tensor = aten::dropout(%new_features.40, %7191, %7192) # torch/nn/functional.py:973:17
      -> (%7196)
    block1():
      -> (%new_features.40)
  %7197 : Tensor[] = aten::append(%features.1, %new_features.37) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7198 : Tensor = prim::Uninitialized()
  %7199 : bool = prim::GetAttr[name="memory_efficient"](%5388)
  %7200 : bool = prim::If(%7199) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7201 : bool = prim::Uninitialized()
      %7202 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7203 : bool = aten::gt(%7202, %24)
      %7204 : bool, %7205 : bool, %7206 : int = prim::Loop(%18, %7203, %19, %7201, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7207 : int, %7208 : bool, %7209 : bool, %7210 : int):
          %tensor.21 : Tensor = aten::__getitem__(%features.1, %7210) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7212 : bool = prim::requires_grad(%tensor.21)
          %7213 : bool, %7214 : bool = prim::If(%7212) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7201)
          %7215 : int = aten::add(%7210, %27)
          %7216 : bool = aten::lt(%7215, %7202)
          %7217 : bool = aten::__and__(%7216, %7213)
          -> (%7217, %7212, %7214, %7215)
      %7218 : bool = prim::If(%7204)
        block0():
          -> (%7205)
        block1():
          -> (%19)
      -> (%7218)
    block1():
      -> (%19)
  %bottleneck_output.40 : Tensor = prim::If(%7200) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7198)
    block1():
      %concated_features.21 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7221 : __torch__.torch.nn.modules.conv.___torch_mangle_277.Conv2d = prim::GetAttr[name="conv1"](%5388)
      %7222 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_276.BatchNorm2d = prim::GetAttr[name="norm1"](%5388)
      %7223 : int = aten::dim(%concated_features.21) # torch/nn/modules/batchnorm.py:276:11
      %7224 : bool = aten::ne(%7223, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7224) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7225 : bool = prim::GetAttr[name="training"](%7222)
       = prim::If(%7225) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7226 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7222)
          %7227 : Tensor = aten::add(%7226, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7222, %7227)
          -> ()
        block1():
          -> ()
      %7228 : bool = prim::GetAttr[name="training"](%7222)
      %7229 : Tensor = prim::GetAttr[name="running_mean"](%7222)
      %7230 : Tensor = prim::GetAttr[name="running_var"](%7222)
      %7231 : Tensor = prim::GetAttr[name="weight"](%7222)
      %7232 : Tensor = prim::GetAttr[name="bias"](%7222)
       = prim::If(%7228) # torch/nn/functional.py:2011:4
        block0():
          %7233 : int[] = aten::size(%concated_features.21) # torch/nn/functional.py:2012:27
          %size_prods.160 : int = aten::__getitem__(%7233, %24) # torch/nn/functional.py:1991:17
          %7235 : int = aten::len(%7233) # torch/nn/functional.py:1992:19
          %7236 : int = aten::sub(%7235, %26) # torch/nn/functional.py:1992:19
          %size_prods.161 : int = prim::Loop(%7236, %25, %size_prods.160) # torch/nn/functional.py:1992:4
            block0(%i.41 : int, %size_prods.162 : int):
              %7240 : int = aten::add(%i.41, %26) # torch/nn/functional.py:1993:27
              %7241 : int = aten::__getitem__(%7233, %7240) # torch/nn/functional.py:1993:22
              %size_prods.163 : int = aten::mul(%size_prods.162, %7241) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.163)
          %7243 : bool = aten::eq(%size_prods.161, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7243) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7244 : Tensor = aten::batch_norm(%concated_features.21, %7231, %7232, %7229, %7230, %7228, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.41 : Tensor = aten::relu_(%7244) # torch/nn/functional.py:1117:17
      %7246 : Tensor = prim::GetAttr[name="weight"](%7221)
      %7247 : Tensor? = prim::GetAttr[name="bias"](%7221)
      %7248 : int[] = prim::ListConstruct(%27, %27)
      %7249 : int[] = prim::ListConstruct(%24, %24)
      %7250 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.41 : Tensor = aten::conv2d(%result.41, %7246, %7247, %7248, %7249, %7250, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.41)
  %7252 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5388)
  %7253 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5388)
  %7254 : int = aten::dim(%bottleneck_output.40) # torch/nn/modules/batchnorm.py:276:11
  %7255 : bool = aten::ne(%7254, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7255) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7256 : bool = prim::GetAttr[name="training"](%7253)
   = prim::If(%7256) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7257 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7253)
      %7258 : Tensor = aten::add(%7257, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7253, %7258)
      -> ()
    block1():
      -> ()
  %7259 : bool = prim::GetAttr[name="training"](%7253)
  %7260 : Tensor = prim::GetAttr[name="running_mean"](%7253)
  %7261 : Tensor = prim::GetAttr[name="running_var"](%7253)
  %7262 : Tensor = prim::GetAttr[name="weight"](%7253)
  %7263 : Tensor = prim::GetAttr[name="bias"](%7253)
   = prim::If(%7259) # torch/nn/functional.py:2011:4
    block0():
      %7264 : int[] = aten::size(%bottleneck_output.40) # torch/nn/functional.py:2012:27
      %size_prods.164 : int = aten::__getitem__(%7264, %24) # torch/nn/functional.py:1991:17
      %7266 : int = aten::len(%7264) # torch/nn/functional.py:1992:19
      %7267 : int = aten::sub(%7266, %26) # torch/nn/functional.py:1992:19
      %size_prods.165 : int = prim::Loop(%7267, %25, %size_prods.164) # torch/nn/functional.py:1992:4
        block0(%i.42 : int, %size_prods.166 : int):
          %7271 : int = aten::add(%i.42, %26) # torch/nn/functional.py:1993:27
          %7272 : int = aten::__getitem__(%7264, %7271) # torch/nn/functional.py:1993:22
          %size_prods.167 : int = aten::mul(%size_prods.166, %7272) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.167)
      %7274 : bool = aten::eq(%size_prods.165, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7274) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7275 : Tensor = aten::batch_norm(%bottleneck_output.40, %7262, %7263, %7260, %7261, %7259, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.42 : Tensor = aten::relu_(%7275) # torch/nn/functional.py:1117:17
  %7277 : Tensor = prim::GetAttr[name="weight"](%7252)
  %7278 : Tensor? = prim::GetAttr[name="bias"](%7252)
  %7279 : int[] = prim::ListConstruct(%27, %27)
  %7280 : int[] = prim::ListConstruct(%27, %27)
  %7281 : int[] = prim::ListConstruct(%27, %27)
  %new_features.42 : Tensor = aten::conv2d(%result.42, %7277, %7278, %7279, %7280, %7281, %27) # torch/nn/modules/conv.py:415:15
  %7283 : float = prim::GetAttr[name="drop_rate"](%5388)
  %7284 : bool = aten::gt(%7283, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.39 : Tensor = prim::If(%7284) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7286 : float = prim::GetAttr[name="drop_rate"](%5388)
      %7287 : bool = prim::GetAttr[name="training"](%5388)
      %7288 : bool = aten::lt(%7286, %16) # torch/nn/functional.py:968:7
      %7289 : bool = prim::If(%7288) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7290 : bool = aten::gt(%7286, %17) # torch/nn/functional.py:968:17
          -> (%7290)
       = prim::If(%7289) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7291 : Tensor = aten::dropout(%new_features.42, %7286, %7287) # torch/nn/functional.py:973:17
      -> (%7291)
    block1():
      -> (%new_features.42)
  %7292 : Tensor[] = aten::append(%features.1, %new_features.39) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7293 : Tensor = prim::Uninitialized()
  %7294 : bool = prim::GetAttr[name="memory_efficient"](%5389)
  %7295 : bool = prim::If(%7294) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7296 : bool = prim::Uninitialized()
      %7297 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7298 : bool = aten::gt(%7297, %24)
      %7299 : bool, %7300 : bool, %7301 : int = prim::Loop(%18, %7298, %19, %7296, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7302 : int, %7303 : bool, %7304 : bool, %7305 : int):
          %tensor.22 : Tensor = aten::__getitem__(%features.1, %7305) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7307 : bool = prim::requires_grad(%tensor.22)
          %7308 : bool, %7309 : bool = prim::If(%7307) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7296)
          %7310 : int = aten::add(%7305, %27)
          %7311 : bool = aten::lt(%7310, %7297)
          %7312 : bool = aten::__and__(%7311, %7308)
          -> (%7312, %7307, %7309, %7310)
      %7313 : bool = prim::If(%7299)
        block0():
          -> (%7300)
        block1():
          -> (%19)
      -> (%7313)
    block1():
      -> (%19)
  %bottleneck_output.42 : Tensor = prim::If(%7295) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7293)
    block1():
      %concated_features.22 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7316 : __torch__.torch.nn.modules.conv.___torch_mangle_280.Conv2d = prim::GetAttr[name="conv1"](%5389)
      %7317 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_279.BatchNorm2d = prim::GetAttr[name="norm1"](%5389)
      %7318 : int = aten::dim(%concated_features.22) # torch/nn/modules/batchnorm.py:276:11
      %7319 : bool = aten::ne(%7318, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7319) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7320 : bool = prim::GetAttr[name="training"](%7317)
       = prim::If(%7320) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7321 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7317)
          %7322 : Tensor = aten::add(%7321, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7317, %7322)
          -> ()
        block1():
          -> ()
      %7323 : bool = prim::GetAttr[name="training"](%7317)
      %7324 : Tensor = prim::GetAttr[name="running_mean"](%7317)
      %7325 : Tensor = prim::GetAttr[name="running_var"](%7317)
      %7326 : Tensor = prim::GetAttr[name="weight"](%7317)
      %7327 : Tensor = prim::GetAttr[name="bias"](%7317)
       = prim::If(%7323) # torch/nn/functional.py:2011:4
        block0():
          %7328 : int[] = aten::size(%concated_features.22) # torch/nn/functional.py:2012:27
          %size_prods.168 : int = aten::__getitem__(%7328, %24) # torch/nn/functional.py:1991:17
          %7330 : int = aten::len(%7328) # torch/nn/functional.py:1992:19
          %7331 : int = aten::sub(%7330, %26) # torch/nn/functional.py:1992:19
          %size_prods.169 : int = prim::Loop(%7331, %25, %size_prods.168) # torch/nn/functional.py:1992:4
            block0(%i.43 : int, %size_prods.170 : int):
              %7335 : int = aten::add(%i.43, %26) # torch/nn/functional.py:1993:27
              %7336 : int = aten::__getitem__(%7328, %7335) # torch/nn/functional.py:1993:22
              %size_prods.171 : int = aten::mul(%size_prods.170, %7336) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.171)
          %7338 : bool = aten::eq(%size_prods.169, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7338) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7339 : Tensor = aten::batch_norm(%concated_features.22, %7326, %7327, %7324, %7325, %7323, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.43 : Tensor = aten::relu_(%7339) # torch/nn/functional.py:1117:17
      %7341 : Tensor = prim::GetAttr[name="weight"](%7316)
      %7342 : Tensor? = prim::GetAttr[name="bias"](%7316)
      %7343 : int[] = prim::ListConstruct(%27, %27)
      %7344 : int[] = prim::ListConstruct(%24, %24)
      %7345 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.43 : Tensor = aten::conv2d(%result.43, %7341, %7342, %7343, %7344, %7345, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.43)
  %7347 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5389)
  %7348 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5389)
  %7349 : int = aten::dim(%bottleneck_output.42) # torch/nn/modules/batchnorm.py:276:11
  %7350 : bool = aten::ne(%7349, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7350) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7351 : bool = prim::GetAttr[name="training"](%7348)
   = prim::If(%7351) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7352 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7348)
      %7353 : Tensor = aten::add(%7352, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7348, %7353)
      -> ()
    block1():
      -> ()
  %7354 : bool = prim::GetAttr[name="training"](%7348)
  %7355 : Tensor = prim::GetAttr[name="running_mean"](%7348)
  %7356 : Tensor = prim::GetAttr[name="running_var"](%7348)
  %7357 : Tensor = prim::GetAttr[name="weight"](%7348)
  %7358 : Tensor = prim::GetAttr[name="bias"](%7348)
   = prim::If(%7354) # torch/nn/functional.py:2011:4
    block0():
      %7359 : int[] = aten::size(%bottleneck_output.42) # torch/nn/functional.py:2012:27
      %size_prods.172 : int = aten::__getitem__(%7359, %24) # torch/nn/functional.py:1991:17
      %7361 : int = aten::len(%7359) # torch/nn/functional.py:1992:19
      %7362 : int = aten::sub(%7361, %26) # torch/nn/functional.py:1992:19
      %size_prods.173 : int = prim::Loop(%7362, %25, %size_prods.172) # torch/nn/functional.py:1992:4
        block0(%i.44 : int, %size_prods.174 : int):
          %7366 : int = aten::add(%i.44, %26) # torch/nn/functional.py:1993:27
          %7367 : int = aten::__getitem__(%7359, %7366) # torch/nn/functional.py:1993:22
          %size_prods.175 : int = aten::mul(%size_prods.174, %7367) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.175)
      %7369 : bool = aten::eq(%size_prods.173, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7369) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7370 : Tensor = aten::batch_norm(%bottleneck_output.42, %7357, %7358, %7355, %7356, %7354, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.44 : Tensor = aten::relu_(%7370) # torch/nn/functional.py:1117:17
  %7372 : Tensor = prim::GetAttr[name="weight"](%7347)
  %7373 : Tensor? = prim::GetAttr[name="bias"](%7347)
  %7374 : int[] = prim::ListConstruct(%27, %27)
  %7375 : int[] = prim::ListConstruct(%27, %27)
  %7376 : int[] = prim::ListConstruct(%27, %27)
  %new_features.44 : Tensor = aten::conv2d(%result.44, %7372, %7373, %7374, %7375, %7376, %27) # torch/nn/modules/conv.py:415:15
  %7378 : float = prim::GetAttr[name="drop_rate"](%5389)
  %7379 : bool = aten::gt(%7378, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.41 : Tensor = prim::If(%7379) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7381 : float = prim::GetAttr[name="drop_rate"](%5389)
      %7382 : bool = prim::GetAttr[name="training"](%5389)
      %7383 : bool = aten::lt(%7381, %16) # torch/nn/functional.py:968:7
      %7384 : bool = prim::If(%7383) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7385 : bool = aten::gt(%7381, %17) # torch/nn/functional.py:968:17
          -> (%7385)
       = prim::If(%7384) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7386 : Tensor = aten::dropout(%new_features.44, %7381, %7382) # torch/nn/functional.py:973:17
      -> (%7386)
    block1():
      -> (%new_features.44)
  %7387 : Tensor[] = aten::append(%features.1, %new_features.41) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7388 : Tensor = prim::Uninitialized()
  %7389 : bool = prim::GetAttr[name="memory_efficient"](%5390)
  %7390 : bool = prim::If(%7389) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7391 : bool = prim::Uninitialized()
      %7392 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7393 : bool = aten::gt(%7392, %24)
      %7394 : bool, %7395 : bool, %7396 : int = prim::Loop(%18, %7393, %19, %7391, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7397 : int, %7398 : bool, %7399 : bool, %7400 : int):
          %tensor.23 : Tensor = aten::__getitem__(%features.1, %7400) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7402 : bool = prim::requires_grad(%tensor.23)
          %7403 : bool, %7404 : bool = prim::If(%7402) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7391)
          %7405 : int = aten::add(%7400, %27)
          %7406 : bool = aten::lt(%7405, %7392)
          %7407 : bool = aten::__and__(%7406, %7403)
          -> (%7407, %7402, %7404, %7405)
      %7408 : bool = prim::If(%7394)
        block0():
          -> (%7395)
        block1():
          -> (%19)
      -> (%7408)
    block1():
      -> (%19)
  %bottleneck_output.44 : Tensor = prim::If(%7390) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7388)
    block1():
      %concated_features.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7411 : __torch__.torch.nn.modules.conv.___torch_mangle_283.Conv2d = prim::GetAttr[name="conv1"](%5390)
      %7412 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_282.BatchNorm2d = prim::GetAttr[name="norm1"](%5390)
      %7413 : int = aten::dim(%concated_features.23) # torch/nn/modules/batchnorm.py:276:11
      %7414 : bool = aten::ne(%7413, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7414) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7415 : bool = prim::GetAttr[name="training"](%7412)
       = prim::If(%7415) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7416 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7412)
          %7417 : Tensor = aten::add(%7416, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7412, %7417)
          -> ()
        block1():
          -> ()
      %7418 : bool = prim::GetAttr[name="training"](%7412)
      %7419 : Tensor = prim::GetAttr[name="running_mean"](%7412)
      %7420 : Tensor = prim::GetAttr[name="running_var"](%7412)
      %7421 : Tensor = prim::GetAttr[name="weight"](%7412)
      %7422 : Tensor = prim::GetAttr[name="bias"](%7412)
       = prim::If(%7418) # torch/nn/functional.py:2011:4
        block0():
          %7423 : int[] = aten::size(%concated_features.23) # torch/nn/functional.py:2012:27
          %size_prods.176 : int = aten::__getitem__(%7423, %24) # torch/nn/functional.py:1991:17
          %7425 : int = aten::len(%7423) # torch/nn/functional.py:1992:19
          %7426 : int = aten::sub(%7425, %26) # torch/nn/functional.py:1992:19
          %size_prods.177 : int = prim::Loop(%7426, %25, %size_prods.176) # torch/nn/functional.py:1992:4
            block0(%i.45 : int, %size_prods.178 : int):
              %7430 : int = aten::add(%i.45, %26) # torch/nn/functional.py:1993:27
              %7431 : int = aten::__getitem__(%7423, %7430) # torch/nn/functional.py:1993:22
              %size_prods.179 : int = aten::mul(%size_prods.178, %7431) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.179)
          %7433 : bool = aten::eq(%size_prods.177, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7433) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7434 : Tensor = aten::batch_norm(%concated_features.23, %7421, %7422, %7419, %7420, %7418, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.45 : Tensor = aten::relu_(%7434) # torch/nn/functional.py:1117:17
      %7436 : Tensor = prim::GetAttr[name="weight"](%7411)
      %7437 : Tensor? = prim::GetAttr[name="bias"](%7411)
      %7438 : int[] = prim::ListConstruct(%27, %27)
      %7439 : int[] = prim::ListConstruct(%24, %24)
      %7440 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.45 : Tensor = aten::conv2d(%result.45, %7436, %7437, %7438, %7439, %7440, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.45)
  %7442 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5390)
  %7443 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5390)
  %7444 : int = aten::dim(%bottleneck_output.44) # torch/nn/modules/batchnorm.py:276:11
  %7445 : bool = aten::ne(%7444, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7445) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7446 : bool = prim::GetAttr[name="training"](%7443)
   = prim::If(%7446) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7447 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7443)
      %7448 : Tensor = aten::add(%7447, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7443, %7448)
      -> ()
    block1():
      -> ()
  %7449 : bool = prim::GetAttr[name="training"](%7443)
  %7450 : Tensor = prim::GetAttr[name="running_mean"](%7443)
  %7451 : Tensor = prim::GetAttr[name="running_var"](%7443)
  %7452 : Tensor = prim::GetAttr[name="weight"](%7443)
  %7453 : Tensor = prim::GetAttr[name="bias"](%7443)
   = prim::If(%7449) # torch/nn/functional.py:2011:4
    block0():
      %7454 : int[] = aten::size(%bottleneck_output.44) # torch/nn/functional.py:2012:27
      %size_prods.180 : int = aten::__getitem__(%7454, %24) # torch/nn/functional.py:1991:17
      %7456 : int = aten::len(%7454) # torch/nn/functional.py:1992:19
      %7457 : int = aten::sub(%7456, %26) # torch/nn/functional.py:1992:19
      %size_prods.181 : int = prim::Loop(%7457, %25, %size_prods.180) # torch/nn/functional.py:1992:4
        block0(%i.46 : int, %size_prods.182 : int):
          %7461 : int = aten::add(%i.46, %26) # torch/nn/functional.py:1993:27
          %7462 : int = aten::__getitem__(%7454, %7461) # torch/nn/functional.py:1993:22
          %size_prods.183 : int = aten::mul(%size_prods.182, %7462) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.183)
      %7464 : bool = aten::eq(%size_prods.181, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7464) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7465 : Tensor = aten::batch_norm(%bottleneck_output.44, %7452, %7453, %7450, %7451, %7449, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.46 : Tensor = aten::relu_(%7465) # torch/nn/functional.py:1117:17
  %7467 : Tensor = prim::GetAttr[name="weight"](%7442)
  %7468 : Tensor? = prim::GetAttr[name="bias"](%7442)
  %7469 : int[] = prim::ListConstruct(%27, %27)
  %7470 : int[] = prim::ListConstruct(%27, %27)
  %7471 : int[] = prim::ListConstruct(%27, %27)
  %new_features.46 : Tensor = aten::conv2d(%result.46, %7467, %7468, %7469, %7470, %7471, %27) # torch/nn/modules/conv.py:415:15
  %7473 : float = prim::GetAttr[name="drop_rate"](%5390)
  %7474 : bool = aten::gt(%7473, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.43 : Tensor = prim::If(%7474) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7476 : float = prim::GetAttr[name="drop_rate"](%5390)
      %7477 : bool = prim::GetAttr[name="training"](%5390)
      %7478 : bool = aten::lt(%7476, %16) # torch/nn/functional.py:968:7
      %7479 : bool = prim::If(%7478) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7480 : bool = aten::gt(%7476, %17) # torch/nn/functional.py:968:17
          -> (%7480)
       = prim::If(%7479) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7481 : Tensor = aten::dropout(%new_features.46, %7476, %7477) # torch/nn/functional.py:973:17
      -> (%7481)
    block1():
      -> (%new_features.46)
  %7482 : Tensor[] = aten::append(%features.1, %new_features.43) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7483 : Tensor = prim::Uninitialized()
  %7484 : bool = prim::GetAttr[name="memory_efficient"](%5391)
  %7485 : bool = prim::If(%7484) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7486 : bool = prim::Uninitialized()
      %7487 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7488 : bool = aten::gt(%7487, %24)
      %7489 : bool, %7490 : bool, %7491 : int = prim::Loop(%18, %7488, %19, %7486, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7492 : int, %7493 : bool, %7494 : bool, %7495 : int):
          %tensor.24 : Tensor = aten::__getitem__(%features.1, %7495) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7497 : bool = prim::requires_grad(%tensor.24)
          %7498 : bool, %7499 : bool = prim::If(%7497) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7486)
          %7500 : int = aten::add(%7495, %27)
          %7501 : bool = aten::lt(%7500, %7487)
          %7502 : bool = aten::__and__(%7501, %7498)
          -> (%7502, %7497, %7499, %7500)
      %7503 : bool = prim::If(%7489)
        block0():
          -> (%7490)
        block1():
          -> (%19)
      -> (%7503)
    block1():
      -> (%19)
  %bottleneck_output.46 : Tensor = prim::If(%7485) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7483)
    block1():
      %concated_features.24 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7506 : __torch__.torch.nn.modules.conv.___torch_mangle_289.Conv2d = prim::GetAttr[name="conv1"](%5391)
      %7507 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_286.BatchNorm2d = prim::GetAttr[name="norm1"](%5391)
      %7508 : int = aten::dim(%concated_features.24) # torch/nn/modules/batchnorm.py:276:11
      %7509 : bool = aten::ne(%7508, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7509) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7510 : bool = prim::GetAttr[name="training"](%7507)
       = prim::If(%7510) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7511 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7507)
          %7512 : Tensor = aten::add(%7511, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7507, %7512)
          -> ()
        block1():
          -> ()
      %7513 : bool = prim::GetAttr[name="training"](%7507)
      %7514 : Tensor = prim::GetAttr[name="running_mean"](%7507)
      %7515 : Tensor = prim::GetAttr[name="running_var"](%7507)
      %7516 : Tensor = prim::GetAttr[name="weight"](%7507)
      %7517 : Tensor = prim::GetAttr[name="bias"](%7507)
       = prim::If(%7513) # torch/nn/functional.py:2011:4
        block0():
          %7518 : int[] = aten::size(%concated_features.24) # torch/nn/functional.py:2012:27
          %size_prods.184 : int = aten::__getitem__(%7518, %24) # torch/nn/functional.py:1991:17
          %7520 : int = aten::len(%7518) # torch/nn/functional.py:1992:19
          %7521 : int = aten::sub(%7520, %26) # torch/nn/functional.py:1992:19
          %size_prods.185 : int = prim::Loop(%7521, %25, %size_prods.184) # torch/nn/functional.py:1992:4
            block0(%i.47 : int, %size_prods.186 : int):
              %7525 : int = aten::add(%i.47, %26) # torch/nn/functional.py:1993:27
              %7526 : int = aten::__getitem__(%7518, %7525) # torch/nn/functional.py:1993:22
              %size_prods.187 : int = aten::mul(%size_prods.186, %7526) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.187)
          %7528 : bool = aten::eq(%size_prods.185, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7528) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7529 : Tensor = aten::batch_norm(%concated_features.24, %7516, %7517, %7514, %7515, %7513, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.47 : Tensor = aten::relu_(%7529) # torch/nn/functional.py:1117:17
      %7531 : Tensor = prim::GetAttr[name="weight"](%7506)
      %7532 : Tensor? = prim::GetAttr[name="bias"](%7506)
      %7533 : int[] = prim::ListConstruct(%27, %27)
      %7534 : int[] = prim::ListConstruct(%24, %24)
      %7535 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.47 : Tensor = aten::conv2d(%result.47, %7531, %7532, %7533, %7534, %7535, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.47)
  %7537 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5391)
  %7538 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5391)
  %7539 : int = aten::dim(%bottleneck_output.46) # torch/nn/modules/batchnorm.py:276:11
  %7540 : bool = aten::ne(%7539, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7540) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7541 : bool = prim::GetAttr[name="training"](%7538)
   = prim::If(%7541) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7542 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7538)
      %7543 : Tensor = aten::add(%7542, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7538, %7543)
      -> ()
    block1():
      -> ()
  %7544 : bool = prim::GetAttr[name="training"](%7538)
  %7545 : Tensor = prim::GetAttr[name="running_mean"](%7538)
  %7546 : Tensor = prim::GetAttr[name="running_var"](%7538)
  %7547 : Tensor = prim::GetAttr[name="weight"](%7538)
  %7548 : Tensor = prim::GetAttr[name="bias"](%7538)
   = prim::If(%7544) # torch/nn/functional.py:2011:4
    block0():
      %7549 : int[] = aten::size(%bottleneck_output.46) # torch/nn/functional.py:2012:27
      %size_prods.188 : int = aten::__getitem__(%7549, %24) # torch/nn/functional.py:1991:17
      %7551 : int = aten::len(%7549) # torch/nn/functional.py:1992:19
      %7552 : int = aten::sub(%7551, %26) # torch/nn/functional.py:1992:19
      %size_prods.189 : int = prim::Loop(%7552, %25, %size_prods.188) # torch/nn/functional.py:1992:4
        block0(%i.48 : int, %size_prods.190 : int):
          %7556 : int = aten::add(%i.48, %26) # torch/nn/functional.py:1993:27
          %7557 : int = aten::__getitem__(%7549, %7556) # torch/nn/functional.py:1993:22
          %size_prods.191 : int = aten::mul(%size_prods.190, %7557) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.191)
      %7559 : bool = aten::eq(%size_prods.189, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7559) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7560 : Tensor = aten::batch_norm(%bottleneck_output.46, %7547, %7548, %7545, %7546, %7544, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.48 : Tensor = aten::relu_(%7560) # torch/nn/functional.py:1117:17
  %7562 : Tensor = prim::GetAttr[name="weight"](%7537)
  %7563 : Tensor? = prim::GetAttr[name="bias"](%7537)
  %7564 : int[] = prim::ListConstruct(%27, %27)
  %7565 : int[] = prim::ListConstruct(%27, %27)
  %7566 : int[] = prim::ListConstruct(%27, %27)
  %new_features.48 : Tensor = aten::conv2d(%result.48, %7562, %7563, %7564, %7565, %7566, %27) # torch/nn/modules/conv.py:415:15
  %7568 : float = prim::GetAttr[name="drop_rate"](%5391)
  %7569 : bool = aten::gt(%7568, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.45 : Tensor = prim::If(%7569) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7571 : float = prim::GetAttr[name="drop_rate"](%5391)
      %7572 : bool = prim::GetAttr[name="training"](%5391)
      %7573 : bool = aten::lt(%7571, %16) # torch/nn/functional.py:968:7
      %7574 : bool = prim::If(%7573) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7575 : bool = aten::gt(%7571, %17) # torch/nn/functional.py:968:17
          -> (%7575)
       = prim::If(%7574) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7576 : Tensor = aten::dropout(%new_features.48, %7571, %7572) # torch/nn/functional.py:973:17
      -> (%7576)
    block1():
      -> (%new_features.48)
  %7577 : Tensor[] = aten::append(%features.1, %new_features.45) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %7578 : Tensor = prim::Uninitialized()
  %7579 : bool = prim::GetAttr[name="memory_efficient"](%5392)
  %7580 : bool = prim::If(%7579) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:11
    block0():
      %7581 : bool = prim::Uninitialized()
      %7582 : int = aten::len(%features.1) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
      %7583 : bool = aten::gt(%7582, %24)
      %7584 : bool, %7585 : bool, %7586 : int = prim::Loop(%18, %7583, %19, %7581, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
        block0(%7587 : int, %7588 : bool, %7589 : bool, %7590 : int):
          %tensor.1 : Tensor = aten::__getitem__(%features.1, %7590) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:47:8
          %7592 : bool = prim::requires_grad(%tensor.1)
          %7593 : bool, %7594 : bool = prim::If(%7592) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:48:12
            block0():
              -> (%19, %25)
            block1():
              -> (%25, %7581)
          %7595 : int = aten::add(%7590, %27)
          %7596 : bool = aten::lt(%7595, %7582)
          %7597 : bool = aten::__and__(%7596, %7593)
          -> (%7597, %7592, %7594, %7595)
      %7598 : bool = prim::If(%7584)
        block0():
          -> (%7585)
        block1():
          -> (%19)
      -> (%7598)
    block1():
      -> (%19)
  %bottleneck_output : Tensor = prim::If(%7580) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:78:8
    block0():
       = prim::RaiseException(%23) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:80:16
      -> (%7578)
    block1():
      %concated_features.1 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:40:28
      %7601 : __torch__.torch.nn.modules.conv.___torch_mangle_292.Conv2d = prim::GetAttr[name="conv1"](%5392)
      %7602 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_291.BatchNorm2d = prim::GetAttr[name="norm1"](%5392)
      %7603 : int = aten::dim(%concated_features.1) # torch/nn/modules/batchnorm.py:276:11
      %7604 : bool = aten::ne(%7603, %22) # torch/nn/modules/batchnorm.py:276:11
       = prim::If(%7604) # torch/nn/modules/batchnorm.py:276:8
        block0():
           = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
          -> ()
        block1():
          -> ()
      %7605 : bool = prim::GetAttr[name="training"](%7602)
       = prim::If(%7605) # torch/nn/modules/batchnorm.py:110:11
        block0():
          %7606 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7602)
          %7607 : Tensor = aten::add(%7606, %27, %27) # torch/nn/modules/batchnorm.py:113:43
           = prim::SetAttr[name="num_batches_tracked"](%7602, %7607)
          -> ()
        block1():
          -> ()
      %7608 : bool = prim::GetAttr[name="training"](%7602)
      %7609 : Tensor = prim::GetAttr[name="running_mean"](%7602)
      %7610 : Tensor = prim::GetAttr[name="running_var"](%7602)
      %7611 : Tensor = prim::GetAttr[name="weight"](%7602)
      %7612 : Tensor = prim::GetAttr[name="bias"](%7602)
       = prim::If(%7608) # torch/nn/functional.py:2011:4
        block0():
          %7613 : int[] = aten::size(%concated_features.1) # torch/nn/functional.py:2012:27
          %size_prods.2 : int = aten::__getitem__(%7613, %24) # torch/nn/functional.py:1991:17
          %7615 : int = aten::len(%7613) # torch/nn/functional.py:1992:19
          %7616 : int = aten::sub(%7615, %26) # torch/nn/functional.py:1992:19
          %size_prods.4 : int = prim::Loop(%7616, %25, %size_prods.2) # torch/nn/functional.py:1992:4
            block0(%i.2 : int, %size_prods.7 : int):
              %7620 : int = aten::add(%i.2, %26) # torch/nn/functional.py:1993:27
              %7621 : int = aten::__getitem__(%7613, %7620) # torch/nn/functional.py:1993:22
              %size_prods.5 : int = aten::mul(%size_prods.7, %7621) # torch/nn/functional.py:1993:8
              -> (%25, %size_prods.5)
          %7623 : bool = aten::eq(%size_prods.4, %27) # torch/nn/functional.py:1994:7
           = prim::If(%7623) # torch/nn/functional.py:1994:4
            block0():
               = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
              -> ()
            block1():
              -> ()
          -> ()
        block1():
          -> ()
      %7624 : Tensor = aten::batch_norm(%concated_features.1, %7611, %7612, %7609, %7610, %7608, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
      %result.158 : Tensor = aten::relu_(%7624) # torch/nn/functional.py:1117:17
      %7626 : Tensor = prim::GetAttr[name="weight"](%7601)
      %7627 : Tensor? = prim::GetAttr[name="bias"](%7601)
      %7628 : int[] = prim::ListConstruct(%27, %27)
      %7629 : int[] = prim::ListConstruct(%24, %24)
      %7630 : int[] = prim::ListConstruct(%27, %27)
      %bottleneck_output.2 : Tensor = aten::conv2d(%result.158, %7626, %7627, %7628, %7629, %7630, %27) # torch/nn/modules/conv.py:415:15
      -> (%bottleneck_output.2)
  %7632 : __torch__.torch.nn.modules.conv.___torch_mangle_164.Conv2d = prim::GetAttr[name="conv2"](%5392)
  %7633 : __torch__.torch.nn.modules.batchnorm.___torch_mangle_81.BatchNorm2d = prim::GetAttr[name="norm2"](%5392)
  %7634 : int = aten::dim(%bottleneck_output) # torch/nn/modules/batchnorm.py:276:11
  %7635 : bool = aten::ne(%7634, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7635) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7636 : bool = prim::GetAttr[name="training"](%7633)
   = prim::If(%7636) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7637 : Tensor = prim::GetAttr[name="num_batches_tracked"](%7633)
      %7638 : Tensor = aten::add(%7637, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%7633, %7638)
      -> ()
    block1():
      -> ()
  %7639 : bool = prim::GetAttr[name="training"](%7633)
  %7640 : Tensor = prim::GetAttr[name="running_mean"](%7633)
  %7641 : Tensor = prim::GetAttr[name="running_var"](%7633)
  %7642 : Tensor = prim::GetAttr[name="weight"](%7633)
  %7643 : Tensor = prim::GetAttr[name="bias"](%7633)
   = prim::If(%7639) # torch/nn/functional.py:2011:4
    block0():
      %7644 : int[] = aten::size(%bottleneck_output) # torch/nn/functional.py:2012:27
      %size_prods.640 : int = aten::__getitem__(%7644, %24) # torch/nn/functional.py:1991:17
      %7646 : int = aten::len(%7644) # torch/nn/functional.py:1992:19
      %7647 : int = aten::sub(%7646, %26) # torch/nn/functional.py:1992:19
      %size_prods.641 : int = prim::Loop(%7647, %25, %size_prods.640) # torch/nn/functional.py:1992:4
        block0(%i.161 : int, %size_prods.642 : int):
          %7651 : int = aten::add(%i.161, %26) # torch/nn/functional.py:1993:27
          %7652 : int = aten::__getitem__(%7644, %7651) # torch/nn/functional.py:1993:22
          %size_prods.643 : int = aten::mul(%size_prods.642, %7652) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.643)
      %7654 : bool = aten::eq(%size_prods.641, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7654) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %7655 : Tensor = aten::batch_norm(%bottleneck_output, %7642, %7643, %7640, %7641, %7639, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %result.157 : Tensor = aten::relu_(%7655) # torch/nn/functional.py:1117:17
  %7657 : Tensor = prim::GetAttr[name="weight"](%7632)
  %7658 : Tensor? = prim::GetAttr[name="bias"](%7632)
  %7659 : int[] = prim::ListConstruct(%27, %27)
  %7660 : int[] = prim::ListConstruct(%27, %27)
  %7661 : int[] = prim::ListConstruct(%27, %27)
  %new_features.1 : Tensor = aten::conv2d(%result.157, %7657, %7658, %7659, %7660, %7661, %27) # torch/nn/modules/conv.py:415:15
  %7663 : float = prim::GetAttr[name="drop_rate"](%5392)
  %7664 : bool = aten::gt(%7663, %24) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:11
  %new_features.47 : Tensor = prim::If(%7664) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:87:8
    block0():
      %7666 : float = prim::GetAttr[name="drop_rate"](%5392)
      %7667 : bool = prim::GetAttr[name="training"](%5392)
      %7668 : bool = aten::lt(%7666, %16) # torch/nn/functional.py:968:7
      %7669 : bool = prim::If(%7668) # torch/nn/functional.py:968:7
        block0():
          -> (%25)
        block1():
          %7670 : bool = aten::gt(%7666, %17) # torch/nn/functional.py:968:17
          -> (%7670)
       = prim::If(%7669) # torch/nn/functional.py:968:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:969:8
          -> ()
        block1():
          -> ()
      %7671 : Tensor = aten::dropout(%new_features.1, %7666, %7667) # torch/nn/functional.py:973:17
      -> (%7671)
    block1():
      -> (%new_features.1)
  %7672 : Tensor[] = aten::append(%features.1, %new_features.47) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:112:12
  %input.23 : Tensor = aten::cat(%features.1, %27) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:113:15
  %7674 : int = aten::dim(%input.23) # torch/nn/modules/batchnorm.py:276:11
  %7675 : bool = aten::ne(%7674, %22) # torch/nn/modules/batchnorm.py:276:11
   = prim::If(%7675) # torch/nn/modules/batchnorm.py:276:8
    block0():
       = prim::RaiseException(%23) # torch/nn/modules/batchnorm.py:277:12
      -> ()
    block1():
      -> ()
  %7676 : bool = prim::GetAttr[name="training"](%38)
   = prim::If(%7676) # torch/nn/modules/batchnorm.py:110:11
    block0():
      %7677 : Tensor = prim::GetAttr[name="num_batches_tracked"](%38)
      %7678 : Tensor = aten::add(%7677, %27, %27) # torch/nn/modules/batchnorm.py:113:43
       = prim::SetAttr[name="num_batches_tracked"](%38, %7678)
      -> ()
    block1():
      -> ()
  %7679 : bool = prim::GetAttr[name="training"](%38)
  %7680 : Tensor = prim::GetAttr[name="running_mean"](%38)
  %7681 : Tensor = prim::GetAttr[name="running_var"](%38)
  %7682 : Tensor = prim::GetAttr[name="weight"](%38)
  %7683 : Tensor = prim::GetAttr[name="bias"](%38)
   = prim::If(%7679) # torch/nn/functional.py:2011:4
    block0():
      %7684 : int[] = aten::size(%input.23) # torch/nn/functional.py:2012:27
      %size_prods.1 : int = aten::__getitem__(%7684, %24) # torch/nn/functional.py:1991:17
      %7686 : int = aten::len(%7684) # torch/nn/functional.py:1992:19
      %7687 : int = aten::sub(%7686, %26) # torch/nn/functional.py:1992:19
      %size_prods : int = prim::Loop(%7687, %25, %size_prods.1) # torch/nn/functional.py:1992:4
        block0(%i.1 : int, %size_prods.6 : int):
          %7691 : int = aten::add(%i.1, %26) # torch/nn/functional.py:1993:27
          %7692 : int = aten::__getitem__(%7684, %7691) # torch/nn/functional.py:1993:22
          %size_prods.3 : int = aten::mul(%size_prods.6, %7692) # torch/nn/functional.py:1993:8
          -> (%25, %size_prods.3)
      %7694 : bool = aten::eq(%size_prods, %27) # torch/nn/functional.py:1994:7
       = prim::If(%7694) # torch/nn/functional.py:1994:4
        block0():
           = prim::RaiseException(%23) # torch/nn/functional.py:1995:8
          -> ()
        block1():
          -> ()
      -> ()
    block1():
      -> ()
  %features.5 : Tensor = aten::batch_norm(%input.23, %7682, %7683, %7680, %7681, %7679, %exponential_average_factor.2, %20, %25) # torch/nn/functional.py:2014:11
  %out.1 : Tensor = prim::If(%5) # torch/nn/functional.py:1116:4
    block0():
      %result.1 : Tensor = aten::relu_(%features.5) # torch/nn/functional.py:1117:17
      -> (%result.1)
    block1():
      %result.2 : Tensor = aten::relu(%features.5) # torch/nn/functional.py:1119:17
      -> (%result.2)
  %10 : int[] = prim::ListConstruct(%6, %6)
  %7699 : str = prim::Constant[value="Exception"]() # <string>:5:2
  %7700 : int[] = aten::size(%out.1) # torch/nn/functional.py:925:51
  %7701 : int = aten::len(%7700) # <string>:5:9
  %7702 : int = aten::len(%10) # <string>:5:25
  %7703 : bool = aten::gt(%7701, %7702) # <string>:5:9
   = prim::If(%7703) # <string>:5:2
    block0():
      -> ()
    block1():
       = prim::RaiseException(%7699) # <string>:5:2
      -> ()
  %out.3 : Tensor = aten::adaptive_avg_pool2d(%out.1, %10) # torch/nn/functional.py:926:11
  %out.5 : Tensor = aten::flatten(%out.3, %6, %2) # torch/hub/pytorch_vision_master/torchvision/models/densenet.py:195:14
  %13 : __torch__.torch.nn.modules.linear.___torch_mangle_297.Linear = prim::GetAttr[name="classifier"](%self)
  %7705 : int = prim::Constant[value=1]()
  %7706 : int = prim::Constant[value=2]() # torch/nn/functional.py:1672:22
  %7707 : Tensor = prim::GetAttr[name="weight"](%13)
  %7708 : Tensor = prim::GetAttr[name="bias"](%13)
  %7709 : int = aten::dim(%out.5) # torch/nn/functional.py:1672:7
  %7710 : bool = aten::eq(%7709, %7706) # torch/nn/functional.py:1672:7
  %out.7 : Tensor = prim::If(%7710) # torch/nn/functional.py:1672:4
    block0():
      %7712 : Tensor = aten::t(%7707) # torch/nn/functional.py:1674:39
      %ret.1 : Tensor = aten::addmm(%7708, %out.5, %7712, %7705, %7705) # torch/nn/functional.py:1674:14
      -> (%ret.1)
    block1():
      %7714 : Tensor = aten::t(%7707) # torch/nn/functional.py:1676:30
      %output.1 : Tensor = aten::matmul(%out.5, %7714) # torch/nn/functional.py:1676:17
      %output.3 : Tensor = aten::add_(%output.1, %7708, %7705) # torch/nn/functional.py:1678:12
      -> (%output.3)
  return (%out.7)
